(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(e){function n(n){for(var r,o,l=n[0],s=n[1],c=n[2],u=0,p=[];u<l.length;u++)o=l[u],Object.prototype.hasOwnProperty.call(a,o)&&a[o]&&p.push(a[o][0]),a[o]=0;for(r in s)Object.prototype.hasOwnProperty.call(s,r)&&(e[r]=s[r]);for(d&&d(n);p.length;)p.shift()();return i.push.apply(i,c||[]),t()}function t(){for(var e,n=0;n<i.length;n++){for(var t=i[n],r=!0,l=1;l<t.length;l++){var s=t[l];0!==a[s]&&(r=!1)}r&&(i.splice(n--,1),e=o(o.s=t[0]))}return e}var r={},a={1:0},i=[];function o(n){if(r[n])return r[n].exports;var t=r[n]={i:n,l:!1,exports:{}};return e[n].call(t.exports,t,t.exports,o),t.l=!0,t.exports}o.e=function(e){var n=[],t=a[e];if(0!==t)if(t)n.push(t[2]);else{var r=new Promise((function(n,r){t=a[e]=[n,r]}));n.push(t[2]=r);var i,l=document.createElement("script");l.charset="utf-8",l.timeout=120,o.nc&&l.setAttribute("nonce",o.nc),l.src=function(e){return o.p+"assets/js/"+({}[e]||e)+"."+{2:"933750c6",3:"5c09ac9d",4:"73518a0f",5:"94727770",6:"0b558b79",7:"7eb70372",8:"0ec7f6e8",9:"6e274fca",10:"2cc95be8",11:"c7521a10",12:"38d1989d",13:"850793b0",14:"1141ec27",15:"12d1dea8",16:"60cde607",17:"acb3bf3d",18:"e491edb6",19:"daf31819",20:"cb4a5c13",21:"24c0f101",22:"dd779f94",23:"4f896de4",24:"6ba53300",25:"41dca26b",26:"6337eac0",27:"de80589e",28:"95a1fb6b",29:"371a40e1",30:"e32f6b31",31:"a532917f",32:"13b70e91",33:"64ab8d0c",34:"6d3cc288",35:"98e20e9a",36:"be98cc35",37:"82be3556",38:"40fe2658",39:"84b6e82e",40:"591baf2a",41:"f961e422",42:"b34f1599",43:"5a509601",44:"64fd3383",45:"685623ac",46:"7624d38c",47:"5646b068",48:"a1b270df",49:"ee9681bc",50:"b424b2c0",51:"ef213b56",52:"18c75eba",53:"f76e624c",54:"3098ec9f",55:"4c46841d",56:"26c4fd80",57:"36581a94",58:"e5a83b70",59:"7b7e3faf",60:"66a7f41d",61:"52ab8e4e",62:"82fff0c9",63:"392eea42",64:"f22c5bcd",65:"1c5bfacf",66:"e67fcc68",67:"b666ce97",68:"1b76c178",69:"7ac52a2f",70:"7279b1df",71:"4f744076",72:"44c999da",73:"526febf1",74:"26f8d9a5",75:"dc7fdd0f",76:"32858593",77:"7b3f6980",78:"31a353ef",79:"e59dfac4",80:"fdcb5fa1",81:"d124398a",82:"6d0d2919",83:"65db62ac",84:"e5207e82",85:"f69467b2",86:"31709d48",87:"67fcabb9",88:"93192130",89:"e6cb2516",90:"cdc8eb8f",91:"8ad18b18",92:"b554f1c7",93:"fb2397b7",94:"1eade7f0",95:"96f5db04",96:"fc68b295",97:"0a9a6187",98:"10f4b27a",99:"4a895015",100:"2b21dce5",101:"96ca71f6"}[e]+".js"}(e);var s=new Error;i=function(n){l.onerror=l.onload=null,clearTimeout(c);var t=a[e];if(0!==t){if(t){var r=n&&("load"===n.type?"missing":n.type),i=n&&n.target&&n.target.src;s.message="Loading chunk "+e+" failed.\n("+r+": "+i+")",s.name="ChunkLoadError",s.type=r,s.request=i,t[1](s)}a[e]=void 0}};var c=setTimeout((function(){i({type:"timeout",target:l})}),12e4);l.onerror=l.onload=i,document.head.appendChild(l)}return Promise.all(n)},o.m=e,o.c=r,o.d=function(e,n,t){o.o(e,n)||Object.defineProperty(e,n,{enumerable:!0,get:t})},o.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},o.t=function(e,n){if(1&n&&(e=o(e)),8&n)return e;if(4&n&&"object"==typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(o.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&n&&"string"!=typeof e)for(var r in e)o.d(t,r,function(n){return e[n]}.bind(null,r));return t},o.n=function(e){var n=e&&e.__esModule?function(){return e.default}:function(){return e};return o.d(n,"a",n),n},o.o=function(e,n){return Object.prototype.hasOwnProperty.call(e,n)},o.p="/",o.oe=function(e){throw console.error(e),e};var l=window.webpackJsonp=window.webpackJsonp||[],s=l.push.bind(l);l.push=n,l=l.slice();for(var c=0;c<l.length;c++)n(l[c]);var d=s;i.push([107,0]),t()}([function(e,n,t){"use strict";var r=function(e){return e&&e.Math===Math&&e};e.exports=r("object"==typeof globalThis&&globalThis)||r("object"==typeof window&&window)||r("object"==typeof self&&self)||r("object"==typeof global&&global)||r("object"==typeof this&&this)||function(){return this}()||Function("return this")()},function(e,n,t){"use strict";var r="object"==typeof document&&document.all;e.exports=void 0===r&&void 0!==r?function(e){return"function"==typeof e||e===r}:function(e){return"function"==typeof e}},function(e,n,t){"use strict";var r=t(26),a=Function.prototype,i=a.call,o=r&&a.bind.bind(i,i);e.exports=r?o:function(e){return function(){return i.apply(e,arguments)}}},function(e,n,t){"use strict";e.exports=function(e){try{return!!e()}catch(e){return!0}}},function(e,n,t){"use strict";function r(e,n,t,r,a,i,o,l){var s,c="function"==typeof e?e.options:e;if(n&&(c.render=n,c.staticRenderFns=t,c._compiled=!0),r&&(c.functional=!0),i&&(c._scopeId="data-v-"+i),o?(s=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),a&&a.call(this,e),e&&e._registeredComponents&&e._registeredComponents.add(o)},c._ssrRegister=s):a&&(s=l?function(){a.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:a),s)if(c.functional){c._injectStyles=s;var d=c.render;c.render=function(e,n){return s.call(n),d(e,n)}}else{var u=c.beforeCreate;c.beforeCreate=u?[].concat(u,s):[s]}return{exports:e,options:c}}t.d(n,"a",(function(){return r}))},function(e,n,t){"use strict";var r=t(3);e.exports=!r((function(){return 7!==Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(e,n){var t=Array.isArray;e.exports=t},function(e,n,t){"use strict";var r=t(1);e.exports=function(e){return"object"==typeof e?null!==e:r(e)}},function(e,n,t){var r=t(69),a="object"==typeof self&&self&&self.Object===Object&&self,i=r||a||Function("return this")();e.exports=i},function(e,n,t){"use strict";var r=t(2),a=t(31),i=r({}.hasOwnProperty);e.exports=Object.hasOwn||function(e,n){return i(a(e),n)}},function(e,n,t){var r=t(165),a=t(168);e.exports=function(e,n){var t=a(e,n);return r(t)?t:void 0}},function(e,n,t){"use strict";t.d(n,"e",(function(){return r})),t.d(n,"b",(function(){return i})),t.d(n,"j",(function(){return o})),t.d(n,"g",(function(){return s})),t.d(n,"h",(function(){return c})),t.d(n,"i",(function(){return d})),t.d(n,"c",(function(){return u})),t.d(n,"f",(function(){return p})),t.d(n,"l",(function(){return h})),t.d(n,"m",(function(){return f})),t.d(n,"d",(function(){return b})),t.d(n,"k",(function(){return v})),t.d(n,"n",(function(){return y})),t.d(n,"a",(function(){return k}));t(16);const r=/#.*$/,a=/\.(md|html)$/,i=/\/$/,o=/^[a-z]+:/i;function l(e){return decodeURI(e).replace(r,"").replace(a,"")}function s(e){return o.test(e)}function c(e){return/^mailto:/.test(e)}function d(e){return/^tel:/.test(e)}function u(e){if(s(e))return e;if(!e)return"404";const n=e.match(r),t=n?n[0]:"",a=l(e);return i.test(a)?e:a+".html"+t}function p(e,n){const t=e.hash,a=function(e){const n=e&&e.match(r);if(n)return n[0]}(n);if(a&&t!==a)return!1;return l(e.path)===l(n)}function h(e,n,t){if(s(n))return{type:"external",path:n};t&&(n=function(e,n,t){const r=e.charAt(0);if("/"===r)return e;if("?"===r||"#"===r)return n+e;const a=n.split("/");t&&a[a.length-1]||a.pop();const i=e.replace(/^\//,"").split("/");for(let e=0;e<i.length;e++){const n=i[e];".."===n?a.pop():"."!==n&&a.push(n)}""!==a[0]&&a.unshift("");return a.join("/")}(n,t));const r=l(n);for(let n=0;n<e.length;n++)if(l(e[n].regularPath)===r)return Object.assign({},e[n],{type:"page",path:u(e[n].path)});return console.error(`[vuepress] No matching page found for sidebar item "${n}"`),{}}function f(e,n,t,r){const{pages:a,themeConfig:i}=t,o=r&&i.locales&&i.locales[r]||i;if("auto"===(e.frontmatter.sidebar||o.sidebar||i.sidebar))return m(e);const l=o.sidebar||i.sidebar;if(l){const{base:t,config:r}=function(e,n){if(Array.isArray(n))return{base:"/",config:n};for(const r in n)if(0===(t=e,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(r)))return{base:r,config:n[r]};var t;return{}}(n,l);return"auto"===r?m(e):r?r.map(e=>function e(n,t,r,a=1){if("string"==typeof n)return h(t,n,r);if(Array.isArray(n))return Object.assign(h(t,n[0],r),{title:n[1]});{a>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const i=n.children||[];return 0===i.length&&n.path?Object.assign(h(t,n.path,r),{title:n.title}):{type:"group",path:n.path,title:n.title,sidebarDepth:n.sidebarDepth,initialOpenGroupIndex:n.initialOpenGroupIndex,children:i.map(n=>e(n,t,r,a+1)),collapsable:!1!==n.collapsable}}}(e,a,t)):[]}return[]}function m(e){const n=b(e.headers||[]);return[{type:"group",collapsable:!1,title:e.title,path:null,children:n.map(n=>({type:"auto",title:n.title,basePath:e.path,path:e.path+"#"+n.slug,children:n.children||[]}))}]}function b(e){let n;return(e=e.map(e=>Object.assign({},e))).forEach(e=>{2===e.level?n=e:n&&(n.children||(n.children=[])).push(e)}),e.filter(e=>2===e.level)}function v(e){return Object.assign(e,{type:e.items&&e.items.length?"links":"link"})}function y(e){return Object.prototype.toString.call(e).match(/\[object (.*?)\]/)[1].toLowerCase()}function g(e){let n=e.frontmatter.date||e.lastUpdated||new Date,t=new Date(n);return"Invalid Date"==t&&n&&(t=new Date(n.replace(/-/g,"/"))),t.getTime()}function k(e,n){return g(n)-g(e)}},function(e,n){e.exports=function(e){return null!=e&&"object"==typeof e}},function(e,n,t){var r=t(15),a=t(150),i=t(151),o=r?r.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":o&&o in Object(e)?a(e):i(e)}},function(e,n,t){"use strict";var r=t(5),a=t(17),i=t(34);e.exports=r?function(e,n,t){return a.f(e,n,i(1,t))}:function(e,n,t){return e[n]=t,e}},function(e,n,t){var r=t(8).Symbol;e.exports=r},function(e,n,t){"use strict";var r=t(25),a=t(31),i=t(32),o=t(144),l=t(146);r({target:"Array",proto:!0,arity:1,forced:t(3)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(e){return e instanceof TypeError}}()},{push:function(e){var n=a(this),t=i(n),r=arguments.length;l(t+r);for(var s=0;s<r;s++)n[t]=arguments[s],t++;return o(n,t),t}})},function(e,n,t){"use strict";var r=t(5),a=t(64),i=t(101),o=t(47),l=t(54),s=TypeError,c=Object.defineProperty,d=Object.getOwnPropertyDescriptor;n.f=r?i?function(e,n,t){if(o(e),n=l(n),o(t),"function"==typeof e&&"prototype"===n&&"value"in t&&"writable"in t&&!t.writable){var r=d(e,n);r&&r.writable&&(e[n]=t.value,t={configurable:"configurable"in t?t.configurable:r.configurable,enumerable:"enumerable"in t?t.enumerable:r.enumerable,writable:!1})}return c(e,n,t)}:c:function(e,n,t){if(o(e),n=l(n),o(t),a)try{return c(e,n,t)}catch(e){}if("get"in t||"set"in t)throw new s("Accessors not supported");return"value"in t&&(e[n]=t.value),e}},function(e,n,t){"use strict";var r=t(2),a=r({}.toString),i=r("".slice);e.exports=function(e){return i(a(e),8,-1)}},function(e,n,t){var r=t(155),a=t(156),i=t(157),o=t(158),l=t(159);function s(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var r=e[n];this.set(r[0],r[1])}}s.prototype.clear=r,s.prototype.delete=a,s.prototype.get=i,s.prototype.has=o,s.prototype.set=l,e.exports=s},function(e,n,t){var r=t(71);e.exports=function(e,n){for(var t=e.length;t--;)if(r(e[t][0],n))return t;return-1}},function(e,n,t){var r=t(10)(Object,"create");e.exports=r},function(e,n,t){var r=t(177);e.exports=function(e,n){var t=e.__data__;return r(n)?t["string"==typeof n?"string":"hash"]:t.map}},function(e,n,t){var r=t(45);e.exports=function(e){if("string"==typeof e||r(e))return e;var n=e+"";return"0"==n&&1/e==-1/0?"-0":n}},function(e,n,t){var r,a;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(a="function"==typeof(r=function(){var e,n,t={version:"0.2.0"},r=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function a(e,n,t){return e<n?n:e>t?t:e}function i(e){return 100*(-1+e)}t.configure=function(e){var n,t;for(n in e)void 0!==(t=e[n])&&e.hasOwnProperty(n)&&(r[n]=t);return this},t.status=null,t.set=function(e){var n=t.isStarted();e=a(e,r.minimum,1),t.status=1===e?null:e;var s=t.render(!n),c=s.querySelector(r.barSelector),d=r.speed,u=r.easing;return s.offsetWidth,o((function(n){""===r.positionUsing&&(r.positionUsing=t.getPositioningCSS()),l(c,function(e,n,t){var a;return(a="translate3d"===r.positionUsing?{transform:"translate3d("+i(e)+"%,0,0)"}:"translate"===r.positionUsing?{transform:"translate("+i(e)+"%,0)"}:{"margin-left":i(e)+"%"}).transition="all "+n+"ms "+t,a}(e,d,u)),1===e?(l(s,{transition:"none",opacity:1}),s.offsetWidth,setTimeout((function(){l(s,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),n()}),d)}),d)):setTimeout(n,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var e=function(){setTimeout((function(){t.status&&(t.trickle(),e())}),r.trickleSpeed)};return r.trickle&&e(),this},t.done=function(e){return e||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(e){var n=t.status;return n?("number"!=typeof e&&(e=(1-n)*a(Math.random()*n,.1,.95)),n=a(n+e,0,.994),t.set(n)):t.start()},t.trickle=function(){return t.inc(Math.random()*r.trickleRate)},e=0,n=0,t.promise=function(r){return r&&"resolved"!==r.state()?(0===n&&t.start(),e++,n++,r.always((function(){0==--n?(e=0,t.done()):t.set((e-n)/e)})),this):this},t.render=function(e){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var n=document.createElement("div");n.id="nprogress",n.innerHTML=r.template;var a,o=n.querySelector(r.barSelector),s=e?"-100":i(t.status||0),d=document.querySelector(r.parent);return l(o,{transition:"all 0 linear",transform:"translate3d("+s+"%,0,0)"}),r.showSpinner||(a=n.querySelector(r.spinnerSelector))&&p(a),d!=document.body&&c(d,"nprogress-custom-parent"),d.appendChild(n),n},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(r.parent),"nprogress-custom-parent");var e=document.getElementById("nprogress");e&&p(e)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var e=document.body.style,n="WebkitTransform"in e?"Webkit":"MozTransform"in e?"Moz":"msTransform"in e?"ms":"OTransform"in e?"O":"";return n+"Perspective"in e?"translate3d":n+"Transform"in e?"translate":"margin"};var o=function(){var e=[];function n(){var t=e.shift();t&&t(n)}return function(t){e.push(t),1==e.length&&n()}}(),l=function(){var e=["Webkit","O","Moz","ms"],n={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(e,n){return n.toUpperCase()})),n[t]||(n[t]=function(n){var t=document.body.style;if(n in t)return n;for(var r,a=e.length,i=n.charAt(0).toUpperCase()+n.slice(1);a--;)if((r=e[a]+i)in t)return r;return n}(t))}function r(e,n,r){n=t(n),e.style[n]=r}return function(e,n){var t,a,i=arguments;if(2==i.length)for(t in n)void 0!==(a=n[t])&&n.hasOwnProperty(t)&&r(e,t,a);else r(e,i[1],i[2])}}();function s(e,n){return("string"==typeof e?e:u(e)).indexOf(" "+n+" ")>=0}function c(e,n){var t=u(e),r=t+n;s(t,n)||(e.className=r.substring(1))}function d(e,n){var t,r=u(e);s(e,n)&&(t=r.replace(" "+n+" "," "),e.className=t.substring(1,t.length-1))}function u(e){return(" "+(e.className||"")+" ").replace(/\s+/gi," ")}function p(e){e&&e.parentNode&&e.parentNode.removeChild(e)}return t})?r.call(n,t,n,e):r)||(e.exports=a)},function(e,n,t){"use strict";var r=t(0),a=t(52).f,i=t(14),o=t(97),l=t(37),s=t(65),c=t(125);e.exports=function(e,n){var t,d,u,p,h,f=e.target,m=e.global,b=e.stat;if(t=m?r:b?r[f]||l(f,{}):r[f]&&r[f].prototype)for(d in n){if(p=n[d],u=e.dontCallGetSet?(h=a(t,d))&&h.value:t[d],!c(m?d:f+(b?".":"#")+d,e.forced)&&void 0!==u){if(typeof p==typeof u)continue;s(p,u)}(e.sham||u&&u.sham)&&i(p,"sham",!0),o(t,d,p,e)}}},function(e,n,t){"use strict";var r=t(3);e.exports=!r((function(){var e=function(){}.bind();return"function"!=typeof e||e.hasOwnProperty("prototype")}))},function(e,n,t){"use strict";var r=t(48),a=t(35);e.exports=function(e){return r(a(e))}},function(e,n,t){"use strict";var r=t(0),a=t(1),i=function(e){return a(e)?e:void 0};e.exports=function(e,n){return arguments.length<2?i(r[e]):r[e]&&r[e][n]}},function(e,n,t){"use strict";var r=t(1),a=t(112),i=TypeError;e.exports=function(e){if(r(e))return e;throw new i(a(e)+" is not a function")}},function(e,n,t){"use strict";var r=t(0),a=t(61),i=t(9),o=t(63),l=t(58),s=t(57),c=r.Symbol,d=a("wks"),u=s?c.for||c:c&&c.withoutSetter||o;e.exports=function(e){return i(d,e)||(d[e]=l&&i(c,e)?c[e]:u("Symbol."+e)),d[e]}},function(e,n,t){"use strict";var r=t(35),a=Object;e.exports=function(e){return a(r(e))}},function(e,n,t){"use strict";var r=t(123);e.exports=function(e){return r(e.length)}},function(e,n,t){"use strict";var r=t(26),a=Function.prototype.call;e.exports=r?a.bind(a):function(){return a.apply(a,arguments)}},function(e,n,t){"use strict";e.exports=function(e,n){return{enumerable:!(1&e),configurable:!(2&e),writable:!(4&e),value:n}}},function(e,n,t){"use strict";var r=t(53),a=TypeError;e.exports=function(e){if(r(e))throw new a("Can't call method on "+e);return e}},function(e,n,t){"use strict";var r=t(62),a=t(0),i=t(37),o=e.exports=a["__core-js_shared__"]||i("__core-js_shared__",{});(o.versions||(o.versions=[])).push({version:"3.38.1",mode:r?"pure":"global",copyright:"© 2014-2024 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.38.1/LICENSE",source:"https://github.com/zloirock/core-js"})},function(e,n,t){"use strict";var r=t(0),a=Object.defineProperty;e.exports=function(e,n){try{a(r,e,{value:n,configurable:!0,writable:!0})}catch(t){r[e]=n}return n}},function(e,n,t){var r=t(149),a=t(12),i=Object.prototype,o=i.hasOwnProperty,l=i.propertyIsEnumerable,s=r(function(){return arguments}())?r:function(e){return a(e)&&o.call(e,"callee")&&!l.call(e,"callee")};e.exports=s},function(e,n,t){var r=t(10)(t(8),"Map");e.exports=r},function(e,n){e.exports=function(e){var n=typeof e;return null!=e&&("object"==n||"function"==n)}},function(e,n,t){var r=t(169),a=t(176),i=t(178),o=t(179),l=t(180);function s(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var r=e[n];this.set(r[0],r[1])}}s.prototype.clear=r,s.prototype.delete=a,s.prototype.get=i,s.prototype.has=o,s.prototype.set=l,e.exports=s},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e){t[++n]=e})),t}},function(e,n){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},function(e,n,t){var r=t(6),a=t(45),i=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,o=/^\w*$/;e.exports=function(e,n){if(r(e))return!1;var t=typeof e;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=e&&!a(e))||(o.test(e)||!i.test(e)||null!=n&&e in Object(n))}},function(e,n,t){var r=t(13),a=t(12);e.exports=function(e){return"symbol"==typeof e||a(e)&&"[object Symbol]"==r(e)}},function(e,n){e.exports=function(e){return e}},function(e,n,t){"use strict";var r=t(7),a=String,i=TypeError;e.exports=function(e){if(r(e))return e;throw new i(a(e)+" is not an object")}},function(e,n,t){"use strict";var r=t(2),a=t(3),i=t(18),o=Object,l=r("".split);e.exports=a((function(){return!o("z").propertyIsEnumerable(0)}))?function(e){return"String"===i(e)?l(e,""):o(e)}:o},function(e,n,t){"use strict";e.exports={}},function(e,n){e.exports=function(e){return e.webpackPolyfill||(e.deprecate=function(){},e.paths=[],e.children||(e.children=[]),Object.defineProperty(e,"loaded",{enumerable:!0,get:function(){return e.l}}),Object.defineProperty(e,"id",{enumerable:!0,get:function(){return e.i}}),e.webpackPolyfill=1),e}},function(e,n){var t=/^\s+|\s+$/g,r=/^[-+]0x[0-9a-f]+$/i,a=/^0b[01]+$/i,i=/^0o[0-7]+$/i,o=parseInt,l="object"==typeof global&&global&&global.Object===Object&&global,s="object"==typeof self&&self&&self.Object===Object&&self,c=l||s||Function("return this")(),d=Object.prototype.toString,u=Math.max,p=Math.min,h=function(){return c.Date.now()};function f(e){var n=typeof e;return!!e&&("object"==n||"function"==n)}function m(e){if("number"==typeof e)return e;if(function(e){return"symbol"==typeof e||function(e){return!!e&&"object"==typeof e}(e)&&"[object Symbol]"==d.call(e)}(e))return NaN;if(f(e)){var n="function"==typeof e.valueOf?e.valueOf():e;e=f(n)?n+"":n}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(t,"");var l=a.test(e);return l||i.test(e)?o(e.slice(2),l?2:8):r.test(e)?NaN:+e}e.exports=function(e,n,t){var r,a,i,o,l,s,c=0,d=!1,b=!1,v=!0;if("function"!=typeof e)throw new TypeError("Expected a function");function y(n){var t=r,i=a;return r=a=void 0,c=n,o=e.apply(i,t)}function g(e){return c=e,l=setTimeout(x,n),d?y(e):o}function k(e){var t=e-s;return void 0===s||t>=n||t<0||b&&e-c>=i}function x(){var e=h();if(k(e))return C(e);l=setTimeout(x,function(e){var t=n-(e-s);return b?p(t,i-(e-c)):t}(e))}function C(e){return l=void 0,v&&r?y(e):(r=a=void 0,o)}function w(){var e=h(),t=k(e);if(r=arguments,a=this,s=e,t){if(void 0===l)return g(s);if(b)return l=setTimeout(x,n),y(s)}return void 0===l&&(l=setTimeout(x,n)),o}return n=m(n)||0,f(t)&&(d=!!t.leading,i=(b="maxWait"in t)?u(m(t.maxWait)||0,n):i,v="trailing"in t?!!t.trailing:v),w.cancel=function(){void 0!==l&&clearTimeout(l),c=0,r=s=a=l=void 0},w.flush=function(){return void 0===l?o:C(h())},w}},function(e,n,t){"use strict";var r=t(5),a=t(33),i=t(109),o=t(34),l=t(27),s=t(54),c=t(9),d=t(64),u=Object.getOwnPropertyDescriptor;n.f=r?u:function(e,n){if(e=l(e),n=s(n),d)try{return u(e,n)}catch(e){}if(c(e,n))return o(!a(i.f,e,n),e[n])}},function(e,n,t){"use strict";e.exports=function(e){return null==e}},function(e,n,t){"use strict";var r=t(110),a=t(55);e.exports=function(e){var n=r(e,"string");return a(n)?n:n+""}},function(e,n,t){"use strict";var r=t(28),a=t(1),i=t(56),o=t(57),l=Object;e.exports=o?function(e){return"symbol"==typeof e}:function(e){var n=r("Symbol");return a(n)&&i(n.prototype,l(e))}},function(e,n,t){"use strict";var r=t(2);e.exports=r({}.isPrototypeOf)},function(e,n,t){"use strict";var r=t(58);e.exports=r&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(e,n,t){"use strict";var r=t(59),a=t(3),i=t(0).String;e.exports=!!Object.getOwnPropertySymbols&&!a((function(){var e=Symbol("symbol detection");return!i(e)||!(Object(e)instanceof Symbol)||!Symbol.sham&&r&&r<41}))},function(e,n,t){"use strict";var r,a,i=t(0),o=t(60),l=i.process,s=i.Deno,c=l&&l.versions||s&&s.version,d=c&&c.v8;d&&(a=(r=d.split("."))[0]>0&&r[0]<4?1:+(r[0]+r[1])),!a&&o&&(!(r=o.match(/Edge\/(\d+)/))||r[1]>=74)&&(r=o.match(/Chrome\/(\d+)/))&&(a=+r[1]),e.exports=a},function(e,n,t){"use strict";var r=t(0).navigator,a=r&&r.userAgent;e.exports=a?String(a):""},function(e,n,t){"use strict";var r=t(36);e.exports=function(e,n){return r[e]||(r[e]=n||{})}},function(e,n,t){"use strict";e.exports=!1},function(e,n,t){"use strict";var r=t(2),a=0,i=Math.random(),o=r(1..toString);e.exports=function(e){return"Symbol("+(void 0===e?"":e)+")_"+o(++a+i,36)}},function(e,n,t){"use strict";var r=t(5),a=t(3),i=t(100);e.exports=!r&&!a((function(){return 7!==Object.defineProperty(i("div"),"a",{get:function(){return 7}}).a}))},function(e,n,t){"use strict";var r=t(9),a=t(118),i=t(52),o=t(17);e.exports=function(e,n,t){for(var l=a(n),s=o.f,c=i.f,d=0;d<l.length;d++){var u=l[d];r(e,u)||t&&r(t,u)||s(e,u,c(n,u))}}},function(e,n,t){"use strict";var r=t(122);e.exports=function(e){var n=+e;return n!=n||0===n?0:r(n)}},function(e,n,t){"use strict";var r=t(132),a=t(7),i=t(35),o=t(133);e.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var e,n=!1,t={};try{(e=r(Object.prototype,"__proto__","set"))(t,[]),n=t instanceof Array}catch(e){}return function(t,r){return i(t),o(r),a(t)?(n?e(t,r):t.__proto__=r,t):t}}():void 0)},function(e,n){e.exports=function(e,n){for(var t=-1,r=n.length,a=e.length;++t<r;)e[a+t]=n[t];return e}},function(e,n){var t="object"==typeof global&&global&&global.Object===Object&&global;e.exports=t},function(e,n,t){var r=t(19),a=t(160),i=t(161),o=t(162),l=t(163),s=t(164);function c(e){var n=this.__data__=new r(e);this.size=n.size}c.prototype.clear=a,c.prototype.delete=i,c.prototype.get=o,c.prototype.has=l,c.prototype.set=s,e.exports=c},function(e,n){e.exports=function(e,n){return e===n||e!=e&&n!=n}},function(e,n,t){var r=t(13),a=t(40);e.exports=function(e){if(!a(e))return!1;var n=r(e);return"[object Function]"==n||"[object GeneratorFunction]"==n||"[object AsyncFunction]"==n||"[object Proxy]"==n}},function(e,n){var t=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return t.call(e)}catch(e){}try{return e+""}catch(e){}}return""}},function(e,n,t){var r=t(181),a=t(12);e.exports=function e(n,t,i,o,l){return n===t||(null==n||null==t||!a(n)&&!a(t)?n!=n&&t!=t:r(n,t,i,o,e,l))}},function(e,n,t){var r=t(76),a=t(184),i=t(77);e.exports=function(e,n,t,o,l,s){var c=1&t,d=e.length,u=n.length;if(d!=u&&!(c&&u>d))return!1;var p=s.get(e),h=s.get(n);if(p&&h)return p==n&&h==e;var f=-1,m=!0,b=2&t?new r:void 0;for(s.set(e,n),s.set(n,e);++f<d;){var v=e[f],y=n[f];if(o)var g=c?o(y,v,f,n,e,s):o(v,y,f,e,n,s);if(void 0!==g){if(g)continue;m=!1;break}if(b){if(!a(n,(function(e,n){if(!i(b,n)&&(v===e||l(v,e,t,o,s)))return b.push(n)}))){m=!1;break}}else if(v!==y&&!l(v,y,t,o,s)){m=!1;break}}return s.delete(e),s.delete(n),m}},function(e,n,t){var r=t(41),a=t(182),i=t(183);function o(e){var n=-1,t=null==e?0:e.length;for(this.__data__=new r;++n<t;)this.add(e[n])}o.prototype.add=o.prototype.push=a,o.prototype.has=i,e.exports=o},function(e,n){e.exports=function(e,n){return e.has(n)}},function(e,n,t){var r=t(194),a=t(200),i=t(82);e.exports=function(e){return i(e)?r(e):a(e)}},function(e,n,t){(function(e){var r=t(8),a=t(196),i=n&&!n.nodeType&&n,o=i&&"object"==typeof e&&e&&!e.nodeType&&e,l=o&&o.exports===i?r.Buffer:void 0,s=(l?l.isBuffer:void 0)||a;e.exports=s}).call(this,t(50)(e))},function(e,n){var t=/^(?:0|[1-9]\d*)$/;e.exports=function(e,n){var r=typeof e;return!!(n=null==n?9007199254740991:n)&&("number"==r||"symbol"!=r&&t.test(e))&&e>-1&&e%1==0&&e<n}},function(e,n,t){var r=t(197),a=t(198),i=t(199),o=i&&i.isTypedArray,l=o?a(o):r;e.exports=l},function(e,n,t){var r=t(72),a=t(43);e.exports=function(e){return null!=e&&a(e.length)&&!r(e)}},function(e,n,t){var r=t(10)(t(8),"Set");e.exports=r},function(e,n,t){var r=t(40);e.exports=function(e){return e==e&&!r(e)}},function(e,n){e.exports=function(e,n){return function(t){return null!=t&&(t[e]===n&&(void 0!==n||e in Object(t)))}}},function(e,n,t){var r=t(87),a=t(23);e.exports=function(e,n){for(var t=0,i=(n=r(n,e)).length;null!=e&&t<i;)e=e[a(n[t++])];return t&&t==i?e:void 0}},function(e,n,t){var r=t(6),a=t(44),i=t(211),o=t(214);e.exports=function(e,n){return r(e)?e:a(e,n)?[e]:i(o(e))}},function(e,n,t){},function(e,n,t){},function(e,n,t){},function(e,n,t){},function(e,n,t){var r=t(147),a=t(152),i=t(223),o=t(231),l=t(240),s=t(106),c=i((function(e){var n=s(e);return l(n)&&(n=void 0),o(r(e,1,l,!0),a(n,2))}));e.exports=c},function(e,n,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var r=/["'&<>]/;e.exports=function(e){var n,t=""+e,a=r.exec(t);if(!a)return t;var i="",o=0,l=0;for(o=a.index;o<t.length;o++){switch(t.charCodeAt(o)){case 34:n="&quot;";break;case 38:n="&amp;";break;case 39:n="&#39;";break;case 60:n="&lt;";break;case 62:n="&gt;";break;default:continue}l!==o&&(i+=t.substring(l,o)),l=o+1,i+=n}return l!==o?i+t.substring(l,o):i}},function(e){e.exports=JSON.parse('{"en-US":{"author":"author","beforeAuthor":"Copyright © ","afterAuthor":"\\nLink: "},"zh-CN":{"author":"作者","beforeAuthor":"著作权归","afterAuthor":"所有。\\n链接："}}')},function(e,n,t){"use strict";t.r(n);var r={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},a=(t(243),t(4)),i=Object(a.a)(r,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);n.default=i.exports},function(e,n,t){"use strict";t.r(n);var r={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(e){this.codeTabs.forEach(e=>{e.elm.classList.remove("theme-code-block__active")}),this.codeTabs[e].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(e=>Boolean(e.componentOptions)).map((e,n)=>(""===e.componentOptions.propsData.active&&(this.activeCodeTabIndex=n),{title:e.componentOptions.propsData.title,elm:e.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(e){this.activeCodeTabIndex=e}}},a=(t(244),t(4)),i=Object(a.a)(r,(function(){var e=this,n=e._self._c;return n("div",{staticClass:"theme-code-group"},[n("div",{staticClass:"theme-code-group__nav"},[n("ul",{staticClass:"theme-code-group__ul"},e._l(e.codeTabs,(function(t,r){return n("li",{key:t.title,staticClass:"theme-code-group__li"},[n("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":r===e.activeCodeTabIndex},on:{click:function(n){return e.changeCodeTab(r)}}},[e._v("\n            "+e._s(t.title)+"\n          ")])])})),0)]),e._v(" "),e._t("default"),e._v(" "),e.codeTabs.length<1?n("pre",{staticClass:"pre-blank"},[e._v("// Make sure to add code blocks to your code group")]):e._e()],2)}),[],!1,null,"2f5f1757",null);n.default=i.exports},function(e,n,t){"use strict";var r=t(1),a=t(17),i=t(102),o=t(37);e.exports=function(e,n,t,l){l||(l={});var s=l.enumerable,c=void 0!==l.name?l.name:n;if(r(t)&&i(t,c,l),l.global)s?e[n]=t:o(n,t);else{try{l.unsafe?e[n]&&(s=!0):delete e[n]}catch(e){}s?e[n]=t:a.f(e,n,{value:t,enumerable:!1,configurable:!l.nonConfigurable,writable:!l.nonWritable})}return e}},function(e,n,t){"use strict";e.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(e,n,t){"use strict";var r=t(138),a=String;e.exports=function(e){if("Symbol"===r(e))throw new TypeError("Cannot convert a Symbol value to a string");return a(e)}},function(e,n,t){"use strict";var r=t(0),a=t(7),i=r.document,o=a(i)&&a(i.createElement);e.exports=function(e){return o?i.createElement(e):{}}},function(e,n,t){"use strict";var r=t(5),a=t(3);e.exports=r&&a((function(){return 42!==Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(e,n,t){"use strict";var r=t(2),a=t(3),i=t(1),o=t(9),l=t(5),s=t(114).CONFIGURABLE,c=t(115),d=t(116),u=d.enforce,p=d.get,h=String,f=Object.defineProperty,m=r("".slice),b=r("".replace),v=r([].join),y=l&&!a((function(){return 8!==f((function(){}),"length",{value:8}).length})),g=String(String).split("String"),k=e.exports=function(e,n,t){"Symbol("===m(h(n),0,7)&&(n="["+b(h(n),/^Symbol\(([^)]*)\).*$/,"$1")+"]"),t&&t.getter&&(n="get "+n),t&&t.setter&&(n="set "+n),(!o(e,"name")||s&&e.name!==n)&&(l?f(e,"name",{value:n,configurable:!0}):e.name=n),y&&t&&o(t,"arity")&&e.length!==t.arity&&f(e,"length",{value:t.arity});try{t&&o(t,"constructor")&&t.constructor?l&&f(e,"prototype",{writable:!1}):e.prototype&&(e.prototype=void 0)}catch(e){}var r=u(e);return o(r,"source")||(r.source=v(g,"string"==typeof n?n:"")),e};Function.prototype.toString=k((function(){return i(this)&&p(this).source||c(this)}),"toString")},function(e,n,t){"use strict";var r=t(61),a=t(63),i=r("keys");e.exports=function(e){return i[e]||(i[e]=a(e))}},function(e,n,t){"use strict";var r=t(2),a=t(9),i=t(27),o=t(120).indexOf,l=t(49),s=r([].push);e.exports=function(e,n){var t,r=i(e),c=0,d=[];for(t in r)!a(l,t)&&a(r,t)&&s(d,t);for(;n.length>c;)a(r,t=n[c++])&&(~o(d,t)||s(d,t));return d}},function(e,n,t){"use strict";var r=t(25),a=t(0),i=t(130),o=t(131),l=a.WebAssembly,s=7!==new Error("e",{cause:7}).cause,c=function(e,n){var t={};t[e]=o(e,n,s),r({global:!0,constructor:!0,arity:1,forced:s},t)},d=function(e,n){if(l&&l[e]){var t={};t[e]=o("WebAssembly."+e,n,s),r({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:s},t)}};c("Error",(function(e){return function(n){return i(e,this,arguments)}})),c("EvalError",(function(e){return function(n){return i(e,this,arguments)}})),c("RangeError",(function(e){return function(n){return i(e,this,arguments)}})),c("ReferenceError",(function(e){return function(n){return i(e,this,arguments)}})),c("SyntaxError",(function(e){return function(n){return i(e,this,arguments)}})),c("TypeError",(function(e){return function(n){return i(e,this,arguments)}})),c("URIError",(function(e){return function(n){return i(e,this,arguments)}})),d("CompileError",(function(e){return function(n){return i(e,this,arguments)}})),d("LinkError",(function(e){return function(n){return i(e,this,arguments)}})),d("RuntimeError",(function(e){return function(n){return i(e,this,arguments)}}))},function(e,n){e.exports=function(e){var n=null==e?0:e.length;return n?e[n-1]:void 0}},function(e,n,t){e.exports=t(249)},function(e,n,t){"use strict";var r=t(25),a=t(126).left,i=t(127),o=t(59);r({target:"Array",proto:!0,forced:!t(128)&&o>79&&o<83||!i("reduce")},{reduce:function(e){var n=arguments.length;return a(this,e,n,n>1?arguments[1]:void 0)}})},function(e,n,t){"use strict";var r={}.propertyIsEnumerable,a=Object.getOwnPropertyDescriptor,i=a&&!r.call({1:2},1);n.f=i?function(e){var n=a(this,e);return!!n&&n.enumerable}:r},function(e,n,t){"use strict";var r=t(33),a=t(7),i=t(55),o=t(111),l=t(113),s=t(30),c=TypeError,d=s("toPrimitive");e.exports=function(e,n){if(!a(e)||i(e))return e;var t,s=o(e,d);if(s){if(void 0===n&&(n="default"),t=r(s,e,n),!a(t)||i(t))return t;throw new c("Can't convert object to primitive value")}return void 0===n&&(n="number"),l(e,n)}},function(e,n,t){"use strict";var r=t(29),a=t(53);e.exports=function(e,n){var t=e[n];return a(t)?void 0:r(t)}},function(e,n,t){"use strict";var r=String;e.exports=function(e){try{return r(e)}catch(e){return"Object"}}},function(e,n,t){"use strict";var r=t(33),a=t(1),i=t(7),o=TypeError;e.exports=function(e,n){var t,l;if("string"===n&&a(t=e.toString)&&!i(l=r(t,e)))return l;if(a(t=e.valueOf)&&!i(l=r(t,e)))return l;if("string"!==n&&a(t=e.toString)&&!i(l=r(t,e)))return l;throw new o("Can't convert object to primitive value")}},function(e,n,t){"use strict";var r=t(5),a=t(9),i=Function.prototype,o=r&&Object.getOwnPropertyDescriptor,l=a(i,"name"),s=l&&"something"===function(){}.name,c=l&&(!r||r&&o(i,"name").configurable);e.exports={EXISTS:l,PROPER:s,CONFIGURABLE:c}},function(e,n,t){"use strict";var r=t(2),a=t(1),i=t(36),o=r(Function.toString);a(i.inspectSource)||(i.inspectSource=function(e){return o(e)}),e.exports=i.inspectSource},function(e,n,t){"use strict";var r,a,i,o=t(117),l=t(0),s=t(7),c=t(14),d=t(9),u=t(36),p=t(103),h=t(49),f=l.TypeError,m=l.WeakMap;if(o||u.state){var b=u.state||(u.state=new m);b.get=b.get,b.has=b.has,b.set=b.set,r=function(e,n){if(b.has(e))throw new f("Object already initialized");return n.facade=e,b.set(e,n),n},a=function(e){return b.get(e)||{}},i=function(e){return b.has(e)}}else{var v=p("state");h[v]=!0,r=function(e,n){if(d(e,v))throw new f("Object already initialized");return n.facade=e,c(e,v,n),n},a=function(e){return d(e,v)?e[v]:{}},i=function(e){return d(e,v)}}e.exports={set:r,get:a,has:i,enforce:function(e){return i(e)?a(e):r(e,{})},getterFor:function(e){return function(n){var t;if(!s(n)||(t=a(n)).type!==e)throw new f("Incompatible receiver, "+e+" required");return t}}}},function(e,n,t){"use strict";var r=t(0),a=t(1),i=r.WeakMap;e.exports=a(i)&&/native code/.test(String(i))},function(e,n,t){"use strict";var r=t(28),a=t(2),i=t(119),o=t(124),l=t(47),s=a([].concat);e.exports=r("Reflect","ownKeys")||function(e){var n=i.f(l(e)),t=o.f;return t?s(n,t(e)):n}},function(e,n,t){"use strict";var r=t(104),a=t(98).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(e){return r(e,a)}},function(e,n,t){"use strict";var r=t(27),a=t(121),i=t(32),o=function(e){return function(n,t,o){var l=r(n),s=i(l);if(0===s)return!e&&-1;var c,d=a(o,s);if(e&&t!=t){for(;s>d;)if((c=l[d++])!=c)return!0}else for(;s>d;d++)if((e||d in l)&&l[d]===t)return e||d||0;return!e&&-1}};e.exports={includes:o(!0),indexOf:o(!1)}},function(e,n,t){"use strict";var r=t(66),a=Math.max,i=Math.min;e.exports=function(e,n){var t=r(e);return t<0?a(t+n,0):i(t,n)}},function(e,n,t){"use strict";var r=Math.ceil,a=Math.floor;e.exports=Math.trunc||function(e){var n=+e;return(n>0?a:r)(n)}},function(e,n,t){"use strict";var r=t(66),a=Math.min;e.exports=function(e){var n=r(e);return n>0?a(n,9007199254740991):0}},function(e,n,t){"use strict";n.f=Object.getOwnPropertySymbols},function(e,n,t){"use strict";var r=t(3),a=t(1),i=/#|\.prototype\./,o=function(e,n){var t=s[l(e)];return t===d||t!==c&&(a(n)?r(n):!!n)},l=o.normalize=function(e){return String(e).replace(i,".").toLowerCase()},s=o.data={},c=o.NATIVE="N",d=o.POLYFILL="P";e.exports=o},function(e,n,t){"use strict";var r=t(29),a=t(31),i=t(48),o=t(32),l=TypeError,s="Reduce of empty array with no initial value",c=function(e){return function(n,t,c,d){var u=a(n),p=i(u),h=o(u);if(r(t),0===h&&c<2)throw new l(s);var f=e?h-1:0,m=e?-1:1;if(c<2)for(;;){if(f in p){d=p[f],f+=m;break}if(f+=m,e?f<0:h<=f)throw new l(s)}for(;e?f>=0:h>f;f+=m)f in p&&(d=t(d,p[f],f,u));return d}};e.exports={left:c(!1),right:c(!0)}},function(e,n,t){"use strict";var r=t(3);e.exports=function(e,n){var t=[][e];return!!t&&r((function(){t.call(null,n||function(){return 1},1)}))}},function(e,n,t){"use strict";var r=t(129);e.exports="NODE"===r},function(e,n,t){"use strict";var r=t(0),a=t(60),i=t(18),o=function(e){return a.slice(0,e.length)===e};e.exports=o("Bun/")?"BUN":o("Cloudflare-Workers")?"CLOUDFLARE":o("Deno/")?"DENO":o("Node.js/")?"NODE":r.Bun&&"string"==typeof Bun.version?"BUN":r.Deno&&"object"==typeof Deno.version?"DENO":"process"===i(r.process)?"NODE":r.window&&r.document?"BROWSER":"REST"},function(e,n,t){"use strict";var r=t(26),a=Function.prototype,i=a.apply,o=a.call;e.exports="object"==typeof Reflect&&Reflect.apply||(r?o.bind(i):function(){return o.apply(i,arguments)})},function(e,n,t){"use strict";var r=t(28),a=t(9),i=t(14),o=t(56),l=t(67),s=t(65),c=t(135),d=t(136),u=t(137),p=t(140),h=t(141),f=t(5),m=t(62);e.exports=function(e,n,t,b){var v=b?2:1,y=e.split("."),g=y[y.length-1],k=r.apply(null,y);if(k){var x=k.prototype;if(!m&&a(x,"cause")&&delete x.cause,!t)return k;var C=r("Error"),w=n((function(e,n){var t=u(b?n:e,void 0),r=b?new k(e):new k;return void 0!==t&&i(r,"message",t),h(r,w,r.stack,2),this&&o(x,this)&&d(r,this,w),arguments.length>v&&p(r,arguments[v]),r}));if(w.prototype=x,"Error"!==g?l?l(w,C):s(w,C,{name:!0}):f&&"stackTraceLimit"in k&&(c(w,k,"stackTraceLimit"),c(w,k,"prepareStackTrace")),s(w,k),!m)try{x.name!==g&&i(x,"name",g),x.constructor=w}catch(e){}return w}}},function(e,n,t){"use strict";var r=t(2),a=t(29);e.exports=function(e,n,t){try{return r(a(Object.getOwnPropertyDescriptor(e,n)[t]))}catch(e){}}},function(e,n,t){"use strict";var r=t(134),a=String,i=TypeError;e.exports=function(e){if(r(e))return e;throw new i("Can't set "+a(e)+" as a prototype")}},function(e,n,t){"use strict";var r=t(7);e.exports=function(e){return r(e)||null===e}},function(e,n,t){"use strict";var r=t(17).f;e.exports=function(e,n,t){t in e||r(e,t,{configurable:!0,get:function(){return n[t]},set:function(e){n[t]=e}})}},function(e,n,t){"use strict";var r=t(1),a=t(7),i=t(67);e.exports=function(e,n,t){var o,l;return i&&r(o=n.constructor)&&o!==t&&a(l=o.prototype)&&l!==t.prototype&&i(e,l),e}},function(e,n,t){"use strict";var r=t(99);e.exports=function(e,n){return void 0===e?arguments.length<2?"":n:r(e)}},function(e,n,t){"use strict";var r=t(139),a=t(1),i=t(18),o=t(30)("toStringTag"),l=Object,s="Arguments"===i(function(){return arguments}());e.exports=r?i:function(e){var n,t,r;return void 0===e?"Undefined":null===e?"Null":"string"==typeof(t=function(e,n){try{return e[n]}catch(e){}}(n=l(e),o))?t:s?i(n):"Object"===(r=i(n))&&a(n.callee)?"Arguments":r}},function(e,n,t){"use strict";var r={};r[t(30)("toStringTag")]="z",e.exports="[object z]"===String(r)},function(e,n,t){"use strict";var r=t(7),a=t(14);e.exports=function(e,n){r(n)&&"cause"in n&&a(e,"cause",n.cause)}},function(e,n,t){"use strict";var r=t(14),a=t(142),i=t(143),o=Error.captureStackTrace;e.exports=function(e,n,t,l){i&&(o?o(e,n):r(e,"stack",a(t,l)))}},function(e,n,t){"use strict";var r=t(2),a=Error,i=r("".replace),o=String(new a("zxcasd").stack),l=/\n\s*at [^:]*:[^\n]*/,s=l.test(o);e.exports=function(e,n){if(s&&"string"==typeof e&&!a.prepareStackTrace)for(;n--;)e=i(e,l,"");return e}},function(e,n,t){"use strict";var r=t(3),a=t(34);e.exports=!r((function(){var e=new Error("a");return!("stack"in e)||(Object.defineProperty(e,"stack",a(1,7)),7!==e.stack)}))},function(e,n,t){"use strict";var r=t(5),a=t(145),i=TypeError,o=Object.getOwnPropertyDescriptor,l=r&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(e){return e instanceof TypeError}}();e.exports=l?function(e,n){if(a(e)&&!o(e,"length").writable)throw new i("Cannot set read only .length");return e.length=n}:function(e,n){return e.length=n}},function(e,n,t){"use strict";var r=t(18);e.exports=Array.isArray||function(e){return"Array"===r(e)}},function(e,n,t){"use strict";var r=TypeError;e.exports=function(e){if(e>9007199254740991)throw r("Maximum allowed index exceeded");return e}},function(e,n,t){var r=t(68),a=t(148);e.exports=function e(n,t,i,o,l){var s=-1,c=n.length;for(i||(i=a),l||(l=[]);++s<c;){var d=n[s];t>0&&i(d)?t>1?e(d,t-1,i,o,l):r(l,d):o||(l[l.length]=d)}return l}},function(e,n,t){var r=t(15),a=t(38),i=t(6),o=r?r.isConcatSpreadable:void 0;e.exports=function(e){return i(e)||a(e)||!!(o&&e&&e[o])}},function(e,n,t){var r=t(13),a=t(12);e.exports=function(e){return a(e)&&"[object Arguments]"==r(e)}},function(e,n,t){var r=t(15),a=Object.prototype,i=a.hasOwnProperty,o=a.toString,l=r?r.toStringTag:void 0;e.exports=function(e){var n=i.call(e,l),t=e[l];try{e[l]=void 0;var r=!0}catch(e){}var a=o.call(e);return r&&(n?e[l]=t:delete e[l]),a}},function(e,n){var t=Object.prototype.toString;e.exports=function(e){return t.call(e)}},function(e,n,t){var r=t(153),a=t(209),i=t(46),o=t(6),l=t(220);e.exports=function(e){return"function"==typeof e?e:null==e?i:"object"==typeof e?o(e)?a(e[0],e[1]):r(e):l(e)}},function(e,n,t){var r=t(154),a=t(208),i=t(85);e.exports=function(e){var n=a(e);return 1==n.length&&n[0][2]?i(n[0][0],n[0][1]):function(t){return t===e||r(t,e,n)}}},function(e,n,t){var r=t(70),a=t(74);e.exports=function(e,n,t,i){var o=t.length,l=o,s=!i;if(null==e)return!l;for(e=Object(e);o--;){var c=t[o];if(s&&c[2]?c[1]!==e[c[0]]:!(c[0]in e))return!1}for(;++o<l;){var d=(c=t[o])[0],u=e[d],p=c[1];if(s&&c[2]){if(void 0===u&&!(d in e))return!1}else{var h=new r;if(i)var f=i(u,p,d,e,n,h);if(!(void 0===f?a(p,u,3,i,h):f))return!1}}return!0}},function(e,n){e.exports=function(){this.__data__=[],this.size=0}},function(e,n,t){var r=t(20),a=Array.prototype.splice;e.exports=function(e){var n=this.__data__,t=r(n,e);return!(t<0)&&(t==n.length-1?n.pop():a.call(n,t,1),--this.size,!0)}},function(e,n,t){var r=t(20);e.exports=function(e){var n=this.__data__,t=r(n,e);return t<0?void 0:n[t][1]}},function(e,n,t){var r=t(20);e.exports=function(e){return r(this.__data__,e)>-1}},function(e,n,t){var r=t(20);e.exports=function(e,n){var t=this.__data__,a=r(t,e);return a<0?(++this.size,t.push([e,n])):t[a][1]=n,this}},function(e,n,t){var r=t(19);e.exports=function(){this.__data__=new r,this.size=0}},function(e,n){e.exports=function(e){var n=this.__data__,t=n.delete(e);return this.size=n.size,t}},function(e,n){e.exports=function(e){return this.__data__.get(e)}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n,t){var r=t(19),a=t(39),i=t(41);e.exports=function(e,n){var t=this.__data__;if(t instanceof r){var o=t.__data__;if(!a||o.length<199)return o.push([e,n]),this.size=++t.size,this;t=this.__data__=new i(o)}return t.set(e,n),this.size=t.size,this}},function(e,n,t){var r=t(72),a=t(166),i=t(40),o=t(73),l=/^\[object .+?Constructor\]$/,s=Function.prototype,c=Object.prototype,d=s.toString,u=c.hasOwnProperty,p=RegExp("^"+d.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!i(e)||a(e))&&(r(e)?p:l).test(o(e))}},function(e,n,t){var r,a=t(167),i=(r=/[^.]+$/.exec(a&&a.keys&&a.keys.IE_PROTO||""))?"Symbol(src)_1."+r:"";e.exports=function(e){return!!i&&i in e}},function(e,n,t){var r=t(8)["__core-js_shared__"];e.exports=r},function(e,n){e.exports=function(e,n){return null==e?void 0:e[n]}},function(e,n,t){var r=t(170),a=t(19),i=t(39);e.exports=function(){this.size=0,this.__data__={hash:new r,map:new(i||a),string:new r}}},function(e,n,t){var r=t(171),a=t(172),i=t(173),o=t(174),l=t(175);function s(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var r=e[n];this.set(r[0],r[1])}}s.prototype.clear=r,s.prototype.delete=a,s.prototype.get=i,s.prototype.has=o,s.prototype.set=l,e.exports=s},function(e,n,t){var r=t(21);e.exports=function(){this.__data__=r?r(null):{},this.size=0}},function(e,n){e.exports=function(e){var n=this.has(e)&&delete this.__data__[e];return this.size-=n?1:0,n}},function(e,n,t){var r=t(21),a=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;if(r){var t=n[e];return"__lodash_hash_undefined__"===t?void 0:t}return a.call(n,e)?n[e]:void 0}},function(e,n,t){var r=t(21),a=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;return r?void 0!==n[e]:a.call(n,e)}},function(e,n,t){var r=t(21);e.exports=function(e,n){var t=this.__data__;return this.size+=this.has(e)?0:1,t[e]=r&&void 0===n?"__lodash_hash_undefined__":n,this}},function(e,n,t){var r=t(22);e.exports=function(e){var n=r(this,e).delete(e);return this.size-=n?1:0,n}},function(e,n){e.exports=function(e){var n=typeof e;return"string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==e:null===e}},function(e,n,t){var r=t(22);e.exports=function(e){return r(this,e).get(e)}},function(e,n,t){var r=t(22);e.exports=function(e){return r(this,e).has(e)}},function(e,n,t){var r=t(22);e.exports=function(e,n){var t=r(this,e),a=t.size;return t.set(e,n),this.size+=t.size==a?0:1,this}},function(e,n,t){var r=t(70),a=t(75),i=t(185),o=t(188),l=t(204),s=t(6),c=t(79),d=t(81),u="[object Object]",p=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,h,f,m){var b=s(e),v=s(n),y=b?"[object Array]":l(e),g=v?"[object Array]":l(n),k=(y="[object Arguments]"==y?u:y)==u,x=(g="[object Arguments]"==g?u:g)==u,C=y==g;if(C&&c(e)){if(!c(n))return!1;b=!0,k=!1}if(C&&!k)return m||(m=new r),b||d(e)?a(e,n,t,h,f,m):i(e,n,y,t,h,f,m);if(!(1&t)){var w=k&&p.call(e,"__wrapped__"),_=x&&p.call(n,"__wrapped__");if(w||_){var E=w?e.value():e,B=_?n.value():n;return m||(m=new r),f(E,B,t,h,m)}}return!!C&&(m||(m=new r),o(e,n,t,h,f,m))}},function(e,n){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n){e.exports=function(e,n){for(var t=-1,r=null==e?0:e.length;++t<r;)if(n(e[t],t,e))return!0;return!1}},function(e,n,t){var r=t(15),a=t(186),i=t(71),o=t(75),l=t(187),s=t(42),c=r?r.prototype:void 0,d=c?c.valueOf:void 0;e.exports=function(e,n,t,r,c,u,p){switch(t){case"[object DataView]":if(e.byteLength!=n.byteLength||e.byteOffset!=n.byteOffset)return!1;e=e.buffer,n=n.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=n.byteLength||!u(new a(e),new a(n)));case"[object Boolean]":case"[object Date]":case"[object Number]":return i(+e,+n);case"[object Error]":return e.name==n.name&&e.message==n.message;case"[object RegExp]":case"[object String]":return e==n+"";case"[object Map]":var h=l;case"[object Set]":var f=1&r;if(h||(h=s),e.size!=n.size&&!f)return!1;var m=p.get(e);if(m)return m==n;r|=2,p.set(e,n);var b=o(h(e),h(n),r,c,u,p);return p.delete(e),b;case"[object Symbol]":if(d)return d.call(e)==d.call(n)}return!1}},function(e,n,t){var r=t(8).Uint8Array;e.exports=r},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e,r){t[++n]=[r,e]})),t}},function(e,n,t){var r=t(189),a=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,i,o,l){var s=1&t,c=r(e),d=c.length;if(d!=r(n).length&&!s)return!1;for(var u=d;u--;){var p=c[u];if(!(s?p in n:a.call(n,p)))return!1}var h=l.get(e),f=l.get(n);if(h&&f)return h==n&&f==e;var m=!0;l.set(e,n),l.set(n,e);for(var b=s;++u<d;){var v=e[p=c[u]],y=n[p];if(i)var g=s?i(y,v,p,n,e,l):i(v,y,p,e,n,l);if(!(void 0===g?v===y||o(v,y,t,i,l):g)){m=!1;break}b||(b="constructor"==p)}if(m&&!b){var k=e.constructor,x=n.constructor;k==x||!("constructor"in e)||!("constructor"in n)||"function"==typeof k&&k instanceof k&&"function"==typeof x&&x instanceof x||(m=!1)}return l.delete(e),l.delete(n),m}},function(e,n,t){var r=t(190),a=t(191),i=t(78);e.exports=function(e){return r(e,i,a)}},function(e,n,t){var r=t(68),a=t(6);e.exports=function(e,n,t){var i=n(e);return a(e)?i:r(i,t(e))}},function(e,n,t){var r=t(192),a=t(193),i=Object.prototype.propertyIsEnumerable,o=Object.getOwnPropertySymbols,l=o?function(e){return null==e?[]:(e=Object(e),r(o(e),(function(n){return i.call(e,n)})))}:a;e.exports=l},function(e,n){e.exports=function(e,n){for(var t=-1,r=null==e?0:e.length,a=0,i=[];++t<r;){var o=e[t];n(o,t,e)&&(i[a++]=o)}return i}},function(e,n){e.exports=function(){return[]}},function(e,n,t){var r=t(195),a=t(38),i=t(6),o=t(79),l=t(80),s=t(81),c=Object.prototype.hasOwnProperty;e.exports=function(e,n){var t=i(e),d=!t&&a(e),u=!t&&!d&&o(e),p=!t&&!d&&!u&&s(e),h=t||d||u||p,f=h?r(e.length,String):[],m=f.length;for(var b in e)!n&&!c.call(e,b)||h&&("length"==b||u&&("offset"==b||"parent"==b)||p&&("buffer"==b||"byteLength"==b||"byteOffset"==b)||l(b,m))||f.push(b);return f}},function(e,n){e.exports=function(e,n){for(var t=-1,r=Array(e);++t<e;)r[t]=n(t);return r}},function(e,n){e.exports=function(){return!1}},function(e,n,t){var r=t(13),a=t(43),i=t(12),o={};o["[object Float32Array]"]=o["[object Float64Array]"]=o["[object Int8Array]"]=o["[object Int16Array]"]=o["[object Int32Array]"]=o["[object Uint8Array]"]=o["[object Uint8ClampedArray]"]=o["[object Uint16Array]"]=o["[object Uint32Array]"]=!0,o["[object Arguments]"]=o["[object Array]"]=o["[object ArrayBuffer]"]=o["[object Boolean]"]=o["[object DataView]"]=o["[object Date]"]=o["[object Error]"]=o["[object Function]"]=o["[object Map]"]=o["[object Number]"]=o["[object Object]"]=o["[object RegExp]"]=o["[object Set]"]=o["[object String]"]=o["[object WeakMap]"]=!1,e.exports=function(e){return i(e)&&a(e.length)&&!!o[r(e)]}},function(e,n){e.exports=function(e){return function(n){return e(n)}}},function(e,n,t){(function(e){var r=t(69),a=n&&!n.nodeType&&n,i=a&&"object"==typeof e&&e&&!e.nodeType&&e,o=i&&i.exports===a&&r.process,l=function(){try{var e=i&&i.require&&i.require("util").types;return e||o&&o.binding&&o.binding("util")}catch(e){}}();e.exports=l}).call(this,t(50)(e))},function(e,n,t){var r=t(201),a=t(202),i=Object.prototype.hasOwnProperty;e.exports=function(e){if(!r(e))return a(e);var n=[];for(var t in Object(e))i.call(e,t)&&"constructor"!=t&&n.push(t);return n}},function(e,n){var t=Object.prototype;e.exports=function(e){var n=e&&e.constructor;return e===("function"==typeof n&&n.prototype||t)}},function(e,n,t){var r=t(203)(Object.keys,Object);e.exports=r},function(e,n){e.exports=function(e,n){return function(t){return e(n(t))}}},function(e,n,t){var r=t(205),a=t(39),i=t(206),o=t(83),l=t(207),s=t(13),c=t(73),d=c(r),u=c(a),p=c(i),h=c(o),f=c(l),m=s;(r&&"[object DataView]"!=m(new r(new ArrayBuffer(1)))||a&&"[object Map]"!=m(new a)||i&&"[object Promise]"!=m(i.resolve())||o&&"[object Set]"!=m(new o)||l&&"[object WeakMap]"!=m(new l))&&(m=function(e){var n=s(e),t="[object Object]"==n?e.constructor:void 0,r=t?c(t):"";if(r)switch(r){case d:return"[object DataView]";case u:return"[object Map]";case p:return"[object Promise]";case h:return"[object Set]";case f:return"[object WeakMap]"}return n}),e.exports=m},function(e,n,t){var r=t(10)(t(8),"DataView");e.exports=r},function(e,n,t){var r=t(10)(t(8),"Promise");e.exports=r},function(e,n,t){var r=t(10)(t(8),"WeakMap");e.exports=r},function(e,n,t){var r=t(84),a=t(78);e.exports=function(e){for(var n=a(e),t=n.length;t--;){var i=n[t],o=e[i];n[t]=[i,o,r(o)]}return n}},function(e,n,t){var r=t(74),a=t(210),i=t(217),o=t(44),l=t(84),s=t(85),c=t(23);e.exports=function(e,n){return o(e)&&l(n)?s(c(e),n):function(t){var o=a(t,e);return void 0===o&&o===n?i(t,e):r(n,o,3)}}},function(e,n,t){var r=t(86);e.exports=function(e,n,t){var a=null==e?void 0:r(e,n);return void 0===a?t:a}},function(e,n,t){var r=t(212),a=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,i=/\\(\\)?/g,o=r((function(e){var n=[];return 46===e.charCodeAt(0)&&n.push(""),e.replace(a,(function(e,t,r,a){n.push(r?a.replace(i,"$1"):t||e)})),n}));e.exports=o},function(e,n,t){var r=t(213);e.exports=function(e){var n=r(e,(function(e){return 500===t.size&&t.clear(),e})),t=n.cache;return n}},function(e,n,t){var r=t(41);function a(e,n){if("function"!=typeof e||null!=n&&"function"!=typeof n)throw new TypeError("Expected a function");var t=function(){var r=arguments,a=n?n.apply(this,r):r[0],i=t.cache;if(i.has(a))return i.get(a);var o=e.apply(this,r);return t.cache=i.set(a,o)||i,o};return t.cache=new(a.Cache||r),t}a.Cache=r,e.exports=a},function(e,n,t){var r=t(215);e.exports=function(e){return null==e?"":r(e)}},function(e,n,t){var r=t(15),a=t(216),i=t(6),o=t(45),l=r?r.prototype:void 0,s=l?l.toString:void 0;e.exports=function e(n){if("string"==typeof n)return n;if(i(n))return a(n,e)+"";if(o(n))return s?s.call(n):"";var t=n+"";return"0"==t&&1/n==-1/0?"-0":t}},function(e,n){e.exports=function(e,n){for(var t=-1,r=null==e?0:e.length,a=Array(r);++t<r;)a[t]=n(e[t],t,e);return a}},function(e,n,t){var r=t(218),a=t(219);e.exports=function(e,n){return null!=e&&a(e,n,r)}},function(e,n){e.exports=function(e,n){return null!=e&&n in Object(e)}},function(e,n,t){var r=t(87),a=t(38),i=t(6),o=t(80),l=t(43),s=t(23);e.exports=function(e,n,t){for(var c=-1,d=(n=r(n,e)).length,u=!1;++c<d;){var p=s(n[c]);if(!(u=null!=e&&t(e,p)))break;e=e[p]}return u||++c!=d?u:!!(d=null==e?0:e.length)&&l(d)&&o(p,d)&&(i(e)||a(e))}},function(e,n,t){var r=t(221),a=t(222),i=t(44),o=t(23);e.exports=function(e){return i(e)?r(o(e)):a(e)}},function(e,n){e.exports=function(e){return function(n){return null==n?void 0:n[e]}}},function(e,n,t){var r=t(86);e.exports=function(e){return function(n){return r(n,e)}}},function(e,n,t){var r=t(46),a=t(224),i=t(226);e.exports=function(e,n){return i(a(e,n,r),e+"")}},function(e,n,t){var r=t(225),a=Math.max;e.exports=function(e,n,t){return n=a(void 0===n?e.length-1:n,0),function(){for(var i=arguments,o=-1,l=a(i.length-n,0),s=Array(l);++o<l;)s[o]=i[n+o];o=-1;for(var c=Array(n+1);++o<n;)c[o]=i[o];return c[n]=t(s),r(e,this,c)}}},function(e,n){e.exports=function(e,n,t){switch(t.length){case 0:return e.call(n);case 1:return e.call(n,t[0]);case 2:return e.call(n,t[0],t[1]);case 3:return e.call(n,t[0],t[1],t[2])}return e.apply(n,t)}},function(e,n,t){var r=t(227),a=t(230)(r);e.exports=a},function(e,n,t){var r=t(228),a=t(229),i=t(46),o=a?function(e,n){return a(e,"toString",{configurable:!0,enumerable:!1,value:r(n),writable:!0})}:i;e.exports=o},function(e,n){e.exports=function(e){return function(){return e}}},function(e,n,t){var r=t(10),a=function(){try{var e=r(Object,"defineProperty");return e({},"",{}),e}catch(e){}}();e.exports=a},function(e,n){var t=Date.now;e.exports=function(e){var n=0,r=0;return function(){var a=t(),i=16-(a-r);if(r=a,i>0){if(++n>=800)return arguments[0]}else n=0;return e.apply(void 0,arguments)}}},function(e,n,t){var r=t(76),a=t(232),i=t(237),o=t(77),l=t(238),s=t(42);e.exports=function(e,n,t){var c=-1,d=a,u=e.length,p=!0,h=[],f=h;if(t)p=!1,d=i;else if(u>=200){var m=n?null:l(e);if(m)return s(m);p=!1,d=o,f=new r}else f=n?[]:h;e:for(;++c<u;){var b=e[c],v=n?n(b):b;if(b=t||0!==b?b:0,p&&v==v){for(var y=f.length;y--;)if(f[y]===v)continue e;n&&f.push(v),h.push(b)}else d(f,v,t)||(f!==h&&f.push(v),h.push(b))}return h}},function(e,n,t){var r=t(233);e.exports=function(e,n){return!!(null==e?0:e.length)&&r(e,n,0)>-1}},function(e,n,t){var r=t(234),a=t(235),i=t(236);e.exports=function(e,n,t){return n==n?i(e,n,t):r(e,a,t)}},function(e,n){e.exports=function(e,n,t,r){for(var a=e.length,i=t+(r?1:-1);r?i--:++i<a;)if(n(e[i],i,e))return i;return-1}},function(e,n){e.exports=function(e){return e!=e}},function(e,n){e.exports=function(e,n,t){for(var r=t-1,a=e.length;++r<a;)if(e[r]===n)return r;return-1}},function(e,n){e.exports=function(e,n,t){for(var r=-1,a=null==e?0:e.length;++r<a;)if(t(n,e[r]))return!0;return!1}},function(e,n,t){var r=t(83),a=t(239),i=t(42),o=r&&1/i(new r([,-0]))[1]==1/0?function(e){return new r(e)}:a;e.exports=o},function(e,n){e.exports=function(){}},function(e,n,t){var r=t(82),a=t(12);e.exports=function(e){return a(e)&&r(e)}},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";t(88)},function(e,n,t){"use strict";t(89)},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";t(90)},function(e,n,t){"use strict";t(91)},function(e,n,t){"use strict";t.r(n);
/*!
 * Vue.js v2.7.16
 * (c) 2014-2023 Evan You
 * Released under the MIT License.
 */
var r=Object.freeze({}),a=Array.isArray;function i(e){return null==e}function o(e){return null!=e}function l(e){return!0===e}function s(e){return"string"==typeof e||"number"==typeof e||"symbol"==typeof e||"boolean"==typeof e}function c(e){return"function"==typeof e}function d(e){return null!==e&&"object"==typeof e}var u=Object.prototype.toString;function p(e){return"[object Object]"===u.call(e)}function f(e){return"[object RegExp]"===u.call(e)}function m(e){var n=parseFloat(String(e));return n>=0&&Math.floor(n)===n&&isFinite(e)}function b(e){return o(e)&&"function"==typeof e.then&&"function"==typeof e.catch}function v(e){return null==e?"":Array.isArray(e)||p(e)&&e.toString===u?JSON.stringify(e,y,2):String(e)}function y(e,n){return n&&n.__v_isRef?n.value:n}function g(e){var n=parseFloat(e);return isNaN(n)?e:n}function k(e,n){for(var t=Object.create(null),r=e.split(","),a=0;a<r.length;a++)t[r[a]]=!0;return n?function(e){return t[e.toLowerCase()]}:function(e){return t[e]}}k("slot,component",!0);var x=k("key,ref,slot,slot-scope,is");function C(e,n){var t=e.length;if(t){if(n===e[t-1])return void(e.length=t-1);var r=e.indexOf(n);if(r>-1)return e.splice(r,1)}}var w=Object.prototype.hasOwnProperty;function _(e,n){return w.call(e,n)}function E(e){var n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}var B=/-(\w)/g,S=E((function(e){return e.replace(B,(function(e,n){return n?n.toUpperCase():""}))})),R=E((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})),I=/\B([A-Z])/g,O=E((function(e){return e.replace(I,"-$1").toLowerCase()}));var A=Function.prototype.bind?function(e,n){return e.bind(n)}:function(e,n){function t(t){var r=arguments.length;return r?r>1?e.apply(n,arguments):e.call(n,t):e.call(n)}return t._length=e.length,t};function T(e,n){n=n||0;for(var t=e.length-n,r=new Array(t);t--;)r[t]=e[t+n];return r}function N(e,n){for(var t in n)e[t]=n[t];return e}function z(e){for(var n={},t=0;t<e.length;t++)e[t]&&N(n,e[t]);return n}function L(e,n,t){}var P=function(e,n,t){return!1},D=function(e){return e};function F(e,n){if(e===n)return!0;var t=d(e),r=d(n);if(!t||!r)return!t&&!r&&String(e)===String(n);try{var a=Array.isArray(e),i=Array.isArray(n);if(a&&i)return e.length===n.length&&e.every((function(e,t){return F(e,n[t])}));if(e instanceof Date&&n instanceof Date)return e.getTime()===n.getTime();if(a||i)return!1;var o=Object.keys(e),l=Object.keys(n);return o.length===l.length&&o.every((function(t){return F(e[t],n[t])}))}catch(e){return!1}}function U(e,n){for(var t=0;t<e.length;t++)if(F(e[t],n))return t;return-1}function H(e){var n=!1;return function(){n||(n=!0,e.apply(this,arguments))}}function j(e,n){return e===n?0===e&&1/e!=1/n:e==e||n==n}var M=["component","directive","filter"],K=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],q={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:P,isReservedAttr:P,isUnknownElement:P,getTagNamespace:L,parsePlatformTagName:D,mustUseProp:P,async:!0,_lifecycleHooks:K},G=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function W(e){var n=(e+"").charCodeAt(0);return 36===n||95===n}function V(e,n,t,r){Object.defineProperty(e,n,{value:t,enumerable:!!r,writable:!0,configurable:!0})}var J=new RegExp("[^".concat(G.source,".$_\\d]"));var $="__proto__"in{},Z="undefined"!=typeof window,X=Z&&window.navigator.userAgent.toLowerCase(),Q=X&&/msie|trident/.test(X),Y=X&&X.indexOf("msie 9.0")>0,ee=X&&X.indexOf("edge/")>0;X&&X.indexOf("android");var ne=X&&/iphone|ipad|ipod|ios/.test(X);X&&/chrome\/\d+/.test(X),X&&/phantomjs/.test(X);var te,re=X&&X.match(/firefox\/(\d+)/),ae={}.watch,ie=!1;if(Z)try{var oe={};Object.defineProperty(oe,"passive",{get:function(){ie=!0}}),window.addEventListener("test-passive",null,oe)}catch(e){}var le=function(){return void 0===te&&(te=!Z&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),te},se=Z&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ce(e){return"function"==typeof e&&/native code/.test(e.toString())}var de,ue="undefined"!=typeof Symbol&&ce(Symbol)&&"undefined"!=typeof Reflect&&ce(Reflect.ownKeys);de="undefined"!=typeof Set&&ce(Set)?Set:function(){function e(){this.set=Object.create(null)}return e.prototype.has=function(e){return!0===this.set[e]},e.prototype.add=function(e){this.set[e]=!0},e.prototype.clear=function(){this.set=Object.create(null)},e}();var pe=null;function he(e){void 0===e&&(e=null),e||pe&&pe._scope.off(),pe=e,e&&e._scope.on()}var fe=function(){function e(e,n,t,r,a,i,o,l){this.tag=e,this.data=n,this.children=t,this.text=r,this.elm=a,this.ns=void 0,this.context=i,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=n&&n.key,this.componentOptions=o,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=l,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(e.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),e}(),me=function(e){void 0===e&&(e="");var n=new fe;return n.text=e,n.isComment=!0,n};function be(e){return new fe(void 0,void 0,void 0,String(e))}function ve(e){var n=new fe(e.tag,e.data,e.children&&e.children.slice(),e.text,e.elm,e.context,e.componentOptions,e.asyncFactory);return n.ns=e.ns,n.isStatic=e.isStatic,n.key=e.key,n.isComment=e.isComment,n.fnContext=e.fnContext,n.fnOptions=e.fnOptions,n.fnScopeId=e.fnScopeId,n.asyncMeta=e.asyncMeta,n.isCloned=!0,n}"function"==typeof SuppressedError&&SuppressedError;var ye=0,ge=[],ke=function(){function e(){this._pending=!1,this.id=ye++,this.subs=[]}return e.prototype.addSub=function(e){this.subs.push(e)},e.prototype.removeSub=function(e){this.subs[this.subs.indexOf(e)]=null,this._pending||(this._pending=!0,ge.push(this))},e.prototype.depend=function(n){e.target&&e.target.addDep(this)},e.prototype.notify=function(e){var n=this.subs.filter((function(e){return e}));for(var t=0,r=n.length;t<r;t++){0,n[t].update()}},e}();ke.target=null;var xe=[];function Ce(e){xe.push(e),ke.target=e}function we(){xe.pop(),ke.target=xe[xe.length-1]}var _e=Array.prototype,Ee=Object.create(_e);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(e){var n=_e[e];V(Ee,e,(function(){for(var t=[],r=0;r<arguments.length;r++)t[r]=arguments[r];var a,i=n.apply(this,t),o=this.__ob__;switch(e){case"push":case"unshift":a=t;break;case"splice":a=t.slice(2)}return a&&o.observeArray(a),o.dep.notify(),i}))}));var Be=Object.getOwnPropertyNames(Ee),Se={},Re=!0;function Ie(e){Re=e}var Oe={notify:L,depend:L,addSub:L,removeSub:L},Ae=function(){function e(e,n,t){if(void 0===n&&(n=!1),void 0===t&&(t=!1),this.value=e,this.shallow=n,this.mock=t,this.dep=t?Oe:new ke,this.vmCount=0,V(e,"__ob__",this),a(e)){if(!t)if($)e.__proto__=Ee;else for(var r=0,i=Be.length;r<i;r++){V(e,l=Be[r],Ee[l])}n||this.observeArray(e)}else{var o=Object.keys(e);for(r=0;r<o.length;r++){var l;Ne(e,l=o[r],Se,void 0,n,t)}}}return e.prototype.observeArray=function(e){for(var n=0,t=e.length;n<t;n++)Te(e[n],!1,this.mock)},e}();function Te(e,n,t){return e&&_(e,"__ob__")&&e.__ob__ instanceof Ae?e.__ob__:!Re||!t&&le()||!a(e)&&!p(e)||!Object.isExtensible(e)||e.__v_skip||He(e)||e instanceof fe?void 0:new Ae(e,n,t)}function Ne(e,n,t,r,i,o,l){void 0===l&&(l=!1);var s=new ke,c=Object.getOwnPropertyDescriptor(e,n);if(!c||!1!==c.configurable){var d=c&&c.get,u=c&&c.set;d&&!u||t!==Se&&2!==arguments.length||(t=e[n]);var p=i?t&&t.__ob__:Te(t,!1,o);return Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){var n=d?d.call(e):t;return ke.target&&(s.depend(),p&&(p.dep.depend(),a(n)&&Pe(n))),He(n)&&!i?n.value:n},set:function(n){var r=d?d.call(e):t;if(j(r,n)){if(u)u.call(e,n);else{if(d)return;if(!i&&He(r)&&!He(n))return void(r.value=n);t=n}p=i?n&&n.__ob__:Te(n,!1,o),s.notify()}}}),s}}function ze(e,n,t){if(!Ue(e)){var r=e.__ob__;return a(e)&&m(n)?(e.length=Math.max(e.length,n),e.splice(n,1,t),r&&!r.shallow&&r.mock&&Te(t,!1,!0),t):n in e&&!(n in Object.prototype)?(e[n]=t,t):e._isVue||r&&r.vmCount?t:r?(Ne(r.value,n,t,void 0,r.shallow,r.mock),r.dep.notify(),t):(e[n]=t,t)}}function Le(e,n){if(a(e)&&m(n))e.splice(n,1);else{var t=e.__ob__;e._isVue||t&&t.vmCount||Ue(e)||_(e,n)&&(delete e[n],t&&t.dep.notify())}}function Pe(e){for(var n=void 0,t=0,r=e.length;t<r;t++)(n=e[t])&&n.__ob__&&n.__ob__.dep.depend(),a(n)&&Pe(n)}function De(e){return Fe(e,!0),V(e,"__v_isShallow",!0),e}function Fe(e,n){if(!Ue(e)){Te(e,n,le());0}}function Ue(e){return!(!e||!e.__v_isReadonly)}function He(e){return!(!e||!0!==e.__v_isRef)}function je(e,n,t){Object.defineProperty(e,t,{enumerable:!0,configurable:!0,get:function(){var e=n[t];if(He(e))return e.value;var r=e&&e.__ob__;return r&&r.dep.depend(),e},set:function(e){var r=n[t];He(r)&&!He(e)?r.value=e:n[t]=e}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Me;var Ke=function(){function e(e){void 0===e&&(e=!1),this.detached=e,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Me,!e&&Me&&(this.index=(Me.scopes||(Me.scopes=[])).push(this)-1)}return e.prototype.run=function(e){if(this.active){var n=Me;try{return Me=this,e()}finally{Me=n}}else 0},e.prototype.on=function(){Me=this},e.prototype.off=function(){Me=this.parent},e.prototype.stop=function(e){if(this.active){var n=void 0,t=void 0;for(n=0,t=this.effects.length;n<t;n++)this.effects[n].teardown();for(n=0,t=this.cleanups.length;n<t;n++)this.cleanups[n]();if(this.scopes)for(n=0,t=this.scopes.length;n<t;n++)this.scopes[n].stop(!0);if(!this.detached&&this.parent&&!e){var r=this.parent.scopes.pop();r&&r!==this&&(this.parent.scopes[this.index]=r,r.index=this.index)}this.parent=void 0,this.active=!1}},e}();function qe(e){var n=e._provided,t=e.$parent&&e.$parent._provided;return t===n?e._provided=Object.create(t):n}var Ge=E((function(e){var n="&"===e.charAt(0),t="~"===(e=n?e.slice(1):e).charAt(0),r="!"===(e=t?e.slice(1):e).charAt(0);return{name:e=r?e.slice(1):e,once:t,capture:r,passive:n}}));function We(e,n){function t(){var e=t.fns;if(!a(e))return In(e,null,arguments,n,"v-on handler");for(var r=e.slice(),i=0;i<r.length;i++)In(r[i],null,arguments,n,"v-on handler")}return t.fns=e,t}function Ve(e,n,t,r,a,o){var s,c,d,u;for(s in e)c=e[s],d=n[s],u=Ge(s),i(c)||(i(d)?(i(c.fns)&&(c=e[s]=We(c,o)),l(u.once)&&(c=e[s]=a(u.name,c,u.capture)),t(u.name,c,u.capture,u.passive,u.params)):c!==d&&(d.fns=c,e[s]=d));for(s in n)i(e[s])&&r((u=Ge(s)).name,n[s],u.capture)}function Je(e,n,t){var r;e instanceof fe&&(e=e.data.hook||(e.data.hook={}));var a=e[n];function s(){t.apply(this,arguments),C(r.fns,s)}i(a)?r=We([s]):o(a.fns)&&l(a.merged)?(r=a).fns.push(s):r=We([a,s]),r.merged=!0,e[n]=r}function $e(e,n,t,r,a){if(o(n)){if(_(n,t))return e[t]=n[t],a||delete n[t],!0;if(_(n,r))return e[t]=n[r],a||delete n[r],!0}return!1}function Ze(e){return s(e)?[be(e)]:a(e)?function e(n,t){var r,c,d,u,p=[];for(r=0;r<n.length;r++)i(c=n[r])||"boolean"==typeof c||(d=p.length-1,u=p[d],a(c)?c.length>0&&(Xe((c=e(c,"".concat(t||"","_").concat(r)))[0])&&Xe(u)&&(p[d]=be(u.text+c[0].text),c.shift()),p.push.apply(p,c)):s(c)?Xe(u)?p[d]=be(u.text+c):""!==c&&p.push(be(c)):Xe(c)&&Xe(u)?p[d]=be(u.text+c.text):(l(n._isVList)&&o(c.tag)&&i(c.key)&&o(t)&&(c.key="__vlist".concat(t,"_").concat(r,"__")),p.push(c)));return p}(e):void 0}function Xe(e){return o(e)&&o(e.text)&&!1===e.isComment}function Qe(e,n){var t,r,i,l,s=null;if(a(e)||"string"==typeof e)for(s=new Array(e.length),t=0,r=e.length;t<r;t++)s[t]=n(e[t],t);else if("number"==typeof e)for(s=new Array(e),t=0;t<e;t++)s[t]=n(t+1,t);else if(d(e))if(ue&&e[Symbol.iterator]){s=[];for(var c=e[Symbol.iterator](),u=c.next();!u.done;)s.push(n(u.value,s.length)),u=c.next()}else for(i=Object.keys(e),s=new Array(i.length),t=0,r=i.length;t<r;t++)l=i[t],s[t]=n(e[l],l,t);return o(s)||(s=[]),s._isVList=!0,s}function Ye(e,n,t,r){var a,i=this.$scopedSlots[e];i?(t=t||{},r&&(t=N(N({},r),t)),a=i(t)||(c(n)?n():n)):a=this.$slots[e]||(c(n)?n():n);var o=t&&t.slot;return o?this.$createElement("template",{slot:o},a):a}function en(e){return Tt(this.$options,"filters",e,!0)||D}function nn(e,n){return a(e)?-1===e.indexOf(n):e!==n}function tn(e,n,t,r,a){var i=q.keyCodes[n]||t;return a&&r&&!q.keyCodes[n]?nn(a,r):i?nn(i,e):r?O(r)!==n:void 0===e}function rn(e,n,t,r,i){if(t)if(d(t)){a(t)&&(t=z(t));var o=void 0,l=function(a){if("class"===a||"style"===a||x(a))o=e;else{var l=e.attrs&&e.attrs.type;o=r||q.mustUseProp(n,l,a)?e.domProps||(e.domProps={}):e.attrs||(e.attrs={})}var s=S(a),c=O(a);s in o||c in o||(o[a]=t[a],i&&((e.on||(e.on={}))["update:".concat(a)]=function(e){t[a]=e}))};for(var s in t)l(s)}else;return e}function an(e,n){var t=this._staticTrees||(this._staticTrees=[]),r=t[e];return r&&!n||ln(r=t[e]=this.$options.staticRenderFns[e].call(this._renderProxy,this._c,this),"__static__".concat(e),!1),r}function on(e,n,t){return ln(e,"__once__".concat(n).concat(t?"_".concat(t):""),!0),e}function ln(e,n,t){if(a(e))for(var r=0;r<e.length;r++)e[r]&&"string"!=typeof e[r]&&sn(e[r],"".concat(n,"_").concat(r),t);else sn(e,n,t)}function sn(e,n,t){e.isStatic=!0,e.key=n,e.isOnce=t}function cn(e,n){if(n)if(p(n)){var t=e.on=e.on?N({},e.on):{};for(var r in n){var a=t[r],i=n[r];t[r]=a?[].concat(a,i):i}}else;return e}function dn(e,n,t,r){n=n||{$stable:!t};for(var i=0;i<e.length;i++){var o=e[i];a(o)?dn(o,n,t):o&&(o.proxy&&(o.fn.proxy=!0),n[o.key]=o.fn)}return r&&(n.$key=r),n}function un(e,n){for(var t=0;t<n.length;t+=2){var r=n[t];"string"==typeof r&&r&&(e[n[t]]=n[t+1])}return e}function pn(e,n){return"string"==typeof e?n+e:e}function hn(e){e._o=on,e._n=g,e._s=v,e._l=Qe,e._t=Ye,e._q=F,e._i=U,e._m=an,e._f=en,e._k=tn,e._b=rn,e._v=be,e._e=me,e._u=dn,e._g=cn,e._d=un,e._p=pn}function fn(e,n){if(!e||!e.length)return{};for(var t={},r=0,a=e.length;r<a;r++){var i=e[r],o=i.data;if(o&&o.attrs&&o.attrs.slot&&delete o.attrs.slot,i.context!==n&&i.fnContext!==n||!o||null==o.slot)(t.default||(t.default=[])).push(i);else{var l=o.slot,s=t[l]||(t[l]=[]);"template"===i.tag?s.push.apply(s,i.children||[]):s.push(i)}}for(var c in t)t[c].every(mn)&&delete t[c];return t}function mn(e){return e.isComment&&!e.asyncFactory||" "===e.text}function bn(e){return e.isComment&&e.asyncFactory}function vn(e,n,t,a){var i,o=Object.keys(t).length>0,l=n?!!n.$stable:!o,s=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(l&&a&&a!==r&&s===a.$key&&!o&&!a.$hasNormal)return a;for(var c in i={},n)n[c]&&"$"!==c[0]&&(i[c]=yn(e,t,c,n[c]))}else i={};for(var d in t)d in i||(i[d]=gn(t,d));return n&&Object.isExtensible(n)&&(n._normalized=i),V(i,"$stable",l),V(i,"$key",s),V(i,"$hasNormal",o),i}function yn(e,n,t,r){var i=function(){var n=pe;he(e);var t=arguments.length?r.apply(null,arguments):r({}),i=(t=t&&"object"==typeof t&&!a(t)?[t]:Ze(t))&&t[0];return he(n),t&&(!i||1===t.length&&i.isComment&&!bn(i))?void 0:t};return r.proxy&&Object.defineProperty(n,t,{get:i,enumerable:!0,configurable:!0}),i}function gn(e,n){return function(){return e[n]}}function kn(e){return{get attrs(){if(!e._attrsProxy){var n=e._attrsProxy={};V(n,"_v_attr_proxy",!0),xn(n,e.$attrs,r,e,"$attrs")}return e._attrsProxy},get listeners(){e._listenersProxy||xn(e._listenersProxy={},e.$listeners,r,e,"$listeners");return e._listenersProxy},get slots(){return function(e){e._slotsProxy||wn(e._slotsProxy={},e.$scopedSlots);return e._slotsProxy}(e)},emit:A(e.$emit,e),expose:function(n){n&&Object.keys(n).forEach((function(t){return je(e,n,t)}))}}}function xn(e,n,t,r,a){var i=!1;for(var o in n)o in e?n[o]!==t[o]&&(i=!0):(i=!0,Cn(e,o,r,a));for(var o in e)o in n||(i=!0,delete e[o]);return i}function Cn(e,n,t,r){Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){return t[r][n]}})}function wn(e,n){for(var t in n)e[t]=n[t];for(var t in e)t in n||delete e[t]}var _n=null;function En(e,n){return(e.__esModule||ue&&"Module"===e[Symbol.toStringTag])&&(e=e.default),d(e)?n.extend(e):e}function Bn(e){if(a(e))for(var n=0;n<e.length;n++){var t=e[n];if(o(t)&&(o(t.componentOptions)||bn(t)))return t}}function Sn(e,n,t,r,u,p){return(a(t)||s(t))&&(u=r,r=t,t=void 0),l(p)&&(u=2),function(e,n,t,r,s){if(o(t)&&o(t.__ob__))return me();o(t)&&o(t.is)&&(n=t.is);if(!n)return me();0;a(r)&&c(r[0])&&((t=t||{}).scopedSlots={default:r[0]},r.length=0);2===s?r=Ze(r):1===s&&(r=function(e){for(var n=0;n<e.length;n++)if(a(e[n]))return Array.prototype.concat.apply([],e);return e}(r));var u,p;if("string"==typeof n){var h=void 0;p=e.$vnode&&e.$vnode.ns||q.getTagNamespace(n),u=q.isReservedTag(n)?new fe(q.parsePlatformTagName(n),t,r,void 0,void 0,e):t&&t.pre||!o(h=Tt(e.$options,"components",n))?new fe(n,t,r,void 0,void 0,e):Ct(h,t,e,r,n)}else u=Ct(n,t,e,r);return a(u)?u:o(u)?(o(p)&&function e(n,t,r){n.ns=t,"foreignObject"===n.tag&&(t=void 0,r=!0);if(o(n.children))for(var a=0,s=n.children.length;a<s;a++){var c=n.children[a];o(c.tag)&&(i(c.ns)||l(r)&&"svg"!==c.tag)&&e(c,t,r)}}(u,p),o(t)&&function(e){d(e.style)&&qn(e.style);d(e.class)&&qn(e.class)}(t),u):me()}(e,n,t,r,u)}function Rn(e,n,t){Ce();try{if(n)for(var r=n;r=r.$parent;){var a=r.$options.errorCaptured;if(a)for(var i=0;i<a.length;i++)try{if(!1===a[i].call(r,e,n,t))return}catch(e){On(e,r,"errorCaptured hook")}}On(e,n,t)}finally{we()}}function In(e,n,t,r,a){var i;try{(i=t?e.apply(n,t):e.call(n))&&!i._isVue&&b(i)&&!i._handled&&(i.catch((function(e){return Rn(e,r,a+" (Promise/async)")})),i._handled=!0)}catch(e){Rn(e,r,a)}return i}function On(e,n,t){if(q.errorHandler)try{return q.errorHandler.call(null,e,n,t)}catch(n){n!==e&&An(n,null,"config.errorHandler")}An(e,n,t)}function An(e,n,t){if(!Z||"undefined"==typeof console)throw e;console.error(e)}var Tn,Nn=!1,zn=[],Ln=!1;function Pn(){Ln=!1;var e=zn.slice(0);zn.length=0;for(var n=0;n<e.length;n++)e[n]()}if("undefined"!=typeof Promise&&ce(Promise)){var Dn=Promise.resolve();Tn=function(){Dn.then(Pn),ne&&setTimeout(L)},Nn=!0}else if(Q||"undefined"==typeof MutationObserver||!ce(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Tn="undefined"!=typeof setImmediate&&ce(setImmediate)?function(){setImmediate(Pn)}:function(){setTimeout(Pn,0)};else{var Fn=1,Un=new MutationObserver(Pn),Hn=document.createTextNode(String(Fn));Un.observe(Hn,{characterData:!0}),Tn=function(){Fn=(Fn+1)%2,Hn.data=String(Fn)},Nn=!0}function jn(e,n){var t;if(zn.push((function(){if(e)try{e.call(n)}catch(e){Rn(e,n,"nextTick")}else t&&t(n)})),Ln||(Ln=!0,Tn()),!e&&"undefined"!=typeof Promise)return new Promise((function(e){t=e}))}function Mn(e){return function(n,t){if(void 0===t&&(t=pe),t)return function(e,n,t){var r=e.$options;r[n]=Rt(r[n],t)}(t,e,n)}}Mn("beforeMount"),Mn("mounted"),Mn("beforeUpdate"),Mn("updated"),Mn("beforeDestroy"),Mn("destroyed"),Mn("activated"),Mn("deactivated"),Mn("serverPrefetch"),Mn("renderTracked"),Mn("renderTriggered"),Mn("errorCaptured");var Kn=new de;function qn(e){return function e(n,t){var r,i,o=a(n);if(!o&&!d(n)||n.__v_skip||Object.isFrozen(n)||n instanceof fe)return;if(n.__ob__){var l=n.__ob__.dep.id;if(t.has(l))return;t.add(l)}if(o)for(r=n.length;r--;)e(n[r],t);else if(He(n))e(n.value,t);else for(i=Object.keys(n),r=i.length;r--;)e(n[i[r]],t)}(e,Kn),Kn.clear(),e}var Gn,Wn=0,Vn=function(){function e(e,n,t,r,a){var i,o;i=this,void 0===(o=Me&&!Me._vm?Me:e?e._scope:void 0)&&(o=Me),o&&o.active&&o.effects.push(i),(this.vm=e)&&a&&(e._watcher=this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++Wn,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new de,this.newDepIds=new de,this.expression="",c(n)?this.getter=n:(this.getter=function(e){if(!J.test(e)){var n=e.split(".");return function(e){for(var t=0;t<n.length;t++){if(!e)return;e=e[n[t]]}return e}}}(n),this.getter||(this.getter=L)),this.value=this.lazy?void 0:this.get()}return e.prototype.get=function(){var e;Ce(this);var n=this.vm;try{e=this.getter.call(n,n)}catch(e){if(!this.user)throw e;Rn(e,n,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&qn(e),we(),this.cleanupDeps()}return e},e.prototype.addDep=function(e){var n=e.id;this.newDepIds.has(n)||(this.newDepIds.add(n),this.newDeps.push(e),this.depIds.has(n)||e.addSub(this))},e.prototype.cleanupDeps=function(){for(var e=this.deps.length;e--;){var n=this.deps[e];this.newDepIds.has(n.id)||n.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},e.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():ft(this)},e.prototype.run=function(){if(this.active){var e=this.get();if(e!==this.value||d(e)||this.deep){var n=this.value;if(this.value=e,this.user){var t='callback for watcher "'.concat(this.expression,'"');In(this.cb,this.vm,[e,n],this.vm,t)}else this.cb.call(this.vm,e,n)}}},e.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},e.prototype.depend=function(){for(var e=this.deps.length;e--;)this.deps[e].depend()},e.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&C(this.vm._scope.effects,this),this.active){for(var e=this.deps.length;e--;)this.deps[e].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},e}();function Jn(e,n){Gn.$on(e,n)}function $n(e,n){Gn.$off(e,n)}function Zn(e,n){var t=Gn;return function r(){var a=n.apply(null,arguments);null!==a&&t.$off(e,r)}}function Xn(e,n,t){Gn=e,Ve(n,t||{},Jn,$n,Zn,e),Gn=void 0}var Qn=null;function Yn(e){var n=Qn;return Qn=e,function(){Qn=n}}function et(e){for(;e&&(e=e.$parent);)if(e._inactive)return!0;return!1}function nt(e,n){if(n){if(e._directInactive=!1,et(e))return}else if(e._directInactive)return;if(e._inactive||null===e._inactive){e._inactive=!1;for(var t=0;t<e.$children.length;t++)nt(e.$children[t]);tt(e,"activated")}}function tt(e,n,t,r){void 0===r&&(r=!0),Ce();var a=pe,i=Me;r&&he(e);var o=e.$options[n],l="".concat(n," hook");if(o)for(var s=0,c=o.length;s<c;s++)In(o[s],e,t||null,e,l);e._hasHookEvent&&e.$emit("hook:"+n),r&&(he(a),i&&i.on()),we()}var rt=[],at=[],it={},ot=!1,lt=!1,st=0;var ct=0,dt=Date.now;if(Z&&!Q){var ut=window.performance;ut&&"function"==typeof ut.now&&dt()>document.createEvent("Event").timeStamp&&(dt=function(){return ut.now()})}var pt=function(e,n){if(e.post){if(!n.post)return 1}else if(n.post)return-1;return e.id-n.id};function ht(){var e,n;for(ct=dt(),lt=!0,rt.sort(pt),st=0;st<rt.length;st++)(e=rt[st]).before&&e.before(),n=e.id,it[n]=null,e.run();var t=at.slice(),r=rt.slice();st=rt.length=at.length=0,it={},ot=lt=!1,function(e){for(var n=0;n<e.length;n++)e[n]._inactive=!0,nt(e[n],!0)}(t),function(e){var n=e.length;for(;n--;){var t=e[n],r=t.vm;r&&r._watcher===t&&r._isMounted&&!r._isDestroyed&&tt(r,"updated")}}(r),function(){for(var e=0;e<ge.length;e++){var n=ge[e];n.subs=n.subs.filter((function(e){return e})),n._pending=!1}ge.length=0}(),se&&q.devtools&&se.emit("flush")}function ft(e){var n=e.id;if(null==it[n]&&(e!==ke.target||!e.noRecurse)){if(it[n]=!0,lt){for(var t=rt.length-1;t>st&&rt[t].id>e.id;)t--;rt.splice(t+1,0,e)}else rt.push(e);ot||(ot=!0,jn(ht))}}function mt(e,n){if(e){for(var t=Object.create(null),r=ue?Reflect.ownKeys(e):Object.keys(e),a=0;a<r.length;a++){var i=r[a];if("__ob__"!==i){var o=e[i].from;if(o in n._provided)t[i]=n._provided[o];else if("default"in e[i]){var l=e[i].default;t[i]=c(l)?l.call(n):l}else 0}}return t}}function bt(e,n,t,i,o){var s,c=this,d=o.options;_(i,"_uid")?(s=Object.create(i))._original=i:(s=i,i=i._original);var u=l(d._compiled),p=!u;this.data=e,this.props=n,this.children=t,this.parent=i,this.listeners=e.on||r,this.injections=mt(d.inject,i),this.slots=function(){return c.$slots||vn(i,e.scopedSlots,c.$slots=fn(t,i)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return vn(i,e.scopedSlots,this.slots())}}),u&&(this.$options=d,this.$slots=this.slots(),this.$scopedSlots=vn(i,e.scopedSlots,this.$slots)),d._scopeId?this._c=function(e,n,t,r){var o=Sn(s,e,n,t,r,p);return o&&!a(o)&&(o.fnScopeId=d._scopeId,o.fnContext=i),o}:this._c=function(e,n,t,r){return Sn(s,e,n,t,r,p)}}function vt(e,n,t,r,a){var i=ve(e);return i.fnContext=t,i.fnOptions=r,n.slot&&((i.data||(i.data={})).slot=n.slot),i}function yt(e,n){for(var t in n)e[S(t)]=n[t]}function gt(e){return e.name||e.__name||e._componentTag}hn(bt.prototype);var kt={init:function(e,n){if(e.componentInstance&&!e.componentInstance._isDestroyed&&e.data.keepAlive){var t=e;kt.prepatch(t,t)}else{(e.componentInstance=function(e,n){var t={_isComponent:!0,_parentVnode:e,parent:n},r=e.data.inlineTemplate;o(r)&&(t.render=r.render,t.staticRenderFns=r.staticRenderFns);return new e.componentOptions.Ctor(t)}(e,Qn)).$mount(n?e.elm:void 0,n)}},prepatch:function(e,n){var t=n.componentOptions;!function(e,n,t,a,i){var o=a.data.scopedSlots,l=e.$scopedSlots,s=!!(o&&!o.$stable||l!==r&&!l.$stable||o&&e.$scopedSlots.$key!==o.$key||!o&&e.$scopedSlots.$key),c=!!(i||e.$options._renderChildren||s),d=e.$vnode;e.$options._parentVnode=a,e.$vnode=a,e._vnode&&(e._vnode.parent=a),e.$options._renderChildren=i;var u=a.data.attrs||r;e._attrsProxy&&xn(e._attrsProxy,u,d.data&&d.data.attrs||r,e,"$attrs")&&(c=!0),e.$attrs=u,t=t||r;var p=e.$options._parentListeners;if(e._listenersProxy&&xn(e._listenersProxy,t,p||r,e,"$listeners"),e.$listeners=e.$options._parentListeners=t,Xn(e,t,p),n&&e.$options.props){Ie(!1);for(var h=e._props,f=e.$options._propKeys||[],m=0;m<f.length;m++){var b=f[m],v=e.$options.props;h[b]=Nt(b,v,n,e)}Ie(!0),e.$options.propsData=n}c&&(e.$slots=fn(i,a.context),e.$forceUpdate())}(n.componentInstance=e.componentInstance,t.propsData,t.listeners,n,t.children)},insert:function(e){var n,t=e.context,r=e.componentInstance;r._isMounted||(r._isMounted=!0,tt(r,"mounted")),e.data.keepAlive&&(t._isMounted?((n=r)._inactive=!1,at.push(n)):nt(r,!0))},destroy:function(e){var n=e.componentInstance;n._isDestroyed||(e.data.keepAlive?function e(n,t){if(!(t&&(n._directInactive=!0,et(n))||n._inactive)){n._inactive=!0;for(var r=0;r<n.$children.length;r++)e(n.$children[r]);tt(n,"deactivated")}}(n,!0):n.$destroy())}},xt=Object.keys(kt);function Ct(e,n,t,s,c){if(!i(e)){var u=t.$options._base;if(d(e)&&(e=u.extend(e)),"function"==typeof e){var p;if(i(e.cid)&&void 0===(e=function(e,n){if(l(e.error)&&o(e.errorComp))return e.errorComp;if(o(e.resolved))return e.resolved;var t=_n;if(t&&o(e.owners)&&-1===e.owners.indexOf(t)&&e.owners.push(t),l(e.loading)&&o(e.loadingComp))return e.loadingComp;if(t&&!o(e.owners)){var r=e.owners=[t],a=!0,s=null,c=null;t.$on("hook:destroyed",(function(){return C(r,t)}));var u=function(e){for(var n=0,t=r.length;n<t;n++)r[n].$forceUpdate();e&&(r.length=0,null!==s&&(clearTimeout(s),s=null),null!==c&&(clearTimeout(c),c=null))},p=H((function(t){e.resolved=En(t,n),a?r.length=0:u(!0)})),h=H((function(n){o(e.errorComp)&&(e.error=!0,u(!0))})),f=e(p,h);return d(f)&&(b(f)?i(e.resolved)&&f.then(p,h):b(f.component)&&(f.component.then(p,h),o(f.error)&&(e.errorComp=En(f.error,n)),o(f.loading)&&(e.loadingComp=En(f.loading,n),0===f.delay?e.loading=!0:s=setTimeout((function(){s=null,i(e.resolved)&&i(e.error)&&(e.loading=!0,u(!1))}),f.delay||200)),o(f.timeout)&&(c=setTimeout((function(){c=null,i(e.resolved)&&h(null)}),f.timeout)))),a=!1,e.loading?e.loadingComp:e.resolved}}(p=e,u)))return function(e,n,t,r,a){var i=me();return i.asyncFactory=e,i.asyncMeta={data:n,context:t,children:r,tag:a},i}(p,n,t,s,c);n=n||{},Vt(e),o(n.model)&&function(e,n){var t=e.model&&e.model.prop||"value",r=e.model&&e.model.event||"input";(n.attrs||(n.attrs={}))[t]=n.model.value;var i=n.on||(n.on={}),l=i[r],s=n.model.callback;o(l)?(a(l)?-1===l.indexOf(s):l!==s)&&(i[r]=[s].concat(l)):i[r]=s}(e.options,n);var h=function(e,n,t){var r=n.options.props;if(!i(r)){var a={},l=e.attrs,s=e.props;if(o(l)||o(s))for(var c in r){var d=O(c);$e(a,s,c,d,!0)||$e(a,l,c,d,!1)}return a}}(n,e);if(l(e.options.functional))return function(e,n,t,i,l){var s=e.options,c={},d=s.props;if(o(d))for(var u in d)c[u]=Nt(u,d,n||r);else o(t.attrs)&&yt(c,t.attrs),o(t.props)&&yt(c,t.props);var p=new bt(t,c,l,i,e),h=s.render.call(null,p._c,p);if(h instanceof fe)return vt(h,t,p.parent,s,p);if(a(h)){for(var f=Ze(h)||[],m=new Array(f.length),b=0;b<f.length;b++)m[b]=vt(f[b],t,p.parent,s,p);return m}}(e,h,n,t,s);var f=n.on;if(n.on=n.nativeOn,l(e.options.abstract)){var m=n.slot;n={},m&&(n.slot=m)}!function(e){for(var n=e.hook||(e.hook={}),t=0;t<xt.length;t++){var r=xt[t],a=n[r],i=kt[r];a===i||a&&a._merged||(n[r]=a?wt(i,a):i)}}(n);var v=gt(e.options)||c;return new fe("vue-component-".concat(e.cid).concat(v?"-".concat(v):""),n,void 0,void 0,void 0,t,{Ctor:e,propsData:h,listeners:f,tag:c,children:s},p)}}}function wt(e,n){var t=function(t,r){e(t,r),n(t,r)};return t._merged=!0,t}var _t=L,Et=q.optionMergeStrategies;function Bt(e,n,t){if(void 0===t&&(t=!0),!n)return e;for(var r,a,i,o=ue?Reflect.ownKeys(n):Object.keys(n),l=0;l<o.length;l++)"__ob__"!==(r=o[l])&&(a=e[r],i=n[r],t&&_(e,r)?a!==i&&p(a)&&p(i)&&Bt(a,i):ze(e,r,i));return e}function St(e,n,t){return t?function(){var r=c(n)?n.call(t,t):n,a=c(e)?e.call(t,t):e;return r?Bt(r,a):a}:n?e?function(){return Bt(c(n)?n.call(this,this):n,c(e)?e.call(this,this):e)}:n:e}function Rt(e,n){var t=n?e?e.concat(n):a(n)?n:[n]:e;return t?function(e){for(var n=[],t=0;t<e.length;t++)-1===n.indexOf(e[t])&&n.push(e[t]);return n}(t):t}function It(e,n,t,r){var a=Object.create(e||null);return n?N(a,n):a}Et.data=function(e,n,t){return t?St(e,n,t):n&&"function"!=typeof n?e:St(e,n)},K.forEach((function(e){Et[e]=Rt})),M.forEach((function(e){Et[e+"s"]=It})),Et.watch=function(e,n,t,r){if(e===ae&&(e=void 0),n===ae&&(n=void 0),!n)return Object.create(e||null);if(!e)return n;var i={};for(var o in N(i,e),n){var l=i[o],s=n[o];l&&!a(l)&&(l=[l]),i[o]=l?l.concat(s):a(s)?s:[s]}return i},Et.props=Et.methods=Et.inject=Et.computed=function(e,n,t,r){if(!e)return n;var a=Object.create(null);return N(a,e),n&&N(a,n),a},Et.provide=function(e,n){return e?function(){var t=Object.create(null);return Bt(t,c(e)?e.call(this):e),n&&Bt(t,c(n)?n.call(this):n,!1),t}:n};var Ot=function(e,n){return void 0===n?e:n};function At(e,n,t){if(c(n)&&(n=n.options),function(e,n){var t=e.props;if(t){var r,i,o={};if(a(t))for(r=t.length;r--;)"string"==typeof(i=t[r])&&(o[S(i)]={type:null});else if(p(t))for(var l in t)i=t[l],o[S(l)]=p(i)?i:{type:i};else 0;e.props=o}}(n),function(e,n){var t=e.inject;if(t){var r=e.inject={};if(a(t))for(var i=0;i<t.length;i++)r[t[i]]={from:t[i]};else if(p(t))for(var o in t){var l=t[o];r[o]=p(l)?N({from:o},l):{from:l}}else 0}}(n),function(e){var n=e.directives;if(n)for(var t in n){var r=n[t];c(r)&&(n[t]={bind:r,update:r})}}(n),!n._base&&(n.extends&&(e=At(e,n.extends,t)),n.mixins))for(var r=0,i=n.mixins.length;r<i;r++)e=At(e,n.mixins[r],t);var o,l={};for(o in e)s(o);for(o in n)_(e,o)||s(o);function s(r){var a=Et[r]||Ot;l[r]=a(e[r],n[r],t,r)}return l}function Tt(e,n,t,r){if("string"==typeof t){var a=e[n];if(_(a,t))return a[t];var i=S(t);if(_(a,i))return a[i];var o=R(i);return _(a,o)?a[o]:a[t]||a[i]||a[o]}}function Nt(e,n,t,r){var a=n[e],i=!_(t,e),o=t[e],l=Dt(Boolean,a.type);if(l>-1)if(i&&!_(a,"default"))o=!1;else if(""===o||o===O(e)){var s=Dt(String,a.type);(s<0||l<s)&&(o=!0)}if(void 0===o){o=function(e,n,t){if(!_(n,"default"))return;var r=n.default;0;if(e&&e.$options.propsData&&void 0===e.$options.propsData[t]&&void 0!==e._props[t])return e._props[t];return c(r)&&"Function"!==Lt(n.type)?r.call(e):r}(r,a,e);var d=Re;Ie(!0),Te(o),Ie(d)}return o}var zt=/^\s*function (\w+)/;function Lt(e){var n=e&&e.toString().match(zt);return n?n[1]:""}function Pt(e,n){return Lt(e)===Lt(n)}function Dt(e,n){if(!a(n))return Pt(n,e)?0:-1;for(var t=0,r=n.length;t<r;t++)if(Pt(n[t],e))return t;return-1}var Ft={enumerable:!0,configurable:!0,get:L,set:L};function Ut(e,n,t){Ft.get=function(){return this[n][t]},Ft.set=function(e){this[n][t]=e},Object.defineProperty(e,t,Ft)}function Ht(e){var n=e.$options;if(n.props&&function(e,n){var t=e.$options.propsData||{},r=e._props=De({}),a=e.$options._propKeys=[];e.$parent&&Ie(!1);var i=function(i){a.push(i);var o=Nt(i,n,t,e);Ne(r,i,o,void 0,!0),i in e||Ut(e,"_props",i)};for(var o in n)i(o);Ie(!0)}(e,n.props),function(e){var n=e.$options,t=n.setup;if(t){var r=e._setupContext=kn(e);he(e),Ce();var a=In(t,null,[e._props||De({}),r],e,"setup");if(we(),he(),c(a))n.render=a;else if(d(a))if(e._setupState=a,a.__sfc){var i=e._setupProxy={};for(var o in a)"__sfc"!==o&&je(i,a,o)}else for(var o in a)W(o)||je(e,a,o);else 0}}(e),n.methods&&function(e,n){e.$options.props;for(var t in n)e[t]="function"!=typeof n[t]?L:A(n[t],e)}(e,n.methods),n.data)!function(e){var n=e.$options.data;p(n=e._data=c(n)?function(e,n){Ce();try{return e.call(n,n)}catch(e){return Rn(e,n,"data()"),{}}finally{we()}}(n,e):n||{})||(n={});var t=Object.keys(n),r=e.$options.props,a=(e.$options.methods,t.length);for(;a--;){var i=t[a];0,r&&_(r,i)||W(i)||Ut(e,"_data",i)}var o=Te(n);o&&o.vmCount++}(e);else{var t=Te(e._data={});t&&t.vmCount++}n.computed&&function(e,n){var t=e._computedWatchers=Object.create(null),r=le();for(var a in n){var i=n[a],o=c(i)?i:i.get;0,r||(t[a]=new Vn(e,o||L,L,jt)),a in e||Mt(e,a,i)}}(e,n.computed),n.watch&&n.watch!==ae&&function(e,n){for(var t in n){var r=n[t];if(a(r))for(var i=0;i<r.length;i++)Gt(e,t,r[i]);else Gt(e,t,r)}}(e,n.watch)}var jt={lazy:!0};function Mt(e,n,t){var r=!le();c(t)?(Ft.get=r?Kt(n):qt(t),Ft.set=L):(Ft.get=t.get?r&&!1!==t.cache?Kt(n):qt(t.get):L,Ft.set=t.set||L),Object.defineProperty(e,n,Ft)}function Kt(e){return function(){var n=this._computedWatchers&&this._computedWatchers[e];if(n)return n.dirty&&n.evaluate(),ke.target&&n.depend(),n.value}}function qt(e){return function(){return e.call(this,this)}}function Gt(e,n,t,r){return p(t)&&(r=t,t=t.handler),"string"==typeof t&&(t=e[t]),e.$watch(n,t,r)}var Wt=0;function Vt(e){var n=e.options;if(e.super){var t=Vt(e.super);if(t!==e.superOptions){e.superOptions=t;var r=function(e){var n,t=e.options,r=e.sealedOptions;for(var a in t)t[a]!==r[a]&&(n||(n={}),n[a]=t[a]);return n}(e);r&&N(e.extendOptions,r),(n=e.options=At(t,e.extendOptions)).name&&(n.components[n.name]=e)}}return n}function Jt(e){this._init(e)}function $t(e){e.cid=0;var n=1;e.extend=function(e){e=e||{};var t=this,r=t.cid,a=e._Ctor||(e._Ctor={});if(a[r])return a[r];var i=gt(e)||gt(t.options);var o=function(e){this._init(e)};return(o.prototype=Object.create(t.prototype)).constructor=o,o.cid=n++,o.options=At(t.options,e),o.super=t,o.options.props&&function(e){var n=e.options.props;for(var t in n)Ut(e.prototype,"_props",t)}(o),o.options.computed&&function(e){var n=e.options.computed;for(var t in n)Mt(e.prototype,t,n[t])}(o),o.extend=t.extend,o.mixin=t.mixin,o.use=t.use,M.forEach((function(e){o[e]=t[e]})),i&&(o.options.components[i]=o),o.superOptions=t.options,o.extendOptions=e,o.sealedOptions=N({},o.options),a[r]=o,o}}function Zt(e){return e&&(gt(e.Ctor.options)||e.tag)}function Xt(e,n){return a(e)?e.indexOf(n)>-1:"string"==typeof e?e.split(",").indexOf(n)>-1:!!f(e)&&e.test(n)}function Qt(e,n){var t=e.cache,r=e.keys,a=e._vnode,i=e.$vnode;for(var o in t){var l=t[o];if(l){var s=l.name;s&&!n(s)&&Yt(t,o,r,a)}}i.componentOptions.children=void 0}function Yt(e,n,t,r){var a=e[n];!a||r&&a.tag===r.tag||a.componentInstance.$destroy(),e[n]=null,C(t,n)}!function(e){e.prototype._init=function(e){var n=this;n._uid=Wt++,n._isVue=!0,n.__v_skip=!0,n._scope=new Ke(!0),n._scope.parent=void 0,n._scope._vm=!0,e&&e._isComponent?function(e,n){var t=e.$options=Object.create(e.constructor.options),r=n._parentVnode;t.parent=n.parent,t._parentVnode=r;var a=r.componentOptions;t.propsData=a.propsData,t._parentListeners=a.listeners,t._renderChildren=a.children,t._componentTag=a.tag,n.render&&(t.render=n.render,t.staticRenderFns=n.staticRenderFns)}(n,e):n.$options=At(Vt(n.constructor),e||{},n),n._renderProxy=n,n._self=n,function(e){var n=e.$options,t=n.parent;if(t&&!n.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(e)}e.$parent=t,e.$root=t?t.$root:e,e.$children=[],e.$refs={},e._provided=t?t._provided:Object.create(null),e._watcher=null,e._inactive=null,e._directInactive=!1,e._isMounted=!1,e._isDestroyed=!1,e._isBeingDestroyed=!1}(n),function(e){e._events=Object.create(null),e._hasHookEvent=!1;var n=e.$options._parentListeners;n&&Xn(e,n)}(n),function(e){e._vnode=null,e._staticTrees=null;var n=e.$options,t=e.$vnode=n._parentVnode,a=t&&t.context;e.$slots=fn(n._renderChildren,a),e.$scopedSlots=t?vn(e.$parent,t.data.scopedSlots,e.$slots):r,e._c=function(n,t,r,a){return Sn(e,n,t,r,a,!1)},e.$createElement=function(n,t,r,a){return Sn(e,n,t,r,a,!0)};var i=t&&t.data;Ne(e,"$attrs",i&&i.attrs||r,null,!0),Ne(e,"$listeners",n._parentListeners||r,null,!0)}(n),tt(n,"beforeCreate",void 0,!1),function(e){var n=mt(e.$options.inject,e);n&&(Ie(!1),Object.keys(n).forEach((function(t){Ne(e,t,n[t])})),Ie(!0))}(n),Ht(n),function(e){var n=e.$options.provide;if(n){var t=c(n)?n.call(e):n;if(!d(t))return;for(var r=qe(e),a=ue?Reflect.ownKeys(t):Object.keys(t),i=0;i<a.length;i++){var o=a[i];Object.defineProperty(r,o,Object.getOwnPropertyDescriptor(t,o))}}}(n),tt(n,"created"),n.$options.el&&n.$mount(n.$options.el)}}(Jt),function(e){var n={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(e.prototype,"$data",n),Object.defineProperty(e.prototype,"$props",t),e.prototype.$set=ze,e.prototype.$delete=Le,e.prototype.$watch=function(e,n,t){if(p(n))return Gt(this,e,n,t);(t=t||{}).user=!0;var r=new Vn(this,e,n,t);if(t.immediate){var a='callback for immediate watcher "'.concat(r.expression,'"');Ce(),In(n,this,[r.value],this,a),we()}return function(){r.teardown()}}}(Jt),function(e){var n=/^hook:/;e.prototype.$on=function(e,t){var r=this;if(a(e))for(var i=0,o=e.length;i<o;i++)r.$on(e[i],t);else(r._events[e]||(r._events[e]=[])).push(t),n.test(e)&&(r._hasHookEvent=!0);return r},e.prototype.$once=function(e,n){var t=this;function r(){t.$off(e,r),n.apply(t,arguments)}return r.fn=n,t.$on(e,r),t},e.prototype.$off=function(e,n){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(a(e)){for(var r=0,i=e.length;r<i;r++)t.$off(e[r],n);return t}var o,l=t._events[e];if(!l)return t;if(!n)return t._events[e]=null,t;for(var s=l.length;s--;)if((o=l[s])===n||o.fn===n){l.splice(s,1);break}return t},e.prototype.$emit=function(e){var n=this,t=n._events[e];if(t){t=t.length>1?T(t):t;for(var r=T(arguments,1),a='event handler for "'.concat(e,'"'),i=0,o=t.length;i<o;i++)In(t[i],n,r,n,a)}return n}}(Jt),function(e){e.prototype._update=function(e,n){var t=this,r=t.$el,a=t._vnode,i=Yn(t);t._vnode=e,t.$el=a?t.__patch__(a,e):t.__patch__(t.$el,e,n,!1),i(),r&&(r.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var o=t;o&&o.$vnode&&o.$parent&&o.$vnode===o.$parent._vnode;)o.$parent.$el=o.$el,o=o.$parent},e.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},e.prototype.$destroy=function(){var e=this;if(!e._isBeingDestroyed){tt(e,"beforeDestroy"),e._isBeingDestroyed=!0;var n=e.$parent;!n||n._isBeingDestroyed||e.$options.abstract||C(n.$children,e),e._scope.stop(),e._data.__ob__&&e._data.__ob__.vmCount--,e._isDestroyed=!0,e.__patch__(e._vnode,null),tt(e,"destroyed"),e.$off(),e.$el&&(e.$el.__vue__=null),e.$vnode&&(e.$vnode.parent=null)}}}(Jt),function(e){hn(e.prototype),e.prototype.$nextTick=function(e){return jn(e,this)},e.prototype._render=function(){var e=this,n=e.$options,t=n.render,r=n._parentVnode;r&&e._isMounted&&(e.$scopedSlots=vn(e.$parent,r.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&wn(e._slotsProxy,e.$scopedSlots)),e.$vnode=r;var i,o=pe,l=_n;try{he(e),_n=e,i=t.call(e._renderProxy,e.$createElement)}catch(n){Rn(n,e,"render"),i=e._vnode}finally{_n=l,he(o)}return a(i)&&1===i.length&&(i=i[0]),i instanceof fe||(i=me()),i.parent=r,i}}(Jt);var er=[String,RegExp,Array],nr={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:er,exclude:er,max:[String,Number]},methods:{cacheVNode:function(){var e=this.cache,n=this.keys,t=this.vnodeToCache,r=this.keyToCache;if(t){var a=t.tag,i=t.componentInstance,o=t.componentOptions;e[r]={name:Zt(o),tag:a,componentInstance:i},n.push(r),this.max&&n.length>parseInt(this.max)&&Yt(e,n[0],n,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var e in this.cache)Yt(this.cache,e,this.keys)},mounted:function(){var e=this;this.cacheVNode(),this.$watch("include",(function(n){Qt(e,(function(e){return Xt(n,e)}))})),this.$watch("exclude",(function(n){Qt(e,(function(e){return!Xt(n,e)}))}))},updated:function(){this.cacheVNode()},render:function(){var e=this.$slots.default,n=Bn(e),t=n&&n.componentOptions;if(t){var r=Zt(t),a=this.include,i=this.exclude;if(a&&(!r||!Xt(a,r))||i&&r&&Xt(i,r))return n;var o=this.cache,l=this.keys,s=null==n.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):n.key;o[s]?(n.componentInstance=o[s].componentInstance,C(l,s),l.push(s)):(this.vnodeToCache=n,this.keyToCache=s),n.data.keepAlive=!0}return n||e&&e[0]}}};!function(e){var n={get:function(){return q}};Object.defineProperty(e,"config",n),e.util={warn:_t,extend:N,mergeOptions:At,defineReactive:Ne},e.set=ze,e.delete=Le,e.nextTick=jn,e.observable=function(e){return Te(e),e},e.options=Object.create(null),M.forEach((function(n){e.options[n+"s"]=Object.create(null)})),e.options._base=e,N(e.options.components,nr),function(e){e.use=function(e){var n=this._installedPlugins||(this._installedPlugins=[]);if(n.indexOf(e)>-1)return this;var t=T(arguments,1);return t.unshift(this),c(e.install)?e.install.apply(e,t):c(e)&&e.apply(null,t),n.push(e),this}}(e),function(e){e.mixin=function(e){return this.options=At(this.options,e),this}}(e),$t(e),function(e){M.forEach((function(n){e[n]=function(e,t){return t?("component"===n&&p(t)&&(t.name=t.name||e,t=this.options._base.extend(t)),"directive"===n&&c(t)&&(t={bind:t,update:t}),this.options[n+"s"][e]=t,t):this.options[n+"s"][e]}}))}(e)}(Jt),Object.defineProperty(Jt.prototype,"$isServer",{get:le}),Object.defineProperty(Jt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Jt,"FunctionalRenderContext",{value:bt}),Jt.version="2.7.16";var tr=k("style,class"),rr=k("input,textarea,option,select,progress"),ar=k("contenteditable,draggable,spellcheck"),ir=k("events,caret,typing,plaintext-only"),or=k("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),lr="http://www.w3.org/1999/xlink",sr=function(e){return":"===e.charAt(5)&&"xlink"===e.slice(0,5)},cr=function(e){return sr(e)?e.slice(6,e.length):""},dr=function(e){return null==e||!1===e};function ur(e){for(var n=e.data,t=e,r=e;o(r.componentInstance);)(r=r.componentInstance._vnode)&&r.data&&(n=pr(r.data,n));for(;o(t=t.parent);)t&&t.data&&(n=pr(n,t.data));return function(e,n){if(o(e)||o(n))return hr(e,fr(n));return""}(n.staticClass,n.class)}function pr(e,n){return{staticClass:hr(e.staticClass,n.staticClass),class:o(e.class)?[e.class,n.class]:n.class}}function hr(e,n){return e?n?e+" "+n:e:n||""}function fr(e){return Array.isArray(e)?function(e){for(var n,t="",r=0,a=e.length;r<a;r++)o(n=fr(e[r]))&&""!==n&&(t&&(t+=" "),t+=n);return t}(e):d(e)?function(e){var n="";for(var t in e)e[t]&&(n&&(n+=" "),n+=t);return n}(e):"string"==typeof e?e:""}var mr={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},br=k("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),vr=k("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),yr=function(e){return br(e)||vr(e)};var gr=Object.create(null);var kr=k("text,number,password,search,email,tel,url");var xr=Object.freeze({__proto__:null,createElement:function(e,n){var t=document.createElement(e);return"select"!==e||n.data&&n.data.attrs&&void 0!==n.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(e,n){return document.createElementNS(mr[e],n)},createTextNode:function(e){return document.createTextNode(e)},createComment:function(e){return document.createComment(e)},insertBefore:function(e,n,t){e.insertBefore(n,t)},removeChild:function(e,n){e.removeChild(n)},appendChild:function(e,n){e.appendChild(n)},parentNode:function(e){return e.parentNode},nextSibling:function(e){return e.nextSibling},tagName:function(e){return e.tagName},setTextContent:function(e,n){e.textContent=n},setStyleScope:function(e,n){e.setAttribute(n,"")}}),Cr={create:function(e,n){wr(n)},update:function(e,n){e.data.ref!==n.data.ref&&(wr(e,!0),wr(n))},destroy:function(e){wr(e,!0)}};function wr(e,n){var t=e.data.ref;if(o(t)){var r=e.context,i=e.componentInstance||e.elm,l=n?null:i,s=n?void 0:i;if(c(t))In(t,r,[l],r,"template ref function");else{var d=e.data.refInFor,u="string"==typeof t||"number"==typeof t,p=He(t),h=r.$refs;if(u||p)if(d){var f=u?h[t]:t.value;n?a(f)&&C(f,i):a(f)?f.includes(i)||f.push(i):u?(h[t]=[i],_r(r,t,h[t])):t.value=[i]}else if(u){if(n&&h[t]!==i)return;h[t]=s,_r(r,t,l)}else if(p){if(n&&t.value!==i)return;t.value=l}else 0}}}function _r(e,n,t){var r=e._setupState;r&&_(r,n)&&(He(r[n])?r[n].value=t:r[n]=t)}var Er=new fe("",{},[]),Br=["create","activate","update","remove","destroy"];function Sr(e,n){return e.key===n.key&&e.asyncFactory===n.asyncFactory&&(e.tag===n.tag&&e.isComment===n.isComment&&o(e.data)===o(n.data)&&function(e,n){if("input"!==e.tag)return!0;var t,r=o(t=e.data)&&o(t=t.attrs)&&t.type,a=o(t=n.data)&&o(t=t.attrs)&&t.type;return r===a||kr(r)&&kr(a)}(e,n)||l(e.isAsyncPlaceholder)&&i(n.asyncFactory.error))}function Rr(e,n,t){var r,a,i={};for(r=n;r<=t;++r)o(a=e[r].key)&&(i[a]=r);return i}var Ir={create:Or,update:Or,destroy:function(e){Or(e,Er)}};function Or(e,n){(e.data.directives||n.data.directives)&&function(e,n){var t,r,a,i=e===Er,o=n===Er,l=Tr(e.data.directives,e.context),s=Tr(n.data.directives,n.context),c=[],d=[];for(t in s)r=l[t],a=s[t],r?(a.oldValue=r.value,a.oldArg=r.arg,zr(a,"update",n,e),a.def&&a.def.componentUpdated&&d.push(a)):(zr(a,"bind",n,e),a.def&&a.def.inserted&&c.push(a));if(c.length){var u=function(){for(var t=0;t<c.length;t++)zr(c[t],"inserted",n,e)};i?Je(n,"insert",u):u()}d.length&&Je(n,"postpatch",(function(){for(var t=0;t<d.length;t++)zr(d[t],"componentUpdated",n,e)}));if(!i)for(t in l)s[t]||zr(l[t],"unbind",e,e,o)}(e,n)}var Ar=Object.create(null);function Tr(e,n){var t,r,a=Object.create(null);if(!e)return a;for(t=0;t<e.length;t++){if((r=e[t]).modifiers||(r.modifiers=Ar),a[Nr(r)]=r,n._setupState&&n._setupState.__sfc){var i=r.def||Tt(n,"_setupState","v-"+r.name);r.def="function"==typeof i?{bind:i,update:i}:i}r.def=r.def||Tt(n.$options,"directives",r.name)}return a}function Nr(e){return e.rawName||"".concat(e.name,".").concat(Object.keys(e.modifiers||{}).join("."))}function zr(e,n,t,r,a){var i=e.def&&e.def[n];if(i)try{i(t.elm,e,t,r,a)}catch(r){Rn(r,t.context,"directive ".concat(e.name," ").concat(n," hook"))}}var Lr=[Cr,Ir];function Pr(e,n){var t=n.componentOptions;if(!(o(t)&&!1===t.Ctor.options.inheritAttrs||i(e.data.attrs)&&i(n.data.attrs))){var r,a,s=n.elm,c=e.data.attrs||{},d=n.data.attrs||{};for(r in(o(d.__ob__)||l(d._v_attr_proxy))&&(d=n.data.attrs=N({},d)),d)a=d[r],c[r]!==a&&Dr(s,r,a,n.data.pre);for(r in(Q||ee)&&d.value!==c.value&&Dr(s,"value",d.value),c)i(d[r])&&(sr(r)?s.removeAttributeNS(lr,cr(r)):ar(r)||s.removeAttribute(r))}}function Dr(e,n,t,r){r||e.tagName.indexOf("-")>-1?Fr(e,n,t):or(n)?dr(t)?e.removeAttribute(n):(t="allowfullscreen"===n&&"EMBED"===e.tagName?"true":n,e.setAttribute(n,t)):ar(n)?e.setAttribute(n,function(e,n){return dr(n)||"false"===n?"false":"contenteditable"===e&&ir(n)?n:"true"}(n,t)):sr(n)?dr(t)?e.removeAttributeNS(lr,cr(n)):e.setAttributeNS(lr,n,t):Fr(e,n,t)}function Fr(e,n,t){if(dr(t))e.removeAttribute(n);else{if(Q&&!Y&&"TEXTAREA"===e.tagName&&"placeholder"===n&&""!==t&&!e.__ieph){var r=function(n){n.stopImmediatePropagation(),e.removeEventListener("input",r)};e.addEventListener("input",r),e.__ieph=!0}e.setAttribute(n,t)}}var Ur={create:Pr,update:Pr};function Hr(e,n){var t=n.elm,r=n.data,a=e.data;if(!(i(r.staticClass)&&i(r.class)&&(i(a)||i(a.staticClass)&&i(a.class)))){var l=ur(n),s=t._transitionClasses;o(s)&&(l=hr(l,fr(s))),l!==t._prevClass&&(t.setAttribute("class",l),t._prevClass=l)}}var jr,Mr={create:Hr,update:Hr};function Kr(e,n,t){var r=jr;return function a(){var i=n.apply(null,arguments);null!==i&&Wr(e,a,t,r)}}var qr=Nn&&!(re&&Number(re[1])<=53);function Gr(e,n,t,r){if(qr){var a=ct,i=n;n=i._wrapper=function(e){if(e.target===e.currentTarget||e.timeStamp>=a||e.timeStamp<=0||e.target.ownerDocument!==document)return i.apply(this,arguments)}}jr.addEventListener(e,n,ie?{capture:t,passive:r}:t)}function Wr(e,n,t,r){(r||jr).removeEventListener(e,n._wrapper||n,t)}function Vr(e,n){if(!i(e.data.on)||!i(n.data.on)){var t=n.data.on||{},r=e.data.on||{};jr=n.elm||e.elm,function(e){if(o(e.__r)){var n=Q?"change":"input";e[n]=[].concat(e.__r,e[n]||[]),delete e.__r}o(e.__c)&&(e.change=[].concat(e.__c,e.change||[]),delete e.__c)}(t),Ve(t,r,Gr,Wr,Kr,n.context),jr=void 0}}var Jr,$r={create:Vr,update:Vr,destroy:function(e){return Vr(e,Er)}};function Zr(e,n){if(!i(e.data.domProps)||!i(n.data.domProps)){var t,r,a=n.elm,s=e.data.domProps||{},c=n.data.domProps||{};for(t in(o(c.__ob__)||l(c._v_attr_proxy))&&(c=n.data.domProps=N({},c)),s)t in c||(a[t]="");for(t in c){if(r=c[t],"textContent"===t||"innerHTML"===t){if(n.children&&(n.children.length=0),r===s[t])continue;1===a.childNodes.length&&a.removeChild(a.childNodes[0])}if("value"===t&&"PROGRESS"!==a.tagName){a._value=r;var d=i(r)?"":String(r);Xr(a,d)&&(a.value=d)}else if("innerHTML"===t&&vr(a.tagName)&&i(a.innerHTML)){(Jr=Jr||document.createElement("div")).innerHTML="<svg>".concat(r,"</svg>");for(var u=Jr.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;u.firstChild;)a.appendChild(u.firstChild)}else if(r!==s[t])try{a[t]=r}catch(e){}}}}function Xr(e,n){return!e.composing&&("OPTION"===e.tagName||function(e,n){var t=!0;try{t=document.activeElement!==e}catch(e){}return t&&e.value!==n}(e,n)||function(e,n){var t=e.value,r=e._vModifiers;if(o(r)){if(r.number)return g(t)!==g(n);if(r.trim)return t.trim()!==n.trim()}return t!==n}(e,n))}var Qr={create:Zr,update:Zr},Yr=E((function(e){var n={},t=/:(.+)/;return e.split(/;(?![^(]*\))/g).forEach((function(e){if(e){var r=e.split(t);r.length>1&&(n[r[0].trim()]=r[1].trim())}})),n}));function ea(e){var n=na(e.style);return e.staticStyle?N(e.staticStyle,n):n}function na(e){return Array.isArray(e)?z(e):"string"==typeof e?Yr(e):e}var ta,ra=/^--/,aa=/\s*!important$/,ia=function(e,n,t){if(ra.test(n))e.style.setProperty(n,t);else if(aa.test(t))e.style.setProperty(O(n),t.replace(aa,""),"important");else{var r=la(n);if(Array.isArray(t))for(var a=0,i=t.length;a<i;a++)e.style[r]=t[a];else e.style[r]=t}},oa=["Webkit","Moz","ms"],la=E((function(e){if(ta=ta||document.createElement("div").style,"filter"!==(e=S(e))&&e in ta)return e;for(var n=e.charAt(0).toUpperCase()+e.slice(1),t=0;t<oa.length;t++){var r=oa[t]+n;if(r in ta)return r}}));function sa(e,n){var t=n.data,r=e.data;if(!(i(t.staticStyle)&&i(t.style)&&i(r.staticStyle)&&i(r.style))){var a,l,s=n.elm,c=r.staticStyle,d=r.normalizedStyle||r.style||{},u=c||d,p=na(n.data.style)||{};n.data.normalizedStyle=o(p.__ob__)?N({},p):p;var h=function(e,n){var t,r={};if(n)for(var a=e;a.componentInstance;)(a=a.componentInstance._vnode)&&a.data&&(t=ea(a.data))&&N(r,t);(t=ea(e.data))&&N(r,t);for(var i=e;i=i.parent;)i.data&&(t=ea(i.data))&&N(r,t);return r}(n,!0);for(l in u)i(h[l])&&ia(s,l,"");for(l in h)a=h[l],ia(s,l,null==a?"":a)}}var ca={create:sa,update:sa},da=/\s+/;function ua(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(da).forEach((function(n){return e.classList.add(n)})):e.classList.add(n);else{var t=" ".concat(e.getAttribute("class")||""," ");t.indexOf(" "+n+" ")<0&&e.setAttribute("class",(t+n).trim())}}function pa(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(da).forEach((function(n){return e.classList.remove(n)})):e.classList.remove(n),e.classList.length||e.removeAttribute("class");else{for(var t=" ".concat(e.getAttribute("class")||""," "),r=" "+n+" ";t.indexOf(r)>=0;)t=t.replace(r," ");(t=t.trim())?e.setAttribute("class",t):e.removeAttribute("class")}}function ha(e){if(e){if("object"==typeof e){var n={};return!1!==e.css&&N(n,fa(e.name||"v")),N(n,e),n}return"string"==typeof e?fa(e):void 0}}var fa=E((function(e){return{enterClass:"".concat(e,"-enter"),enterToClass:"".concat(e,"-enter-to"),enterActiveClass:"".concat(e,"-enter-active"),leaveClass:"".concat(e,"-leave"),leaveToClass:"".concat(e,"-leave-to"),leaveActiveClass:"".concat(e,"-leave-active")}})),ma=Z&&!Y,ba="transition",va="transitionend",ya="animation",ga="animationend";ma&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(ba="WebkitTransition",va="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(ya="WebkitAnimation",ga="webkitAnimationEnd"));var ka=Z?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(e){return e()};function xa(e){ka((function(){ka(e)}))}function Ca(e,n){var t=e._transitionClasses||(e._transitionClasses=[]);t.indexOf(n)<0&&(t.push(n),ua(e,n))}function wa(e,n){e._transitionClasses&&C(e._transitionClasses,n),pa(e,n)}function _a(e,n,t){var r=Ba(e,n),a=r.type,i=r.timeout,o=r.propCount;if(!a)return t();var l="transition"===a?va:ga,s=0,c=function(){e.removeEventListener(l,d),t()},d=function(n){n.target===e&&++s>=o&&c()};setTimeout((function(){s<o&&c()}),i+1),e.addEventListener(l,d)}var Ea=/\b(transform|all)(,|$)/;function Ba(e,n){var t,r=window.getComputedStyle(e),a=(r[ba+"Delay"]||"").split(", "),i=(r[ba+"Duration"]||"").split(", "),o=Sa(a,i),l=(r[ya+"Delay"]||"").split(", "),s=(r[ya+"Duration"]||"").split(", "),c=Sa(l,s),d=0,u=0;return"transition"===n?o>0&&(t="transition",d=o,u=i.length):"animation"===n?c>0&&(t="animation",d=c,u=s.length):u=(t=(d=Math.max(o,c))>0?o>c?"transition":"animation":null)?"transition"===t?i.length:s.length:0,{type:t,timeout:d,propCount:u,hasTransform:"transition"===t&&Ea.test(r[ba+"Property"])}}function Sa(e,n){for(;e.length<n.length;)e=e.concat(e);return Math.max.apply(null,n.map((function(n,t){return Ra(n)+Ra(e[t])})))}function Ra(e){return 1e3*Number(e.slice(0,-1).replace(",","."))}function Ia(e,n){var t=e.elm;o(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var r=ha(e.data.transition);if(!i(r)&&!o(t._enterCb)&&1===t.nodeType){for(var a=r.css,l=r.type,s=r.enterClass,u=r.enterToClass,p=r.enterActiveClass,h=r.appearClass,f=r.appearToClass,m=r.appearActiveClass,b=r.beforeEnter,v=r.enter,y=r.afterEnter,k=r.enterCancelled,x=r.beforeAppear,C=r.appear,w=r.afterAppear,_=r.appearCancelled,E=r.duration,B=Qn,S=Qn.$vnode;S&&S.parent;)B=S.context,S=S.parent;var R=!B._isMounted||!e.isRootInsert;if(!R||C||""===C){var I=R&&h?h:s,O=R&&m?m:p,A=R&&f?f:u,T=R&&x||b,N=R&&c(C)?C:v,z=R&&w||y,L=R&&_||k,P=g(d(E)?E.enter:E);0;var D=!1!==a&&!Y,F=Ta(N),U=t._enterCb=H((function(){D&&(wa(t,A),wa(t,O)),U.cancelled?(D&&wa(t,I),L&&L(t)):z&&z(t),t._enterCb=null}));e.data.show||Je(e,"insert",(function(){var n=t.parentNode,r=n&&n._pending&&n._pending[e.key];r&&r.tag===e.tag&&r.elm._leaveCb&&r.elm._leaveCb(),N&&N(t,U)})),T&&T(t),D&&(Ca(t,I),Ca(t,O),xa((function(){wa(t,I),U.cancelled||(Ca(t,A),F||(Aa(P)?setTimeout(U,P):_a(t,l,U)))}))),e.data.show&&(n&&n(),N&&N(t,U)),D||F||U()}}}function Oa(e,n){var t=e.elm;o(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var r=ha(e.data.transition);if(i(r)||1!==t.nodeType)return n();if(!o(t._leaveCb)){var a=r.css,l=r.type,s=r.leaveClass,c=r.leaveToClass,u=r.leaveActiveClass,p=r.beforeLeave,h=r.leave,f=r.afterLeave,m=r.leaveCancelled,b=r.delayLeave,v=r.duration,y=!1!==a&&!Y,k=Ta(h),x=g(d(v)?v.leave:v);0;var C=t._leaveCb=H((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[e.key]=null),y&&(wa(t,c),wa(t,u)),C.cancelled?(y&&wa(t,s),m&&m(t)):(n(),f&&f(t)),t._leaveCb=null}));b?b(w):w()}function w(){C.cancelled||(!e.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[e.key]=e),p&&p(t),y&&(Ca(t,s),Ca(t,u),xa((function(){wa(t,s),C.cancelled||(Ca(t,c),k||(Aa(x)?setTimeout(C,x):_a(t,l,C)))}))),h&&h(t,C),y||k||C())}}function Aa(e){return"number"==typeof e&&!isNaN(e)}function Ta(e){if(i(e))return!1;var n=e.fns;return o(n)?Ta(Array.isArray(n)?n[0]:n):(e._length||e.length)>1}function Na(e,n){!0!==n.data.show&&Ia(n)}var za=function(e){var n,t,r={},c=e.modules,d=e.nodeOps;for(n=0;n<Br.length;++n)for(r[Br[n]]=[],t=0;t<c.length;++t)o(c[t][Br[n]])&&r[Br[n]].push(c[t][Br[n]]);function u(e){var n=d.parentNode(e);o(n)&&d.removeChild(n,e)}function p(e,n,t,a,i,s,c){if(o(e.elm)&&o(s)&&(e=s[c]=ve(e)),e.isRootInsert=!i,!function(e,n,t,a){var i=e.data;if(o(i)){var s=o(e.componentInstance)&&i.keepAlive;if(o(i=i.hook)&&o(i=i.init)&&i(e,!1),o(e.componentInstance))return h(e,n),f(t,e.elm,a),l(s)&&function(e,n,t,a){var i,l=e;for(;l.componentInstance;)if(l=l.componentInstance._vnode,o(i=l.data)&&o(i=i.transition)){for(i=0;i<r.activate.length;++i)r.activate[i](Er,l);n.push(l);break}f(t,e.elm,a)}(e,n,t,a),!0}}(e,n,t,a)){var u=e.data,p=e.children,b=e.tag;o(b)?(e.elm=e.ns?d.createElementNS(e.ns,b):d.createElement(b,e),y(e),m(e,p,n),o(u)&&v(e,n),f(t,e.elm,a)):l(e.isComment)?(e.elm=d.createComment(e.text),f(t,e.elm,a)):(e.elm=d.createTextNode(e.text),f(t,e.elm,a))}}function h(e,n){o(e.data.pendingInsert)&&(n.push.apply(n,e.data.pendingInsert),e.data.pendingInsert=null),e.elm=e.componentInstance.$el,b(e)?(v(e,n),y(e)):(wr(e),n.push(e))}function f(e,n,t){o(e)&&(o(t)?d.parentNode(t)===e&&d.insertBefore(e,n,t):d.appendChild(e,n))}function m(e,n,t){if(a(n)){0;for(var r=0;r<n.length;++r)p(n[r],t,e.elm,null,!0,n,r)}else s(e.text)&&d.appendChild(e.elm,d.createTextNode(String(e.text)))}function b(e){for(;e.componentInstance;)e=e.componentInstance._vnode;return o(e.tag)}function v(e,t){for(var a=0;a<r.create.length;++a)r.create[a](Er,e);o(n=e.data.hook)&&(o(n.create)&&n.create(Er,e),o(n.insert)&&t.push(e))}function y(e){var n;if(o(n=e.fnScopeId))d.setStyleScope(e.elm,n);else for(var t=e;t;)o(n=t.context)&&o(n=n.$options._scopeId)&&d.setStyleScope(e.elm,n),t=t.parent;o(n=Qn)&&n!==e.context&&n!==e.fnContext&&o(n=n.$options._scopeId)&&d.setStyleScope(e.elm,n)}function g(e,n,t,r,a,i){for(;r<=a;++r)p(t[r],i,e,n,!1,t,r)}function x(e){var n,t,a=e.data;if(o(a))for(o(n=a.hook)&&o(n=n.destroy)&&n(e),n=0;n<r.destroy.length;++n)r.destroy[n](e);if(o(n=e.children))for(t=0;t<e.children.length;++t)x(e.children[t])}function C(e,n,t){for(;n<=t;++n){var r=e[n];o(r)&&(o(r.tag)?(w(r),x(r)):u(r.elm))}}function w(e,n){if(o(n)||o(e.data)){var t,a=r.remove.length+1;for(o(n)?n.listeners+=a:n=function(e,n){function t(){0==--t.listeners&&u(e)}return t.listeners=n,t}(e.elm,a),o(t=e.componentInstance)&&o(t=t._vnode)&&o(t.data)&&w(t,n),t=0;t<r.remove.length;++t)r.remove[t](e,n);o(t=e.data.hook)&&o(t=t.remove)?t(e,n):n()}else u(e.elm)}function _(e,n,t,r){for(var a=t;a<r;a++){var i=n[a];if(o(i)&&Sr(e,i))return a}}function E(e,n,t,a,s,c){if(e!==n){o(n.elm)&&o(a)&&(n=a[s]=ve(n));var u=n.elm=e.elm;if(l(e.isAsyncPlaceholder))o(n.asyncFactory.resolved)?R(e.elm,n,t):n.isAsyncPlaceholder=!0;else if(l(n.isStatic)&&l(e.isStatic)&&n.key===e.key&&(l(n.isCloned)||l(n.isOnce)))n.componentInstance=e.componentInstance;else{var h,f=n.data;o(f)&&o(h=f.hook)&&o(h=h.prepatch)&&h(e,n);var m=e.children,v=n.children;if(o(f)&&b(n)){for(h=0;h<r.update.length;++h)r.update[h](e,n);o(h=f.hook)&&o(h=h.update)&&h(e,n)}i(n.text)?o(m)&&o(v)?m!==v&&function(e,n,t,r,a){var l,s,c,u=0,h=0,f=n.length-1,m=n[0],b=n[f],v=t.length-1,y=t[0],k=t[v],x=!a;for(0;u<=f&&h<=v;)i(m)?m=n[++u]:i(b)?b=n[--f]:Sr(m,y)?(E(m,y,r,t,h),m=n[++u],y=t[++h]):Sr(b,k)?(E(b,k,r,t,v),b=n[--f],k=t[--v]):Sr(m,k)?(E(m,k,r,t,v),x&&d.insertBefore(e,m.elm,d.nextSibling(b.elm)),m=n[++u],k=t[--v]):Sr(b,y)?(E(b,y,r,t,h),x&&d.insertBefore(e,b.elm,m.elm),b=n[--f],y=t[++h]):(i(l)&&(l=Rr(n,u,f)),i(s=o(y.key)?l[y.key]:_(y,n,u,f))?p(y,r,e,m.elm,!1,t,h):Sr(c=n[s],y)?(E(c,y,r,t,h),n[s]=void 0,x&&d.insertBefore(e,c.elm,m.elm)):p(y,r,e,m.elm,!1,t,h),y=t[++h]);u>f?g(e,i(t[v+1])?null:t[v+1].elm,t,h,v,r):h>v&&C(n,u,f)}(u,m,v,t,c):o(v)?(o(e.text)&&d.setTextContent(u,""),g(u,null,v,0,v.length-1,t)):o(m)?C(m,0,m.length-1):o(e.text)&&d.setTextContent(u,""):e.text!==n.text&&d.setTextContent(u,n.text),o(f)&&o(h=f.hook)&&o(h=h.postpatch)&&h(e,n)}}}function B(e,n,t){if(l(t)&&o(e.parent))e.parent.data.pendingInsert=n;else for(var r=0;r<n.length;++r)n[r].data.hook.insert(n[r])}var S=k("attrs,class,staticClass,staticStyle,key");function R(e,n,t,r){var a,i=n.tag,s=n.data,c=n.children;if(r=r||s&&s.pre,n.elm=e,l(n.isComment)&&o(n.asyncFactory))return n.isAsyncPlaceholder=!0,!0;if(o(s)&&(o(a=s.hook)&&o(a=a.init)&&a(n,!0),o(a=n.componentInstance)))return h(n,t),!0;if(o(i)){if(o(c))if(e.hasChildNodes())if(o(a=s)&&o(a=a.domProps)&&o(a=a.innerHTML)){if(a!==e.innerHTML)return!1}else{for(var d=!0,u=e.firstChild,p=0;p<c.length;p++){if(!u||!R(u,c[p],t,r)){d=!1;break}u=u.nextSibling}if(!d||u)return!1}else m(n,c,t);if(o(s)){var f=!1;for(var b in s)if(!S(b)){f=!0,v(n,t);break}!f&&s.class&&qn(s.class)}}else e.data!==n.text&&(e.data=n.text);return!0}return function(e,n,t,a){if(!i(n)){var s,c=!1,u=[];if(i(e))c=!0,p(n,u);else{var h=o(e.nodeType);if(!h&&Sr(e,n))E(e,n,u,null,null,a);else{if(h){if(1===e.nodeType&&e.hasAttribute("data-server-rendered")&&(e.removeAttribute("data-server-rendered"),t=!0),l(t)&&R(e,n,u))return B(n,u,!0),e;s=e,e=new fe(d.tagName(s).toLowerCase(),{},[],void 0,s)}var f=e.elm,m=d.parentNode(f);if(p(n,u,f._leaveCb?null:m,d.nextSibling(f)),o(n.parent))for(var v=n.parent,y=b(n);v;){for(var g=0;g<r.destroy.length;++g)r.destroy[g](v);if(v.elm=n.elm,y){for(var k=0;k<r.create.length;++k)r.create[k](Er,v);var w=v.data.hook.insert;if(w.merged)for(var _=w.fns.slice(1),S=0;S<_.length;S++)_[S]()}else wr(v);v=v.parent}o(m)?C([e],0,0):o(e.tag)&&x(e)}}return B(n,u,c),n.elm}o(e)&&x(e)}}({nodeOps:xr,modules:[Ur,Mr,$r,Qr,ca,Z?{create:Na,activate:Na,remove:function(e,n){!0!==e.data.show?Oa(e,n):n()}}:{}].concat(Lr)});Y&&document.addEventListener("selectionchange",(function(){var e=document.activeElement;e&&e.vmodel&&Ma(e,"input")}));var La={inserted:function(e,n,t,r){"select"===t.tag?(r.elm&&!r.elm._vOptions?Je(t,"postpatch",(function(){La.componentUpdated(e,n,t)})):Pa(e,n,t.context),e._vOptions=[].map.call(e.options,Ua)):("textarea"===t.tag||kr(e.type))&&(e._vModifiers=n.modifiers,n.modifiers.lazy||(e.addEventListener("compositionstart",Ha),e.addEventListener("compositionend",ja),e.addEventListener("change",ja),Y&&(e.vmodel=!0)))},componentUpdated:function(e,n,t){if("select"===t.tag){Pa(e,n,t.context);var r=e._vOptions,a=e._vOptions=[].map.call(e.options,Ua);if(a.some((function(e,n){return!F(e,r[n])})))(e.multiple?n.value.some((function(e){return Fa(e,a)})):n.value!==n.oldValue&&Fa(n.value,a))&&Ma(e,"change")}}};function Pa(e,n,t){Da(e,n,t),(Q||ee)&&setTimeout((function(){Da(e,n,t)}),0)}function Da(e,n,t){var r=n.value,a=e.multiple;if(!a||Array.isArray(r)){for(var i,o,l=0,s=e.options.length;l<s;l++)if(o=e.options[l],a)i=U(r,Ua(o))>-1,o.selected!==i&&(o.selected=i);else if(F(Ua(o),r))return void(e.selectedIndex!==l&&(e.selectedIndex=l));a||(e.selectedIndex=-1)}}function Fa(e,n){return n.every((function(n){return!F(n,e)}))}function Ua(e){return"_value"in e?e._value:e.value}function Ha(e){e.target.composing=!0}function ja(e){e.target.composing&&(e.target.composing=!1,Ma(e.target,"input"))}function Ma(e,n){var t=document.createEvent("HTMLEvents");t.initEvent(n,!0,!0),e.dispatchEvent(t)}function Ka(e){return!e.componentInstance||e.data&&e.data.transition?e:Ka(e.componentInstance._vnode)}var qa={model:La,show:{bind:function(e,n,t){var r=n.value,a=(t=Ka(t)).data&&t.data.transition,i=e.__vOriginalDisplay="none"===e.style.display?"":e.style.display;r&&a?(t.data.show=!0,Ia(t,(function(){e.style.display=i}))):e.style.display=r?i:"none"},update:function(e,n,t){var r=n.value;!r!=!n.oldValue&&((t=Ka(t)).data&&t.data.transition?(t.data.show=!0,r?Ia(t,(function(){e.style.display=e.__vOriginalDisplay})):Oa(t,(function(){e.style.display="none"}))):e.style.display=r?e.__vOriginalDisplay:"none")},unbind:function(e,n,t,r,a){a||(e.style.display=e.__vOriginalDisplay)}}},Ga={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Wa(e){var n=e&&e.componentOptions;return n&&n.Ctor.options.abstract?Wa(Bn(n.children)):e}function Va(e){var n={},t=e.$options;for(var r in t.propsData)n[r]=e[r];var a=t._parentListeners;for(var r in a)n[S(r)]=a[r];return n}function Ja(e,n){if(/\d-keep-alive$/.test(n.tag))return e("keep-alive",{props:n.componentOptions.propsData})}var $a=function(e){return e.tag||bn(e)},Za=function(e){return"show"===e.name},Xa={name:"transition",props:Ga,abstract:!0,render:function(e){var n=this,t=this.$slots.default;if(t&&(t=t.filter($a)).length){0;var r=this.mode;0;var a=t[0];if(function(e){for(;e=e.parent;)if(e.data.transition)return!0}(this.$vnode))return a;var i=Wa(a);if(!i)return a;if(this._leaving)return Ja(e,a);var o="__transition-".concat(this._uid,"-");i.key=null==i.key?i.isComment?o+"comment":o+i.tag:s(i.key)?0===String(i.key).indexOf(o)?i.key:o+i.key:i.key;var l=(i.data||(i.data={})).transition=Va(this),c=this._vnode,d=Wa(c);if(i.data.directives&&i.data.directives.some(Za)&&(i.data.show=!0),d&&d.data&&!function(e,n){return n.key===e.key&&n.tag===e.tag}(i,d)&&!bn(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var u=d.data.transition=N({},l);if("out-in"===r)return this._leaving=!0,Je(u,"afterLeave",(function(){n._leaving=!1,n.$forceUpdate()})),Ja(e,a);if("in-out"===r){if(bn(i))return c;var p,h=function(){p()};Je(l,"afterEnter",h),Je(l,"enterCancelled",h),Je(u,"delayLeave",(function(e){p=e}))}}return a}}},Qa=N({tag:String,moveClass:String},Ga);function Ya(e){e.elm._moveCb&&e.elm._moveCb(),e.elm._enterCb&&e.elm._enterCb()}function ei(e){e.data.newPos=e.elm.getBoundingClientRect()}function ni(e){var n=e.data.pos,t=e.data.newPos,r=n.left-t.left,a=n.top-t.top;if(r||a){e.data.moved=!0;var i=e.elm.style;i.transform=i.WebkitTransform="translate(".concat(r,"px,").concat(a,"px)"),i.transitionDuration="0s"}}delete Qa.mode;var ti={Transition:Xa,TransitionGroup:{props:Qa,beforeMount:function(){var e=this,n=this._update;this._update=function(t,r){var a=Yn(e);e.__patch__(e._vnode,e.kept,!1,!0),e._vnode=e.kept,a(),n.call(e,t,r)}},render:function(e){for(var n=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),r=this.prevChildren=this.children,a=this.$slots.default||[],i=this.children=[],o=Va(this),l=0;l<a.length;l++){if((d=a[l]).tag)if(null!=d.key&&0!==String(d.key).indexOf("__vlist"))i.push(d),t[d.key]=d,(d.data||(d.data={})).transition=o;else;}if(r){var s=[],c=[];for(l=0;l<r.length;l++){var d;(d=r[l]).data.transition=o,d.data.pos=d.elm.getBoundingClientRect(),t[d.key]?s.push(d):c.push(d)}this.kept=e(n,null,s),this.removed=c}return e(n,null,i)},updated:function(){var e=this.prevChildren,n=this.moveClass||(this.name||"v")+"-move";e.length&&this.hasMove(e[0].elm,n)&&(e.forEach(Ya),e.forEach(ei),e.forEach(ni),this._reflow=document.body.offsetHeight,e.forEach((function(e){if(e.data.moved){var t=e.elm,r=t.style;Ca(t,n),r.transform=r.WebkitTransform=r.transitionDuration="",t.addEventListener(va,t._moveCb=function e(r){r&&r.target!==t||r&&!/transform$/.test(r.propertyName)||(t.removeEventListener(va,e),t._moveCb=null,wa(t,n))})}})))},methods:{hasMove:function(e,n){if(!ma)return!1;if(this._hasMove)return this._hasMove;var t=e.cloneNode();e._transitionClasses&&e._transitionClasses.forEach((function(e){pa(t,e)})),ua(t,n),t.style.display="none",this.$el.appendChild(t);var r=Ba(t);return this.$el.removeChild(t),this._hasMove=r.hasTransform}}}};function ri(e,n){for(var t in n)e[t]=n[t];return e}Jt.config.mustUseProp=function(e,n,t){return"value"===t&&rr(e)&&"button"!==n||"selected"===t&&"option"===e||"checked"===t&&"input"===e||"muted"===t&&"video"===e},Jt.config.isReservedTag=yr,Jt.config.isReservedAttr=tr,Jt.config.getTagNamespace=function(e){return vr(e)?"svg":"math"===e?"math":void 0},Jt.config.isUnknownElement=function(e){if(!Z)return!0;if(yr(e))return!1;if(e=e.toLowerCase(),null!=gr[e])return gr[e];var n=document.createElement(e);return e.indexOf("-")>-1?gr[e]=n.constructor===window.HTMLUnknownElement||n.constructor===window.HTMLElement:gr[e]=/HTMLUnknownElement/.test(n.toString())},N(Jt.options.directives,qa),N(Jt.options.components,ti),Jt.prototype.__patch__=Z?za:L,Jt.prototype.$mount=function(e,n){return function(e,n,t){var r;e.$el=n,e.$options.render||(e.$options.render=me),tt(e,"beforeMount"),r=function(){e._update(e._render(),t)},new Vn(e,r,L,{before:function(){e._isMounted&&!e._isDestroyed&&tt(e,"beforeUpdate")}},!0),t=!1;var a=e._preWatchers;if(a)for(var i=0;i<a.length;i++)a[i].run();return null==e.$vnode&&(e._isMounted=!0,tt(e,"mounted")),e}(this,e=e&&Z?function(e){if("string"==typeof e){var n=document.querySelector(e);return n||document.createElement("div")}return e}(e):void 0,n)},Z&&setTimeout((function(){q.devtools&&se&&se.emit("init",Jt)}),0);var ai=/[!'()*]/g,ii=function(e){return"%"+e.charCodeAt(0).toString(16)},oi=/%2C/g,li=function(e){return encodeURIComponent(e).replace(ai,ii).replace(oi,",")};function si(e){try{return decodeURIComponent(e)}catch(e){0}return e}var ci=function(e){return null==e||"object"==typeof e?e:String(e)};function di(e){var n={};return(e=e.trim().replace(/^(\?|#|&)/,""))?(e.split("&").forEach((function(e){var t=e.replace(/\+/g," ").split("="),r=si(t.shift()),a=t.length>0?si(t.join("=")):null;void 0===n[r]?n[r]=a:Array.isArray(n[r])?n[r].push(a):n[r]=[n[r],a]})),n):n}function ui(e){var n=e?Object.keys(e).map((function(n){var t=e[n];if(void 0===t)return"";if(null===t)return li(n);if(Array.isArray(t)){var r=[];return t.forEach((function(e){void 0!==e&&(null===e?r.push(li(n)):r.push(li(n)+"="+li(e)))})),r.join("&")}return li(n)+"="+li(t)})).filter((function(e){return e.length>0})).join("&"):null;return n?"?"+n:""}var pi=/\/?$/;function hi(e,n,t,r){var a=r&&r.options.stringifyQuery,i=n.query||{};try{i=fi(i)}catch(e){}var o={name:n.name||e&&e.name,meta:e&&e.meta||{},path:n.path||"/",hash:n.hash||"",query:i,params:n.params||{},fullPath:vi(n,a),matched:e?bi(e):[]};return t&&(o.redirectedFrom=vi(t,a)),Object.freeze(o)}function fi(e){if(Array.isArray(e))return e.map(fi);if(e&&"object"==typeof e){var n={};for(var t in e)n[t]=fi(e[t]);return n}return e}var mi=hi(null,{path:"/"});function bi(e){for(var n=[];e;)n.unshift(e),e=e.parent;return n}function vi(e,n){var t=e.path,r=e.query;void 0===r&&(r={});var a=e.hash;return void 0===a&&(a=""),(t||"/")+(n||ui)(r)+a}function yi(e,n,t){return n===mi?e===n:!!n&&(e.path&&n.path?e.path.replace(pi,"")===n.path.replace(pi,"")&&(t||e.hash===n.hash&&gi(e.query,n.query)):!(!e.name||!n.name)&&(e.name===n.name&&(t||e.hash===n.hash&&gi(e.query,n.query)&&gi(e.params,n.params))))}function gi(e,n){if(void 0===e&&(e={}),void 0===n&&(n={}),!e||!n)return e===n;var t=Object.keys(e).sort(),r=Object.keys(n).sort();return t.length===r.length&&t.every((function(t,a){var i=e[t];if(r[a]!==t)return!1;var o=n[t];return null==i||null==o?i===o:"object"==typeof i&&"object"==typeof o?gi(i,o):String(i)===String(o)}))}function ki(e){for(var n=0;n<e.matched.length;n++){var t=e.matched[n];for(var r in t.instances){var a=t.instances[r],i=t.enteredCbs[r];if(a&&i){delete t.enteredCbs[r];for(var o=0;o<i.length;o++)a._isBeingDestroyed||i[o](a)}}}}var xi={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(e,n){var t=n.props,r=n.children,a=n.parent,i=n.data;i.routerView=!0;for(var o=a.$createElement,l=t.name,s=a.$route,c=a._routerViewCache||(a._routerViewCache={}),d=0,u=!1;a&&a._routerRoot!==a;){var p=a.$vnode?a.$vnode.data:{};p.routerView&&d++,p.keepAlive&&a._directInactive&&a._inactive&&(u=!0),a=a.$parent}if(i.routerViewDepth=d,u){var h=c[l],f=h&&h.component;return f?(h.configProps&&Ci(f,i,h.route,h.configProps),o(f,i,r)):o()}var m=s.matched[d],b=m&&m.components[l];if(!m||!b)return c[l]=null,o();c[l]={component:b},i.registerRouteInstance=function(e,n){var t=m.instances[l];(n&&t!==e||!n&&t===e)&&(m.instances[l]=n)},(i.hook||(i.hook={})).prepatch=function(e,n){m.instances[l]=n.componentInstance},i.hook.init=function(e){e.data.keepAlive&&e.componentInstance&&e.componentInstance!==m.instances[l]&&(m.instances[l]=e.componentInstance),ki(s)};var v=m.props&&m.props[l];return v&&(ri(c[l],{route:s,configProps:v}),Ci(b,i,s,v)),o(b,i,r)}};function Ci(e,n,t,r){var a=n.props=function(e,n){switch(typeof n){case"undefined":return;case"object":return n;case"function":return n(e);case"boolean":return n?e.params:void 0;default:0}}(t,r);if(a){a=n.props=ri({},a);var i=n.attrs=n.attrs||{};for(var o in a)e.props&&o in e.props||(i[o]=a[o],delete a[o])}}function wi(e,n,t){var r=e.charAt(0);if("/"===r)return e;if("?"===r||"#"===r)return n+e;var a=n.split("/");t&&a[a.length-1]||a.pop();for(var i=e.replace(/^\//,"").split("/"),o=0;o<i.length;o++){var l=i[o];".."===l?a.pop():"."!==l&&a.push(l)}return""!==a[0]&&a.unshift(""),a.join("/")}function _i(e){return e.replace(/\/(?:\s*\/)+/g,"/")}var Ei=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)},Bi=Hi,Si=Ti,Ri=function(e,n){return zi(Ti(e,n),n)},Ii=zi,Oi=Ui,Ai=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function Ti(e,n){for(var t,r=[],a=0,i=0,o="",l=n&&n.delimiter||"/";null!=(t=Ai.exec(e));){var s=t[0],c=t[1],d=t.index;if(o+=e.slice(i,d),i=d+s.length,c)o+=c[1];else{var u=e[i],p=t[2],h=t[3],f=t[4],m=t[5],b=t[6],v=t[7];o&&(r.push(o),o="");var y=null!=p&&null!=u&&u!==p,g="+"===b||"*"===b,k="?"===b||"*"===b,x=t[2]||l,C=f||m;r.push({name:h||a++,prefix:p||"",delimiter:x,optional:k,repeat:g,partial:y,asterisk:!!v,pattern:C?Pi(C):v?".*":"[^"+Li(x)+"]+?"})}}return i<e.length&&(o+=e.substr(i)),o&&r.push(o),r}function Ni(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function zi(e,n){for(var t=new Array(e.length),r=0;r<e.length;r++)"object"==typeof e[r]&&(t[r]=new RegExp("^(?:"+e[r].pattern+")$",Fi(n)));return function(n,r){for(var a="",i=n||{},o=(r||{}).pretty?Ni:encodeURIComponent,l=0;l<e.length;l++){var s=e[l];if("string"!=typeof s){var c,d=i[s.name];if(null==d){if(s.optional){s.partial&&(a+=s.prefix);continue}throw new TypeError('Expected "'+s.name+'" to be defined')}if(Ei(d)){if(!s.repeat)throw new TypeError('Expected "'+s.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(s.optional)continue;throw new TypeError('Expected "'+s.name+'" to not be empty')}for(var u=0;u<d.length;u++){if(c=o(d[u]),!t[l].test(c))throw new TypeError('Expected all "'+s.name+'" to match "'+s.pattern+'", but received `'+JSON.stringify(c)+"`");a+=(0===u?s.prefix:s.delimiter)+c}}else{if(c=s.asterisk?encodeURI(d).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):o(d),!t[l].test(c))throw new TypeError('Expected "'+s.name+'" to match "'+s.pattern+'", but received "'+c+'"');a+=s.prefix+c}}else a+=s}return a}}function Li(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function Pi(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function Di(e,n){return e.keys=n,e}function Fi(e){return e&&e.sensitive?"":"i"}function Ui(e,n,t){Ei(n)||(t=n||t,n=[]);for(var r=(t=t||{}).strict,a=!1!==t.end,i="",o=0;o<e.length;o++){var l=e[o];if("string"==typeof l)i+=Li(l);else{var s=Li(l.prefix),c="(?:"+l.pattern+")";n.push(l),l.repeat&&(c+="(?:"+s+c+")*"),i+=c=l.optional?l.partial?s+"("+c+")?":"(?:"+s+"("+c+"))?":s+"("+c+")"}}var d=Li(t.delimiter||"/"),u=i.slice(-d.length)===d;return r||(i=(u?i.slice(0,-d.length):i)+"(?:"+d+"(?=$))?"),i+=a?"$":r&&u?"":"(?="+d+"|$)",Di(new RegExp("^"+i,Fi(t)),n)}function Hi(e,n,t){return Ei(n)||(t=n||t,n=[]),t=t||{},e instanceof RegExp?function(e,n){var t=e.source.match(/\((?!\?)/g);if(t)for(var r=0;r<t.length;r++)n.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return Di(e,n)}(e,n):Ei(e)?function(e,n,t){for(var r=[],a=0;a<e.length;a++)r.push(Hi(e[a],n,t).source);return Di(new RegExp("(?:"+r.join("|")+")",Fi(t)),n)}(e,n,t):function(e,n,t){return Ui(Ti(e,t),n,t)}(e,n,t)}Bi.parse=Si,Bi.compile=Ri,Bi.tokensToFunction=Ii,Bi.tokensToRegExp=Oi;var ji=Object.create(null);function Mi(e,n,t){n=n||{};try{var r=ji[e]||(ji[e]=Bi.compile(e));return"string"==typeof n.pathMatch&&(n[0]=n.pathMatch),r(n,{pretty:!0})}catch(e){return""}finally{delete n[0]}}function Ki(e,n,t,r){var a="string"==typeof e?{path:e}:e;if(a._normalized)return a;if(a.name){var i=(a=ri({},e)).params;return i&&"object"==typeof i&&(a.params=ri({},i)),a}if(!a.path&&a.params&&n){(a=ri({},a))._normalized=!0;var o=ri(ri({},n.params),a.params);if(n.name)a.name=n.name,a.params=o;else if(n.matched.length){var l=n.matched[n.matched.length-1].path;a.path=Mi(l,o,n.path)}else 0;return a}var s=function(e){var n="",t="",r=e.indexOf("#");r>=0&&(n=e.slice(r),e=e.slice(0,r));var a=e.indexOf("?");return a>=0&&(t=e.slice(a+1),e=e.slice(0,a)),{path:e,query:t,hash:n}}(a.path||""),c=n&&n.path||"/",d=s.path?wi(s.path,c,t||a.append):c,u=function(e,n,t){void 0===n&&(n={});var r,a=t||di;try{r=a(e||"")}catch(e){r={}}for(var i in n){var o=n[i];r[i]=Array.isArray(o)?o.map(ci):ci(o)}return r}(s.query,a.query,r&&r.options.parseQuery),p=a.hash||s.hash;return p&&"#"!==p.charAt(0)&&(p="#"+p),{_normalized:!0,path:d,query:u,hash:p}}var qi,Gi=function(){},Wi={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(e){var n=this,t=this.$router,r=this.$route,a=t.resolve(this.to,r,this.append),i=a.location,o=a.route,l=a.href,s={},c=t.options.linkActiveClass,d=t.options.linkExactActiveClass,u=null==c?"router-link-active":c,p=null==d?"router-link-exact-active":d,h=null==this.activeClass?u:this.activeClass,f=null==this.exactActiveClass?p:this.exactActiveClass,m=o.redirectedFrom?hi(null,Ki(o.redirectedFrom),null,t):o;s[f]=yi(r,m,this.exactPath),s[h]=this.exact||this.exactPath?s[f]:function(e,n){return 0===e.path.replace(pi,"/").indexOf(n.path.replace(pi,"/"))&&(!n.hash||e.hash===n.hash)&&function(e,n){for(var t in n)if(!(t in e))return!1;return!0}(e.query,n.query)}(r,m);var b=s[f]?this.ariaCurrentValue:null,v=function(e){Vi(e)&&(n.replace?t.replace(i,Gi):t.push(i,Gi))},y={click:Vi};Array.isArray(this.event)?this.event.forEach((function(e){y[e]=v})):y[this.event]=v;var g={class:s},k=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:l,route:o,navigate:v,isActive:s[h],isExactActive:s[f]});if(k){if(1===k.length)return k[0];if(k.length>1||!k.length)return 0===k.length?e():e("span",{},k)}if("a"===this.tag)g.on=y,g.attrs={href:l,"aria-current":b};else{var x=function e(n){var t;if(n)for(var r=0;r<n.length;r++){if("a"===(t=n[r]).tag)return t;if(t.children&&(t=e(t.children)))return t}}(this.$slots.default);if(x){x.isStatic=!1;var C=x.data=ri({},x.data);for(var w in C.on=C.on||{},C.on){var _=C.on[w];w in y&&(C.on[w]=Array.isArray(_)?_:[_])}for(var E in y)E in C.on?C.on[E].push(y[E]):C.on[E]=v;var B=x.data.attrs=ri({},x.data.attrs);B.href=l,B["aria-current"]=b}else g.on=y}return e(this.tag,g,this.$slots.default)}};function Vi(e){if(!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||e.defaultPrevented||void 0!==e.button&&0!==e.button)){if(e.currentTarget&&e.currentTarget.getAttribute){var n=e.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(n))return}return e.preventDefault&&e.preventDefault(),!0}}var Ji="undefined"!=typeof window;function $i(e,n,t,r,a){var i=n||[],o=t||Object.create(null),l=r||Object.create(null);e.forEach((function(e){!function e(n,t,r,a,i,o){var l=a.path,s=a.name;0;var c=a.pathToRegexpOptions||{},d=function(e,n,t){t||(e=e.replace(/\/$/,""));if("/"===e[0])return e;if(null==n)return e;return _i(n.path+"/"+e)}(l,i,c.strict);"boolean"==typeof a.caseSensitive&&(c.sensitive=a.caseSensitive);var u={path:d,regex:Zi(d,c),components:a.components||{default:a.component},alias:a.alias?"string"==typeof a.alias?[a.alias]:a.alias:[],instances:{},enteredCbs:{},name:s,parent:i,matchAs:o,redirect:a.redirect,beforeEnter:a.beforeEnter,meta:a.meta||{},props:null==a.props?{}:a.components?a.props:{default:a.props}};a.children&&a.children.forEach((function(a){var i=o?_i(o+"/"+a.path):void 0;e(n,t,r,a,u,i)}));t[u.path]||(n.push(u.path),t[u.path]=u);if(void 0!==a.alias)for(var p=Array.isArray(a.alias)?a.alias:[a.alias],h=0;h<p.length;++h){0;var f={path:p[h],children:a.children};e(n,t,r,f,i,u.path||"/")}s&&(r[s]||(r[s]=u))}(i,o,l,e,a)}));for(var s=0,c=i.length;s<c;s++)"*"===i[s]&&(i.push(i.splice(s,1)[0]),c--,s--);return{pathList:i,pathMap:o,nameMap:l}}function Zi(e,n){return Bi(e,[],n)}function Xi(e,n){var t=$i(e),r=t.pathList,a=t.pathMap,i=t.nameMap;function o(e,t,o){var l=Ki(e,t,!1,n),c=l.name;if(c){var d=i[c];if(!d)return s(null,l);var u=d.regex.keys.filter((function(e){return!e.optional})).map((function(e){return e.name}));if("object"!=typeof l.params&&(l.params={}),t&&"object"==typeof t.params)for(var p in t.params)!(p in l.params)&&u.indexOf(p)>-1&&(l.params[p]=t.params[p]);return l.path=Mi(d.path,l.params),s(d,l,o)}if(l.path){l.params={};for(var h=0;h<r.length;h++){var f=r[h],m=a[f];if(Qi(m.regex,l.path,l.params))return s(m,l,o)}}return s(null,l)}function l(e,t){var r=e.redirect,a="function"==typeof r?r(hi(e,t,null,n)):r;if("string"==typeof a&&(a={path:a}),!a||"object"!=typeof a)return s(null,t);var l=a,c=l.name,d=l.path,u=t.query,p=t.hash,h=t.params;if(u=l.hasOwnProperty("query")?l.query:u,p=l.hasOwnProperty("hash")?l.hash:p,h=l.hasOwnProperty("params")?l.params:h,c){i[c];return o({_normalized:!0,name:c,query:u,hash:p,params:h},void 0,t)}if(d){var f=function(e,n){return wi(e,n.parent?n.parent.path:"/",!0)}(d,e);return o({_normalized:!0,path:Mi(f,h),query:u,hash:p},void 0,t)}return s(null,t)}function s(e,t,r){return e&&e.redirect?l(e,r||t):e&&e.matchAs?function(e,n,t){var r=o({_normalized:!0,path:Mi(t,n.params)});if(r){var a=r.matched,i=a[a.length-1];return n.params=r.params,s(i,n)}return s(null,n)}(0,t,e.matchAs):hi(e,t,r,n)}return{match:o,addRoute:function(e,n){var t="object"!=typeof e?i[e]:void 0;$i([n||e],r,a,i,t),t&&t.alias.length&&$i(t.alias.map((function(e){return{path:e,children:[n]}})),r,a,i,t)},getRoutes:function(){return r.map((function(e){return a[e]}))},addRoutes:function(e){$i(e,r,a,i)}}}function Qi(e,n,t){var r=n.match(e);if(!r)return!1;if(!t)return!0;for(var a=1,i=r.length;a<i;++a){var o=e.keys[a-1];o&&(t[o.name||"pathMatch"]="string"==typeof r[a]?si(r[a]):r[a])}return!0}var Yi=Ji&&window.performance&&window.performance.now?window.performance:Date;function eo(){return Yi.now().toFixed(3)}var no=eo();function to(){return no}function ro(e){return no=e}var ao=Object.create(null);function io(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var e=window.location.protocol+"//"+window.location.host,n=window.location.href.replace(e,""),t=ri({},window.history.state);return t.key=to(),window.history.replaceState(t,"",n),window.addEventListener("popstate",so),function(){window.removeEventListener("popstate",so)}}function oo(e,n,t,r){if(e.app){var a=e.options.scrollBehavior;a&&e.app.$nextTick((function(){var i=function(){var e=to();if(e)return ao[e]}(),o=a.call(e,n,t,r?i:null);o&&("function"==typeof o.then?o.then((function(e){fo(e,i)})).catch((function(e){0})):fo(o,i))}))}}function lo(){var e=to();e&&(ao[e]={x:window.pageXOffset,y:window.pageYOffset})}function so(e){lo(),e.state&&e.state.key&&ro(e.state.key)}function co(e){return po(e.x)||po(e.y)}function uo(e){return{x:po(e.x)?e.x:window.pageXOffset,y:po(e.y)?e.y:window.pageYOffset}}function po(e){return"number"==typeof e}var ho=/^#\d/;function fo(e,n){var t,r="object"==typeof e;if(r&&"string"==typeof e.selector){var a=ho.test(e.selector)?document.getElementById(e.selector.slice(1)):document.querySelector(e.selector);if(a){var i=e.offset&&"object"==typeof e.offset?e.offset:{};n=function(e,n){var t=document.documentElement.getBoundingClientRect(),r=e.getBoundingClientRect();return{x:r.left-t.left-n.x,y:r.top-t.top-n.y}}(a,i={x:po((t=i).x)?t.x:0,y:po(t.y)?t.y:0})}else co(e)&&(n=uo(e))}else r&&co(e)&&(n=uo(e));n&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:n.x,top:n.y,behavior:e.behavior}):window.scrollTo(n.x,n.y))}var mo,bo=Ji&&((-1===(mo=window.navigator.userAgent).indexOf("Android 2.")&&-1===mo.indexOf("Android 4.0")||-1===mo.indexOf("Mobile Safari")||-1!==mo.indexOf("Chrome")||-1!==mo.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function vo(e,n){lo();var t=window.history;try{if(n){var r=ri({},t.state);r.key=to(),t.replaceState(r,"",e)}else t.pushState({key:ro(eo())},"",e)}catch(t){window.location[n?"replace":"assign"](e)}}function yo(e){vo(e,!0)}var go={redirected:2,aborted:4,cancelled:8,duplicated:16};function ko(e,n){return Co(e,n,go.redirected,'Redirected when going from "'+e.fullPath+'" to "'+function(e){if("string"==typeof e)return e;if("path"in e)return e.path;var n={};return wo.forEach((function(t){t in e&&(n[t]=e[t])})),JSON.stringify(n,null,2)}(n)+'" via a navigation guard.')}function xo(e,n){return Co(e,n,go.cancelled,'Navigation cancelled from "'+e.fullPath+'" to "'+n.fullPath+'" with a new navigation.')}function Co(e,n,t,r){var a=new Error(r);return a._isRouter=!0,a.from=e,a.to=n,a.type=t,a}var wo=["params","query","hash"];function _o(e){return Object.prototype.toString.call(e).indexOf("Error")>-1}function Eo(e,n){return _o(e)&&e._isRouter&&(null==n||e.type===n)}function Bo(e,n,t){var r=function(a){a>=e.length?t():e[a]?n(e[a],(function(){r(a+1)})):r(a+1)};r(0)}function So(e){return function(n,t,r){var a=!1,i=0,o=null;Ro(e,(function(e,n,t,l){if("function"==typeof e&&void 0===e.cid){a=!0,i++;var s,c=Ao((function(n){var a;((a=n).__esModule||Oo&&"Module"===a[Symbol.toStringTag])&&(n=n.default),e.resolved="function"==typeof n?n:qi.extend(n),t.components[l]=n,--i<=0&&r()})),d=Ao((function(e){var n="Failed to resolve async component "+l+": "+e;o||(o=_o(e)?e:new Error(n),r(o))}));try{s=e(c,d)}catch(e){d(e)}if(s)if("function"==typeof s.then)s.then(c,d);else{var u=s.component;u&&"function"==typeof u.then&&u.then(c,d)}}})),a||r()}}function Ro(e,n){return Io(e.map((function(e){return Object.keys(e.components).map((function(t){return n(e.components[t],e.instances[t],e,t)}))})))}function Io(e){return Array.prototype.concat.apply([],e)}var Oo="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Ao(e){var n=!1;return function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];if(!n)return n=!0,e.apply(this,t)}}var To=function(e,n){this.router=e,this.base=function(e){if(!e)if(Ji){var n=document.querySelector("base");e=(e=n&&n.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else e="/";"/"!==e.charAt(0)&&(e="/"+e);return e.replace(/\/$/,"")}(n),this.current=mi,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function No(e,n,t,r){var a=Ro(e,(function(e,r,a,i){var o=function(e,n){"function"!=typeof e&&(e=qi.extend(e));return e.options[n]}(e,n);if(o)return Array.isArray(o)?o.map((function(e){return t(e,r,a,i)})):t(o,r,a,i)}));return Io(r?a.reverse():a)}function zo(e,n){if(n)return function(){return e.apply(n,arguments)}}To.prototype.listen=function(e){this.cb=e},To.prototype.onReady=function(e,n){this.ready?e():(this.readyCbs.push(e),n&&this.readyErrorCbs.push(n))},To.prototype.onError=function(e){this.errorCbs.push(e)},To.prototype.transitionTo=function(e,n,t){var r,a=this;try{r=this.router.match(e,this.current)}catch(e){throw this.errorCbs.forEach((function(n){n(e)})),e}var i=this.current;this.confirmTransition(r,(function(){a.updateRoute(r),n&&n(r),a.ensureURL(),a.router.afterHooks.forEach((function(e){e&&e(r,i)})),a.ready||(a.ready=!0,a.readyCbs.forEach((function(e){e(r)})))}),(function(e){t&&t(e),e&&!a.ready&&(Eo(e,go.redirected)&&i===mi||(a.ready=!0,a.readyErrorCbs.forEach((function(n){n(e)}))))}))},To.prototype.confirmTransition=function(e,n,t){var r=this,a=this.current;this.pending=e;var i,o,l=function(e){!Eo(e)&&_o(e)&&(r.errorCbs.length?r.errorCbs.forEach((function(n){n(e)})):console.error(e)),t&&t(e)},s=e.matched.length-1,c=a.matched.length-1;if(yi(e,a)&&s===c&&e.matched[s]===a.matched[c])return this.ensureURL(),e.hash&&oo(this.router,a,e,!1),l(((o=Co(i=a,e,go.duplicated,'Avoided redundant navigation to current location: "'+i.fullPath+'".')).name="NavigationDuplicated",o));var d=function(e,n){var t,r=Math.max(e.length,n.length);for(t=0;t<r&&e[t]===n[t];t++);return{updated:n.slice(0,t),activated:n.slice(t),deactivated:e.slice(t)}}(this.current.matched,e.matched),u=d.updated,p=d.deactivated,h=d.activated,f=[].concat(function(e){return No(e,"beforeRouteLeave",zo,!0)}(p),this.router.beforeHooks,function(e){return No(e,"beforeRouteUpdate",zo)}(u),h.map((function(e){return e.beforeEnter})),So(h)),m=function(n,t){if(r.pending!==e)return l(xo(a,e));try{n(e,a,(function(n){!1===n?(r.ensureURL(!0),l(function(e,n){return Co(e,n,go.aborted,'Navigation aborted from "'+e.fullPath+'" to "'+n.fullPath+'" via a navigation guard.')}(a,e))):_o(n)?(r.ensureURL(!0),l(n)):"string"==typeof n||"object"==typeof n&&("string"==typeof n.path||"string"==typeof n.name)?(l(ko(a,e)),"object"==typeof n&&n.replace?r.replace(n):r.push(n)):t(n)}))}catch(e){l(e)}};Bo(f,m,(function(){Bo(function(e){return No(e,"beforeRouteEnter",(function(e,n,t,r){return function(e,n,t){return function(r,a,i){return e(r,a,(function(e){"function"==typeof e&&(n.enteredCbs[t]||(n.enteredCbs[t]=[]),n.enteredCbs[t].push(e)),i(e)}))}}(e,t,r)}))}(h).concat(r.router.resolveHooks),m,(function(){if(r.pending!==e)return l(xo(a,e));r.pending=null,n(e),r.router.app&&r.router.app.$nextTick((function(){ki(e)}))}))}))},To.prototype.updateRoute=function(e){this.current=e,this.cb&&this.cb(e)},To.prototype.setupListeners=function(){},To.prototype.teardown=function(){this.listeners.forEach((function(e){e()})),this.listeners=[],this.current=mi,this.pending=null};var Lo=function(e){function n(n,t){e.call(this,n,t),this._startLocation=Po(this.base)}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router,t=n.options.scrollBehavior,r=bo&&t;r&&this.listeners.push(io());var a=function(){var t=e.current,a=Po(e.base);e.current===mi&&a===e._startLocation||e.transitionTo(a,(function(e){r&&oo(n,e,t,!0)}))};window.addEventListener("popstate",a),this.listeners.push((function(){window.removeEventListener("popstate",a)}))}},n.prototype.go=function(e){window.history.go(e)},n.prototype.push=function(e,n,t){var r=this,a=this.current;this.transitionTo(e,(function(e){vo(_i(r.base+e.fullPath)),oo(r.router,e,a,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var r=this,a=this.current;this.transitionTo(e,(function(e){yo(_i(r.base+e.fullPath)),oo(r.router,e,a,!1),n&&n(e)}),t)},n.prototype.ensureURL=function(e){if(Po(this.base)!==this.current.fullPath){var n=_i(this.base+this.current.fullPath);e?vo(n):yo(n)}},n.prototype.getCurrentLocation=function(){return Po(this.base)},n}(To);function Po(e){var n=window.location.pathname,t=n.toLowerCase(),r=e.toLowerCase();return!e||t!==r&&0!==t.indexOf(_i(r+"/"))||(n=n.slice(e.length)),(n||"/")+window.location.search+window.location.hash}var Do=function(e){function n(n,t,r){e.call(this,n,t),r&&function(e){var n=Po(e);if(!/^\/#/.test(n))return window.location.replace(_i(e+"/#"+n)),!0}(this.base)||Fo()}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router.options.scrollBehavior,t=bo&&n;t&&this.listeners.push(io());var r=function(){var n=e.current;Fo()&&e.transitionTo(Uo(),(function(r){t&&oo(e.router,r,n,!0),bo||Mo(r.fullPath)}))},a=bo?"popstate":"hashchange";window.addEventListener(a,r),this.listeners.push((function(){window.removeEventListener(a,r)}))}},n.prototype.push=function(e,n,t){var r=this,a=this.current;this.transitionTo(e,(function(e){jo(e.fullPath),oo(r.router,e,a,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var r=this,a=this.current;this.transitionTo(e,(function(e){Mo(e.fullPath),oo(r.router,e,a,!1),n&&n(e)}),t)},n.prototype.go=function(e){window.history.go(e)},n.prototype.ensureURL=function(e){var n=this.current.fullPath;Uo()!==n&&(e?jo(n):Mo(n))},n.prototype.getCurrentLocation=function(){return Uo()},n}(To);function Fo(){var e=Uo();return"/"===e.charAt(0)||(Mo("/"+e),!1)}function Uo(){var e=window.location.href,n=e.indexOf("#");return n<0?"":e=e.slice(n+1)}function Ho(e){var n=window.location.href,t=n.indexOf("#");return(t>=0?n.slice(0,t):n)+"#"+e}function jo(e){bo?vo(Ho(e)):window.location.hash=e}function Mo(e){bo?yo(Ho(e)):window.location.replace(Ho(e))}var Ko=function(e){function n(n,t){e.call(this,n,t),this.stack=[],this.index=-1}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.push=function(e,n,t){var r=this;this.transitionTo(e,(function(e){r.stack=r.stack.slice(0,r.index+1).concat(e),r.index++,n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var r=this;this.transitionTo(e,(function(e){r.stack=r.stack.slice(0,r.index).concat(e),n&&n(e)}),t)},n.prototype.go=function(e){var n=this,t=this.index+e;if(!(t<0||t>=this.stack.length)){var r=this.stack[t];this.confirmTransition(r,(function(){var e=n.current;n.index=t,n.updateRoute(r),n.router.afterHooks.forEach((function(n){n&&n(r,e)}))}),(function(e){Eo(e,go.duplicated)&&(n.index=t)}))}},n.prototype.getCurrentLocation=function(){var e=this.stack[this.stack.length-1];return e?e.fullPath:"/"},n.prototype.ensureURL=function(){},n}(To),qo=function(e){void 0===e&&(e={}),this.app=null,this.apps=[],this.options=e,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Xi(e.routes||[],this);var n=e.mode||"hash";switch(this.fallback="history"===n&&!bo&&!1!==e.fallback,this.fallback&&(n="hash"),Ji||(n="abstract"),this.mode=n,n){case"history":this.history=new Lo(this,e.base);break;case"hash":this.history=new Do(this,e.base,this.fallback);break;case"abstract":this.history=new Ko(this,e.base);break;default:0}},Go={currentRoute:{configurable:!0}};qo.prototype.match=function(e,n,t){return this.matcher.match(e,n,t)},Go.currentRoute.get=function(){return this.history&&this.history.current},qo.prototype.init=function(e){var n=this;if(this.apps.push(e),e.$once("hook:destroyed",(function(){var t=n.apps.indexOf(e);t>-1&&n.apps.splice(t,1),n.app===e&&(n.app=n.apps[0]||null),n.app||n.history.teardown()})),!this.app){this.app=e;var t=this.history;if(t instanceof Lo||t instanceof Do){var r=function(e){t.setupListeners(),function(e){var r=t.current,a=n.options.scrollBehavior;bo&&a&&"fullPath"in e&&oo(n,e,r,!1)}(e)};t.transitionTo(t.getCurrentLocation(),r,r)}t.listen((function(e){n.apps.forEach((function(n){n._route=e}))}))}},qo.prototype.beforeEach=function(e){return Vo(this.beforeHooks,e)},qo.prototype.beforeResolve=function(e){return Vo(this.resolveHooks,e)},qo.prototype.afterEach=function(e){return Vo(this.afterHooks,e)},qo.prototype.onReady=function(e,n){this.history.onReady(e,n)},qo.prototype.onError=function(e){this.history.onError(e)},qo.prototype.push=function(e,n,t){var r=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){r.history.push(e,n,t)}));this.history.push(e,n,t)},qo.prototype.replace=function(e,n,t){var r=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){r.history.replace(e,n,t)}));this.history.replace(e,n,t)},qo.prototype.go=function(e){this.history.go(e)},qo.prototype.back=function(){this.go(-1)},qo.prototype.forward=function(){this.go(1)},qo.prototype.getMatchedComponents=function(e){var n=e?e.matched?e:this.resolve(e).route:this.currentRoute;return n?[].concat.apply([],n.matched.map((function(e){return Object.keys(e.components).map((function(n){return e.components[n]}))}))):[]},qo.prototype.resolve=function(e,n,t){var r=Ki(e,n=n||this.history.current,t,this),a=this.match(r,n),i=a.redirectedFrom||a.fullPath;return{location:r,route:a,href:function(e,n,t){var r="hash"===t?"#"+n:n;return e?_i(e+"/"+r):r}(this.history.base,i,this.mode),normalizedTo:r,resolved:a}},qo.prototype.getRoutes=function(){return this.matcher.getRoutes()},qo.prototype.addRoute=function(e,n){this.matcher.addRoute(e,n),this.history.current!==mi&&this.history.transitionTo(this.history.getCurrentLocation())},qo.prototype.addRoutes=function(e){this.matcher.addRoutes(e),this.history.current!==mi&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(qo.prototype,Go);var Wo=qo;function Vo(e,n){return e.push(n),function(){var t=e.indexOf(n);t>-1&&e.splice(t,1)}}qo.install=function e(n){if(!e.installed||qi!==n){e.installed=!0,qi=n;var t=function(e){return void 0!==e},r=function(e,n){var r=e.$options._parentVnode;t(r)&&t(r=r.data)&&t(r=r.registerRouteInstance)&&r(e,n)};n.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),n.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,r(this,this)},destroyed:function(){r(this)}}),Object.defineProperty(n.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(n.prototype,"$route",{get:function(){return this._routerRoot._route}}),n.component("RouterView",xi),n.component("RouterLink",Wi);var a=n.config.optionMergeStrategies;a.beforeRouteEnter=a.beforeRouteLeave=a.beforeRouteUpdate=a.created}},qo.version="3.6.5",qo.isNavigationFailure=Eo,qo.NavigationFailureType=go,qo.START_LOCATION=mi,Ji&&window.Vue&&window.Vue.use(qo);t(108);t(105),t(16);var Jo={NotFound:()=>Promise.all([t.e(0),t.e(6)]).then(t.bind(null,343)),Layout:()=>Promise.all([t.e(0),t.e(2)]).then(t.bind(null,341))},$o={"v-5571592e":()=>t.e(8).then(t.bind(null,345)),"v-9fff2bd8":()=>t.e(9).then(t.bind(null,346)),"v-5f386e4e":()=>t.e(10).then(t.bind(null,347)),"v-7c6bc912":()=>t.e(11).then(t.bind(null,348)),"v-9181d446":()=>t.e(12).then(t.bind(null,349)),"v-bbe08a1e":()=>t.e(13).then(t.bind(null,350)),"v-9cd213c0":()=>t.e(14).then(t.bind(null,351)),"v-7b4cf0a1":()=>t.e(15).then(t.bind(null,352)),"v-c95d3aca":()=>t.e(16).then(t.bind(null,353)),"v-9e5a86d0":()=>t.e(17).then(t.bind(null,354)),"v-3855e824":()=>t.e(18).then(t.bind(null,355)),"v-0794b35a":()=>t.e(19).then(t.bind(null,356)),"v-2e9dfcd8":()=>t.e(20).then(t.bind(null,357)),"v-06d52030":()=>t.e(21).then(t.bind(null,358)),"v-40d02c90":()=>t.e(22).then(t.bind(null,359)),"v-394317e1":()=>t.e(24).then(t.bind(null,360)),"v-6c10e80b":()=>t.e(23).then(t.bind(null,361)),"v-1d6fec1a":()=>t.e(25).then(t.bind(null,362)),"v-18557f3a":()=>t.e(26).then(t.bind(null,363)),"v-a28c2f0e":()=>t.e(27).then(t.bind(null,364)),"v-d302f124":()=>t.e(28).then(t.bind(null,365)),"v-bee30952":()=>t.e(29).then(t.bind(null,366)),"v-5ab6e4c0":()=>t.e(30).then(t.bind(null,367)),"v-6fecdff5":()=>t.e(31).then(t.bind(null,368)),"v-37efdbda":()=>t.e(32).then(t.bind(null,369)),"v-571418de":()=>t.e(33).then(t.bind(null,370)),"v-ac71d918":()=>t.e(34).then(t.bind(null,371)),"v-103a83f1":()=>t.e(35).then(t.bind(null,372)),"v-61d0fb85":()=>t.e(36).then(t.bind(null,373)),"v-f3015bc0":()=>t.e(37).then(t.bind(null,374)),"v-f57141ac":()=>t.e(38).then(t.bind(null,375)),"v-80c18d30":()=>t.e(39).then(t.bind(null,376)),"v-c0ed00cc":()=>t.e(40).then(t.bind(null,377)),"v-65e99d02":()=>t.e(41).then(t.bind(null,378)),"v-049caca4":()=>t.e(42).then(t.bind(null,379)),"v-2915e1b2":()=>t.e(43).then(t.bind(null,380)),"v-cc4a8e7a":()=>t.e(44).then(t.bind(null,381)),"v-74249d63":()=>t.e(45).then(t.bind(null,382)),"v-5763dc5c":()=>t.e(46).then(t.bind(null,383)),"v-a2fb4e08":()=>t.e(47).then(t.bind(null,384)),"v-190cf2c0":()=>t.e(48).then(t.bind(null,385)),"v-6a692486":()=>t.e(49).then(t.bind(null,386)),"v-3da569d0":()=>t.e(50).then(t.bind(null,387)),"v-598109b1":()=>t.e(51).then(t.bind(null,388)),"v-e0e338cc":()=>t.e(52).then(t.bind(null,389)),"v-983a4cec":()=>t.e(53).then(t.bind(null,390)),"v-4be9d482":()=>t.e(54).then(t.bind(null,391)),"v-a7130124":()=>t.e(55).then(t.bind(null,392)),"v-58ecef53":()=>t.e(57).then(t.bind(null,393)),"v-4a3c8296":()=>t.e(58).then(t.bind(null,394)),"v-429c1b42":()=>t.e(59).then(t.bind(null,395)),"v-d01770aa":()=>t.e(60).then(t.bind(null,396)),"v-4f326cac":()=>t.e(61).then(t.bind(null,397)),"v-19b74794":()=>t.e(62).then(t.bind(null,398)),"v-0397123a":()=>t.e(63).then(t.bind(null,399)),"v-188a6e0c":()=>t.e(64).then(t.bind(null,400)),"v-dcb44cc4":()=>t.e(65).then(t.bind(null,401)),"v-57506a62":()=>t.e(66).then(t.bind(null,402)),"v-a7acaf8a":()=>t.e(67).then(t.bind(null,403)),"v-03fe0612":()=>t.e(56).then(t.bind(null,404)),"v-24e06c7a":()=>t.e(68).then(t.bind(null,405)),"v-b6e5e50e":()=>t.e(69).then(t.bind(null,406)),"v-58840c14":()=>t.e(70).then(t.bind(null,407)),"v-0c8edaf7":()=>t.e(71).then(t.bind(null,408)),"v-41c60043":()=>t.e(72).then(t.bind(null,409)),"v-0b6a31ab":()=>t.e(73).then(t.bind(null,410)),"v-7411b637":()=>t.e(74).then(t.bind(null,411)),"v-7a26fbf7":()=>t.e(75).then(t.bind(null,412)),"v-df3caa06":()=>t.e(76).then(t.bind(null,413)),"v-167975b0":()=>t.e(77).then(t.bind(null,414)),"v-0f216ec4":()=>t.e(78).then(t.bind(null,415)),"v-01f7e9de":()=>t.e(80).then(t.bind(null,416)),"v-a11d1cc4":()=>Promise.all([t.e(0),t.e(3)]).then(t.bind(null,417)),"v-38079923":()=>t.e(81).then(t.bind(null,418)),"v-0fe45e7f":()=>t.e(82).then(t.bind(null,419)),"v-1fc37214":()=>t.e(83).then(t.bind(null,420)),"v-6ee174df":()=>t.e(84).then(t.bind(null,421)),"v-6d73d8ca":()=>t.e(85).then(t.bind(null,422)),"v-216630e2":()=>t.e(86).then(t.bind(null,423)),"v-2ce111a7":()=>t.e(87).then(t.bind(null,424)),"v-7923cebe":()=>t.e(88).then(t.bind(null,425)),"v-36c2e4ce":()=>t.e(89).then(t.bind(null,426)),"v-b090e128":()=>t.e(79).then(t.bind(null,427)),"v-5ef7030e":()=>t.e(90).then(t.bind(null,428)),"v-bba36134":()=>t.e(91).then(t.bind(null,429)),"v-6f56ad33":()=>t.e(92).then(t.bind(null,430)),"v-fa43c8ec":()=>t.e(93).then(t.bind(null,431)),"v-a0c228a6":()=>t.e(94).then(t.bind(null,432)),"v-330e9b4e":()=>t.e(95).then(t.bind(null,433)),"v-5be28fe2":()=>t.e(97).then(t.bind(null,434)),"v-4971f274":()=>t.e(96).then(t.bind(null,435)),"v-a4942194":()=>t.e(98).then(t.bind(null,436)),"v-5a65c5ca":()=>t.e(99).then(t.bind(null,437)),"v-4b29b46c":()=>t.e(100).then(t.bind(null,438)),"v-b9edac60":()=>t.e(101).then(t.bind(null,439))};function Zo(e){const n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}const Xo=/-(\w)/g,Qo=Zo(e=>e.replace(Xo,(e,n)=>n?n.toUpperCase():"")),Yo=/\B([A-Z])/g,el=Zo(e=>e.replace(Yo,"-$1").toLowerCase()),nl=Zo(e=>e.charAt(0).toUpperCase()+e.slice(1));function tl(e,n){if(!n)return;if(e(n))return e(n);return n.includes("-")?e(nl(Qo(n))):e(nl(n))||e(el(n))}const rl=Object.assign({},Jo,$o),al=e=>rl[e],il=e=>$o[e],ol=e=>Jo[e],ll=e=>Jt.component(e);function sl(e){return tl(il,e)}function cl(e){return tl(ol,e)}function dl(e){return tl(al,e)}function ul(e){return tl(ll,e)}function pl(...e){return Promise.all(e.filter(e=>e).map(async e=>{if(!ul(e)&&dl(e)){const n=await dl(e)();Jt.component(e,n.default)}}))}function hl(e,n){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[e]=n)}var fl=t(92),ml=t.n(fl),bl=t(93),vl=t.n(bl),yl={created(){if(this.siteMeta=this.$site.headTags.filter(([e])=>"meta"===e).map(([e,n])=>n),this.$ssrContext){const n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map(e=>{let n="<meta";return Object.keys(e).forEach(t=>{n+=` ${t}="${vl()(e[t])}"`}),n+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=kl(this.$canonicalUrl)}var e},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const e=this.getMergedMetaTags();this.currentMetaTags=xl(e,this.currentMetaTags)},getMergedMetaTags(){const e=this.$page.frontmatter.meta||[];return ml()([{name:"description",content:this.$description}],e,this.siteMeta,Cl)},updateCanonicalLink(){gl(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",kl(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){xl(null,this.currentMetaTags),gl()}};function gl(){const e=document.querySelector("link[rel='canonical']");e&&e.remove()}function kl(e=""){return e?`<link href="${e}" rel="canonical" />`:""}function xl(e,n){if(n&&[...n].filter(e=>e.parentNode===document.head).forEach(e=>document.head.removeChild(e)),e)return e.map(e=>{const n=document.createElement("meta");return Object.keys(e).forEach(t=>{n.setAttribute(t,e[t])}),document.head.appendChild(n),n})}function Cl(e){for(const n of["name","property","itemprop"])if(e.hasOwnProperty(n))return e[n]+n;return JSON.stringify(e)}var wl=t(51),_l={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(wl)()((function(){this.setActiveHash()}),300),setActiveHash(){const e=[].slice.call(document.querySelectorAll(".sidebar-link")),n=[].slice.call(document.querySelectorAll(".header-anchor")).filter(n=>e.some(e=>e.hash===n.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),r=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),a=window.innerHeight+t;for(let e=0;e<n.length;e++){const i=n[e],o=n[e+1],l=0===e&&0===t||t>=i.parentElement.offsetTop+10&&(!o||t<o.parentElement.offsetTop-10),s=decodeURIComponent(this.$route.hash);if(l&&s!==decodeURIComponent(i.hash)){const t=i;if(a===r)for(let t=e+1;t<n.length;t++)if(s===decodeURIComponent(n[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},El=t(24),Bl=t.n(El),Sl={mounted(){Bl.a.configure({showSpinner:!1}),this.$router.beforeEach((e,n,t)=>{e.path===n.path||Jt.component(e.name)||Bl.a.start(),t()}),this.$router.afterEach(()=>{Bl.a.done(),this.isSidebarOpen=!1})}},Rl=t(94),Il={noCopy:!0,noSelect:!1,disabled:!1,minLength:100,authorName:""},Ol={props:{html:String,lang:String},created(){this.authorName="string"==typeof Il.authorName?Il.authorName:this.getI18nValue(Il.authorName),this.text=this.getI18nValue(Rl),this.location=String(location).replace(/#.+$/,"")},methods:{getI18nValue(e){return this.lang in e?e[this.lang]:e["en-US"]}}},Al=t(4),Tl=Object(Al.a)(Ol,(function(){var e=this,n=e._self._c;return n("div",[n("p",[e._v(e._s(e.text.beforeAuthor)+e._s(e.authorName||e.text.author)+e._s(e.text.afterAuthor)),n("a",{attrs:{href:e.location}},[e._v(e._s(decodeURIComponent(e.location)))])]),e._v("\n\n"),n("div",{domProps:{innerHTML:e._s(e.html)}})])}),[],!1,null,null,null).exports,Nl={data:()=>({isElement:!1}),created(){this.onCopy=e=>{const n=getSelection().getRangeAt(0);if(String(n).length<this.minLength)return;if(e.preventDefault(),this.noCopy)return;const t=document.createElement("div");t.appendChild(getSelection().getRangeAt(0).cloneContents());const r=this.$lang,a=new Jt({render:e=>e(Tl,{props:{html:t.innerHTML,lang:r}})}).$mount(),{innerHTML:i,innerText:o}=a.$el;e.clipboardData?(e.clipboardData.setData("text/html",i),e.clipboardData.setData("text/plain",o)):window.clipboardData&&window.clipboardData.setData("text",o)}},watch:{isElement(e){if(!e)return;let{copyright:n=!Il.disabled}=this.$frontmatter;if(!n)return;"object"!=typeof n&&(n={});const t=n.noSelect||Il.noSelect;this.minLength=n.minLength||Il.minLength,this.noCopy=n.noCopy||Il.noCopy,t?this.$el.style.userSelect="none":this.$el.addEventListener("copy",this.onCopy)}},updated(){this.isElement="#comment"!==this.$el.nodeName},beforeDestory(){this.$el.removeEventListener("copy",this.onCopy)}};t(241),t(242);class zl{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:e="",duration:n=3e3}){let t=document.createElement("div");t.className="message move-in",t.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${e}</div>\n    `,this.containerEl.appendChild(t),n>0&&setTimeout(()=>{this.close(t)},n)}close(e){e.className=e.className.replace("move-in",""),e.className+="move-out",e.addEventListener("animationend",()=>{e.remove()})}}var Ll={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(e=>{document.querySelectorAll(e).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(e){if(e.classList.contains("codecopy-enabled"))return;const n=document.createElement("i");n.className="code-copy",n.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',n.title="Copy to clipboard",n.addEventListener("click",()=>{this.copyToClipboard(e.innerText)}),e.appendChild(n),e.classList.add("codecopy-enabled")},copyToClipboard(e){const n=document.createElement("textarea");n.value=e,n.setAttribute("readonly",""),n.style.position="absolute",n.style.left="-9999px",document.body.appendChild(n);const t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);n.select(),document.execCommand("copy");(new zl).show({text:"复制成功",duration:1e3}),document.body.removeChild(n),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}};!function(e,n){void 0===n&&(n={});var t=n.insertAt;if(e&&"undefined"!=typeof document){var r=document.head||document.getElementsByTagName("head")[0],a=document.createElement("style");a.type="text/css","top"===t&&r.firstChild?r.insertBefore(a,r.firstChild):r.appendChild(a),a.styleSheet?a.styleSheet.cssText=e:a.appendChild(document.createTextNode(e))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var Pl={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},Dl={},Fl=function(e){return'<div id="app">\n'.concat(e,"\n</div>")},Ul=function(e){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[e]?window.$VUEPRESS_DEMO_BLOCK[e]:Pl[e]},Hl=function e(n,t,r){var a=document.createElement(n);return t&&Object.keys(t).forEach((function(e){if(e.indexOf("data"))a[e]=t[e];else{var n=e.replace("data","");a.dataset[n]=t[e]}})),r&&r.forEach((function(n){var t=n.tag,r=n.attrs,i=n.children;a.appendChild(e(t,r,i))})),a},jl=function(e,n,t){var r,a=(r=e.querySelectorAll(".".concat(n)),Array.prototype.slice.call(r));return 1!==a.length||t?a:a[0]},Ml=function(e,n){var t,r,a=e.match(/<style>([\s\S]+)<\/style>/),i=e.match(/<template>([\s\S]+)<\/template>/),o=e.match(/<script>([\s\S]+)<\/script>/),l={css:a&&a[1].replace(/^\n|\n$/g,""),html:i&&i[1].replace(/^\n|\n$/g,""),js:o&&o[1].replace(/^\n|\n$/g,""),jsLib:n.jsLib||[],cssLib:n.cssLib||[]};l.htmlTpl=Fl(l.html),l.jsTpl=(t=l.js,r=t.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(r,"\n})")),l.script=function(e,n){var t=e.split(/export\s+default/),r="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),a=window.Babel?window.Babel.transform(r,{presets:["es2015"]}).code:r,i=[eval][0](a);return i.template=n,i}(l.js,l.html);var s=Ul("vue");return l.jsLib.unshift(s),l},Kl=function(e,n){var t,r=e.match(/<style>([\s\S]+)<\/style>/),a=e.match(/<html>([\s\S]+)<\/html>/),i=e.match(/<script>([\s\S]+)<\/script>/),o={css:r&&r[1].replace(/^\n|\n$/g,""),html:a&&a[1].replace(/^\n|\n$/g,""),js:i&&i[1].replace(/^\n|\n$/g,""),jsLib:n.jsLib||[],cssLib:n.cssLib||[]};return o.htmlTpl=o.html,o.jsTpl=o.js,o.script=(t=o.js,window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t),o},ql=function(e){return e=e.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),e+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function Gl(){var e=jl(document,"vuepress-plugin-demo-block__wrapper",!0);e.length?e.forEach((function(e){if("true"!==e.dataset.created){e.style.display="block";var n=jl(e,"vuepress-plugin-demo-block__code"),t=jl(e,"vuepress-plugin-demo-block__display"),r=jl(e,"vuepress-plugin-demo-block__footer"),a=jl(t,"vuepress-plugin-demo-block__app"),i=decodeURIComponent(e.dataset.code),o=decodeURIComponent(e.dataset.config),l=decodeURIComponent(e.dataset.type);o=o?JSON.parse(o):{};var s=n.querySelector("div").clientHeight,c="react"===l?function(e,n){var t=(0,window.Babel.transform)(e,{presets:["es2015","react"]}).code,r="(function(exports){var module={};module.exports=exports;".concat(t,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),a=new Function("return ".concat(r))(),i={js:a,css:a.__style__||"",jsLib:n.jsLib||[],cssLib:n.cssLib||[],jsTpl:ql(e),htmlTpl:Fl("")},o=Ul("react"),l=Ul("reactDOM");return i.jsLib.unshift(o,l),i}(i,o):"vanilla"===l?Kl(i,o):Ml(i,o),d=Hl("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(r.appendChild(d),d.addEventListener("click",Wl.bind(null,d,s,n,r)),Ul("jsfiddle")&&r.appendChild(function(e){var n=e.css,t=e.htmlTpl,r=e.jsTpl,a=e.jsLib,i=e.cssLib,o=a.concat(i).concat(Ul("cssLib")).concat(Ul("jsLib")).join(",");return Hl("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:n}},{tag:"input",attrs:{type:"hidden",name:"html",value:t}},{tag:"input",attrs:{type:"hidden",name:"js",value:r}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:o}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}])}(c)),Ul("codepen")&&r.appendChild(function(e){var n=e.css,t=e.htmlTpl,r=e.jsTpl,a=e.jsLib,i=e.cssLib,o=JSON.stringify({css:n,html:t,js:r,js_external:a.concat(Ul("jsLib")).join(";"),css_external:i.concat(Ul("cssLib")).join(";"),layout:Ul("codepenLayout"),js_pre_processor:Ul("codepenJsProcessor"),editors:Ul("codepenEditors")});return Hl("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:o}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(c)),void 0!==o.horizontal?o.horizontal:Ul("horizontal")){e.classList.add("vuepress-plugin-demo-block__horizontal");var u=n.firstChild.cloneNode(!0);u.classList.add("vuepress-plugin-demo-block__h_code"),t.appendChild(u)}if(c.css&&function(e){if(!Dl[e]){var n=Hl("style",{innerHTML:e});document.body.appendChild(n),Dl[e]=!0}}(c.css),"react"===l)ReactDOM.render(React.createElement(c.js),a);else if("vue"===l){var p=(new(Vue.extend(c.script))).$mount();a.appendChild(p.$el)}else"vanilla"===l&&(a.innerHTML=c.html,new Function("return (function(){".concat(c.script,"})()"))());e.dataset.created="true"}})):setTimeout((function(e){Gl()}),300)}function Wl(e,n,t,r){var a="1"!==e.dataset.isExpand;t.style.height=a?"".concat(n,"px"):0,a?r.classList.add("vuepress-plugin-demo-block__show-link"):r.classList.remove("vuepress-plugin-demo-block__show-link"),e.dataset.isExpand=a?"1":"0"}var Vl={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},Gl()},updated:function(){Gl()}},Jl="auto",$l="zoom-in",Zl="zoom-out",Xl="grab",Ql="move";function Yl(e,n,t){var r=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],a={passive:!1};r?e.addEventListener(n,t,a):e.removeEventListener(n,t,a)}function es(e,n){if(e){var t=new Image;t.onload=function(){n&&n(t)},t.src=e}}function ns(e){return e.dataset.original?e.dataset.original:"A"===e.parentNode.tagName?e.parentNode.getAttribute("href"):null}function ts(e,n,t){!function(e){var n=rs,t=as;if(e.transition){var r=e.transition;delete e.transition,e[n]=r}if(e.transform){var a=e.transform;delete e.transform,e[t]=a}}(n);var r=e.style,a={};for(var i in n)t&&(a[i]=r[i]||""),r[i]=n[i];return a}var rs="transition",as="transform",is="transform",os="transitionend";var ls=function(){},ss={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:ls,onClose:ls,onGrab:ls,onMove:ls,onRelease:ls,onBeforeOpen:ls,onBeforeClose:ls,onBeforeGrab:ls,onBeforeRelease:ls,onImageLoading:ls,onImageLoaded:ls},cs={init:function(e){var n,t;n=this,t=e,Object.getOwnPropertyNames(Object.getPrototypeOf(n)).forEach((function(e){n[e]=n[e].bind(t)}))},click:function(e){if(e.preventDefault(),us(e))return window.open(this.target.srcOriginal||e.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(e.currentTarget)},scroll:function(){var e=document.documentElement||document.body.parentNode||document.body,n=window.pageXOffset||e.scrollLeft,t=window.pageYOffset||e.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:n,y:t});var r=this.lastScrollPosition.x-n,a=this.lastScrollPosition.y-t,i=this.options.scrollThreshold;(Math.abs(a)>=i||Math.abs(r)>=i)&&(this.lastScrollPosition=null,this.close())},keydown:function(e){(function(e){return"Escape"===(e.key||e.code)||27===e.keyCode})(e)&&(this.released?this.close():this.release(this.close))},mousedown:function(e){if(ds(e)&&!us(e)){e.preventDefault();var n=e.clientX,t=e.clientY;this.pressTimer=setTimeout(function(){this.grab(n,t)}.bind(this),200)}},mousemove:function(e){this.released||this.move(e.clientX,e.clientY)},mouseup:function(e){ds(e)&&!us(e)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(e){e.preventDefault();var n=e.touches[0],t=n.clientX,r=n.clientY;this.pressTimer=setTimeout(function(){this.grab(t,r)}.bind(this),200)},touchmove:function(e){if(!this.released){var n=e.touches[0],t=n.clientX,r=n.clientY;this.move(t,r)}},touchend:function(e){(function(e){e.targetTouches.length})(e)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function ds(e){return 0===e.button}function us(e){return e.metaKey||e.ctrlKey}var ps={init:function(e){this.el=document.createElement("div"),this.instance=e,this.parent=document.body,ts(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(e.options),Yl(this.el,"click",e.handler.clickOverlay.bind(e))},updateStyle:function(e){ts(this.el,{zIndex:e.zIndex,backgroundColor:e.bgColor,transition:"opacity\n        "+e.transitionDuration+"s\n        "+e.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},hs="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},fs=function(){function e(e,n){for(var t=0;t<n.length;t++){var r=n[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(e,r.key,r)}}return function(n,t,r){return t&&e(n.prototype,t),r&&e(n,r),n}}(),ms=Object.assign||function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(e[r]=t[r])}return e},bs={init:function(e,n){this.el=e,this.instance=n,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=ns(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var e=this.instance.options,n=e.zIndex,t=e.enableGrab,r=e.transitionDuration,a=e.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:n+1,cursor:t?Xl:Zl,transition:is+"\n        "+r+"s\n        "+a,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=ts(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,ts(this.el,{transform:"none"})},grab:function(e,n,t){var r=vs(),a=r.x-e,i=r.y-n;ts(this.el,{cursor:Ql,transform:"translate3d(\n        "+(this.translate.x+a)+"px, "+(this.translate.y+i)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(e,n,t){var r=vs(),a=r.x-e,i=r.y-n;ts(this.el,{transition:is,transform:"translate3d(\n        "+(this.translate.x+a)+"px, "+(this.translate.y+i)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){ts(this.el,this.styleClose)},restoreOpenStyle:function(){ts(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var e=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var n=this.el.cloneNode(!1);n.setAttribute("src",this.srcOriginal),n.style.position="fixed",n.style.visibility="hidden",e.appendChild(n),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),e.removeChild(n)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var e=vs(),n=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:e.x-n,y:e.y-t}},calculateScale:function(){var e=this.el.dataset,n=e.zoomingHeight,t=e.zoomingWidth,r=this.instance.options,a=r.customSize,i=r.scaleBase;if(!a&&n&&t)return{x:t/this.rect.width,y:n/this.rect.height};if(a&&"object"===(void 0===a?"undefined":hs(a)))return{x:a.width/this.rect.width,y:a.height/this.rect.height};var o=this.rect.width/2,l=this.rect.height/2,s=vs(),c={x:s.x-o,y:s.y-l},d=c.x/o,u=c.y/l,p=i+Math.min(d,u);if(a&&"string"==typeof a){var h=t||this.el.naturalWidth,f=n||this.el.naturalHeight,m=parseFloat(a)*h/(100*this.rect.width),b=parseFloat(a)*f/(100*this.rect.height);if(p>m||p>b)return{x:m,y:b}}return{x:p,y:p}}};function vs(){var e=document.documentElement;return{x:Math.min(e.clientWidth,window.innerWidth)/2,y:Math.min(e.clientHeight,window.innerHeight)/2}}function ys(e,n,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(r){Yl(e,r,n[r],t)}))}var gs=function(){function e(n){!function(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}(this,e),this.target=Object.create(bs),this.overlay=Object.create(ps),this.handler=Object.create(cs),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=ms({},ss,n),this.overlay.init(this),this.handler.init(this)}return fs(e,[{key:"listen",value:function(e){if("string"==typeof e)for(var n=document.querySelectorAll(e),t=n.length;t--;)this.listen(n[t]);else"IMG"===e.tagName&&(e.style.cursor=$l,Yl(e,"click",this.handler.click),this.options.preloadImage&&es(ns(e)));return this}},{key:"config",value:function(e){return e?(ms(this.options,e),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(e){var n=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var r="string"==typeof e?document.querySelector(e):e;if("IMG"===r.tagName){if(this.options.onBeforeOpen(r),this.target.init(r,this),!this.options.preloadImage){var a=this.target.srcOriginal;null!=a&&(this.options.onImageLoading(r),es(a,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Yl(document,"scroll",this.handler.scroll),Yl(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Yl(window,"resize",this.handler.resizeWindow);var i=function e(){Yl(r,os,e,!1),n.lock=!1,n.target.upgradeSource(),n.options.enableGrab&&ys(document,n.handler,!0),t(r)};return Yl(r,os,i),this}}}},{key:"close",value:function(){var e=this,n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=Jl,this.overlay.fadeOut(),this.target.zoomOut(),Yl(document,"scroll",this.handler.scroll,!1),Yl(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Yl(window,"resize",this.handler.resizeWindow,!1);var r=function r(){Yl(t,os,r,!1),e.shown=!1,e.lock=!1,e.target.downgradeSource(),e.options.enableGrab&&ys(document,e.handler,!1),e.target.restoreCloseStyle(),e.overlay.remove(),n(t)};return Yl(t,os,r),this}}},{key:"grab",value:function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var a=this.target.el;this.options.onBeforeGrab(a),this.released=!1,this.target.grab(e,n,t);var i=function e(){Yl(a,os,e,!1),r(a)};return Yl(a,os,i),this}}},{key:"move",value:function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Ql,this.target.move(e,n,t);var a=this.target.el,i=function e(){Yl(a,os,e,!1),r(a)};return Yl(a,os,i),this}}},{key:"release",value:function(){var e=this,n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=Jl,this.target.restoreOpenStyle();var r=function r(){Yl(t,os,r,!1),e.lock=!1,e.released=!0,n(t)};return Yl(t,os,r),this}}}]),e}();const ks=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),xs=Number("500");class Cs{constructor(){this.instance=new gs(ks)}update(e=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(e)}updateDelay(e=".theme-vdoing-content img:not(.no-zoom)",n=xs){setTimeout(()=>this.update(e),n)}}var ws=[yl,_l,Sl,Nl,Ll,Vl,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new Cs,this.$vuepress.zooming.updateDelay()}}],_s={name:"GlobalLayout",computed:{layout(){const e=this.getLayout();return hl("layout",e),Jt.component(e)}},methods:{getLayout(){if(this.$page.path){const e=this.$page.frontmatter.layout;return e&&(this.$vuepress.getLayoutAsyncComponent(e)||this.$vuepress.getVueComponent(e))?e:"Layout"}return"NotFound"}}},Es=Object(Al.a)(_s,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(e,n,t){switch(n){case"components":e[n]||(e[n]={}),Object.assign(e[n],t);break;case"mixins":e[n]||(e[n]=[]),e[n].push(...t);break;default:throw new Error("Unknown option name.")}}(Es,"mixins",ws);const Bs=[{name:"v-5571592e",path:"/pages/fccd91/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-5571592e").then(t)}},{path:"/pages/fccd91/index.html",redirect:"/pages/fccd91/"},{path:"/01.热门算法/01.热门算法/01.Bloom Filter.html",redirect:"/pages/fccd91/"},{name:"v-9fff2bd8",path:"/pages/1e28a2/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-9fff2bd8").then(t)}},{path:"/pages/1e28a2/index.html",redirect:"/pages/1e28a2/"},{path:"/01.热门算法/01.热门算法/02.Consistent Hashing.html",redirect:"/pages/1e28a2/"},{name:"v-5f386e4e",path:"/pages/8624c5/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-5f386e4e").then(t)}},{path:"/pages/8624c5/index.html",redirect:"/pages/8624c5/"},{path:"/01.热门算法/01.热门算法/03.Count-Min Sketch.html",redirect:"/pages/8624c5/"},{name:"v-7c6bc912",path:"/pages/87589a/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-7c6bc912").then(t)}},{path:"/pages/87589a/index.html",redirect:"/pages/87589a/"},{path:"/01.热门算法/01.热门算法/04.LRU.html",redirect:"/pages/87589a/"},{name:"v-9181d446",path:"/pages/7d22be/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-9181d446").then(t)}},{path:"/pages/7d22be/index.html",redirect:"/pages/7d22be/"},{path:"/01.热门算法/01.热门算法/05.LFU.html",redirect:"/pages/7d22be/"},{name:"v-bbe08a1e",path:"/pages/2d43d1/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-bbe08a1e").then(t)}},{path:"/pages/2d43d1/index.html",redirect:"/pages/2d43d1/"},{path:"/01.热门算法/01.热门算法/06.hash & rehash.html",redirect:"/pages/2d43d1/"},{name:"v-9cd213c0",path:"/pages/44dcc2/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-9cd213c0").then(t)}},{path:"/pages/44dcc2/index.html",redirect:"/pages/44dcc2/"},{path:"/01.热门算法/01.热门算法/10.Timing Wheels.html",redirect:"/pages/44dcc2/"},{name:"v-7b4cf0a1",path:"/pages/b9733b/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-7b4cf0a1").then(t)}},{path:"/pages/b9733b/index.html",redirect:"/pages/b9733b/"},{path:"/02.Kafka  系统设计/01.一、前言/01.介绍.html",redirect:"/pages/b9733b/"},{name:"v-c95d3aca",path:"/pages/e21d7f/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-c95d3aca").then(t)}},{path:"/pages/e21d7f/index.html",redirect:"/pages/e21d7f/"},{path:"/02.Kafka  系统设计/01.一、前言/01.指南.html",redirect:"/pages/e21d7f/"},{name:"v-9e5a86d0",path:"/pages/46a58a/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-9e5a86d0").then(t)}},{path:"/pages/46a58a/index.html",redirect:"/pages/46a58a/"},{path:"/02.Kafka  系统设计/06.三、主线任务/03.Kafka 整体架构.html",redirect:"/pages/46a58a/"},{name:"v-3855e824",path:"/pages/bb1005/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-3855e824").then(t)}},{path:"/pages/bb1005/index.html",redirect:"/pages/bb1005/"},{path:"/02.Kafka  系统设计/06.三、主线任务/05.生产者.html",redirect:"/pages/bb1005/"},{name:"v-0794b35a",path:"/pages/aa0392/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-0794b35a").then(t)}},{path:"/pages/aa0392/index.html",redirect:"/pages/aa0392/"},{path:"/02.Kafka  系统设计/06.三、主线任务/07.Broker.html",redirect:"/pages/aa0392/"},{name:"v-2e9dfcd8",path:"/pages/ca141b/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-2e9dfcd8").then(t)}},{path:"/pages/ca141b/index.html",redirect:"/pages/ca141b/"},{path:"/02.Kafka  系统设计/06.三、主线任务/10.消费者.html",redirect:"/pages/ca141b/"},{name:"v-06d52030",path:"/pages/19bf78/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-06d52030").then(t)}},{path:"/pages/19bf78/index.html",redirect:"/pages/19bf78/"},{path:"/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/10.Kafka Producer 设计分析.html",redirect:"/pages/19bf78/"},{name:"v-40d02c90",path:"/pages/32a8ff/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-40d02c90").then(t)}},{path:"/pages/32a8ff/index.html",redirect:"/pages/32a8ff/"},{path:"/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/15.Kafka Producer 源码分析.html",redirect:"/pages/32a8ff/"},{name:"v-394317e1",path:"/pages/bf1d55/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-394317e1").then(t)}},{path:"/pages/bf1d55/index.html",redirect:"/pages/bf1d55/"},{path:"/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/40.Sender 类代码分析.html",redirect:"/pages/bf1d55/"},{name:"v-6c10e80b",path:"/pages/b77953/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-6c10e80b").then(t)}},{path:"/pages/b77953/index.html",redirect:"/pages/b77953/"},{path:"/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/20.RecordAccumulator 源码分析.html",redirect:"/pages/b77953/"},{name:"v-1d6fec1a",path:"/pages/b2caf1/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-1d6fec1a").then(t)}},{path:"/pages/b2caf1/index.html",redirect:"/pages/b2caf1/"},{path:"/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/50.NetworkClient 类代码分析.html",redirect:"/pages/b2caf1/"},{name:"v-18557f3a",path:"/pages/6c0fd7/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-18557f3a").then(t)}},{path:"/pages/6c0fd7/index.html",redirect:"/pages/6c0fd7/"},{path:"/02.Kafka  系统设计/06.三、主线任务/600.「消费者」源码分析/05.KafkaConsumer 类代码分析.html",redirect:"/pages/6c0fd7/"},{name:"v-a28c2f0e",path:"/pages/d9df7d/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-a28c2f0e").then(t)}},{path:"/pages/d9df7d/index.html",redirect:"/pages/d9df7d/"},{path:"/02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/05.控制器.html",redirect:"/pages/d9df7d/"},{name:"v-d302f124",path:"/pages/b60a98/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-d302f124").then(t)}},{path:"/pages/b60a98/index.html",redirect:"/pages/b60a98/"},{path:"/02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/10.协调器.html",redirect:"/pages/b60a98/"},{name:"v-bee30952",path:"/pages/b6cf91/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-bee30952").then(t)}},{path:"/pages/b6cf91/index.html",redirect:"/pages/b6cf91/"},{path:"/02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/15.日志管理器.html",redirect:"/pages/b6cf91/"},{name:"v-5ab6e4c0",path:"/pages/f57330/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-5ab6e4c0").then(t)}},{path:"/pages/f57330/index.html",redirect:"/pages/f57330/"},{path:"/02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/20.副本管理器.html",redirect:"/pages/f57330/"},{name:"v-6fecdff5",path:"/pages/2dec11/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-6fecdff5").then(t)}},{path:"/pages/2dec11/index.html",redirect:"/pages/2dec11/"},{path:"/02.Kafka  系统设计/08.四、设计目标/20.高可用探究.html",redirect:"/pages/2dec11/"},{name:"v-37efdbda",path:"/pages/168766/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-37efdbda").then(t)}},{path:"/pages/168766/index.html",redirect:"/pages/168766/"},{path:"/02.Kafka  系统设计/08.四、设计目标/25.高扩展探究.html",redirect:"/pages/168766/"},{name:"v-571418de",path:"/pages/dd50fc/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-571418de").then(t)}},{path:"/pages/dd50fc/index.html",redirect:"/pages/dd50fc/"},{path:"/02.Kafka  系统设计/08.四、设计目标/35.高并发探究.html",redirect:"/pages/dd50fc/"},{name:"v-ac71d918",path:"/pages/4601ca/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-ac71d918").then(t)}},{path:"/pages/4601ca/index.html",redirect:"/pages/4601ca/"},{path:"/03.Nginx 系统设计/01.Nginx 系统设计/01.介绍.html",redirect:"/pages/4601ca/"},{name:"v-103a83f1",path:"/pages/52ebd8/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-103a83f1").then(t)}},{path:"/pages/52ebd8/index.html",redirect:"/pages/52ebd8/"},{path:"/06.动态/01.碎碎念.html",redirect:"/pages/52ebd8/"},{name:"v-61d0fb85",path:"/archives/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-61d0fb85").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-f3015bc0",path:"/pages/bfab10/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-f3015bc0").then(t)}},{path:"/pages/bfab10/index.html",redirect:"/pages/bfab10/"},{path:"/Netty 系统设计/05.一、前言/05.指南.html",redirect:"/pages/bfab10/"},{name:"v-f57141ac",path:"/pages/c0367e/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-f57141ac").then(t)}},{path:"/pages/c0367e/index.html",redirect:"/pages/c0367e/"},{path:"/Netty 系统设计/10.二、基础知识/07.IO 多路复用详解.html",redirect:"/pages/c0367e/"},{name:"v-80c18d30",path:"/pages/3f7882/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-80c18d30").then(t)}},{path:"/pages/3f7882/index.html",redirect:"/pages/3f7882/"},{path:"/Netty 系统设计/10.二、基础知识/15.零拷贝详解.html",redirect:"/pages/3f7882/"},{name:"v-c0ed00cc",path:"/pages/cb89e4/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-c0ed00cc").then(t)}},{path:"/pages/cb89e4/index.html",redirect:"/pages/cb89e4/"},{path:"/Netty 系统设计/10.二、基础知识/20.Future 和 Promise.html",redirect:"/pages/cb89e4/"},{name:"v-65e99d02",path:"/pages/9b88cb/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-65e99d02").then(t)}},{path:"/pages/9b88cb/index.html",redirect:"/pages/9b88cb/"},{path:"/Netty 系统设计/10.二、基础知识/30.TCP 拆包与粘包.html",redirect:"/pages/9b88cb/"},{name:"v-049caca4",path:"/pages/cab36c/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-049caca4").then(t)}},{path:"/pages/cab36c/index.html",redirect:"/pages/cab36c/"},{path:"/Netty 系统设计/10.二、基础知识/40.心跳机制详解.html",redirect:"/pages/cab36c/"},{name:"v-2915e1b2",path:"/pages/b0bc66/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-2915e1b2").then(t)}},{path:"/pages/b0bc66/index.html",redirect:"/pages/b0bc66/"},{path:"/Netty 系统设计/20.三、主线任务/01.Netty 框架概述.html",redirect:"/pages/b0bc66/"},{name:"v-cc4a8e7a",path:"/pages/9582d0/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-cc4a8e7a").then(t)}},{path:"/pages/9582d0/index.html",redirect:"/pages/9582d0/"},{path:"/Netty 系统设计/20.三、主线任务/02.Bootstrap（client）源码解析.html",redirect:"/pages/9582d0/"},{name:"v-74249d63",path:"/pages/b2a14a/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-74249d63").then(t)}},{path:"/pages/b2a14a/index.html",redirect:"/pages/b2a14a/"},{path:"/Netty 系统设计/20.三、主线任务/03.Bootstrap（server）源码解析.html",redirect:"/pages/b2a14a/"},{name:"v-5763dc5c",path:"/pages/8d1ba9/Channel/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-5763dc5c").then(t)}},{path:"/pages/8d1ba9/Channel/index.html",redirect:"/pages/8d1ba9/Channel/"},{path:"/Netty 系统设计/20.三、主线任务/05.Channel 源码解析.html",redirect:"/pages/8d1ba9/Channel/"},{name:"v-a2fb4e08",path:"/pages/397456/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-a2fb4e08").then(t)}},{path:"/pages/397456/index.html",redirect:"/pages/397456/"},{path:"/Netty 系统设计/20.三、主线任务/10.EventLoop 源码解析.html",redirect:"/pages/397456/"},{name:"v-190cf2c0",path:"/pages/ce1f78/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-190cf2c0").then(t)}},{path:"/pages/ce1f78/index.html",redirect:"/pages/ce1f78/"},{path:"/Netty 系统设计/20.三、主线任务/20.ChannelHandler 源码解析.html",redirect:"/pages/ce1f78/"},{name:"v-6a692486",path:"/pages/4234c0/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-6a692486").then(t)}},{path:"/pages/4234c0/index.html",redirect:"/pages/4234c0/"},{path:"/Netty 系统设计/20.三、主线任务/30.ChannelPipeline 源码解析.html",redirect:"/pages/4234c0/"},{name:"v-3da569d0",path:"/pages/43eb30/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-3da569d0").then(t)}},{path:"/pages/43eb30/index.html",redirect:"/pages/43eb30/"},{path:"/Netty 系统设计/20.三、主线任务/40.Bytebuf 源码解析.html",redirect:"/pages/43eb30/"},{name:"v-598109b1",path:"/pages/65674f/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-598109b1").then(t)}},{path:"/pages/65674f/index.html",redirect:"/pages/65674f/"},{path:"/Netty 系统设计/25.四、深入 Netty 核心/05.第一课：透过 Netty 看 IO 模型.html",redirect:"/pages/65674f/"},{name:"v-e0e338cc",path:"/pages/440035/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-e0e338cc").then(t)}},{path:"/pages/440035/index.html",redirect:"/pages/440035/"},{path:"/Netty 系统设计/25.四、深入 Netty 核心/10.第二课：Reactor在Netty中的实现（创建篇）.html",redirect:"/pages/440035/"},{name:"v-983a4cec",path:"/pages/bb668a/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-983a4cec").then(t)}},{path:"/pages/bb668a/index.html",redirect:"/pages/bb668a/"},{path:"/Netty 系统设计/25.四、深入 Netty 核心/12.第三课：Netty 核心引擎 Reactor 的运转架构.html",redirect:"/pages/bb668a/"},{name:"v-4be9d482",path:"/pages/be98dc/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-4be9d482").then(t)}},{path:"/pages/be98dc/index.html",redirect:"/pages/be98dc/"},{path:"/Netty 系统设计/25.四、深入 Netty 核心/15.第四课：Netty 如何高效接收网络连接.html",redirect:"/pages/be98dc/"},{name:"v-a7130124",path:"/pages/0938c1/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-a7130124").then(t)}},{path:"/pages/0938c1/index.html",redirect:"/pages/0938c1/"},{path:"/Netty 系统设计/25.四、深入 Netty 核心/18.第五课：Netty 如何高效接收网络数据.html",redirect:"/pages/0938c1/"},{name:"v-58ecef53",path:"/pages/a1b0fe/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-58ecef53").then(t)}},{path:"/pages/a1b0fe/index.html",redirect:"/pages/a1b0fe/"},{path:"/Netty 系统设计/25.四、深入 Netty 核心/25.第七课：Netty 中IO事件的触发时机和传播流程.html",redirect:"/pages/a1b0fe/"},{name:"v-4a3c8296",path:"/pages/8be98a/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-4a3c8296").then(t)}},{path:"/pages/8be98a/index.html",redirect:"/pages/8be98a/"},{path:"/Netty 系统设计/30.五、深入 Netty 内存管理/05.第一课：ByteBuf 体系的设计与实现.html",redirect:"/pages/8be98a/"},{name:"v-429c1b42",path:"/pages/252196/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-429c1b42").then(t)}},{path:"/pages/252196/index.html",redirect:"/pages/252196/"},{path:"/Redis 系统设计/01.一、前言/01.指南.html",redirect:"/pages/252196/"},{name:"v-d01770aa",path:"/pages/69fbd7/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-d01770aa").then(t)}},{path:"/pages/69fbd7/index.html",redirect:"/pages/69fbd7/"},{path:"/Redis 系统设计/01.一、前言/05.Redis 伪码蓝图【必看】.html",redirect:"/pages/69fbd7/"},{name:"v-4f326cac",path:"/pages/bdae41/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-4f326cac").then(t)}},{path:"/pages/bdae41/index.html",redirect:"/pages/bdae41/"},{path:"/Redis 系统设计/02.二、基础知识/01.String 设计与实现.html",redirect:"/pages/bdae41/"},{name:"v-19b74794",path:"/pages/bd1e41/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-19b74794").then(t)}},{path:"/pages/bd1e41/index.html",redirect:"/pages/bd1e41/"},{path:"/Redis 系统设计/02.二、基础知识/02.List 设计与实现.html",redirect:"/pages/bd1e41/"},{name:"v-0397123a",path:"/pages/2d4311/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-0397123a").then(t)}},{path:"/pages/2d4311/index.html",redirect:"/pages/2d4311/"},{path:"/Redis 系统设计/02.二、基础知识/05.Hash 设计与实现.html",redirect:"/pages/2d4311/"},{name:"v-188a6e0c",path:"/pages/2d4312/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-188a6e0c").then(t)}},{path:"/pages/2d4312/index.html",redirect:"/pages/2d4312/"},{path:"/Redis 系统设计/02.二、基础知识/10.ZSet 设计与实现.html",redirect:"/pages/2d4312/"},{name:"v-dcb44cc4",path:"/pages/34fa27/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-dcb44cc4").then(t)}},{path:"/pages/34fa27/index.html",redirect:"/pages/34fa27/"},{path:"/Redis 系统设计/03.三、主线任务/01.Linux 中的 IO 多路复用.html",redirect:"/pages/34fa27/"},{name:"v-57506a62",path:"/pages/d4ecb9/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-57506a62").then(t)}},{path:"/pages/d4ecb9/index.html",redirect:"/pages/d4ecb9/"},{path:"/Redis 系统设计/03.三、主线任务/03.Redis Server 初始化.html",redirect:"/pages/d4ecb9/"},{name:"v-a7acaf8a",path:"/pages/d6b00d/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-a7acaf8a").then(t)}},{path:"/pages/d6b00d/index.html",redirect:"/pages/d6b00d/"},{path:"/Redis 系统设计/03.三、主线任务/05.Redis 的 Reactor 模型.html",redirect:"/pages/d6b00d/"},{name:"v-03fe0612",path:"/pages/a73206/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-03fe0612").then(t)}},{path:"/pages/a73206/index.html",redirect:"/pages/a73206/"},{path:"/Netty 系统设计/25.四、深入 Netty 核心/20.第六课：Netty 如何高效发送网络数据.html",redirect:"/pages/a73206/"},{name:"v-24e06c7a",path:"/pages/264b06/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-24e06c7a").then(t)}},{path:"/pages/264b06/index.html",redirect:"/pages/264b06/"},{path:"/Redis 系统设计/03.三、主线任务/08.深入 Redis 事件驱动框架.html",redirect:"/pages/264b06/"},{name:"v-b6e5e50e",path:"/pages/e6d8ef/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-b6e5e50e").then(t)}},{path:"/pages/e6d8ef/index.html",redirect:"/pages/e6d8ef/"},{path:"/Redis 系统设计/03.三、主线任务/09.Redis 的执行模式.html",redirect:"/pages/e6d8ef/"},{name:"v-58840c14",path:"/pages/0850b6/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-58840c14").then(t)}},{path:"/pages/0850b6/index.html",redirect:"/pages/0850b6/"},{path:"/Redis 系统设计/03.三、主线任务/16.Redis 多IO线程.html",redirect:"/pages/0850b6/"},{name:"v-0c8edaf7",path:"/pages/b43a19/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-0c8edaf7").then(t)}},{path:"/pages/b43a19/index.html",redirect:"/pages/b43a19/"},{path:"/Redis 系统设计/04.四、支线任务/05.LRU 策略.html",redirect:"/pages/b43a19/"},{name:"v-41c60043",path:"/pages/b43a89/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-41c60043").then(t)}},{path:"/pages/b43a89/index.html",redirect:"/pages/b43a89/"},{path:"/Redis 系统设计/04.四、支线任务/10.LFU 策略.html",redirect:"/pages/b43a89/"},{name:"v-0b6a31ab",path:"/pages/f44fbe/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-0b6a31ab").then(t)}},{path:"/pages/f44fbe/index.html",redirect:"/pages/f44fbe/"},{path:"/Redis 系统设计/04.四、支线任务/13.Redis 过期策略.html",redirect:"/pages/f44fbe/"},{name:"v-7411b637",path:"/pages/9b17a6/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-7411b637").then(t)}},{path:"/pages/9b17a6/index.html",redirect:"/pages/9b17a6/"},{path:"/Redis 系统设计/04.四、支线任务/15.RDB 持久化.html",redirect:"/pages/9b17a6/"},{name:"v-7a26fbf7",path:"/pages/9b17a7/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-7a26fbf7").then(t)}},{path:"/pages/9b17a7/index.html",redirect:"/pages/9b17a7/"},{path:"/Redis 系统设计/04.四、支线任务/17.AOF 持久化.html",redirect:"/pages/9b17a7/"},{name:"v-df3caa06",path:"/pages/aa75e9/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-df3caa06").then(t)}},{path:"/pages/aa75e9/index.html",redirect:"/pages/aa75e9/"},{path:"/Redis 系统设计/04.四、支线任务/20.Redis 中的延迟监控.html",redirect:"/pages/aa75e9/"},{name:"v-167975b0",path:"/pages/61d908/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-167975b0").then(t)}},{path:"/pages/61d908/index.html",redirect:"/pages/61d908/"},{path:"/Redis 系统设计/04.四、支线任务/25.发布与订阅.html",redirect:"/pages/61d908/"},{name:"v-0f216ec4",path:"/pages/ebc8dc/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-0f216ec4").then(t)}},{path:"/pages/ebc8dc/index.html",redirect:"/pages/ebc8dc/"},{path:"/Redis 系统设计/05.五、集群/25.主从复制.html",redirect:"/pages/ebc8dc/"},{name:"v-01f7e9de",path:"/pages/040403/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-01f7e9de").then(t)}},{path:"/pages/040403/index.html",redirect:"/pages/040403/"},{path:"/Redis 系统设计/05.五、集群/35.cluster.html",redirect:"/pages/040403/"},{name:"v-a11d1cc4",path:"/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-a11d1cc4").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-38079923",path:"/pages/84cb49/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-38079923").then(t)}},{path:"/pages/84cb49/index.html",redirect:"/pages/84cb49/"},{path:"/实战系统设计/01.设计基础设施/01.分布式缓存.html",redirect:"/pages/84cb49/"},{name:"v-0fe45e7f",path:"/pages/57d5a5/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-0fe45e7f").then(t)}},{path:"/pages/57d5a5/index.html",redirect:"/pages/57d5a5/"},{path:"/实战系统设计/01.设计基础设施/02.限流器.html",redirect:"/pages/57d5a5/"},{name:"v-1fc37214",path:"/pages/5dcb6b/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-1fc37214").then(t)}},{path:"/pages/5dcb6b/index.html",redirect:"/pages/5dcb6b/"},{path:"/实战系统设计/01.设计基础设施/03.热点探查（Top k）.html",redirect:"/pages/5dcb6b/"},{name:"v-6ee174df",path:"/pages/567090/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-6ee174df").then(t)}},{path:"/pages/567090/index.html",redirect:"/pages/567090/"},{path:"/实战系统设计/01.设计基础设施/04.消息队列.html",redirect:"/pages/567090/"},{name:"v-6d73d8ca",path:"/pages/8416e6/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-6d73d8ca").then(t)}},{path:"/pages/8416e6/index.html",redirect:"/pages/8416e6/"},{path:"/实战系统设计/01.设计基础设施/05.订阅发布.html",redirect:"/pages/8416e6/"},{name:"v-216630e2",path:"/pages/d81a42/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-216630e2").then(t)}},{path:"/pages/d81a42/index.html",redirect:"/pages/d81a42/"},{path:"/实战系统设计/01.设计基础设施/06.动态线程池.html",redirect:"/pages/d81a42/"},{name:"v-2ce111a7",path:"/pages/a95d7d/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-2ce111a7").then(t)}},{path:"/pages/a95d7d/index.html",redirect:"/pages/a95d7d/"},{path:"/实战系统设计/05.设计热门应用/01.设计 微信.html",redirect:"/pages/a95d7d/"},{name:"v-7923cebe",path:"/pages/90ad66/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-7923cebe").then(t)}},{path:"/pages/90ad66/index.html",redirect:"/pages/90ad66/"},{path:"/实战系统设计/05.设计热门应用/02.设计Twitter.html",redirect:"/pages/90ad66/"},{name:"v-36c2e4ce",path:"/pages/def08a/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-36c2e4ce").then(t)}},{path:"/pages/def08a/index.html",redirect:"/pages/def08a/"},{path:"/实战系统设计/07.经典场景设计/01.双写一致性.html",redirect:"/pages/def08a/"},{name:"v-b090e128",path:"/pages/af8752/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-b090e128").then(t)}},{path:"/pages/af8752/index.html",redirect:"/pages/af8752/"},{path:"/Redis 系统设计/05.五、集群/30.哨兵.html",redirect:"/pages/af8752/"},{name:"v-5ef7030e",path:"/pages/1e9e8e/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-5ef7030e").then(t)}},{path:"/pages/1e9e8e/index.html",redirect:"/pages/1e9e8e/"},{path:"/实战系统设计/07.经典场景设计/02.缓存穿透.html",redirect:"/pages/1e9e8e/"},{name:"v-bba36134",path:"/pages/1d96b2/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-bba36134").then(t)}},{path:"/pages/1d96b2/index.html",redirect:"/pages/1d96b2/"},{path:"/实战系统设计/07.经典场景设计/03.缓存击穿.html",redirect:"/pages/1d96b2/"},{name:"v-6f56ad33",path:"/pages/24abe0/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-6f56ad33").then(t)}},{path:"/pages/24abe0/index.html",redirect:"/pages/24abe0/"},{path:"/实战系统设计/07.经典场景设计/04.任务补偿.html",redirect:"/pages/24abe0/"},{name:"v-fa43c8ec",path:"/pages/a72629/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-fa43c8ec").then(t)}},{path:"/pages/a72629/index.html",redirect:"/pages/a72629/"},{path:"/实战系统设计/07.经典场景设计/05.秒杀.html",redirect:"/pages/a72629/"},{name:"v-a0c228a6",path:"/pages/8a57f2/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-a0c228a6").then(t)}},{path:"/pages/8a57f2/index.html",redirect:"/pages/8a57f2/"},{path:"/实战系统设计/07.经典场景设计/06.超卖.html",redirect:"/pages/8a57f2/"},{name:"v-330e9b4e",path:"/pages/51aa8b/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-330e9b4e").then(t)}},{path:"/pages/51aa8b/index.html",redirect:"/pages/51aa8b/"},{path:"/实战系统设计/07.经典场景设计/07.多级缓存.html",redirect:"/pages/51aa8b/"},{name:"v-5be28fe2",path:"/pages/4fc8cb/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-5be28fe2").then(t)}},{path:"/pages/4fc8cb/index.html",redirect:"/pages/4fc8cb/"},{path:"/实战系统设计/07.经典场景设计/09.幂等&防重.html",redirect:"/pages/4fc8cb/"},{name:"v-4971f274",path:"/pages/0dfb49/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-4971f274").then(t)}},{path:"/pages/0dfb49/index.html",redirect:"/pages/0dfb49/"},{path:"/实战系统设计/07.经典场景设计/08.超时&重试.html",redirect:"/pages/0dfb49/"},{name:"v-a4942194",path:"/pages/f3295f/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-a4942194").then(t)}},{path:"/pages/f3295f/index.html",redirect:"/pages/f3295f/"},{path:"/实战系统设计/07.经典场景设计/10.海量数据计数.html",redirect:"/pages/f3295f/"},{name:"v-5a65c5ca",path:"/pages/6b9d68/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-5a65c5ca").then(t)}},{path:"/pages/6b9d68/index.html",redirect:"/pages/6b9d68/"},{path:"/实战系统设计/07.经典场景设计/11.消息未读数系统.html",redirect:"/pages/6b9d68/"},{name:"v-4b29b46c",path:"/pages/9e787a/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-4b29b46c").then(t)}},{path:"/pages/9e787a/index.html",redirect:"/pages/9e787a/"},{path:"/实战系统设计/10.设计组件/01.高性能排行榜系统.html",redirect:"/pages/9e787a/"},{name:"v-b9edac60",path:"/pages/bbce3f/",component:Es,beforeEnter:(e,n,t)=>{pl("Layout","v-b9edac60").then(t)}},{path:"/pages/bbce3f/index.html",redirect:"/pages/bbce3f/"},{path:"/实战系统设计/10.设计组件/05.基于 Redis 实现延时消息.html",redirect:"/pages/bbce3f/"},{path:"*",component:Es}],Ss={title:"EchoDesign",description:"系统设计之「讲解 + 面试指南」，水滴石穿，设计无银弹！",base:"/",headTags:[["link",{rel:"icon",href:"/img/favicon.ico"}],["meta",{name:"theme-color",content:"#11a8cd"}]],pages:[{title:"Bloom Filter",frontmatter:{title:"Bloom Filter",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/fccd91/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.Bloom%20Filter.html",relativePath:"01.热门算法/01.热门算法/01.Bloom Filter.md",key:"v-5571592e",path:"/pages/fccd91/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:292},{level:3,title:"优点",slug:"优点",normalizedTitle:"优点",charIndex:467},{level:3,title:"缺点",slug:"缺点",normalizedTitle:"缺点",charIndex:599},{level:3,title:"使用场景",slug:"使用场景",normalizedTitle:"使用场景",charIndex:655},{level:2,title:"布隆过滤器的原理",slug:"布隆过滤器的原理",normalizedTitle:"布隆过滤器的原理",charIndex:936},{level:3,title:"数据结构",slug:"数据结构",normalizedTitle:"数据结构",charIndex:581},{level:3,title:"空间计算",slug:"空间计算",normalizedTitle:"空间计算",charIndex:1220},{level:3,title:"增加元素",slug:"增加元素",normalizedTitle:"增加元素",charIndex:1232},{level:3,title:"查询元素",slug:"查询元素",normalizedTitle:"查询元素",charIndex:484},{level:3,title:"修改元素",slug:"修改元素",normalizedTitle:"修改元素",charIndex:2138},{level:3,title:"删除元素",slug:"删除元素",normalizedTitle:"删除元素",charIndex:260},{level:2,title:"Redis 中的 布隆过滤器",slug:"redis-中的-布隆过滤器",normalizedTitle:"redis 中的 布隆过滤器",charIndex:2275},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:2294}],headersStr:"前言 优点 缺点 使用场景 布隆过滤器的原理 数据结构 空间计算 增加元素 查询元素 修改元素 删除元素 Redis 中的 布隆过滤器 参考文献",content:'提出问题是一切智慧的开端\n\n 1. 如何在超大规模数据集合中快速判断某个元素是否存在，同时避免耗费大量内存？\n 2. 你知道为什么布隆过滤器可以在不存储元素本身的情况下，提供高效的存在性查询吗？\n 3. 为什么布隆过滤器在一些应用场景中会产生误判，但仍然被广泛使用？\n 4. 布隆过滤器是如何通过节省空间来解决 Redis 缓存穿透问题的？\n 5. 如何根据预期的元素数量和允许的错误率，计算布隆过滤器的最佳大小和哈希函数个数？\n 6. 布隆过滤器在数据查询中如何做到 "一定不存在或可能存在" 的判定？\n 7. 为什么删除元素对于布隆过滤器来说是个挑战，是否有改进的方法？\n\n\n# 前言\n\n布隆过滤器（Bloom Filter）是 1970 年由布隆提出的。它实际上是 一个很长的二进制向量 和 一系列随机映射函数。布隆过滤器可以用于 检索一个元素是否在一个集合中。\n\n如果还是不太好理解的话，就可以把布隆过滤器理解为一个 set 集合，我们可以通过 add 往里面添加元素，通过 contains 来判断是否包含某个元素\n\n\n# 优点\n\n * 时间复杂度低，增加和查询元素的时间复杂为 O(N)，（N 为哈希函数的个数，通常情况比较小）\n * 保密性强，布隆过滤器不存储元素本身\n * 存储空间小，如果允许存在一定的误判，布隆过滤器是非常节省空间的（相比其他数据结构如 Set 集合）\n\n\n# 缺点\n\n * 有点一定的误判率，但是可以通过调整参数来降低\n * 无法获取元素本身\n * 很难删除元素\n\n\n# 使用场景\n\n布隆过滤器可以告诉我们 “某样东西一定不存在或者可能存在”，也就是说布隆过滤器说这个数不存在则一定不存，布隆过滤器说这个数存在则可能不存在（误判，后续会讲），利用这个判断是否存在的特点可以做很多有趣的事情。\n\n * 解决 Redis 缓存穿透问题（面试重点）\n * 邮件过滤，使用布隆过滤器来做邮件黑名单过滤\n * 对爬虫网址进行过滤，爬过的不再爬\n * 解决新闻推荐过的不再推荐(类似抖音刷过的往下滑动不再刷到)\n * HBase RocksDB LevelDB 等数据库内置布隆过滤器，用于判断数据是否存在，可以减少数据库的 IO 请求\n\n\n# 布隆过滤器的原理\n\n\n# 数据结构\n\n布隆过滤器它实际上是 一个很长的二进制向量 和 一系列随机映射函数。以 Redis 中的布隆过滤器实现为例，Redis 中的布隆过滤器底层是一个大型位数组（二进制数组）+多个无偏 hash 函数。\n\n\n\n多个无偏 hash 函数\n\n无偏 hash 函数就是能把元素的 hash 值计算的 比较均匀 的 hash 函数，能使得计算后的元素下标比较均匀的映射到位数组中。能有效减少误差。\n\n如下就是一个简单的布隆过滤器示意图，其中 k1、k2 代表增加的元素，a、b、c 即为无偏 hash 函数，最下层则为二进制数组。\n\n\n\n\n# 空间计算\n\n在布隆过滤器增加元素之前，首先需要初始化布隆过滤器的空间，也就是上面说的二进制数组，除此之外还需要计算无偏 hash 函数的个数。\n\n布隆过滤器提供了两个参数，分别是预计加入元素的大小 n，运行的错误率 p。\n\n布隆过滤器中有算法根据这两个参数会计算出二进制数组的大小 m，以及无偏 hash 函数的个数 k。\n\n它们之间的关系比较简单：\n\n如下地址是一个免费的在线布隆过滤器在线计算的网址：\n\n> https://krisives.github.io/bloom-calculator/\n\n\n\n\n# 增加元素\n\n往布隆过滤器增加元素，添加的 key 需要根据k个无偏 hash 函数计算得到多个 hash 值，然后对数组长度进行取模得到数组下标的位置，然后将对应数组下标的位置的值置为 1\n\n * 通过 k 个无偏 hash 函数计算得到 k 个 hash 值\n * 依次取模数组长度，得到数组索引\n * 将计算得到的数组索引下标位置数据修改为 1\n\n例如，key = Liziba，无偏hash函数的个数k=3，分别为 hash1、hash2、hash3。三个 hash 函数计算后得到三个数组下标值，并将其值修改为 1.\n\n如图所示\n\n\n\n\n# 查询元素\n\n布隆过滤器最大的用处就在于判断某样东西一定不存在或者可能存在，而这个就是查询元素的结果。其查询元素的过程如下：\n\n * 通过 k 个无偏 hash 函数计算得到 k 个 hash 值\n * 依次取模数组长度，得到数组索引\n * 判断索引处的值是否全部为 1，如果全部为 1 则存在（这种存在可能是误判），如果存在一个 0 则必定不存在\n\n关于误判，其实非常好理解，hash 函数再怎么牛逼，也无法完全避免 hash 冲突，也就是说可能会存在多个元素计算的 hash 值是相同的，那么它们取模数组长度后的到的数组索引也是相同的，这就是误判的原因。例如李子捌和李子柒的 hash 值取模后得到的数组索引都是 1，但其实这里只有李子捌，如果此时判断李子柒在不在这里，误判就出现啦！因此布隆过滤器最大的缺点误判只要知道其判断元素是否存在的原理就很容易明白了！\n\n\n# 修改元素\n\n不允许修改\n\n\n# 删除元素\n\n布隆过滤器对元素的删除不太支持，目前有一些变形的特定布隆过滤器支持元素的删除！关于为什么对删除不太支持，其实也非常好理解，hash 冲突必然存在，删除肯定是很苦难的！你将 A 的数组下标置为 0，那可能 B 也为受到影响\n\n\n# Redis 中的 布隆过滤器\n\n\n# 参考文献\n\n布隆(Bloom Filter)过滤器——全面讲解，建议收藏-CSDN 博客',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 如何在超大规模数据集合中快速判断某个元素是否存在，同时避免耗费大量内存？\n 2. 你知道为什么布隆过滤器可以在不存储元素本身的情况下，提供高效的存在性查询吗？\n 3. 为什么布隆过滤器在一些应用场景中会产生误判，但仍然被广泛使用？\n 4. 布隆过滤器是如何通过节省空间来解决 redis 缓存穿透问题的？\n 5. 如何根据预期的元素数量和允许的错误率，计算布隆过滤器的最佳大小和哈希函数个数？\n 6. 布隆过滤器在数据查询中如何做到 "一定不存在或可能存在" 的判定？\n 7. 为什么删除元素对于布隆过滤器来说是个挑战，是否有改进的方法？\n\n\n# 前言\n\n布隆过滤器（bloom filter）是 1970 年由布隆提出的。它实际上是 一个很长的二进制向量 和 一系列随机映射函数。布隆过滤器可以用于 检索一个元素是否在一个集合中。\n\n如果还是不太好理解的话，就可以把布隆过滤器理解为一个 set 集合，我们可以通过 add 往里面添加元素，通过 contains 来判断是否包含某个元素\n\n\n# 优点\n\n * 时间复杂度低，增加和查询元素的时间复杂为 o(n)，（n 为哈希函数的个数，通常情况比较小）\n * 保密性强，布隆过滤器不存储元素本身\n * 存储空间小，如果允许存在一定的误判，布隆过滤器是非常节省空间的（相比其他数据结构如 set 集合）\n\n\n# 缺点\n\n * 有点一定的误判率，但是可以通过调整参数来降低\n * 无法获取元素本身\n * 很难删除元素\n\n\n# 使用场景\n\n布隆过滤器可以告诉我们 “某样东西一定不存在或者可能存在”，也就是说布隆过滤器说这个数不存在则一定不存，布隆过滤器说这个数存在则可能不存在（误判，后续会讲），利用这个判断是否存在的特点可以做很多有趣的事情。\n\n * 解决 redis 缓存穿透问题（面试重点）\n * 邮件过滤，使用布隆过滤器来做邮件黑名单过滤\n * 对爬虫网址进行过滤，爬过的不再爬\n * 解决新闻推荐过的不再推荐(类似抖音刷过的往下滑动不再刷到)\n * hbase rocksdb leveldb 等数据库内置布隆过滤器，用于判断数据是否存在，可以减少数据库的 io 请求\n\n\n# 布隆过滤器的原理\n\n\n# 数据结构\n\n布隆过滤器它实际上是 一个很长的二进制向量 和 一系列随机映射函数。以 redis 中的布隆过滤器实现为例，redis 中的布隆过滤器底层是一个大型位数组（二进制数组）+多个无偏 hash 函数。\n\n\n\n多个无偏 hash 函数\n\n无偏 hash 函数就是能把元素的 hash 值计算的 比较均匀 的 hash 函数，能使得计算后的元素下标比较均匀的映射到位数组中。能有效减少误差。\n\n如下就是一个简单的布隆过滤器示意图，其中 k1、k2 代表增加的元素，a、b、c 即为无偏 hash 函数，最下层则为二进制数组。\n\n\n\n\n# 空间计算\n\n在布隆过滤器增加元素之前，首先需要初始化布隆过滤器的空间，也就是上面说的二进制数组，除此之外还需要计算无偏 hash 函数的个数。\n\n布隆过滤器提供了两个参数，分别是预计加入元素的大小 n，运行的错误率 p。\n\n布隆过滤器中有算法根据这两个参数会计算出二进制数组的大小 m，以及无偏 hash 函数的个数 k。\n\n它们之间的关系比较简单：\n\n如下地址是一个免费的在线布隆过滤器在线计算的网址：\n\n> https://krisives.github.io/bloom-calculator/\n\n\n\n\n# 增加元素\n\n往布隆过滤器增加元素，添加的 key 需要根据k个无偏 hash 函数计算得到多个 hash 值，然后对数组长度进行取模得到数组下标的位置，然后将对应数组下标的位置的值置为 1\n\n * 通过 k 个无偏 hash 函数计算得到 k 个 hash 值\n * 依次取模数组长度，得到数组索引\n * 将计算得到的数组索引下标位置数据修改为 1\n\n例如，key = liziba，无偏hash函数的个数k=3，分别为 hash1、hash2、hash3。三个 hash 函数计算后得到三个数组下标值，并将其值修改为 1.\n\n如图所示\n\n\n\n\n# 查询元素\n\n布隆过滤器最大的用处就在于判断某样东西一定不存在或者可能存在，而这个就是查询元素的结果。其查询元素的过程如下：\n\n * 通过 k 个无偏 hash 函数计算得到 k 个 hash 值\n * 依次取模数组长度，得到数组索引\n * 判断索引处的值是否全部为 1，如果全部为 1 则存在（这种存在可能是误判），如果存在一个 0 则必定不存在\n\n关于误判，其实非常好理解，hash 函数再怎么牛逼，也无法完全避免 hash 冲突，也就是说可能会存在多个元素计算的 hash 值是相同的，那么它们取模数组长度后的到的数组索引也是相同的，这就是误判的原因。例如李子捌和李子柒的 hash 值取模后得到的数组索引都是 1，但其实这里只有李子捌，如果此时判断李子柒在不在这里，误判就出现啦！因此布隆过滤器最大的缺点误判只要知道其判断元素是否存在的原理就很容易明白了！\n\n\n# 修改元素\n\n不允许修改\n\n\n# 删除元素\n\n布隆过滤器对元素的删除不太支持，目前有一些变形的特定布隆过滤器支持元素的删除！关于为什么对删除不太支持，其实也非常好理解，hash 冲突必然存在，删除肯定是很苦难的！你将 a 的数组下标置为 0，那可能 b 也为受到影响\n\n\n# redis 中的 布隆过滤器\n\n\n# 参考文献\n\n布隆(bloom filter)过滤器——全面讲解，建议收藏-csdn 博客',charsets:{cjk:!0},lastUpdated:"2024/09/15, 19:31:25",lastUpdatedTimestamp:1726428685e3},{title:"Consistent Hashing",frontmatter:{title:"Consistent Hashing",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/1e28a2/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/02.Consistent%20Hashing.html",relativePath:"01.热门算法/01.热门算法/02.Consistent Hashing.md",key:"v-9fff2bd8",path:"/pages/1e28a2/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:192},{level:3,title:"普通 hash算法 与 使用场景描述",slug:"普通-hash算法-与-使用场景描述",normalizedTitle:"普通 hash算法 与 使用场景描述",charIndex:199},{level:3,title:"缺陷",slug:"缺陷",normalizedTitle:"缺陷",charIndex:973},{level:2,title:"一致性哈希算法",slug:"一致性哈希算法",normalizedTitle:"一致性哈希算法",charIndex:83},{level:3,title:"什么是 一致性 hash 算法",slug:"什么是-一致性-hash-算法",normalizedTitle:"什么是 一致性 hash 算法",charIndex:1330},{level:3,title:"一致性 hash 算法的优点",slug:"一致性-hash-算法的优点",normalizedTitle:"一致性 hash 算法的优点",charIndex:2252},{level:3,title:"hash 环的倾斜与虚拟节点",slug:"hash-环的倾斜与虚拟节点",normalizedTitle:"hash 环的倾斜与虚拟节点",charIndex:2690},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:3089}],headersStr:"前言 普通 hash算法 与 使用场景描述 缺陷 一致性哈希算法 什么是 一致性 hash 算法 一致性 hash 算法的优点 hash 环的倾斜与虚拟节点 参考文献",content:"提出问题是一切智慧的开端\n\n * 当缓存服务器数量发生变化时，如何避免整个系统的缓存失效？\n * 为什么简单的取模算法在分布式缓存中容易导致缓存雪崩？\n * 如何通过一致性哈希算法，确保数据在新增或移除服务器时，尽量减少缓存的重定位？\n * 如果服务器太少，如何避免缓存数据分布不均，导致某台服务器压力过大？\n * 虚拟节点是如何帮助优化一致性哈希算法，解决数据倾斜问题的？\n\n\n# 前言\n\n\n# 普通 hash算法 与 使用场景描述\n\n在了解一致性哈希算法之前，我们先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。\n\n假设我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为 0号、1号、2号，现在有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存1万张左右的图片，那么我们应该怎样做呢？常见的做法是对缓存项的键进行哈希，将hash后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上\n\n我们举例说明，以刚才描述的场景为例，假设图片名称是不重复的，那我们就可以使用图片名称作为访问图片的key，使用如下公式，计算出图片应该存放在哪台服务器上。\n\nhash(图片名称) % N\n\n当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有3台服务器，使用哈希后的结果对3求余，那么余数一定是0、1或者2；如果求余的结果为0， 就把当前图片缓存在0号服务器上，如果余数为1，就缓存在1号服务器上，以此类推；同理，当我们访问任意图片时，只要再次对图片名称进行上述运算，即可得出图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将3万张图片随机的分布到3台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，我们暂时称上述算法为 HASH 算法或者取模算法，取模算法的过程可以用下图表示：\n\n\n# 缺陷\n\n上述 HASH 算法时，会出现一些缺陷：如果服务器已经不能满足缓存需求，就需要增加服务器数量，假设我们增加了一台缓存服务器，此时如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，最终导致所有缓存的位置都要发生改变，也就是说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据；同理，假设突然有一台缓存服务器出现了故障，那么我们则需要将故障机器移除，那么缓存服务器数量从3台变为2台，同样会导致大量缓存在同一时间失效，造成了缓存的雪崩，后端服务器将会承受巨大的压力，整个系统很有可能被压垮。为了解决这种情况，就有了一致性哈希算法。\n\n\n# 一致性哈希算法\n\n\n# 什么是 一致性 hash 算法\n\n一致性哈希算法也是使用取模的方法，但是取模算法是对服务器的数量进行取模，而一致性哈希算法是对 2^32 取模，具体步骤如下：\n\n 1. 一致性哈希算法将整个哈希值空间按照顺时针方向组织成一个虚拟的圆环，称为 Hash 环；\n 2. 接着将各个服务器使用 Hash 函数进行哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，从而确定每台机器在哈希环上的位置\n 3. 最后使用算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针寻找，第一台遇到的服务器就是其应该定位到的服务器\n\n下面我们使用具体案例说明一下一致性哈希算法的具体流程：\n\n步骤一：哈希环的组织\n\n我们将 2^32 想象成一个圆，像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由**2^32个点**组成的圆，示意图如下：\n\n\n\n圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1,也就是说0点左侧的第一个点代表2^32-1，我们把这个由 2^32 个点组成的圆环称为hash环。\n\n步骤二：确定服务器在哈希环的位置\n\n哈希算法：hash(服务器的IP) % 2^32\n\n上述公式的计算结果一定是 0 到 2^32-1 之间的整数，那么上图中的 hash 环上必定有一个点与这个整数对应，所以我们可以使用这个整数代表服务器，也就是服务器就可以映射到这个环上，假设我们有 ABC 三台服务器，那么它们在哈希环上的示意图如下：\n\n步骤三：将数据映射到哈希环上\n\n我们还是使用图片的名称作为 key，所以我们使用下面算法将图片映射在哈希环上：hash（图片名称） % 2^32，假设我们有4张图片，映射后的示意图如下，其中橘黄色的点表示图片：\n\n\n\n那么，怎么算出上图中的图片应该被缓存到哪一台服务上面呢？我们只要从图片的位置开始，沿顺时针方向遇到的第一个服务器就是图片存放的服务器了。最终，1号、2号图片将会被缓存到服务器A上，3号图片将会被缓存到服务器B上，4号图片将会被缓存到服务器C上。\n\n\n# 一致性 hash 算法的优点\n\n前面提到，如果简单对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，而使用一致性哈希算法就可以很好的解决这个问题，因为一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，只有部分缓存会失效，不至于将所有压力都在同一时间集中到后端服务器上，具有较好的容错性和可扩展性。\n\n假设服务器B出现了故障，需要将服务器B移除，那么移除前后的示意图如下图所示\n\n在服务器B未移除时，图片3应该被缓存到服务器B中，可是当服务器B移除以后，按照之前描述的一致性哈希算法的规则，图片3应该被缓存到服务器C中，因为从图片3的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器C，也就是说，如果服务器B出现故障被移除时，图片3的缓存位置会发生改变，但是，图片4仍然会被缓存到服务器C中，图片1与图片2仍然会被缓存到服务器A中，这与服务器B移除之前并没有任何区别，这就是一致性哈希算法的优点。\n\n\n# hash 环的倾斜与虚拟节点\n\n一致性哈希算法在服务节点太少的情况下，容易因为节点分部不均匀而造成数据倾斜问题，也就是被缓存的对象大部分集中缓存在某一台服务器上，从而出现数据分布不均匀的情况，这种情况就称为 hash 环的倾斜。如下图所示：\n\n\n\n上述左图为理想情况，右图为出现了数据倾斜的情况\n\nhash 环的倾斜在极端情况下，仍然有可能引起系统的崩溃，为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点，一个实际物理节点可以对应多个虚拟节点，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大，hash环倾斜所带来的影响就越小，同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射。具体做法可以在服务器ip或主机名的后面增加编号来实现，加入虚拟节点以后的hash环如下：\n\n\n# 参考文献\n\n一致性哈希算法原理详解-CSDN博客\n\n白话解析：一致性哈希算法 consistent hashing-朱双印博客 (zsythink.net)",normalizedContent:"提出问题是一切智慧的开端\n\n * 当缓存服务器数量发生变化时，如何避免整个系统的缓存失效？\n * 为什么简单的取模算法在分布式缓存中容易导致缓存雪崩？\n * 如何通过一致性哈希算法，确保数据在新增或移除服务器时，尽量减少缓存的重定位？\n * 如果服务器太少，如何避免缓存数据分布不均，导致某台服务器压力过大？\n * 虚拟节点是如何帮助优化一致性哈希算法，解决数据倾斜问题的？\n\n\n# 前言\n\n\n# 普通 hash算法 与 使用场景描述\n\n在了解一致性哈希算法之前，我们先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。\n\n假设我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为 0号、1号、2号，现在有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存1万张左右的图片，那么我们应该怎样做呢？常见的做法是对缓存项的键进行哈希，将hash后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上\n\n我们举例说明，以刚才描述的场景为例，假设图片名称是不重复的，那我们就可以使用图片名称作为访问图片的key，使用如下公式，计算出图片应该存放在哪台服务器上。\n\nhash(图片名称) % n\n\n当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有3台服务器，使用哈希后的结果对3求余，那么余数一定是0、1或者2；如果求余的结果为0， 就把当前图片缓存在0号服务器上，如果余数为1，就缓存在1号服务器上，以此类推；同理，当我们访问任意图片时，只要再次对图片名称进行上述运算，即可得出图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将3万张图片随机的分布到3台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，我们暂时称上述算法为 hash 算法或者取模算法，取模算法的过程可以用下图表示：\n\n\n# 缺陷\n\n上述 hash 算法时，会出现一些缺陷：如果服务器已经不能满足缓存需求，就需要增加服务器数量，假设我们增加了一台缓存服务器，此时如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，最终导致所有缓存的位置都要发生改变，也就是说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据；同理，假设突然有一台缓存服务器出现了故障，那么我们则需要将故障机器移除，那么缓存服务器数量从3台变为2台，同样会导致大量缓存在同一时间失效，造成了缓存的雪崩，后端服务器将会承受巨大的压力，整个系统很有可能被压垮。为了解决这种情况，就有了一致性哈希算法。\n\n\n# 一致性哈希算法\n\n\n# 什么是 一致性 hash 算法\n\n一致性哈希算法也是使用取模的方法，但是取模算法是对服务器的数量进行取模，而一致性哈希算法是对 2^32 取模，具体步骤如下：\n\n 1. 一致性哈希算法将整个哈希值空间按照顺时针方向组织成一个虚拟的圆环，称为 hash 环；\n 2. 接着将各个服务器使用 hash 函数进行哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，从而确定每台机器在哈希环上的位置\n 3. 最后使用算法定位数据访问到相应服务器：将数据key使用相同的函数hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针寻找，第一台遇到的服务器就是其应该定位到的服务器\n\n下面我们使用具体案例说明一下一致性哈希算法的具体流程：\n\n步骤一：哈希环的组织\n\n我们将 2^32 想象成一个圆，像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由**2^32个点**组成的圆，示意图如下：\n\n\n\n圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1,也就是说0点左侧的第一个点代表2^32-1，我们把这个由 2^32 个点组成的圆环称为hash环。\n\n步骤二：确定服务器在哈希环的位置\n\n哈希算法：hash(服务器的ip) % 2^32\n\n上述公式的计算结果一定是 0 到 2^32-1 之间的整数，那么上图中的 hash 环上必定有一个点与这个整数对应，所以我们可以使用这个整数代表服务器，也就是服务器就可以映射到这个环上，假设我们有 abc 三台服务器，那么它们在哈希环上的示意图如下：\n\n步骤三：将数据映射到哈希环上\n\n我们还是使用图片的名称作为 key，所以我们使用下面算法将图片映射在哈希环上：hash（图片名称） % 2^32，假设我们有4张图片，映射后的示意图如下，其中橘黄色的点表示图片：\n\n\n\n那么，怎么算出上图中的图片应该被缓存到哪一台服务上面呢？我们只要从图片的位置开始，沿顺时针方向遇到的第一个服务器就是图片存放的服务器了。最终，1号、2号图片将会被缓存到服务器a上，3号图片将会被缓存到服务器b上，4号图片将会被缓存到服务器c上。\n\n\n# 一致性 hash 算法的优点\n\n前面提到，如果简单对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，而使用一致性哈希算法就可以很好的解决这个问题，因为一致性hash算法对于节点的增减都只需重定位环空间中的一小部分数据，只有部分缓存会失效，不至于将所有压力都在同一时间集中到后端服务器上，具有较好的容错性和可扩展性。\n\n假设服务器b出现了故障，需要将服务器b移除，那么移除前后的示意图如下图所示\n\n在服务器b未移除时，图片3应该被缓存到服务器b中，可是当服务器b移除以后，按照之前描述的一致性哈希算法的规则，图片3应该被缓存到服务器c中，因为从图片3的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器c，也就是说，如果服务器b出现故障被移除时，图片3的缓存位置会发生改变，但是，图片4仍然会被缓存到服务器c中，图片1与图片2仍然会被缓存到服务器a中，这与服务器b移除之前并没有任何区别，这就是一致性哈希算法的优点。\n\n\n# hash 环的倾斜与虚拟节点\n\n一致性哈希算法在服务节点太少的情况下，容易因为节点分部不均匀而造成数据倾斜问题，也就是被缓存的对象大部分集中缓存在某一台服务器上，从而出现数据分布不均匀的情况，这种情况就称为 hash 环的倾斜。如下图所示：\n\n\n\n上述左图为理想情况，右图为出现了数据倾斜的情况\n\nhash 环的倾斜在极端情况下，仍然有可能引起系统的崩溃，为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点，一个实际物理节点可以对应多个虚拟节点，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大，hash环倾斜所带来的影响就越小，同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射。具体做法可以在服务器ip或主机名的后面增加编号来实现，加入虚拟节点以后的hash环如下：\n\n\n# 参考文献\n\n一致性哈希算法原理详解-csdn博客\n\n白话解析：一致性哈希算法 consistent hashing-朱双印博客 (zsythink.net)",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"Count-Min Sketch",frontmatter:{title:"Count-Min Sketch",date:"2024-09-14T13:30:19.000Z",permalink:"/pages/8624c5"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/03.Count-Min%20Sketch.html",relativePath:"01.热门算法/01.热门算法/03.Count-Min Sketch.md",key:"v-5f386e4e",path:"/pages/8624c5/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:389},{level:4,title:"hashmap 解决",slug:"hashmap-解决",normalizedTitle:"hashmap 解决",charIndex:460},{level:2,title:"CMS简介",slug:"cms简介",normalizedTitle:"cms简介",charIndex:1228},{level:4,title:"举个栗子",slug:"举个栗子",normalizedTitle:"举个栗子",charIndex:1419},{level:2,title:"CMS 的具体实现",slug:"cms-的具体实现",normalizedTitle:"cms 的具体实现",charIndex:1864},{level:2,title:"CMS 的参数选择",slug:"cms-的参数选择",normalizedTitle:"cms 的参数选择",charIndex:3294},{level:2,title:"Count-Mean-Min-Sketch",slug:"count-mean-min-sketch",normalizedTitle:"count-mean-min-sketch",charIndex:3546},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:4256}],headersStr:"前言 hashmap 解决 CMS简介 举个栗子 CMS 的具体实现 CMS 的参数选择 Count-Mean-Min-Sketch 参考文献",content:"提出问题是一切智慧的开端\n\n 1. 在面对亿级数据流时，如何在不占用过多内存的情况下，实时统计某个元素的出现频率？\n 2. 为什么在大数据场景下，使用普通的哈希表（HashMap）进行频率统计往往不可行？\n 3. Count-Min Sketch 是如何通过牺牲部分准确性，换取高效的频率查询能力的？\n 4. 你如何在 Count-Min Sketch 中确保低频数据的计数不受哈希冲突影响，保证统计结果的合理性？\n 5. 如何根据允许的误差范围和冲突概率，优化 Count-Min Sketch 的哈希函数数量和内存空间配置？\n 6. 在什么场景下 Count-Mean-Min Sketch 能有效改善 CMS 在处理长尾数据时的准确性？\n 7. 如果你正在构建一个需要实时统计用户访问频率的系统，如何在内存和准确性之间权衡选择 Count-Min Sketch？\n\n\n# 前言\n\n问题： 如果老板让你统计一个实时的数据流中元素出现的频率，并且准备随时回答某个元素出现的频率，不需要的精确的计数，那该怎么办？\n\n# hashmap 解决\n\n在大数据场景下，比如网页的 TopK 问题，爬虫的是否访问过的问题，都是一种出现频次相关的问题，那么在系统设计的时候，如何选择策略和数据结构去存储相关的数据是最高效合适的呢？\n\n计算元素的出现频次，如果出现与普通的场景下，简单的方案就是用 hashmap 来记录元素出现的次数：\n\n// 用HashMap存储元素及其频率\nHashMap<String, Integer> freq = new HashMap<>();\n\n// 统计频率\nfor (String e : elements) {\n    if (!freq.containsKey(e)) {\n    \tfreq.put(e, 1);\n    } else {\n    \tfreq.put(e, freq.get(e) + 1);\n    }\n}\n\n\n但是这种方式在大量数据流的情况下，如果存在大量唯一元素的情况下，会占用大量的内存，导致其无法应对大数据场景，因此在”时间换空间” 的策略选择中，这里就需要考虑通过时间，或者准确率等其他的因素来换空间。\n\n通常来说，针对大数据场景，会无限扩张的数据结构显然是不适用的，所以希望能用固定的空间来进行计数的管理，同时希望尽量不要影响到运行的时间，换言之，可以牺牲掉一定的准确性，来实现节省空间的效果。\n\n基于上述需求，我们可以想到 Hash 算法：将无限大的空间映射到固定的 size 的输出上；而大数据场景下的 Hash 会遇到冲突会被无限放大的问题\n\n如何解决冲突是最核心的问题\n\n * 基于概率数据结构实现的 Bloom Filter 算法采取多 Hash 的方法来减少冲突\n * 而其衍生出来的 CMS 算法以同样的思想，基于不同的设计，更为适应这种计数场景 下面介绍该方法的具体实现\n\n\n# CMS简介\n\nCount-min Sketch 算法是一个可以用来计数的算法，在数据大小非常大时，一种高效的计数算法，通过牺牲准确性提高的效率。\n\n * 是一个概率数据机构\n\n * 算法效率高\n * 提供计数上线\n\n其中，重要参数包括\n\n * Hash 哈希函数的数量： k\n * 计数表格列的数量： m\n * 内存中用空间： k x m x size of counter\n\n# 举个栗子\n\n我们规定一个 m=5，k=3 的Count-min Sketch，用来计数，其中所有hash函数如下\n\n\n\n注意，所有hash函数的结果需 mod m\n\n下面开始填表，首先初始状态为\n\n\n\n首先，向里面添加字母B，其ASCII码为66，求hash函数的结果为\n\n\n\n因此，表格变为\n\n\n\n接下来，我们查询字母A，其ASCII码为65，求hash函数的结果为\n\n\n\n用这个结果去读表，发现其对应位置均为0，因此字母A最多出现0次，这个值是准确的。\n\n然后，我们在查询字母G，其ASCII码为71，求hash函数的结果为\n\n\n\n用这个结果去读表，发现其对应位置均为1，因此字母G最多出现1次；**出错了！**我们从未向里面添加过字母G，这就是一次collision。Count-min Sketch的确会有这种问题，因为这个模型是从Bloom Filter衍生过来的。所以说Count-min Sketch是一个概率模型，返回的结果是一个上限值（upper-bound）。\n\n\n# CMS 的具体实现\n\n首先第一点，通过 hash 来实现数值空间的转换，通过哈希函数 H 将输入元素 x 映射到一维数组上，通过该 index 的值来判断元素的 Count（是否存在）\n\nfor (char x : input_element)\n{\n\tidx = Hash(x);\n\tarray[idx] += 1;\n}\n\n\n实际上这就是 Bloom Filter 的基础思想，然而无论是定长数组的”有限”还是 Hash 函数本身，都需要考虑冲突问题（两个元素被映射到同一个 index 上），冲突会导致 Count 比真实的大。\n\n于是接下来面临的问题就是：**如何降低冲突的概率？**如何提高计数的准确性（实际上也包含在降低冲突的概率中）\n\n可以参考 Bloom Filter 的策略，其通过多个 Hash 函数来映射同一个数，从而来降低元素的冲突概率（未考虑超大数据场景），进而也能提高计数的准确性，那么我们看一下 bloom filter 方法：\n\n> Bloom Filter 算法解决的是存在性问题，因此只需要一个 01 向量，当且仅当所有 Hash 计算出来的 index 的值都为 1 的时候，这个元素才可能存在；\n\n考虑将该方法向 Count 问题上迁移：\n\n * 计数过程中：使用 n 个 Hash 函数计算 idx{1~n} ，然后 vec[idx[i]] += 1 对count+1\n * 查询过程中：使用 n 个 Hash 函数计算 idx{1~n}，然后取 vec[idx[i]] 的最小值，考虑冲突场景可知，这个最小值>=实际的 count。\n\nint query_count = INT_MAX;\nfor (size_t i=0; i < function_size; ++i){\n\tint idx = Hash[i](query);\n\tint tmp_count = count_set[idx];\n\tquery_count = (tmp_count < query_count)? tmp_count: query_count;\n}\n\n\n实际上取多个 hash 的最小值就是 Count-Min Sketch 的核心，但如果仅是如此很明显有个问题，就是多个 hash 函数算出的多个 idx 会进一步的“污染”计数，得不偿失，导致 Count 更加不准确。\n\n实际上很容易想到，为了通过多个 hash 来减少冲突，并使得多 hash 的索引更加的唯一，最好的办法就是使得每个 hash 对应的计数空间是独立的，也就是将我们的计数空间在拓展成二维数组,其 size 为 depth × width 其中 depth 就代表 hash 函数的个数。\n\n那么假设每个 Hash 函数的冲突概率是 p~i~ 那么优化后的冲突概率就从 min(P~i~) 减小到\n\n\n\nfor (size_t i=0; i<function_size; ++i){\n\tint idx = Hash[i](query);\n\tint tmp_count = count_set[i][idx];\n\tquery_count = (tmp_count < query_count)? tmp_count: query_count;\n}\n\n\n结合了这个二维数组就是完整的 CMS 算法了，最终求得的 count 是实际 Count 的近似值（上界）。\n\n\n# CMS 的参数选择\n\n如果确定使用 CMS，接下来面对的就是计数的精度问题，那么如何选择这个数组的 shape 才能尽可能的减少误差呢？（很明显都是越大越好，那么怎么样是最优/达标的呢）\n\n确定一些变量参数：\n\n\n\n设定误差范围：\n\n\n\n以及结果在这个范围内的概率为:\n\n\n\n那么可以计算出：e 是自然常数\n\n\n\n计算公式来自论文，有效性分析也可以从论文中阅读\n\n> 添加一个新哈希函数以指数级别迅速降低超出边界异常数据的概率；当然，增加矩阵的宽度也可以增加减少冲突的概率，但这个只是线性级别。\n\n\n# Count-Mean-Min-Sketch\n\n由于 Hash 的冲突，CMS 对于低频的元素误差还是太大了，引入噪音对于高频元素可以接受（topk）但是对于低频长尾来说太不准确了，因此有了以下的改进：\n\n * 首先按照 CMS 的流程取出 d 个 sketch\n * 对于每个 hash 估计出一个噪音，噪音为该行的所有整数（除了被查询元素）的平均值\n * 该行的 sketch 减去该行的噪音，作为真正的 sketch\n * 返回 d 个 sketch 的中位数\n\nclass CountMeanMinSketch {\n    // initialization and addition procedures as in CountMinSketch\n    // n is total number of added elements\n    long estimateFrequency(value) {\n        long e[] = new long[d]\n        for(i = 0; i < d; i++) {\n            sketchCounter = estimators[i][ hash(value, i) ]\n            noiseEstimation = (n - sketchCounter) / (m - 1)\n            e[i] = sketchCounter – noiseEstimator\n        }\n        return median(e)\n    }\n}\n\n\n该算法显著改善了在长尾数据上的精确度。\n\n\n# 参考文献\n\nCount-min Sketch 算法 - 知乎 (zhihu.com)\n\nCount_Min Sketch算法 - AikenH Blogs",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 在面对亿级数据流时，如何在不占用过多内存的情况下，实时统计某个元素的出现频率？\n 2. 为什么在大数据场景下，使用普通的哈希表（hashmap）进行频率统计往往不可行？\n 3. count-min sketch 是如何通过牺牲部分准确性，换取高效的频率查询能力的？\n 4. 你如何在 count-min sketch 中确保低频数据的计数不受哈希冲突影响，保证统计结果的合理性？\n 5. 如何根据允许的误差范围和冲突概率，优化 count-min sketch 的哈希函数数量和内存空间配置？\n 6. 在什么场景下 count-mean-min sketch 能有效改善 cms 在处理长尾数据时的准确性？\n 7. 如果你正在构建一个需要实时统计用户访问频率的系统，如何在内存和准确性之间权衡选择 count-min sketch？\n\n\n# 前言\n\n问题： 如果老板让你统计一个实时的数据流中元素出现的频率，并且准备随时回答某个元素出现的频率，不需要的精确的计数，那该怎么办？\n\n# hashmap 解决\n\n在大数据场景下，比如网页的 topk 问题，爬虫的是否访问过的问题，都是一种出现频次相关的问题，那么在系统设计的时候，如何选择策略和数据结构去存储相关的数据是最高效合适的呢？\n\n计算元素的出现频次，如果出现与普通的场景下，简单的方案就是用 hashmap 来记录元素出现的次数：\n\n// 用hashmap存储元素及其频率\nhashmap<string, integer> freq = new hashmap<>();\n\n// 统计频率\nfor (string e : elements) {\n    if (!freq.containskey(e)) {\n    \tfreq.put(e, 1);\n    } else {\n    \tfreq.put(e, freq.get(e) + 1);\n    }\n}\n\n\n但是这种方式在大量数据流的情况下，如果存在大量唯一元素的情况下，会占用大量的内存，导致其无法应对大数据场景，因此在”时间换空间” 的策略选择中，这里就需要考虑通过时间，或者准确率等其他的因素来换空间。\n\n通常来说，针对大数据场景，会无限扩张的数据结构显然是不适用的，所以希望能用固定的空间来进行计数的管理，同时希望尽量不要影响到运行的时间，换言之，可以牺牲掉一定的准确性，来实现节省空间的效果。\n\n基于上述需求，我们可以想到 hash 算法：将无限大的空间映射到固定的 size 的输出上；而大数据场景下的 hash 会遇到冲突会被无限放大的问题\n\n如何解决冲突是最核心的问题\n\n * 基于概率数据结构实现的 bloom filter 算法采取多 hash 的方法来减少冲突\n * 而其衍生出来的 cms 算法以同样的思想，基于不同的设计，更为适应这种计数场景 下面介绍该方法的具体实现\n\n\n# cms简介\n\ncount-min sketch 算法是一个可以用来计数的算法，在数据大小非常大时，一种高效的计数算法，通过牺牲准确性提高的效率。\n\n * 是一个概率数据机构\n\n * 算法效率高\n * 提供计数上线\n\n其中，重要参数包括\n\n * hash 哈希函数的数量： k\n * 计数表格列的数量： m\n * 内存中用空间： k x m x size of counter\n\n# 举个栗子\n\n我们规定一个 m=5，k=3 的count-min sketch，用来计数，其中所有hash函数如下\n\n\n\n注意，所有hash函数的结果需 mod m\n\n下面开始填表，首先初始状态为\n\n\n\n首先，向里面添加字母b，其ascii码为66，求hash函数的结果为\n\n\n\n因此，表格变为\n\n\n\n接下来，我们查询字母a，其ascii码为65，求hash函数的结果为\n\n\n\n用这个结果去读表，发现其对应位置均为0，因此字母a最多出现0次，这个值是准确的。\n\n然后，我们在查询字母g，其ascii码为71，求hash函数的结果为\n\n\n\n用这个结果去读表，发现其对应位置均为1，因此字母g最多出现1次；**出错了！**我们从未向里面添加过字母g，这就是一次collision。count-min sketch的确会有这种问题，因为这个模型是从bloom filter衍生过来的。所以说count-min sketch是一个概率模型，返回的结果是一个上限值（upper-bound）。\n\n\n# cms 的具体实现\n\n首先第一点，通过 hash 来实现数值空间的转换，通过哈希函数 h 将输入元素 x 映射到一维数组上，通过该 index 的值来判断元素的 count（是否存在）\n\nfor (char x : input_element)\n{\n\tidx = hash(x);\n\tarray[idx] += 1;\n}\n\n\n实际上这就是 bloom filter 的基础思想，然而无论是定长数组的”有限”还是 hash 函数本身，都需要考虑冲突问题（两个元素被映射到同一个 index 上），冲突会导致 count 比真实的大。\n\n于是接下来面临的问题就是：**如何降低冲突的概率？**如何提高计数的准确性（实际上也包含在降低冲突的概率中）\n\n可以参考 bloom filter 的策略，其通过多个 hash 函数来映射同一个数，从而来降低元素的冲突概率（未考虑超大数据场景），进而也能提高计数的准确性，那么我们看一下 bloom filter 方法：\n\n> bloom filter 算法解决的是存在性问题，因此只需要一个 01 向量，当且仅当所有 hash 计算出来的 index 的值都为 1 的时候，这个元素才可能存在；\n\n考虑将该方法向 count 问题上迁移：\n\n * 计数过程中：使用 n 个 hash 函数计算 idx{1~n} ，然后 vec[idx[i]] += 1 对count+1\n * 查询过程中：使用 n 个 hash 函数计算 idx{1~n}，然后取 vec[idx[i]] 的最小值，考虑冲突场景可知，这个最小值>=实际的 count。\n\nint query_count = int_max;\nfor (size_t i=0; i < function_size; ++i){\n\tint idx = hash[i](query);\n\tint tmp_count = count_set[idx];\n\tquery_count = (tmp_count < query_count)? tmp_count: query_count;\n}\n\n\n实际上取多个 hash 的最小值就是 count-min sketch 的核心，但如果仅是如此很明显有个问题，就是多个 hash 函数算出的多个 idx 会进一步的“污染”计数，得不偿失，导致 count 更加不准确。\n\n实际上很容易想到，为了通过多个 hash 来减少冲突，并使得多 hash 的索引更加的唯一，最好的办法就是使得每个 hash 对应的计数空间是独立的，也就是将我们的计数空间在拓展成二维数组,其 size 为 depth × width 其中 depth 就代表 hash 函数的个数。\n\n那么假设每个 hash 函数的冲突概率是 p~i~ 那么优化后的冲突概率就从 min(p~i~) 减小到\n\n\n\nfor (size_t i=0; i<function_size; ++i){\n\tint idx = hash[i](query);\n\tint tmp_count = count_set[i][idx];\n\tquery_count = (tmp_count < query_count)? tmp_count: query_count;\n}\n\n\n结合了这个二维数组就是完整的 cms 算法了，最终求得的 count 是实际 count 的近似值（上界）。\n\n\n# cms 的参数选择\n\n如果确定使用 cms，接下来面对的就是计数的精度问题，那么如何选择这个数组的 shape 才能尽可能的减少误差呢？（很明显都是越大越好，那么怎么样是最优/达标的呢）\n\n确定一些变量参数：\n\n\n\n设定误差范围：\n\n\n\n以及结果在这个范围内的概率为:\n\n\n\n那么可以计算出：e 是自然常数\n\n\n\n计算公式来自论文，有效性分析也可以从论文中阅读\n\n> 添加一个新哈希函数以指数级别迅速降低超出边界异常数据的概率；当然，增加矩阵的宽度也可以增加减少冲突的概率，但这个只是线性级别。\n\n\n# count-mean-min-sketch\n\n由于 hash 的冲突，cms 对于低频的元素误差还是太大了，引入噪音对于高频元素可以接受（topk）但是对于低频长尾来说太不准确了，因此有了以下的改进：\n\n * 首先按照 cms 的流程取出 d 个 sketch\n * 对于每个 hash 估计出一个噪音，噪音为该行的所有整数（除了被查询元素）的平均值\n * 该行的 sketch 减去该行的噪音，作为真正的 sketch\n * 返回 d 个 sketch 的中位数\n\nclass countmeanminsketch {\n    // initialization and addition procedures as in countminsketch\n    // n is total number of added elements\n    long estimatefrequency(value) {\n        long e[] = new long[d]\n        for(i = 0; i < d; i++) {\n            sketchcounter = estimators[i][ hash(value, i) ]\n            noiseestimation = (n - sketchcounter) / (m - 1)\n            e[i] = sketchcounter – noiseestimator\n        }\n        return median(e)\n    }\n}\n\n\n该算法显著改善了在长尾数据上的精确度。\n\n\n# 参考文献\n\ncount-min sketch 算法 - 知乎 (zhihu.com)\n\ncount_min sketch算法 - aikenh blogs",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"LRU",frontmatter:{title:"LRU",date:"2024-09-14T16:39:57.000Z",permalink:"/pages/87589a"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/04.LRU.html",relativePath:"01.热门算法/01.热门算法/04.LRU.md",key:"v-7c6bc912",path:"/pages/87589a/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:335},{level:2,title:"基于 HashMap 和 双向链表 实现 LRU",slug:"基于-hashmap-和-双向链表-实现-lru",normalizedTitle:"基于 hashmap 和 双向链表 实现 lru",charIndex:764},{level:2,title:"Redis 如何实现 LRU",slug:"redis-如何实现-lru",normalizedTitle:"redis 如何实现 lru",charIndex:3212},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:3245},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:3777}],headersStr:"前言 基于 HashMap 和 双向链表 实现 LRU Redis 如何实现 LRU 总结 参考文献",content:"提出问题是一切智慧的开端\n\n 1. 当系统内存资源不足时，如何有效决定哪些数据应该被优先淘汰？\n 2. 为什么 LRU 算法能够在缓存系统中提供高效的数据置换策略？它的核心原理是什么？\n 3. 基于 HashMap 和双向链表实现的 LRU 缓存机制，是如何确保快速查找和淘汰数据的？\n 4. 如果缓存容量已满，LRU 是如何在插入新数据时，选择最久未使用的数据进行淘汰的？\n 5. 在实际项目中，使用 LRU 缓存时，你该如何选择缓存容量，以及衡量淘汰策略的有效性？\n 6. Redis 为什么没有使用标准的 LRU 算法，而是采用近似 LRU 算法来优化性能？\n 7. 如何通过合理配置 Redis 的内存淘汰策略，确保缓存系统在高并发下的稳定性和高效性？\n\n\n# 前言\n\nLRU 是 Least Recently Used 的缩写，即最近最少使用置换算法，最经典的场景是作为虚拟页式存储管理服务的，是根据页面调入内存后的使用情况进行决策了。由于无法预测各页面将来的使用情况，只能利用“最近的过去”作为“最近的将来”的近似，因此，LRU 算法就是将最近最久未使用的页面予以淘汰。\n\n操作系统课程里有学过，在内存不够的场景下，淘汰旧内容的策略。LRU … Least Recent Used，淘汰掉最不经常使用的。可以稍微多补充两句，因为计算机体系结构中，最大的最可靠的存储是硬盘，它容量很大，并且内容可以固化，但是访问速度很慢，所以需要把使用的内容载入内存中；内存速度很快，但是容量有限，并且断电后内容会丢失，并且为了进一步提升性能，还有 CPU 内部的 L1 Cache，L2 Cache 等概念。因为速度越快的地方，它的单位成本越高，容量越小，新的内容不断被载入，旧的内容肯定要被淘汰，所以就有这样的使用背景。\n\n\n# 基于 HashMap 和 双向链表 实现 LRU\n\n146. LRU 缓存 - 力扣（LeetCode）\n\npublic class LRUCache {\n    class DLinkedNode {\n        int key;\n        int value;\n        DLinkedNode prev;\n        DLinkedNode next;\n        public DLinkedNode() {}\n        public DLinkedNode(int _key, int _value) {key = _key; value = _value;}\n    }\n\n    private Map<Integer, DLinkedNode> cache = new HashMap<Integer, DLinkedNode>();\n    private int size;\n    private int capacity;\n    private DLinkedNode head, tail;\n\n    public LRUCache(int capacity) {\n        this.size = 0;\n        this.capacity = capacity;\n        // 使用伪头部和伪尾部节点\n        head = new DLinkedNode();\n        tail = new DLinkedNode();\n        head.next = tail;\n        tail.prev = head;\n    }\n\n    public int get(int key) {\n        DLinkedNode node = cache.get(key);\n        if (node == null) {\n            return -1;\n        }\n        // 如果 key 存在，先通过哈希表定位，再移到头部\n        moveToHead(node);\n        return node.value;\n    }\n\n    public void put(int key, int value) {\n        DLinkedNode node = cache.get(key);\n        if (node == null) {\n            // 如果 key 不存在，创建一个新的节点\n            DLinkedNode newNode = new DLinkedNode(key, value);\n            // 添加进哈希表\n            cache.put(key, newNode);\n            // 添加至双向链表的头部\n            addToHead(newNode);\n            ++size;\n            if (size > capacity) {\n                // 如果超出容量，删除双向链表的尾部节点\n                DLinkedNode tail = removeTail();\n                // 删除哈希表中对应的项\n                cache.remove(tail.key);\n                --size;\n            }\n        }\n        else {\n            // 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部\n            node.value = value;\n            moveToHead(node);\n        }\n    }\n\n    private void addToHead(DLinkedNode node) {\n        node.prev = head;\n        node.next = head.next;\n        head.next.prev = node;\n        head.next = node;\n    }\n\n    private void removeNode(DLinkedNode node) {\n        node.prev.next = node.next;\n        node.next.prev = node.prev;\n    }\n\n    private void moveToHead(DLinkedNode node) {\n        removeNode(node);\n        addToHead(node);\n    }\n\n    private DLinkedNode removeTail() {\n        DLinkedNode res = tail.prev;\n        removeNode(res);\n        return res;\n    }\n}\n\n\nLRU 算法的执行，可以分成三种情况来掌握\n\n * 当有新数据插入时，LRU 算法会把该数据插入到链表头部，同时把原来链表头部的数据及其之后的数据，都向尾部移动一位。\n * 当有数据刚被访问了一次之后，LRU 算法就会把该数据从它在链表中的当前位置，移动到链表头部。同时，把从链表头部到它当前位置的其他数据，都向尾部移动一位。\n * 情况三：当链表长度无法再容纳更多数据时，若再有新数据插入，LRU 算法就会去除链表尾部的数据，这也相当于将数据从缓存中淘汰掉。\n\n\n# Redis 如何实现 LRU\n\nRedis 中的 LRU\n\n\n# 总结\n\n你现在应该知道了 Redis 是如何实现 LRU 算法来进行缓存数据替换的。其中，我们根据 LRU 算法的基本原理，可以发现如果严格按照原理来实现 LRU 算法，那么开发的系统就需要用额外的内存空间来保存 LRU 链表，而且系统运行时也会受到 LRU 链表操作的开销影响。\n\n而对于 Redis 来说，内存资源和性能都很重要，所以 Redis 实现了近似 LRU 算法。而为了实现近似 LRU 算法，Redis 首先是设置了全局 LRU 时钟，并在键值对创建时获取全局 LRU 时钟值作为访问时间戳，以及在每次访问时获取全局 LRU 时钟值，更新访问时间戳。\n\n然后，当 Redis 每处理一个命令时，都会调用 freeMemoryIfNeeded 函数来判断是否需要释放内存。如果已使用内存超出了 maxmemory，那么，近似 LRU 算法就会随机选择一些键值对，组成待淘汰候选集合，并根据它们的访问时间戳，选出最旧的数据，将其淘汰。\n\nRedis 计算实例内存时，不会把「主从复制」的缓冲区计算在内，也就是说不管一个实例后面挂了多少个从库，主库不会把主从复制所需的「缓冲区」内存，计算到实例内存中，即这部分内存增加，不会对数据淘汰产生影响。\n\n\n# 参考文献\n\nRedis 源码剖析与实战 (geekbang.org)",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 当系统内存资源不足时，如何有效决定哪些数据应该被优先淘汰？\n 2. 为什么 lru 算法能够在缓存系统中提供高效的数据置换策略？它的核心原理是什么？\n 3. 基于 hashmap 和双向链表实现的 lru 缓存机制，是如何确保快速查找和淘汰数据的？\n 4. 如果缓存容量已满，lru 是如何在插入新数据时，选择最久未使用的数据进行淘汰的？\n 5. 在实际项目中，使用 lru 缓存时，你该如何选择缓存容量，以及衡量淘汰策略的有效性？\n 6. redis 为什么没有使用标准的 lru 算法，而是采用近似 lru 算法来优化性能？\n 7. 如何通过合理配置 redis 的内存淘汰策略，确保缓存系统在高并发下的稳定性和高效性？\n\n\n# 前言\n\nlru 是 least recently used 的缩写，即最近最少使用置换算法，最经典的场景是作为虚拟页式存储管理服务的，是根据页面调入内存后的使用情况进行决策了。由于无法预测各页面将来的使用情况，只能利用“最近的过去”作为“最近的将来”的近似，因此，lru 算法就是将最近最久未使用的页面予以淘汰。\n\n操作系统课程里有学过，在内存不够的场景下，淘汰旧内容的策略。lru … least recent used，淘汰掉最不经常使用的。可以稍微多补充两句，因为计算机体系结构中，最大的最可靠的存储是硬盘，它容量很大，并且内容可以固化，但是访问速度很慢，所以需要把使用的内容载入内存中；内存速度很快，但是容量有限，并且断电后内容会丢失，并且为了进一步提升性能，还有 cpu 内部的 l1 cache，l2 cache 等概念。因为速度越快的地方，它的单位成本越高，容量越小，新的内容不断被载入，旧的内容肯定要被淘汰，所以就有这样的使用背景。\n\n\n# 基于 hashmap 和 双向链表 实现 lru\n\n146. lru 缓存 - 力扣（leetcode）\n\npublic class lrucache {\n    class dlinkednode {\n        int key;\n        int value;\n        dlinkednode prev;\n        dlinkednode next;\n        public dlinkednode() {}\n        public dlinkednode(int _key, int _value) {key = _key; value = _value;}\n    }\n\n    private map<integer, dlinkednode> cache = new hashmap<integer, dlinkednode>();\n    private int size;\n    private int capacity;\n    private dlinkednode head, tail;\n\n    public lrucache(int capacity) {\n        this.size = 0;\n        this.capacity = capacity;\n        // 使用伪头部和伪尾部节点\n        head = new dlinkednode();\n        tail = new dlinkednode();\n        head.next = tail;\n        tail.prev = head;\n    }\n\n    public int get(int key) {\n        dlinkednode node = cache.get(key);\n        if (node == null) {\n            return -1;\n        }\n        // 如果 key 存在，先通过哈希表定位，再移到头部\n        movetohead(node);\n        return node.value;\n    }\n\n    public void put(int key, int value) {\n        dlinkednode node = cache.get(key);\n        if (node == null) {\n            // 如果 key 不存在，创建一个新的节点\n            dlinkednode newnode = new dlinkednode(key, value);\n            // 添加进哈希表\n            cache.put(key, newnode);\n            // 添加至双向链表的头部\n            addtohead(newnode);\n            ++size;\n            if (size > capacity) {\n                // 如果超出容量，删除双向链表的尾部节点\n                dlinkednode tail = removetail();\n                // 删除哈希表中对应的项\n                cache.remove(tail.key);\n                --size;\n            }\n        }\n        else {\n            // 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部\n            node.value = value;\n            movetohead(node);\n        }\n    }\n\n    private void addtohead(dlinkednode node) {\n        node.prev = head;\n        node.next = head.next;\n        head.next.prev = node;\n        head.next = node;\n    }\n\n    private void removenode(dlinkednode node) {\n        node.prev.next = node.next;\n        node.next.prev = node.prev;\n    }\n\n    private void movetohead(dlinkednode node) {\n        removenode(node);\n        addtohead(node);\n    }\n\n    private dlinkednode removetail() {\n        dlinkednode res = tail.prev;\n        removenode(res);\n        return res;\n    }\n}\n\n\nlru 算法的执行，可以分成三种情况来掌握\n\n * 当有新数据插入时，lru 算法会把该数据插入到链表头部，同时把原来链表头部的数据及其之后的数据，都向尾部移动一位。\n * 当有数据刚被访问了一次之后，lru 算法就会把该数据从它在链表中的当前位置，移动到链表头部。同时，把从链表头部到它当前位置的其他数据，都向尾部移动一位。\n * 情况三：当链表长度无法再容纳更多数据时，若再有新数据插入，lru 算法就会去除链表尾部的数据，这也相当于将数据从缓存中淘汰掉。\n\n\n# redis 如何实现 lru\n\nredis 中的 lru\n\n\n# 总结\n\n你现在应该知道了 redis 是如何实现 lru 算法来进行缓存数据替换的。其中，我们根据 lru 算法的基本原理，可以发现如果严格按照原理来实现 lru 算法，那么开发的系统就需要用额外的内存空间来保存 lru 链表，而且系统运行时也会受到 lru 链表操作的开销影响。\n\n而对于 redis 来说，内存资源和性能都很重要，所以 redis 实现了近似 lru 算法。而为了实现近似 lru 算法，redis 首先是设置了全局 lru 时钟，并在键值对创建时获取全局 lru 时钟值作为访问时间戳，以及在每次访问时获取全局 lru 时钟值，更新访问时间戳。\n\n然后，当 redis 每处理一个命令时，都会调用 freememoryifneeded 函数来判断是否需要释放内存。如果已使用内存超出了 maxmemory，那么，近似 lru 算法就会随机选择一些键值对，组成待淘汰候选集合，并根据它们的访问时间戳，选出最旧的数据，将其淘汰。\n\nredis 计算实例内存时，不会把「主从复制」的缓冲区计算在内，也就是说不管一个实例后面挂了多少个从库，主库不会把主从复制所需的「缓冲区」内存，计算到实例内存中，即这部分内存增加，不会对数据淘汰产生影响。\n\n\n# 参考文献\n\nredis 源码剖析与实战 (geekbang.org)",charsets:{cjk:!0},lastUpdated:"2024/09/16, 06:30:11",lastUpdatedTimestamp:1726468211e3},{title:"LFU",frontmatter:{title:"LFU",date:"2024-09-14T18:14:42.000Z",permalink:"/pages/7d22be/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/05.LFU.html",relativePath:"01.热门算法/01.热门算法/05.LFU.md",key:"v-9181d446",path:"/pages/7d22be/",headers:[{level:3,title:"Redis 如何实现 LFU",slug:"redis-如何实现-lfu",normalizedTitle:"redis 如何实现 lfu",charIndex:2}],headersStr:"Redis 如何实现 LFU",content:"# Redis 如何实现 LFU\n\nRedis 中的 LFU",normalizedContent:"# redis 如何实现 lfu\n\nredis 中的 lfu",charsets:{cjk:!0},lastUpdated:"2024/09/16, 06:30:11",lastUpdatedTimestamp:1726468211e3},{title:"hash & rehash",frontmatter:{title:"hash & rehash",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d43d1/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/06.hash%20&%20rehash.html",relativePath:"01.热门算法/01.热门算法/06.hash & rehash.md",key:"v-bbe08a1e",path:"/pages/2d43d1/",headersStr:null,content:"Redis 中的 Hash",normalizedContent:"redis 中的 hash",charsets:{cjk:!0},lastUpdated:"2024/09/16, 09:48:07",lastUpdatedTimestamp:1726480087e3},{title:"Timing Wheels",frontmatter:{title:"Timing Wheels",date:"2024-09-15T02:25:42.000Z",permalink:"/pages/44dcc2/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/10.Timing%20Wheels.html",relativePath:"01.热门算法/01.热门算法/10.Timing Wheels.md",key:"v-9cd213c0",path:"/pages/44dcc2/",headers:[{level:2,title:"带着疑问",slug:"带着疑问",normalizedTitle:"带着疑问",charIndex:295},{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:1520},{level:2,title:"添加定时任务",slug:"添加定时任务",normalizedTitle:"添加定时任务",charIndex:2073},{level:2,title:'"动态"时间轮',slug:"动态-时间轮",normalizedTitle:"&quot;动态&quot;时间轮",charIndex:null},{level:3,title:"复用时间格",slug:"复用时间格",normalizedTitle:"复用时间格",charIndex:2688},{level:3,title:"时间轮升级",slug:"时间轮升级",normalizedTitle:"时间轮升级",charIndex:2937},{level:3,title:"层级时间轮",slug:"层级时间轮",normalizedTitle:"层级时间轮",charIndex:3225},{level:3,title:"添加定时任务",slug:"添加定时任务-2",normalizedTitle:"添加定时任务",charIndex:2073},{level:3,title:'"动态"层级时间轮',slug:"动态-层级时间轮",normalizedTitle:"&quot;动态&quot;层级时间轮",charIndex:null},{level:3,title:"时间轮降级",slug:"时间轮降级",normalizedTitle:"时间轮降级",charIndex:4189},{level:3,title:"时间轮的推进",slug:"时间轮的推进",normalizedTitle:"时间轮的推进",charIndex:4625},{level:2,title:"时间轮在 Kafka 中的实现",slug:"时间轮在-kafka-中的实现",normalizedTitle:"时间轮在 kafka 中的实现",charIndex:4873},{level:3,title:"时间轮的数据结构",slug:"时间轮的数据结构",normalizedTitle:"时间轮的数据结构",charIndex:5242},{level:3,title:"时间轮中的任务存放",slug:"时间轮中的任务存放",normalizedTitle:"时间轮中的任务存放",charIndex:6065},{level:3,title:"时间轮的升降级",slug:"时间轮的升降级",normalizedTitle:"时间轮的升降级",charIndex:6572},{level:3,title:"任务添加和驱动时间轮滚动核心流程图",slug:"任务添加和驱动时间轮滚动核心流程图",normalizedTitle:"任务添加和驱动时间轮滚动核心流程图",charIndex:7828},{level:3,title:"重点代码介绍",slug:"重点代码介绍",normalizedTitle:"重点代码介绍",charIndex:7852},{level:3,title:"DelayQueue 与 kafka 时间轮",slug:"delayqueue-与-kafka-时间轮",normalizedTitle:"delayqueue 与 kafka 时间轮",charIndex:10994},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:11690},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:11914}],headersStr:'带着疑问 前言 添加定时任务 "动态"时间轮 复用时间格 时间轮升级 层级时间轮 添加定时任务 "动态"层级时间轮 时间轮降级 时间轮的推进 时间轮在 Kafka 中的实现 时间轮的数据结构 时间轮中的任务存放 时间轮的升降级 任务添加和驱动时间轮滚动核心流程图 重点代码介绍 DelayQueue 与 kafka 时间轮 总结 参考文献',content:'提出问题是一切智慧的开端\n\n 1. 如何在高并发场景下高效管理数十万个定时任务，而不会造成性能瓶颈？\n 2. 为什么常规的定时任务扫描方式效率低下，时间轮如何通过环形队列提升任务调度的性能？\n 3. 当定时任务的延迟时间超过了当前时间轮的范围时，时间轮是如何通过层级机制处理这种复杂情况的？\n 4. 如何避免时间轮在处理少量定时任务时的“空推进”问题，从而提升资源利用率？\n 5. Kafka 等高性能系统为什么选择时间轮来管理延迟任务，而不使用JDK自带的Timer或DelayQueue？\n 6. 在一个拥有数百万级别任务的系统中，如何通过多层时间轮实现任务的精确调度和执行？\n\n\n# 带着疑问\n\n第一个问题：如果一台机器上有 10w 个定时任务，如何做到高效触发？\n\n具体场景是：\n\n> 有一个 APP 实时消息通道系统，对每个用户会维护一个 APP 到服务器的 TCP 连接，用来实时收发消息，对这个 TCP 连接，有这样一个需求：“如果连续 30s 没有请求包（例如登录，消息，keepalive 包），服务端就要将这个用户的状态置为离线”。\n> \n> 其中，单机 TCP 同时在线量约在 10w 级别，keepalive 请求包较分散大概 30s 一次，吞吐量约在 3000qps。\n\n怎么做？\n\n常用方案使用 time 定时任务，每秒扫描一次所有连接的集合 Map<uid, last_packet_time>，把连接时间（每次有新的请求更新对应连接的连接时间）比当前时间的差值大 30s 的连接找出来处理。\n\n另一种方案，使用环形队列法：\n\n三个重要的数据结构：\n\n 1. 30s 超时，就创建一个 index 从 0 到 30 的环形队列（本质是个数组）\n 2. 环上每一个 slot 是一个 Set，任务集合\n 3. 同时还有一个 Map<uid, index>，记录 uid 落在环上的哪个 slot 里\n\n这样当有某用户 uid 有请求包到达时：\n\n 1. 从 Map 结构中，查找出这个 uid 存储在哪一个 slot 里\n 2. 从这个 slot 的 Set 结构中，删除这个 uid\n 3. 将 uid 重新加入到新的 slot 中，具体是哪一个 slot 呢 => Current Index 指针所指向的上一个 slot，因为这个 slot，会被 timer 在 30s 之后扫描到\n 4. 更新 Map，这个 uid 对应 slot 的 index 值\n\n哪些元素会被超时掉呢？\n\nCurrent Index 每秒种移动一个 slot，这个 slot 对应的 Set中所有 uid 都应该被集体超时！如果最近 30s 有请求包来到，一定被放到 Current Index 的前一个 slot 了，Current Index 所在的 slot 对应 Set 中所有元素，都是最近 30s 没有请求包来到的。\n\n所以，当没有超时时，Current Index 扫到的每一个 slot 的 Set 中应该都没有元素。\n\n两种方案对比：\n\n方案一每次都要轮询所有数据，而方案二使用环形队列只需要轮询这一刻需要过期的数据，如果没有数据过期则没有数据要处理，并且是批量超时，并且由于是环形结构更加节约空间，这很适合高性能场景。\n\n第二个问题： 在开发过程中有延迟一定时间的任务要执行，怎么做？\n\n如果不重复造轮子的话，我们的选择当然是延迟队列或者 Timer。\n\n延迟队列和在 Timer 中增 加延时任务采用数组表示的最小堆的数据结构实现，每次放入新元素和移除队首元素时间复杂度为 O(nlog(n))。\n\n\n# 前言\n\n时间轮，是一种实现延迟功能（定时器）的巧妙算法，在 Netty，Zookeeper，Kafka 等各种框架中，甚至Linux内核中都有用到。\n\n设计源于生活\n\n时间轮，其设计正是来源于生活中的时钟。\n\n如图就是一个简单的时间轮：\n\n\n\n图中大圆的圆心位置表示的是当前的时间，随着时间推移, 圆心处的时间也会不断跳动。\n\n下面我们对着这个图，来说说Kafka的时间轮TimingWheel。\n\nKafka时间轮的底层就是一个环形数组，而数组中每个元素都存放一个双向链表TimerTaskList，链表中封装了很多延时任务。\n\nKafka中一个时间轮TimingWheel是由20个时间格组成，wheelSize = 20；每格的时间跨度是1ms，tickMs = 1ms。参照Kafka，上图中也用了20个灰边小圆表示时间格，为了动画演示可以看得清楚，我们这里每个小圆的时间跨度是1s。\n\n所以现在整个时间轮的时间跨度就是 tickMs * wheelSize ，也就是 20s。从0s到19s，我们都分别有一个灰边小圆来承载。\n\nKafka的时间轮还有一个表盘指针 currentTime，表示时间轮当前所处的时间。也就是图中用黑色粗线表示的圆，随着时间推移, 这个指针也会不断前进;\n\n\n\n\n# 添加定时任务\n\n有了时间轮，现在可以往里面添加定时任务了。我们用一个粉红色的小圆来表示一个定时任务。\n\n\n\n这里先讲一下设定，每一个定时任务都有延时时间delayTime，和过期时间ExpiredTime。比如当前时间是10s，我们添加了个延时时间为2s的任务，那么这个任务的过期时间就是12s，也就是当前时间10s再走两秒，变成了12s的时候，就到了触发这个定时任务的时间。\n\n而时间轮上代表时间格的灰边小圆上显示的数字，可以理解为任务的过期时间。\n\n\n\n讲清楚这些设定后，我们就开始添加定时任务吧。\n\n初始的时候, 时间轮的指针定格在0。此时添加一个超时时间为2s的任务, 那么这个任务将会插入到第二个时间格中。\n\n\n\n当时间轮的指针到达第二个时间格时, 会处理该时间格上对应的任务。在动画上就是让红色的小圆消失!\n\n\n\n如果这个时候又插入一个延时时间为8s的任务进来, 这个任务的过期时间就是在当前时间2s的基础上加8s, 也就是10s, 那么这个任务将会插入到过期时间为10s的时间格中。\n\n\n\n\n# "动态"时间轮\n\n到目前为止，一切都很好理解。\n\n那么如果在当前时间是2s的时候, 插入一个延时时间为19s的任务时,这个任务的过期时间就是在当前时间2s的基础上加19s, 也就是21s。\n\n请看下图，当前的时间轮是没有过期时间为21s的时间格。这个任务将会插入到过期时间为1s的时间格中，这是怎么回事呢？\n\n\n\n\n# 复用时间格\n\n为了解答上面的问题，我们先来点魔法， 让时间轮上的时间都动起来！\n\n\n\n其实呢，当指针定格在2s的位置时, 时间格0s, 1s和2s就已经是过期的时间格。\n\n也就是说指针可以用来划分过期的时间格[0,2]和未来的时间格 [3,19]。而过期的时间格可以继续复用。比如过期的时间格0s就变成了20s, 存放过期时间为20s的任务。\n\n理解了时间格的复用之后，再看回刚刚的例子，当前时间是2s时，添加延时时间为19s的任务，那么这个任务就会插入到过期时间为21s的时间格中。\n\n\n\n\n# 时间轮升级\n\n下面，新的问题来了，请坐好扶稳。\n\n如果在当前时间是2s的时候, 插入一个延时时间为22s的任务, 这个任务的过期时间就是在2s的基础上加22s，也就是24s。\n\n\n\n显然当前时间轮是无法找到过期时间格为24秒的时间格，因为当前过期时间最大的时间格才到21s。而且我们也没办法像前面那样再复用时间格，因为除了过期时间为2s的时间格，其他的时间格都还没过期呢。当前时间轮无法承载这个定时任务,那么应该怎么办呢?\n\n当然我们可以选择扩展时间轮上的时间格, 但是这样一来，时间轮就失去了意义。\n\n是时候要升级时间轮了！\n\n我们先来理解下多层时间轮之间的联系。\n\n\n# 层级时间轮\n\n如图是一个两层的时间轮:\n\n\n\n第二层时间轮也是由20个时间格组成, 每个时间格的跨度是20s。\n\n图中展示了每个时间格对应的过期时间范围, 我们可以清晰地看到, 第二层时间轮的第0个时间格的过期时间范围是 [0,19]。也就是说, 第二层时间轮的一个时间格就可以表示第一层时间轮的所有(20个)时间格;\n\n为了进一步理清第一层时间轮和第二层时间轮的关系, 我们拉着时间的小手, 一起观看下面的动图:\n\n\n\n可以看到，第二层时间轮同样也有自己的指针, 每当第一层时间轮走完一个周期，第二层时间轮的指针就会推进一格。\n\n\n# 添加定时任务\n\n回到一开始的问题，在当前时间是2s的时候, 插入一个延时时间为22s的任务，该任务过期时间为24s。\n\n\n\n当第一层时间轮容纳不下时，进入第二层时间轮，并插入到过期时间为[20,39]的时间格中。\n\n我们再来个例子，如果在当前时间是2s的时候, 插入一个延时时间为350s的任务, 这个任务的过期时间就是在2s的基础上加350s，也就是352s。\n\n\n\n从图中可以看到，该任务插入到第二层时间轮过期时间为[340,359]s的时间格中，也就是第17格的位置。\n\n\n# "动态"层级时间轮\n\n通常来说, 第二层时间轮的第0个时间格是用来表示第一层时间轮的, 这一格是存放不了任务的, 因为超时时间0-20s的任务, 第一层时间轮就可以处理了。\n\n但是! 事情往往没这么简单, 我们时间轮上的时间格都是可以复用的! 那么这在第二层时间轮上又是怎么体现的呢?\n\n下面是魔法时间， 我们让时间轮上的过期时间都动起来！\n\n\n\n从图中可以看到，当第一层时间轮的指针定格在1s时，超时时间0s的时间格就过期了。而这个时候，第二层时间轮第0个时间格的时间范围就从[0,19]分为了过期的[0],和未过期的[1,19]。而过期的[0]就会被新的过期时间[400]复用。\n\n[0-19]\n\n[400][1,19]\n\n[400,401][2,19]\n\n......\n\n[400,419]\n\n\n所以，如果在当前时间是2s的时候, 插入一个延时时间为399s的任务, 这个任务的过期时间就是在2s的基础上加399s，也就是401s。如图，这个任务还是会插到第二层时间轮第0个时间格中去。\n\n\n\n\n# 时间轮降级\n\n还是用回这个大家都已经耳熟能详的例子，在当前时间是2s的时候, 插入一个延时时间为22s的任务，该任务过期时间为24s。最后进入第二层时间轮，并插入到过期时间为[20,39]的时间格中。\n\n当二层时间轮上的定时任务到期后，时间轮是怎么做的呢？\n\n\n\n从图中可以看到，随着当前时间从2s继续往前推进，一直到20s的时候，总共经过了18s。此时第二层时间轮中，超时时间为[20-39s]的时间格上的任务到期。\n\n原本超时时间为24s的任务会被取出来，重新加入时间轮。此时该定时任务的延时时间从原本的22s，到现在还剩下4s（22s-18s）。最后停留在第一层时间轮超时时间为24s的时间格，也就是第4个时间格。\n\n随着当前时间继续推进，再经过4s后，该定时任务到期被执行。\n\n从这里可以看出时间轮的巧妙之处，两层时间轮只用了40个数组元素，却可以承载[0-399s]的定时任务。而三层时间轮用60个数组元素，就可以承载[0-7999s]的定时任务！\n\n\n\n\n# 时间轮的推进\n\n从动画中可以注意到, 随着时间推进, 时间轮的指针循环往复地定格在每一个时间格上, 每一次都要判断当前定格的时间格里是不是有任务存在;\n\n其中有很多时间格都是没有任务的, 指针定格在这种空的时间格中, 就是一次"空推进";\n\n比如说, 插入一个延时时间400s的任务, 指针就要执行399次"空推进", 这是一种浪费!\n\n那么Kafka是怎么解决这个问题的呢？这就要从延迟队列DelayQueue开始讲起了！时间轮搭配延迟队列DelayQueue，会发生什么化学反应呢？\n\n\n# 时间轮在 Kafka 中的实现\n\n方案二所采用的环形队列，就是时间轮的底层数据结构，它能够让需要处理的数据（任务的抽象）集中，在 Kafka 中存在大量的延迟操作，比如延迟生产、延迟拉取以及延迟删除等。Kafka 并没有使用 JDK 自带的 Timer 或者 DelayQueue 来实现延迟的功能，而是基于时间轮自定义了一个用于实现延迟功能的定时器（SystemTimer）。JDK 的 Timer 和 DelayQueue 插入和删除操作的平均时间复杂度为 O(nlog(n))，并不能满足 Kafka 的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为 O(1)。时间轮的应用并非 Kafka 独有，其应用场景还有很多，在 Netty、Akka、Quartz、Zookeeper 等组件中都存在时间轮的踪影。\n\n\n# 时间轮的数据结构\n\n参考下图，Kafka 中的时间轮（TimingWheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表（TimerTaskList）。TimerTaskList 是一个环形的双向链表，链表中的每一项表示的都是定时任务项（TimerTaskEntry），其中封装了真正的定时任务 TimerTask。在 Kafka 源码中对这个 TimeTaskList 是用一个名称为 buckets 的数组表示的，所以后面介绍中可能 TimerTaskList 也会被称为 bucket。\n\n\n\n针对上图的几个名词简单解释下：\n\n * tickMs： 时间轮由多个时间格组成，每个时间格就是 tickMs，它代表当前时间轮的基本时间跨度。\n * wheelSize： 代表每一层时间轮的格数\n * interval： 当前时间轮的总体时间跨度，interval=tickMs × wheelSize\n * startMs： 构造当层时间轮时候的当前时间，第一层的时间轮的 startMs 是 TimeUnit.NANOSECONDS.toMillis(nanoseconds()),上层时间轮的 startMs 为下层时间轮的 currentTime。\n * currentTime： 表示时间轮当前所处的时间，currentTime 是 tickMs 的整数倍（通过 currentTime=startMs - (startMs % tickMs 来保正 currentTime 一定是 tickMs 的整数倍），这个运算类比钟表中分钟里 65 秒分钟指针指向的还是 1 分钟）。currentTime 可以将整个时间轮划分为到期部分和未到期部分，currentTime 当前指向的时间格也属于到期部分，表示刚好到期，需要处理此时间格所对应的 TimerTaskList 的所有任务。\n\n\n# 时间轮中的任务存放\n\n若时间轮的 tickMs=1ms，wheelSize=20，那么可以计算得出 interval 为 20ms。初始情况下表盘指针 currentTime 指向时间格 0，此时有一个定时为 2ms 的任务插入进来会存放到时间格为 2 的 TimerTaskList 中。随着时间的不断推移，指针 currentTime 不断向前推进，过了 2ms 之后，当到达时间格 2 时，就需要将时间格 2 所对应的 TimeTaskList 中的任务做相应的到期操作。此时若又有一个定时为 8ms 的任务插入进来，则会存放到时间格 10 中，currentTime 再过 8ms 后会指向时间格 10。如果同时有一个定时为 19ms 的任务插入进来怎么办？新来的 TimerTaskEntry 会复用原来的 TimerTaskList，所以它会插入到原本已经到期的时间格 1 中。总之，整个时间轮的总体跨度是不变的，随着指针 currentTime 的不断推进，当前时间轮所能处理的时间段也在不断后移，总体时间范围在 currentTime 和 currentTime+interval 之间。\n\n\n# 时间轮的升降级\n\n如果此时有个定时为 350ms 的任务该如何处理？直接扩充 wheelSize 的大小么？Kafka 中不乏几万甚至几十万毫秒的定时任务，这个 wheelSize 的扩充没有底线，就算将所有的定时任务的到期时间都设定一个上限，比如 100 万毫秒，那么这个 wheelSize 为 100 万毫秒的时间轮不仅占用很大的内存空间，而且效率也会拉低。Kafka 为此引入了层级时间轮的概念，当任务的到期时间超过了当前时间轮所表示的时间范围时，就会尝试添加到上层时间轮中。\n\n\n\n参考上图，复用之前的案例，第一层的时间轮 tickMs=1ms, wheelSize=20, interval=20ms。第二层的时间轮的 tickMs 为第一层时间轮的 interval，即为 20ms。每一层时间轮的 wheelSize 是固定的，都是 20，那么第二层的时间轮的总体时间跨度 interval 为 400ms。以此类推，这个 400ms 也是第三层的 tickMs 的大小，第三层的时间轮的总体时间跨度为 8000ms。\n\n刚才提到的 350ms 的任务，不会插入到第一层时间轮，会插入到 interval=20*20 的第二层时间轮中，具体插入到时间轮的哪个 bucket 呢？先用 350/tickMs(20)=virtualId(17)，然后 virtualId(17) %wheelSize (20) = 17，所以 350 会放在第 17 个 bucket。如果此时有一个 450ms 后执行的任务，那么会放在第三层时间轮中，按照刚才的计算公式，会放在第 0 个 bucket。第 0 个 bucket 里会包含[400,800)ms 的任务。随着时间流逝，当时间过去了 400ms，那么 450ms 后就要执行的任务还剩下 50ms 的时间才能执行，此时有一个时间轮降级的操作，将 50ms 任务重新提交到层级时间轮中，那么此时 50ms 的任务根据公式会放入第二个时间轮的第 2 个 bucket 中，此 bucket 的时间范围为[40,60)ms，然后再经过 40ms，这个 50ms 的任务又会被监控到，此时距离任务执行还有 10ms，同样将 10ms 的任务提交到层级时间轮，此时会加入到第一层时间轮的第 10 个 bucket，所以再经过 10ms 后，此任务到期，最终执行。\n\n整个时间轮的升级降级操作是不是很类似于我们的时钟？ 第一层时间轮 tickMs=1s, wheelSize=60，interval=1min，此为秒钟；第二层 tickMs=1min，wheelSize=60，interval=1hour，此为分钟；第三层 tickMs=1hour，wheelSize 为 12，interval 为 12hours，此为时钟。而钟表的指针就对应程序中的 currentTime，这个后面分析代码时候会讲到（对这个的理解也是时间轮理解的重点和难点）。\n\n\n# 任务添加和驱动时间轮滚动核心流程图\n\n\n\n\n# 重点代码介绍\n\n这是往 SystenTimer 中添加一个任务。\n\n//在Systemtimer中添加一个任务，任务被包装为一个TimerTaskEntry\nprivate def addTimerTaskEntry(timerTaskEntry: TimerTaskEntry): Unit = {\n//先判断是否可以添加进时间轮中，如果不可以添加进去代表任务已经过期或者任务被取消，注意这里的timingWheel持有上一层时间轮的引用，所以可能存在递归调用\n  if (!timingWheel.add(timerTaskEntry)) {\n    // Already expired or cancelled\n    if (!timerTaskEntry.cancelled)\n     //过期任务直接线程池异步执行掉\n      taskExecutor.submit(timerTaskEntry.timerTask)\n  }\n}\n//timingWheel添加任务，递归添加直到添加该任务进合适的时间轮的bucket中\ndef add(timerTaskEntry: TimerTaskEntry): Boolean = {\n  val expiration = timerTaskEntry.expirationMs\n  //任务取消\n  if (timerTaskEntry.cancelled) {\n    // Cancelled\n    false\n  } else if (expiration < currentTime + tickMs) {\n    // 任务过期后会被执行\n    false\n  } else if (expiration < currentTime + interval) {//任务过期时间比当前时间轮时间加周期小说明任务过期时间在本时间轮周期内\n    val virtualId = expiration / tickMs\n    //找到任务对应本时间轮的bucket\n    val bucket = buckets((virtualId % wheelSize.toLong).toInt)\n    bucket.add(timerTaskEntry)\n    // Set the bucket expiration time\n   //只有本bucket内的任务都过期后才会bucket.setExpiration返回true此时将bucket放入延迟队列\n    if (bucket.setExpiration(virtualId * tickMs)) {\n     //bucket是一个TimerTaskList，它实现了java.util.concurrent.Delayed接口，里面是一个多任务组成的链表，图2有说明\n      queue.offer(bucket)\n    }\n    true\n  } else {\n    // Out of the interval. Put it into the parent timer\n    //任务的过期时间不在本时间轮周期内说明需要升级时间轮，如果不存在则构造上一层时间轮，继续用上一层时间轮添加任务\n    if (overflowWheel == null) addOverflowWheel()\n    overflowWheel.add(timerTaskEntry)\n  }\n}\n\n\n在本层级时间轮里添加上一层时间轮里的过程，注意的是在下一层时间轮的 interval 为上一层时间轮的 tickMs。\n\nprivate[this] def addOverflowWheel(): Unit = {\n  synchronized {\n    if (overflowWheel == null) {\n      overflowWheel = new TimingWheel(\n        tickMs = interval,\n        wheelSize = wheelSize,\n        startMs = currentTime,\n        taskCounter = taskCounter,\n        queue\n      )\n    }\n  }\n}\n\n\n驱动时间轮滚动过程：\n\n注意这里会存在一个递归，一直驱动时间轮的指针滚动直到时间不足于驱动上层的时间轮滚动。\n\ndef advanceClock(timeMs: Long): Unit = {\n  if (timeMs >= currentTime + tickMs) {\n   //把当前时间打平为时间轮tickMs的整数倍\n    currentTime = timeMs - (timeMs % tickMs)\n    // Try to advance the clock of the overflow wheel if present\n    //驱动上层时间轮，这里的传给上层的currentTime时间是本层时间轮打平过的，但是在上层时间轮还是会继续打平\n    if (overflowWheel != null) overflowWheel.advanceClock(currentTime)\n  }\n}\n\n\n驱动源：\n\n//循环bucket里面的任务列表，一个个重新添加进时间轮，对符合条件的时间轮进行升降级或者执行任务\nprivate[this] val reinsert = (timerTaskEntry: TimerTaskEntry) => addTimerTaskEntry(timerTaskEntry)\n \n/*\n * Advances the clock if there is an expired bucket. If there isn\'t any expired bucket when called,\n * waits up to timeoutMs before giving up.\n */\ndef advanceClock(timeoutMs: Long): Boolean = {\n  var bucket = delayQueue.poll(timeoutMs, TimeUnit.MILLISECONDS)\n  if (bucket != null) {\n    writeLock.lock()\n    try {\n      while (bucket != null) {\n        //驱动时间轮\n        timingWheel.advanceClock(bucket.getExpiration())\n       //循环buckek也就是任务列表，任务列表一个个继续添加进时间轮以此来升级或者降级时间轮，把过期任务找出来执行\n        bucket.flush(reinsert)\n       //循环\n        //这里就是从延迟队列取出bucket，bucket是有延迟时间的，取出代表该bucket过期，我们通过bucket能取到bucket包含的任务列表\n        bucket = delayQueue.poll()\n      }\n    } finally {\n      writeLock.unlock()\n    }\n    true\n  } else {\n    false\n  }\n}\n\n\n\n# DelayQueue 与 kafka 时间轮\n\nkafka 的延迟队列使用时间轮实现，能够支持大量任务的高效触发，但是在 kafka 延迟队列实现方案里还是看到了 delayQueue 的影子，使用 delayQueue 是对时间轮里面的 bucket 放入延迟队列，以此来推动时间轮滚动，但是基于将插入和删除操作则放入时间轮中，将这些操作的时间复杂度都降为 O(1)，提升效率。Kafka 对性能的极致追求让它把最合适的组件放在最适合的位置。\n\n如何推进时间轮的前进，让时间轮的时间往前走。\n\n * Netty 中的时间轮是通过工作线程按照固定的时间间隔 tickDuration 推进的\n   * 如果长时间没有到期任务，这种方案会带来空推进的问题，从而造成一定的性能损耗；\n * Kafka 则是通过 DelayQueue 来推进，是一种空间换时间的思想；\n   * DelayQueue 中保存着所有的 TimerTaskList 对象，根据时间来排序，这样延时越小的任务排在越前面。\n   * 外部通过一个线程（叫做ExpiredOperationReaper）从 DelayQueue 中获取超时的任务列表 TimerTaskList，然后根据 TimerTaskList 的 过期时间来精确推进时间轮的时间，这样就不会存在空推进的问题啦。\n\n其实 Kafka 采用的是一种权衡的策略，把 DelayQueue 用在了合适的地方。DelayQueue 只存放了 TimerTaskList，并不是所有的 TimerTask，数量并不多，相比空推进带来的影响是利大于弊的。\n\n\n# 总结\n\n * Kafka 使用时间轮来实现延时队列，因为其底层是任务的添加和删除是基于链表实现的，是 O(1) 的时间复杂度，满足高性能的要求；\n * 对于时间跨度大的延时任务，Kafka 引入了层级时间轮，能更好控制时间粒度，可以应对更加复杂的定时任务处理场景；\n * 对于如何实现时间轮的推进和避免空推进影响性能，Kafka 采用空间换时间的思想，通过 DelayQueue 来推进时间轮，算是一个经典的 trade off（权衡）。\n\n\n# 参考文献\n\n一张图理解Kafka时间轮(TimingWheel),看不懂算我输!时间轮，是一种实现延迟功能（定时器）的巧妙算法，在N - 掘金 (juejin.cn)\n\n面试官：你给我说一下什么是时间轮吧？今天我带大家来卷一下时间轮吧，这个玩意其实还是挺实用的。 常见于各种框架之中，偶现于 - 掘金 (juejin.cn)\n\n任务调度之时间轮实现 | 京东云技术团队在生活中太阳的东升西落，鸟类的南飞北归，四级的轮换，每天的上下班，海水的潮汐，每 - 掘金 (juejin.cn)\n\n一张图理解Kafka时间轮(TimingWheel) - 知乎 (zhihu.com)\n\n时间轮在Kafka的实践_移动_滴滴技术_InfoQ精选文章',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 如何在高并发场景下高效管理数十万个定时任务，而不会造成性能瓶颈？\n 2. 为什么常规的定时任务扫描方式效率低下，时间轮如何通过环形队列提升任务调度的性能？\n 3. 当定时任务的延迟时间超过了当前时间轮的范围时，时间轮是如何通过层级机制处理这种复杂情况的？\n 4. 如何避免时间轮在处理少量定时任务时的“空推进”问题，从而提升资源利用率？\n 5. kafka 等高性能系统为什么选择时间轮来管理延迟任务，而不使用jdk自带的timer或delayqueue？\n 6. 在一个拥有数百万级别任务的系统中，如何通过多层时间轮实现任务的精确调度和执行？\n\n\n# 带着疑问\n\n第一个问题：如果一台机器上有 10w 个定时任务，如何做到高效触发？\n\n具体场景是：\n\n> 有一个 app 实时消息通道系统，对每个用户会维护一个 app 到服务器的 tcp 连接，用来实时收发消息，对这个 tcp 连接，有这样一个需求：“如果连续 30s 没有请求包（例如登录，消息，keepalive 包），服务端就要将这个用户的状态置为离线”。\n> \n> 其中，单机 tcp 同时在线量约在 10w 级别，keepalive 请求包较分散大概 30s 一次，吞吐量约在 3000qps。\n\n怎么做？\n\n常用方案使用 time 定时任务，每秒扫描一次所有连接的集合 map<uid, last_packet_time>，把连接时间（每次有新的请求更新对应连接的连接时间）比当前时间的差值大 30s 的连接找出来处理。\n\n另一种方案，使用环形队列法：\n\n三个重要的数据结构：\n\n 1. 30s 超时，就创建一个 index 从 0 到 30 的环形队列（本质是个数组）\n 2. 环上每一个 slot 是一个 set，任务集合\n 3. 同时还有一个 map<uid, index>，记录 uid 落在环上的哪个 slot 里\n\n这样当有某用户 uid 有请求包到达时：\n\n 1. 从 map 结构中，查找出这个 uid 存储在哪一个 slot 里\n 2. 从这个 slot 的 set 结构中，删除这个 uid\n 3. 将 uid 重新加入到新的 slot 中，具体是哪一个 slot 呢 => current index 指针所指向的上一个 slot，因为这个 slot，会被 timer 在 30s 之后扫描到\n 4. 更新 map，这个 uid 对应 slot 的 index 值\n\n哪些元素会被超时掉呢？\n\ncurrent index 每秒种移动一个 slot，这个 slot 对应的 set中所有 uid 都应该被集体超时！如果最近 30s 有请求包来到，一定被放到 current index 的前一个 slot 了，current index 所在的 slot 对应 set 中所有元素，都是最近 30s 没有请求包来到的。\n\n所以，当没有超时时，current index 扫到的每一个 slot 的 set 中应该都没有元素。\n\n两种方案对比：\n\n方案一每次都要轮询所有数据，而方案二使用环形队列只需要轮询这一刻需要过期的数据，如果没有数据过期则没有数据要处理，并且是批量超时，并且由于是环形结构更加节约空间，这很适合高性能场景。\n\n第二个问题： 在开发过程中有延迟一定时间的任务要执行，怎么做？\n\n如果不重复造轮子的话，我们的选择当然是延迟队列或者 timer。\n\n延迟队列和在 timer 中增 加延时任务采用数组表示的最小堆的数据结构实现，每次放入新元素和移除队首元素时间复杂度为 o(nlog(n))。\n\n\n# 前言\n\n时间轮，是一种实现延迟功能（定时器）的巧妙算法，在 netty，zookeeper，kafka 等各种框架中，甚至linux内核中都有用到。\n\n设计源于生活\n\n时间轮，其设计正是来源于生活中的时钟。\n\n如图就是一个简单的时间轮：\n\n\n\n图中大圆的圆心位置表示的是当前的时间，随着时间推移, 圆心处的时间也会不断跳动。\n\n下面我们对着这个图，来说说kafka的时间轮timingwheel。\n\nkafka时间轮的底层就是一个环形数组，而数组中每个元素都存放一个双向链表timertasklist，链表中封装了很多延时任务。\n\nkafka中一个时间轮timingwheel是由20个时间格组成，wheelsize = 20；每格的时间跨度是1ms，tickms = 1ms。参照kafka，上图中也用了20个灰边小圆表示时间格，为了动画演示可以看得清楚，我们这里每个小圆的时间跨度是1s。\n\n所以现在整个时间轮的时间跨度就是 tickms * wheelsize ，也就是 20s。从0s到19s，我们都分别有一个灰边小圆来承载。\n\nkafka的时间轮还有一个表盘指针 currenttime，表示时间轮当前所处的时间。也就是图中用黑色粗线表示的圆，随着时间推移, 这个指针也会不断前进;\n\n\n\n\n# 添加定时任务\n\n有了时间轮，现在可以往里面添加定时任务了。我们用一个粉红色的小圆来表示一个定时任务。\n\n\n\n这里先讲一下设定，每一个定时任务都有延时时间delaytime，和过期时间expiredtime。比如当前时间是10s，我们添加了个延时时间为2s的任务，那么这个任务的过期时间就是12s，也就是当前时间10s再走两秒，变成了12s的时候，就到了触发这个定时任务的时间。\n\n而时间轮上代表时间格的灰边小圆上显示的数字，可以理解为任务的过期时间。\n\n\n\n讲清楚这些设定后，我们就开始添加定时任务吧。\n\n初始的时候, 时间轮的指针定格在0。此时添加一个超时时间为2s的任务, 那么这个任务将会插入到第二个时间格中。\n\n\n\n当时间轮的指针到达第二个时间格时, 会处理该时间格上对应的任务。在动画上就是让红色的小圆消失!\n\n\n\n如果这个时候又插入一个延时时间为8s的任务进来, 这个任务的过期时间就是在当前时间2s的基础上加8s, 也就是10s, 那么这个任务将会插入到过期时间为10s的时间格中。\n\n\n\n\n# "动态"时间轮\n\n到目前为止，一切都很好理解。\n\n那么如果在当前时间是2s的时候, 插入一个延时时间为19s的任务时,这个任务的过期时间就是在当前时间2s的基础上加19s, 也就是21s。\n\n请看下图，当前的时间轮是没有过期时间为21s的时间格。这个任务将会插入到过期时间为1s的时间格中，这是怎么回事呢？\n\n\n\n\n# 复用时间格\n\n为了解答上面的问题，我们先来点魔法， 让时间轮上的时间都动起来！\n\n\n\n其实呢，当指针定格在2s的位置时, 时间格0s, 1s和2s就已经是过期的时间格。\n\n也就是说指针可以用来划分过期的时间格[0,2]和未来的时间格 [3,19]。而过期的时间格可以继续复用。比如过期的时间格0s就变成了20s, 存放过期时间为20s的任务。\n\n理解了时间格的复用之后，再看回刚刚的例子，当前时间是2s时，添加延时时间为19s的任务，那么这个任务就会插入到过期时间为21s的时间格中。\n\n\n\n\n# 时间轮升级\n\n下面，新的问题来了，请坐好扶稳。\n\n如果在当前时间是2s的时候, 插入一个延时时间为22s的任务, 这个任务的过期时间就是在2s的基础上加22s，也就是24s。\n\n\n\n显然当前时间轮是无法找到过期时间格为24秒的时间格，因为当前过期时间最大的时间格才到21s。而且我们也没办法像前面那样再复用时间格，因为除了过期时间为2s的时间格，其他的时间格都还没过期呢。当前时间轮无法承载这个定时任务,那么应该怎么办呢?\n\n当然我们可以选择扩展时间轮上的时间格, 但是这样一来，时间轮就失去了意义。\n\n是时候要升级时间轮了！\n\n我们先来理解下多层时间轮之间的联系。\n\n\n# 层级时间轮\n\n如图是一个两层的时间轮:\n\n\n\n第二层时间轮也是由20个时间格组成, 每个时间格的跨度是20s。\n\n图中展示了每个时间格对应的过期时间范围, 我们可以清晰地看到, 第二层时间轮的第0个时间格的过期时间范围是 [0,19]。也就是说, 第二层时间轮的一个时间格就可以表示第一层时间轮的所有(20个)时间格;\n\n为了进一步理清第一层时间轮和第二层时间轮的关系, 我们拉着时间的小手, 一起观看下面的动图:\n\n\n\n可以看到，第二层时间轮同样也有自己的指针, 每当第一层时间轮走完一个周期，第二层时间轮的指针就会推进一格。\n\n\n# 添加定时任务\n\n回到一开始的问题，在当前时间是2s的时候, 插入一个延时时间为22s的任务，该任务过期时间为24s。\n\n\n\n当第一层时间轮容纳不下时，进入第二层时间轮，并插入到过期时间为[20,39]的时间格中。\n\n我们再来个例子，如果在当前时间是2s的时候, 插入一个延时时间为350s的任务, 这个任务的过期时间就是在2s的基础上加350s，也就是352s。\n\n\n\n从图中可以看到，该任务插入到第二层时间轮过期时间为[340,359]s的时间格中，也就是第17格的位置。\n\n\n# "动态"层级时间轮\n\n通常来说, 第二层时间轮的第0个时间格是用来表示第一层时间轮的, 这一格是存放不了任务的, 因为超时时间0-20s的任务, 第一层时间轮就可以处理了。\n\n但是! 事情往往没这么简单, 我们时间轮上的时间格都是可以复用的! 那么这在第二层时间轮上又是怎么体现的呢?\n\n下面是魔法时间， 我们让时间轮上的过期时间都动起来！\n\n\n\n从图中可以看到，当第一层时间轮的指针定格在1s时，超时时间0s的时间格就过期了。而这个时候，第二层时间轮第0个时间格的时间范围就从[0,19]分为了过期的[0],和未过期的[1,19]。而过期的[0]就会被新的过期时间[400]复用。\n\n[0-19]\n\n[400][1,19]\n\n[400,401][2,19]\n\n......\n\n[400,419]\n\n\n所以，如果在当前时间是2s的时候, 插入一个延时时间为399s的任务, 这个任务的过期时间就是在2s的基础上加399s，也就是401s。如图，这个任务还是会插到第二层时间轮第0个时间格中去。\n\n\n\n\n# 时间轮降级\n\n还是用回这个大家都已经耳熟能详的例子，在当前时间是2s的时候, 插入一个延时时间为22s的任务，该任务过期时间为24s。最后进入第二层时间轮，并插入到过期时间为[20,39]的时间格中。\n\n当二层时间轮上的定时任务到期后，时间轮是怎么做的呢？\n\n\n\n从图中可以看到，随着当前时间从2s继续往前推进，一直到20s的时候，总共经过了18s。此时第二层时间轮中，超时时间为[20-39s]的时间格上的任务到期。\n\n原本超时时间为24s的任务会被取出来，重新加入时间轮。此时该定时任务的延时时间从原本的22s，到现在还剩下4s（22s-18s）。最后停留在第一层时间轮超时时间为24s的时间格，也就是第4个时间格。\n\n随着当前时间继续推进，再经过4s后，该定时任务到期被执行。\n\n从这里可以看出时间轮的巧妙之处，两层时间轮只用了40个数组元素，却可以承载[0-399s]的定时任务。而三层时间轮用60个数组元素，就可以承载[0-7999s]的定时任务！\n\n\n\n\n# 时间轮的推进\n\n从动画中可以注意到, 随着时间推进, 时间轮的指针循环往复地定格在每一个时间格上, 每一次都要判断当前定格的时间格里是不是有任务存在;\n\n其中有很多时间格都是没有任务的, 指针定格在这种空的时间格中, 就是一次"空推进";\n\n比如说, 插入一个延时时间400s的任务, 指针就要执行399次"空推进", 这是一种浪费!\n\n那么kafka是怎么解决这个问题的呢？这就要从延迟队列delayqueue开始讲起了！时间轮搭配延迟队列delayqueue，会发生什么化学反应呢？\n\n\n# 时间轮在 kafka 中的实现\n\n方案二所采用的环形队列，就是时间轮的底层数据结构，它能够让需要处理的数据（任务的抽象）集中，在 kafka 中存在大量的延迟操作，比如延迟生产、延迟拉取以及延迟删除等。kafka 并没有使用 jdk 自带的 timer 或者 delayqueue 来实现延迟的功能，而是基于时间轮自定义了一个用于实现延迟功能的定时器（systemtimer）。jdk 的 timer 和 delayqueue 插入和删除操作的平均时间复杂度为 o(nlog(n))，并不能满足 kafka 的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为 o(1)。时间轮的应用并非 kafka 独有，其应用场景还有很多，在 netty、akka、quartz、zookeeper 等组件中都存在时间轮的踪影。\n\n\n# 时间轮的数据结构\n\n参考下图，kafka 中的时间轮（timingwheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表（timertasklist）。timertasklist 是一个环形的双向链表，链表中的每一项表示的都是定时任务项（timertaskentry），其中封装了真正的定时任务 timertask。在 kafka 源码中对这个 timetasklist 是用一个名称为 buckets 的数组表示的，所以后面介绍中可能 timertasklist 也会被称为 bucket。\n\n\n\n针对上图的几个名词简单解释下：\n\n * tickms： 时间轮由多个时间格组成，每个时间格就是 tickms，它代表当前时间轮的基本时间跨度。\n * wheelsize： 代表每一层时间轮的格数\n * interval： 当前时间轮的总体时间跨度，interval=tickms × wheelsize\n * startms： 构造当层时间轮时候的当前时间，第一层的时间轮的 startms 是 timeunit.nanoseconds.tomillis(nanoseconds()),上层时间轮的 startms 为下层时间轮的 currenttime。\n * currenttime： 表示时间轮当前所处的时间，currenttime 是 tickms 的整数倍（通过 currenttime=startms - (startms % tickms 来保正 currenttime 一定是 tickms 的整数倍），这个运算类比钟表中分钟里 65 秒分钟指针指向的还是 1 分钟）。currenttime 可以将整个时间轮划分为到期部分和未到期部分，currenttime 当前指向的时间格也属于到期部分，表示刚好到期，需要处理此时间格所对应的 timertasklist 的所有任务。\n\n\n# 时间轮中的任务存放\n\n若时间轮的 tickms=1ms，wheelsize=20，那么可以计算得出 interval 为 20ms。初始情况下表盘指针 currenttime 指向时间格 0，此时有一个定时为 2ms 的任务插入进来会存放到时间格为 2 的 timertasklist 中。随着时间的不断推移，指针 currenttime 不断向前推进，过了 2ms 之后，当到达时间格 2 时，就需要将时间格 2 所对应的 timetasklist 中的任务做相应的到期操作。此时若又有一个定时为 8ms 的任务插入进来，则会存放到时间格 10 中，currenttime 再过 8ms 后会指向时间格 10。如果同时有一个定时为 19ms 的任务插入进来怎么办？新来的 timertaskentry 会复用原来的 timertasklist，所以它会插入到原本已经到期的时间格 1 中。总之，整个时间轮的总体跨度是不变的，随着指针 currenttime 的不断推进，当前时间轮所能处理的时间段也在不断后移，总体时间范围在 currenttime 和 currenttime+interval 之间。\n\n\n# 时间轮的升降级\n\n如果此时有个定时为 350ms 的任务该如何处理？直接扩充 wheelsize 的大小么？kafka 中不乏几万甚至几十万毫秒的定时任务，这个 wheelsize 的扩充没有底线，就算将所有的定时任务的到期时间都设定一个上限，比如 100 万毫秒，那么这个 wheelsize 为 100 万毫秒的时间轮不仅占用很大的内存空间，而且效率也会拉低。kafka 为此引入了层级时间轮的概念，当任务的到期时间超过了当前时间轮所表示的时间范围时，就会尝试添加到上层时间轮中。\n\n\n\n参考上图，复用之前的案例，第一层的时间轮 tickms=1ms, wheelsize=20, interval=20ms。第二层的时间轮的 tickms 为第一层时间轮的 interval，即为 20ms。每一层时间轮的 wheelsize 是固定的，都是 20，那么第二层的时间轮的总体时间跨度 interval 为 400ms。以此类推，这个 400ms 也是第三层的 tickms 的大小，第三层的时间轮的总体时间跨度为 8000ms。\n\n刚才提到的 350ms 的任务，不会插入到第一层时间轮，会插入到 interval=20*20 的第二层时间轮中，具体插入到时间轮的哪个 bucket 呢？先用 350/tickms(20)=virtualid(17)，然后 virtualid(17) %wheelsize (20) = 17，所以 350 会放在第 17 个 bucket。如果此时有一个 450ms 后执行的任务，那么会放在第三层时间轮中，按照刚才的计算公式，会放在第 0 个 bucket。第 0 个 bucket 里会包含[400,800)ms 的任务。随着时间流逝，当时间过去了 400ms，那么 450ms 后就要执行的任务还剩下 50ms 的时间才能执行，此时有一个时间轮降级的操作，将 50ms 任务重新提交到层级时间轮中，那么此时 50ms 的任务根据公式会放入第二个时间轮的第 2 个 bucket 中，此 bucket 的时间范围为[40,60)ms，然后再经过 40ms，这个 50ms 的任务又会被监控到，此时距离任务执行还有 10ms，同样将 10ms 的任务提交到层级时间轮，此时会加入到第一层时间轮的第 10 个 bucket，所以再经过 10ms 后，此任务到期，最终执行。\n\n整个时间轮的升级降级操作是不是很类似于我们的时钟？ 第一层时间轮 tickms=1s, wheelsize=60，interval=1min，此为秒钟；第二层 tickms=1min，wheelsize=60，interval=1hour，此为分钟；第三层 tickms=1hour，wheelsize 为 12，interval 为 12hours，此为时钟。而钟表的指针就对应程序中的 currenttime，这个后面分析代码时候会讲到（对这个的理解也是时间轮理解的重点和难点）。\n\n\n# 任务添加和驱动时间轮滚动核心流程图\n\n\n\n\n# 重点代码介绍\n\n这是往 systentimer 中添加一个任务。\n\n//在systemtimer中添加一个任务，任务被包装为一个timertaskentry\nprivate def addtimertaskentry(timertaskentry: timertaskentry): unit = {\n//先判断是否可以添加进时间轮中，如果不可以添加进去代表任务已经过期或者任务被取消，注意这里的timingwheel持有上一层时间轮的引用，所以可能存在递归调用\n  if (!timingwheel.add(timertaskentry)) {\n    // already expired or cancelled\n    if (!timertaskentry.cancelled)\n     //过期任务直接线程池异步执行掉\n      taskexecutor.submit(timertaskentry.timertask)\n  }\n}\n//timingwheel添加任务，递归添加直到添加该任务进合适的时间轮的bucket中\ndef add(timertaskentry: timertaskentry): boolean = {\n  val expiration = timertaskentry.expirationms\n  //任务取消\n  if (timertaskentry.cancelled) {\n    // cancelled\n    false\n  } else if (expiration < currenttime + tickms) {\n    // 任务过期后会被执行\n    false\n  } else if (expiration < currenttime + interval) {//任务过期时间比当前时间轮时间加周期小说明任务过期时间在本时间轮周期内\n    val virtualid = expiration / tickms\n    //找到任务对应本时间轮的bucket\n    val bucket = buckets((virtualid % wheelsize.tolong).toint)\n    bucket.add(timertaskentry)\n    // set the bucket expiration time\n   //只有本bucket内的任务都过期后才会bucket.setexpiration返回true此时将bucket放入延迟队列\n    if (bucket.setexpiration(virtualid * tickms)) {\n     //bucket是一个timertasklist，它实现了java.util.concurrent.delayed接口，里面是一个多任务组成的链表，图2有说明\n      queue.offer(bucket)\n    }\n    true\n  } else {\n    // out of the interval. put it into the parent timer\n    //任务的过期时间不在本时间轮周期内说明需要升级时间轮，如果不存在则构造上一层时间轮，继续用上一层时间轮添加任务\n    if (overflowwheel == null) addoverflowwheel()\n    overflowwheel.add(timertaskentry)\n  }\n}\n\n\n在本层级时间轮里添加上一层时间轮里的过程，注意的是在下一层时间轮的 interval 为上一层时间轮的 tickms。\n\nprivate[this] def addoverflowwheel(): unit = {\n  synchronized {\n    if (overflowwheel == null) {\n      overflowwheel = new timingwheel(\n        tickms = interval,\n        wheelsize = wheelsize,\n        startms = currenttime,\n        taskcounter = taskcounter,\n        queue\n      )\n    }\n  }\n}\n\n\n驱动时间轮滚动过程：\n\n注意这里会存在一个递归，一直驱动时间轮的指针滚动直到时间不足于驱动上层的时间轮滚动。\n\ndef advanceclock(timems: long): unit = {\n  if (timems >= currenttime + tickms) {\n   //把当前时间打平为时间轮tickms的整数倍\n    currenttime = timems - (timems % tickms)\n    // try to advance the clock of the overflow wheel if present\n    //驱动上层时间轮，这里的传给上层的currenttime时间是本层时间轮打平过的，但是在上层时间轮还是会继续打平\n    if (overflowwheel != null) overflowwheel.advanceclock(currenttime)\n  }\n}\n\n\n驱动源：\n\n//循环bucket里面的任务列表，一个个重新添加进时间轮，对符合条件的时间轮进行升降级或者执行任务\nprivate[this] val reinsert = (timertaskentry: timertaskentry) => addtimertaskentry(timertaskentry)\n \n/*\n * advances the clock if there is an expired bucket. if there isn\'t any expired bucket when called,\n * waits up to timeoutms before giving up.\n */\ndef advanceclock(timeoutms: long): boolean = {\n  var bucket = delayqueue.poll(timeoutms, timeunit.milliseconds)\n  if (bucket != null) {\n    writelock.lock()\n    try {\n      while (bucket != null) {\n        //驱动时间轮\n        timingwheel.advanceclock(bucket.getexpiration())\n       //循环buckek也就是任务列表，任务列表一个个继续添加进时间轮以此来升级或者降级时间轮，把过期任务找出来执行\n        bucket.flush(reinsert)\n       //循环\n        //这里就是从延迟队列取出bucket，bucket是有延迟时间的，取出代表该bucket过期，我们通过bucket能取到bucket包含的任务列表\n        bucket = delayqueue.poll()\n      }\n    } finally {\n      writelock.unlock()\n    }\n    true\n  } else {\n    false\n  }\n}\n\n\n\n# delayqueue 与 kafka 时间轮\n\nkafka 的延迟队列使用时间轮实现，能够支持大量任务的高效触发，但是在 kafka 延迟队列实现方案里还是看到了 delayqueue 的影子，使用 delayqueue 是对时间轮里面的 bucket 放入延迟队列，以此来推动时间轮滚动，但是基于将插入和删除操作则放入时间轮中，将这些操作的时间复杂度都降为 o(1)，提升效率。kafka 对性能的极致追求让它把最合适的组件放在最适合的位置。\n\n如何推进时间轮的前进，让时间轮的时间往前走。\n\n * netty 中的时间轮是通过工作线程按照固定的时间间隔 tickduration 推进的\n   * 如果长时间没有到期任务，这种方案会带来空推进的问题，从而造成一定的性能损耗；\n * kafka 则是通过 delayqueue 来推进，是一种空间换时间的思想；\n   * delayqueue 中保存着所有的 timertasklist 对象，根据时间来排序，这样延时越小的任务排在越前面。\n   * 外部通过一个线程（叫做expiredoperationreaper）从 delayqueue 中获取超时的任务列表 timertasklist，然后根据 timertasklist 的 过期时间来精确推进时间轮的时间，这样就不会存在空推进的问题啦。\n\n其实 kafka 采用的是一种权衡的策略，把 delayqueue 用在了合适的地方。delayqueue 只存放了 timertasklist，并不是所有的 timertask，数量并不多，相比空推进带来的影响是利大于弊的。\n\n\n# 总结\n\n * kafka 使用时间轮来实现延时队列，因为其底层是任务的添加和删除是基于链表实现的，是 o(1) 的时间复杂度，满足高性能的要求；\n * 对于时间跨度大的延时任务，kafka 引入了层级时间轮，能更好控制时间粒度，可以应对更加复杂的定时任务处理场景；\n * 对于如何实现时间轮的推进和避免空推进影响性能，kafka 采用空间换时间的思想，通过 delayqueue 来推进时间轮，算是一个经典的 trade off（权衡）。\n\n\n# 参考文献\n\n一张图理解kafka时间轮(timingwheel),看不懂算我输!时间轮，是一种实现延迟功能（定时器）的巧妙算法，在n - 掘金 (juejin.cn)\n\n面试官：你给我说一下什么是时间轮吧？今天我带大家来卷一下时间轮吧，这个玩意其实还是挺实用的。 常见于各种框架之中，偶现于 - 掘金 (juejin.cn)\n\n任务调度之时间轮实现 | 京东云技术团队在生活中太阳的东升西落，鸟类的南飞北归，四级的轮换，每天的上下班，海水的潮汐，每 - 掘金 (juejin.cn)\n\n一张图理解kafka时间轮(timingwheel) - 知乎 (zhihu.com)\n\n时间轮在kafka的实践_移动_滴滴技术_infoq精选文章',charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"介绍",frontmatter:{title:"介绍",date:"2024-09-15T21:10:46.000Z",permalink:"/pages/b9733b/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80/01.%E4%BB%8B%E7%BB%8D.html",relativePath:"02.Kafka  系统设计/01.一、前言/01.介绍.md",key:"v-7b4cf0a1",path:"/pages/b9733b/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"指南",frontmatter:{title:"指南",date:"2024-09-17T16:51:40.000Z",permalink:"/pages/e21d7f/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80/01.%E6%8C%87%E5%8D%97.html",relativePath:"02.Kafka  系统设计/01.一、前言/01.指南.md",key:"v-c95d3aca",path:"/pages/e21d7f/",headers:[{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:2},{level:2,title:"阅读方法",slug:"阅读方法",normalizedTitle:"阅读方法",charIndex:156},{level:3,title:"蓝图",slug:"蓝图",normalizedTitle:"蓝图",charIndex:165},{level:3,title:"基础",slug:"基础",normalizedTitle:"基础",charIndex:136},{level:3,title:"主线",slug:"主线",normalizedTitle:"主线",charIndex:179},{level:3,title:"支线",slug:"支线",normalizedTitle:"支线",charIndex:186},{level:2,title:"学习资料推荐",slug:"学习资料推荐",normalizedTitle:"学习资料推荐",charIndex:193}],headersStr:"前置知识 阅读方法 蓝图 基础 主线 支线 学习资料推荐",content:"# 前置知识\n\n * 常用数据结构：数组、链表、哈希表、跳表\n * 网络协议：TCP 协议\n * 网络 IO 模型：IO 多路复用、非阻塞 IO、Reactor 网络模型\n * 操作系统：零拷贝（Copy On Write）、常见系统调用、磁盘 IO 机制\n * 编程语言基础：循环、分支、结构体、指针\n\n\n# 阅读方法\n\n\n# 蓝图\n\n\n# 基础\n\n\n# 主线\n\n\n# 支线\n\n\n# 学习资料推荐\n\n * https://kafka.apache.org/\n\n * A Deep Dive into Apache Kafka This is Event Streaming by Andrew Dunnings & Katherine Stanley (youtube.com)\n\n * Kafka权威指南 (豆瓣) (douban.com)\n\n * 深入理解Kafka：核心设计与实践原理 (豆瓣) (douban.com)\n\n * 《吃透MQ系列》之 Kafka 精妙的高性能设计 - 知乎 (zhihu.com)\n\n * Apache Kafka入门教程轻松学-目录 - 爱码叔-iCodeBook\n\n * Kafka源代码分析_爱码叔的博客-CSDN博客",normalizedContent:"# 前置知识\n\n * 常用数据结构：数组、链表、哈希表、跳表\n * 网络协议：tcp 协议\n * 网络 io 模型：io 多路复用、非阻塞 io、reactor 网络模型\n * 操作系统：零拷贝（copy on write）、常见系统调用、磁盘 io 机制\n * 编程语言基础：循环、分支、结构体、指针\n\n\n# 阅读方法\n\n\n# 蓝图\n\n\n# 基础\n\n\n# 主线\n\n\n# 支线\n\n\n# 学习资料推荐\n\n * https://kafka.apache.org/\n\n * a deep dive into apache kafka this is event streaming by andrew dunnings & katherine stanley (youtube.com)\n\n * kafka权威指南 (豆瓣) (douban.com)\n\n * 深入理解kafka：核心设计与实践原理 (豆瓣) (douban.com)\n\n * 《吃透mq系列》之 kafka 精妙的高性能设计 - 知乎 (zhihu.com)\n\n * apache kafka入门教程轻松学-目录 - 爱码叔-icodebook\n\n * kafka源代码分析_爱码叔的博客-csdn博客",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Kafka 整体架构",frontmatter:{title:"Kafka 整体架构",date:"2024-09-18T17:19:19.000Z",permalink:"/pages/46a58a/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/03.Kafka%20%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/03.Kafka 整体架构.md",key:"v-9e5a86d0",path:"/pages/46a58a/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"生产者",frontmatter:{title:"生产者",date:"2024-09-18T12:41:49.000Z",permalink:"/pages/bb1005/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/05.%E7%94%9F%E4%BA%A7%E8%80%85.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/05.生产者.md",key:"v-3855e824",path:"/pages/bb1005/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"整体流程",slug:"整体流程",normalizedTitle:"整体流程",charIndex:9},{level:2,title:"主线程",slug:"主线程",normalizedTitle:"主线程",charIndex:2653},{level:3,title:"初始化与参数配置",slug:"初始化与参数配置",normalizedTitle:"初始化与参数配置",charIndex:2661},{level:3,title:"KafkaProducer",slug:"kafkaproducer",normalizedTitle:"kafkaproducer",charIndex:276},{level:3,title:"ProducerInterceptor",slug:"producerinterceptor",normalizedTitle:"producerinterceptor",charIndex:3222},{level:3,title:"Serializer",slug:"serializer",normalizedTitle:"serializer",charIndex:1538},{level:3,title:"元数据",slug:"元数据",normalizedTitle:"元数据",charIndex:87},{level:3,title:"Partitioner",slug:"partitioner",normalizedTitle:"partitioner",charIndex:382},{level:3,title:"RecordAccumulator",slug:"recordaccumulator",normalizedTitle:"recordaccumulator",charIndex:139},{level:3,title:"内存池",slug:"内存池",normalizedTitle:"内存池",charIndex:9401},{level:3,title:"NIO",slug:"nio",normalizedTitle:"nio",charIndex:16993},{level:4,title:"read",slug:"read",normalizedTitle:"read",charIndex:19561},{level:2,title:"Sender 线程",slug:"sender-线程",normalizedTitle:"sender 线程",charIndex:180},{level:4,title:"发起连接",slug:"发起连接",normalizedTitle:"发起连接",charIndex:26485},{level:4,title:"ClientRequest",slug:"clientrequest",normalizedTitle:"clientrequest",charIndex:27272},{level:4,title:"响应",slug:"响应",normalizedTitle:"响应",charIndex:17122},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:38565},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:38572}],headersStr:"前言 整体流程 主线程 初始化与参数配置 KafkaProducer ProducerInterceptor Serializer 元数据 Partitioner RecordAccumulator 内存池 NIO read Sender 线程 发起连接 ClientRequest 响应 总结 参考资料",content:'# 前言\n\n\n# 整体流程\n\n 1. msg 封装成 ProducerRecord\n 2. 拦截器\n 3. Record 序列化\n 4. 第一次发送时先获取 kafka 集群元数据\n 5. Record 根据元数据得到要发送到 topic 的分区\n 6. Record 添加到 RecordAccumulator 缓冲区(每个分区都有独立的缓冲队列)\n 7. Sender 线程从 RecordAccumulator 获取 batch record 发送到 Broker\n\n\n\n// org.apache.kafka.clients.producer.KafkaProducer\npublic class KafkaProducer<K, V> implements Producer<K, V> {\n  // 分区器，可以自定义\n  private final Partitioner partitioner;\n  // kafka 集群元数据\n  private final Metadata metadata;\n  // 缓冲区\n  private final RecordAccumulator accumulator;\n  // 发送线程\n  private final Sender sender;\n  // 步骤一：封装成 ProducerRecord\n  public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {\n      // intercept the record, which can be potentially modified; this method does not throw exceptions\n      ProducerRecord<K, V> interceptedRecord = this.interceptors == null ? record : this.interceptors.onSend(record);\n      return doSend(interceptedRecord, callback);\n  }\n  private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {\n      TopicPartition tp = null;\n      try {\n          // 第一次发送数据时，先确保已经有 kafka metadata\n          ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);\n          // maxBlockTimeMs 默认 60s\n          long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);\n          Cluster cluster = clusterAndWaitTime.cluster;\n          byte[] serializedKey;\n          // 步骤二：序列化\n          try {\n              serializedKey = keySerializer.serialize(record.topic(), record.key());\n          } \n          byte[] serializedValue;\n          try {\n              serializedValue = valueSerializer.serialize(record.topic(), record.value());\n          } \n\n          // 步骤三：获取分区\n          int partition = partition(record, serializedKey, serializedValue, cluster);\n          int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);\n          ensureValidRecordSize(serializedSize);\n          tp = new TopicPartition(record.topic(), partition);\n          ...\n          // 步骤四：record 添加到 accumulator 缓存\n          RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);\n          if (result.batchIsFull || result.newBatchCreated) {\n              log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);\n              // 满足 batch 发送条件，唤醒 sender 发送数据\n              this.sender.wakeup();\n          }\n          return result.future;\n          ...\n  }\n}\n\n\n\n# 主线程\n\n\n# 初始化与参数配置\n\n在 Kafka 生产者的初始化阶段，主线程主要负责对生产者的各个组件进行配置与初始化。主要涉及的参数包括：\n\n * bootstrap.servers：Kafka 集群的地址。\n * key.serializer 和 value.serializer：消息的键和值的序列化方式。\n * acks：控制消息确认的方式，决定消息的可靠性和延迟之间的平衡。\n * retries：重试次数，在消息发送失败时用于控制重试机制。\n * batch.size：消息批次大小，影响消息的累积和发送频率。\n\n这些参数直接影响 Kafka 生产者的吞吐量、可靠性和延迟，是性能优化的重点。\n\n\n# KafkaProducer\n\nKafkaProducer 类是 Kafka 生产者的核心类，它实现了 Java 的 AutoCloseable 接口，负责管理消息发送的生命周期。其主要职责包括：\n\n * 初始化：初始化生产者相关的拦截器、序列化器、累加器、Sender 线程等组件。\n * 消息发送：提供 send() 方法，用于发送消息。它将消息封装为 ProducerRecord 对象，交由后续模块进行处理。\n\nKafkaProducer 的线程安全性和高效的资源管理使其在高并发场景下能够保持良好的性能。\n\n\n# ProducerInterceptor\n\nProducerInterceptor 主要用于在消息发送前后进行拦截和处理，常用于实现日志记录、统计和自定义逻辑。Kafka 生产者允许配置多个拦截器，它们按顺序执行。\n\n拦截器的典型应用场景包括：\n\n * 修改消息内容：例如给消息添加元数据。\n * 监控与统计：可以统计每条消息的发送时间或发送状态。\n\npublic class CustomProducerInterceptor implements ProducerInterceptor<String, String> {\n    @Override\n    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {\n        // 可以在此处修改消息或添加自定义逻辑\n        return record;\n    }\n\n    @Override\n    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {\n        // 可以在此处处理消息发送成功或失败的逻辑\n    }\n\n    @Override\n    public void close() {\n    }\n\n    @Override\n    public void configure(Map<String, ?> configs) {\n    }\n}\n\n\n\n# Serializer\n\nKafka 生产者中的序列化器负责将 Java 对象转化为字节数组，以便通过网络发送。Kafka 内置了几种常见的序列化器，例如 StringSerializer 和 ByteArraySerializer，也支持用户自定义序列化器。\n\n对于自定义对象，需要实现 Serializer 接口：\n\npublic class CustomSerializer implements Serializer<MyObject> {\n    @Override\n    public byte[] serialize(String topic, MyObject data) {\n        // 将自定义对象转换为字节数组\n        return SerializationUtils.serialize(data);\n    }\n\n    @Override\n    public void close() {\n    }\n}\n\n\n良好的序列化器设计不仅能保证数据的一致性，还能优化传输性能。\n\n\n# 元数据\n\nClient 需要从 Broker 节点获取 Topic 的元数据，才知道将当前 Record 发送哪个分区\n\n// org.apache.kafka.clients.producer.KafkaProducer\nprivate ClusterAndWaitTime waitOnMetadata(String topic, Integer partition, long maxWaitMs) throws InterruptedException {\n    // 添加要获取元数据的 topic，Map 结构\n    metadata.add(topic);\n    Cluster cluster = metadata.fetch();\n    Integer partitionsCount = cluster.partitionCountForTopic(topic);\n    // 如果已经有元数据，直接返回\n    if (partitionsCount != null && (partition == null || partition < partitionsCount))\n        return new ClusterAndWaitTime(cluster, 0);\n    // 死循环一直到获取元数据/超时\n    do {\n        int version = metadata.requestUpdate();\n        // 唤醒 sender 线程去获取\n        sender.wakeup();\n        try {\n            // wait 直到 sender 获取到元数据后唤醒\n            // 或者超时\n            metadata.awaitUpdate(version, remainingWaitMs);\n        } \n        ...\n    } while (partitionsCount == null);\n    ...\n    return new ClusterAndWaitTime(cluster, elapsed);\n}\n/* * Sender.run(this.client.poll(pollTimeout, now);) * NetworkClient.poll -> NetworkClient.handleCompletedReceives * metadataUpdater.maybeUpdate -> NetworkClient.maybeHandleCompletedReceive * metadataRequest = new MetadataRequest(new ArrayList<>(metadata.topics())); // 只获取 metadata 中 topic * NetworkClient.handleResponse -> MetaData.update */\n// org.apache.kafka.clients.Metadata\n// kafka 元数据结构\nprivate Cluster cluster;\npublic synchronized void update(Cluster cluster, long now) {\n  ...\n  if (topicExpiryEnabled) {\n    // Handle expiry of topics from the metadata refresh set.\n    // 移除 TOPIC_EXPIRY_MS(5分钟) 时间段内没有发送数据的 topic\n    // 下次获取元数据时就不获取此 topic 元数据\n    // 获取元数据的条件：cluster 元数据中没有需要 topic 的信息\n    for (Iterator<Map.Entry<String, Long>> it = topics.entrySet().iterator(); it.hasNext(); ) {\n      Map.Entry<String, Long> entry = it.next();\n      long expireMs = entry.getValue();\n      if (expireMs == TOPIC_EXPIRY_NEEDS_UPDATE)\n          entry.setValue(now + TOPIC_EXPIRY_MS);\n      else if (expireMs <= now) {\n          // 当前 topic 过期，移除\n          it.remove();\n          log.debug("Removing unused topic {} from the metadata list, expiryMs {} now {}", entry.getKey(), expireMs, now);\n      }\n    }\n  }\n  // 更新元数据\n  this.cluster = cluster;\n  // 唤醒 metadata\n  notifyAll();\n}\n\n\n\n\n\n# Partitioner\n\nPartitioner 决定消息被发送到 Kafka 集群中哪个分区。Kafka 默认的分区策略是基于消息的键（key）进行哈希分区，但也可以通过实现自定义 Partitioner 来实现复杂的分区逻辑。\n\n典型的场景包括：\n\n * 保证相同 key 的消息被路由到同一分区，确保消息的顺序性。\n * 根据业务需求，实现特定分区策略，例如将 VIP 用户的消息发送到特定分区以便优先处理。\n\n自定义 Partitioner 示例：\n\npublic class CustomPartitioner implements Partitioner {\n    @Override\n    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n        // 自定义分区策略：根据 key 的哈希值决定分区\n        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n        int numPartitions = partitions.size();\n        return (key.hashCode() & Integer.MAX_VALUE) % numPartitions;\n    }\n\n    @Override\n    public void close() {\n    }\n\n    @Override\n    public void configure(Map<String, ?> configs) {\n    }\n}\n\n\n\n# RecordAccumulator\n\n * RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性\n * RecordAccumulator缓存的大小可以通过生产者客户端参数buffer.memory配置，默认值为 33554432B，即 32MB\n * 如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候 KafkaProducer 的 send() 方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms 的配置，此参数的默认值为 60000,即 60 秒。\n * 主线程中发送过来的消息都会被追加到 RecordAccumulator 的某个双端队列中，在 Recordccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是 Producer Batch ，即 Deque<ProducerBatch>。消息写入缓存时，追加到双端队列的尾部\n * Sender 读取消息时，从双端队列的头部读取。Producer Batch 中可以包含一至多个 ProducerRecord。 通俗地说， ProducerRecord 是生产者中创建的消息，而 Producer Batch 是指一个消息批次， Producer Record 会被包含在 Producer Batch 中，这样可以使字节的使用更加紧凑。与此同时，将较小的 ProducerRecord 拼凑成一个较大的 ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量\n * Producer Batch 和消息的具体格式有关，如果生产者客户端需要向很多分区发送消息， 则可以将 buffer.memory 参数适当调大以增加整体的吞吐量\n\n// org.apache.kafka.clients.producer.internals.RecordAccumulator\npublic final class RecordAccumulator {\n  // 真正的缓存 CopyOnWriteMap\n  // Deque 是 ArrayDeque，当作为一个队列时，比 LinkedList 快\n  // 每个分区都有自己的 Deque， 存的是 RecordBatch\n  // RecordBatch 才是 record 真正缓存和 batch 发送\n  private final ConcurrentMap<TopicPartition, Deque<RecordBatch>> batches;\n  /* * record 添加到 缓冲区 * 线程安全，分段加锁提高性能 */\n  public RecordAppendResult append(TopicPartition tp,\n                                    long timestamp,\n                                    byte[] key,\n                                    byte[] value,\n                                    Callback callback,\n                                    long maxTimeToBlock) throws InterruptedException {\n    try {\n        // getOrCreateDeque 获取当前分区缓存，线程安全，CopyOnWriteMap 后续介绍\n        Deque<RecordBatch> dq = getOrCreateDeque(tp);\n        // 第一次加锁\n        synchronized (dq) {\n           ...\n            RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq);\n            if (appendResult != null)\n                return appendResult;\n        }// 解锁\n        // 走到这一步说明，上面的 tryAppend 返回null 即 RecordBatch 为 null\n        // // 可能有两种情况：没有RecordBatch，旧的RecordBatch 已写满要开辟新的 RecordBatch\n        // 内存池(后续介绍)开辟新的 RecordBatch，供添加缓存数据\n        // 开辟新批次的空间耗时，不加锁\n        int size = Math.max(this.batchSize, Records.LOG_OVERHEAD + Record.recordSize(key, value));\n        log.trace("Allocating a new {} byte message buffer for topic {} partition {}", size, tp.topic(), tp.partition());\n        ByteBuffer buffer = free.allocate(size, maxTimeToBlock);\n\n        // 第二次加锁\n        // 一：tryAppend 再次尝试添加\n        // 二：添加成功，说明已存在 RecordBatch，删除之前创建的 buffer\n        // 三：添加失败，利用之前创建的 buffer 新建 RecordBatch，然后添加\n        synchronized (dq) {\n            ...\n            RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq);\n            if (appendResult != null) {\n                // 此时发现已有新批次的 RecordBatch，删除之前创建的 buffer\n                // 结合多线程场景去考虑\n                free.deallocate(buffer);\n                return appendResult;\n            }\n            // 新建 RecordBatch，并将 Record 添加\n            MemoryRecords records = MemoryRecords.emptyRecords(buffer, compression, this.batchSize);\n            RecordBatch batch = new RecordBatch(tp, records, time.milliseconds());\n            FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds()));\n            // 分区队列中添加 RecordBatch，下次不用再创建\n            dq.addLast(batch);\n            incomplete.add(batch);\n            return new RecordAppendResult(future, dq.size() > 1 || batch.records.isFull(), true);\n        } // 解锁\n    ...\n  }\n}\n\n\nbatches 是真正的 Record 缓存数据结构，这里设计也是很精妙的。\n\n// org.apache.kafka.common.utils.CopyOnWriteMap\n// 读优化\n// 这里为了提高队列的性能，使用的数据结构是 ArrayDeque ,即 V = ArrayQueue\npublic class CopyOnWriteMap<K, V> implements ConcurrentMap<K, V> {\n  /** * 读 map 时没加 */\n  public V get(Object k) {\n      return map.get(k);\n  }\n  /** * 重点 * 写 map 时加锁，读写分离思路，适合读多写少的场景即多 get 少 put * put：每个分区只会 put 一次，实际操作中分区数时有有限的 * 注意这里多开辟新的 map ，用于交换：https://www.cnblogs.com/hapjin/p/4840107.html * 疑问：为什么这里要多复制一份而不是直接操作呢？ 因为同个 map 同一时间有增删和遍历时，会报 ConcurrentModificationException * 所以这里多复制一份，保证读原来的 map，写备份的 map 后替换原先的 map */\n  public synchronized V put(K k, V v) {\n    Map<K, V> copy = new HashMap<K, V>(this.map);\n    V prev = copy.put(k, v);\n    this.map = Collections.unmodifiableMap(copy);\n    return prev;\n  }\n  public synchronized V putIfAbsent(K k, V v) {\n    if (!containsKey(k))\n      return put(k, v);\n    else\n      return get(k);\n  }\n}\n\n\n到此我们可以得到 RecordAccumulator 的数据结构图\n\n\n\n\n# 内存池\n\nProducer 发送数据是攒批的，先缓存到 RecordBatch 然后再整体发送。RecordBatch 在 JVM 内存空间，Producer 吞吐很大必然会频繁创建 RecordBatch。对象频繁创建，在 JAVA 世界中不可避免的涉及到 GC，但 kafka 在这一步做了优化。下面一起看看在 Producer 端的内存池优化。\n\n// 获取 RecordBatch 所需的内存空间\n// ByteBuffer buffer = free.allocate(size, maxTimeToBlock);\n\n// org.apache.kafka.clients.producer.internals.BufferPool\npublic final class BufferPool {\n  // 可用的 ByteBuffer 队列\n  // ByteBuffer 是 默认 batch 大小的内存块\n  private final Deque<ByteBuffer> free;\n  // 需要等待开辟 ByteBuffer 的 batchs(没有可缓冲的内存，需要发送 batch 来释放)\n  private final Deque<Condition> waiters;\n  // Deque<ByteBuffer> + availableMemory = 缓存池大小，默认 32M\n  // availableMemory 可分配的内存大小\n  private long availableMemory;\n\n  public ByteBuffer allocate(int size, long maxTimeToBlockMs) throws InterruptedException {\n    // size 大于 总缓存池大小，直接报错，太大了放不下\n    if (size > this.totalMemory)\n    throw new IllegalArgumentException("Attempt to allocate " + size\n                                        + " bytes, but there is a hard limit of "\n                                        + this.totalMemory\n                                        + " on memory allocations.");\n    // 加锁\n    this.lock.lock();\n    try {\n    // 判断 size 是否为设置的 batch size 大小，\n    // 一般都是 batch 大小，除非一条 record 超过 batch size 则要大于 batch size\n    // 这里很关键，是内存池管理的体现：\n    // free:Deque<ByteBuffer> 是 ByteBuffer队列，且 ByteBuffer 大小=batch size\n    // 如果要申请的内存大小是 batch size，那么直接从 free 获取即可，无需创建新的 ByteBuffer 对象\n    if (size == poolableSize && !this.free.isEmpty())\n        return this.free.pollFirst();\n\n    // now check if the request is immediately satisfiable with the\n    // memory on hand or if we need to block\n    int freeListSize = this.free.size() * this.poolableSize;\n    if (this.availableMemory + freeListSize >= size) {\n        // 第一种情况：\n        // 当前可用的内存（availableMemory + free）大于申请的 size，开辟后返回\n\n        // 释放 free 中的 ByteBuffer 到 availableMemory，保证 availableMemory 空间大于申请的 size\n        freeUp(size);\n        this.availableMemory -= size;\n        lock.unlock();\n        return ByteBuffer.allocate(size);\n    } else {\n        // 第二种情况：\n        // 当前可用的内存（availableMemory + free）小于申请的 size\n        // 当前线程 block，直到（availableMemory + free）大于申请的 size，等待 释放\n        // 如果常常发生这种情况，建议加大缓冲区大小\n        int accumulated = 0;\n        ByteBuffer buffer = null;\n        Condition moreMemory = this.lock.newCondition();\n        long remainingTimeToBlockNs = TimeUnit.MILLISECONDS.toNanos(maxTimeToBlockMs);\n        // 当前开辟 size ，内存空间不足，信息存入 waiters\n        this.waiters.addLast(moreMemory);\n        // 等待，直到 可用内存大于申请的 size\n        while (accumulated < size) {\n            long startWaitNs = time.nanoseconds();\n            long timeNs;\n            boolean waitingTimeElapsed;\n            try {\n                // wait\n                // 超时自动苏醒\n                // 有内存释放被唤醒\n                waitingTimeElapsed = !moreMemory.await(remainingTimeToBlockNs, TimeUnit.NANOSECONDS);\n            } catch (InterruptedException e) {\n                this.waiters.remove(moreMemory);\n                throw e;\n            } finally {\n                long endWaitNs = time.nanoseconds();\n                timeNs = Math.max(0L, endWaitNs - startWaitNs);\n                this.waitTime.record(timeNs, time.milliseconds());\n            }\n            // 如果是超时等待，表示在规定的时间没有可用的内存，直接报错，默认 60s - 获取元数据时间\n            if (waitingTimeElapsed) {\n                this.waiters.remove(moreMemory);\n                throw new TimeoutException("Failed to allocate memory within the configured max blocking time " + maxTimeToBlockMs + " ms.");\n            }\n\n            remainingTimeToBlockNs -= timeNs;\n            // 到这一步，说明有内存释放\n            if (accumulated == 0 && size == this.poolableSize && !this.free.isEmpty()) {\n                // 申请内存size = batch size，把 free 中 ByteBuffer 返回\n                buffer = this.free.pollFirst();\n                accumulated = size;\n            } else {\n                // 累加 可用内存大小，accumulated\n                freeUp(size - accumulated);\n                int got = (int) Math.min(size - accumulated, this.availableMemory);\n                this.availableMemory -= got;\n                accumulated += got;\n            }\n        }\n\n   public void deallocate(ByteBuffer buffer, int size) {\n      lock.lock();\n      try {\n          if (size == this.poolableSize && size == buffer.capacity()) {\n              // 释放的内存大小 = batch size\n              // 清空 buffer，并添加到 free\n              // 内存池管理的体现：用完内存不释放，直接复用，减少 GC\n              buffer.clear();\n              this.free.add(buffer);\n          } else {\n              // 释放的内存大小 != batch size，直接释放等待 GC\n              this.availableMemory += size;\n          }\n          // 有内存释放，唤醒在等待内存开辟的 waiters\n          Condition moreMem = this.waiters.peekFirst();\n          if (moreMem != null)\n              // waitingTimeElapsed = !moreMemory.await(remainingTimeToBlockNs, TimeUnit.NANOSECONDS);\n              moreMem.signal();\n      } finally {\n          lock.unlock();\n      }\n  }\n\n\n内存申请和释放主要代码如上所示，接下来理清设计：\n\n * 缓冲池分为：free 和 availableMemory\n * availableMemory 表示未分配可用的内存，主要是计算是否可分配 size 内存：分配时减少，释放时增加，回收靠 GC\n * free ：ByteBuffer 队列，ByteBuffer 大小 = batch size，一般恰好是每次开辟的大小，释放时清空数据重新添加到队列\n\n按 batch size 大小划分 ByteBuffer，增大复用效果；不等于 batch size 的内存空间直接 GC，不会出现内存碎片问题。\n\n\n\n\n# NIO\n\n有了以上的知识后，我们可以将 record 加入到 batch 中，准备发送到 kafka broker。在发送之前，我们先分析 kafka 的使用的网络框架 NIO，有助于后续分析 sender 线程。网络一般涉及三个过程：建立连接，发送数据，读响应。\n\n// NetworkClient 是 kafka 网络请求实现类\n// NetworkClient 主要是封装 NIO 中 selector 操作\n// 下面主要分析 kafka 中 selector 操作，关于 NetworkClient 放到后续的 sender 线程一起分析\n// org.apache.kafka.common.network.Selector\npublic class Selector implements Selectable {\n  // java nio，不了解的先去熟悉下\n  private final java.nio.channels.Selector nioSelector;\n  // 记录对 kafka broker 连接\n  private final Map<String, KafkaChannel> channels;\n\n  // 与 kafka broker 建立连接，id = kafka node\n  public void connect(String id, InetSocketAddress address, int sendBufferSize, int receiveBufferSize) throws IOException {\n    ...\n    SocketChannel socketChannel = SocketChannel.open();\n    // 设置非阻塞模式\n    socketChannel.configureBlocking(false);\n    Socket socket = socketChannel.socket();\n    socket.setKeepAlive(true);\n    if (sendBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)\n        socket.setSendBufferSize(sendBufferSize);\n    if (receiveBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)\n        socket.setReceiveBufferSize(receiveBufferSize);\n    // tcp 优化\n    socket.setTcpNoDelay(true);\n    boolean connected;\n    try {\n        // 尝试建立连接\n        connected = socketChannel.connect(address);\n    } catch (UnresolvedAddressException e) {\n        socketChannel.close();\n        throw new IOException("Can\'t resolve address: " + address, e);\n    } catch (IOException e) {\n        socketChannel.close();\n        throw e;\n    }\n    // channel 加到 nio selector，并添加 OP_CONNECT 事件\n    // 后续 chanel 建立连接后，由 nio selector 返回 connect 事件\n    SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_CONNECT);\n    // id = kafka node；kafkaChannel 只是对 java channel 的封装\n    KafkaChannel channel = channelBuilder.buildChannel(id, key, maxReceiveSize);\n    // 后续处理时可通过 key.attachment 得到 channel\n    key.attach(channel);\n\n    // 记录 kafka node 对应的 channel\n    this.channels.put(id, channel);\n    ...\n  }\n\n  // 发送数据\n  public void send(Send send) {\n    // 通过send 获取 kafka node 得到对应的 channel\n    KafkaChannel channel = channelOrFail(send.destination());\n    try {\n        // channel 发送数据，此时只是为 channel 添加 OP_WRITE，并未真正发送\n        channel.setSend(send);\n    } catch (CancelledKeyException e) {\n        this.failedSends.add(send.destination());\n        close(channel);\n    }\n  }\n\n  // sender 线程中会定时调用获取 event \n  // 读取 nio selector event，time 表示是否需要阻塞等待事件发生\n  public void poll(long timeout) throws IOException {\n    ...\n    // block 直到有事件\n    // 注意此时会 wait，那么会导致 sender 线程也会休眠，需要调用 sender.wakeup 唤醒\n    int readyKeys = select(timeout);\n\n    long endSelect = time.nanoseconds();\n    this.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());\n\n    // 有需要响应的事件\n    if (readyKeys > 0 || !immediatelyConnectedKeys.isEmpty()) {\n        pollSelectionKeys(this.nioSelector.selectedKeys(), false, endSelect);\n        pollSelectionKeys(immediatelyConnectedKeys, true, endSelect);\n    }\n    // 处理服务端返回的响应,stagedReceives\n    addToCompletedReceives();\n    ...\n  }\n  // 处理 nio selector 返回的事件\n  // connect\n  // read\n  // write\n  private void pollSelectionKeys(Iterable<SelectionKey> selectionKeys,\n                                boolean isImmediatelyConnected,\n                                long currentTimeNanos) {\n    // eventKeys\n    Iterator<SelectionKey> iterator = selectionKeys.iterator();\n    while (iterator.hasNext()) {\n        SelectionKey key = iterator.next();\n        iterator.remove();\n        KafkaChannel channel = channel(key);\n        ...\n        try {\n            // 处理 connect ，与 kafka node 已建立连接：OP_CONNECT\n            if (isImmediatelyConnected || key.isConnectable()) {\n                // 连接建立完成后，channel 上对应的key 去除 OP_CONNECT，添加 OP_READ 事件\n                if (channel.finishConnect()) {\n                    // 记录当前 kafka node 已建立连接\n                    this.connected.add(channel.id());\n                    this.sensors.connectionCreated.record();\n                    SocketChannel socketChannel = (SocketChannel) key.channel();\n                    log.debug("Created socket with SO_RCVBUF = {}, SO_SNDBUF = {}, SO_TIMEOUT = {} to node {}",\n                            socketChannel.socket().getReceiveBufferSize(),\n                            socketChannel.socket().getSendBufferSize(),\n                            socketChannel.socket().getSoTimeout(),\n                            channel.id());\n                } else\n                    continue;\n            }\n            ...\n            // 处理 read：在建立连接后已添加 OP_READ 事件\n            if (channel.ready() && key.isReadable() && !hasStagedReceive(channel)) {\n                NetworkReceive networkReceive;\n                while ((networkReceive = channel.read()) != null)\n                    addToStagedReceives(channel, networkReceive);\n            }\n\n            // 处理 write：OP_WRITE (send(Send send))\n            if (channel.ready() && key.isWritable()) {\n                // channel.setSend(send); 为 chanel 添加 send\n                // 通过 chanel 发送数据：RecordBatchs\n                Send send = channel.write();\n                if (send != null) {\n                    this.completedSends.add(send);\n                    this.sensors.recordBytesSent(channel.id(), send.size());\n                }\n            }\n\n            // 关闭连接\n            if (!key.isValid()) {\n                close(channel);\n                this.disconnected.add(channel.id());\n            }\n         ...\n  }\n}\n\n\n\n\n# read\n\n当使用 NIO ，由于 buffer 大小不匹配问题，必然会碰到粘包或拆包的问题。假设两个响应返回 “hello word” 和 “flink kafka”，有可能是返回(一个返回就好触发一个 read event)\n\n * “hello word”\n * “flink “\n * “kafka”\n\n这叫拆包，也有可能返回\n\n * “hello wordflink”\n * “kafka”\n\n这叫粘包\n\nkafka 的解决方案是相当于在原有 data 基础上增加 header，header 只包含 data size，很朴素和通用方案。两个数据包变成 10”hello word”，11”flink kafka”。此时读取方式变为先读取4字节 size，然后再开辟 size 大小的 buffer 存data，死循环直到 data buffer 读满\n\n// org.apache.kafka.common.network.Selector\nif (channel.ready() && key.isReadable() && !hasStagedReceive(channel)) {\n    NetworkReceive networkReceive;\n    // read receive返回null 表示当前响应包为读取完毕，等待下个 read event再读\n    while ((networkReceive = channel.read()) != null)\n        // receive complete 才算响应读取完毕\n        // receive 添加到 stagedReceives\n        addToStagedReceives(channel, networkReceive);\n}\n// org.apache.kafka.common.network.KafkaChannel\npublic NetworkReceive read() throws IOException {\n    NetworkReceive result = null;\n\n    if (receive == null) {\n        receive = new NetworkReceive(maxReceiveSize, id);\n    }\n    // read\n    receive(receive);\n    // size 和 data 都读取完毕,才算 complete\n    if (receive.complete()) {\n        receive.payload().rewind();\n        result = receive;\n        receive = null;\n    }\n    return result;\n}\nprivate long receive(NetworkReceive receive) throws IOException {\n    return receive.readFrom(transportLayer);\n}\npublic long readFromReadableChannel(ReadableByteChannel channel) throws IOException {\n  int read = 0;\n  // 读取 size；\n  if (size.hasRemaining()) {\n      int bytesRead = channel.read(size);\n      if (bytesRead < 0)\n          throw new EOFException();\n      read += bytesRead;\n      if (!size.hasRemaining()) {\n          size.rewind();\n          int receiveSize = size.getInt();\n          ...\n          // 读取到size，可知 data 长度，申请存放 data 所需的 buffer\n          this.buffer = ByteBuffer.allocate(receiveSize);\n      }\n  }\n  // 读取 data；每次进来都必会读取 buffer\n  if (buffer != null) {\n      int bytesRead = channel.read(buffer);\n      if (bytesRead < 0)\n          throw new EOFException();\n      read += bytesRead;\n  }\n  return read;\n}\n\n\n\n# Sender 线程\n\n整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender 线程(发送线程)。在主线程中由KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器(RecordAccumulator，也称为消息收集器)中。Sender 线程负责从RecordAccumulator中获取消息并将其发送到Kafka中。\n\n到此我们具备了发送 rocordBatch 的环境，开始向服务端发送流程。\n\nSender 是 Kafka 生产者的核心线程，负责从消息累加器中提取消息，并通过网络发送到 Kafka 服务器。其工作流程如下：\n\n 1. 从 RecordAccumulator 中取出已准备好的消息批次。\n 2. 根据消息的分区和主题找到对应的 Kafka 服务器。\n 3. 发送消息，并处理响应（包括重试、超时、失败等情况）。\n\nSender 线程是异步运行的，通过高效的 I/O 和批量发送机制保证了生产者的高性能。在高吞吐量的场景下，调整 Sender 线程的相关参数（如 retries 和 max.in.flight.requests.per.connection）能够显著提升消息发送的效率。\n\n// org.apache.kafka.clients.producer.KafkaProducer\n// 满足 batch 发送条件，唤醒 sender 发送数据\nthis.sender.wakeup();      // 记得为什么这里要唤醒吗？selector.poll 有可能会 wait\n\n// 后台线程发送请求\n// org.apache.kafka.clients.producer.internals.Sender\npublic class Sender implements Runnable {\n  private final KafkaClient client;\n  // while（run）\n  void run(long now) {\n    Cluster cluster = metadata.fetch();\n    // 从缓冲池 accumulator 筛选出满足 batch send 的 kafka nodes\n    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);\n\n    // 如果有 topic 的元数据信息位获取，设置元数据标识\n    if (!result.unknownLeaderTopics.isEmpty()) {\n        for (String topic : result.unknownLeaderTopics)\n            this.metadata.add(topic);\n        this.metadata.requestUpdate();\n    }\n\n    // 从需要发送的 kafka nodes 中移除没有准备好连接的 node，判断条件\n    // 元数据包含当前 kafka node\n    // 与当前 kafka node connection 已建立\n    // 与当前 kafka node channel 已建立\n    // 发送 batch 请求数未超过指定的最大值(默认5)，生产中一般设置为1，保证数据不乱序\n    Iterator<Node> iter = result.readyNodes.iterator();\n    long notReadyTimeout = Long.MAX_VALUE;\n    while (iter.hasNext()) {\n        Node node = iter.next();\n        // ready 中如果发现未连接会去初始化连接 selector.connect\n        if (!this.client.ready(node, now)) {\n            iter.remove();\n            notReadyTimeout = Math.min(notReadyTimeout, this.client.connectionDelay(node, now));\n        }\n    }\n\n    // 从准备好发送的 kafkanodes 中，获取需要发送的 recordBatch \n    // 第一次调用时，与 kafka node 的连接都没有建立，此时 result.readyNodes 为空\n    // 在 poll 函数发现 connect 事件，此时会发起连接，那么下次再准备发送时就可以了\n    Map<Integer, List<RecordBatch>> batches = this.accumulator.drain(cluster,\n                                                                      result.readyNodes,\n                                                                      this.maxRequestSize,\n                                                                      now);\n    if (guaranteeMessageOrder) {\n        // Mute all the partitions drained\n        for (List<RecordBatch> batchList : batches.values()) {\n            for (RecordBatch batch : batchList)\n                this.accumulator.mutePartition(batch.topicPartition);\n        }\n    }\n\n    // 处理超时的发送\n    List<RecordBatch> expiredBatches = this.accumulator.abortExpiredBatches(this.requestTimeout, now);\n    ...\n    // 创建请求：将要发送的 recordbatch 封装成ClientRequest\n    // 每个requests 都有个回调函数(handleProduceResponse)，在收到响应时调用\n    List<ClientRequest> requests = createProduceRequests(batches, now);\n    long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);\n    if (result.readyNodes.size() > 0) {\n        log.trace("Nodes with data ready to send: {}", result.readyNodes);\n        log.trace("Created {} produce requests: {}", requests.size(), requests);\n        pollTimeout = 0;\n    }\n    // 发送数据，这里只是在 select 注册 op_write，并在 channel 添加 send\n    for (ClientRequest request : requests)\n        client.send(request, now);\n\n    // 如果需要有数据发送，select.poll(0)，不等待直接返回，因为有数据要发送\n    // 如果没有需要发送的数据，select.poll(timeout)，距离下次有数据要发送的间隔(linger ms)\n    this.client.poll(pollTimeout, now);\n  }\n}\n\n\nsender 线程逻辑很清晰：\n\n * while 以下步骤\n * 检查缓冲区中满足发送条件的 recordBatch\n * 检查要发送的 recordBatch 的元数据是否已准备\n * 删除未建立连接的 kafka node(未连接时会发起连接事件)\n * 获取已建立连接 kafka node 的 recordBatch\n * 将 recordBatchs 封装成 ClientRequest\n * 发送 ClientRequest\n * poll\n\n\n\n# 发起连接\n\n程序刚运行的时候肯定是没有建立连接，当发现有 recordbatch 需要发送时会先判断是否已连接。\n\n// this.client.ready(node, now)\n// org.apache.kafka.clients.NetworkClient\npublic boolean ready(Node node, long now) {\n  if (node.isEmpty())\n      throw new IllegalArgumentException("Cannot connect to empty node " + node);\n  // entry\n  if (isReady(node, now))\n      return true;\n  // 未连接或失去连接，初始化\n  if (connectionStates.canConnect(node.idString(), now))\n      // selector.connect: OP_CONNECT\n      initiateConnect(node, now);\n  return false;\n}\n/** * 检查是否可以向当前 kafka node 发送消息： * 元数据包含当前 kafka node * 与当前 kafka node connection 已建立 * 与当前 kafka node channel 已建立 * 发送 batch 请求数未超过指定的最大值(默认5) */\npublic boolean isReady(Node node, long now) {\n    // if we need to update our metadata now declare all requests unready to make metadata requests first\n    // priority\n    return !metadataUpdater.isUpdateDue(now) && canSendRequest(node.idString());\n}\n\n\n# ClientRequest\n\n要发送的 recordBatch 封装成 ClientRequest\n\n// List<ClientRequest> requests = createProduceRequests(batches, now);\n// org.apache.kafka.clients.producer.internals.Sender\nprivate List<ClientRequest> createProduceRequests(Map<Integer, List<RecordBatch>> collated, long now) {\n  List<ClientRequest> requests = new ArrayList<ClientRequest>(collated.size());\n  for (Map.Entry<Integer, List<RecordBatch>> entry : collated.entrySet())\n      // 创建请求：acks 非常重要，requestTimeout 默认 30s\n      // 将同个 node 上的多个 batch组装在一起\n      requests.add(produceRequest(now, entry.getKey(), acks, requestTimeout, entry.getValue()));\n  return requests;\n}\nprivate ClientRequest produceRequest(long now, int destination, short acks, int timeout, List<RecordBatch> batches) {\n  Map<TopicPartition, ByteBuffer> produceRecordsByPartition = new HashMap<TopicPartition, ByteBuffer>(batches.size());\n  final Map<TopicPartition, RecordBatch> recordsByPartition = new HashMap<TopicPartition, RecordBatch>(batches.size());\n  for (RecordBatch batch : batches) {\n      TopicPartition tp = batch.topicPartition;\n      produceRecordsByPartition.put(tp, batch.records.buffer());\n      recordsByPartition.put(tp, batch);\n  }\n  ProduceRequest request = new ProduceRequest(acks, timeout, produceRecordsByPartition);\n  // recordBatch 数据存到 RequestSend 数据结构\n  RequestSend send = new RequestSend(Integer.toString(destination),\n                                      // ApiKeys.PRODUCE 标识符，服务端会根据这个标识符处理\n                                      this.client.nextRequestHeader(ApiKeys.PRODUCE),\n                                      request.toStruct());\n  // 设置请求结束的回调函数，后续在响应会用到 \n  RequestCompletionHandler callback = new RequestCompletionHandler() {\n      public void onComplete(ClientResponse response) {\n          handleProduceResponse(response, recordsByPartition, time.milliseconds());\n      }\n  };\n\n  return new ClientRequest(now, acks != 0, send, callback);\n}\n\n\n# 响应\n\n// this.client.poll(pollTimeout, now);\n// org.apache.kafka.clients.NetworkClient\npublic List<ClientResponse> poll(long timeout, long now) {\n  // 是否需要更新元数据\n  long metadataTimeout = metadataUpdater.maybeUpdate(now);\n  try {\n      // selector.poll，处理 OP_CONNECT、OP_WRITE、OP_READ 事件\n      // OP_READ 通过 nio read -> completedReceives\n      this.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs));\n  } catch (IOException e) {\n      log.error("Unexpected error during I/O", e);\n  }\n\n  // process completed actions\n  long updatedNow = this.time.milliseconds();\n  List<ClientResponse> responses = new ArrayList<>();\n  // 处理已发送的 send(recordBatch)\n  // responses.add(new ClientResponse(request, now, false, null));\n  handleCompletedSends(responses, updatedNow);\n  // 处理返回的响应\n  // responses.add(new ClientResponse(req, now, false, body));\n  handleCompletedReceives(responses, updatedNow);\n  handleDisconnections(responses, updatedNow);\n  handleConnections();\n  // 处理超时的请求\n  handleTimedOutRequests(responses, updatedNow);\n\n  // invoke callbacks\n  for (ClientResponse response : responses) {\n      if (response.request().hasCallback()) {\n          try {\n              // 回调请求的 callback，handleProduceResponse\n              response.request().callback().onComplete(response);\n          } catch (Exception e) {\n              log.error("Uncaught error in request completion:", e);\n          }\n      }\n  }\n  return responses;\n}\n\n\n * 发送结束的响应\n\n// org.apache.kafka.clients.NetworkClient\nprivate void handleCompletedSends(List<ClientResponse> responses, long now) {\n    // completedSends.add(send); selector write 时加入\n    for (Send send : this.selector.completedSends()) {\n        ClientRequest request = this.inFlightRequests.lastSent(send.destination());\n        if (!request.expectResponse()) {\n           // ack = 0，才会构建 send 结束响应，这种方式不需要服务端返回结果\n            this.inFlightRequests.completeLastSent(send.destination());\n            // responseBody 为 null\n            responses.add(new ClientResponse(request, now, false, null));\n        }\n    }\n}\n\n\n * 服务端返回的响应\n\nprivate void handleCompletedReceives(List<ClientResponse> responses, long now) {\n  for (NetworkReceive receive : this.selector.completedReceives()) {\n      String source = receive.source();\n      ClientRequest req = inFlightRequests.completeNext(source);\n      // receive 的二进制解析成可识别的数据\n      Struct body = parseResponse(receive.payload(), req.request().header());\n      if (!metadataUpdater.maybeHandleCompletedReceive(req, now, body))\n          // responseBody 有值\n          responses.add(new ClientResponse(req, now, false, body));\n  }\n}\n\n\n * 超时请求的响应\n\nprivate void processDisconnection(List<ClientResponse> responses, String nodeId, long now) {\n  connectionStates.disconnected(nodeId, now);\n  for (ClientRequest request : this.inFlightRequests.clearAll(nodeId)) {\n      log.trace("Cancelled request {} due to node {} being disconnected", request, nodeId);\n      if (!metadataUpdater.maybeHandleDisconnection(request))\n          // responseBody 为 null 且 disconnected 为 true\n          responses.add(new ClientResponse(request, now, true, null));\n  }\n}\n\n\n * 处理响应\n\n// org.apache.kafka.clients.producer.internals.Sender\n// 请求回调\nprivate void handleProduceResponse(ClientResponse response, Map<TopicPartition, RecordBatch> batches, long now) {\n  // batches = 发送请求时携带的 recordBatch\n  // completeBatch\n  int correlationId = response.request().request().header().correlationId();\n  if (response.wasDisconnected()) {\n      // 失去连接\n      log.trace("Cancelled request {} due to node {} being disconnected", response, response.request()\n                                                                                            .request()\n                                                                                            .destination());\n      // Errors = Errors.NETWORK_EXCEPTION，\n      for (RecordBatch batch : batches.values())\n          completeBatch(batch, Errors.NETWORK_EXCEPTION, -1L, Record.NO_TIMESTAMP, correlationId, now);\n  } else {\n      log.trace("Received produce response from node {} with correlation id {}",\n                response.request().request().destination(),\n                correlationId);\n      if (response.hasResponse()) {\n          // ack != 0\n          ProduceResponse produceResponse = new ProduceResponse(response.responseBody());\n          for (Map.Entry<TopicPartition, ProduceResponse.PartitionResponse> entry : produceResponse.responses().entrySet()) {\n              // 每个 batch 处理\n              TopicPartition tp = entry.getKey();\n              ProduceResponse.PartitionResponse partResp = entry.getValue();\n              Errors error = Errors.forCode(partResp.errorCode);\n              RecordBatch batch = batches.get(tp);\n              completeBatch(batch, error, partResp.baseOffset, partResp.timestamp, correlationId, now);\n          }\n          this.sensors.recordLatency(response.request().request().destination(), response.requestLatencyMs());\n          this.sensors.recordThrottleTime(response.request().request().destination(),\n                                          produceResponse.getThrottleTime());\n      } else {\n          // ack = 0\n          for (RecordBatch batch : batches.values())\n              completeBatch(batch, Errors.NONE, -1L, Record.NO_TIMESTAMP, correlationId, now);\n      }\n  }\n}\n// recordBatch 处理\nprivate void completeBatch(RecordBatch batch, Errors error, long baseOffset, long timestamp, long correlationId, long now) {\n  if (error != Errors.NONE && canRetry(batch, error)) { // 有异常且可以重新发送\n      // retry\n      log.warn("Got error produce response with correlation id {} on topic-partition {}, retrying ({} attempts left). Error: {}",\n                correlationId,\n                batch.topicPartition,\n                this.retries - batch.attempts - 1,\n                error);\n      // 重新加入 缓冲区\n      this.accumulator.reenqueue(batch, now);\n  } else {\n      // 无异常\n      // 有异常且不可以重新发送\n      RuntimeException exception;\n      if (error == Errors.TOPIC_AUTHORIZATION_FAILED)\n          exception = new TopicAuthorizationException(batch.topicPartition.topic());\n      else\n          exception = error.exception();\n      // record 回调函数\n      batch.done(baseOffset, timestamp, exception);\n      // recordBatch 发送结束，内存池回收该内存\n      this.accumulator.deallocate(batch);\n  ...\n}\n// record 回调 producer.send(,callback)\npublic void done(long baseOffset, long timestamp, RuntimeException exception) {\n  // thunks = records\n  for (int i = 0; i < this.thunks.size(); i++) {\n      try {\n          Thunk thunk = this.thunks.get(i);\n          if (exception == null) {\n              // If the timestamp returned by server is NoTimestamp, that means CreateTime is used. Otherwise LogAppendTime is used.\n              RecordMetadata metadata = new RecordMetadata(this.topicPartition,  baseOffset, thunk.future.relativeOffset(),\n                                                            timestamp == Record.NO_TIMESTAMP ? thunk.future.timestamp() : timestamp,\n                                                            thunk.future.checksum(),\n                                                            thunk.future.serializedKeySize(),\n                                                            thunk.future.serializedValueSize());\n              // 无异常时 record 回调函数\n              thunk.callback.onCompletion(metadata, null);\n          } else {\n              // 有异常时 record 回调函数\n              thunk.callback.onCompletion(null, exception);\n          }\n      } \n}\n\n\n\n\n\n# 总结\n\n\n# 参考资料\n\n * 深入理解 Kafka 之 Producer - 阿飞的博客 | Danner Blog (vendanner.github.io)',normalizedContent:'# 前言\n\n\n# 整体流程\n\n 1. msg 封装成 producerrecord\n 2. 拦截器\n 3. record 序列化\n 4. 第一次发送时先获取 kafka 集群元数据\n 5. record 根据元数据得到要发送到 topic 的分区\n 6. record 添加到 recordaccumulator 缓冲区(每个分区都有独立的缓冲队列)\n 7. sender 线程从 recordaccumulator 获取 batch record 发送到 broker\n\n\n\n// org.apache.kafka.clients.producer.kafkaproducer\npublic class kafkaproducer<k, v> implements producer<k, v> {\n  // 分区器，可以自定义\n  private final partitioner partitioner;\n  // kafka 集群元数据\n  private final metadata metadata;\n  // 缓冲区\n  private final recordaccumulator accumulator;\n  // 发送线程\n  private final sender sender;\n  // 步骤一：封装成 producerrecord\n  public future<recordmetadata> send(producerrecord<k, v> record, callback callback) {\n      // intercept the record, which can be potentially modified; this method does not throw exceptions\n      producerrecord<k, v> interceptedrecord = this.interceptors == null ? record : this.interceptors.onsend(record);\n      return dosend(interceptedrecord, callback);\n  }\n  private future<recordmetadata> dosend(producerrecord<k, v> record, callback callback) {\n      topicpartition tp = null;\n      try {\n          // 第一次发送数据时，先确保已经有 kafka metadata\n          clusterandwaittime clusterandwaittime = waitonmetadata(record.topic(), record.partition(), maxblocktimems);\n          // maxblocktimems 默认 60s\n          long remainingwaitms = math.max(0, maxblocktimems - clusterandwaittime.waitedonmetadatams);\n          cluster cluster = clusterandwaittime.cluster;\n          byte[] serializedkey;\n          // 步骤二：序列化\n          try {\n              serializedkey = keyserializer.serialize(record.topic(), record.key());\n          } \n          byte[] serializedvalue;\n          try {\n              serializedvalue = valueserializer.serialize(record.topic(), record.value());\n          } \n\n          // 步骤三：获取分区\n          int partition = partition(record, serializedkey, serializedvalue, cluster);\n          int serializedsize = records.log_overhead + record.recordsize(serializedkey, serializedvalue);\n          ensurevalidrecordsize(serializedsize);\n          tp = new topicpartition(record.topic(), partition);\n          ...\n          // 步骤四：record 添加到 accumulator 缓存\n          recordaccumulator.recordappendresult result = accumulator.append(tp, timestamp, serializedkey, serializedvalue, interceptcallback, remainingwaitms);\n          if (result.batchisfull || result.newbatchcreated) {\n              log.trace("waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);\n              // 满足 batch 发送条件，唤醒 sender 发送数据\n              this.sender.wakeup();\n          }\n          return result.future;\n          ...\n  }\n}\n\n\n\n# 主线程\n\n\n# 初始化与参数配置\n\n在 kafka 生产者的初始化阶段，主线程主要负责对生产者的各个组件进行配置与初始化。主要涉及的参数包括：\n\n * bootstrap.servers：kafka 集群的地址。\n * key.serializer 和 value.serializer：消息的键和值的序列化方式。\n * acks：控制消息确认的方式，决定消息的可靠性和延迟之间的平衡。\n * retries：重试次数，在消息发送失败时用于控制重试机制。\n * batch.size：消息批次大小，影响消息的累积和发送频率。\n\n这些参数直接影响 kafka 生产者的吞吐量、可靠性和延迟，是性能优化的重点。\n\n\n# kafkaproducer\n\nkafkaproducer 类是 kafka 生产者的核心类，它实现了 java 的 autocloseable 接口，负责管理消息发送的生命周期。其主要职责包括：\n\n * 初始化：初始化生产者相关的拦截器、序列化器、累加器、sender 线程等组件。\n * 消息发送：提供 send() 方法，用于发送消息。它将消息封装为 producerrecord 对象，交由后续模块进行处理。\n\nkafkaproducer 的线程安全性和高效的资源管理使其在高并发场景下能够保持良好的性能。\n\n\n# producerinterceptor\n\nproducerinterceptor 主要用于在消息发送前后进行拦截和处理，常用于实现日志记录、统计和自定义逻辑。kafka 生产者允许配置多个拦截器，它们按顺序执行。\n\n拦截器的典型应用场景包括：\n\n * 修改消息内容：例如给消息添加元数据。\n * 监控与统计：可以统计每条消息的发送时间或发送状态。\n\npublic class customproducerinterceptor implements producerinterceptor<string, string> {\n    @override\n    public producerrecord<string, string> onsend(producerrecord<string, string> record) {\n        // 可以在此处修改消息或添加自定义逻辑\n        return record;\n    }\n\n    @override\n    public void onacknowledgement(recordmetadata metadata, exception exception) {\n        // 可以在此处处理消息发送成功或失败的逻辑\n    }\n\n    @override\n    public void close() {\n    }\n\n    @override\n    public void configure(map<string, ?> configs) {\n    }\n}\n\n\n\n# serializer\n\nkafka 生产者中的序列化器负责将 java 对象转化为字节数组，以便通过网络发送。kafka 内置了几种常见的序列化器，例如 stringserializer 和 bytearrayserializer，也支持用户自定义序列化器。\n\n对于自定义对象，需要实现 serializer 接口：\n\npublic class customserializer implements serializer<myobject> {\n    @override\n    public byte[] serialize(string topic, myobject data) {\n        // 将自定义对象转换为字节数组\n        return serializationutils.serialize(data);\n    }\n\n    @override\n    public void close() {\n    }\n}\n\n\n良好的序列化器设计不仅能保证数据的一致性，还能优化传输性能。\n\n\n# 元数据\n\nclient 需要从 broker 节点获取 topic 的元数据，才知道将当前 record 发送哪个分区\n\n// org.apache.kafka.clients.producer.kafkaproducer\nprivate clusterandwaittime waitonmetadata(string topic, integer partition, long maxwaitms) throws interruptedexception {\n    // 添加要获取元数据的 topic，map 结构\n    metadata.add(topic);\n    cluster cluster = metadata.fetch();\n    integer partitionscount = cluster.partitioncountfortopic(topic);\n    // 如果已经有元数据，直接返回\n    if (partitionscount != null && (partition == null || partition < partitionscount))\n        return new clusterandwaittime(cluster, 0);\n    // 死循环一直到获取元数据/超时\n    do {\n        int version = metadata.requestupdate();\n        // 唤醒 sender 线程去获取\n        sender.wakeup();\n        try {\n            // wait 直到 sender 获取到元数据后唤醒\n            // 或者超时\n            metadata.awaitupdate(version, remainingwaitms);\n        } \n        ...\n    } while (partitionscount == null);\n    ...\n    return new clusterandwaittime(cluster, elapsed);\n}\n/* * sender.run(this.client.poll(polltimeout, now);) * networkclient.poll -> networkclient.handlecompletedreceives * metadataupdater.maybeupdate -> networkclient.maybehandlecompletedreceive * metadatarequest = new metadatarequest(new arraylist<>(metadata.topics())); // 只获取 metadata 中 topic * networkclient.handleresponse -> metadata.update */\n// org.apache.kafka.clients.metadata\n// kafka 元数据结构\nprivate cluster cluster;\npublic synchronized void update(cluster cluster, long now) {\n  ...\n  if (topicexpiryenabled) {\n    // handle expiry of topics from the metadata refresh set.\n    // 移除 topic_expiry_ms(5分钟) 时间段内没有发送数据的 topic\n    // 下次获取元数据时就不获取此 topic 元数据\n    // 获取元数据的条件：cluster 元数据中没有需要 topic 的信息\n    for (iterator<map.entry<string, long>> it = topics.entryset().iterator(); it.hasnext(); ) {\n      map.entry<string, long> entry = it.next();\n      long expirems = entry.getvalue();\n      if (expirems == topic_expiry_needs_update)\n          entry.setvalue(now + topic_expiry_ms);\n      else if (expirems <= now) {\n          // 当前 topic 过期，移除\n          it.remove();\n          log.debug("removing unused topic {} from the metadata list, expiryms {} now {}", entry.getkey(), expirems, now);\n      }\n    }\n  }\n  // 更新元数据\n  this.cluster = cluster;\n  // 唤醒 metadata\n  notifyall();\n}\n\n\n\n\n\n# partitioner\n\npartitioner 决定消息被发送到 kafka 集群中哪个分区。kafka 默认的分区策略是基于消息的键（key）进行哈希分区，但也可以通过实现自定义 partitioner 来实现复杂的分区逻辑。\n\n典型的场景包括：\n\n * 保证相同 key 的消息被路由到同一分区，确保消息的顺序性。\n * 根据业务需求，实现特定分区策略，例如将 vip 用户的消息发送到特定分区以便优先处理。\n\n自定义 partitioner 示例：\n\npublic class custompartitioner implements partitioner {\n    @override\n    public int partition(string topic, object key, byte[] keybytes, object value, byte[] valuebytes, cluster cluster) {\n        // 自定义分区策略：根据 key 的哈希值决定分区\n        list<partitioninfo> partitions = cluster.partitionsfortopic(topic);\n        int numpartitions = partitions.size();\n        return (key.hashcode() & integer.max_value) % numpartitions;\n    }\n\n    @override\n    public void close() {\n    }\n\n    @override\n    public void configure(map<string, ?> configs) {\n    }\n}\n\n\n\n# recordaccumulator\n\n * recordaccumulator 主要用来缓存消息以便 sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性\n * recordaccumulator缓存的大小可以通过生产者客户端参数buffer.memory配置，默认值为 33554432b，即 32mb\n * 如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候 kafkaproducer 的 send() 方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms 的配置，此参数的默认值为 60000,即 60 秒。\n * 主线程中发送过来的消息都会被追加到 recordaccumulator 的某个双端队列中，在 recordccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是 producer batch ，即 deque<producerbatch>。消息写入缓存时，追加到双端队列的尾部\n * sender 读取消息时，从双端队列的头部读取。producer batch 中可以包含一至多个 producerrecord。 通俗地说， producerrecord 是生产者中创建的消息，而 producer batch 是指一个消息批次， producer record 会被包含在 producer batch 中，这样可以使字节的使用更加紧凑。与此同时，将较小的 producerrecord 拼凑成一个较大的 producerbatch，也可以减少网络请求的次数以提升整体的吞吐量\n * producer batch 和消息的具体格式有关，如果生产者客户端需要向很多分区发送消息， 则可以将 buffer.memory 参数适当调大以增加整体的吞吐量\n\n// org.apache.kafka.clients.producer.internals.recordaccumulator\npublic final class recordaccumulator {\n  // 真正的缓存 copyonwritemap\n  // deque 是 arraydeque，当作为一个队列时，比 linkedlist 快\n  // 每个分区都有自己的 deque， 存的是 recordbatch\n  // recordbatch 才是 record 真正缓存和 batch 发送\n  private final concurrentmap<topicpartition, deque<recordbatch>> batches;\n  /* * record 添加到 缓冲区 * 线程安全，分段加锁提高性能 */\n  public recordappendresult append(topicpartition tp,\n                                    long timestamp,\n                                    byte[] key,\n                                    byte[] value,\n                                    callback callback,\n                                    long maxtimetoblock) throws interruptedexception {\n    try {\n        // getorcreatedeque 获取当前分区缓存，线程安全，copyonwritemap 后续介绍\n        deque<recordbatch> dq = getorcreatedeque(tp);\n        // 第一次加锁\n        synchronized (dq) {\n           ...\n            recordappendresult appendresult = tryappend(timestamp, key, value, callback, dq);\n            if (appendresult != null)\n                return appendresult;\n        }// 解锁\n        // 走到这一步说明，上面的 tryappend 返回null 即 recordbatch 为 null\n        // // 可能有两种情况：没有recordbatch，旧的recordbatch 已写满要开辟新的 recordbatch\n        // 内存池(后续介绍)开辟新的 recordbatch，供添加缓存数据\n        // 开辟新批次的空间耗时，不加锁\n        int size = math.max(this.batchsize, records.log_overhead + record.recordsize(key, value));\n        log.trace("allocating a new {} byte message buffer for topic {} partition {}", size, tp.topic(), tp.partition());\n        bytebuffer buffer = free.allocate(size, maxtimetoblock);\n\n        // 第二次加锁\n        // 一：tryappend 再次尝试添加\n        // 二：添加成功，说明已存在 recordbatch，删除之前创建的 buffer\n        // 三：添加失败，利用之前创建的 buffer 新建 recordbatch，然后添加\n        synchronized (dq) {\n            ...\n            recordappendresult appendresult = tryappend(timestamp, key, value, callback, dq);\n            if (appendresult != null) {\n                // 此时发现已有新批次的 recordbatch，删除之前创建的 buffer\n                // 结合多线程场景去考虑\n                free.deallocate(buffer);\n                return appendresult;\n            }\n            // 新建 recordbatch，并将 record 添加\n            memoryrecords records = memoryrecords.emptyrecords(buffer, compression, this.batchsize);\n            recordbatch batch = new recordbatch(tp, records, time.milliseconds());\n            futurerecordmetadata future = utils.notnull(batch.tryappend(timestamp, key, value, callback, time.milliseconds()));\n            // 分区队列中添加 recordbatch，下次不用再创建\n            dq.addlast(batch);\n            incomplete.add(batch);\n            return new recordappendresult(future, dq.size() > 1 || batch.records.isfull(), true);\n        } // 解锁\n    ...\n  }\n}\n\n\nbatches 是真正的 record 缓存数据结构，这里设计也是很精妙的。\n\n// org.apache.kafka.common.utils.copyonwritemap\n// 读优化\n// 这里为了提高队列的性能，使用的数据结构是 arraydeque ,即 v = arrayqueue\npublic class copyonwritemap<k, v> implements concurrentmap<k, v> {\n  /** * 读 map 时没加 */\n  public v get(object k) {\n      return map.get(k);\n  }\n  /** * 重点 * 写 map 时加锁，读写分离思路，适合读多写少的场景即多 get 少 put * put：每个分区只会 put 一次，实际操作中分区数时有有限的 * 注意这里多开辟新的 map ，用于交换：https://www.cnblogs.com/hapjin/p/4840107.html * 疑问：为什么这里要多复制一份而不是直接操作呢？ 因为同个 map 同一时间有增删和遍历时，会报 concurrentmodificationexception * 所以这里多复制一份，保证读原来的 map，写备份的 map 后替换原先的 map */\n  public synchronized v put(k k, v v) {\n    map<k, v> copy = new hashmap<k, v>(this.map);\n    v prev = copy.put(k, v);\n    this.map = collections.unmodifiablemap(copy);\n    return prev;\n  }\n  public synchronized v putifabsent(k k, v v) {\n    if (!containskey(k))\n      return put(k, v);\n    else\n      return get(k);\n  }\n}\n\n\n到此我们可以得到 recordaccumulator 的数据结构图\n\n\n\n\n# 内存池\n\nproducer 发送数据是攒批的，先缓存到 recordbatch 然后再整体发送。recordbatch 在 jvm 内存空间，producer 吞吐很大必然会频繁创建 recordbatch。对象频繁创建，在 java 世界中不可避免的涉及到 gc，但 kafka 在这一步做了优化。下面一起看看在 producer 端的内存池优化。\n\n// 获取 recordbatch 所需的内存空间\n// bytebuffer buffer = free.allocate(size, maxtimetoblock);\n\n// org.apache.kafka.clients.producer.internals.bufferpool\npublic final class bufferpool {\n  // 可用的 bytebuffer 队列\n  // bytebuffer 是 默认 batch 大小的内存块\n  private final deque<bytebuffer> free;\n  // 需要等待开辟 bytebuffer 的 batchs(没有可缓冲的内存，需要发送 batch 来释放)\n  private final deque<condition> waiters;\n  // deque<bytebuffer> + availablememory = 缓存池大小，默认 32m\n  // availablememory 可分配的内存大小\n  private long availablememory;\n\n  public bytebuffer allocate(int size, long maxtimetoblockms) throws interruptedexception {\n    // size 大于 总缓存池大小，直接报错，太大了放不下\n    if (size > this.totalmemory)\n    throw new illegalargumentexception("attempt to allocate " + size\n                                        + " bytes, but there is a hard limit of "\n                                        + this.totalmemory\n                                        + " on memory allocations.");\n    // 加锁\n    this.lock.lock();\n    try {\n    // 判断 size 是否为设置的 batch size 大小，\n    // 一般都是 batch 大小，除非一条 record 超过 batch size 则要大于 batch size\n    // 这里很关键，是内存池管理的体现：\n    // free:deque<bytebuffer> 是 bytebuffer队列，且 bytebuffer 大小=batch size\n    // 如果要申请的内存大小是 batch size，那么直接从 free 获取即可，无需创建新的 bytebuffer 对象\n    if (size == poolablesize && !this.free.isempty())\n        return this.free.pollfirst();\n\n    // now check if the request is immediately satisfiable with the\n    // memory on hand or if we need to block\n    int freelistsize = this.free.size() * this.poolablesize;\n    if (this.availablememory + freelistsize >= size) {\n        // 第一种情况：\n        // 当前可用的内存（availablememory + free）大于申请的 size，开辟后返回\n\n        // 释放 free 中的 bytebuffer 到 availablememory，保证 availablememory 空间大于申请的 size\n        freeup(size);\n        this.availablememory -= size;\n        lock.unlock();\n        return bytebuffer.allocate(size);\n    } else {\n        // 第二种情况：\n        // 当前可用的内存（availablememory + free）小于申请的 size\n        // 当前线程 block，直到（availablememory + free）大于申请的 size，等待 释放\n        // 如果常常发生这种情况，建议加大缓冲区大小\n        int accumulated = 0;\n        bytebuffer buffer = null;\n        condition morememory = this.lock.newcondition();\n        long remainingtimetoblockns = timeunit.milliseconds.tonanos(maxtimetoblockms);\n        // 当前开辟 size ，内存空间不足，信息存入 waiters\n        this.waiters.addlast(morememory);\n        // 等待，直到 可用内存大于申请的 size\n        while (accumulated < size) {\n            long startwaitns = time.nanoseconds();\n            long timens;\n            boolean waitingtimeelapsed;\n            try {\n                // wait\n                // 超时自动苏醒\n                // 有内存释放被唤醒\n                waitingtimeelapsed = !morememory.await(remainingtimetoblockns, timeunit.nanoseconds);\n            } catch (interruptedexception e) {\n                this.waiters.remove(morememory);\n                throw e;\n            } finally {\n                long endwaitns = time.nanoseconds();\n                timens = math.max(0l, endwaitns - startwaitns);\n                this.waittime.record(timens, time.milliseconds());\n            }\n            // 如果是超时等待，表示在规定的时间没有可用的内存，直接报错，默认 60s - 获取元数据时间\n            if (waitingtimeelapsed) {\n                this.waiters.remove(morememory);\n                throw new timeoutexception("failed to allocate memory within the configured max blocking time " + maxtimetoblockms + " ms.");\n            }\n\n            remainingtimetoblockns -= timens;\n            // 到这一步，说明有内存释放\n            if (accumulated == 0 && size == this.poolablesize && !this.free.isempty()) {\n                // 申请内存size = batch size，把 free 中 bytebuffer 返回\n                buffer = this.free.pollfirst();\n                accumulated = size;\n            } else {\n                // 累加 可用内存大小，accumulated\n                freeup(size - accumulated);\n                int got = (int) math.min(size - accumulated, this.availablememory);\n                this.availablememory -= got;\n                accumulated += got;\n            }\n        }\n\n   public void deallocate(bytebuffer buffer, int size) {\n      lock.lock();\n      try {\n          if (size == this.poolablesize && size == buffer.capacity()) {\n              // 释放的内存大小 = batch size\n              // 清空 buffer，并添加到 free\n              // 内存池管理的体现：用完内存不释放，直接复用，减少 gc\n              buffer.clear();\n              this.free.add(buffer);\n          } else {\n              // 释放的内存大小 != batch size，直接释放等待 gc\n              this.availablememory += size;\n          }\n          // 有内存释放，唤醒在等待内存开辟的 waiters\n          condition moremem = this.waiters.peekfirst();\n          if (moremem != null)\n              // waitingtimeelapsed = !morememory.await(remainingtimetoblockns, timeunit.nanoseconds);\n              moremem.signal();\n      } finally {\n          lock.unlock();\n      }\n  }\n\n\n内存申请和释放主要代码如上所示，接下来理清设计：\n\n * 缓冲池分为：free 和 availablememory\n * availablememory 表示未分配可用的内存，主要是计算是否可分配 size 内存：分配时减少，释放时增加，回收靠 gc\n * free ：bytebuffer 队列，bytebuffer 大小 = batch size，一般恰好是每次开辟的大小，释放时清空数据重新添加到队列\n\n按 batch size 大小划分 bytebuffer，增大复用效果；不等于 batch size 的内存空间直接 gc，不会出现内存碎片问题。\n\n\n\n\n# nio\n\n有了以上的知识后，我们可以将 record 加入到 batch 中，准备发送到 kafka broker。在发送之前，我们先分析 kafka 的使用的网络框架 nio，有助于后续分析 sender 线程。网络一般涉及三个过程：建立连接，发送数据，读响应。\n\n// networkclient 是 kafka 网络请求实现类\n// networkclient 主要是封装 nio 中 selector 操作\n// 下面主要分析 kafka 中 selector 操作，关于 networkclient 放到后续的 sender 线程一起分析\n// org.apache.kafka.common.network.selector\npublic class selector implements selectable {\n  // java nio，不了解的先去熟悉下\n  private final java.nio.channels.selector nioselector;\n  // 记录对 kafka broker 连接\n  private final map<string, kafkachannel> channels;\n\n  // 与 kafka broker 建立连接，id = kafka node\n  public void connect(string id, inetsocketaddress address, int sendbuffersize, int receivebuffersize) throws ioexception {\n    ...\n    socketchannel socketchannel = socketchannel.open();\n    // 设置非阻塞模式\n    socketchannel.configureblocking(false);\n    socket socket = socketchannel.socket();\n    socket.setkeepalive(true);\n    if (sendbuffersize != selectable.use_default_buffer_size)\n        socket.setsendbuffersize(sendbuffersize);\n    if (receivebuffersize != selectable.use_default_buffer_size)\n        socket.setreceivebuffersize(receivebuffersize);\n    // tcp 优化\n    socket.settcpnodelay(true);\n    boolean connected;\n    try {\n        // 尝试建立连接\n        connected = socketchannel.connect(address);\n    } catch (unresolvedaddressexception e) {\n        socketchannel.close();\n        throw new ioexception("can\'t resolve address: " + address, e);\n    } catch (ioexception e) {\n        socketchannel.close();\n        throw e;\n    }\n    // channel 加到 nio selector，并添加 op_connect 事件\n    // 后续 chanel 建立连接后，由 nio selector 返回 connect 事件\n    selectionkey key = socketchannel.register(nioselector, selectionkey.op_connect);\n    // id = kafka node；kafkachannel 只是对 java channel 的封装\n    kafkachannel channel = channelbuilder.buildchannel(id, key, maxreceivesize);\n    // 后续处理时可通过 key.attachment 得到 channel\n    key.attach(channel);\n\n    // 记录 kafka node 对应的 channel\n    this.channels.put(id, channel);\n    ...\n  }\n\n  // 发送数据\n  public void send(send send) {\n    // 通过send 获取 kafka node 得到对应的 channel\n    kafkachannel channel = channelorfail(send.destination());\n    try {\n        // channel 发送数据，此时只是为 channel 添加 op_write，并未真正发送\n        channel.setsend(send);\n    } catch (cancelledkeyexception e) {\n        this.failedsends.add(send.destination());\n        close(channel);\n    }\n  }\n\n  // sender 线程中会定时调用获取 event \n  // 读取 nio selector event，time 表示是否需要阻塞等待事件发生\n  public void poll(long timeout) throws ioexception {\n    ...\n    // block 直到有事件\n    // 注意此时会 wait，那么会导致 sender 线程也会休眠，需要调用 sender.wakeup 唤醒\n    int readykeys = select(timeout);\n\n    long endselect = time.nanoseconds();\n    this.sensors.selecttime.record(endselect - startselect, time.milliseconds());\n\n    // 有需要响应的事件\n    if (readykeys > 0 || !immediatelyconnectedkeys.isempty()) {\n        pollselectionkeys(this.nioselector.selectedkeys(), false, endselect);\n        pollselectionkeys(immediatelyconnectedkeys, true, endselect);\n    }\n    // 处理服务端返回的响应,stagedreceives\n    addtocompletedreceives();\n    ...\n  }\n  // 处理 nio selector 返回的事件\n  // connect\n  // read\n  // write\n  private void pollselectionkeys(iterable<selectionkey> selectionkeys,\n                                boolean isimmediatelyconnected,\n                                long currenttimenanos) {\n    // eventkeys\n    iterator<selectionkey> iterator = selectionkeys.iterator();\n    while (iterator.hasnext()) {\n        selectionkey key = iterator.next();\n        iterator.remove();\n        kafkachannel channel = channel(key);\n        ...\n        try {\n            // 处理 connect ，与 kafka node 已建立连接：op_connect\n            if (isimmediatelyconnected || key.isconnectable()) {\n                // 连接建立完成后，channel 上对应的key 去除 op_connect，添加 op_read 事件\n                if (channel.finishconnect()) {\n                    // 记录当前 kafka node 已建立连接\n                    this.connected.add(channel.id());\n                    this.sensors.connectioncreated.record();\n                    socketchannel socketchannel = (socketchannel) key.channel();\n                    log.debug("created socket with so_rcvbuf = {}, so_sndbuf = {}, so_timeout = {} to node {}",\n                            socketchannel.socket().getreceivebuffersize(),\n                            socketchannel.socket().getsendbuffersize(),\n                            socketchannel.socket().getsotimeout(),\n                            channel.id());\n                } else\n                    continue;\n            }\n            ...\n            // 处理 read：在建立连接后已添加 op_read 事件\n            if (channel.ready() && key.isreadable() && !hasstagedreceive(channel)) {\n                networkreceive networkreceive;\n                while ((networkreceive = channel.read()) != null)\n                    addtostagedreceives(channel, networkreceive);\n            }\n\n            // 处理 write：op_write (send(send send))\n            if (channel.ready() && key.iswritable()) {\n                // channel.setsend(send); 为 chanel 添加 send\n                // 通过 chanel 发送数据：recordbatchs\n                send send = channel.write();\n                if (send != null) {\n                    this.completedsends.add(send);\n                    this.sensors.recordbytessent(channel.id(), send.size());\n                }\n            }\n\n            // 关闭连接\n            if (!key.isvalid()) {\n                close(channel);\n                this.disconnected.add(channel.id());\n            }\n         ...\n  }\n}\n\n\n\n\n# read\n\n当使用 nio ，由于 buffer 大小不匹配问题，必然会碰到粘包或拆包的问题。假设两个响应返回 “hello word” 和 “flink kafka”，有可能是返回(一个返回就好触发一个 read event)\n\n * “hello word”\n * “flink “\n * “kafka”\n\n这叫拆包，也有可能返回\n\n * “hello wordflink”\n * “kafka”\n\n这叫粘包\n\nkafka 的解决方案是相当于在原有 data 基础上增加 header，header 只包含 data size，很朴素和通用方案。两个数据包变成 10”hello word”，11”flink kafka”。此时读取方式变为先读取4字节 size，然后再开辟 size 大小的 buffer 存data，死循环直到 data buffer 读满\n\n// org.apache.kafka.common.network.selector\nif (channel.ready() && key.isreadable() && !hasstagedreceive(channel)) {\n    networkreceive networkreceive;\n    // read receive返回null 表示当前响应包为读取完毕，等待下个 read event再读\n    while ((networkreceive = channel.read()) != null)\n        // receive complete 才算响应读取完毕\n        // receive 添加到 stagedreceives\n        addtostagedreceives(channel, networkreceive);\n}\n// org.apache.kafka.common.network.kafkachannel\npublic networkreceive read() throws ioexception {\n    networkreceive result = null;\n\n    if (receive == null) {\n        receive = new networkreceive(maxreceivesize, id);\n    }\n    // read\n    receive(receive);\n    // size 和 data 都读取完毕,才算 complete\n    if (receive.complete()) {\n        receive.payload().rewind();\n        result = receive;\n        receive = null;\n    }\n    return result;\n}\nprivate long receive(networkreceive receive) throws ioexception {\n    return receive.readfrom(transportlayer);\n}\npublic long readfromreadablechannel(readablebytechannel channel) throws ioexception {\n  int read = 0;\n  // 读取 size；\n  if (size.hasremaining()) {\n      int bytesread = channel.read(size);\n      if (bytesread < 0)\n          throw new eofexception();\n      read += bytesread;\n      if (!size.hasremaining()) {\n          size.rewind();\n          int receivesize = size.getint();\n          ...\n          // 读取到size，可知 data 长度，申请存放 data 所需的 buffer\n          this.buffer = bytebuffer.allocate(receivesize);\n      }\n  }\n  // 读取 data；每次进来都必会读取 buffer\n  if (buffer != null) {\n      int bytesread = channel.read(buffer);\n      if (bytesread < 0)\n          throw new eofexception();\n      read += bytesread;\n  }\n  return read;\n}\n\n\n\n# sender 线程\n\n整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和sender 线程(发送线程)。在主线程中由kafkaproducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器(recordaccumulator，也称为消息收集器)中。sender 线程负责从recordaccumulator中获取消息并将其发送到kafka中。\n\n到此我们具备了发送 rocordbatch 的环境，开始向服务端发送流程。\n\nsender 是 kafka 生产者的核心线程，负责从消息累加器中提取消息，并通过网络发送到 kafka 服务器。其工作流程如下：\n\n 1. 从 recordaccumulator 中取出已准备好的消息批次。\n 2. 根据消息的分区和主题找到对应的 kafka 服务器。\n 3. 发送消息，并处理响应（包括重试、超时、失败等情况）。\n\nsender 线程是异步运行的，通过高效的 i/o 和批量发送机制保证了生产者的高性能。在高吞吐量的场景下，调整 sender 线程的相关参数（如 retries 和 max.in.flight.requests.per.connection）能够显著提升消息发送的效率。\n\n// org.apache.kafka.clients.producer.kafkaproducer\n// 满足 batch 发送条件，唤醒 sender 发送数据\nthis.sender.wakeup();      // 记得为什么这里要唤醒吗？selector.poll 有可能会 wait\n\n// 后台线程发送请求\n// org.apache.kafka.clients.producer.internals.sender\npublic class sender implements runnable {\n  private final kafkaclient client;\n  // while（run）\n  void run(long now) {\n    cluster cluster = metadata.fetch();\n    // 从缓冲池 accumulator 筛选出满足 batch send 的 kafka nodes\n    recordaccumulator.readycheckresult result = this.accumulator.ready(cluster, now);\n\n    // 如果有 topic 的元数据信息位获取，设置元数据标识\n    if (!result.unknownleadertopics.isempty()) {\n        for (string topic : result.unknownleadertopics)\n            this.metadata.add(topic);\n        this.metadata.requestupdate();\n    }\n\n    // 从需要发送的 kafka nodes 中移除没有准备好连接的 node，判断条件\n    // 元数据包含当前 kafka node\n    // 与当前 kafka node connection 已建立\n    // 与当前 kafka node channel 已建立\n    // 发送 batch 请求数未超过指定的最大值(默认5)，生产中一般设置为1，保证数据不乱序\n    iterator<node> iter = result.readynodes.iterator();\n    long notreadytimeout = long.max_value;\n    while (iter.hasnext()) {\n        node node = iter.next();\n        // ready 中如果发现未连接会去初始化连接 selector.connect\n        if (!this.client.ready(node, now)) {\n            iter.remove();\n            notreadytimeout = math.min(notreadytimeout, this.client.connectiondelay(node, now));\n        }\n    }\n\n    // 从准备好发送的 kafkanodes 中，获取需要发送的 recordbatch \n    // 第一次调用时，与 kafka node 的连接都没有建立，此时 result.readynodes 为空\n    // 在 poll 函数发现 connect 事件，此时会发起连接，那么下次再准备发送时就可以了\n    map<integer, list<recordbatch>> batches = this.accumulator.drain(cluster,\n                                                                      result.readynodes,\n                                                                      this.maxrequestsize,\n                                                                      now);\n    if (guaranteemessageorder) {\n        // mute all the partitions drained\n        for (list<recordbatch> batchlist : batches.values()) {\n            for (recordbatch batch : batchlist)\n                this.accumulator.mutepartition(batch.topicpartition);\n        }\n    }\n\n    // 处理超时的发送\n    list<recordbatch> expiredbatches = this.accumulator.abortexpiredbatches(this.requesttimeout, now);\n    ...\n    // 创建请求：将要发送的 recordbatch 封装成clientrequest\n    // 每个requests 都有个回调函数(handleproduceresponse)，在收到响应时调用\n    list<clientrequest> requests = createproducerequests(batches, now);\n    long polltimeout = math.min(result.nextreadycheckdelayms, notreadytimeout);\n    if (result.readynodes.size() > 0) {\n        log.trace("nodes with data ready to send: {}", result.readynodes);\n        log.trace("created {} produce requests: {}", requests.size(), requests);\n        polltimeout = 0;\n    }\n    // 发送数据，这里只是在 select 注册 op_write，并在 channel 添加 send\n    for (clientrequest request : requests)\n        client.send(request, now);\n\n    // 如果需要有数据发送，select.poll(0)，不等待直接返回，因为有数据要发送\n    // 如果没有需要发送的数据，select.poll(timeout)，距离下次有数据要发送的间隔(linger ms)\n    this.client.poll(polltimeout, now);\n  }\n}\n\n\nsender 线程逻辑很清晰：\n\n * while 以下步骤\n * 检查缓冲区中满足发送条件的 recordbatch\n * 检查要发送的 recordbatch 的元数据是否已准备\n * 删除未建立连接的 kafka node(未连接时会发起连接事件)\n * 获取已建立连接 kafka node 的 recordbatch\n * 将 recordbatchs 封装成 clientrequest\n * 发送 clientrequest\n * poll\n\n\n\n# 发起连接\n\n程序刚运行的时候肯定是没有建立连接，当发现有 recordbatch 需要发送时会先判断是否已连接。\n\n// this.client.ready(node, now)\n// org.apache.kafka.clients.networkclient\npublic boolean ready(node node, long now) {\n  if (node.isempty())\n      throw new illegalargumentexception("cannot connect to empty node " + node);\n  // entry\n  if (isready(node, now))\n      return true;\n  // 未连接或失去连接，初始化\n  if (connectionstates.canconnect(node.idstring(), now))\n      // selector.connect: op_connect\n      initiateconnect(node, now);\n  return false;\n}\n/** * 检查是否可以向当前 kafka node 发送消息： * 元数据包含当前 kafka node * 与当前 kafka node connection 已建立 * 与当前 kafka node channel 已建立 * 发送 batch 请求数未超过指定的最大值(默认5) */\npublic boolean isready(node node, long now) {\n    // if we need to update our metadata now declare all requests unready to make metadata requests first\n    // priority\n    return !metadataupdater.isupdatedue(now) && cansendrequest(node.idstring());\n}\n\n\n# clientrequest\n\n要发送的 recordbatch 封装成 clientrequest\n\n// list<clientrequest> requests = createproducerequests(batches, now);\n// org.apache.kafka.clients.producer.internals.sender\nprivate list<clientrequest> createproducerequests(map<integer, list<recordbatch>> collated, long now) {\n  list<clientrequest> requests = new arraylist<clientrequest>(collated.size());\n  for (map.entry<integer, list<recordbatch>> entry : collated.entryset())\n      // 创建请求：acks 非常重要，requesttimeout 默认 30s\n      // 将同个 node 上的多个 batch组装在一起\n      requests.add(producerequest(now, entry.getkey(), acks, requesttimeout, entry.getvalue()));\n  return requests;\n}\nprivate clientrequest producerequest(long now, int destination, short acks, int timeout, list<recordbatch> batches) {\n  map<topicpartition, bytebuffer> producerecordsbypartition = new hashmap<topicpartition, bytebuffer>(batches.size());\n  final map<topicpartition, recordbatch> recordsbypartition = new hashmap<topicpartition, recordbatch>(batches.size());\n  for (recordbatch batch : batches) {\n      topicpartition tp = batch.topicpartition;\n      producerecordsbypartition.put(tp, batch.records.buffer());\n      recordsbypartition.put(tp, batch);\n  }\n  producerequest request = new producerequest(acks, timeout, producerecordsbypartition);\n  // recordbatch 数据存到 requestsend 数据结构\n  requestsend send = new requestsend(integer.tostring(destination),\n                                      // apikeys.produce 标识符，服务端会根据这个标识符处理\n                                      this.client.nextrequestheader(apikeys.produce),\n                                      request.tostruct());\n  // 设置请求结束的回调函数，后续在响应会用到 \n  requestcompletionhandler callback = new requestcompletionhandler() {\n      public void oncomplete(clientresponse response) {\n          handleproduceresponse(response, recordsbypartition, time.milliseconds());\n      }\n  };\n\n  return new clientrequest(now, acks != 0, send, callback);\n}\n\n\n# 响应\n\n// this.client.poll(polltimeout, now);\n// org.apache.kafka.clients.networkclient\npublic list<clientresponse> poll(long timeout, long now) {\n  // 是否需要更新元数据\n  long metadatatimeout = metadataupdater.maybeupdate(now);\n  try {\n      // selector.poll，处理 op_connect、op_write、op_read 事件\n      // op_read 通过 nio read -> completedreceives\n      this.selector.poll(utils.min(timeout, metadatatimeout, requesttimeoutms));\n  } catch (ioexception e) {\n      log.error("unexpected error during i/o", e);\n  }\n\n  // process completed actions\n  long updatednow = this.time.milliseconds();\n  list<clientresponse> responses = new arraylist<>();\n  // 处理已发送的 send(recordbatch)\n  // responses.add(new clientresponse(request, now, false, null));\n  handlecompletedsends(responses, updatednow);\n  // 处理返回的响应\n  // responses.add(new clientresponse(req, now, false, body));\n  handlecompletedreceives(responses, updatednow);\n  handledisconnections(responses, updatednow);\n  handleconnections();\n  // 处理超时的请求\n  handletimedoutrequests(responses, updatednow);\n\n  // invoke callbacks\n  for (clientresponse response : responses) {\n      if (response.request().hascallback()) {\n          try {\n              // 回调请求的 callback，handleproduceresponse\n              response.request().callback().oncomplete(response);\n          } catch (exception e) {\n              log.error("uncaught error in request completion:", e);\n          }\n      }\n  }\n  return responses;\n}\n\n\n * 发送结束的响应\n\n// org.apache.kafka.clients.networkclient\nprivate void handlecompletedsends(list<clientresponse> responses, long now) {\n    // completedsends.add(send); selector write 时加入\n    for (send send : this.selector.completedsends()) {\n        clientrequest request = this.inflightrequests.lastsent(send.destination());\n        if (!request.expectresponse()) {\n           // ack = 0，才会构建 send 结束响应，这种方式不需要服务端返回结果\n            this.inflightrequests.completelastsent(send.destination());\n            // responsebody 为 null\n            responses.add(new clientresponse(request, now, false, null));\n        }\n    }\n}\n\n\n * 服务端返回的响应\n\nprivate void handlecompletedreceives(list<clientresponse> responses, long now) {\n  for (networkreceive receive : this.selector.completedreceives()) {\n      string source = receive.source();\n      clientrequest req = inflightrequests.completenext(source);\n      // receive 的二进制解析成可识别的数据\n      struct body = parseresponse(receive.payload(), req.request().header());\n      if (!metadataupdater.maybehandlecompletedreceive(req, now, body))\n          // responsebody 有值\n          responses.add(new clientresponse(req, now, false, body));\n  }\n}\n\n\n * 超时请求的响应\n\nprivate void processdisconnection(list<clientresponse> responses, string nodeid, long now) {\n  connectionstates.disconnected(nodeid, now);\n  for (clientrequest request : this.inflightrequests.clearall(nodeid)) {\n      log.trace("cancelled request {} due to node {} being disconnected", request, nodeid);\n      if (!metadataupdater.maybehandledisconnection(request))\n          // responsebody 为 null 且 disconnected 为 true\n          responses.add(new clientresponse(request, now, true, null));\n  }\n}\n\n\n * 处理响应\n\n// org.apache.kafka.clients.producer.internals.sender\n// 请求回调\nprivate void handleproduceresponse(clientresponse response, map<topicpartition, recordbatch> batches, long now) {\n  // batches = 发送请求时携带的 recordbatch\n  // completebatch\n  int correlationid = response.request().request().header().correlationid();\n  if (response.wasdisconnected()) {\n      // 失去连接\n      log.trace("cancelled request {} due to node {} being disconnected", response, response.request()\n                                                                                            .request()\n                                                                                            .destination());\n      // errors = errors.network_exception，\n      for (recordbatch batch : batches.values())\n          completebatch(batch, errors.network_exception, -1l, record.no_timestamp, correlationid, now);\n  } else {\n      log.trace("received produce response from node {} with correlation id {}",\n                response.request().request().destination(),\n                correlationid);\n      if (response.hasresponse()) {\n          // ack != 0\n          produceresponse produceresponse = new produceresponse(response.responsebody());\n          for (map.entry<topicpartition, produceresponse.partitionresponse> entry : produceresponse.responses().entryset()) {\n              // 每个 batch 处理\n              topicpartition tp = entry.getkey();\n              produceresponse.partitionresponse partresp = entry.getvalue();\n              errors error = errors.forcode(partresp.errorcode);\n              recordbatch batch = batches.get(tp);\n              completebatch(batch, error, partresp.baseoffset, partresp.timestamp, correlationid, now);\n          }\n          this.sensors.recordlatency(response.request().request().destination(), response.requestlatencyms());\n          this.sensors.recordthrottletime(response.request().request().destination(),\n                                          produceresponse.getthrottletime());\n      } else {\n          // ack = 0\n          for (recordbatch batch : batches.values())\n              completebatch(batch, errors.none, -1l, record.no_timestamp, correlationid, now);\n      }\n  }\n}\n// recordbatch 处理\nprivate void completebatch(recordbatch batch, errors error, long baseoffset, long timestamp, long correlationid, long now) {\n  if (error != errors.none && canretry(batch, error)) { // 有异常且可以重新发送\n      // retry\n      log.warn("got error produce response with correlation id {} on topic-partition {}, retrying ({} attempts left). error: {}",\n                correlationid,\n                batch.topicpartition,\n                this.retries - batch.attempts - 1,\n                error);\n      // 重新加入 缓冲区\n      this.accumulator.reenqueue(batch, now);\n  } else {\n      // 无异常\n      // 有异常且不可以重新发送\n      runtimeexception exception;\n      if (error == errors.topic_authorization_failed)\n          exception = new topicauthorizationexception(batch.topicpartition.topic());\n      else\n          exception = error.exception();\n      // record 回调函数\n      batch.done(baseoffset, timestamp, exception);\n      // recordbatch 发送结束，内存池回收该内存\n      this.accumulator.deallocate(batch);\n  ...\n}\n// record 回调 producer.send(,callback)\npublic void done(long baseoffset, long timestamp, runtimeexception exception) {\n  // thunks = records\n  for (int i = 0; i < this.thunks.size(); i++) {\n      try {\n          thunk thunk = this.thunks.get(i);\n          if (exception == null) {\n              // if the timestamp returned by server is notimestamp, that means createtime is used. otherwise logappendtime is used.\n              recordmetadata metadata = new recordmetadata(this.topicpartition,  baseoffset, thunk.future.relativeoffset(),\n                                                            timestamp == record.no_timestamp ? thunk.future.timestamp() : timestamp,\n                                                            thunk.future.checksum(),\n                                                            thunk.future.serializedkeysize(),\n                                                            thunk.future.serializedvaluesize());\n              // 无异常时 record 回调函数\n              thunk.callback.oncompletion(metadata, null);\n          } else {\n              // 有异常时 record 回调函数\n              thunk.callback.oncompletion(null, exception);\n          }\n      } \n}\n\n\n\n\n\n# 总结\n\n\n# 参考资料\n\n * 深入理解 kafka 之 producer - 阿飞的博客 | danner blog (vendanner.github.io)',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Broker",frontmatter:{title:"Broker",date:"2024-09-18T12:58:30.000Z",permalink:"/pages/aa0392/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/07.Broker.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/07.Broker.md",key:"v-0794b35a",path:"/pages/aa0392/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"网络",slug:"网络",normalizedTitle:"网络",charIndex:9},{level:2,title:"日志",slug:"日志",normalizedTitle:"日志",charIndex:16},{level:2,title:"副本",slug:"副本",normalizedTitle:"副本",charIndex:23},{level:2,title:"集群管理",slug:"集群管理",normalizedTitle:"集群管理",charIndex:30}],headersStr:"前言 网络 日志 副本 集群管理",content:"# 前言\n\n\n# 网络\n\n\n# 日志\n\n\n# 副本\n\n\n# 集群管理",normalizedContent:"# 前言\n\n\n# 网络\n\n\n# 日志\n\n\n# 副本\n\n\n# 集群管理",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"消费者",frontmatter:{title:"消费者",date:"2024-09-18T12:42:00.000Z",permalink:"/pages/ca141b/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/10.%E6%B6%88%E8%B4%B9%E8%80%85.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/10.消费者.md",key:"v-2e9dfcd8",path:"/pages/ca141b/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"初始化",slug:"初始化",normalizedTitle:"初始化",charIndex:11},{level:3,title:"参数配置",slug:"参数配置",normalizedTitle:"参数配置",charIndex:19},{level:3,title:"订阅主题与分区",slug:"订阅主题与分区",normalizedTitle:"订阅主题与分区",charIndex:486},{level:2,title:"消费过程",slug:"消费过程",normalizedTitle:"消费过程",charIndex:672},{level:3,title:"反序列化",slug:"反序列化",normalizedTitle:"反序列化",charIndex:415},{level:3,title:"消息消费",slug:"消息消费",normalizedTitle:"消息消费",charIndex:1099},{level:3,title:"位移提交",slug:"位移提交",normalizedTitle:"位移提交",charIndex:273},{level:3,title:"控制或关闭消费",slug:"控制或关闭消费",normalizedTitle:"控制或关闭消费",charIndex:1777},{level:3,title:"指定位移消费",slug:"指定位移消费",normalizedTitle:"指定位移消费",charIndex:2058},{level:2,title:"再均衡",slug:"再均衡",normalizedTitle:"再均衡",charIndex:2332},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:3095}],headersStr:"前言 初始化 参数配置 订阅主题与分区 消费过程 反序列化 消息消费 位移提交 控制或关闭消费 指定位移消费 再均衡 总结",content:'# 前言\n\n\n\n\n# 初始化\n\n\n# 参数配置\n\n在 Kafka 消费者的初始化过程中，需要配置多个关键参数，以保证消费者能够稳定、高效地从 Kafka 集群中读取消息。以下是一些常用的消费者参数：\n\n * bootstrap.servers：Kafka 集群地址，用于连接到 Kafka 集群。\n * group.id：消费者组 ID，Kafka 消费者必须属于某一个组，Kafka 使用消费者组来实现消息的分发和负载均衡。\n * enable.auto.commit：是否自动提交位移，默认值为 true。若设置为 false，则需要手动控制位移提交。\n * auto.offset.reset：当没有初始位移或位移超出范围时，如何处理。常见的选项有 earliest（从最早的消息开始消费）和 latest（从最新的消息开始消费）。\n * key.deserializer 和 value.deserializer：指定用于反序列化消息键和值的类。常见的反序列化器包括 StringDeserializer 和 ByteArrayDeserializer。\n\n\n# 订阅主题与分区\n\nKafka 消费者必须订阅一个或多个主题以开始消费消息。Kafka 提供了两种订阅模式：\n\n 1. 主题订阅：消费者可以通过 subscribe() 方法订阅一个或多个主题，并让 Kafka 自动分配分区。\n 2. 分区分配：消费者可以使用 assign() 方法直接指定消费的分区。这种方式常用于精细化控制，特别是在消息顺序性要求较高的场景。\n\n\n# 消费过程\n\n\n# 反序列化\n\nKafka 消费者从 Kafka 服务器拉取的消息是字节数组，因此需要使用反序列化器将字节数组转换为可读的 Java 对象。key.deserializer 和 value.deserializer 分别用于处理消息的键和值。\n\nKafka 内置了多种常见的反序列化器，开发者也可以根据业务需求自定义反序列化逻辑，需实现 Deserializer 接口。例如：\n\npublic class CustomDeserializer implements Deserializer<MyObject> {\n    @Override\n    public MyObject deserialize(String topic, byte[] data) {\n        // 自定义反序列化逻辑\n        return SerializationUtils.deserialize(data);\n    }\n}\n\n\n\n# 消息消费\n\n消息消费的核心是从 Kafka 中拉取消息并处理它们。Kafka 提供了 poll() 方法来拉取消息，这个方法会返回一个 ConsumerRecords 对象，其中包含多个 ConsumerRecord，每个 ConsumerRecord 代表一条消息。\n\nwhile (true) {\n    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));\n    for (ConsumerRecord<String, String> record : records) {\n        // 处理消息\n        System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());\n    }\n}\n\n\n\n# 位移提交\n\nKafka 消费者需要定期提交位移（offset），以标识已经成功处理的消息。Kafka 提供了两种位移提交方式：\n\n 1. 自动提交：由 enable.auto.commit 参数控制，Kafka 会在 poll() 之后自动提交当前位移。但这种方式可能导致消息丢失或重复消费。\n 2. 手动提交：通过调用 commitSync() 或 commitAsync() 方法手动提交位移。手动提交可以更精确地控制消息的处理，但需要开发者在适当的时间点提交位移。\n\n\n# 控制或关闭消费\n\n在某些情况下，需要暂停或恢复消费，Kafka 提供了 pause() 和 resume() 方法用于控制指定分区的消费。例如，消费者可以在处理慢速批量任务时暂停消费，待任务完成后恢复消费。\n\nconsumer.pause(Collections.singletonList(new TopicPartition("my-topic", 0)));\n// 处理完慢速任务后恢复\nconsumer.resume(Collections.singletonList(new TopicPartition("my-topic", 0)));\n\n\n\n# 指定位移消费\n\n除了自动消费最新的消息外，Kafka 也允许消费者从指定的位移位置开始消费消息。通过 seek() 方法，可以精确地控制消费的开始位置，常用于故障恢复或特定时间点的数据回溯。\n\nTopicPartition partition = new TopicPartition("my-topic", 0);\nconsumer.assign(Collections.singletonList(partition));\nconsumer.seek(partition, 1234L); // 从 offset 1234 开始消费\n\n\n\n# 再均衡\n\nKafka 消费者组中的多个消费者会共同消费一个或多个主题的分区。Kafka 通过再均衡（rebalance）机制来动态调整分区分配，以确保每个消费者处理相应的分区。在以下情况下，Kafka 会触发再均衡：\n\n * 有新的消费者加入或已有消费者离开。\n * 消费者组中的某个消费者长时间未发送心跳。\n\n再均衡对消费过程有一定影响，可能导致分区的临时不可用，或使正在处理的消息丢失。因此，在设计高可用消费应用时，需要考虑再均衡的处理。可以通过实现 ConsumerRebalanceListener 来控制再均衡的逻辑，特别是保存和恢复位移。\n\nconsumer.subscribe(Arrays.asList("my-topic"), new ConsumerRebalanceListener() {\n    @Override\n    public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n        // 再均衡之前，保存当前位移\n        consumer.commitSync();\n    }\n\n    @Override\n    public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n        // 再均衡之后，恢复位移\n        for (TopicPartition partition : partitions) {\n            consumer.seek(partition, getOffset(partition));\n        }\n    }\n});\n\n\n\n# 总结',normalizedContent:'# 前言\n\n\n\n\n# 初始化\n\n\n# 参数配置\n\n在 kafka 消费者的初始化过程中，需要配置多个关键参数，以保证消费者能够稳定、高效地从 kafka 集群中读取消息。以下是一些常用的消费者参数：\n\n * bootstrap.servers：kafka 集群地址，用于连接到 kafka 集群。\n * group.id：消费者组 id，kafka 消费者必须属于某一个组，kafka 使用消费者组来实现消息的分发和负载均衡。\n * enable.auto.commit：是否自动提交位移，默认值为 true。若设置为 false，则需要手动控制位移提交。\n * auto.offset.reset：当没有初始位移或位移超出范围时，如何处理。常见的选项有 earliest（从最早的消息开始消费）和 latest（从最新的消息开始消费）。\n * key.deserializer 和 value.deserializer：指定用于反序列化消息键和值的类。常见的反序列化器包括 stringdeserializer 和 bytearraydeserializer。\n\n\n# 订阅主题与分区\n\nkafka 消费者必须订阅一个或多个主题以开始消费消息。kafka 提供了两种订阅模式：\n\n 1. 主题订阅：消费者可以通过 subscribe() 方法订阅一个或多个主题，并让 kafka 自动分配分区。\n 2. 分区分配：消费者可以使用 assign() 方法直接指定消费的分区。这种方式常用于精细化控制，特别是在消息顺序性要求较高的场景。\n\n\n# 消费过程\n\n\n# 反序列化\n\nkafka 消费者从 kafka 服务器拉取的消息是字节数组，因此需要使用反序列化器将字节数组转换为可读的 java 对象。key.deserializer 和 value.deserializer 分别用于处理消息的键和值。\n\nkafka 内置了多种常见的反序列化器，开发者也可以根据业务需求自定义反序列化逻辑，需实现 deserializer 接口。例如：\n\npublic class customdeserializer implements deserializer<myobject> {\n    @override\n    public myobject deserialize(string topic, byte[] data) {\n        // 自定义反序列化逻辑\n        return serializationutils.deserialize(data);\n    }\n}\n\n\n\n# 消息消费\n\n消息消费的核心是从 kafka 中拉取消息并处理它们。kafka 提供了 poll() 方法来拉取消息，这个方法会返回一个 consumerrecords 对象，其中包含多个 consumerrecord，每个 consumerrecord 代表一条消息。\n\nwhile (true) {\n    consumerrecords<string, string> records = consumer.poll(duration.ofmillis(100));\n    for (consumerrecord<string, string> record : records) {\n        // 处理消息\n        system.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());\n    }\n}\n\n\n\n# 位移提交\n\nkafka 消费者需要定期提交位移（offset），以标识已经成功处理的消息。kafka 提供了两种位移提交方式：\n\n 1. 自动提交：由 enable.auto.commit 参数控制，kafka 会在 poll() 之后自动提交当前位移。但这种方式可能导致消息丢失或重复消费。\n 2. 手动提交：通过调用 commitsync() 或 commitasync() 方法手动提交位移。手动提交可以更精确地控制消息的处理，但需要开发者在适当的时间点提交位移。\n\n\n# 控制或关闭消费\n\n在某些情况下，需要暂停或恢复消费，kafka 提供了 pause() 和 resume() 方法用于控制指定分区的消费。例如，消费者可以在处理慢速批量任务时暂停消费，待任务完成后恢复消费。\n\nconsumer.pause(collections.singletonlist(new topicpartition("my-topic", 0)));\n// 处理完慢速任务后恢复\nconsumer.resume(collections.singletonlist(new topicpartition("my-topic", 0)));\n\n\n\n# 指定位移消费\n\n除了自动消费最新的消息外，kafka 也允许消费者从指定的位移位置开始消费消息。通过 seek() 方法，可以精确地控制消费的开始位置，常用于故障恢复或特定时间点的数据回溯。\n\ntopicpartition partition = new topicpartition("my-topic", 0);\nconsumer.assign(collections.singletonlist(partition));\nconsumer.seek(partition, 1234l); // 从 offset 1234 开始消费\n\n\n\n# 再均衡\n\nkafka 消费者组中的多个消费者会共同消费一个或多个主题的分区。kafka 通过再均衡（rebalance）机制来动态调整分区分配，以确保每个消费者处理相应的分区。在以下情况下，kafka 会触发再均衡：\n\n * 有新的消费者加入或已有消费者离开。\n * 消费者组中的某个消费者长时间未发送心跳。\n\n再均衡对消费过程有一定影响，可能导致分区的临时不可用，或使正在处理的消息丢失。因此，在设计高可用消费应用时，需要考虑再均衡的处理。可以通过实现 consumerrebalancelistener 来控制再均衡的逻辑，特别是保存和恢复位移。\n\nconsumer.subscribe(arrays.aslist("my-topic"), new consumerrebalancelistener() {\n    @override\n    public void onpartitionsrevoked(collection<topicpartition> partitions) {\n        // 再均衡之前，保存当前位移\n        consumer.commitsync();\n    }\n\n    @override\n    public void onpartitionsassigned(collection<topicpartition> partitions) {\n        // 再均衡之后，恢复位移\n        for (topicpartition partition : partitions) {\n            consumer.seek(partition, getoffset(partition));\n        }\n    }\n});\n\n\n\n# 总结',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Kafka Producer 设计分析",frontmatter:{title:"Kafka Producer 设计分析",date:"2024-09-18T15:32:05.000Z",permalink:"/pages/19bf78/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/500.%E3%80%8C%E7%94%9F%E4%BA%A7%E8%80%85%E3%80%8D%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/10.Kafka%20Producer%20%E8%AE%BE%E8%AE%A1%E5%88%86%E6%9E%90.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/10.Kafka Producer 设计分析.md",key:"v-06d52030",path:"/pages/19bf78/",headers:[{level:3,title:"Kafka Producer 设计分析",slug:"kafka-producer-设计分析",normalizedTitle:"kafka producer 设计分析",charIndex:2},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1840}],headersStr:"Kafka Producer 设计分析 参考资料",content:"# Kafka Producer 设计分析\n\n客户端通过调用producer进行消息发送，这是消息的起源，所以我们最先分析Producer源代码。但如果一上来就切入代码，难免晦涩难以理解，在这里我先以生活中的例子作为开始，帮助理解Producer的设计理念。\n\n我们举一个很常见的例子：发快递。为什么举这个例子呢，因为Kafka采用NIO通讯，如果大家学习过NIO，会知道经常用发快递来举例讲解NIO。另外发快递的场景确实也很像Kafka producer发送消息的场景。\n\n快递公司一般是这样运作，由快递员上门取件，统一送回站点，站点进行分拣，同一个地区的包裹会装进一辆车，装满后发送出去。\n\n在这个过程中，有如下几个角色\n\n * 收件员，负责把快递运送回站点\n * 分拣员，负责把快递按地区分类，比如唐山和秦皇岛，虽然具体的地址不一样但是都属于一个大区。\n * 运输车，负责把快递运输出去，送到指定地区。\n\n我们思考一下快递公司这样设计的好处\n\n * 角色分工，收件员只负责收取快递，黏贴快递单，协助打包，然后送回站点。分拣员负责分类。运输车负责运输。分工明确，相互之间不需要知道对方在做什么。多人并行工作。\n * 快递累积，分拣员把快递按地区分类，够一车后，发送出去。而不是收到一个件就发送出去。这是快递公司的通常做法，显然效率是更高的。然而实效性要求更高的闪送，则是收到一个件，马上发送出去，这样做是延迟最小的方式。\n * 同区归并，真正发送的时候，唐山的快递和秦皇岛的快递会在一辆车发送出去，先统一发往河北总站。到达总站后再分车运往具体的城市。这样减少了发车的频次。如果唐山一车，秦皇岛一车，如果装不满一车，就会造成资源浪费，并且发车次数变多。\n\n了解了快递公司的工作方式后，我们在宏观上看一下KafkaProducer的设计。\n\nKafkaProducer的设计理念如出一辙，首先主线程把要发送的消息按照主题分区进行累积，达到一定数量后，触发发送线程进行发送。为了提高发送的效率，把发往同一个服务器的消息进行归并，一次性发往相应的服务器。\n\nProducer设计中也有相应的角色：\n\n * 收件员-KafkaProducer。实际除收件员外，它还承担了更多的工作。我们发送消息第一步就是调用KafkaProducer.send()方法\n * 分拣员-RecordAccumulator。负责把消息按照分区分类，放入相应队列的ProducerBatch中\n * 运输车-Sender。负责运输，把消息真正发送出去。其实它内部还很复杂，通过NIO实现网络传输。\n\n此外还有些相关类如下：\n\n * ProducerBatch，每个ProducerBatch是一个信件箱，而同一个patition的信件箱码放在一起，程序中这就是Deque\n * ClientRequest，可以理解为运输车的货箱，在运输前，我们会把发往同一个服务器的消息放入ClientRequest，那么只需要发送一ClientRequest，就可以把不同主题不同分区，但发往同一台服务器上的消息，一次性发送过去。\n\n通过以上的讲解，Producer涉及到的主要类都已经进行了简单讲解，各自负责的事情也很清晰。下面我给出一张图，通过此图来讲解producer工作的主流程：\n\n\n\n图中可以看到，有两个线程同时在工作，一个线程负责把消息送往消息站进行分组，另外一个线程负责把消息真正发送出去。\n\n客户端发送消息时调用KafkaProducer的send方法。内部逻辑如下：\n\n 1. 首先对消息进行加工，如序列化，选择分区等\n 2. 然后通过RecordAccumulator把加工好的消息放入相应的ProducerBatch中\n 3. 当batch满时，触发sender线程工作\n 4. sender线程首先把batch从原列表中取出来，按照发往的broker进行分组，然后封装到ClientRequest中\n 5. 最后通过NIO的方式把ClientRequest发往相应的broker\n\n至此，我们应该已经理解了 Kafka Producer 的设计思路。可见所有软件设计都来源于生活，都是对生活中的相应场景进行抽象和面向对象的设计。软件是无形的，但是实际生活中我们所采用的工作方式是有型的。通过参照实际生活场景和解决方案做软件的设计，让你的设计贴近实际场景，这样的代码写出来易于理解和扩展。\n\n\n# 参考资料\n\n你绝对能看懂的Kafka源代码分析-Kafka Producer设计分析_apache kafka源码剖析-CSDN博客",normalizedContent:"# kafka producer 设计分析\n\n客户端通过调用producer进行消息发送，这是消息的起源，所以我们最先分析producer源代码。但如果一上来就切入代码，难免晦涩难以理解，在这里我先以生活中的例子作为开始，帮助理解producer的设计理念。\n\n我们举一个很常见的例子：发快递。为什么举这个例子呢，因为kafka采用nio通讯，如果大家学习过nio，会知道经常用发快递来举例讲解nio。另外发快递的场景确实也很像kafka producer发送消息的场景。\n\n快递公司一般是这样运作，由快递员上门取件，统一送回站点，站点进行分拣，同一个地区的包裹会装进一辆车，装满后发送出去。\n\n在这个过程中，有如下几个角色\n\n * 收件员，负责把快递运送回站点\n * 分拣员，负责把快递按地区分类，比如唐山和秦皇岛，虽然具体的地址不一样但是都属于一个大区。\n * 运输车，负责把快递运输出去，送到指定地区。\n\n我们思考一下快递公司这样设计的好处\n\n * 角色分工，收件员只负责收取快递，黏贴快递单，协助打包，然后送回站点。分拣员负责分类。运输车负责运输。分工明确，相互之间不需要知道对方在做什么。多人并行工作。\n * 快递累积，分拣员把快递按地区分类，够一车后，发送出去。而不是收到一个件就发送出去。这是快递公司的通常做法，显然效率是更高的。然而实效性要求更高的闪送，则是收到一个件，马上发送出去，这样做是延迟最小的方式。\n * 同区归并，真正发送的时候，唐山的快递和秦皇岛的快递会在一辆车发送出去，先统一发往河北总站。到达总站后再分车运往具体的城市。这样减少了发车的频次。如果唐山一车，秦皇岛一车，如果装不满一车，就会造成资源浪费，并且发车次数变多。\n\n了解了快递公司的工作方式后，我们在宏观上看一下kafkaproducer的设计。\n\nkafkaproducer的设计理念如出一辙，首先主线程把要发送的消息按照主题分区进行累积，达到一定数量后，触发发送线程进行发送。为了提高发送的效率，把发往同一个服务器的消息进行归并，一次性发往相应的服务器。\n\nproducer设计中也有相应的角色：\n\n * 收件员-kafkaproducer。实际除收件员外，它还承担了更多的工作。我们发送消息第一步就是调用kafkaproducer.send()方法\n * 分拣员-recordaccumulator。负责把消息按照分区分类，放入相应队列的producerbatch中\n * 运输车-sender。负责运输，把消息真正发送出去。其实它内部还很复杂，通过nio实现网络传输。\n\n此外还有些相关类如下：\n\n * producerbatch，每个producerbatch是一个信件箱，而同一个patition的信件箱码放在一起，程序中这就是deque\n * clientrequest，可以理解为运输车的货箱，在运输前，我们会把发往同一个服务器的消息放入clientrequest，那么只需要发送一clientrequest，就可以把不同主题不同分区，但发往同一台服务器上的消息，一次性发送过去。\n\n通过以上的讲解，producer涉及到的主要类都已经进行了简单讲解，各自负责的事情也很清晰。下面我给出一张图，通过此图来讲解producer工作的主流程：\n\n\n\n图中可以看到，有两个线程同时在工作，一个线程负责把消息送往消息站进行分组，另外一个线程负责把消息真正发送出去。\n\n客户端发送消息时调用kafkaproducer的send方法。内部逻辑如下：\n\n 1. 首先对消息进行加工，如序列化，选择分区等\n 2. 然后通过recordaccumulator把加工好的消息放入相应的producerbatch中\n 3. 当batch满时，触发sender线程工作\n 4. sender线程首先把batch从原列表中取出来，按照发往的broker进行分组，然后封装到clientrequest中\n 5. 最后通过nio的方式把clientrequest发往相应的broker\n\n至此，我们应该已经理解了 kafka producer 的设计思路。可见所有软件设计都来源于生活，都是对生活中的相应场景进行抽象和面向对象的设计。软件是无形的，但是实际生活中我们所采用的工作方式是有型的。通过参照实际生活场景和解决方案做软件的设计，让你的设计贴近实际场景，这样的代码写出来易于理解和扩展。\n\n\n# 参考资料\n\n你绝对能看懂的kafka源代码分析-kafka producer设计分析_apache kafka源码剖析-csdn博客",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Kafka Producer 源码分析",frontmatter:{title:"Kafka Producer 源码分析",date:"2024-09-18T15:32:32.000Z",permalink:"/pages/32a8ff/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/500.%E3%80%8C%E7%94%9F%E4%BA%A7%E8%80%85%E3%80%8D%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/15.Kafka%20Producer%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/15.Kafka Producer 源码分析.md",key:"v-40d02c90",path:"/pages/32a8ff/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"核心源码",slug:"核心源码",normalizedTitle:"核心源码",charIndex:2323},{level:3,title:"1、判断producer是否可用",slug:"_1、判断producer是否可用",normalizedTitle:"1、判断producer是否可用",charIndex:2332},{level:3,title:"2、判断这个topic的metadata是否可用",slug:"_2、判断这个topic的metadata是否可用",normalizedTitle:"2、判断这个topic的metadata是否可用",charIndex:2625},{level:3,title:"3、对key和value进行序列化",slug:"_3、对key和value进行序列化",normalizedTitle:"3、对key和value进行序列化",charIndex:3122},{level:3,title:"4、获取消息要发往的分区编号",slug:"_4、获取消息要发往的分区编号",normalizedTitle:"4、获取消息要发往的分区编号",charIndex:3330},{level:3,title:"5、计算序列化后大小",slug:"_5、计算序列化后大小",normalizedTitle:"5、计算序列化后大小",charIndex:3534},{level:3,title:"5、通过RecordAccumulator把消息加入batch中",slug:"_5、通过recordaccumulator把消息加入batch中",normalizedTitle:"5、通过recordaccumulator把消息加入batch中",charIndex:3828},{level:3,title:"6、如果batch正好满了，或者创建了新batch来容纳消息，则唤醒sender线程执行发送。",slug:"_6、如果batch正好满了-或者创建了新batch来容纳消息-则唤醒sender线程执行发送。",normalizedTitle:"6、如果batch正好满了，或者创建了新batch来容纳消息，则唤醒sender线程执行发送。",charIndex:4126},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4390},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:5791}],headersStr:"概述 核心源码 1、判断producer是否可用 2、判断这个topic的metadata是否可用 3、对key和value进行序列化 4、获取消息要发往的分区编号 5、计算序列化后大小 5、通过RecordAccumulator把消息加入batch中 6、如果batch正好满了，或者创建了新batch来容纳消息，则唤醒sender线程执行发送。 总结 参考资料",content:'# 概述\n\n客户端发送消息时调用KafkaProducer的send方法，所以我们先分析KafkaProducer，再层层深入。\n\nKafkaProducer相当于整个快递公司的总控员，操作收件员收件，命令分拣员进行分拣，最终通知货车可以发车了。货车发货则在另外一个线程中进行。\n\n消息发送的顶层逻辑都在KafkaProducer中。在看代码前，我先宏观介绍下KafkaProducer中send方法的主流程，帮助代码理解，括号中以发快递类比：\n\n 1. 拦截器处理（对快递进行发送前的预处理）\n 2. 判断producer是否可用。依据是负责io的线程是否工作。（车队罢工了还如何发送快递？）\n 3. 判断metadata是否可用。metadata相当于调度员的指挥图，存储了kafka集群的各种信息，包括topic、分区等等。（如果没有快网点的信息，如何进行调度派发？）\n 4. 对key和value进行序列化。（对快递进行包装）\n 5. 获取要发往的分区编号（快递目的地部分地址）\n 6. 计算序列化后大小（快件称重）\n 7. 通过RecordAccumulator把消息加入batch中（分拣员进行分拣）\n 8. 如果batch满了，或者创建了新batch来容纳消息，则唤醒sender线程执行发送。（已经有封箱的货物了，发货吧！）\n\nsend方法的整体逻辑如上，每一步其实都和我们真实场景相对应，这就是设计的巧妙之处。\n\n接下来我们进入代码部分，我们先看下KafkaProducer中的部分重要属性\n\nprivate final Partitioner partitioner;\n说明：分区选择器，根据一定策略，选择出消息要发往的分区\nprivate final int maxRequestSize;\n说明：消息最大程度，包括了消息头、序列化后的key和value\nprivate final long totalMemorySize;\n说明：单个消息发送的缓冲区大小\nprivate final Metadata metadata;\n说明：kafka集群元数据\nprivate final RecordAccumulator accumulator;\n说明：前面所说的分拣员，维护不同分区的batch，负责分拣消息。等待sender来发送\nprivate final Sender sender;\n说明：发送消息，实现Runable，ioThread中启动运行\nprivate final Thread ioThread;\n说明：运行sender的线程，负责发送消息到kafka集群\nprivate final CompressionType compressionType;\n说明：压缩算法类型，gzip等。消息数量越多，压缩效果越好\nprivate final Serializer<K> keySerializer;\n说明：key的序列化器\nprivate final Serializer<V> valueSerializer;\n说明：value的序列化器\nprivate final ProducerConfig producerConfig;\n说明：生产者的配置\nprivate final long maxBlockTimeMs;\n说明：等待更新kafka集群元数据的最大时长\nprivate final ProducerInterceptors<K, V> interceptors;\n说明：发送前消息钱，要先通过一组拦截器的处理。也可以先于用户的callback预处理\n\n\nKafkaProducer 构造器会初始化上面的属性。另外在构造函数最后可以看到启动了ioThread。\n\nthis.sender = newSender(logContext, kafkaClient, this.metadata);\nString ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;\nthis.ioThread = new KafkaThread(ioThreadName, this.sender, true);\nthis.ioThread.start();\n\n\n接下来我们看一下send方法代码：\n\npublic Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {\n    // intercept the record, which can be potentially modified; this method does not throw exceptions\n    ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);\n    return doSend(interceptedRecord, callback);\n}\n\n\nProducerInterceptors对象中维护了List<ProducerInterceptor<K, V>> interceptors。在onSend方法中，会循环这个列表，调用每个ProducerInterceptor的onSend方法做预处理。\n\n拦截器处理后，再调用doSend方法。发送的主要逻辑都在doSend方法中，我按照上面介绍的发送主流程结合代码来讲解。这里不粘贴doSend方法的整段\n\n\n# 核心源码\n\n\n# 1、判断producer是否可用\n\n可以看到一进来就调用了throwIfProducerClosed()方法，这个方法里逻辑如下：代码，大家自行参考源代码。\n\nprivate void throwIfProducerClosed() {\n    if (ioThread == null || !ioThread.isAlive())\n        throw new IllegalStateException("Cannot perform operation after producer has been closed");\n}\n\n\n很简单，就是在检查io线程状态。\n\n\n# 2、判断这个topic的metadata是否可用\n\n代码如下：\n\nClusterAndWaitTime clusterAndWaitTime;\ntry {\n    clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);\n} catch (KafkaException e) {\n    if (metadata.isClosed())\n        throw new KafkaException("Producer closed while send in progress", e);\n    throw e;\n}\n\n\n主要逻辑在waitOnMetadata方法中。这里不展开讲，方法中做的事情是从缓存获取元数据中topic的信息，判断是否可用，如果缓存中存在分区并且请求分区在范围内，则直接返回cluster信息，否则发送更新请求。再判断分区是否正常并且请求更新的分区在此topic分区的范围内。这个方法的返回值是Cluster原数据以及所花费的时长。\n\n\n# 3、对key和value进行序列化\n\nserializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());\nserializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());\n\n\n\n# 4、获取消息要发往的分区编号\n\nint partition = partition(record, serializedKey, serializedValue, cluster);\ntp = new TopicPartition(record.topic(), partition);\n\n\nTopicPartition对象实际上只是封装了topic和partition，那么消息的发送地址就齐全了\n\n\n# 5、计算序列化后大小\n\nint serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),\n        compressionType, serializedKey, serializedValue, headers);\nensureValidRecordSize(serializedSize);\n\n\nensureValidRecordSize方法中验证size是否未超过maxRequestSize及totalMemorySize\n\n\n# 5、通过RecordAccumulator把消息加入batch中\n\nRecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,\n        serializedValue, headers, interceptCallback, remainingWaitMs);\n\n\naccumulator.append方法中做分拣逻辑处理，后面会重点讲解RecordAccumulator。这里我们只需要知道通过这个方法处理，你的消息已经缓存到待发送Batch中。\n\n\n# 6、如果batch正好满了，或者创建了新batch来容纳消息，则唤醒sender线程执行发送。\n\nif (result.batchIsFull || result.newBatchCreated) {\n    log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);\n    this.sender.wakeup();\n\n\n\n# 总结\n\n至此Producer中send方法的主要代码逻辑已经分析完毕\n\n附KafkaProducer类部分注释翻译：\n\n> producer 线程安全，跨线程共享一个producer对象，通常会比多个对象更快。\n> \n> producer由一个buffer空间池组成，它容纳了没有被传输到server的数据。同时有一个后台 I/O线程负责把这些数据记录转化为reqeust，然后把他们发送给集群。如果producer用后未成功关闭，这些资源将被泄漏。\n> \n> send()方法是异步的。当调用他添加记录到等待发送的数据缓冲区会立即得到返回。这允许producer把独立的消息打包起来，这样会更为高效。\n> \n> ack配置项控制认为请求完成的条件。设置为“all”，数据全部提交前，是不会返回结果的，这是最慢，但是持久性最好的。\n> \n> 如果request失败，producer会自动重试，但是如果我们设置了retries为0，则不会重试。开启重试，会出现数据重复的可能性。\n> \n> producer维护了每个partition对于未发送消息的缓冲区。缓冲区的大小通过batch.size配置项指定。设置的大一点，可以让batch更大，但是也需要更多的内存。\n> \n> 默认的，即使buffer还有未使用的空间，也是可以被立即发送出去的。然而，如果你想减少请求的次数，你可以设置linger.ms为大于0的值。这会让producer等待相应的时间，以让更多的数据到达batch后再发送。这个和Nagle在TCP里的算法是类似得。例如上面的代码片段，因为我们设置了linger为1ms，可能100条记录会在一次请求中发送出去。你也可以再加1ms，让请求等待更多的数据到达，如果我们的bufer还没填满的话。注意，相近时间到达的数据通常会在同一个batch中，即使linger.ms=0。所以，由于不计后果的linger设置，将会导致出现过重的数据负载batch。所以当你负载并不重，如果把linger设为大于0，会让请求更少，更高效，这只会造成很小的延迟。\n> \n> buffer.memory控制producer可用的全部buffe的r内存大小。如果record发送的速度快于传输到server的速度，buffer将被耗尽。当buffer耗尽时，发送请求将被block。block的时长通过max.block.ms指定。在这个时间长度后，将会抛出TimeoutException\n> \n> key.serialize和value.serializer用来指出如何转化key和value的值为byte。你可以使用内置的org.apache.kafka.common.serialization.ByteArraySerializer或者org.apache.kafka.common.serialization.StringSerializer处理简单的string或者byte类型\n> \n> 从kafka0.11开始，kafkaProducer支持两种新模式：幂等的producer和事务producer。幂等的producer让发送的语义从至少一次加强为仅一次。在特殊的producer重试时，不再会产生重复。事务性producer允许应用原子性发送消息给多个分区（及topic！）。\n\n\n# 参考资料\n\n你绝对能看懂的Kafka源代码分析-KafkaProducer类代码分析_kafka producer源码-CSDN博客',normalizedContent:'# 概述\n\n客户端发送消息时调用kafkaproducer的send方法，所以我们先分析kafkaproducer，再层层深入。\n\nkafkaproducer相当于整个快递公司的总控员，操作收件员收件，命令分拣员进行分拣，最终通知货车可以发车了。货车发货则在另外一个线程中进行。\n\n消息发送的顶层逻辑都在kafkaproducer中。在看代码前，我先宏观介绍下kafkaproducer中send方法的主流程，帮助代码理解，括号中以发快递类比：\n\n 1. 拦截器处理（对快递进行发送前的预处理）\n 2. 判断producer是否可用。依据是负责io的线程是否工作。（车队罢工了还如何发送快递？）\n 3. 判断metadata是否可用。metadata相当于调度员的指挥图，存储了kafka集群的各种信息，包括topic、分区等等。（如果没有快网点的信息，如何进行调度派发？）\n 4. 对key和value进行序列化。（对快递进行包装）\n 5. 获取要发往的分区编号（快递目的地部分地址）\n 6. 计算序列化后大小（快件称重）\n 7. 通过recordaccumulator把消息加入batch中（分拣员进行分拣）\n 8. 如果batch满了，或者创建了新batch来容纳消息，则唤醒sender线程执行发送。（已经有封箱的货物了，发货吧！）\n\nsend方法的整体逻辑如上，每一步其实都和我们真实场景相对应，这就是设计的巧妙之处。\n\n接下来我们进入代码部分，我们先看下kafkaproducer中的部分重要属性\n\nprivate final partitioner partitioner;\n说明：分区选择器，根据一定策略，选择出消息要发往的分区\nprivate final int maxrequestsize;\n说明：消息最大程度，包括了消息头、序列化后的key和value\nprivate final long totalmemorysize;\n说明：单个消息发送的缓冲区大小\nprivate final metadata metadata;\n说明：kafka集群元数据\nprivate final recordaccumulator accumulator;\n说明：前面所说的分拣员，维护不同分区的batch，负责分拣消息。等待sender来发送\nprivate final sender sender;\n说明：发送消息，实现runable，iothread中启动运行\nprivate final thread iothread;\n说明：运行sender的线程，负责发送消息到kafka集群\nprivate final compressiontype compressiontype;\n说明：压缩算法类型，gzip等。消息数量越多，压缩效果越好\nprivate final serializer<k> keyserializer;\n说明：key的序列化器\nprivate final serializer<v> valueserializer;\n说明：value的序列化器\nprivate final producerconfig producerconfig;\n说明：生产者的配置\nprivate final long maxblocktimems;\n说明：等待更新kafka集群元数据的最大时长\nprivate final producerinterceptors<k, v> interceptors;\n说明：发送前消息钱，要先通过一组拦截器的处理。也可以先于用户的callback预处理\n\n\nkafkaproducer 构造器会初始化上面的属性。另外在构造函数最后可以看到启动了iothread。\n\nthis.sender = newsender(logcontext, kafkaclient, this.metadata);\nstring iothreadname = network_thread_prefix + " | " + clientid;\nthis.iothread = new kafkathread(iothreadname, this.sender, true);\nthis.iothread.start();\n\n\n接下来我们看一下send方法代码：\n\npublic future<recordmetadata> send(producerrecord<k, v> record, callback callback) {\n    // intercept the record, which can be potentially modified; this method does not throw exceptions\n    producerrecord<k, v> interceptedrecord = this.interceptors.onsend(record);\n    return dosend(interceptedrecord, callback);\n}\n\n\nproducerinterceptors对象中维护了list<producerinterceptor<k, v>> interceptors。在onsend方法中，会循环这个列表，调用每个producerinterceptor的onsend方法做预处理。\n\n拦截器处理后，再调用dosend方法。发送的主要逻辑都在dosend方法中，我按照上面介绍的发送主流程结合代码来讲解。这里不粘贴dosend方法的整段\n\n\n# 核心源码\n\n\n# 1、判断producer是否可用\n\n可以看到一进来就调用了throwifproducerclosed()方法，这个方法里逻辑如下：代码，大家自行参考源代码。\n\nprivate void throwifproducerclosed() {\n    if (iothread == null || !iothread.isalive())\n        throw new illegalstateexception("cannot perform operation after producer has been closed");\n}\n\n\n很简单，就是在检查io线程状态。\n\n\n# 2、判断这个topic的metadata是否可用\n\n代码如下：\n\nclusterandwaittime clusterandwaittime;\ntry {\n    clusterandwaittime = waitonmetadata(record.topic(), record.partition(), maxblocktimems);\n} catch (kafkaexception e) {\n    if (metadata.isclosed())\n        throw new kafkaexception("producer closed while send in progress", e);\n    throw e;\n}\n\n\n主要逻辑在waitonmetadata方法中。这里不展开讲，方法中做的事情是从缓存获取元数据中topic的信息，判断是否可用，如果缓存中存在分区并且请求分区在范围内，则直接返回cluster信息，否则发送更新请求。再判断分区是否正常并且请求更新的分区在此topic分区的范围内。这个方法的返回值是cluster原数据以及所花费的时长。\n\n\n# 3、对key和value进行序列化\n\nserializedkey = keyserializer.serialize(record.topic(), record.headers(), record.key());\nserializedvalue = valueserializer.serialize(record.topic(), record.headers(), record.value());\n\n\n\n# 4、获取消息要发往的分区编号\n\nint partition = partition(record, serializedkey, serializedvalue, cluster);\ntp = new topicpartition(record.topic(), partition);\n\n\ntopicpartition对象实际上只是封装了topic和partition，那么消息的发送地址就齐全了\n\n\n# 5、计算序列化后大小\n\nint serializedsize = abstractrecords.estimatesizeinbytesupperbound(apiversions.maxusableproducemagic(),\n        compressiontype, serializedkey, serializedvalue, headers);\nensurevalidrecordsize(serializedsize);\n\n\nensurevalidrecordsize方法中验证size是否未超过maxrequestsize及totalmemorysize\n\n\n# 5、通过recordaccumulator把消息加入batch中\n\nrecordaccumulator.recordappendresult result = accumulator.append(tp, timestamp, serializedkey,\n        serializedvalue, headers, interceptcallback, remainingwaitms);\n\n\naccumulator.append方法中做分拣逻辑处理，后面会重点讲解recordaccumulator。这里我们只需要知道通过这个方法处理，你的消息已经缓存到待发送batch中。\n\n\n# 6、如果batch正好满了，或者创建了新batch来容纳消息，则唤醒sender线程执行发送。\n\nif (result.batchisfull || result.newbatchcreated) {\n    log.trace("waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);\n    this.sender.wakeup();\n\n\n\n# 总结\n\n至此producer中send方法的主要代码逻辑已经分析完毕\n\n附kafkaproducer类部分注释翻译：\n\n> producer 线程安全，跨线程共享一个producer对象，通常会比多个对象更快。\n> \n> producer由一个buffer空间池组成，它容纳了没有被传输到server的数据。同时有一个后台 i/o线程负责把这些数据记录转化为reqeust，然后把他们发送给集群。如果producer用后未成功关闭，这些资源将被泄漏。\n> \n> send()方法是异步的。当调用他添加记录到等待发送的数据缓冲区会立即得到返回。这允许producer把独立的消息打包起来，这样会更为高效。\n> \n> ack配置项控制认为请求完成的条件。设置为“all”，数据全部提交前，是不会返回结果的，这是最慢，但是持久性最好的。\n> \n> 如果request失败，producer会自动重试，但是如果我们设置了retries为0，则不会重试。开启重试，会出现数据重复的可能性。\n> \n> producer维护了每个partition对于未发送消息的缓冲区。缓冲区的大小通过batch.size配置项指定。设置的大一点，可以让batch更大，但是也需要更多的内存。\n> \n> 默认的，即使buffer还有未使用的空间，也是可以被立即发送出去的。然而，如果你想减少请求的次数，你可以设置linger.ms为大于0的值。这会让producer等待相应的时间，以让更多的数据到达batch后再发送。这个和nagle在tcp里的算法是类似得。例如上面的代码片段，因为我们设置了linger为1ms，可能100条记录会在一次请求中发送出去。你也可以再加1ms，让请求等待更多的数据到达，如果我们的bufer还没填满的话。注意，相近时间到达的数据通常会在同一个batch中，即使linger.ms=0。所以，由于不计后果的linger设置，将会导致出现过重的数据负载batch。所以当你负载并不重，如果把linger设为大于0，会让请求更少，更高效，这只会造成很小的延迟。\n> \n> buffer.memory控制producer可用的全部buffe的r内存大小。如果record发送的速度快于传输到server的速度，buffer将被耗尽。当buffer耗尽时，发送请求将被block。block的时长通过max.block.ms指定。在这个时间长度后，将会抛出timeoutexception\n> \n> key.serialize和value.serializer用来指出如何转化key和value的值为byte。你可以使用内置的org.apache.kafka.common.serialization.bytearrayserializer或者org.apache.kafka.common.serialization.stringserializer处理简单的string或者byte类型\n> \n> 从kafka0.11开始，kafkaproducer支持两种新模式：幂等的producer和事务producer。幂等的producer让发送的语义从至少一次加强为仅一次。在特殊的producer重试时，不再会产生重复。事务性producer允许应用原子性发送消息给多个分区（及topic！）。\n\n\n# 参考资料\n\n你绝对能看懂的kafka源代码分析-kafkaproducer类代码分析_kafka producer源码-csdn博客',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Sender 类代码分析",frontmatter:{title:"Sender 类代码分析",date:"2024-09-18T15:33:30.000Z",permalink:"/pages/bf1d55/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/500.%E3%80%8C%E7%94%9F%E4%BA%A7%E8%80%85%E3%80%8D%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/40.Sender%20%E7%B1%BB%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/40.Sender 类代码分析.md",key:"v-394317e1",path:"/pages/bf1d55/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"Sender 类核心代码",slug:"sender-类核心代码",normalizedTitle:"sender 类核心代码",charIndex:675},{level:3,title:"run()方法",slug:"run-方法",normalizedTitle:"run()方法",charIndex:869},{level:3,title:"run(long now) 方法",slug:"run-long-now-方法",normalizedTitle:"run(long now) 方法",charIndex:2922},{level:3,title:"sendProducerData()方法",slug:"sendproducerdata-方法",normalizedTitle:"sendproducerdata()方法",charIndex:3190},{level:3,title:"sendProduceRequest() 方法",slug:"sendproducerequest-方法",normalizedTitle:"sendproducerequest() 方法",charIndex:8525},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:12010},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:12204}],headersStr:"前言 Sender 类核心代码 run()方法 run(long now) 方法 sendProducerData()方法 sendProduceRequest() 方法 总结 参考资料",content:'# 前言\n\n上文我们讲到待发送的消息已经缓存在RecordAccumulator中。KafkaProducer得知有Batch已经满了，就会通知Sender开始干活。本篇我们就来看一看Sender的设计和实现。\n\nSender在KafkaProducer中以一个单独的线程运行，把RecordAccumulator中封箱后的ProducerBatch发送出去。网络IO是由Sender触发的。\n\nSender把ProducerBatch按照要发往的broker node进行分类，发往同一个node的ProducerBatch会被打包在一起，只触发一次网络IO。例如主题A分区1和主题B分区2存放在同一个node上，那么Send在IO前会把这两个主题分区的消息打包在一起，生成一个请求对象，一并发过去，这样大大减少了网络IO的开销。\n\n我还是以发送快递举例类比，这次是把货箱装车的场景：\n\n\n\n绿色衣服的小人是装箱工作人员，负责把货箱装到对应的车上。可以看到车是按照快递站点区分的，那么朝阳北京和河北唐山的货箱都要装到华北一区的车上。这样一趟车就可以把北京和唐山的包裹一起发送出去，而不需要两辆车分别发送。\n\n其实这个绿色小人就是sender，它负责把不同主题分区的ProducerBatch（货箱）封装进同一个ClientRequest（货车集装箱），前提是这些主题分区存储在同一个broker上（都属华北一区），然后通过NetWorkClient（货车）真正网络IO发往Broker（华北一区站点）。\n\n下面进入Sender的代码讲解。\n\n\n# Sender 类核心代码\n\n从Sender设计可以看到，Sender内部有两个重要的实例引用，一个就是仓库：RecordAccumulator，另外一个就是那一辆辆车：负责网络IO的KafkaClient对象。KafkaClient负责让不同的车找到通往各自目的地的路，把货物运送过去。KafkaClient后面会单独讲解，本篇我们专注于分析sender怎么把消息重新分组装车。\n\n\n# run()方法\n\nSender实现Runnable接口，重写run方法，这也是线程的启动入口，代码如下：\n\npublic void run() {\n    log.debug("Starting Kafka producer I/O thread.");\n \n    // main loop, runs until close is called\n    while (running) {\n        try {\n            run(time.milliseconds());\n        } catch (Exception e) {\n            log.error("Uncaught error in kafka producer I/O thread: ", e);\n        }\n    }\n \n    log.debug("Beginning shutdown of Kafka producer I/O thread, sending remaining records.");\n \n    // okay we stopped accepting requests but there may still be\n    // requests in the accumulator or waiting for acknowledgment,\n    // wait until these are completed.\n    while (!forceClose && (this.accumulator.hasUndrained() || this.client.inFlightRequestCount() > 0)) {\n        try {\n            run(time.milliseconds());\n        } catch (Exception e) {\n            log.error("Uncaught error in kafka producer I/O thread: ", e);\n        }\n    }\n    if (forceClose) {\n        // We need to fail all the incomplete batches and wake up the threads waiting on\n        // the futures.\n        log.debug("Aborting incomplete batches due to forced shutdown");\n        this.accumulator.abortIncompleteBatches();\n    }\n    try {\n        this.client.close();\n    } catch (Exception e) {\n        log.error("Failed to close network client", e);\n    }\n \n    log.debug("Shutdown of Kafka producer I/O thread has completed.");\n}\n\n\nrun 方法主循环中调用 run(time.milliseconds()) 方法，sender 的主要逻辑实际在这个方法中：一旦running为false跳出主循环，根据状态判断是继续发送完成，还是强制关闭。强制关闭的话，通过 accumulator.abortIncompleteBatches() 把RecourdAccumulator中incomplete集合中保存的未完成ProducerBatch做相应的处理，对他们进行封箱，防止继续有新的消息被追加进来，然后从所属Deque中删除掉，释放掉BufferPool中的空间。这部分核心的代码如下：\n\nvoid abortBatches(final RuntimeException reason) {\n    for (ProducerBatch batch : incomplete.copyAll()) {\n        Deque<ProducerBatch> dq = getDeque(batch.topicPartition);\n        synchronized (dq) {\n            batch.abortRecordAppends();\n            dq.remove(batch);\n        }\n        batch.abort(reason);\n        deallocate(batch);\n    }\n}\n\n\nrun 方法最后关闭负责网络 IO 的NetworkClient\n\n\n# run(long now) 方法\n\nsender的主要逻辑实际在这个方法中。这个方法前面一大段逻辑都是处理开启事物的消息发送，我们为了简化代码理解，不做这部分分析，直接进入非事务的消息发送逻辑中\n\n非事务的消息发送只有两行关键代码：\n\nlong pollTimeout = sendProducerData(now);\nclient.poll(pollTimeout, now);\n\n\n 1. 准备发送的数据请求\n\n 2. 把准备好的消息请求真正发送出去\n\n下面我们先看准备数据请求的方法sendProducerData()。\n\n\n# sendProducerData()方法\n\n这个方法是sender的主流程，需要认真理解。我先贴代码，代码下紧跟逻辑讲解\n\nprivate long sendProducerData(long now) {\n \n    Cluster cluster = metadata.fetch();\n    // get the list of partitions with data ready to send\n    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);\n \n    // if there are any partitions whose leaders are not known yet, force metadata update\n    if (!result.unknownLeaderTopics.isEmpty()) {\n        // The set of topics with unknown leader contains topics with leader election pending as well as\n        // topics which may have expired. Add the topic again to metadata to ensure it is included\n        // and request metadata update, since there are messages to send to the topic.\n        for (String topic : result.unknownLeaderTopics)\n            this.metadata.add(topic);\n \n        log.debug("Requesting metadata update due to unknown leader topics from the batched records: {}",\n            result.unknownLeaderTopics);\n        this.metadata.requestUpdate();\n    }\n \n    // remove any nodes we aren\'t ready to send to\n    Iterator<Node> iter = result.readyNodes.iterator();\n    long notReadyTimeout = Long.MAX_VALUE;\n    while (iter.hasNext()) {\n        Node node = iter.next();\n        if (!this.client.ready(node, now)) {\n            iter.remove();\n            notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));\n        }\n    }\n \n    // create produce requests\n    Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);\n    addToInflightBatches(batches);\n    if (guaranteeMessageOrder) {\n        // Mute all the partitions drained\n        for (List<ProducerBatch> batchList : batches.values()) {\n            for (ProducerBatch batch : batchList)\n                this.accumulator.mutePartition(batch.topicPartition);\n        }\n    }\n \n    accumulator.resetNextBatchExpiryTime();\n    List<ProducerBatch> expiredInflightBatches = getExpiredInflightBatches(now);\n    List<ProducerBatch> expiredBatches = this.accumulator.expiredBatches(now);\n    expiredBatches.addAll(expiredInflightBatches);\n \n    // Reset the producer id if an expired batch has previously been sent to the broker. Also update the metrics\n    // for expired batches. see the documentation of @TransactionState.resetProducerId to understand why\n    // we need to reset the producer id here.\n    if (!expiredBatches.isEmpty())\n        log.trace("Expired {} batches in accumulator", expiredBatches.size());\n    for (ProducerBatch expiredBatch : expiredBatches) {\n        String errorMessage = "Expiring " + expiredBatch.recordCount + " record(s) for " + expiredBatch.topicPartition\n            + ":" + (now - expiredBatch.createdMs) + " ms has passed since batch creation";\n        failBatch(expiredBatch, -1, NO_TIMESTAMP, new TimeoutException(errorMessage), false);\n        if (transactionManager != null && expiredBatch.inRetry()) {\n            // This ensures that no new batches are drained until the current in flight batches are fully resolved.\n            transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);\n        }\n    }\n    sensors.updateProduceRequestMetrics(batches);\n \n    // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately\n    // loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry\n    // time, and the delay time for checking data availability. Note that the nodes may have data that isn\'t yet\n    // sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data\n    // that aren\'t ready to send since they would cause busy looping.\n    long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);\n    pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);\n    pollTimeout = Math.max(pollTimeout, 0);\n    if (!result.readyNodes.isEmpty()) {\n        log.trace("Nodes with data ready to send: {}", result.readyNodes);\n        // if some partitions are already ready to be sent, the select time would be 0;\n        // otherwise if some partition already has some data accumulated but not ready yet,\n        // the select time will be the time difference between now and its linger expiry time;\n        // otherwise the select time will be the time difference between now and the metadata expiry time;\n        pollTimeout = 0;\n    }\n    sendProduceRequests(batches, now);\n    return pollTimeout;\n}\n\n\n主要逻辑如下：\n\n 1. 根据accumulator中待发送消息对应的主题分区，检查kafka集群对应的node哪些可用，哪些不可用。得到ReadyCheckResult 结果\n 2. 如果ReadyCheckResult 中的unknownLeaderTopics有值，那么则需要更新Kafka集群元数据\n 3. 循环readyNodes，检查KafkaClient对该node是否符合网络IO的条件，不符合的从集合中删除。\n 4. 通过accumulator.drain()方法把待发送的消息按node号进行分组，返回Map<Integer, List>\n 5. 把待发送的batch添加到Sender的inFlightBatches中。inFlightBatches是Map<TopicPartition,List>，可见是照主题分区来存储的。\n 6. 获取所有过期的batch，循环做过期处理\n 7. 计算接下来外层程序逻辑中调用NetWorkClient的poll操作时的timeout时间\n 8. 调用sendProduceRequests()方法，将待发送的ProducerBatch封装成为ClientRequest，然后“发送”出去。注意这里的发送，其实是加入发送的队列。等到NetWorkClient进行poll操作时，才发生网络IO\n 9. 返回第7步中计算的poll操作timeout时间\n\n下面我们重点来来第8步的sendProduceRequests()方法，这个方法中实际是按照node编号循环第4步得到的Map<Integer, List<ProducerBatch>> batches，取得node对应的List<ProducerBatch>,然后调用sendProduceRequest()。封装 ClientRequest是在这个方法中进行的。\n\n\n# sendProduceRequest() 方法\n\nprivate void sendProduceRequest(long now, int destination, short acks, int timeout, List<ProducerBatch> batches) {\n        if (batches.isEmpty())\n            return;\n \n        Map<TopicPartition, MemoryRecords> produceRecordsByPartition = new HashMap<>(batches.size());\n        final Map<TopicPartition, ProducerBatch> recordsByPartition = new HashMap<>(batches.size());\n \n        // find the minimum magic version used when creating the record sets\n        byte minUsedMagic = apiVersions.maxUsableProduceMagic();\n        for (ProducerBatch batch : batches) {\n            if (batch.magic() < minUsedMagic)\n                minUsedMagic = batch.magic();\n        }\n \n        for (ProducerBatch batch : batches) {\n            TopicPartition tp = batch.topicPartition;\n            MemoryRecords records = batch.records();\n \n            // down convert if necessary to the minimum magic used. In general, there can be a delay between the time\n            // that the producer starts building the batch and the time that we send the request, and we may have\n            // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use\n            // the new message format, but found that the broker didn\'t support it, so we need to down-convert on the\n            // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may\n            // not all support the same message format version. For example, if a partition migrates from a broker\n            // which is supporting the new magic version to one which doesn\'t, then we will need to convert.\n            if (!records.hasMatchingMagic(minUsedMagic))\n                records = batch.records().downConvert(minUsedMagic, 0, time).records();\n            produceRecordsByPartition.put(tp, records);\n            recordsByPartition.put(tp, batch);\n        }\n \n        String transactionalId = null;\n        if (transactionManager != null && transactionManager.isTransactional()) {\n            transactionalId = transactionManager.transactionalId();\n        }\n        ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout,\n                produceRecordsByPartition, transactionalId);\n        RequestCompletionHandler callback = new RequestCompletionHandler() {\n            public void onComplete(ClientResponse response) {\n                handleProduceResponse(response, recordsByPartition, time.milliseconds());\n            }\n        };\n \n        String nodeId = Integer.toString(destination);\n        ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0,\n                requestTimeoutMs, callback);\n        client.send(clientRequest, now);\n        log.trace("Sent produce request to {}: {}", nodeId, requestBuilder);\n    }\n\n\n这个方法的输入的List<ProducerBatch> batches已经是按照node分好组的，列表中的ProducerBatch不一定在同一个主题分区，但所属主题分区肯定在同一个node上。\n\n 1. 循环batches，MemoryRecords records = batch.records(); 获取ProducerBatch中组织好的消息内容也。按照主题分区分堆ProducerBatch和MemoryRecords得到两个以TopicPartition为Key的Map，produceRecordsByPartition和recordsByPartition\n\n 2. 声明ProduceRequest.Builder对象，他内部有引用指向produceRecordsByPartition\n\n 3. 生成ClientRequest对象\n\n 4. 调用NetWorkClient的send方法。\n\n在NetWorkClient的send方法中，通过Builder对象的build方法生成ProduceRequest对象。再通过request.toSend(destination, header)，得到NetworkSend对象send。生成InFlightRequest对象，保存起来。最后调用selector.send(send)；这个方法会把send请求加入队列等待随后的poll方法把它发送出去。\n\n\n# 总结\n\n讲解到这里，因为涉及到NIO的概念，而Kafka又对NIO做了封装，所以会难以理解。本篇大家只要能明白我们在sendProduceRequest()方法中，生成按kafka node划分的请求对象ClientRequest，然后交给NetWorkClient处理。关于NetWorkClient如何通过NIO方式把ClientRequest发送出去，我们以后的章节再讲。\n\n\n# 参考资料\n\n你绝对能看懂的Kafka源代码分析-Sender类代码分析_kafka producer源码sender线程-CSDN博客',normalizedContent:'# 前言\n\n上文我们讲到待发送的消息已经缓存在recordaccumulator中。kafkaproducer得知有batch已经满了，就会通知sender开始干活。本篇我们就来看一看sender的设计和实现。\n\nsender在kafkaproducer中以一个单独的线程运行，把recordaccumulator中封箱后的producerbatch发送出去。网络io是由sender触发的。\n\nsender把producerbatch按照要发往的broker node进行分类，发往同一个node的producerbatch会被打包在一起，只触发一次网络io。例如主题a分区1和主题b分区2存放在同一个node上，那么send在io前会把这两个主题分区的消息打包在一起，生成一个请求对象，一并发过去，这样大大减少了网络io的开销。\n\n我还是以发送快递举例类比，这次是把货箱装车的场景：\n\n\n\n绿色衣服的小人是装箱工作人员，负责把货箱装到对应的车上。可以看到车是按照快递站点区分的，那么朝阳北京和河北唐山的货箱都要装到华北一区的车上。这样一趟车就可以把北京和唐山的包裹一起发送出去，而不需要两辆车分别发送。\n\n其实这个绿色小人就是sender，它负责把不同主题分区的producerbatch（货箱）封装进同一个clientrequest（货车集装箱），前提是这些主题分区存储在同一个broker上（都属华北一区），然后通过networkclient（货车）真正网络io发往broker（华北一区站点）。\n\n下面进入sender的代码讲解。\n\n\n# sender 类核心代码\n\n从sender设计可以看到，sender内部有两个重要的实例引用，一个就是仓库：recordaccumulator，另外一个就是那一辆辆车：负责网络io的kafkaclient对象。kafkaclient负责让不同的车找到通往各自目的地的路，把货物运送过去。kafkaclient后面会单独讲解，本篇我们专注于分析sender怎么把消息重新分组装车。\n\n\n# run()方法\n\nsender实现runnable接口，重写run方法，这也是线程的启动入口，代码如下：\n\npublic void run() {\n    log.debug("starting kafka producer i/o thread.");\n \n    // main loop, runs until close is called\n    while (running) {\n        try {\n            run(time.milliseconds());\n        } catch (exception e) {\n            log.error("uncaught error in kafka producer i/o thread: ", e);\n        }\n    }\n \n    log.debug("beginning shutdown of kafka producer i/o thread, sending remaining records.");\n \n    // okay we stopped accepting requests but there may still be\n    // requests in the accumulator or waiting for acknowledgment,\n    // wait until these are completed.\n    while (!forceclose && (this.accumulator.hasundrained() || this.client.inflightrequestcount() > 0)) {\n        try {\n            run(time.milliseconds());\n        } catch (exception e) {\n            log.error("uncaught error in kafka producer i/o thread: ", e);\n        }\n    }\n    if (forceclose) {\n        // we need to fail all the incomplete batches and wake up the threads waiting on\n        // the futures.\n        log.debug("aborting incomplete batches due to forced shutdown");\n        this.accumulator.abortincompletebatches();\n    }\n    try {\n        this.client.close();\n    } catch (exception e) {\n        log.error("failed to close network client", e);\n    }\n \n    log.debug("shutdown of kafka producer i/o thread has completed.");\n}\n\n\nrun 方法主循环中调用 run(time.milliseconds()) 方法，sender 的主要逻辑实际在这个方法中：一旦running为false跳出主循环，根据状态判断是继续发送完成，还是强制关闭。强制关闭的话，通过 accumulator.abortincompletebatches() 把recourdaccumulator中incomplete集合中保存的未完成producerbatch做相应的处理，对他们进行封箱，防止继续有新的消息被追加进来，然后从所属deque中删除掉，释放掉bufferpool中的空间。这部分核心的代码如下：\n\nvoid abortbatches(final runtimeexception reason) {\n    for (producerbatch batch : incomplete.copyall()) {\n        deque<producerbatch> dq = getdeque(batch.topicpartition);\n        synchronized (dq) {\n            batch.abortrecordappends();\n            dq.remove(batch);\n        }\n        batch.abort(reason);\n        deallocate(batch);\n    }\n}\n\n\nrun 方法最后关闭负责网络 io 的networkclient\n\n\n# run(long now) 方法\n\nsender的主要逻辑实际在这个方法中。这个方法前面一大段逻辑都是处理开启事物的消息发送，我们为了简化代码理解，不做这部分分析，直接进入非事务的消息发送逻辑中\n\n非事务的消息发送只有两行关键代码：\n\nlong polltimeout = sendproducerdata(now);\nclient.poll(polltimeout, now);\n\n\n 1. 准备发送的数据请求\n\n 2. 把准备好的消息请求真正发送出去\n\n下面我们先看准备数据请求的方法sendproducerdata()。\n\n\n# sendproducerdata()方法\n\n这个方法是sender的主流程，需要认真理解。我先贴代码，代码下紧跟逻辑讲解\n\nprivate long sendproducerdata(long now) {\n \n    cluster cluster = metadata.fetch();\n    // get the list of partitions with data ready to send\n    recordaccumulator.readycheckresult result = this.accumulator.ready(cluster, now);\n \n    // if there are any partitions whose leaders are not known yet, force metadata update\n    if (!result.unknownleadertopics.isempty()) {\n        // the set of topics with unknown leader contains topics with leader election pending as well as\n        // topics which may have expired. add the topic again to metadata to ensure it is included\n        // and request metadata update, since there are messages to send to the topic.\n        for (string topic : result.unknownleadertopics)\n            this.metadata.add(topic);\n \n        log.debug("requesting metadata update due to unknown leader topics from the batched records: {}",\n            result.unknownleadertopics);\n        this.metadata.requestupdate();\n    }\n \n    // remove any nodes we aren\'t ready to send to\n    iterator<node> iter = result.readynodes.iterator();\n    long notreadytimeout = long.max_value;\n    while (iter.hasnext()) {\n        node node = iter.next();\n        if (!this.client.ready(node, now)) {\n            iter.remove();\n            notreadytimeout = math.min(notreadytimeout, this.client.polldelayms(node, now));\n        }\n    }\n \n    // create produce requests\n    map<integer, list<producerbatch>> batches = this.accumulator.drain(cluster, result.readynodes, this.maxrequestsize, now);\n    addtoinflightbatches(batches);\n    if (guaranteemessageorder) {\n        // mute all the partitions drained\n        for (list<producerbatch> batchlist : batches.values()) {\n            for (producerbatch batch : batchlist)\n                this.accumulator.mutepartition(batch.topicpartition);\n        }\n    }\n \n    accumulator.resetnextbatchexpirytime();\n    list<producerbatch> expiredinflightbatches = getexpiredinflightbatches(now);\n    list<producerbatch> expiredbatches = this.accumulator.expiredbatches(now);\n    expiredbatches.addall(expiredinflightbatches);\n \n    // reset the producer id if an expired batch has previously been sent to the broker. also update the metrics\n    // for expired batches. see the documentation of @transactionstate.resetproducerid to understand why\n    // we need to reset the producer id here.\n    if (!expiredbatches.isempty())\n        log.trace("expired {} batches in accumulator", expiredbatches.size());\n    for (producerbatch expiredbatch : expiredbatches) {\n        string errormessage = "expiring " + expiredbatch.recordcount + " record(s) for " + expiredbatch.topicpartition\n            + ":" + (now - expiredbatch.createdms) + " ms has passed since batch creation";\n        failbatch(expiredbatch, -1, no_timestamp, new timeoutexception(errormessage), false);\n        if (transactionmanager != null && expiredbatch.inretry()) {\n            // this ensures that no new batches are drained until the current in flight batches are fully resolved.\n            transactionmanager.marksequenceunresolved(expiredbatch.topicpartition);\n        }\n    }\n    sensors.updateproducerequestmetrics(batches);\n \n    // if we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately\n    // loop and try sending more data. otherwise, the timeout will be the smaller value between next batch expiry\n    // time, and the delay time for checking data availability. note that the nodes may have data that isn\'t yet\n    // sendable due to lingering, backing off, etc. this specifically does not include nodes with sendable data\n    // that aren\'t ready to send since they would cause busy looping.\n    long polltimeout = math.min(result.nextreadycheckdelayms, notreadytimeout);\n    polltimeout = math.min(polltimeout, this.accumulator.nextexpirytimems() - now);\n    polltimeout = math.max(polltimeout, 0);\n    if (!result.readynodes.isempty()) {\n        log.trace("nodes with data ready to send: {}", result.readynodes);\n        // if some partitions are already ready to be sent, the select time would be 0;\n        // otherwise if some partition already has some data accumulated but not ready yet,\n        // the select time will be the time difference between now and its linger expiry time;\n        // otherwise the select time will be the time difference between now and the metadata expiry time;\n        polltimeout = 0;\n    }\n    sendproducerequests(batches, now);\n    return polltimeout;\n}\n\n\n主要逻辑如下：\n\n 1. 根据accumulator中待发送消息对应的主题分区，检查kafka集群对应的node哪些可用，哪些不可用。得到readycheckresult 结果\n 2. 如果readycheckresult 中的unknownleadertopics有值，那么则需要更新kafka集群元数据\n 3. 循环readynodes，检查kafkaclient对该node是否符合网络io的条件，不符合的从集合中删除。\n 4. 通过accumulator.drain()方法把待发送的消息按node号进行分组，返回map<integer, list>\n 5. 把待发送的batch添加到sender的inflightbatches中。inflightbatches是map<topicpartition,list>，可见是照主题分区来存储的。\n 6. 获取所有过期的batch，循环做过期处理\n 7. 计算接下来外层程序逻辑中调用networkclient的poll操作时的timeout时间\n 8. 调用sendproducerequests()方法，将待发送的producerbatch封装成为clientrequest，然后“发送”出去。注意这里的发送，其实是加入发送的队列。等到networkclient进行poll操作时，才发生网络io\n 9. 返回第7步中计算的poll操作timeout时间\n\n下面我们重点来来第8步的sendproducerequests()方法，这个方法中实际是按照node编号循环第4步得到的map<integer, list<producerbatch>> batches，取得node对应的list<producerbatch>,然后调用sendproducerequest()。封装 clientrequest是在这个方法中进行的。\n\n\n# sendproducerequest() 方法\n\nprivate void sendproducerequest(long now, int destination, short acks, int timeout, list<producerbatch> batches) {\n        if (batches.isempty())\n            return;\n \n        map<topicpartition, memoryrecords> producerecordsbypartition = new hashmap<>(batches.size());\n        final map<topicpartition, producerbatch> recordsbypartition = new hashmap<>(batches.size());\n \n        // find the minimum magic version used when creating the record sets\n        byte minusedmagic = apiversions.maxusableproducemagic();\n        for (producerbatch batch : batches) {\n            if (batch.magic() < minusedmagic)\n                minusedmagic = batch.magic();\n        }\n \n        for (producerbatch batch : batches) {\n            topicpartition tp = batch.topicpartition;\n            memoryrecords records = batch.records();\n \n            // down convert if necessary to the minimum magic used. in general, there can be a delay between the time\n            // that the producer starts building the batch and the time that we send the request, and we may have\n            // chosen the message format based on out-dated metadata. in the worst case, we optimistically chose to use\n            // the new message format, but found that the broker didn\'t support it, so we need to down-convert on the\n            // client before sending. this is intended to handle edge cases around cluster upgrades where brokers may\n            // not all support the same message format version. for example, if a partition migrates from a broker\n            // which is supporting the new magic version to one which doesn\'t, then we will need to convert.\n            if (!records.hasmatchingmagic(minusedmagic))\n                records = batch.records().downconvert(minusedmagic, 0, time).records();\n            producerecordsbypartition.put(tp, records);\n            recordsbypartition.put(tp, batch);\n        }\n \n        string transactionalid = null;\n        if (transactionmanager != null && transactionmanager.istransactional()) {\n            transactionalid = transactionmanager.transactionalid();\n        }\n        producerequest.builder requestbuilder = producerequest.builder.formagic(minusedmagic, acks, timeout,\n                producerecordsbypartition, transactionalid);\n        requestcompletionhandler callback = new requestcompletionhandler() {\n            public void oncomplete(clientresponse response) {\n                handleproduceresponse(response, recordsbypartition, time.milliseconds());\n            }\n        };\n \n        string nodeid = integer.tostring(destination);\n        clientrequest clientrequest = client.newclientrequest(nodeid, requestbuilder, now, acks != 0,\n                requesttimeoutms, callback);\n        client.send(clientrequest, now);\n        log.trace("sent produce request to {}: {}", nodeid, requestbuilder);\n    }\n\n\n这个方法的输入的list<producerbatch> batches已经是按照node分好组的，列表中的producerbatch不一定在同一个主题分区，但所属主题分区肯定在同一个node上。\n\n 1. 循环batches，memoryrecords records = batch.records(); 获取producerbatch中组织好的消息内容也。按照主题分区分堆producerbatch和memoryrecords得到两个以topicpartition为key的map，producerecordsbypartition和recordsbypartition\n\n 2. 声明producerequest.builder对象，他内部有引用指向producerecordsbypartition\n\n 3. 生成clientrequest对象\n\n 4. 调用networkclient的send方法。\n\n在networkclient的send方法中，通过builder对象的build方法生成producerequest对象。再通过request.tosend(destination, header)，得到networksend对象send。生成inflightrequest对象，保存起来。最后调用selector.send(send)；这个方法会把send请求加入队列等待随后的poll方法把它发送出去。\n\n\n# 总结\n\n讲解到这里，因为涉及到nio的概念，而kafka又对nio做了封装，所以会难以理解。本篇大家只要能明白我们在sendproducerequest()方法中，生成按kafka node划分的请求对象clientrequest，然后交给networkclient处理。关于networkclient如何通过nio方式把clientrequest发送出去，我们以后的章节再讲。\n\n\n# 参考资料\n\n你绝对能看懂的kafka源代码分析-sender类代码分析_kafka producer源码sender线程-csdn博客',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"RecordAccumulator 源码分析",frontmatter:{title:"RecordAccumulator 源码分析",date:"2024-09-18T15:30:43.000Z",permalink:"/pages/b77953/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/500.%E3%80%8C%E7%94%9F%E4%BA%A7%E8%80%85%E3%80%8D%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/20.RecordAccumulator%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/20.RecordAccumulator 源码分析.md",key:"v-6c10e80b",path:"/pages/b77953/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"RecordAccumulator设计",slug:"recordaccumulator设计",normalizedTitle:"recordaccumulator设计",charIndex:249},{level:2,title:"RecordAccumulator 代码分析",slug:"recordaccumulator-代码分析",normalizedTitle:"recordaccumulator 代码分析",charIndex:1946},{level:3,title:"tryAppend",slug:"tryappend",normalizedTitle:"tryappend",charIndex:2993},{level:2,title:"ProducerBatch类分析",slug:"producerbatch类分析",normalizedTitle:"producerbatch类分析",charIndex:8027},{level:2,title:"MemoryRecordsBuilder类分析",slug:"memoryrecordsbuilder类分析",normalizedTitle:"memoryrecordsbuilder类分析",charIndex:9631},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:838},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:11225}],headersStr:"前言 RecordAccumulator设计 RecordAccumulator 代码分析 tryAppend ProducerBatch类分析 MemoryRecordsBuilder类分析 总结 参考资料",content:'# 前言\n\n我们知道RecordAccumulator是缓存待发送消息的地方，KafkaProducer把消息放进来，当消息满了的时候，通知sender来把消息发出去，释放空间。RecordAccumulator就相当于货运站的仓储，货物不断的往里放，每装满一箱就会通知发货者来取货运走。如下图所示：\n\n从上图可以看到，至少有一个业务主线程和一个sender线程同时操作RecordAccumulator，所以他必须是线程安全的。\n\n下面我们来详细分析 RecordAccumulator\n\n\n# RecordAccumulator设计\n\n我们直接一头扎入程序的设计和代码，会有一定的理解难度。我还是先以真实世界的某个事物做类比来入手。\n\n前文说RecordAccumulator是一个累积消息的仓库，那么我们就拿快递仓库来类比，看看RecordAccumulator是个怎样的仓库，看下图：\n\n\n\n上图是一个快递站的仓库，堆满了货物。分拣员在这里工作。我们可以看到发往不同目的地的大货箱放置在各自对应的区域，分拣员把不同目的地的包裹放入对应目的地的大货箱，每装满一箱就放置在对应的堆放区域。\n\n分拣员工作流程如下：\n\n 1. 分拣员收到一个包裹，先查看目的地是哪里。假设是北京朝阳，他需要找到目的地为北京朝阳的大货箱装进去。\n 2. 当这个大箱子装满后，分拣员会把它封箱，然后搬运到挂有北京朝阳牌子的区域，堆放起来。\n 3. 当分拣员再拿到北京朝阳的包裹时，由于没有可用的北京朝阳大货箱，他需要再拿来一个北京朝阳的大货箱来放置包裹。\n\n以上就是分拣员所做的工作，分拣员是谁呢？分拣员就是 RecordAccumulator！而那些大货箱以及各自所属的堆放区域，就是RecordAccumulator 中缓存消息的地方。所有封箱的大货箱都会等待 sender 来取货发送出去。\n\n如果你看懂了上面这张图，那么你已经充分理解了 RecordAccumulator 的设计\n\n我们总结下仓库里有什么：\n\n 1. 分拣员\n 2. 货物\n 3. 目的地\n 4. 货箱\n 5. 货箱堆放区域\n\n记住这些概念，这些仓库里的东西最终都会体现在代码里。\n\n下面我们来真正讲解 RecordAccumulator 的设计\n\nRecordAccumulator 实现了接收消息，然后以主题分区为单元，把消息以 ProducerBatch 为单位累积缓存。多个 ProducerBatch 保存在 Deque 队列中。当 Deque 中最新的 batch 已不能容纳消息时，就会创建新的 batch 来继续缓存，并将其加入 Deque\n\nRecordAccumulator缓存消息的存储结构如下：\n\n\n\nRecordAccumulator内部存储消息使用的容器是ConcurrentMap<TopicPartition, Deque<ProducerBatch>>，通过上图可以看到消息以主题分区划分存储单元。消息实际是放在ProducerBatch中。ProducerBatch相当于一个个箱子，箱子上写着收件地址：xx主题xx分区。当一个ProducerBatch箱子装满时，就会封箱贴上封条，然后在对应的队列里生成一个新的ProducerBatch，来放置本主题分区新的消息。\n\n由此可见，RecordAccumulator累积消息的过程，就是把消息装进不同收件地址箱子（ProducerBatch），装满封箱，堆放起来（加入Deque<ProducerBatch>），然后继续产生新箱子装消息的过程。\n\n每次封箱操作后都会返回 可以发货 的结果给调用者，调用者 KafkaProducer 再唤醒 sender 把已经封箱的 ProducerBatch 发送出去\n\n图中可以看到，消息真实存储的地方是 DataOutputStream。ProducerBatch 内部有一个 MemoryRecordsBuilder 对象，图中未画，而DataOutputStream 在 MemoryRecordsBuilder中。三者关系：ProducerBatch--\x3eMemoryRecordsBuilder--\x3eDataOutputStream。\n\n接下来对RecordAccumulator的代码分析，主要围绕以下三个类：\n\n * RecordAccumulator：消息累积器的顶层逻辑，维护存放消息的容器\n\n * ProducerBatch：封装 MemoryRecordsBuilder，并且有很多控制用的信息及统计信息\n\n * MemoryRecordsBuilder：消息真正累积存放的地方\n\n\n# RecordAccumulator 代码分析\n\nappend()方法是 RecordAccumulator 暴露的累积消息入口，KafkaProducer 通过此接口累积消息。我们也先从此方法开始层层递进，分析累积消息的逻辑。\n\npublic RecordAppendResult append(TopicPartition tp,\n                                 long timestamp,\n                                 byte[] key,\n                                 byte[] value,\n                                 Header[] headers,\n                                 Callback callback,\n                                 long maxTimeToBlock) throws InterruptedException {\n    // We keep track of the number of appending thread to make sure we do not miss batches in\n    // abortIncompleteBatches().\n    appendsInProgress.incrementAndGet();\n    ByteBuffer buffer = null;\n    if (headers == null) headers = Record.EMPTY_HEADERS;\n    try {\n        // check if we have an in-progress batch\n        Deque<ProducerBatch> dq = getOrCreateDeque(tp);\n        synchronized (dq) {\n            if (closed)\n                throw new KafkaException("Producer closed while send in progress");\n            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);\n            if (appendResult != null)\n                return appendResult;\n        }\n \n        // we don\'t have an in-progress record batch try to allocate a new batch\n        byte maxUsableMagic = apiVersions.maxUsableProduceMagic();\n        int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));\n        log.trace("Allocating a new {} byte message buffer for topic {} partition {}", size, tp.topic(), tp.partition());\n        buffer = free.allocate(size, maxTimeToBlock);\n        synchronized (dq) {\n            // Need to check if producer is closed again after grabbing the dequeue lock.\n            if (closed)\n                throw new KafkaException("Producer closed while send in progress");\n \n            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);\n            if (appendResult != null) {\n                // Somebody else found us a batch, return the one we waited for! Hopefully this doesn\'t happen often...\n                return appendResult;\n            }\n \n            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);\n            ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds());\n            FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));\n \n            dq.addLast(batch);\n            incomplete.add(batch);\n \n            // Don\'t deallocate this buffer in the finally block as it\'s being used in the record batch\n            buffer = null;\n            return new RecordAppendResult(future, dq.size() > 1 || batch.isFull(), true);\n        }\n    } finally {\n        if (buffer != null)\n            free.deallocate(buffer);\n        appendsInProgress.decrementAndGet();\n    }\n}\n\n\n 1.  appendsInProgress.incrementAndGet()：首先通过原子操作，把追加消息的线程计数器+1\n 2.  Deque<ProducerBatch> dq = getOrCreateDeque(tp)：获取主题分区对应的ProducerBatch队列。getOrCreateDeque方法中判断如果没有队列，则新建队列。\n 3.  RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq)：同步代码块中尝试去做一次追加操作tryAppend(),如果成功就直接返回追加的结果对象。tryAppend方法中逻辑是：\n     1. 如果 dq 中有ProducerBatch 则往新一个batch中追加\n        1. 追加不成功，说明最新的batch空间不足，返回null。需要外层逻辑创建新的batch\n        2. 追加成功，返回RecordAppendResult\n     2. dq 中无 producerBatch，返回null，代表没有能追加成功：第一个到达此方法的线程肯定是返回了null，因为还没有消息累积进来，也不存在ProducerBatch对象。如果tryAppend返回null，说明没能直接在现有的batch上追加成功（也可能还没有batch），此时需要初始化新的ProducerBatch\n 4.  预估size大小，从 BufferPool 申请 ByteBuffer：如果BufferPool空间不足就轮询等待。大家可以自己看一下 BufferPool 的代码，这里就不展开讲了，他的目的是控制总的内存消耗以及实现 ByteBuffer 复用。\n 5.  再次tryAppend：由于第4步代码不在同步代码块中，所以接下来的同步代码块中，首先需要再次调用tryAppend，因为可能别的线程已经创建了本主题分区新的ProducerBatch，那么消息直接追加成功\n 6.  创建ProducerBatch：如果上一步返回null，说明还是没有可用的ProducerBatch，我们需要创建新的ProducerBatch。首先构建MemoryRecordsBuilder对象，真正做消息累加的就是这个对象，每个 ProducerBatch 都有一个 MemoryRecordsBuilder 的引用。\n 7.  真正去追加消息：\n     1. 首先调用ProducerBatch的tryAppend()方法，这是真正做消息追加的地方，内部通过MemoryRecordsBuilder实现。后续再详细分析。\n     2. 然后把新的ProducerBatch放入队列中\n 8.  把batch放入incomplete集合：incomplete本质是个存放未完成发送batch的Set\n 9.  释放BufferPool空间\n 10. 累积消息完成后的处理：finally 代码块中再最终确保释放 BufferPool 空间，然后通过原子操作把追加消息的线程计数器 -1\n\n\n# tryAppend\n\nprivate RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Header[] headers,\n                                     Callback callback, Deque<ProducerBatch> deque) {\n    ProducerBatch last = deque.peekLast();\n    if (last != null) {\n        FutureRecordMetadata future = last.tryAppend(timestamp, key, value, headers, callback, time.milliseconds());\n        if (future == null)\n            last.closeForRecordAppends();\n        else\n            return new RecordAppendResult(future, deque.size() > 1 || last.isFull(), false);\n    }\n    return null;\n}\n\n\n逻辑上文已经讲过，这里再对照代码多说两句：\n\n方法如果返回null，说明没有可以成功追加此消息的ProducerBatch，有两种情况：\n\n 1. deque是空的，可能是第一次被进入，也可能是batch都被发送完了。\n 2. deque存在batch，但是所剩空间已经不足以容纳此消息。\n\n如果能取得队列中最新的batch，并且能够成功追加消息，那么就会返回 RecordAppendResult。\n\nappend方法返回对象RecordAppendResult，代码如下：\n\npublic final static class RecordAppendResult {\n    public final FutureRecordMetadata future;\n    public final boolean batchIsFull;\n    public final boolean newBatchCreated;\n \n    public RecordAppendResult(FutureRecordMetadata future, boolean batchIsFull, boolean newBatchCreated) {\n        this.future = future;\n        this.batchIsFull = batchIsFull;\n        this.newBatchCreated = newBatchCreated;\n    }\n}\n\n\n我们可以看到里面有异步发送的Future对象，此外还有两个标识，batchIsFull代表batch是否满了，newBatchCreated代表是否本次append增加了新的batch\n\n大家是否还记得KafkaProducer调用accumulator的apend方法后的逻辑是什么吗？不记得也没有关系，代码如下：\n\nif (result.batchIsFull || result.newBatchCreated) {\n    log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);\n    this.sender.wakeup();\n}\n\n\nKafkaProducer通过这两个标识位来决定是否唤醒sender。翻译过来就是，“已经有封箱的消息了！sender快点把消息发走吧！不要再睡了”\n\n讲到这里，其实整个消息追加的流程已经讲通了。不过消息追加的具体实现我们还没有讲解，那么接下来我们讲解发生消息追加的真正地方：ProducerBatch和MemoryRecordsBuilder。\n\n\n# ProducerBatch类分析\n\n前文说过ProducerBatch可以理解为存放消息的大货箱。此类中的主要方法是 tryAppend，也就是把消息放入箱子的操作，它是消息追加的顶层逻辑，代码如下：\n\npublic FutureRecordMetadata tryAppend(long timestamp, byte[] key, byte[] value, Header[] headers, Callback callback, long now) {\n    if (!recordsBuilder.hasRoomFor(timestamp, key, value, headers)) {\n        return null;\n    } else {\n        Long checksum = this.recordsBuilder.append(timestamp, key, value, headers);\n        this.maxRecordSize = Math.max(this.maxRecordSize, AbstractRecords.estimateSizeInBytesUpperBound(magic(),\n                recordsBuilder.compressionType(), key, value, headers));\n        this.lastAppendTime = now;\n        FutureRecordMetadata future = new FutureRecordMetadata(this.produceFuture, this.recordCount,\n                                                               timestamp, checksum,\n                                                               key == null ? -1 : key.length,\n                                                               value == null ? -1 : value.length,\n                                                               Time.SYSTEM);\n        // we have to keep every future returned to the users in case the batch needs to be\n        // split to several new batches and resent.\n        thunks.add(new Thunk(callback, future));\n        this.recordCount++;\n        return future;\n    }\n}\n\n\n主要做了三件事\n\n 1. 检查是否有足够空间容纳消息，这是通过调用MemoryRecordsBuilder的hasRoomFor()方法。\n 2. 追加消息，通过调用MemoryRecordsBuilder的append()方法\n 3. 保存存放了callback和对应FutureRecordMetadata对象的thunk到List thunks中。\n\n另外还有一个重要的方法就是closeForRecordAppends()，当batch无空间容纳新的消息的时候会调用此方法封箱，这里不展开来讲。\n\n\n# MemoryRecordsBuilder类分析\n\n讲到这里，终于讲到消息追加真正落地的地方了。每个ProducerBatch都维护了一个MemoryRecordsBuilder。ProducerBatch追加消息，实际是调用MemoryRecordsBuilder完成的。\n\n消息最终通过MemoryRecordsBuilder的append方法，追加到MemoryRecordsBuilder的DataOutputStream中。\n\n上一节我们可以看到它有两个主要的方法hasRoomFor()和append()。\n\n 1. hasRoomFor()：这个方法比较简单，通过计算消息的预估大小，以及剩余空间，返回true或者false。代码就不贴了，感兴趣的话自行查看\n\n 2. append()：这个方法我们需要仔细分析一下，消息的追加最终发生在这里。我先简述下逻辑：\n    \n    * 把字节数组形式的key和value转为HeapByteBuffer\n    * 计算写入的offset，如果不是第一次写入，那么lastOffset+1，否则是 baseOffset\n    * 如果magic号大于1，那么调用appendDefaultRecord()，否则调用 appendLegacyRecord()\n\n我们继续看一下appendDefaultRecord()方法，在此方法中最终调用了DefaultRecord.writeTo()来写入appendStream\n\n最后做检查，更新一些统计状态，如消息的数量、lastOffset等。\n\n在DefaultRecord.writeTo()方法中，通过调用Utils.writeTo(DataOutput out, ByteBuffer buffer, int length)，往appendStream写入key，value，header。\n\nUtils.writeTo()代码如下：\n\nif (buffer.hasArray()) {\n    out.write(buffer.array(), buffer.position() + buffer.arrayOffset(), length);\n} else {\n    int pos = buffer.position();\n    for (int i = pos; i < length + pos; i++)\n        out.writeByte(buffer.get(i));\n}\n\n\n我们总结一下MemoryRecordsBuilder：\n\n * 它内部维护了一个buffer，并记录能写入的大小，以及写入的位置在哪里\n * 而每条消息都被追加到DataOutput对象appendStream中\n * appendStream是消息在RecordAccumulator中最终的去处\n\n\n# 总结\n\n至此，对RecordAccumulator的讲解就结束了，其实只是结束了追加消息部分，本片博客的讲解范围限于此。在sender讲解中我们再继续讲解RecordAccumulator另外的内容。\n\n最后我们总结一下：\n\n 1. RecordAccumulator使用ProducerBatch缓存消息。每个主题分区拥有一个ProducerBatch的队列。\n\n 2. 当ProducerBatch队列的队尾batch不能再容纳新消息时，对其进行封箱操作，同时新建ProducerBatch放入队尾来存放新消息。\n\n 3. ProducerBatch对消息追加的操作都是通过MemoryRecordsBuilder进行的。消息最终被追加到MemoryRecordsBuilder中的DataOutputStream appendStream中\n\n\n# 参考资料\n\n * 你绝对能看懂的Kafka源代码分析-RecordAccumulator类代码分析_kafka listenablefuturecallback-CSDN博客',normalizedContent:'# 前言\n\n我们知道recordaccumulator是缓存待发送消息的地方，kafkaproducer把消息放进来，当消息满了的时候，通知sender来把消息发出去，释放空间。recordaccumulator就相当于货运站的仓储，货物不断的往里放，每装满一箱就会通知发货者来取货运走。如下图所示：\n\n从上图可以看到，至少有一个业务主线程和一个sender线程同时操作recordaccumulator，所以他必须是线程安全的。\n\n下面我们来详细分析 recordaccumulator\n\n\n# recordaccumulator设计\n\n我们直接一头扎入程序的设计和代码，会有一定的理解难度。我还是先以真实世界的某个事物做类比来入手。\n\n前文说recordaccumulator是一个累积消息的仓库，那么我们就拿快递仓库来类比，看看recordaccumulator是个怎样的仓库，看下图：\n\n\n\n上图是一个快递站的仓库，堆满了货物。分拣员在这里工作。我们可以看到发往不同目的地的大货箱放置在各自对应的区域，分拣员把不同目的地的包裹放入对应目的地的大货箱，每装满一箱就放置在对应的堆放区域。\n\n分拣员工作流程如下：\n\n 1. 分拣员收到一个包裹，先查看目的地是哪里。假设是北京朝阳，他需要找到目的地为北京朝阳的大货箱装进去。\n 2. 当这个大箱子装满后，分拣员会把它封箱，然后搬运到挂有北京朝阳牌子的区域，堆放起来。\n 3. 当分拣员再拿到北京朝阳的包裹时，由于没有可用的北京朝阳大货箱，他需要再拿来一个北京朝阳的大货箱来放置包裹。\n\n以上就是分拣员所做的工作，分拣员是谁呢？分拣员就是 recordaccumulator！而那些大货箱以及各自所属的堆放区域，就是recordaccumulator 中缓存消息的地方。所有封箱的大货箱都会等待 sender 来取货发送出去。\n\n如果你看懂了上面这张图，那么你已经充分理解了 recordaccumulator 的设计\n\n我们总结下仓库里有什么：\n\n 1. 分拣员\n 2. 货物\n 3. 目的地\n 4. 货箱\n 5. 货箱堆放区域\n\n记住这些概念，这些仓库里的东西最终都会体现在代码里。\n\n下面我们来真正讲解 recordaccumulator 的设计\n\nrecordaccumulator 实现了接收消息，然后以主题分区为单元，把消息以 producerbatch 为单位累积缓存。多个 producerbatch 保存在 deque 队列中。当 deque 中最新的 batch 已不能容纳消息时，就会创建新的 batch 来继续缓存，并将其加入 deque\n\nrecordaccumulator缓存消息的存储结构如下：\n\n\n\nrecordaccumulator内部存储消息使用的容器是concurrentmap<topicpartition, deque<producerbatch>>，通过上图可以看到消息以主题分区划分存储单元。消息实际是放在producerbatch中。producerbatch相当于一个个箱子，箱子上写着收件地址：xx主题xx分区。当一个producerbatch箱子装满时，就会封箱贴上封条，然后在对应的队列里生成一个新的producerbatch，来放置本主题分区新的消息。\n\n由此可见，recordaccumulator累积消息的过程，就是把消息装进不同收件地址箱子（producerbatch），装满封箱，堆放起来（加入deque<producerbatch>），然后继续产生新箱子装消息的过程。\n\n每次封箱操作后都会返回 可以发货 的结果给调用者，调用者 kafkaproducer 再唤醒 sender 把已经封箱的 producerbatch 发送出去\n\n图中可以看到，消息真实存储的地方是 dataoutputstream。producerbatch 内部有一个 memoryrecordsbuilder 对象，图中未画，而dataoutputstream 在 memoryrecordsbuilder中。三者关系：producerbatch--\x3ememoryrecordsbuilder--\x3edataoutputstream。\n\n接下来对recordaccumulator的代码分析，主要围绕以下三个类：\n\n * recordaccumulator：消息累积器的顶层逻辑，维护存放消息的容器\n\n * producerbatch：封装 memoryrecordsbuilder，并且有很多控制用的信息及统计信息\n\n * memoryrecordsbuilder：消息真正累积存放的地方\n\n\n# recordaccumulator 代码分析\n\nappend()方法是 recordaccumulator 暴露的累积消息入口，kafkaproducer 通过此接口累积消息。我们也先从此方法开始层层递进，分析累积消息的逻辑。\n\npublic recordappendresult append(topicpartition tp,\n                                 long timestamp,\n                                 byte[] key,\n                                 byte[] value,\n                                 header[] headers,\n                                 callback callback,\n                                 long maxtimetoblock) throws interruptedexception {\n    // we keep track of the number of appending thread to make sure we do not miss batches in\n    // abortincompletebatches().\n    appendsinprogress.incrementandget();\n    bytebuffer buffer = null;\n    if (headers == null) headers = record.empty_headers;\n    try {\n        // check if we have an in-progress batch\n        deque<producerbatch> dq = getorcreatedeque(tp);\n        synchronized (dq) {\n            if (closed)\n                throw new kafkaexception("producer closed while send in progress");\n            recordappendresult appendresult = tryappend(timestamp, key, value, headers, callback, dq);\n            if (appendresult != null)\n                return appendresult;\n        }\n \n        // we don\'t have an in-progress record batch try to allocate a new batch\n        byte maxusablemagic = apiversions.maxusableproducemagic();\n        int size = math.max(this.batchsize, abstractrecords.estimatesizeinbytesupperbound(maxusablemagic, compression, key, value, headers));\n        log.trace("allocating a new {} byte message buffer for topic {} partition {}", size, tp.topic(), tp.partition());\n        buffer = free.allocate(size, maxtimetoblock);\n        synchronized (dq) {\n            // need to check if producer is closed again after grabbing the dequeue lock.\n            if (closed)\n                throw new kafkaexception("producer closed while send in progress");\n \n            recordappendresult appendresult = tryappend(timestamp, key, value, headers, callback, dq);\n            if (appendresult != null) {\n                // somebody else found us a batch, return the one we waited for! hopefully this doesn\'t happen often...\n                return appendresult;\n            }\n \n            memoryrecordsbuilder recordsbuilder = recordsbuilder(buffer, maxusablemagic);\n            producerbatch batch = new producerbatch(tp, recordsbuilder, time.milliseconds());\n            futurerecordmetadata future = utils.notnull(batch.tryappend(timestamp, key, value, headers, callback, time.milliseconds()));\n \n            dq.addlast(batch);\n            incomplete.add(batch);\n \n            // don\'t deallocate this buffer in the finally block as it\'s being used in the record batch\n            buffer = null;\n            return new recordappendresult(future, dq.size() > 1 || batch.isfull(), true);\n        }\n    } finally {\n        if (buffer != null)\n            free.deallocate(buffer);\n        appendsinprogress.decrementandget();\n    }\n}\n\n\n 1.  appendsinprogress.incrementandget()：首先通过原子操作，把追加消息的线程计数器+1\n 2.  deque<producerbatch> dq = getorcreatedeque(tp)：获取主题分区对应的producerbatch队列。getorcreatedeque方法中判断如果没有队列，则新建队列。\n 3.  recordappendresult appendresult = tryappend(timestamp, key, value, headers, callback, dq)：同步代码块中尝试去做一次追加操作tryappend(),如果成功就直接返回追加的结果对象。tryappend方法中逻辑是：\n     1. 如果 dq 中有producerbatch 则往新一个batch中追加\n        1. 追加不成功，说明最新的batch空间不足，返回null。需要外层逻辑创建新的batch\n        2. 追加成功，返回recordappendresult\n     2. dq 中无 producerbatch，返回null，代表没有能追加成功：第一个到达此方法的线程肯定是返回了null，因为还没有消息累积进来，也不存在producerbatch对象。如果tryappend返回null，说明没能直接在现有的batch上追加成功（也可能还没有batch），此时需要初始化新的producerbatch\n 4.  预估size大小，从 bufferpool 申请 bytebuffer：如果bufferpool空间不足就轮询等待。大家可以自己看一下 bufferpool 的代码，这里就不展开讲了，他的目的是控制总的内存消耗以及实现 bytebuffer 复用。\n 5.  再次tryappend：由于第4步代码不在同步代码块中，所以接下来的同步代码块中，首先需要再次调用tryappend，因为可能别的线程已经创建了本主题分区新的producerbatch，那么消息直接追加成功\n 6.  创建producerbatch：如果上一步返回null，说明还是没有可用的producerbatch，我们需要创建新的producerbatch。首先构建memoryrecordsbuilder对象，真正做消息累加的就是这个对象，每个 producerbatch 都有一个 memoryrecordsbuilder 的引用。\n 7.  真正去追加消息：\n     1. 首先调用producerbatch的tryappend()方法，这是真正做消息追加的地方，内部通过memoryrecordsbuilder实现。后续再详细分析。\n     2. 然后把新的producerbatch放入队列中\n 8.  把batch放入incomplete集合：incomplete本质是个存放未完成发送batch的set\n 9.  释放bufferpool空间\n 10. 累积消息完成后的处理：finally 代码块中再最终确保释放 bufferpool 空间，然后通过原子操作把追加消息的线程计数器 -1\n\n\n# tryappend\n\nprivate recordappendresult tryappend(long timestamp, byte[] key, byte[] value, header[] headers,\n                                     callback callback, deque<producerbatch> deque) {\n    producerbatch last = deque.peeklast();\n    if (last != null) {\n        futurerecordmetadata future = last.tryappend(timestamp, key, value, headers, callback, time.milliseconds());\n        if (future == null)\n            last.closeforrecordappends();\n        else\n            return new recordappendresult(future, deque.size() > 1 || last.isfull(), false);\n    }\n    return null;\n}\n\n\n逻辑上文已经讲过，这里再对照代码多说两句：\n\n方法如果返回null，说明没有可以成功追加此消息的producerbatch，有两种情况：\n\n 1. deque是空的，可能是第一次被进入，也可能是batch都被发送完了。\n 2. deque存在batch，但是所剩空间已经不足以容纳此消息。\n\n如果能取得队列中最新的batch，并且能够成功追加消息，那么就会返回 recordappendresult。\n\nappend方法返回对象recordappendresult，代码如下：\n\npublic final static class recordappendresult {\n    public final futurerecordmetadata future;\n    public final boolean batchisfull;\n    public final boolean newbatchcreated;\n \n    public recordappendresult(futurerecordmetadata future, boolean batchisfull, boolean newbatchcreated) {\n        this.future = future;\n        this.batchisfull = batchisfull;\n        this.newbatchcreated = newbatchcreated;\n    }\n}\n\n\n我们可以看到里面有异步发送的future对象，此外还有两个标识，batchisfull代表batch是否满了，newbatchcreated代表是否本次append增加了新的batch\n\n大家是否还记得kafkaproducer调用accumulator的apend方法后的逻辑是什么吗？不记得也没有关系，代码如下：\n\nif (result.batchisfull || result.newbatchcreated) {\n    log.trace("waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);\n    this.sender.wakeup();\n}\n\n\nkafkaproducer通过这两个标识位来决定是否唤醒sender。翻译过来就是，“已经有封箱的消息了！sender快点把消息发走吧！不要再睡了”\n\n讲到这里，其实整个消息追加的流程已经讲通了。不过消息追加的具体实现我们还没有讲解，那么接下来我们讲解发生消息追加的真正地方：producerbatch和memoryrecordsbuilder。\n\n\n# producerbatch类分析\n\n前文说过producerbatch可以理解为存放消息的大货箱。此类中的主要方法是 tryappend，也就是把消息放入箱子的操作，它是消息追加的顶层逻辑，代码如下：\n\npublic futurerecordmetadata tryappend(long timestamp, byte[] key, byte[] value, header[] headers, callback callback, long now) {\n    if (!recordsbuilder.hasroomfor(timestamp, key, value, headers)) {\n        return null;\n    } else {\n        long checksum = this.recordsbuilder.append(timestamp, key, value, headers);\n        this.maxrecordsize = math.max(this.maxrecordsize, abstractrecords.estimatesizeinbytesupperbound(magic(),\n                recordsbuilder.compressiontype(), key, value, headers));\n        this.lastappendtime = now;\n        futurerecordmetadata future = new futurerecordmetadata(this.producefuture, this.recordcount,\n                                                               timestamp, checksum,\n                                                               key == null ? -1 : key.length,\n                                                               value == null ? -1 : value.length,\n                                                               time.system);\n        // we have to keep every future returned to the users in case the batch needs to be\n        // split to several new batches and resent.\n        thunks.add(new thunk(callback, future));\n        this.recordcount++;\n        return future;\n    }\n}\n\n\n主要做了三件事\n\n 1. 检查是否有足够空间容纳消息，这是通过调用memoryrecordsbuilder的hasroomfor()方法。\n 2. 追加消息，通过调用memoryrecordsbuilder的append()方法\n 3. 保存存放了callback和对应futurerecordmetadata对象的thunk到list thunks中。\n\n另外还有一个重要的方法就是closeforrecordappends()，当batch无空间容纳新的消息的时候会调用此方法封箱，这里不展开来讲。\n\n\n# memoryrecordsbuilder类分析\n\n讲到这里，终于讲到消息追加真正落地的地方了。每个producerbatch都维护了一个memoryrecordsbuilder。producerbatch追加消息，实际是调用memoryrecordsbuilder完成的。\n\n消息最终通过memoryrecordsbuilder的append方法，追加到memoryrecordsbuilder的dataoutputstream中。\n\n上一节我们可以看到它有两个主要的方法hasroomfor()和append()。\n\n 1. hasroomfor()：这个方法比较简单，通过计算消息的预估大小，以及剩余空间，返回true或者false。代码就不贴了，感兴趣的话自行查看\n\n 2. append()：这个方法我们需要仔细分析一下，消息的追加最终发生在这里。我先简述下逻辑：\n    \n    * 把字节数组形式的key和value转为heapbytebuffer\n    * 计算写入的offset，如果不是第一次写入，那么lastoffset+1，否则是 baseoffset\n    * 如果magic号大于1，那么调用appenddefaultrecord()，否则调用 appendlegacyrecord()\n\n我们继续看一下appenddefaultrecord()方法，在此方法中最终调用了defaultrecord.writeto()来写入appendstream\n\n最后做检查，更新一些统计状态，如消息的数量、lastoffset等。\n\n在defaultrecord.writeto()方法中，通过调用utils.writeto(dataoutput out, bytebuffer buffer, int length)，往appendstream写入key，value，header。\n\nutils.writeto()代码如下：\n\nif (buffer.hasarray()) {\n    out.write(buffer.array(), buffer.position() + buffer.arrayoffset(), length);\n} else {\n    int pos = buffer.position();\n    for (int i = pos; i < length + pos; i++)\n        out.writebyte(buffer.get(i));\n}\n\n\n我们总结一下memoryrecordsbuilder：\n\n * 它内部维护了一个buffer，并记录能写入的大小，以及写入的位置在哪里\n * 而每条消息都被追加到dataoutput对象appendstream中\n * appendstream是消息在recordaccumulator中最终的去处\n\n\n# 总结\n\n至此，对recordaccumulator的讲解就结束了，其实只是结束了追加消息部分，本片博客的讲解范围限于此。在sender讲解中我们再继续讲解recordaccumulator另外的内容。\n\n最后我们总结一下：\n\n 1. recordaccumulator使用producerbatch缓存消息。每个主题分区拥有一个producerbatch的队列。\n\n 2. 当producerbatch队列的队尾batch不能再容纳新消息时，对其进行封箱操作，同时新建producerbatch放入队尾来存放新消息。\n\n 3. producerbatch对消息追加的操作都是通过memoryrecordsbuilder进行的。消息最终被追加到memoryrecordsbuilder中的dataoutputstream appendstream中\n\n\n# 参考资料\n\n * 你绝对能看懂的kafka源代码分析-recordaccumulator类代码分析_kafka listenablefuturecallback-csdn博客',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"NetworkClient 类代码分析",frontmatter:{title:"NetworkClient 类代码分析",date:"2024-09-18T15:34:03.000Z",permalink:"/pages/b2caf1/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/500.%E3%80%8C%E7%94%9F%E4%BA%A7%E8%80%85%E3%80%8D%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/50.NetworkClient%20%E7%B1%BB%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/50.NetworkClient 类代码分析.md",key:"v-1d6fec1a",path:"/pages/b2caf1/",headers:[{level:2,title:"Kafka IO部分设计",slug:"kafka-io部分设计",normalizedTitle:"kafka io部分设计",charIndex:199},{level:2,title:"流程分析",slug:"流程分析",normalizedTitle:"流程分析",charIndex:928},{level:3,title:"send()方法",slug:"send-方法",normalizedTitle:"send()方法",charIndex:1154},{level:3,title:"Selector的send()方法",slug:"selector的send-方法",normalizedTitle:"selector的send()方法",charIndex:5247},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:5269},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:5276}],headersStr:"Kafka IO部分设计 流程分析 send()方法 Selector的send()方法 总结 参考资料",content:'# 前言\n\n通过前文的学习，我们知道Sender最终把消息发送出去，依靠的是NetWorkClient。它是Kafka的一个重要组件，负责网络IO，包括连接的建立，读数据、写数据等等。Kafka网络IO的实现是通过java的NIO，Kafka对NIO进行了封装。在学习Kafka网络IO相关之前，大家先参考网上文章对NIO有简单的了解（后续我可能也会写一篇入门教程），再继续阅读本篇文章。\n\n\n# Kafka IO部分设计\n\nKafka IO部分涉及的主要类和依赖关系见下图：\n\n\n\n上半部分是Kafka的类，下半部是 java nio 的类。Kafka的类讲解如下：\n\n 1. NetWorkClient，顾名思义，这是 Kafka IO 对外暴露的客户端。IO操作都是通过它来对外暴露方法调用。实际上它是通过Kafka的KSelector 来实现。\n 2. KSelector，其实此类名称也是Selector，为了区分nio的selector，故称之为KSelector。他拥有nio selector的引用。此外他维护了所有的KafkaChannel。\n 3. KafkaChannel，他对应nio中的Channel概念，它通过TransportLayer间接持有SocketChannel和SelectionKey这两个nio中的核心对象。另外他还维护了发送和接收的数据对象：Send实现及NetWorkReceive。另外请注意唯一一个从下往上的箭头，KafkaChannel还会把自己attach到自己对应的SelectionKey中。这样可以通过SelectionKey方便取到对应KafkaChannel。\n 4. TransportLayer，从名称可以看出这个类实现传输层功能，而传输是通过nio实现，所以他持有SocketChannel和Selector这两个nio的核心对象。他所做的事情就是通过这两个对象实现网络IO。\n 5. Send，这是一个接口，有多个实现，目的就是封装要发送的数据，底层是nio的ByteBuffer。\n 6. NetWorkReceive，接收数据的对象，底层是nio的ByteBuffer\n\n\n# 流程分析\n\nNetWorkClient实现通道的建立，读取消息、发送消息等功能。这些功能上原理是相同的，我们继续从 KafkaProducer 发送消息为入口点，继续分析发送消息的流程。\n\n前文讲到，Sender最终通过NetWorkClient的两个方法完成消息发送，如下：\n\nclient.send(clientRequest, now);\nclient.poll(pollTimeout, now);\n\n\n那么我们就从这两个方法开始分析\n\n\n# send()方法\n\n我们回忆一下sender发送消息流程，sender把batch按照要发往的node分好类，分装为ClientRequest，然后调用NetWorkClient的send方法。在这个方法里并没有真正网络IO，而只是准备好了要发送的请求对象。\n\nSender的send方法中实际调用的是doSend(ClientRequest clientRequest, boolean isInternalRequest, long now)方法。\n\n代码如下：\n\n\tensureActive();\n    String nodeId = clientRequest.destination();\n    if (!isInternalRequest) {\n        // If this request came from outside the NetworkClient, validate\n        // that we can send data.  If the request is internal, we trust\n        // that internal code has done this validation.  Validation\n        // will be slightly different for some internal requests (for\n        // example, ApiVersionsRequests can be sent prior to being in\n        // READY state.)\n        if (!canSendRequest(nodeId, now))\n            throw new IllegalStateException("Attempt to send a request to node " + nodeId + " which is not ready.");\n    }\n    AbstractRequest.Builder<?> builder = clientRequest.requestBuilder();\n    try {\n        NodeApiVersions versionInfo = apiVersions.get(nodeId);\n        short version;\n        // Note: if versionInfo is null, we have no server version information. This would be\n        // the case when sending the initial ApiVersionRequest which fetches the version\n        // information itself.  It is also the case when discoverBrokerVersions is set to false.\n        if (versionInfo == null) {\n            version = builder.latestAllowedVersion();\n            if (discoverBrokerVersions && log.isTraceEnabled())\n                log.trace("No version information found when sending {} with correlation id {} to node {}. " +\n                        "Assuming version {}.", clientRequest.apiKey(), clientRequest.correlationId(), nodeId, version);\n        } else {\n            version = versionInfo.latestUsableVersion(clientRequest.apiKey(), builder.oldestAllowedVersion(),\n                    builder.latestAllowedVersion());\n        }\n        // The call to build may also throw UnsupportedVersionException, if there are essential\n        // fields that cannot be represented in the chosen version.\n        doSend(clientRequest, isInternalRequest, now, builder.build(version));\n    } catch (UnsupportedVersionException unsupportedVersionException) {\n        // If the version is not supported, skip sending the request over the wire.\n        // Instead, simply add it to the local queue of aborted requests.\n        log.debug("Version mismatch when attempting to send {} with correlation id {} to {}", builder,\n                clientRequest.correlationId(), clientRequest.destination(), unsupportedVersionException);\n        ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(builder.latestAllowedVersion()),\n                clientRequest.callback(), clientRequest.destination(), now, now,\n                false, unsupportedVersionException, null, null);\n        abortedSends.add(clientResponse);\n    }\n}\n\n\n此方法逻辑如下：\n\n 1. 检查NetWorkClient状态为激活。\n 2. 取得请求发送目的地的nodeId。\n 3. 如果是非内部请求，检查connectionState是否ready、Channel是否ready、是否达到发送中上限\n 4. 通过ClientRequest携带的AbstractRequest.Builder对象获取的version以及目的地node的api version，来取得最终的version\n 5. 通过builder.build(version)方法，来初始化request，这里实际生成的是ProduceRequest。\n 6. 最后调用doSend(clientRequest, isInternalRequest, now, builder.build(version));\n\n我们继续看doSend(clientRequest, isInternalRequest, now, builder.build(version))方法。 doSend()方法 此方法核心代码如下：\n\nString destination = clientRequest.destination();\nRequestHeader header = clientRequest.makeHeader(request.version());\nSend send = request.toSend(destination, header);\nInFlightRequest inFlightRequest = new InFlightRequest(\n    clientRequest,\n    header,\n    isInternalRequest,\n    request,\n    send,\n    now);\nthis.inFlightRequests.add(inFlightRequest);\nselector.send(send);\n\n\n逻辑如下：\n\n 1. 获取destination，实际上就是要发往的node的id。\n 2. 生成Requestheader对象，包括apiKey 、version、clientId、 correlation这些属性。\n 3. 生成待发送的send对象,这个send对象封装了目的地和header生成的ByteBuffer对象\n 4. 生成InFlightRequest读信息。它持有ClientRequest，request，send等对象。\n 5. 把InFlightRequest添加到inFlightRequests中，InFlightRequests中按照node的id存储InFlightRequest的队列。\n 6. 最后调用通过selector的send(send)方法做IO前的最后准备工作。\n\n\n# Selector的send()方法\n\n\n# 总结\n\n\n# 参考资料\n\n你绝对能看懂的Kafka源代码分析-NetworkClient类代码分析_kafka源码看哪几个类-CSDN博客',normalizedContent:'# 前言\n\n通过前文的学习，我们知道sender最终把消息发送出去，依靠的是networkclient。它是kafka的一个重要组件，负责网络io，包括连接的建立，读数据、写数据等等。kafka网络io的实现是通过java的nio，kafka对nio进行了封装。在学习kafka网络io相关之前，大家先参考网上文章对nio有简单的了解（后续我可能也会写一篇入门教程），再继续阅读本篇文章。\n\n\n# kafka io部分设计\n\nkafka io部分涉及的主要类和依赖关系见下图：\n\n\n\n上半部分是kafka的类，下半部是 java nio 的类。kafka的类讲解如下：\n\n 1. networkclient，顾名思义，这是 kafka io 对外暴露的客户端。io操作都是通过它来对外暴露方法调用。实际上它是通过kafka的kselector 来实现。\n 2. kselector，其实此类名称也是selector，为了区分nio的selector，故称之为kselector。他拥有nio selector的引用。此外他维护了所有的kafkachannel。\n 3. kafkachannel，他对应nio中的channel概念，它通过transportlayer间接持有socketchannel和selectionkey这两个nio中的核心对象。另外他还维护了发送和接收的数据对象：send实现及networkreceive。另外请注意唯一一个从下往上的箭头，kafkachannel还会把自己attach到自己对应的selectionkey中。这样可以通过selectionkey方便取到对应kafkachannel。\n 4. transportlayer，从名称可以看出这个类实现传输层功能，而传输是通过nio实现，所以他持有socketchannel和selector这两个nio的核心对象。他所做的事情就是通过这两个对象实现网络io。\n 5. send，这是一个接口，有多个实现，目的就是封装要发送的数据，底层是nio的bytebuffer。\n 6. networkreceive，接收数据的对象，底层是nio的bytebuffer\n\n\n# 流程分析\n\nnetworkclient实现通道的建立，读取消息、发送消息等功能。这些功能上原理是相同的，我们继续从 kafkaproducer 发送消息为入口点，继续分析发送消息的流程。\n\n前文讲到，sender最终通过networkclient的两个方法完成消息发送，如下：\n\nclient.send(clientrequest, now);\nclient.poll(polltimeout, now);\n\n\n那么我们就从这两个方法开始分析\n\n\n# send()方法\n\n我们回忆一下sender发送消息流程，sender把batch按照要发往的node分好类，分装为clientrequest，然后调用networkclient的send方法。在这个方法里并没有真正网络io，而只是准备好了要发送的请求对象。\n\nsender的send方法中实际调用的是dosend(clientrequest clientrequest, boolean isinternalrequest, long now)方法。\n\n代码如下：\n\n\tensureactive();\n    string nodeid = clientrequest.destination();\n    if (!isinternalrequest) {\n        // if this request came from outside the networkclient, validate\n        // that we can send data.  if the request is internal, we trust\n        // that internal code has done this validation.  validation\n        // will be slightly different for some internal requests (for\n        // example, apiversionsrequests can be sent prior to being in\n        // ready state.)\n        if (!cansendrequest(nodeid, now))\n            throw new illegalstateexception("attempt to send a request to node " + nodeid + " which is not ready.");\n    }\n    abstractrequest.builder<?> builder = clientrequest.requestbuilder();\n    try {\n        nodeapiversions versioninfo = apiversions.get(nodeid);\n        short version;\n        // note: if versioninfo is null, we have no server version information. this would be\n        // the case when sending the initial apiversionrequest which fetches the version\n        // information itself.  it is also the case when discoverbrokerversions is set to false.\n        if (versioninfo == null) {\n            version = builder.latestallowedversion();\n            if (discoverbrokerversions && log.istraceenabled())\n                log.trace("no version information found when sending {} with correlation id {} to node {}. " +\n                        "assuming version {}.", clientrequest.apikey(), clientrequest.correlationid(), nodeid, version);\n        } else {\n            version = versioninfo.latestusableversion(clientrequest.apikey(), builder.oldestallowedversion(),\n                    builder.latestallowedversion());\n        }\n        // the call to build may also throw unsupportedversionexception, if there are essential\n        // fields that cannot be represented in the chosen version.\n        dosend(clientrequest, isinternalrequest, now, builder.build(version));\n    } catch (unsupportedversionexception unsupportedversionexception) {\n        // if the version is not supported, skip sending the request over the wire.\n        // instead, simply add it to the local queue of aborted requests.\n        log.debug("version mismatch when attempting to send {} with correlation id {} to {}", builder,\n                clientrequest.correlationid(), clientrequest.destination(), unsupportedversionexception);\n        clientresponse clientresponse = new clientresponse(clientrequest.makeheader(builder.latestallowedversion()),\n                clientrequest.callback(), clientrequest.destination(), now, now,\n                false, unsupportedversionexception, null, null);\n        abortedsends.add(clientresponse);\n    }\n}\n\n\n此方法逻辑如下：\n\n 1. 检查networkclient状态为激活。\n 2. 取得请求发送目的地的nodeid。\n 3. 如果是非内部请求，检查connectionstate是否ready、channel是否ready、是否达到发送中上限\n 4. 通过clientrequest携带的abstractrequest.builder对象获取的version以及目的地node的api version，来取得最终的version\n 5. 通过builder.build(version)方法，来初始化request，这里实际生成的是producerequest。\n 6. 最后调用dosend(clientrequest, isinternalrequest, now, builder.build(version));\n\n我们继续看dosend(clientrequest, isinternalrequest, now, builder.build(version))方法。 dosend()方法 此方法核心代码如下：\n\nstring destination = clientrequest.destination();\nrequestheader header = clientrequest.makeheader(request.version());\nsend send = request.tosend(destination, header);\ninflightrequest inflightrequest = new inflightrequest(\n    clientrequest,\n    header,\n    isinternalrequest,\n    request,\n    send,\n    now);\nthis.inflightrequests.add(inflightrequest);\nselector.send(send);\n\n\n逻辑如下：\n\n 1. 获取destination，实际上就是要发往的node的id。\n 2. 生成requestheader对象，包括apikey 、version、clientid、 correlation这些属性。\n 3. 生成待发送的send对象,这个send对象封装了目的地和header生成的bytebuffer对象\n 4. 生成inflightrequest读信息。它持有clientrequest，request，send等对象。\n 5. 把inflightrequest添加到inflightrequests中，inflightrequests中按照node的id存储inflightrequest的队列。\n 6. 最后调用通过selector的send(send)方法做io前的最后准备工作。\n\n\n# selector的send()方法\n\n\n# 总结\n\n\n# 参考资料\n\n你绝对能看懂的kafka源代码分析-networkclient类代码分析_kafka源码看哪几个类-csdn博客',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"KafkaConsumer 类代码分析",frontmatter:{title:"KafkaConsumer 类代码分析",date:"2024-09-18T17:25:41.000Z",permalink:"/pages/6c0fd7/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/600.%E3%80%8C%E6%B6%88%E8%B4%B9%E8%80%85%E3%80%8D%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/05.KafkaConsumer%20%E7%B1%BB%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/600.「消费者」源码分析/05.KafkaConsumer 类代码分析.md",key:"v-18557f3a",path:"/pages/6c0fd7/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"poll()方法",slug:"poll-方法",normalizedTitle:"poll()方法",charIndex:1813},{level:2,title:"pollForFetches()方法",slug:"pollforfetches-方法",normalizedTitle:"pollforfetches()方法",charIndex:4830},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4667},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:7106}],headersStr:"前言 poll()方法 pollForFetches()方法 总结 参考资料",content:'# 前言\n\n前文我们分析了 Kafka 生产者端的源代码，了解了生产者产生消息的过程。消息由生产者发布到某个主题的某个分区上，其实最终是被存储在服务端的某个broker上。而消费者由订阅行为来决定它所要消费的主题和分区。消费者通过poll操作，不断的从服务端拉取该主题分区上产生的消息。\n\n相信有兴趣看kafka源代码的同学，肯定对kafka的基本概念和原理有所了解。关于消费者，我们知道在服务端会有GroupCoordinator（组协调器），负责每个consumer group的leader的选举，以及分发分区分配结果，而coumer的leader则会根据分区分配策略进行分区分配。这里需要注意，分区分配结果并不是由leader分发给同组的consumer，而是leader返回给GroupCoordinator，再有GroupCoordinator进行分发。\n\n每当Broker有变化，或者Consumer Group有出入组的变化时，会触发ConsumerGroup的rebalance。也就是上述的分区分配工作。\n\n另外消费者本地保存了它所负责主题分区的消费状态，通过手动和自动的方式提交到服务端的内部主题中。rebalance过后，消费者重新从内部主题获取对应主题分区的消费位置。\n\n上面我们回顾了Consumer的设计和流程，为我们进入源代码分析做好铺垫。接下来我们将从KafkaConsumer入手，进行代码分析。\n\n我们先看下使用KafkaConsumer进行消费的部分代码：\n\nprivate final KafkaConsumer<Integer, String> consumer;\n \n.........\n \nconsumer.subscribe(Collections.singletonList(this.topic));\n \nConsumerRecords<Integer, String> records = consumer.poll(Duration.ofSeconds(1));\n \nfor (ConsumerRecord<Integer, String> record : records) {\n    System.out.println("Received message: (" + record.key() + ", " + record.value() + ") at offset " + record.offset());\n\n}\n\n\n以上代码来自于源代码包中的例子，我们可以看到KafkaConsumer先订阅topic，然后通过poll方法进行消息拉取。\n\n可以看到KafkaConsumer通过poll方法进行消费，这也是KafkaConsumer最主要的方法。\n\n我们先看看KafkaConsumer内部的其他组件有哪些，见下图：\n\n\n\n上图介绍了KafkaConsumer内部的几个重要组件：\n\n 1. 前文说过消费者要自己记录消费的位置（但也需要提交到服务端保存，为了rebalance后的消费能衔接上），所以我们需要SubScriptionState来保存消费的状态。\n 2. ConsumerCoordinator 负责和 GroupCoordinator 通讯，例如在 leader 选举，入组，分区分配等过程。\n 3. ConsumerNetworkClient 是对 NetworkClient 的封装，如果你是从 producer 看过来的，想必对 NetworkClient 十分了解，他对nio的组件进行封装，实现网络 IO。\n 4. PartitionAssignor，这是分区分配策略，在进行分区分配的时候会用到。\n 5. Fetcher 负责组织拉取消息的请求，以及处理返回。不过需要注意它并不做网络IO，网络IO还是由ConsumerNetworkClient完成。它其实对应生产者中的Sender。\n\n我们抛开订阅、rebalance这些流程，先以kafka消费流程为主，进行分析。有些组件在消费流程中是涉及不到的。消费流程主要涉及到Fetcher、SubScriptionState和ConsumerNetworkClient。特别是 Fetcher，承担了重要的工作。不过我们还需要一步步来，先进入poll方法的分析。\n\n\n# poll()方法\n\n这是消息拉取的入口方法，他会从上次消费的位置拉取消息，也可以手动指定消费位置。入参是阻塞的时长，如果有消息将会立即返回，否则会阻塞到超时，如果没有数据则返回空的数据集合。\n\n代码如下：\n\nprivate ConsumerRecords<K, V> poll(final Timer timer, final boolean includeMetadataInTimeout) {\n    acquireAndEnsureOpen();\n    try {\n        if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) {\n            throw new IllegalStateException("Consumer is not subscribed to any topics or assigned any partitions");\n        }\n \n        // poll for new data until the timeout expires\n        do {\n            client.maybeTriggerWakeup();\n \n            if (includeMetadataInTimeout) {\n                if (!updateAssignmentMetadataIfNeeded(timer)) {\n                    return ConsumerRecords.empty();\n                }\n            } else {\n                while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {\n                    log.warn("Still waiting for metadata");\n                }\n            }\n \n            final Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);\n            if (!records.isEmpty()) {\n                // before returning the fetched records, we can send off the next round of fetches\n                // and avoid block waiting for their responses to enable pipelining while the user\n                // is handling the fetched records.\n                //\n                // NOTE: since the consumed position has already been updated, we must not allow\n                // wakeups or any other errors to be triggered prior to returning the fetched records.\n                if (fetcher.sendFetches() > 0 || client.hasPendingRequests()) {\n                    client.pollNoWakeup();\n                }\n \n                return this.interceptors.onConsume(new ConsumerRecords<>(records));\n            }\n        } while (timer.notExpired());\n \n        return ConsumerRecords.empty();\n    } finally {\n        release();\n    }\n}\n\n\n逻辑说明：\n\n 1. 通过 acquireAndEnsureOpen() 确保本对象是单线程进入，这是因为 KafkaConsumer 非线程安全。\n 2. 检查是否订阅了 topic\n 3. 进入主循环，条件是没有超时\n 4. 在主循环中通过 pollForFetches() 拉取一次消息。这个方法中先检查是否经存在拉取过的未加工消息，这是因为上一轮次拉取做了提前拉取处理。有可能已经拉取回消息等待处理。如果没有已拉取未加工数据，则准备新的拉取请求，网络 IO 拉取消息，加工拉取回来的数据。\n 5. 如果上一步拉取到消息，并不会立即返回，而是再一次触发消息拉取，并且使用的是非阻塞方式，调用 client.pollNoWakeup()。这样做的目的是，提前网络 IO，把消息拉取请求发出去。在网络 IO 的同时，消息数据返回给 consumer 的调用者进行业务处理。这样做到了并行处理，提高了效率。等下次调用 KafkaConsumer 进行 poll，当进行到第4步时，有可能直接返回了上轮次提前拉取到的消息，从而省去了网络 IO 时间。\n\n我们通过下图帮助理解上面4、5步的设计：\n\n\n\n图中带颜色的方框代表在整个拉取消息的流程中，不同的处理过程，分布于不同的对象中。图中下半部分展示的是Kafka处理逻辑。可以看到在第一轮次调用了两次ConusmerNetworkClient进行IO处理，第二次IO的同时，调用者已经开始拿到返回的消息进行业务处理，这里实现了并行处理。进入第二轮次，我们发现 kafkaConsumer可以直接取到上轮第二次IO回来的消息进行加工，加工后返回调用者，进行业务处理，同时下一轮次的消息拉取异步进行中。可以看到第二轮次的总时长已经没有了网络IO的时长，因为这部分工作在上一轮次已经异步进行完成。\n\n如果不这样做，会怎么样呢？我们看图中上半部分，我们发现每个轮次都是一样的，网络IO都需要同步等待，从第二轮开始，整个消息拉取处理的时长明显增加了IO部分，会更长。\n\n以上情况比较极端，每次提前IO都会返回数据，并且消息的业务处理时长大于网络IO。这种情况下，能最大发挥出异步IO的优势。\n\n以上这种设计的小细节真的值得我们来学习。读源代码在了解原理的同时，我们也要多总结优秀的设计思想，对我们的工作很有帮助。\n\n从上面的分析看到，真正消息拉取的代码是：\n\nfinal Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);\n\n\n下面我们继续分析pollForFetches方法\n\n\n# pollForFetches()方法\n\n这个方法完成了从服务端拉取消息的动作，这个过程主要使用了Fetcher和ConsumerNetworkClient两个组件。\n\n 1. Fetcher负责准备好拉取消息的request、处理response、并且把消息转化为对调用者友好的格式\n 2. ConsumerNetworkClient负责把请求发送出去，接收返回，也就是网络IO工作\n\n它的主要流程是如下四步：\n\n 1. 查看是否已经存在拉取回来未加工的消息原始数据，有的话立即调用fetcher.fetchedRecords()加工，然后返回。\n\n 2. 如果没有未加工的原始数据，那么调用fetcher.sendFetches()准备拉取请求。\n\n 3. 通过ConsumerNetworkClient发送拉取请求。\n\n 4. 加工拉取回的原始数据，返回。\n\n其实正常来说2，3，4步流程就足够了。为什么会有第1步呢？那些已经存在的未加工的数据哪里来的？如果你理解了前面所讲的异步拉取设计，那么你应该知道答案。这些已经存在的未加工数据来自于上一轮次的异步IO。正是因为有了异步的IO拉取，才会有第一步的处理可能。\n\n完整代码如下：\n\nprivate Map<TopicPartition, List<ConsumerRecord<K, V>>> pollForFetches(Timer timer) {\n    long pollTimeout = Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs());\n \n    // if data is available already, return it immediately\n    final Map<TopicPartition, List<ConsumerRecord<K, V>>> records = fetcher.fetchedRecords();\n    if (!records.isEmpty()) {\n        return records;\n    }\n \n    // send any new fetches (won\'t resend pending fetches)\n    fetcher.sendFetches();\n \n    // We do not want to be stuck blocking in poll if we are missing some positions\n    // since the offset lookup may be backing off after a failure\n \n    // NOTE: the use of cachedSubscriptionHashAllFetchPositions means we MUST call\n    // updateAssignmentMetadataIfNeeded before this method.\n    if (!cachedSubscriptionHashAllFetchPositions && pollTimeout > retryBackoffMs) {\n        pollTimeout = retryBackoffMs;\n    }\n \n    Timer pollTimer = time.timer(pollTimeout);\n    client.poll(pollTimer, () -> {\n        // since a fetch might be completed by the background thread, we need this poll condition\n        // to ensure that we do not block unnecessarily in poll()\n        return !fetcher.hasCompletedFetches();\n    });\n    timer.update(pollTimer.currentTimeMs());\n \n    // after the long poll, we should check whether the group needs to rebalance\n    // prior to returning data so that the group can stabilize faster\n    if (coordinator.rejoinNeededOrPending()) {\n        return Collections.emptyMap();\n    }\n \n    return fetcher.fetchedRecords();\n}\n\n\n可以看到以上过程除了IO操作外，都是通过Fetcher完成的，足以体现他的重要。接下来的章节将会重点分析Fetcher。\n\n\n# 总结\n\n本篇先是回顾了Kafka消费者的设计，然后从KafkaConsumer的Poll方法入手对拉取的逻辑进行分析。Kafka很巧妙的采用异步IO方式，缩短整个流程的时长。接下来我们将会进入Fetcher的分析，看其如何准备拉取消息的请求，并完成消息的转化处理。\n\n\n# 参考资料\n\n * 你绝对能看懂的Kafka源代码分析-KafkaConsumer类代码分析_consumer is not subscribed to any topics or assign-CSDN博客',normalizedContent:'# 前言\n\n前文我们分析了 kafka 生产者端的源代码，了解了生产者产生消息的过程。消息由生产者发布到某个主题的某个分区上，其实最终是被存储在服务端的某个broker上。而消费者由订阅行为来决定它所要消费的主题和分区。消费者通过poll操作，不断的从服务端拉取该主题分区上产生的消息。\n\n相信有兴趣看kafka源代码的同学，肯定对kafka的基本概念和原理有所了解。关于消费者，我们知道在服务端会有groupcoordinator（组协调器），负责每个consumer group的leader的选举，以及分发分区分配结果，而coumer的leader则会根据分区分配策略进行分区分配。这里需要注意，分区分配结果并不是由leader分发给同组的consumer，而是leader返回给groupcoordinator，再有groupcoordinator进行分发。\n\n每当broker有变化，或者consumer group有出入组的变化时，会触发consumergroup的rebalance。也就是上述的分区分配工作。\n\n另外消费者本地保存了它所负责主题分区的消费状态，通过手动和自动的方式提交到服务端的内部主题中。rebalance过后，消费者重新从内部主题获取对应主题分区的消费位置。\n\n上面我们回顾了consumer的设计和流程，为我们进入源代码分析做好铺垫。接下来我们将从kafkaconsumer入手，进行代码分析。\n\n我们先看下使用kafkaconsumer进行消费的部分代码：\n\nprivate final kafkaconsumer<integer, string> consumer;\n \n.........\n \nconsumer.subscribe(collections.singletonlist(this.topic));\n \nconsumerrecords<integer, string> records = consumer.poll(duration.ofseconds(1));\n \nfor (consumerrecord<integer, string> record : records) {\n    system.out.println("received message: (" + record.key() + ", " + record.value() + ") at offset " + record.offset());\n\n}\n\n\n以上代码来自于源代码包中的例子，我们可以看到kafkaconsumer先订阅topic，然后通过poll方法进行消息拉取。\n\n可以看到kafkaconsumer通过poll方法进行消费，这也是kafkaconsumer最主要的方法。\n\n我们先看看kafkaconsumer内部的其他组件有哪些，见下图：\n\n\n\n上图介绍了kafkaconsumer内部的几个重要组件：\n\n 1. 前文说过消费者要自己记录消费的位置（但也需要提交到服务端保存，为了rebalance后的消费能衔接上），所以我们需要subscriptionstate来保存消费的状态。\n 2. consumercoordinator 负责和 groupcoordinator 通讯，例如在 leader 选举，入组，分区分配等过程。\n 3. consumernetworkclient 是对 networkclient 的封装，如果你是从 producer 看过来的，想必对 networkclient 十分了解，他对nio的组件进行封装，实现网络 io。\n 4. partitionassignor，这是分区分配策略，在进行分区分配的时候会用到。\n 5. fetcher 负责组织拉取消息的请求，以及处理返回。不过需要注意它并不做网络io，网络io还是由consumernetworkclient完成。它其实对应生产者中的sender。\n\n我们抛开订阅、rebalance这些流程，先以kafka消费流程为主，进行分析。有些组件在消费流程中是涉及不到的。消费流程主要涉及到fetcher、subscriptionstate和consumernetworkclient。特别是 fetcher，承担了重要的工作。不过我们还需要一步步来，先进入poll方法的分析。\n\n\n# poll()方法\n\n这是消息拉取的入口方法，他会从上次消费的位置拉取消息，也可以手动指定消费位置。入参是阻塞的时长，如果有消息将会立即返回，否则会阻塞到超时，如果没有数据则返回空的数据集合。\n\n代码如下：\n\nprivate consumerrecords<k, v> poll(final timer timer, final boolean includemetadataintimeout) {\n    acquireandensureopen();\n    try {\n        if (this.subscriptions.hasnosubscriptionoruserassignment()) {\n            throw new illegalstateexception("consumer is not subscribed to any topics or assigned any partitions");\n        }\n \n        // poll for new data until the timeout expires\n        do {\n            client.maybetriggerwakeup();\n \n            if (includemetadataintimeout) {\n                if (!updateassignmentmetadataifneeded(timer)) {\n                    return consumerrecords.empty();\n                }\n            } else {\n                while (!updateassignmentmetadataifneeded(time.timer(long.max_value))) {\n                    log.warn("still waiting for metadata");\n                }\n            }\n \n            final map<topicpartition, list<consumerrecord<k, v>>> records = pollforfetches(timer);\n            if (!records.isempty()) {\n                // before returning the fetched records, we can send off the next round of fetches\n                // and avoid block waiting for their responses to enable pipelining while the user\n                // is handling the fetched records.\n                //\n                // note: since the consumed position has already been updated, we must not allow\n                // wakeups or any other errors to be triggered prior to returning the fetched records.\n                if (fetcher.sendfetches() > 0 || client.haspendingrequests()) {\n                    client.pollnowakeup();\n                }\n \n                return this.interceptors.onconsume(new consumerrecords<>(records));\n            }\n        } while (timer.notexpired());\n \n        return consumerrecords.empty();\n    } finally {\n        release();\n    }\n}\n\n\n逻辑说明：\n\n 1. 通过 acquireandensureopen() 确保本对象是单线程进入，这是因为 kafkaconsumer 非线程安全。\n 2. 检查是否订阅了 topic\n 3. 进入主循环，条件是没有超时\n 4. 在主循环中通过 pollforfetches() 拉取一次消息。这个方法中先检查是否经存在拉取过的未加工消息，这是因为上一轮次拉取做了提前拉取处理。有可能已经拉取回消息等待处理。如果没有已拉取未加工数据，则准备新的拉取请求，网络 io 拉取消息，加工拉取回来的数据。\n 5. 如果上一步拉取到消息，并不会立即返回，而是再一次触发消息拉取，并且使用的是非阻塞方式，调用 client.pollnowakeup()。这样做的目的是，提前网络 io，把消息拉取请求发出去。在网络 io 的同时，消息数据返回给 consumer 的调用者进行业务处理。这样做到了并行处理，提高了效率。等下次调用 kafkaconsumer 进行 poll，当进行到第4步时，有可能直接返回了上轮次提前拉取到的消息，从而省去了网络 io 时间。\n\n我们通过下图帮助理解上面4、5步的设计：\n\n\n\n图中带颜色的方框代表在整个拉取消息的流程中，不同的处理过程，分布于不同的对象中。图中下半部分展示的是kafka处理逻辑。可以看到在第一轮次调用了两次conusmernetworkclient进行io处理，第二次io的同时，调用者已经开始拿到返回的消息进行业务处理，这里实现了并行处理。进入第二轮次，我们发现 kafkaconsumer可以直接取到上轮第二次io回来的消息进行加工，加工后返回调用者，进行业务处理，同时下一轮次的消息拉取异步进行中。可以看到第二轮次的总时长已经没有了网络io的时长，因为这部分工作在上一轮次已经异步进行完成。\n\n如果不这样做，会怎么样呢？我们看图中上半部分，我们发现每个轮次都是一样的，网络io都需要同步等待，从第二轮开始，整个消息拉取处理的时长明显增加了io部分，会更长。\n\n以上情况比较极端，每次提前io都会返回数据，并且消息的业务处理时长大于网络io。这种情况下，能最大发挥出异步io的优势。\n\n以上这种设计的小细节真的值得我们来学习。读源代码在了解原理的同时，我们也要多总结优秀的设计思想，对我们的工作很有帮助。\n\n从上面的分析看到，真正消息拉取的代码是：\n\nfinal map<topicpartition, list<consumerrecord<k, v>>> records = pollforfetches(timer);\n\n\n下面我们继续分析pollforfetches方法\n\n\n# pollforfetches()方法\n\n这个方法完成了从服务端拉取消息的动作，这个过程主要使用了fetcher和consumernetworkclient两个组件。\n\n 1. fetcher负责准备好拉取消息的request、处理response、并且把消息转化为对调用者友好的格式\n 2. consumernetworkclient负责把请求发送出去，接收返回，也就是网络io工作\n\n它的主要流程是如下四步：\n\n 1. 查看是否已经存在拉取回来未加工的消息原始数据，有的话立即调用fetcher.fetchedrecords()加工，然后返回。\n\n 2. 如果没有未加工的原始数据，那么调用fetcher.sendfetches()准备拉取请求。\n\n 3. 通过consumernetworkclient发送拉取请求。\n\n 4. 加工拉取回的原始数据，返回。\n\n其实正常来说2，3，4步流程就足够了。为什么会有第1步呢？那些已经存在的未加工的数据哪里来的？如果你理解了前面所讲的异步拉取设计，那么你应该知道答案。这些已经存在的未加工数据来自于上一轮次的异步io。正是因为有了异步的io拉取，才会有第一步的处理可能。\n\n完整代码如下：\n\nprivate map<topicpartition, list<consumerrecord<k, v>>> pollforfetches(timer timer) {\n    long polltimeout = math.min(coordinator.timetonextpoll(timer.currenttimems()), timer.remainingms());\n \n    // if data is available already, return it immediately\n    final map<topicpartition, list<consumerrecord<k, v>>> records = fetcher.fetchedrecords();\n    if (!records.isempty()) {\n        return records;\n    }\n \n    // send any new fetches (won\'t resend pending fetches)\n    fetcher.sendfetches();\n \n    // we do not want to be stuck blocking in poll if we are missing some positions\n    // since the offset lookup may be backing off after a failure\n \n    // note: the use of cachedsubscriptionhashallfetchpositions means we must call\n    // updateassignmentmetadataifneeded before this method.\n    if (!cachedsubscriptionhashallfetchpositions && polltimeout > retrybackoffms) {\n        polltimeout = retrybackoffms;\n    }\n \n    timer polltimer = time.timer(polltimeout);\n    client.poll(polltimer, () -> {\n        // since a fetch might be completed by the background thread, we need this poll condition\n        // to ensure that we do not block unnecessarily in poll()\n        return !fetcher.hascompletedfetches();\n    });\n    timer.update(polltimer.currenttimems());\n \n    // after the long poll, we should check whether the group needs to rebalance\n    // prior to returning data so that the group can stabilize faster\n    if (coordinator.rejoinneededorpending()) {\n        return collections.emptymap();\n    }\n \n    return fetcher.fetchedrecords();\n}\n\n\n可以看到以上过程除了io操作外，都是通过fetcher完成的，足以体现他的重要。接下来的章节将会重点分析fetcher。\n\n\n# 总结\n\n本篇先是回顾了kafka消费者的设计，然后从kafkaconsumer的poll方法入手对拉取的逻辑进行分析。kafka很巧妙的采用异步io方式，缩短整个流程的时长。接下来我们将会进入fetcher的分析，看其如何准备拉取消息的请求，并完成消息的转化处理。\n\n\n# 参考资料\n\n * 你绝对能看懂的kafka源代码分析-kafkaconsumer类代码分析_consumer is not subscribed to any topics or assign-csdn博客',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"控制器",frontmatter:{title:"控制器",date:"2024-09-18T17:40:39.000Z",permalink:"/pages/d9df7d/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/700.%E3%80%8CBroker%E3%80%8D%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/05.%E6%8E%A7%E5%88%B6%E5%99%A8.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/05.控制器.md",key:"v-a28c2f0e",path:"/pages/d9df7d/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"控制器选举",slug:"控制器选举",normalizedTitle:"控制器选举",charIndex:264},{level:2,title:"控制器初始化",slug:"控制器初始化",normalizedTitle:"控制器初始化",charIndex:1296},{level:2,title:"故障转移",slug:"故障转移",normalizedTitle:"故障转移",charIndex:1991},{level:3,title:"代理上下线",slug:"代理上下线",normalizedTitle:"代理上下线",charIndex:3587},{level:4,title:"代理上线：",slug:"代理上线",normalizedTitle:"代理上线：",charIndex:3646},{level:4,title:"代理下线：",slug:"代理下线",normalizedTitle:"代理下线：",charIndex:4031},{level:3,title:"主题管理",slug:"主题管理",normalizedTitle:"主题管理",charIndex:2562},{level:3,title:"分区管理",slug:"分区管理",normalizedTitle:"分区管理",charIndex:2209},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1907},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:5444}],headersStr:"前言 控制器选举 控制器初始化 故障转移 代理上下线 代理上线： 代理下线： 主题管理 分区管理 总结 参考资料",content:"# 前言\n\nKafka主要的组件如下：\n\n 1. 控制器\n 2. 协调器\n 3. 日志管理器\n 4. 副本管理器\n\n我们已经知道 Kafka 的集群由n个的broker所组成，每个broker就是一个kafka的实例或者称之为kafka的服务。其实控制器也是一个broker，控制器也叫 leader broker。\n\n他除了具有一般 broker 的功能外，还负责分区 leader 的选取，也就是负责选举 partition 的 leader replica。\n\n控制器是kafka核心中的核心，需要重点学习和理解\n\n\n# 控制器选举\n\nkafka 每个 broker 启动的时候，都会实例化一个 KafkaController，并将 broker 的 id 注册到 zookeeper，这在第二章中已经通过例子做过讲解。集群在启动过程中，通过选举机制选举出其中一个 broker 作为 leader，也就是前面所说的控制器。\n\n包括集群启动在内，有三种情况触发控制器选举：\n\n 1. 集群启动\n 2. 控制器所在代理发生故障\n 3. zookeeper心跳感知，控制器与自己的session过期\n\n按照惯例，先看图。我们根据下图来讲解集群启动时，控制器选举过程。\n\n\n\n假设此集群有三个broker，同时启动。\n\n 1. 3个broker从zookeeper获取/controller临时节点信息。/controller存储的是选举出来的leader信息。此举是为了确认是否已经存在leader。\n 2. 如果还没有选举出leader，那么此节点是不存在的，返回-1。如果返回的不是-1，而是leader的json数据，那么说明已经有leader存在，选举结束。\n 3. 三个broker发现返回-1，了解到目前没有leader，于是均会触发向临时节点/controller写入自己的信息。最先写入的就会成为leader。\n 4. 假设broker 0的速度最快，他先写入了/controller节点，那么他就成为了leader。而broker1、broker2很不幸，因为晚了一步，他们在写/controller的过程中会抛出ZkNodeExistsException，也就是zk告诉他们，此节点已经存在了。\n\n经过以上四步，broker 0成功写入/controller节点，其它broker写入失败了，所以broker 0成功当选leader。\n\n此外zk中还有controller_epoch节点，存储了leader的变更次数，初始值为0，以后leader每变一次，该值+1。所有向控制器发起的请求，都会携带此值。如果控制器和自己内存中比较，请求值小，说明kafka集群已经发生了新的选举，此请求过期，此请求无效。如果请求值大于控制器内存的值，说明已经有新的控制器当选了，自己已经退位，请求无效。kafka通过controller_epoch保证集群控制器的唯一性及操作的一致性。\n\n由此可见，Kafka控制器选举就是看谁先争抢到/controller节点写入自身信息\n\n\n# 控制器初始化\n\n控制器的初始化，其实是初始化控制器所用到的组件及监听器，准备元数据。\n\n前面提到过每个 broker 都会实例化并启动一个 KafkaController。KafkaController 和他的组件关系，以及各个组件的介绍如下图：\n\n\n\n图中箭头为组件层级关系，组件下面还会再初始化其他组件。可见控制器内部还是有些复杂的，主要有以下组件：\n\n 1. ControllerContext，此对象存储了控制器工作需要的所有上下文信息，包括存活的代理、所有主题及分区分配方案、每个分区的AR、leader、ISR等信息。\n 2. 一系列的listener，通过对zookeeper的监听，触发相应的操作，黄色的框的均为listener\n 3. 分区和副本状态机，管理分区和副本。\n 4. 当前代理选举器ZookeeperLeaderElector，此选举器有上位和退位的相关回调方法。\n 5. 分区leader选举器，PartitionLeaderSelector\n 6. 主题删除管理器，TopicDeletetionManager\n 7. leader向broker批量通信的ControllerBrokerRequestBatch。缓存状态机处理后产生的request，然后统一发送出去。\n 8. 控制器平衡操作的KafkaScheduler，仅在broker作为leader时有效。\n\n图片是我根据资料所总结，个人认为对于理解kafkaController的全貌很有帮助。本章节后面讲到相应组件和流程时，还需要反复回来理解此图，思考组件所处的位置，对整体的作用。\n\n\n# 故障转移\n\n故障转移其实就是leader所在broker发生故障，leader转移为其他的broker。转移的过程就是重新选举leader的过程。\n\n重新选举leader后，需要为该broker注册相应权限，调用的是ZookeeperLeaderElector的onControllerFailover()方法。在这个方法中初始化和启动了一系列的组件来完成leader的各种操作。具体如下，其实和控制器初始化有很大的相似度。\n\n1、注册分区管理的相关监听器\n\n监听名称                               监听ZOOKEEPER节点                       作用\nPartitionsReassignedListener       /admin/reassign_partitions          节点变化将会引发分区重分配\nIsrChangeNotificationListener      /isr_change_notification            处理分区的ISR发生变化引发的操作\nPreferredReplicaElectionListener   /admin/preferred_replica_election   将优先副本选举为leader副本\n\n2、注册主题管理的相关监听\n\n监听名称                   监听ZOOKEEPER节点          作用\nTopicChangeListener    /brokers/topics        监听主题发生变化时进行相应操作\nDeleteTopicsListener   /admin/delete_topics   完成服务器端删除主题的相应操作。否则客户端删除主题仅仅是表示删除\n\n3、注册代理变化监听器\n\n监听名称                   监听ZOOKEEPER节点   作用\nBrokerChangeListener   /brokers/ids    代理发生增减的时候进行相应的处理\n\n4、重新初始化ControllerContext，\n\n5、启动控制器和其他代理之间通信的ControllerChannelManager\n\n6、创建用于删除主题的TopicDeletionManager对象,并启动。\n\n7、启动分区状态机和副本状态机\n\n8、轮询每个主题，添加监听分区变化的PartitionModificationsListener\n\n9、如果设置了分区平衡定时操作，那么创建分区平衡的定时任务，默认300秒检查并执行。\n\n除了这些组件的启动外，onControllerFailover方法中还做了如下操作：\n\n1、/controller_epoch值+1，并且更新到ControllerContext\n\n2、检查是否出发分区重分配，并做相关操作\n\n3、检查需要将优先副本选为leader，并做相关操作\n\n4、向kafka集群所有代理发送更新元数据的请求。\n\n下面来看leader权限被取消时，调用的方法onControllerResignation\n\n1、该方法中注销了控制器的权限。取消在zookeeper中对于分区、副本感知的相应监听器的监听。\n\n2、关闭启动的各个组件\n\n3、最后把ControllerContext中记录控制器版本的数值清零，并设置当前broker为RunnignAsBroker，变为普通的broker。\n\n通过对控制器启动过程的学习，我们应该已经对kafka工作的原理有了了解，核心是监听zookeeper的相关节点，节点变化时触发相应的操作。其它的处理流程都是相类似的。本篇教程接下来做简要介绍，想要了解详情的，可以先找其它资料。我后续也会再补充更为详细的教程。\n\n\n# 代理上下线\n\n有新的broker加入集群时，称为代理上线。反之，当broker关闭，推出集群时，称为代理下线。\n\n# 代理上线：\n\n1、新代理启动时向/brokers/ids写数据\n\n2、BrokerChangeListener监听到变化。对新上线节点调用controllerChannelManager.addBroker()，完成新上线代理网络层初始化\n\n3、调用KafkaController.onBrokerStartup()处理\n\n3.1通过向所有代理发送UpdateMetadataRequest，告诉所有代理有新代理加入\n\n3.2根据分配给新上线节点的副本集合，对副本状态做变迁。对分区也进行处理。\n\n3.3触发一次leader选举，确认新加入的是否为分区leader\n\n3.4轮询分配给新broker的副本，调用KafkaController.onPartitionReassignment()，执行分区副本分配\n\n3.5 恢复因新代理上线暂停的删除主题操作线程\n\n\n# 代理下线：\n\n1、查找下线节点集合\n\n2、轮询下线节点，调用controllerChannelManager.removeBroker()，关闭每个下线节点网络连接。清空下线节点消息队列，关闭下线节点request请求\n\n3、轮询下线节点，调用KafkaController.onBrokerFailure处理\n\n3.1处理leader副本在下线节点上上的分区，重新选出leader副本，发送updateMetadataRequest请求。\n\n3.2处理下线节点上的副本集合，做下线处理，从ISR集合中删除，不再同步，发送updateMetadataRequest请求。\n\n\n4、向集群全部存活代理发送updateMetadataRequest请求\n\n\n# 主题管理\n\n通过分区状态机及副本状态机来进行主题管理\n\n1、创建主题\n\n/brokers/topics下创建主题对应子节点\n\nTopicChangeListener监听此节点\n\n变化时获取重入锁ReentrantLock,调用handleChildChange方法进行处理。\n\n通过对比zookeeper中/brokers/topics存储的主题集合及控制器的ControllerContext中缓存的主题集合的差集，得到新增的主题。反过来求差集，得到删除的主题。\n\n接下来遍历新增的主题集合，进行主题操作的实质性操作。之前仅仅是在zookeeper中添加了主题。新增主题涉及的操作有分区、副本状态的转化、分区leader的分配、分区存储日志的创建等。\n\n2、删除主题\n\n/admin/delete_topics创建删除主题的子节点\n\nDeleteTopicsListener监听此节点，\n\n变化时获取重入锁ReentrantLock,进行处理\n\n具体的删除逻辑再次就不再详述。\n\n\n# 分区管理\n\n1、分区自动平衡\n\nonControllerFailover方法中启动分区自动平衡任务。定时检查是否失去平衡。\n\n自动平衡的操作就是把优先副本选为分区leader，AR中第一个副本为优先副本。\n\n先查出所有可用副本，以分区AR头节点分组。\n\n轮询代理节点，判断分区不平衡率是否超过10%(leader为非优先副本的分区/该代理分区总数)，则调用onPreferredReplicaElection()，让优先副本成为leader。达到自动平衡。\n\n分区平衡操作的流程已经在第三章做了很详细的讲解，此处不再重复，可以参考kafka核心概念。\n\n2、分区重分配\n\n当zk节点/admin/reassign_partitions变化时，触发分区重分配操作。该节点存储分区重分配的方案。\n\n通过计算主题分区原AR（OAR）和重新分配后的AR（RAR），分别做相应处理：\n\n1、OAR+RAR：更新到该主题分区AR，并通知副本节点同步。leader_epoch+1\n\n2、RAR-OAR：副本设为NewReplica。\n\n3、（OAR+RAR）- RAR：需要下线的副本，做下线操作\n\n具体流程不再详述\n\n\n# 总结\n\n关于控制器的相关知识点就先讲到这里，控制器初始化中的那张图需要充分去理解，理解了此图，对控制器内部的构造，以及控制器要做什么事情、如何做的，就已经掌握了。另外考虑本教程定位为入门轻松学，所以具体的流程没有展开来讲，以后我会再写相应的主题文章来说明。\n\n\n# 参考资料\n\nApache Kafka 核心组件和流程-控制器 - 爱码叔-iCodeBook",normalizedContent:"# 前言\n\nkafka主要的组件如下：\n\n 1. 控制器\n 2. 协调器\n 3. 日志管理器\n 4. 副本管理器\n\n我们已经知道 kafka 的集群由n个的broker所组成，每个broker就是一个kafka的实例或者称之为kafka的服务。其实控制器也是一个broker，控制器也叫 leader broker。\n\n他除了具有一般 broker 的功能外，还负责分区 leader 的选取，也就是负责选举 partition 的 leader replica。\n\n控制器是kafka核心中的核心，需要重点学习和理解\n\n\n# 控制器选举\n\nkafka 每个 broker 启动的时候，都会实例化一个 kafkacontroller，并将 broker 的 id 注册到 zookeeper，这在第二章中已经通过例子做过讲解。集群在启动过程中，通过选举机制选举出其中一个 broker 作为 leader，也就是前面所说的控制器。\n\n包括集群启动在内，有三种情况触发控制器选举：\n\n 1. 集群启动\n 2. 控制器所在代理发生故障\n 3. zookeeper心跳感知，控制器与自己的session过期\n\n按照惯例，先看图。我们根据下图来讲解集群启动时，控制器选举过程。\n\n\n\n假设此集群有三个broker，同时启动。\n\n 1. 3个broker从zookeeper获取/controller临时节点信息。/controller存储的是选举出来的leader信息。此举是为了确认是否已经存在leader。\n 2. 如果还没有选举出leader，那么此节点是不存在的，返回-1。如果返回的不是-1，而是leader的json数据，那么说明已经有leader存在，选举结束。\n 3. 三个broker发现返回-1，了解到目前没有leader，于是均会触发向临时节点/controller写入自己的信息。最先写入的就会成为leader。\n 4. 假设broker 0的速度最快，他先写入了/controller节点，那么他就成为了leader。而broker1、broker2很不幸，因为晚了一步，他们在写/controller的过程中会抛出zknodeexistsexception，也就是zk告诉他们，此节点已经存在了。\n\n经过以上四步，broker 0成功写入/controller节点，其它broker写入失败了，所以broker 0成功当选leader。\n\n此外zk中还有controller_epoch节点，存储了leader的变更次数，初始值为0，以后leader每变一次，该值+1。所有向控制器发起的请求，都会携带此值。如果控制器和自己内存中比较，请求值小，说明kafka集群已经发生了新的选举，此请求过期，此请求无效。如果请求值大于控制器内存的值，说明已经有新的控制器当选了，自己已经退位，请求无效。kafka通过controller_epoch保证集群控制器的唯一性及操作的一致性。\n\n由此可见，kafka控制器选举就是看谁先争抢到/controller节点写入自身信息\n\n\n# 控制器初始化\n\n控制器的初始化，其实是初始化控制器所用到的组件及监听器，准备元数据。\n\n前面提到过每个 broker 都会实例化并启动一个 kafkacontroller。kafkacontroller 和他的组件关系，以及各个组件的介绍如下图：\n\n\n\n图中箭头为组件层级关系，组件下面还会再初始化其他组件。可见控制器内部还是有些复杂的，主要有以下组件：\n\n 1. controllercontext，此对象存储了控制器工作需要的所有上下文信息，包括存活的代理、所有主题及分区分配方案、每个分区的ar、leader、isr等信息。\n 2. 一系列的listener，通过对zookeeper的监听，触发相应的操作，黄色的框的均为listener\n 3. 分区和副本状态机，管理分区和副本。\n 4. 当前代理选举器zookeeperleaderelector，此选举器有上位和退位的相关回调方法。\n 5. 分区leader选举器，partitionleaderselector\n 6. 主题删除管理器，topicdeletetionmanager\n 7. leader向broker批量通信的controllerbrokerrequestbatch。缓存状态机处理后产生的request，然后统一发送出去。\n 8. 控制器平衡操作的kafkascheduler，仅在broker作为leader时有效。\n\n图片是我根据资料所总结，个人认为对于理解kafkacontroller的全貌很有帮助。本章节后面讲到相应组件和流程时，还需要反复回来理解此图，思考组件所处的位置，对整体的作用。\n\n\n# 故障转移\n\n故障转移其实就是leader所在broker发生故障，leader转移为其他的broker。转移的过程就是重新选举leader的过程。\n\n重新选举leader后，需要为该broker注册相应权限，调用的是zookeeperleaderelector的oncontrollerfailover()方法。在这个方法中初始化和启动了一系列的组件来完成leader的各种操作。具体如下，其实和控制器初始化有很大的相似度。\n\n1、注册分区管理的相关监听器\n\n监听名称                               监听zookeeper节点                       作用\npartitionsreassignedlistener       /admin/reassign_partitions          节点变化将会引发分区重分配\nisrchangenotificationlistener      /isr_change_notification            处理分区的isr发生变化引发的操作\npreferredreplicaelectionlistener   /admin/preferred_replica_election   将优先副本选举为leader副本\n\n2、注册主题管理的相关监听\n\n监听名称                   监听zookeeper节点          作用\ntopicchangelistener    /brokers/topics        监听主题发生变化时进行相应操作\ndeletetopicslistener   /admin/delete_topics   完成服务器端删除主题的相应操作。否则客户端删除主题仅仅是表示删除\n\n3、注册代理变化监听器\n\n监听名称                   监听zookeeper节点   作用\nbrokerchangelistener   /brokers/ids    代理发生增减的时候进行相应的处理\n\n4、重新初始化controllercontext，\n\n5、启动控制器和其他代理之间通信的controllerchannelmanager\n\n6、创建用于删除主题的topicdeletionmanager对象,并启动。\n\n7、启动分区状态机和副本状态机\n\n8、轮询每个主题，添加监听分区变化的partitionmodificationslistener\n\n9、如果设置了分区平衡定时操作，那么创建分区平衡的定时任务，默认300秒检查并执行。\n\n除了这些组件的启动外，oncontrollerfailover方法中还做了如下操作：\n\n1、/controller_epoch值+1，并且更新到controllercontext\n\n2、检查是否出发分区重分配，并做相关操作\n\n3、检查需要将优先副本选为leader，并做相关操作\n\n4、向kafka集群所有代理发送更新元数据的请求。\n\n下面来看leader权限被取消时，调用的方法oncontrollerresignation\n\n1、该方法中注销了控制器的权限。取消在zookeeper中对于分区、副本感知的相应监听器的监听。\n\n2、关闭启动的各个组件\n\n3、最后把controllercontext中记录控制器版本的数值清零，并设置当前broker为runnignasbroker，变为普通的broker。\n\n通过对控制器启动过程的学习，我们应该已经对kafka工作的原理有了了解，核心是监听zookeeper的相关节点，节点变化时触发相应的操作。其它的处理流程都是相类似的。本篇教程接下来做简要介绍，想要了解详情的，可以先找其它资料。我后续也会再补充更为详细的教程。\n\n\n# 代理上下线\n\n有新的broker加入集群时，称为代理上线。反之，当broker关闭，推出集群时，称为代理下线。\n\n# 代理上线：\n\n1、新代理启动时向/brokers/ids写数据\n\n2、brokerchangelistener监听到变化。对新上线节点调用controllerchannelmanager.addbroker()，完成新上线代理网络层初始化\n\n3、调用kafkacontroller.onbrokerstartup()处理\n\n3.1通过向所有代理发送updatemetadatarequest，告诉所有代理有新代理加入\n\n3.2根据分配给新上线节点的副本集合，对副本状态做变迁。对分区也进行处理。\n\n3.3触发一次leader选举，确认新加入的是否为分区leader\n\n3.4轮询分配给新broker的副本，调用kafkacontroller.onpartitionreassignment()，执行分区副本分配\n\n3.5 恢复因新代理上线暂停的删除主题操作线程\n\n\n# 代理下线：\n\n1、查找下线节点集合\n\n2、轮询下线节点，调用controllerchannelmanager.removebroker()，关闭每个下线节点网络连接。清空下线节点消息队列，关闭下线节点request请求\n\n3、轮询下线节点，调用kafkacontroller.onbrokerfailure处理\n\n3.1处理leader副本在下线节点上上的分区，重新选出leader副本，发送updatemetadatarequest请求。\n\n3.2处理下线节点上的副本集合，做下线处理，从isr集合中删除，不再同步，发送updatemetadatarequest请求。\n\n\n4、向集群全部存活代理发送updatemetadatarequest请求\n\n\n# 主题管理\n\n通过分区状态机及副本状态机来进行主题管理\n\n1、创建主题\n\n/brokers/topics下创建主题对应子节点\n\ntopicchangelistener监听此节点\n\n变化时获取重入锁reentrantlock,调用handlechildchange方法进行处理。\n\n通过对比zookeeper中/brokers/topics存储的主题集合及控制器的controllercontext中缓存的主题集合的差集，得到新增的主题。反过来求差集，得到删除的主题。\n\n接下来遍历新增的主题集合，进行主题操作的实质性操作。之前仅仅是在zookeeper中添加了主题。新增主题涉及的操作有分区、副本状态的转化、分区leader的分配、分区存储日志的创建等。\n\n2、删除主题\n\n/admin/delete_topics创建删除主题的子节点\n\ndeletetopicslistener监听此节点，\n\n变化时获取重入锁reentrantlock,进行处理\n\n具体的删除逻辑再次就不再详述。\n\n\n# 分区管理\n\n1、分区自动平衡\n\noncontrollerfailover方法中启动分区自动平衡任务。定时检查是否失去平衡。\n\n自动平衡的操作就是把优先副本选为分区leader，ar中第一个副本为优先副本。\n\n先查出所有可用副本，以分区ar头节点分组。\n\n轮询代理节点，判断分区不平衡率是否超过10%(leader为非优先副本的分区/该代理分区总数)，则调用onpreferredreplicaelection()，让优先副本成为leader。达到自动平衡。\n\n分区平衡操作的流程已经在第三章做了很详细的讲解，此处不再重复，可以参考kafka核心概念。\n\n2、分区重分配\n\n当zk节点/admin/reassign_partitions变化时，触发分区重分配操作。该节点存储分区重分配的方案。\n\n通过计算主题分区原ar（oar）和重新分配后的ar（rar），分别做相应处理：\n\n1、oar+rar：更新到该主题分区ar，并通知副本节点同步。leader_epoch+1\n\n2、rar-oar：副本设为newreplica。\n\n3、（oar+rar）- rar：需要下线的副本，做下线操作\n\n具体流程不再详述\n\n\n# 总结\n\n关于控制器的相关知识点就先讲到这里，控制器初始化中的那张图需要充分去理解，理解了此图，对控制器内部的构造，以及控制器要做什么事情、如何做的，就已经掌握了。另外考虑本教程定位为入门轻松学，所以具体的流程没有展开来讲，以后我会再写相应的主题文章来说明。\n\n\n# 参考资料\n\napache kafka 核心组件和流程-控制器 - 爱码叔-icodebook",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Untitled",frontmatter:{title:"Untitled",date:"2024-09-18T18:05:57.000Z",permalink:"/pages/b60a98/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/700.%E3%80%8CBroker%E3%80%8D%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/10.%E5%8D%8F%E8%B0%83%E5%99%A8.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/10.协调器.md",key:"v-d302f124",path:"/pages/b60a98/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:163},{level:2,title:"消费者协调器",slug:"消费者协调器",normalizedTitle:"消费者协调器",charIndex:277},{level:2,title:"组协调器",slug:"组协调器",normalizedTitle:"组协调器",charIndex:308},{level:2,title:"消费偏移量管理",slug:"消费偏移量管理",normalizedTitle:"消费偏移量管理",charIndex:1482},{level:3,title:"偏移量有两种提交方式",slug:"偏移量有两种提交方式",normalizedTitle:"偏移量有两种提交方式",charIndex:2266},{level:4,title:"1、自动提交偏移量",slug:"_1、自动提交偏移量",normalizedTitle:"1、自动提交偏移量",charIndex:2280},{level:4,title:"2、手动提交偏移量",slug:"_2、手动提交偏移量",normalizedTitle:"2、手动提交偏移量",charIndex:2503},{level:5,title:"偏移量提交的最佳实践",slug:"偏移量提交的最佳实践",normalizedTitle:"偏移量提交的最佳实践",charIndex:3192},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4069},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:4196}],headersStr:"前言 概述 消费者协调器 组协调器 消费偏移量管理 偏移量有两种提交方式 1、自动提交偏移量 2、手动提交偏移量 偏移量提交的最佳实践 总结 参考资料",content:"# 前言\n\n上一节介绍了kafka工作的核心组件--控制器。本节将介绍消费者密切相关的组件--协调器。它负责消费者的出入组工作。大家可以回想一下kafka核心概念中关于吃苹果的场景，如果我邀请了100个人过来吃苹果，如果没有人告诉每个吃苹果的人哪个是他的盘子，那岂不是要乱了套？协调器做的就是这个工作。当然还有更多。\n\n\n# 概述\n\n顾名思义，协调器负责协调工作。本节所讲的协调器，是用来协调消费者工作分配的。简单点说，就是消费者启动后，到可以正常消费前，这个阶段的初始化工作。消费者能够正常运转起来，全有赖于协调器。\n\n主要的协调器有如下两个：\n\n1、消费者协调器（ConsumerCoordinator）\n\n2、组协调器（GroupCoordinator）\n\n此外还有任务管理协调器（WorkCoordinator），用作kafka connect的works管理，本教程不做讲解。\n\nkafka引入协调器有其历史过程，原来consumer信息依赖于zookeeper存储，当代理或消费者发生变化时，引发消费者平衡，此时消费者之间是互不透明的，每个消费者和zookeeper单独通信，容易造成羊群效应和脑裂问题。\n\n为了解决这些问题，kafka引入了协调器。服务端引入组协调器（GroupCoordinator），消费者端引入消费者协调器（ConsumerCoordinator）。每个broker启动的时候，都会创建GroupCoordinator实例，管理部分消费组（集群负载均衡）和组下每个消费者消费的偏移量（offset）。每个consumer实例化时，同时实例化一个ConsumerCoordinator对象，负责同一个消费组下各个消费者和服务端组协调器之前的通信。如下图：\n\n\n\n\n# 消费者协调器\n\n消费者协调器，可以看作是消费者做操作的代理类（其实并不是），消费者很多操作通过消费者协调器进行处理。\n\n消费者协调器主要负责如下工作：\n\n1、更新消费者缓存的MetaData\n\n2、向组协调器申请加入组\n\n3、消费者加入组后的相应处理\n\n4、请求离开消费组\n\n5、向组协调器提交偏移量\n\n6、通过心跳，保持组协调器的连接感知。\n\n7、被组协调器选为leader的消费者的协调器，负责消费者分区分配。分配结果发送给组协调器。\n\n8、非leader的消费者，通过消费者协调器和组协调器同步分配结果。\n\n消费者协调器主要依赖的组件和说明见下图：\n\n\n\n可以看到这些组件和消费者协调器担负的工作是可以对照上的。\n\n\n# 组协调器\n\n组协调器负责处理消费者协调器发过来的各种请求。它主要提供如下功能：\n\n在与之连接的消费者中选举出消费者leader 下发leader消费者返回的消费者分区分配结果给所有的消费者 管理消费者的消费偏移量提交，保存在kafka的内部主题中 和消费者心跳保持，知道哪些消费者已经死掉，组中存活的消费者是哪些。 组协调器在broker启动的时候实例化，每个组协调器负责一部分消费组的管理。它主要依赖的组件见下图：\n\n\n\n这些组件也是和组协调器的功能能够对应上的。具体内容不在详述。\n\n2.3 消费者入组过程 下图展示了消费者启动选取leader、入组的过程。\n\n\n\n消费者入组的过程，很好的展示了消费者协调器和组协调器之间是如何配合工作的。leader consumer会承担分区分配的工作，这样kafka集群的压力会小很多。同组的consumer通过组协调器保持同步。消费者和分区的对应关系持久化在kafka内部主题。\n\n\n# 消费偏移量管理\n\n消费者消费时，会在本地维护消费到的位置（offset），就是偏移量，这样下次消费才知道从哪里开始消费。如果整个环境没有变化，这样做就足够了。但一旦消费者平衡操作或者分区变化后，消费者不再对应原来的分区，而每个消费者的offset也没有同步到服务器，这样就无法接着前任的工作继续进行了。\n\n因此只有把消费偏移量定期发送到服务器，由GroupCoordinator集中式管理，分区重分配后，各个消费者从GroupCoordinator读取自己对应分区的offset，在新的分区上继续前任的工作。\n\n下图展示了不提交offset到服务端的问题：\n\n\n\n开始时，consumer 0消费partition 0 和1，后来由于新的consumer 2入组，分区重新进行了分配。consumer 0不再消费partition2，而由consumer 2来消费partition 2，但由于consumer之间是不能通讯的，所有consumer2并不知道从哪里开始自己的消费。\n\n因此consumer需要定期提交自己消费的offset到服务端，这样在重分区操作后，每个consumer都能在服务端查到分配给自己的partition所消费到的offset，继续消费。\n\n由于kafka有高可用和横向扩展的特性，当有新的分区出现或者新的消费入组后，需要重新分配消费者对应的分区，所以如果偏移量提交的有问题，会重复消费或者丢消息。偏移量提交的时机和方式要格外注意！！\n\n下面两种情况分别会造成重复消费和丢消息：\n\n如果提交的偏移量小于消费者最后一次消费的偏移量，那么再均衡后，两个offset之间的消息就会被重复消费 如果提交的偏移量大于消费者最后一次消费的偏移量，那么再均衡后，两个offset之间的消息就会丢失 以上两种情况是如何产生的呢？我们继续往下看。\n\n\n# 偏移量有两种提交方式\n\n# 1、自动提交偏移量\n\n设置 enable.auto.commit为true，设定好周期，默认5s。消费者每次调用轮询消息的poll() 方法时，会检查是否超过了5s没有提交偏移量，如果是，提交上一次轮询返回的偏移量。\n\n这样做很方便，但是会带来重复消费的问题。假如最近一次偏移量提交3s后，触发了再均衡，服务器端存储的还是上次提交的偏移量，那么再均衡结束后，新的消费者会从最后一次提交的偏移量开始拉取消息，此3s内消费的消息会被重复消费。\n\n# 2、手动提交偏移量\n\n设置 enable.auto.commit为false。程序中手动调用commitSync()提交偏移量，此时提交的是poll方法返回的最新的偏移量。\n\n我们来看下面两个提交时机：\n\n 1. 如果poll完马上调用commitSync(),那么一旦处理到中间某条消息的时候异常，由于偏移量已经提交，那么出问题的消息位置到提交偏移量之间的消息就会丢失。\n\n\n\n 1. 如果处理完所有消息后才调用commitSync()。有可能在处理到一半的时候发生再均衡，此时偏移量还未提交，那么再均衡后，会从上次提交的位置开始消费，造成重复消费。\n\n\n\n比较起来，重复消费要比丢消息好一些，所以我们程序应采用第二种方式，同时消费逻辑中，要能够检查重复消费。\n\ncommitSync()是同步提交偏移量，主程序会一直阻塞，偏移量提交成功后才往下运行。这样会限制程序的吞吐量。如果降低提交频次，又很容易发生重复消费。\n\n这里我们可以使用commitAsync()异步提交偏移量。只管提交，而不会等待broker返回提交结果\n\ncommitSync只要没有发生不可恢复错误，会进行重试，直到成功。而commitAsync不会进行重试，失败就是失败了。commitAsync不重试，是因为重试提交时，可能已经有其它更大偏移量已经提交成功了，如果此时重试提交成功，那么更小的偏移量会覆盖大的偏移量。那么如果此时发生再均衡，新的消费者将会重复消费消息。\n\ncommitAsync也支持回调，由于上述原因，回调中最好不要因为失败而重试提交。而是应该记录错误，以便后续分析和补偿。\n\n# 偏移量提交的最佳实践\n\n关于偏移量的提交方式和时机，上文已经有了大量的讲解。但看完后好像还不知道应该怎么提交偏移量才是最合适的。是不是觉得无论怎么提交，都无法避免重复消费？没错，事实就是这样，我们只能采用合理的方式，最大可能的去降低发生此类问题的概率。此外做好补偿处理。\n\n一般来说，偶尔的提交失败，不去重试，是没有问题的。因为一般是因为临时的问题而失败，后续的提交总会成功。如果我们在关闭消费者或者再均衡前，确保所有的消费者都能成功提交一次偏移量，也可以保证再均衡后，消费者能接着消费数据。\n\n因此我们采用同步和异步混合的方式提交偏移量。\n\n 1. 正常消费消息时，消费结束提交偏移量，采用异步方式\n 2. 如果程序报错，finally中，提交偏移量，采用同步方式，确保提交成功\n 3. 再均衡前的回调方法中，提交偏移量，采用同步方式，确保提交成功\n\n这样既保证了吞吐量，也保证了提交偏移量的安全性。另外由于再均衡前提交偏移量，降低了重复消费可能。\n\nkafka还提供了提交特定偏移量的方法。我们可以指定分区和offset进行提交。分区和offset的值可以从消息对象中取得。\n\n另外，如果担心一次取回数据量太大，可能处理到一半的时候出现再均衡，导致偏移量没有提交，重复消费。那么可以每n条提交一次。\n\n而当n=1时，也就是处理一条数据就提交一次，会把重复消费的可能降到最低。同时由于增加了和服务端的通讯，效率大大降低。\n\n其实即使这样，也是可能重复消费的，试想如下场景：\n\n消费者拉取到数据后，开始逻辑处理 处理第一条offset=2，成功了，提交offset=3 开始处理offset=3的消息，处理完成后，但提交offset=4前，此消费者突然意外挂掉了，所以也没能进入异常处理。偏移量没能成功提交。 消费者进行了再均衡，新的消费者接手此分区进行消费，取到的offset还是上一次提交的3，那么将会重复消费offset=3的消息。 所以我们应平衡重复消费发生的概率和程序的效率，来设置提交的时机。同时程序逻辑一定做好重复消费的检查工作！\n\n\n# 总结\n\n本节从协调器讲起，首先介绍了消费者协调器和组协调器，以及他们是如何配合工作的。从消费偏移量的管理展开，详细介绍了偏移量的提交，及提交的最佳实践。本节没有涉及代码部分，所有知识点相关的代码将在最后一章中统一给出。现在的要求只是理解知识点。\n\n\n# 参考资料\n\nApache Kafka核心组件和流程-协调器 - 爱码叔-iCodeBook",normalizedContent:"# 前言\n\n上一节介绍了kafka工作的核心组件--控制器。本节将介绍消费者密切相关的组件--协调器。它负责消费者的出入组工作。大家可以回想一下kafka核心概念中关于吃苹果的场景，如果我邀请了100个人过来吃苹果，如果没有人告诉每个吃苹果的人哪个是他的盘子，那岂不是要乱了套？协调器做的就是这个工作。当然还有更多。\n\n\n# 概述\n\n顾名思义，协调器负责协调工作。本节所讲的协调器，是用来协调消费者工作分配的。简单点说，就是消费者启动后，到可以正常消费前，这个阶段的初始化工作。消费者能够正常运转起来，全有赖于协调器。\n\n主要的协调器有如下两个：\n\n1、消费者协调器（consumercoordinator）\n\n2、组协调器（groupcoordinator）\n\n此外还有任务管理协调器（workcoordinator），用作kafka connect的works管理，本教程不做讲解。\n\nkafka引入协调器有其历史过程，原来consumer信息依赖于zookeeper存储，当代理或消费者发生变化时，引发消费者平衡，此时消费者之间是互不透明的，每个消费者和zookeeper单独通信，容易造成羊群效应和脑裂问题。\n\n为了解决这些问题，kafka引入了协调器。服务端引入组协调器（groupcoordinator），消费者端引入消费者协调器（consumercoordinator）。每个broker启动的时候，都会创建groupcoordinator实例，管理部分消费组（集群负载均衡）和组下每个消费者消费的偏移量（offset）。每个consumer实例化时，同时实例化一个consumercoordinator对象，负责同一个消费组下各个消费者和服务端组协调器之前的通信。如下图：\n\n\n\n\n# 消费者协调器\n\n消费者协调器，可以看作是消费者做操作的代理类（其实并不是），消费者很多操作通过消费者协调器进行处理。\n\n消费者协调器主要负责如下工作：\n\n1、更新消费者缓存的metadata\n\n2、向组协调器申请加入组\n\n3、消费者加入组后的相应处理\n\n4、请求离开消费组\n\n5、向组协调器提交偏移量\n\n6、通过心跳，保持组协调器的连接感知。\n\n7、被组协调器选为leader的消费者的协调器，负责消费者分区分配。分配结果发送给组协调器。\n\n8、非leader的消费者，通过消费者协调器和组协调器同步分配结果。\n\n消费者协调器主要依赖的组件和说明见下图：\n\n\n\n可以看到这些组件和消费者协调器担负的工作是可以对照上的。\n\n\n# 组协调器\n\n组协调器负责处理消费者协调器发过来的各种请求。它主要提供如下功能：\n\n在与之连接的消费者中选举出消费者leader 下发leader消费者返回的消费者分区分配结果给所有的消费者 管理消费者的消费偏移量提交，保存在kafka的内部主题中 和消费者心跳保持，知道哪些消费者已经死掉，组中存活的消费者是哪些。 组协调器在broker启动的时候实例化，每个组协调器负责一部分消费组的管理。它主要依赖的组件见下图：\n\n\n\n这些组件也是和组协调器的功能能够对应上的。具体内容不在详述。\n\n2.3 消费者入组过程 下图展示了消费者启动选取leader、入组的过程。\n\n\n\n消费者入组的过程，很好的展示了消费者协调器和组协调器之间是如何配合工作的。leader consumer会承担分区分配的工作，这样kafka集群的压力会小很多。同组的consumer通过组协调器保持同步。消费者和分区的对应关系持久化在kafka内部主题。\n\n\n# 消费偏移量管理\n\n消费者消费时，会在本地维护消费到的位置（offset），就是偏移量，这样下次消费才知道从哪里开始消费。如果整个环境没有变化，这样做就足够了。但一旦消费者平衡操作或者分区变化后，消费者不再对应原来的分区，而每个消费者的offset也没有同步到服务器，这样就无法接着前任的工作继续进行了。\n\n因此只有把消费偏移量定期发送到服务器，由groupcoordinator集中式管理，分区重分配后，各个消费者从groupcoordinator读取自己对应分区的offset，在新的分区上继续前任的工作。\n\n下图展示了不提交offset到服务端的问题：\n\n\n\n开始时，consumer 0消费partition 0 和1，后来由于新的consumer 2入组，分区重新进行了分配。consumer 0不再消费partition2，而由consumer 2来消费partition 2，但由于consumer之间是不能通讯的，所有consumer2并不知道从哪里开始自己的消费。\n\n因此consumer需要定期提交自己消费的offset到服务端，这样在重分区操作后，每个consumer都能在服务端查到分配给自己的partition所消费到的offset，继续消费。\n\n由于kafka有高可用和横向扩展的特性，当有新的分区出现或者新的消费入组后，需要重新分配消费者对应的分区，所以如果偏移量提交的有问题，会重复消费或者丢消息。偏移量提交的时机和方式要格外注意！！\n\n下面两种情况分别会造成重复消费和丢消息：\n\n如果提交的偏移量小于消费者最后一次消费的偏移量，那么再均衡后，两个offset之间的消息就会被重复消费 如果提交的偏移量大于消费者最后一次消费的偏移量，那么再均衡后，两个offset之间的消息就会丢失 以上两种情况是如何产生的呢？我们继续往下看。\n\n\n# 偏移量有两种提交方式\n\n# 1、自动提交偏移量\n\n设置 enable.auto.commit为true，设定好周期，默认5s。消费者每次调用轮询消息的poll() 方法时，会检查是否超过了5s没有提交偏移量，如果是，提交上一次轮询返回的偏移量。\n\n这样做很方便，但是会带来重复消费的问题。假如最近一次偏移量提交3s后，触发了再均衡，服务器端存储的还是上次提交的偏移量，那么再均衡结束后，新的消费者会从最后一次提交的偏移量开始拉取消息，此3s内消费的消息会被重复消费。\n\n# 2、手动提交偏移量\n\n设置 enable.auto.commit为false。程序中手动调用commitsync()提交偏移量，此时提交的是poll方法返回的最新的偏移量。\n\n我们来看下面两个提交时机：\n\n 1. 如果poll完马上调用commitsync(),那么一旦处理到中间某条消息的时候异常，由于偏移量已经提交，那么出问题的消息位置到提交偏移量之间的消息就会丢失。\n\n\n\n 1. 如果处理完所有消息后才调用commitsync()。有可能在处理到一半的时候发生再均衡，此时偏移量还未提交，那么再均衡后，会从上次提交的位置开始消费，造成重复消费。\n\n\n\n比较起来，重复消费要比丢消息好一些，所以我们程序应采用第二种方式，同时消费逻辑中，要能够检查重复消费。\n\ncommitsync()是同步提交偏移量，主程序会一直阻塞，偏移量提交成功后才往下运行。这样会限制程序的吞吐量。如果降低提交频次，又很容易发生重复消费。\n\n这里我们可以使用commitasync()异步提交偏移量。只管提交，而不会等待broker返回提交结果\n\ncommitsync只要没有发生不可恢复错误，会进行重试，直到成功。而commitasync不会进行重试，失败就是失败了。commitasync不重试，是因为重试提交时，可能已经有其它更大偏移量已经提交成功了，如果此时重试提交成功，那么更小的偏移量会覆盖大的偏移量。那么如果此时发生再均衡，新的消费者将会重复消费消息。\n\ncommitasync也支持回调，由于上述原因，回调中最好不要因为失败而重试提交。而是应该记录错误，以便后续分析和补偿。\n\n# 偏移量提交的最佳实践\n\n关于偏移量的提交方式和时机，上文已经有了大量的讲解。但看完后好像还不知道应该怎么提交偏移量才是最合适的。是不是觉得无论怎么提交，都无法避免重复消费？没错，事实就是这样，我们只能采用合理的方式，最大可能的去降低发生此类问题的概率。此外做好补偿处理。\n\n一般来说，偶尔的提交失败，不去重试，是没有问题的。因为一般是因为临时的问题而失败，后续的提交总会成功。如果我们在关闭消费者或者再均衡前，确保所有的消费者都能成功提交一次偏移量，也可以保证再均衡后，消费者能接着消费数据。\n\n因此我们采用同步和异步混合的方式提交偏移量。\n\n 1. 正常消费消息时，消费结束提交偏移量，采用异步方式\n 2. 如果程序报错，finally中，提交偏移量，采用同步方式，确保提交成功\n 3. 再均衡前的回调方法中，提交偏移量，采用同步方式，确保提交成功\n\n这样既保证了吞吐量，也保证了提交偏移量的安全性。另外由于再均衡前提交偏移量，降低了重复消费可能。\n\nkafka还提供了提交特定偏移量的方法。我们可以指定分区和offset进行提交。分区和offset的值可以从消息对象中取得。\n\n另外，如果担心一次取回数据量太大，可能处理到一半的时候出现再均衡，导致偏移量没有提交，重复消费。那么可以每n条提交一次。\n\n而当n=1时，也就是处理一条数据就提交一次，会把重复消费的可能降到最低。同时由于增加了和服务端的通讯，效率大大降低。\n\n其实即使这样，也是可能重复消费的，试想如下场景：\n\n消费者拉取到数据后，开始逻辑处理 处理第一条offset=2，成功了，提交offset=3 开始处理offset=3的消息，处理完成后，但提交offset=4前，此消费者突然意外挂掉了，所以也没能进入异常处理。偏移量没能成功提交。 消费者进行了再均衡，新的消费者接手此分区进行消费，取到的offset还是上一次提交的3，那么将会重复消费offset=3的消息。 所以我们应平衡重复消费发生的概率和程序的效率，来设置提交的时机。同时程序逻辑一定做好重复消费的检查工作！\n\n\n# 总结\n\n本节从协调器讲起，首先介绍了消费者协调器和组协调器，以及他们是如何配合工作的。从消费偏移量的管理展开，详细介绍了偏移量的提交，及提交的最佳实践。本节没有涉及代码部分，所有知识点相关的代码将在最后一章中统一给出。现在的要求只是理解知识点。\n\n\n# 参考资料\n\napache kafka核心组件和流程-协调器 - 爱码叔-icodebook",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"日志管理器",frontmatter:{title:"日志管理器",date:"2024-09-18T18:09:47.000Z",permalink:"/pages/b6cf91/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/700.%E3%80%8CBroker%E3%80%8D%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/15.%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86%E5%99%A8.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/15.日志管理器.md",key:"v-bee30952",path:"/pages/b6cf91/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"日志管理器",slug:"日志管理器",normalizedTitle:"日志管理器",charIndex:80},{level:3,title:"日志的存储",slug:"日志的存储",normalizedTitle:"日志的存储",charIndex:90},{level:3,title:"日志定位",slug:"日志定位",normalizedTitle:"日志定位",charIndex:996},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1867},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1874}],headersStr:"前言 日志管理器 日志的存储 日志定位 总结 参考资料",content:"# 前言\n\n上一节介绍了协调器。协调器主要负责消费者和kafka集群间的协调。那么消费者消费时，如何定位消息呢？消息是如何存储呢？本节将为你揭开答案。\n\n\n# 日志管理器\n\n\n# 日志的存储\n\nKafka的消息以日志文件的形式进行存储。不同主题下不同分区的消息是分开存储的。同一个分区的不同副本也是以日志的形式，分布在不同的broker上存储。\n\n这样看起来，日志的存储是以副本为单位的。在程序逻辑上，日志确实是以副本为单位的，每个副本对应一个log对象。但实际在物理上，一个log又划分为多个logSegment进行存储。\n\n举个例子，创建一个topic名为test，拥有3个分区。为了简化例子，我们设定只有1个broker，1个副本。那么所有的分区副本都存储在同一个broker上。\n\n\n\n第二章中，我们在kafka的配置文件中配置了log.dirs=/tmp/kafka-logs。此时在/tmp/kafka-logs下面会创建test-0，test-1，test-2三个文件夹，代表三个分区。命名规则为“topic名称-分区编号”\n\n我们看test-0这个文件夹，注意里面的logSegment并不代表这个文件夹，logSegment代表逻辑上的一组文件，这组文件就是.log、.index、.timeindex这三个不同文件扩展名，但是同文件名的文件。\n\n 1. .log存储消息\n 2. .index存储消息的索引\n 3. .timeIndex，时间索引文件，通过时间戳做索引。\n\n这三个文件配合使用，用来保存和消费时快速查找消息。\n\n刚才说到同一个logSegment的三个文件，文件名是一样的。命名规则为.log文件中第一条消息的前一条消息偏移量，也称为基础偏移量，左边补0，补齐20位。比如说第一个LogSegement的日志文件名为00000000000000000000.log，假如存储了200条消息后，达到了log.segment.bytes配置的阈值（默认1个G），那么将会创建新的logSegment，文件名为00000000000000000200.log。以此类推。另外即使没有达到log.segment.bytes的阈值，而是达到了log.roll.ms或者log.roll.hours设置的时间触发阈值，同样会触发产生新的logSegment。\n\n\n# 日志定位\n\n日志定位也就是消息定位，输入一个消息的offset，kafka如何定位到这条消息呢？\n\n日志定位的过程如下:\n\n1、根据offset定位logSegment。（kafka将基础偏移量也就是logsegment的名称作为key存在concurrentSkipListMap中）\n\n2、根据logSegment的index文件查找到距离目标offset最近的被索引的offset的position x。\n\n3、找到logSegment的.log文件中的x位置，向下逐条查找，找到目标offset的消息。\n\n结合下图中例子，我再做详细的讲解：\n\n\n\n这里先说明一下.index文件的存储方式。.index文件中存储了消息的索引，存储内容是消息的offset及物理位置position。并不是每条消息都有自己的索引，kafka采用的是稀疏索引，说白了就是隔n条消息存一条索引数据。这样做比每一条消息都建索引，查找起来会慢，但是也极大的节省了存储空间。此例中我们假设跨度为2，实际kafka中跨度并不是固定条数，而是取决于消息累积字节数大小。\n\n例子中consumer要消费offset=15的消息。我们假设目前可供消费的消息已经存储了三个logsegment，分别是00000000000000000，0000000000000000010，0000000000000000020。为了讲解方便，下面提到名称时，会把前面零去掉。\n\n下面我们详细讲一下查找过程。\n\n 1. kafka收到查询offset=15的消息请求后，通过二分查找，从concurrentSkipListMap中找到对应的logsegment名称，也就是10。\n 2. 从10.index中找到offset小于等于15的最大值，offset=14，它对应的position=340\n 3. 从10.log文件中物理位置340，顺序往下扫描文件，找到offset=15的消息内容。\n\n可以看到通过稀疏索引，kafka既加快了消息查找的速度，也顾及了存储的开销。\n\n\n# 总结\n\n\n# 参考资料\n\nApache Kafka 核心组件和流程-日志管理器 - 爱码叔-iCodeBook",normalizedContent:"# 前言\n\n上一节介绍了协调器。协调器主要负责消费者和kafka集群间的协调。那么消费者消费时，如何定位消息呢？消息是如何存储呢？本节将为你揭开答案。\n\n\n# 日志管理器\n\n\n# 日志的存储\n\nkafka的消息以日志文件的形式进行存储。不同主题下不同分区的消息是分开存储的。同一个分区的不同副本也是以日志的形式，分布在不同的broker上存储。\n\n这样看起来，日志的存储是以副本为单位的。在程序逻辑上，日志确实是以副本为单位的，每个副本对应一个log对象。但实际在物理上，一个log又划分为多个logsegment进行存储。\n\n举个例子，创建一个topic名为test，拥有3个分区。为了简化例子，我们设定只有1个broker，1个副本。那么所有的分区副本都存储在同一个broker上。\n\n\n\n第二章中，我们在kafka的配置文件中配置了log.dirs=/tmp/kafka-logs。此时在/tmp/kafka-logs下面会创建test-0，test-1，test-2三个文件夹，代表三个分区。命名规则为“topic名称-分区编号”\n\n我们看test-0这个文件夹，注意里面的logsegment并不代表这个文件夹，logsegment代表逻辑上的一组文件，这组文件就是.log、.index、.timeindex这三个不同文件扩展名，但是同文件名的文件。\n\n 1. .log存储消息\n 2. .index存储消息的索引\n 3. .timeindex，时间索引文件，通过时间戳做索引。\n\n这三个文件配合使用，用来保存和消费时快速查找消息。\n\n刚才说到同一个logsegment的三个文件，文件名是一样的。命名规则为.log文件中第一条消息的前一条消息偏移量，也称为基础偏移量，左边补0，补齐20位。比如说第一个logsegement的日志文件名为00000000000000000000.log，假如存储了200条消息后，达到了log.segment.bytes配置的阈值（默认1个g），那么将会创建新的logsegment，文件名为00000000000000000200.log。以此类推。另外即使没有达到log.segment.bytes的阈值，而是达到了log.roll.ms或者log.roll.hours设置的时间触发阈值，同样会触发产生新的logsegment。\n\n\n# 日志定位\n\n日志定位也就是消息定位，输入一个消息的offset，kafka如何定位到这条消息呢？\n\n日志定位的过程如下:\n\n1、根据offset定位logsegment。（kafka将基础偏移量也就是logsegment的名称作为key存在concurrentskiplistmap中）\n\n2、根据logsegment的index文件查找到距离目标offset最近的被索引的offset的position x。\n\n3、找到logsegment的.log文件中的x位置，向下逐条查找，找到目标offset的消息。\n\n结合下图中例子，我再做详细的讲解：\n\n\n\n这里先说明一下.index文件的存储方式。.index文件中存储了消息的索引，存储内容是消息的offset及物理位置position。并不是每条消息都有自己的索引，kafka采用的是稀疏索引，说白了就是隔n条消息存一条索引数据。这样做比每一条消息都建索引，查找起来会慢，但是也极大的节省了存储空间。此例中我们假设跨度为2，实际kafka中跨度并不是固定条数，而是取决于消息累积字节数大小。\n\n例子中consumer要消费offset=15的消息。我们假设目前可供消费的消息已经存储了三个logsegment，分别是00000000000000000，0000000000000000010，0000000000000000020。为了讲解方便，下面提到名称时，会把前面零去掉。\n\n下面我们详细讲一下查找过程。\n\n 1. kafka收到查询offset=15的消息请求后，通过二分查找，从concurrentskiplistmap中找到对应的logsegment名称，也就是10。\n 2. 从10.index中找到offset小于等于15的最大值，offset=14，它对应的position=340\n 3. 从10.log文件中物理位置340，顺序往下扫描文件，找到offset=15的消息内容。\n\n可以看到通过稀疏索引，kafka既加快了消息查找的速度，也顾及了存储的开销。\n\n\n# 总结\n\n\n# 参考资料\n\napache kafka 核心组件和流程-日志管理器 - 爱码叔-icodebook",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"副本管理器",frontmatter:{title:"副本管理器",date:"2024-09-18T18:10:37.000Z",permalink:"/pages/f57330/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/06.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/700.%E3%80%8CBroker%E3%80%8D%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/20.%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86%E5%99%A8.html",relativePath:"02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/20.副本管理器.md",key:"v-5ab6e4c0",path:"/pages/f57330/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:3,title:"副本管理器",slug:"副本管理器",normalizedTitle:"副本管理器",charIndex:13}],headersStr:"前言 副本管理器",content:"# 前言\n\n本章简单介绍了副本管理器，副本管理器负责分区及其副本的管理。副本管理器具体的工作流程可以参考牟大恩所著的《Kafka入门与实践》。\n\n\n# 副本管理器\n\n副本机制使得kafka整个集群中，只要有一个代理存活，就可以保证集群正常运行。这大大提高了Kafka的可靠性和稳定性。\n\nKafka中代理的存活，需要满足以下两个条件：\n\n 1. 存活的节点要维持和zookeeper的session连接，通过zookeeper的心跳机制实现\n 2. Follower副本要与leader副本保持同步，不能落后太多。\n\n满足以上条件的节点在ISR中，一旦宕机，或者中断时间太长，Leader就会把同步副本从ISR中踢出。\n\n所有节点中，leader节点负责接收客户端的读写操作，follower节点从leader复制数据。\n\n副本管理器负责对副本管理。由于副本是分区的副本，所以对副本的管理体现在对分区的管理。\n\n在第三章已经对分区和副本有了详细的讲解，这里再介绍两个重要的概念，LEO和HW。\n\n 1. LEO是Log End Offset缩写。表示每个分区副本的最后一条消息的位置，也就是说每个副本都有LEO。\n 2. HW是Hight Watermark缩写，他是一个分区所有副本中，最小的那个LEO。\n\n看下图：\n\n\n\n分区test-0有三个副本，每个副本的LEO就是自己最后一条消息的offset。可以看到最小的LEO是Replica2的，等于3，也就是说HW=3。这代表offset=4的消息还没有被所有副本复制，是无法被消费的。而offset<=3的数据已经被所有副本复制，是可以被消费的。\n\n副本管理器所承担的职责如下：\n\n 1. 副本过期检查\n 2. 追加消息\n 3. 拉取消息\n 4. 副本同步过程\n 5. 副本角色转换\n 6. 关闭副本\n\n本文就不再一一讲解。",normalizedContent:"# 前言\n\n本章简单介绍了副本管理器，副本管理器负责分区及其副本的管理。副本管理器具体的工作流程可以参考牟大恩所著的《kafka入门与实践》。\n\n\n# 副本管理器\n\n副本机制使得kafka整个集群中，只要有一个代理存活，就可以保证集群正常运行。这大大提高了kafka的可靠性和稳定性。\n\nkafka中代理的存活，需要满足以下两个条件：\n\n 1. 存活的节点要维持和zookeeper的session连接，通过zookeeper的心跳机制实现\n 2. follower副本要与leader副本保持同步，不能落后太多。\n\n满足以上条件的节点在isr中，一旦宕机，或者中断时间太长，leader就会把同步副本从isr中踢出。\n\n所有节点中，leader节点负责接收客户端的读写操作，follower节点从leader复制数据。\n\n副本管理器负责对副本管理。由于副本是分区的副本，所以对副本的管理体现在对分区的管理。\n\n在第三章已经对分区和副本有了详细的讲解，这里再介绍两个重要的概念，leo和hw。\n\n 1. leo是log end offset缩写。表示每个分区副本的最后一条消息的位置，也就是说每个副本都有leo。\n 2. hw是hight watermark缩写，他是一个分区所有副本中，最小的那个leo。\n\n看下图：\n\n\n\n分区test-0有三个副本，每个副本的leo就是自己最后一条消息的offset。可以看到最小的leo是replica2的，等于3，也就是说hw=3。这代表offset=4的消息还没有被所有副本复制，是无法被消费的。而offset<=3的数据已经被所有副本复制，是可以被消费的。\n\n副本管理器所承担的职责如下：\n\n 1. 副本过期检查\n 2. 追加消息\n 3. 拉取消息\n 4. 副本同步过程\n 5. 副本角色转换\n 6. 关闭副本\n\n本文就不再一一讲解。",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"高可用探究",frontmatter:{title:"高可用探究",date:"2024-09-18T13:29:28.000Z",permalink:"/pages/2dec11/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/08.%E5%9B%9B%E3%80%81%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87/20.%E9%AB%98%E5%8F%AF%E7%94%A8%E6%8E%A2%E7%A9%B6.html",relativePath:"02.Kafka  系统设计/08.四、设计目标/20.高可用探究.md",key:"v-6fecdff5",path:"/pages/2dec11/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"集群架构",slug:"集群架构",normalizedTitle:"集群架构",charIndex:728},{level:2,title:"Kafka集群选举",slug:"kafka集群选举",normalizedTitle:"kafka集群选举",charIndex:859},{level:3,title:"ISR 与 OSR",slug:"isr-与-osr",normalizedTitle:"isr 与 osr",charIndex:873},{level:3,title:"LEO和HW",slug:"leo和hw",normalizedTitle:"leo和hw",charIndex:2020},{level:3,title:"Kafka分区Leader选举",slug:"kafka分区leader选举",normalizedTitle:"kafka分区leader选举",charIndex:2964},{level:4,title:"Leader Replica选举策略",slug:"leader-replica选举策略",normalizedTitle:"leader replica选举策略",charIndex:3543},{level:4,title:"Leader Replica选举过程",slug:"leader-replica选举过程",normalizedTitle:"leader replica选举过程",charIndex:4328},{level:2,title:"副本机制(Replication）",slug:"副本机制-replication",normalizedTitle:"副本机制(replication）",charIndex:5593},{level:2,title:"消费者组和再均衡",slug:"消费者组和再均衡",normalizedTitle:"消费者组和再均衡",charIndex:5797},{level:3,title:"消费者组",slug:"消费者组",normalizedTitle:"消费者组",charIndex:340},{level:3,title:"再均衡(重平衡)",slug:"再均衡-重平衡",normalizedTitle:"再均衡(重平衡)",charIndex:6776},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:7065}],headersStr:"前言 集群架构 Kafka集群选举 ISR 与 OSR LEO和HW Kafka分区Leader选举 Leader Replica选举策略 Leader Replica选举过程 副本机制(Replication） 消费者组和再均衡 消费者组 再均衡(重平衡) 参考资料",content:'# 前言\n\nApache Kafka 的高可用设计主要通过以下几个方面来实现：\n\n * 副本机制（Replication）：在 Kafka 中，每个 partition 都有多个副本，其中一个作为 leader，其他的作为follower。所有的读写操作都是通过 leader 来进行的，follower 则负责从 leader 同步数据。当 leader出现故障时，会从 follower 中选举出新的 leader，以保证服务的可用性。\n * 分区机制（Partitioning）：Kafka 的 topic 可以分为多个 partition，每个 partition可以在不同的服务器上，这样即使某个服务器出现故障，也不会影响到其他 partition 的正常服务。\n * 消费者组（Consumer Groups）：Kafka 允许多个消费者组同时消费同一个 topic，每个消费者组都会维护自己的offset，这样即使某个消费者组出现故障，也不会影响到其他消费者组的消费\n * ZooKeeper 集群：Kafka 使用 ZooKeeper 来管理集群的元数据信息，如 broker、topic 和 partition 的信息等。ZooKeeper 本身也是一个分布式服务，可以通过多个节点组成集群，提供高可用性。\n * ISR（In-Sync Replicas）机制：Kafka 通过 ISR 机制来保证数据的一致性。只有在 ISR 列表中的 follower 才有资格被选为新的 leader，这样可以保证新的 leader 拥有所有的数据副本。\n\n通过以上的设计，Kafka 能够在面对故障时，仍能保证服务的可用性和数据的一致性。\n\n\n# 集群架构\n\nKafka 的服务器端由被称为 Broker 的服务进程构成，即一个 Kafka 集群由多个 Broker 组成 这样如果集群中某一台机器宕机，其他机器上的 Broker 也依然能够对外提供服务。这其实就是 Kafka 提供高可用的基础\n\n\n\n\n# Kafka集群选举\n\n\n# ISR 与 OSR\n\nKafka为了对消息进行分类，引入了Topic（主题）的概念。生产者在发送消息的时候，需要指定发送到某个Topic，然后消息者订阅这个Topic并进行消费消息。\n\nKafka为了提升性能，又在Topic的基础上，引入了Partition（分区）的概念。Topic是逻辑概念，而Partition是物理分组。一个Topic可以包含多个Partition，生产者在发送消息的时候，需要指定发送到某个Topic的某个Partition，然后消息者订阅这个Topic并消费这个Partition中的消息。\n\nKafka为了提高系统的吞吐量和可扩展性，把一个Topic的不同Partition放到多个Broker节点上，充分利用机器资源，也便于扩展Partition。\n\nKafka为了保证数据的安全性和服务的高可用，又在Partition的基础上，引入Replica（副本）的概念。一个Partition包含多个Replica，Replica之间是一主多从的关系，有两种类型Leader Replica（领导者副本）和Follower Replica（跟随者副本），Replica分布在不同的Broker节点上。\n\nLeader Replica负责读写请求，Follower Replica只负责同步Leader Replica数据，不对外提供服务。当Leader Replica发生故障，就从Follower Replica选举出一个新的Leader Replica继续对外提供服务，实现了故障自动转移。\n\n\n\nKafka为了提升Replica的同步效率和数据写入效率，又对Replica进行分类。针对一个Partition的所有Replica集合统称为AR（Assigned Replicas，已分配的副本），包含Leader Replica和Follower Replica。与Leader Replica保持同步的Replica集合称为ISR（In-Sync Replicas，同步副本），与Leader Replica保持失去同步的Replica集合称为OSR（Out-of-Sync Replicas，失去同步的副本），AR = ISR + OSR。 Leader Replica将消息写入磁盘前，需要等ISR中的所有副本同步完成。如果ISR中某个Follower Replica同步数据落后Leader Replica过多，会被转移到OSR中。如果OSR中的某个Follower Replica同步数据追上了Leader Replica，会被转移到ISR中。当Leader Replica发生故障的时候，只会从ISR中选举出新的Leader Replica\n\n\n# LEO和HW\n\nKafka为了记录副本的同步状态，以及控制消费者消费消息的范围，于是引入了LEO（Log End Offset，日志结束偏移量）和HW（High Watermark，高水位）。 LEO表示分区中的下一个被写入消息的偏移量，也是分区中的最大偏移量。LEO用于记录Leader Replica和Follower Replica之间的数据同步进度，每个副本中各有一份。\n\nLeader : LEO 1000 HW : 950\nFollower1 : LEO 980\nFollower2 : LEO 1000\nFollower3 : LEO 950\n\n\nHW表示所有副本（Leader和Follower）都已成功复制的最小偏移量，是所有副本共享的数据值。换句话说，HW之前的消息都被视为已提交，消费者可以消费这些消息。用于确保消息的一致性和只读一次。\n\nLEO和HW的更新流程：\n\n 1. 初始状态，三个副本中各有0和1两条消息，LEO都是2，位置2是空的，表示是即将被写入消息的位置。HW也都是2，表示Leader Replica中的所有消息已经全部同步到Follower Replica中，消费者可以消费0和1两条消息。\n    \n    \n\n 2. 生产者往Leader Replica中发送两条消息，此时Leader Replica的LEO的值增加2，变成4。由于还没有开始往Follower Replica同步消息，所以HW值和Follower Replica中LEO值都没有变。由于消费者只能消费HW之前的消息，也就是0和1两条消息\n    \n    \n\n 3. Leader Replica开始向Follower Replica同步消息，同步速率不同，Follower1的两条消息2和3已经同步完成，而Follower2只同步了一条消息2。此时，Leader和Follower1的LEO都是4，而Follower2的LEO是3，HW表示已成功同步的最小偏移量，值是3，表示此时消费者只能读到0、1、2，三条消息\n    \n    \n\n 4. 所有消息都同步完成，三个副本的LEO都是4，HW也是4，消费者可以读到0、1、2、3，四条消息\n    \n    \n\n\n# Kafka分区Leader选举\n\n常见的有以下几种情况会触发Partition的Leader Replica选举：\n\n 1. Leader Replica 失效：当 Leader Replica 出现故障或者失去连接时，Kafka 会触发 Leader Replica 选举。\n 2. Broker 宕机：当 Leader Replica 所在的 Broker 节点发生故障或者宕机时，Kafka 也会触发 Leader Replica 选举。\n 3. 新增 Broker：当集群中新增 Broker 节点时，Kafka 还会触发 Leader Replica 选举，以重新分配 Partition 的 Leader。\n 4. 新建分区：当一个新的分区被创建时，需要选举一个 Leader Replica。\n 5. SR 列表数量减少：当 Partition 的 ISR 列表数量减少时，可能会触发 Leader Replica 选举。当 ISR 列表中副本数量小于Replication Factor（副本因子）时，为了保证数据的安全性，就会触发 Leader Replica 选举。\n 6. 手动触发：通过 Kafka 管理工具（kafka-preferred-replica-election.sh），可以手动触发选举，以平衡负载或实现集群维护\n\n# Leader Replica选举策略\n\n在 Kafka 集群中，常见的 Leader Replica 选举策略有以下三种：\n\n 1. ISR 选举策略：默认情况下，Kafka 只会从 ISR 集合的副本中选举出新的 Leader Replica，OSR 集合中的副本不具备参选资格。\n 2. 不干净副本选举策略（Unclean Leader Election）：在某些情况下，ISR 选举策略可能会失败，例如当所有 ISR 副本都不可用时。在这种情况下，可以使用 Unclean Leader 选举策略。Unclean Leader 选举策略会从所有副本中（包含OSR集合）选择一个副本作为新的 Leader 副本，即使这个副本与当前 Leader 副本不同步。这种选举策略可能会导致数据丢失，默认关闭\n 3. 首选副本选举策略（Preferred Replica Election）：首选副本选举策略也是 Kafka 默认的选举策略。在这种策略下，每个分区都有一个首选副本（Preferred Replica），通常是副本集合中的第一个副本。当触发选举时，控制器会优先选择该首选副本作为新的 Leader Replica，只有在首选副本不可用的情况下，才会考虑其他副本。 当然，可以使用命令手动指定每个分区的首选副本：\n\n> bin/kafka-topics.sh --zookeeper localhost:2181 --topic my-topic-name --replica-assignment 0:1,1:2,2:0 --partitions 3 意思是：my-topic-name有3个partition，partition0的首选副本是Broker1，partition1首选副本是Broker2，partition2的首选副本是Broker0\n\n# Leader Replica选举过程\n\n谁来主持选举？\n\nkafka先在brokers里面选一个broker作为Controller主持选举。Controller是使用zookeeper选举出来的，每个broker都往zk里面写一个/controller节点，谁先写成功，谁就成为Controller。如果Controller失去连接，zk上的临时节点就会消失。其它的broker通过watcher监听到Controller下线的消息后，开始选举新的Controller。\n\n> 一个Broker节点相当于一台机器，多个Broker节点组成一个Kafka集群。Controller节点也叫控制器节点 , 他负责直接与zookeeper进行通信，并负责管理整个集群的状态和元数据信息\n\nController的责任\n\n * 监听Broker的变化\n * 监听Topic变化\n * 监听Partition变化\n * 获取和管理Broker、Topic、Partition的信息\n * 管理Partition的主从信息\n\n当Leader Replica宕机或失效时，就会触发 Leader Replica 选举，分为两个阶段，第一个阶段是候选人的提名和投票阶段，第二个阶段是Leader的确认阶段。具体过程如下：\n\n> lag(滞后）是kafka消费队列性能监控的重要指标，lag的值越大，表示kafka的消息堆积越严重\n\n 1. 候选人提名和投票阶段 在Leader Replica失效时，ISR集合中所有Follower Replica都可以成为新的Leader Replica候选人。每个Follower Replica会在选举开始时向其他Follower Replica发送成为候选人的请求，并附带自己的元数据信息，包括自己的当前状态和Lag值。而Preferred replica优先成为候选人。 其他Follower Replica在收到候选人请求后，会根据请求中的元数据信息，计算每个候选人的Lag值，并将自己的选票投给Lag最小的候选人。如果多个候选人的Lag值相同，则随机选择一个候选人。\n 2. Leader确认阶段 在第一阶段结束后，所有的Follower Replica会重新计算每位候选人的Lag值，并投票给Lag值最小的候选人。此时，选举的结果并不一定出现对候选人的全局共识。为了避免出现这种情况，Kafka中使用了ZooKeeper来实现分布式锁，确保只有一个候选人能够成为新的Leader Replica。 当ZooKeeper确认有一个候选人已经获得了分布式锁时，该候选人就成为了新的Leader Replica，并向所有的Follower Replica发送一个LeaderAndIsrRequest请求，更新Partition的元数据信息。其他Follower Replica接收到请求后，会更新自己的Partition元数据信息，将新的Leader Replica的ID添加到ISR列表中\n\n\n# 副本机制(Replication）\n\n\n\nKafka 中消息的备份又叫做 副本（Replica）\n\nKafka 定义了两类副本：\n\n * 领导者副本（Leader Replica）: 负责数据读写\n * 追随者副本（Follower Replica）: 只负责数据备份\n * 当领导者副本所在节点宕机之后, 会从追随者副本中选举一个节点, 升级为领导者副本 , 对外提供数据读写服务, 保证数据安全\n\n\n# 消费者组和再均衡\n\n\n# 消费者组\n\n消费者组（Consumer Group）是由一个或多个消费者实例（Consumer Instance）组成的群组，具有可扩展性和可容错性的一种机制。消费者组内的消费者共享一个消费者组ID，这个ID 也叫做 Group ID，组内的消费者共同对一个主题进行订阅和消费，同一个组中只能够由一个消费者去消费某一个分区的数据，多余的消费者会闲置，派不上用场。\n\n\n\n> 同一个分区只能被一个消费者组中的一个消费者消费 , 一个消费者组中的某一个消费者, 可以消费多个分区\n\n一个生产者发送一条消息只能被一个消费者消费 : 让消费者处于同一个组中即可 一个生产者发送一条消息需要被多个消费者消费 : 让消费者处于不同的组中\n\n@Component\npublic class KafkaConsumerListener {\n\n    @KafkaListener(topics = "kafka.topic.my-topic1",groupId = "group1")\n    public void listenTopic1group1(ConsumerRecord<String, String> record) {\n        String key = record.key();\n        String value = record.value();\n        System.out.println("group1中的消费者接收到消息:"+key + " : " + value);\n    }\n\n    @KafkaListener(topics = "kafka.topic.my-topic1",groupId = "group2")\n    public void listenTopic1group2(ConsumerRecord<String, String> record) {\n        String key = record.key();\n        String value = record.value();\n        System.out.println("group2中的消费者接收到消息:"+key + " : " + value);\n    }\n}\n\n\n\n\n# 再均衡(重平衡)\n\n> 再均衡就是指 当消费者组中的消费者发生变更的时候(新增消费者, 消费者宕机) , 重新为消费者分配消费分区的过程\n\n\n\n当消费者组中重新加入消费者 , 或者消费者组中有消费者宕机 , 这个时候Kafka会为消费者组中的消费者从新分配消费分区的过程就是再均衡\n\n\n\n重平衡(再均衡)非常重要，它为消费者群组带来了高可用性 和 伸缩性，我们可以放心的添加消费者或移除消费者，不过在正常情况下我们并不希望发生这样的行为。在重平衡期间，消费者无法读取消息，造成整个消费者组在重平衡的期间都不可用 , 并且在发生再均衡的时候有可能导致消息的丢失和重复消费\n\n\n# 参考资料\n\nkafka-高可用设计详解（集群架构、备份机制、消费者组、重平衡）_kafka高可用-CSDN博客',normalizedContent:'# 前言\n\napache kafka 的高可用设计主要通过以下几个方面来实现：\n\n * 副本机制（replication）：在 kafka 中，每个 partition 都有多个副本，其中一个作为 leader，其他的作为follower。所有的读写操作都是通过 leader 来进行的，follower 则负责从 leader 同步数据。当 leader出现故障时，会从 follower 中选举出新的 leader，以保证服务的可用性。\n * 分区机制（partitioning）：kafka 的 topic 可以分为多个 partition，每个 partition可以在不同的服务器上，这样即使某个服务器出现故障，也不会影响到其他 partition 的正常服务。\n * 消费者组（consumer groups）：kafka 允许多个消费者组同时消费同一个 topic，每个消费者组都会维护自己的offset，这样即使某个消费者组出现故障，也不会影响到其他消费者组的消费\n * zookeeper 集群：kafka 使用 zookeeper 来管理集群的元数据信息，如 broker、topic 和 partition 的信息等。zookeeper 本身也是一个分布式服务，可以通过多个节点组成集群，提供高可用性。\n * isr（in-sync replicas）机制：kafka 通过 isr 机制来保证数据的一致性。只有在 isr 列表中的 follower 才有资格被选为新的 leader，这样可以保证新的 leader 拥有所有的数据副本。\n\n通过以上的设计，kafka 能够在面对故障时，仍能保证服务的可用性和数据的一致性。\n\n\n# 集群架构\n\nkafka 的服务器端由被称为 broker 的服务进程构成，即一个 kafka 集群由多个 broker 组成 这样如果集群中某一台机器宕机，其他机器上的 broker 也依然能够对外提供服务。这其实就是 kafka 提供高可用的基础\n\n\n\n\n# kafka集群选举\n\n\n# isr 与 osr\n\nkafka为了对消息进行分类，引入了topic（主题）的概念。生产者在发送消息的时候，需要指定发送到某个topic，然后消息者订阅这个topic并进行消费消息。\n\nkafka为了提升性能，又在topic的基础上，引入了partition（分区）的概念。topic是逻辑概念，而partition是物理分组。一个topic可以包含多个partition，生产者在发送消息的时候，需要指定发送到某个topic的某个partition，然后消息者订阅这个topic并消费这个partition中的消息。\n\nkafka为了提高系统的吞吐量和可扩展性，把一个topic的不同partition放到多个broker节点上，充分利用机器资源，也便于扩展partition。\n\nkafka为了保证数据的安全性和服务的高可用，又在partition的基础上，引入replica（副本）的概念。一个partition包含多个replica，replica之间是一主多从的关系，有两种类型leader replica（领导者副本）和follower replica（跟随者副本），replica分布在不同的broker节点上。\n\nleader replica负责读写请求，follower replica只负责同步leader replica数据，不对外提供服务。当leader replica发生故障，就从follower replica选举出一个新的leader replica继续对外提供服务，实现了故障自动转移。\n\n\n\nkafka为了提升replica的同步效率和数据写入效率，又对replica进行分类。针对一个partition的所有replica集合统称为ar（assigned replicas，已分配的副本），包含leader replica和follower replica。与leader replica保持同步的replica集合称为isr（in-sync replicas，同步副本），与leader replica保持失去同步的replica集合称为osr（out-of-sync replicas，失去同步的副本），ar = isr + osr。 leader replica将消息写入磁盘前，需要等isr中的所有副本同步完成。如果isr中某个follower replica同步数据落后leader replica过多，会被转移到osr中。如果osr中的某个follower replica同步数据追上了leader replica，会被转移到isr中。当leader replica发生故障的时候，只会从isr中选举出新的leader replica\n\n\n# leo和hw\n\nkafka为了记录副本的同步状态，以及控制消费者消费消息的范围，于是引入了leo（log end offset，日志结束偏移量）和hw（high watermark，高水位）。 leo表示分区中的下一个被写入消息的偏移量，也是分区中的最大偏移量。leo用于记录leader replica和follower replica之间的数据同步进度，每个副本中各有一份。\n\nleader : leo 1000 hw : 950\nfollower1 : leo 980\nfollower2 : leo 1000\nfollower3 : leo 950\n\n\nhw表示所有副本（leader和follower）都已成功复制的最小偏移量，是所有副本共享的数据值。换句话说，hw之前的消息都被视为已提交，消费者可以消费这些消息。用于确保消息的一致性和只读一次。\n\nleo和hw的更新流程：\n\n 1. 初始状态，三个副本中各有0和1两条消息，leo都是2，位置2是空的，表示是即将被写入消息的位置。hw也都是2，表示leader replica中的所有消息已经全部同步到follower replica中，消费者可以消费0和1两条消息。\n    \n    \n\n 2. 生产者往leader replica中发送两条消息，此时leader replica的leo的值增加2，变成4。由于还没有开始往follower replica同步消息，所以hw值和follower replica中leo值都没有变。由于消费者只能消费hw之前的消息，也就是0和1两条消息\n    \n    \n\n 3. leader replica开始向follower replica同步消息，同步速率不同，follower1的两条消息2和3已经同步完成，而follower2只同步了一条消息2。此时，leader和follower1的leo都是4，而follower2的leo是3，hw表示已成功同步的最小偏移量，值是3，表示此时消费者只能读到0、1、2，三条消息\n    \n    \n\n 4. 所有消息都同步完成，三个副本的leo都是4，hw也是4，消费者可以读到0、1、2、3，四条消息\n    \n    \n\n\n# kafka分区leader选举\n\n常见的有以下几种情况会触发partition的leader replica选举：\n\n 1. leader replica 失效：当 leader replica 出现故障或者失去连接时，kafka 会触发 leader replica 选举。\n 2. broker 宕机：当 leader replica 所在的 broker 节点发生故障或者宕机时，kafka 也会触发 leader replica 选举。\n 3. 新增 broker：当集群中新增 broker 节点时，kafka 还会触发 leader replica 选举，以重新分配 partition 的 leader。\n 4. 新建分区：当一个新的分区被创建时，需要选举一个 leader replica。\n 5. sr 列表数量减少：当 partition 的 isr 列表数量减少时，可能会触发 leader replica 选举。当 isr 列表中副本数量小于replication factor（副本因子）时，为了保证数据的安全性，就会触发 leader replica 选举。\n 6. 手动触发：通过 kafka 管理工具（kafka-preferred-replica-election.sh），可以手动触发选举，以平衡负载或实现集群维护\n\n# leader replica选举策略\n\n在 kafka 集群中，常见的 leader replica 选举策略有以下三种：\n\n 1. isr 选举策略：默认情况下，kafka 只会从 isr 集合的副本中选举出新的 leader replica，osr 集合中的副本不具备参选资格。\n 2. 不干净副本选举策略（unclean leader election）：在某些情况下，isr 选举策略可能会失败，例如当所有 isr 副本都不可用时。在这种情况下，可以使用 unclean leader 选举策略。unclean leader 选举策略会从所有副本中（包含osr集合）选择一个副本作为新的 leader 副本，即使这个副本与当前 leader 副本不同步。这种选举策略可能会导致数据丢失，默认关闭\n 3. 首选副本选举策略（preferred replica election）：首选副本选举策略也是 kafka 默认的选举策略。在这种策略下，每个分区都有一个首选副本（preferred replica），通常是副本集合中的第一个副本。当触发选举时，控制器会优先选择该首选副本作为新的 leader replica，只有在首选副本不可用的情况下，才会考虑其他副本。 当然，可以使用命令手动指定每个分区的首选副本：\n\n> bin/kafka-topics.sh --zookeeper localhost:2181 --topic my-topic-name --replica-assignment 0:1,1:2,2:0 --partitions 3 意思是：my-topic-name有3个partition，partition0的首选副本是broker1，partition1首选副本是broker2，partition2的首选副本是broker0\n\n# leader replica选举过程\n\n谁来主持选举？\n\nkafka先在brokers里面选一个broker作为controller主持选举。controller是使用zookeeper选举出来的，每个broker都往zk里面写一个/controller节点，谁先写成功，谁就成为controller。如果controller失去连接，zk上的临时节点就会消失。其它的broker通过watcher监听到controller下线的消息后，开始选举新的controller。\n\n> 一个broker节点相当于一台机器，多个broker节点组成一个kafka集群。controller节点也叫控制器节点 , 他负责直接与zookeeper进行通信，并负责管理整个集群的状态和元数据信息\n\ncontroller的责任\n\n * 监听broker的变化\n * 监听topic变化\n * 监听partition变化\n * 获取和管理broker、topic、partition的信息\n * 管理partition的主从信息\n\n当leader replica宕机或失效时，就会触发 leader replica 选举，分为两个阶段，第一个阶段是候选人的提名和投票阶段，第二个阶段是leader的确认阶段。具体过程如下：\n\n> lag(滞后）是kafka消费队列性能监控的重要指标，lag的值越大，表示kafka的消息堆积越严重\n\n 1. 候选人提名和投票阶段 在leader replica失效时，isr集合中所有follower replica都可以成为新的leader replica候选人。每个follower replica会在选举开始时向其他follower replica发送成为候选人的请求，并附带自己的元数据信息，包括自己的当前状态和lag值。而preferred replica优先成为候选人。 其他follower replica在收到候选人请求后，会根据请求中的元数据信息，计算每个候选人的lag值，并将自己的选票投给lag最小的候选人。如果多个候选人的lag值相同，则随机选择一个候选人。\n 2. leader确认阶段 在第一阶段结束后，所有的follower replica会重新计算每位候选人的lag值，并投票给lag值最小的候选人。此时，选举的结果并不一定出现对候选人的全局共识。为了避免出现这种情况，kafka中使用了zookeeper来实现分布式锁，确保只有一个候选人能够成为新的leader replica。 当zookeeper确认有一个候选人已经获得了分布式锁时，该候选人就成为了新的leader replica，并向所有的follower replica发送一个leaderandisrrequest请求，更新partition的元数据信息。其他follower replica接收到请求后，会更新自己的partition元数据信息，将新的leader replica的id添加到isr列表中\n\n\n# 副本机制(replication）\n\n\n\nkafka 中消息的备份又叫做 副本（replica）\n\nkafka 定义了两类副本：\n\n * 领导者副本（leader replica）: 负责数据读写\n * 追随者副本（follower replica）: 只负责数据备份\n * 当领导者副本所在节点宕机之后, 会从追随者副本中选举一个节点, 升级为领导者副本 , 对外提供数据读写服务, 保证数据安全\n\n\n# 消费者组和再均衡\n\n\n# 消费者组\n\n消费者组（consumer group）是由一个或多个消费者实例（consumer instance）组成的群组，具有可扩展性和可容错性的一种机制。消费者组内的消费者共享一个消费者组id，这个id 也叫做 group id，组内的消费者共同对一个主题进行订阅和消费，同一个组中只能够由一个消费者去消费某一个分区的数据，多余的消费者会闲置，派不上用场。\n\n\n\n> 同一个分区只能被一个消费者组中的一个消费者消费 , 一个消费者组中的某一个消费者, 可以消费多个分区\n\n一个生产者发送一条消息只能被一个消费者消费 : 让消费者处于同一个组中即可 一个生产者发送一条消息需要被多个消费者消费 : 让消费者处于不同的组中\n\n@component\npublic class kafkaconsumerlistener {\n\n    @kafkalistener(topics = "kafka.topic.my-topic1",groupid = "group1")\n    public void listentopic1group1(consumerrecord<string, string> record) {\n        string key = record.key();\n        string value = record.value();\n        system.out.println("group1中的消费者接收到消息:"+key + " : " + value);\n    }\n\n    @kafkalistener(topics = "kafka.topic.my-topic1",groupid = "group2")\n    public void listentopic1group2(consumerrecord<string, string> record) {\n        string key = record.key();\n        string value = record.value();\n        system.out.println("group2中的消费者接收到消息:"+key + " : " + value);\n    }\n}\n\n\n\n\n# 再均衡(重平衡)\n\n> 再均衡就是指 当消费者组中的消费者发生变更的时候(新增消费者, 消费者宕机) , 重新为消费者分配消费分区的过程\n\n\n\n当消费者组中重新加入消费者 , 或者消费者组中有消费者宕机 , 这个时候kafka会为消费者组中的消费者从新分配消费分区的过程就是再均衡\n\n\n\n重平衡(再均衡)非常重要，它为消费者群组带来了高可用性 和 伸缩性，我们可以放心的添加消费者或移除消费者，不过在正常情况下我们并不希望发生这样的行为。在重平衡期间，消费者无法读取消息，造成整个消费者组在重平衡的期间都不可用 , 并且在发生再均衡的时候有可能导致消息的丢失和重复消费\n\n\n# 参考资料\n\nkafka-高可用设计详解（集群架构、备份机制、消费者组、重平衡）_kafka高可用-csdn博客',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"高扩展探究",frontmatter:{title:"高扩展探究",date:"2024-09-18T13:29:40.000Z",permalink:"/pages/168766/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/08.%E5%9B%9B%E3%80%81%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87/25.%E9%AB%98%E6%89%A9%E5%B1%95%E6%8E%A2%E7%A9%B6.html",relativePath:"02.Kafka  系统设计/08.四、设计目标/25.高扩展探究.md",key:"v-37efdbda",path:"/pages/168766/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"高并发探究",frontmatter:{title:"高并发探究",date:"2024-09-18T13:30:04.000Z",permalink:"/pages/dd50fc/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/08.%E5%9B%9B%E3%80%81%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87/35.%E9%AB%98%E5%B9%B6%E5%8F%91%E6%8E%A2%E7%A9%B6.html",relativePath:"02.Kafka  系统设计/08.四、设计目标/35.高并发探究.md",key:"v-571418de",path:"/pages/dd50fc/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"Kafka高性能探究",slug:"kafka高性能探究",normalizedTitle:"kafka高性能探究",charIndex:9},{level:3,title:"异步发送",slug:"异步发送",normalizedTitle:"异步发送",charIndex:77},{level:3,title:"批量发送",slug:"批量发送",normalizedTitle:"批量发送",charIndex:85},{level:3,title:"压缩技术",slug:"压缩技术",normalizedTitle:"压缩技术",charIndex:93},{level:3,title:"Pagecache 机制&顺序追加落盘",slug:"pagecache-机制-顺序追加落盘",normalizedTitle:"pagecache 机制&amp;顺序追加落盘",charIndex:null},{level:3,title:"零拷贝",slug:"零拷贝",normalizedTitle:"零拷贝",charIndex:124},{level:3,title:"稀疏索引",slug:"稀疏索引",normalizedTitle:"稀疏索引",charIndex:131},{level:3,title:"Broker & 数据分区",slug:"broker-数据分区",normalizedTitle:"broker &amp; 数据分区",charIndex:null},{level:3,title:"多 Reactor 多线程网络模型",slug:"多-reactor-多线程网络模型",normalizedTitle:"多 reactor 多线程网络模型",charIndex:156},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4142},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:4149}],headersStr:"前言 Kafka高性能探究 异步发送 批量发送 压缩技术 Pagecache 机制&顺序追加落盘 零拷贝 稀疏索引 Broker & 数据分区 多 Reactor 多线程网络模型 总结 参考资料",content:"# 前言\n\n\n# Kafka高性能探究\n\nKafka 高性能的核心是保障系统低延迟、高吞吐地处理消息，为此，Kafaka 采用了许多精妙的设计：\n\n * 异步发送\n * 批量发送\n * 压缩技术\n * Pagecache 机制&顺序追加落盘\n * 零拷贝\n * 稀疏索引\n * Broker & 数据分区\n * 多 Reactor 多线程网络模型\n\n\n# 异步发送\n\n如上文所述，Kafka 提供了异步和同步两种消息发送方式。在异步发送中，整个流程都是异步的。调用异步发送方法后，消息会被写入 Channel，然后立即返回成功。Dispatcher 协程会从 Channel 轮询消息，将其发送到 Broker，同时会有另一个异步协程负责处理 Broker 返回的结果。同步发送本质上也是异步的，但是在处理结果时，同步发送通过 WaitGroup 将异步操作转换为同步。使用异步发送可以最大化提高消息发送的吞吐能力。\n\n\n# 批量发送\n\nKafka 支持批量发送消息，将多个消息打包成一个批次进行发送，从而减少网络传输的开销，提高网络传输的效率和吞吐量。\n\nKafka 的批量发送消息是通过以下两个参数来控制的：\n\n\\1. Batch.size：控制批量发送消息的大小，默认值为 16KB，可适当增加 Batch.size 参数值提升吞吐。但是，需要注意的是，如果批量发送的大小设置得过大，可能会导致消息发送的延迟增加，因此需要根据实际情况进行调整。\n\n\\2. linger.ms：控制消息在批量发送前的等待时间，默认值为0。当 Linger.ms 大于0时，如果有消息发送，Kafka 会等待指定的时间，如果等待时间到达或者批量大小达到 Batch.size，就会将消息打包成一个批次进行发送。可适当增加 Linger.ms 参数值提升吞吐，比如10～100。\n\n在 Kafka 的生产者客户端中，当发送消息时，如果启用了批量发送，Kafka 会将消息缓存到缓冲区中。当缓冲区中的消息大小达到 Batch.size 或者等待时间到达 Linger.ms 时，Kafka 会将缓冲区中的消息打包成一个批次进行发送。如果在等待时间内没有达到 Batch.size，Kafka 也会将缓冲区中的消息发送出去，从而避免消息积压。\n\n\n# 压缩技术\n\nKafka 支持压缩技术，通过将消息进行压缩后再进行传输，从而减少网络传输的开销(压缩和解压缩的过程会消耗一定的 CPU 资源，因此需要根据实际情况进行调整。)，提高网络传输的效率和吞吐量。Kafka 支持多种压缩算法，在 Kafka 2.1.0版本之前，仅支持 GZIP，Snappy 和 LZ4，2.1.0 后还支持 Zstandard 算法（Facebook 开源，能够提供超高压缩比）。这些压缩算法性能对比（两指标都是越高越好）如下：\n\n * 吞吐量：LZ4>Snappy>zstd 和 GZIP，压缩比：zstd>LZ4>GZIP>Snappy。\n\n在 Kafka 中，压缩技术是通过以下两个参数来控制的：\n\n\\1. Compression.type：控制压缩算法的类型，默认值为None，表示不进行压缩。\n\n\\2. Compression.level：控制压缩的级别，取值范围为0-9，默认值为-1。当值为-1时，表示使用默认的压缩级别。\n\n在 Kafka 的生产者客户端中，当发送消息时，如果启用了压缩技术，Kafka 会将消息进行压缩后再进行传输。在消费者客户端中，如果消息进行了压缩，Kafka 会在消费消息时将其解压缩。注意：Broker 如果设置了和生产者不通的压缩算法，接收消息后会解压后重新压缩保存。Broker 如果存在消息版本兼容也会触发解压后再压缩。\n\n\n# Pagecache 机制&顺序追加落盘\n\nKafka 为了提升系统吞吐、降低时延，Broker 接收到消息后只是将数据写入PageCache后便认为消息已写入成功，而 PageCache 中的数据通过 Linux 的 Flusher 程序进行异步刷盘（避免了同步刷盘的巨大系统开销），将数据顺序追加写到磁盘日志文件中。由于 Pagecache 是在内存中进行缓存，因此读写速度非常快，可以大大提高读写效率。顺序追加写充分利用顺序 I/O 写操作，避免了缓慢的随机 I/O 操作，可有效提升 Kafka 吞吐。\n\n\n\n如上图所示，消息被顺序追加到每个分区日志文件的尾部。\n\n\n# 零拷贝\n\nKafka 中存在大量的网络数据持久化到磁盘（Producer 到 Broker）和磁盘文件通过网络发送（Broker 到 Consumer）的过程，这一过程的性能直接影响 Kafka 的整体吞吐量。传统的 IO 操作存在多次数据拷贝和上下文切换，性能比较低。Kafka 利用零拷贝技术提升上述过程性能，其中网络数据持久化磁盘主要用 mmap 技术，网络数据传输环节主要使用 Sendfile 技术。\n\n网络数据持久化磁盘之mmap\n\n传统模式下，数据从网络传输到文件需要 4 次数据拷贝、4 次上下文切换和两次系统调用。如下图所示：\n\n\n\n为了减少上下文切换以及数据拷贝带来的性能开销，Broker 在对 Producer 传来的网络数据进行持久化时使用了 mmap 技术。通过这种技术手段， Broker 读取到 Socket Buffer 的网络数据，可以直接在内核空间完成落盘，没有必要将 Socket Buffer 的网络数据读取到应用进程缓冲区。\n\n\n\n网络数据传输之 Sendfile\n\n传统方式实现：先读取磁盘、再用 Socket 发送，实际也是进过四次 Copy。如下图所示\n\n\n\n为了减少上下文切换以及数据拷贝带来的性能开销，Kafka 在 Consumer 从 Broker 读数据过程中使用了 Sendfile 技术。具体在这里采用的方案是通过 NIO 的 transferTo/transferFrom 调用操作系统的 Sendfile 实现零拷贝。总共发生 2 次内核数据拷贝、2 次上下文切换和一次系统调用，消除了 CPU 数据拷贝，如下：\n\n\n\n\n# 稀疏索引\n\n为了方便对日志进行检索和过期清理，Kafka 日志文件除了有用于存储日志的.log文件，还有一个位移索引文件.index 和一个时间戳索引文件.timeindex 文件，并且三文件的名字完全相同，如下：\n\n\n\nKafka 的索引文件是按照稀疏索引的思想进行设计的。稀疏索引的核心是不会为每个记录都保存索引，而是写入一定的记录之后才会增加一个索引值，具体这个间隔有多大则通过 log.index.interval.bytes 参数进行控制，默认大小为 4 KB，意味着 Kafka 至少写入 4KB 消息数据之后，才会在索引文件中增加一个索引项。可见，单条消息大小会影响 Kakfa 索引的插入频率，因此 log.index.interval.bytes 也是 Kafka 调优一个重要参数值。由于索引文件也是按照消息的顺序性进行增加索引项的，因此 Kafka 可以利用二分查找算法来搜索目标索引项，把时间复杂度降到了 O(lgN)，大大减少了查找的时间。\n\n位移索引文件.index\n\n位移索引文件的索引项结构如下：\n\n\n\n相对位移：保存于索引文件名字上面的起始位移的差值，假设一个索引文件为：00000000000000000100.index，那么起始位移值即 100，当存储位移为 150 的消息索引时，在索引文件中的相对位移则为 150 - 100 = 50，这么做的好处是使用 4 字节保存位移即可，可以节省非常多的磁盘空间。\n\n文件物理位置：消息在 log 文件中保存的位置，也就是说 Kafka 可根据消息位移，通过位移索引文件快速找到消息在 Log 文件中的物理位置，有了该物理位置的值，我们就可以快速地从 Log 文件中找到对应的消息了。\n\n下面我用图来表示 Kafka 是如何快速检索消息：\n\n\n\n假设 Kafka 需要找出位移为 3550 的消息，那么 Kafka 首先会使用二分查找算法找到小于 3550 的最大索引项：[3528, 2310272]，得到索引项之后，Kafka 会根据该索引项的文件物理位置在 Log 文件中从位置 2310272 开始顺序查找，直至找到位移为 3550 的消息记录为止。\n\n时间戳索引文件.timeindex\n\nKafka 在 0.10.0.0 以后的版本当中，消息中增加了时间戳信息，为了满足用户需要根据时间戳查询消息记录，Kafka 增加了时间戳索引文件，时间戳索引文件的索引项结构如下：\n\n\n\n时间戳索引文件的检索与位移索引文件类似，如下快速检索消息示意图：\n\n\n\n\n# Broker & 数据分区\n\nKafka 集群包含多个 Broker。一个 Topic下通常有多个 Partition，Partition 分布在不同的 Broker 上，用于存储 Topic 的消息，这使 Kafka 可以在多台机器上处理、存储消息，给 Kafka 提供给了并行的消息处理能力和横向扩容能力。\n\n\n# 多 Reactor 多线程网络模型\n\n多 Reactor 多线程网络模型 是一种高效的网络通信模型，可以充分利用多核 CPU 的性能，提高系统的吞吐量和响应速度。Kafka 为了提升系统的吞吐，在 Broker 端处理消息时采用了该模型，示意如下：\n\n\n\nSocketServer 和 KafkaRequestHandlerPool 是其中最重要的两个组件：\n\n * SocketServer：实现 Reactor 模式，用于处理多个 Client（包括客户端和其他 Broker 节点）的并发请求，并将处理结果返回给 Client。\n * KafkaRequestHandlerPool：Reactor 模式中的 Worker 线程池，里面定义了多个工作线程，用于处理实际的 I/O 请求逻辑。\n\n\n# 总结\n\n\n# 参考资料",normalizedContent:"# 前言\n\n\n# kafka高性能探究\n\nkafka 高性能的核心是保障系统低延迟、高吞吐地处理消息，为此，kafaka 采用了许多精妙的设计：\n\n * 异步发送\n * 批量发送\n * 压缩技术\n * pagecache 机制&顺序追加落盘\n * 零拷贝\n * 稀疏索引\n * broker & 数据分区\n * 多 reactor 多线程网络模型\n\n\n# 异步发送\n\n如上文所述，kafka 提供了异步和同步两种消息发送方式。在异步发送中，整个流程都是异步的。调用异步发送方法后，消息会被写入 channel，然后立即返回成功。dispatcher 协程会从 channel 轮询消息，将其发送到 broker，同时会有另一个异步协程负责处理 broker 返回的结果。同步发送本质上也是异步的，但是在处理结果时，同步发送通过 waitgroup 将异步操作转换为同步。使用异步发送可以最大化提高消息发送的吞吐能力。\n\n\n# 批量发送\n\nkafka 支持批量发送消息，将多个消息打包成一个批次进行发送，从而减少网络传输的开销，提高网络传输的效率和吞吐量。\n\nkafka 的批量发送消息是通过以下两个参数来控制的：\n\n\\1. batch.size：控制批量发送消息的大小，默认值为 16kb，可适当增加 batch.size 参数值提升吞吐。但是，需要注意的是，如果批量发送的大小设置得过大，可能会导致消息发送的延迟增加，因此需要根据实际情况进行调整。\n\n\\2. linger.ms：控制消息在批量发送前的等待时间，默认值为0。当 linger.ms 大于0时，如果有消息发送，kafka 会等待指定的时间，如果等待时间到达或者批量大小达到 batch.size，就会将消息打包成一个批次进行发送。可适当增加 linger.ms 参数值提升吞吐，比如10～100。\n\n在 kafka 的生产者客户端中，当发送消息时，如果启用了批量发送，kafka 会将消息缓存到缓冲区中。当缓冲区中的消息大小达到 batch.size 或者等待时间到达 linger.ms 时，kafka 会将缓冲区中的消息打包成一个批次进行发送。如果在等待时间内没有达到 batch.size，kafka 也会将缓冲区中的消息发送出去，从而避免消息积压。\n\n\n# 压缩技术\n\nkafka 支持压缩技术，通过将消息进行压缩后再进行传输，从而减少网络传输的开销(压缩和解压缩的过程会消耗一定的 cpu 资源，因此需要根据实际情况进行调整。)，提高网络传输的效率和吞吐量。kafka 支持多种压缩算法，在 kafka 2.1.0版本之前，仅支持 gzip，snappy 和 lz4，2.1.0 后还支持 zstandard 算法（facebook 开源，能够提供超高压缩比）。这些压缩算法性能对比（两指标都是越高越好）如下：\n\n * 吞吐量：lz4>snappy>zstd 和 gzip，压缩比：zstd>lz4>gzip>snappy。\n\n在 kafka 中，压缩技术是通过以下两个参数来控制的：\n\n\\1. compression.type：控制压缩算法的类型，默认值为none，表示不进行压缩。\n\n\\2. compression.level：控制压缩的级别，取值范围为0-9，默认值为-1。当值为-1时，表示使用默认的压缩级别。\n\n在 kafka 的生产者客户端中，当发送消息时，如果启用了压缩技术，kafka 会将消息进行压缩后再进行传输。在消费者客户端中，如果消息进行了压缩，kafka 会在消费消息时将其解压缩。注意：broker 如果设置了和生产者不通的压缩算法，接收消息后会解压后重新压缩保存。broker 如果存在消息版本兼容也会触发解压后再压缩。\n\n\n# pagecache 机制&顺序追加落盘\n\nkafka 为了提升系统吞吐、降低时延，broker 接收到消息后只是将数据写入pagecache后便认为消息已写入成功，而 pagecache 中的数据通过 linux 的 flusher 程序进行异步刷盘（避免了同步刷盘的巨大系统开销），将数据顺序追加写到磁盘日志文件中。由于 pagecache 是在内存中进行缓存，因此读写速度非常快，可以大大提高读写效率。顺序追加写充分利用顺序 i/o 写操作，避免了缓慢的随机 i/o 操作，可有效提升 kafka 吞吐。\n\n\n\n如上图所示，消息被顺序追加到每个分区日志文件的尾部。\n\n\n# 零拷贝\n\nkafka 中存在大量的网络数据持久化到磁盘（producer 到 broker）和磁盘文件通过网络发送（broker 到 consumer）的过程，这一过程的性能直接影响 kafka 的整体吞吐量。传统的 io 操作存在多次数据拷贝和上下文切换，性能比较低。kafka 利用零拷贝技术提升上述过程性能，其中网络数据持久化磁盘主要用 mmap 技术，网络数据传输环节主要使用 sendfile 技术。\n\n网络数据持久化磁盘之mmap\n\n传统模式下，数据从网络传输到文件需要 4 次数据拷贝、4 次上下文切换和两次系统调用。如下图所示：\n\n\n\n为了减少上下文切换以及数据拷贝带来的性能开销，broker 在对 producer 传来的网络数据进行持久化时使用了 mmap 技术。通过这种技术手段， broker 读取到 socket buffer 的网络数据，可以直接在内核空间完成落盘，没有必要将 socket buffer 的网络数据读取到应用进程缓冲区。\n\n\n\n网络数据传输之 sendfile\n\n传统方式实现：先读取磁盘、再用 socket 发送，实际也是进过四次 copy。如下图所示\n\n\n\n为了减少上下文切换以及数据拷贝带来的性能开销，kafka 在 consumer 从 broker 读数据过程中使用了 sendfile 技术。具体在这里采用的方案是通过 nio 的 transferto/transferfrom 调用操作系统的 sendfile 实现零拷贝。总共发生 2 次内核数据拷贝、2 次上下文切换和一次系统调用，消除了 cpu 数据拷贝，如下：\n\n\n\n\n# 稀疏索引\n\n为了方便对日志进行检索和过期清理，kafka 日志文件除了有用于存储日志的.log文件，还有一个位移索引文件.index 和一个时间戳索引文件.timeindex 文件，并且三文件的名字完全相同，如下：\n\n\n\nkafka 的索引文件是按照稀疏索引的思想进行设计的。稀疏索引的核心是不会为每个记录都保存索引，而是写入一定的记录之后才会增加一个索引值，具体这个间隔有多大则通过 log.index.interval.bytes 参数进行控制，默认大小为 4 kb，意味着 kafka 至少写入 4kb 消息数据之后，才会在索引文件中增加一个索引项。可见，单条消息大小会影响 kakfa 索引的插入频率，因此 log.index.interval.bytes 也是 kafka 调优一个重要参数值。由于索引文件也是按照消息的顺序性进行增加索引项的，因此 kafka 可以利用二分查找算法来搜索目标索引项，把时间复杂度降到了 o(lgn)，大大减少了查找的时间。\n\n位移索引文件.index\n\n位移索引文件的索引项结构如下：\n\n\n\n相对位移：保存于索引文件名字上面的起始位移的差值，假设一个索引文件为：00000000000000000100.index，那么起始位移值即 100，当存储位移为 150 的消息索引时，在索引文件中的相对位移则为 150 - 100 = 50，这么做的好处是使用 4 字节保存位移即可，可以节省非常多的磁盘空间。\n\n文件物理位置：消息在 log 文件中保存的位置，也就是说 kafka 可根据消息位移，通过位移索引文件快速找到消息在 log 文件中的物理位置，有了该物理位置的值，我们就可以快速地从 log 文件中找到对应的消息了。\n\n下面我用图来表示 kafka 是如何快速检索消息：\n\n\n\n假设 kafka 需要找出位移为 3550 的消息，那么 kafka 首先会使用二分查找算法找到小于 3550 的最大索引项：[3528, 2310272]，得到索引项之后，kafka 会根据该索引项的文件物理位置在 log 文件中从位置 2310272 开始顺序查找，直至找到位移为 3550 的消息记录为止。\n\n时间戳索引文件.timeindex\n\nkafka 在 0.10.0.0 以后的版本当中，消息中增加了时间戳信息，为了满足用户需要根据时间戳查询消息记录，kafka 增加了时间戳索引文件，时间戳索引文件的索引项结构如下：\n\n\n\n时间戳索引文件的检索与位移索引文件类似，如下快速检索消息示意图：\n\n\n\n\n# broker & 数据分区\n\nkafka 集群包含多个 broker。一个 topic下通常有多个 partition，partition 分布在不同的 broker 上，用于存储 topic 的消息，这使 kafka 可以在多台机器上处理、存储消息，给 kafka 提供给了并行的消息处理能力和横向扩容能力。\n\n\n# 多 reactor 多线程网络模型\n\n多 reactor 多线程网络模型 是一种高效的网络通信模型，可以充分利用多核 cpu 的性能，提高系统的吞吐量和响应速度。kafka 为了提升系统的吞吐，在 broker 端处理消息时采用了该模型，示意如下：\n\n\n\nsocketserver 和 kafkarequesthandlerpool 是其中最重要的两个组件：\n\n * socketserver：实现 reactor 模式，用于处理多个 client（包括客户端和其他 broker 节点）的并发请求，并将处理结果返回给 client。\n * kafkarequesthandlerpool：reactor 模式中的 worker 线程池，里面定义了多个工作线程，用于处理实际的 i/o 请求逻辑。\n\n\n# 总结\n\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"介绍",frontmatter:{title:"介绍",date:"2024-09-15T21:10:47.000Z",permalink:"/pages/4601ca/"},regularPath:"/03.Nginx%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.Nginx%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E4%BB%8B%E7%BB%8D.html",relativePath:"03.Nginx 系统设计/01.Nginx 系统设计/01.介绍.md",key:"v-ac71d918",path:"/pages/4601ca/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"碎碎念",frontmatter:{title:"碎碎念",date:"2024-09-15T01:18:31.000Z",permalink:"/pages/52ebd8/"},regularPath:"/06.%E5%8A%A8%E6%80%81/01.%E7%A2%8E%E7%A2%8E%E5%BF%B5.html",relativePath:"06.动态/01.碎碎念.md",key:"v-103a83f1",path:"/pages/52ebd8/",headers:[{level:2,title:"如何写好一篇文档",slug:"如何写好一篇文档",normalizedTitle:"如何写好一篇文档",charIndex:2},{level:2,title:"如何拥有持续输出的能力",slug:"如何拥有持续输出的能力",normalizedTitle:"如何拥有持续输出的能力",charIndex:345}],headersStr:"如何写好一篇文档 如何拥有持续输出的能力",content:"# 如何写好一篇文档\n\n 1.  总分总结构\n 2.  多用图片 数据 代码块 引用文献\n 3.  排版简约 避免一段文字超过 8 行 尽量有主句\n 4.  每发一篇文章之前，至少要看 5 篇类似的优质文章做调研，如果找不到，就别发了\n 5.  要有自己的特色，我认为我要在文章中多穿插问答题，以及多插入自己的见解，例如：echo 认为 xxx\n 6.  把每一个栏目做好做精致，切勿急于求成，把观众留下\n 7.  每一篇文章末尾都要搜集相关问答\n 8.  在开篇设置悬念是一个很好的办法\n 9.  定期进行 “Code Review”\n 10. AI 审查 建议收藏：2024新版GPT/Claude超详细论文润色指南「更新至24-08-03」_chatgpt润色-CSDN博客\n\n\n# 如何拥有持续输出的能力\n\n 1. 对每一个栏目有一个总的理解，然后对该栏目需要发布哪些内容有个抽象的理解，然后对每一个抽象去做调研\n 2. 多看优质文章，学习他们的思路与表达方式\n 3. 学习英语，外网也有很多优质的文章\n 4. 写好自己的每一篇文章，每一篇文章都要做到最优",normalizedContent:"# 如何写好一篇文档\n\n 1.  总分总结构\n 2.  多用图片 数据 代码块 引用文献\n 3.  排版简约 避免一段文字超过 8 行 尽量有主句\n 4.  每发一篇文章之前，至少要看 5 篇类似的优质文章做调研，如果找不到，就别发了\n 5.  要有自己的特色，我认为我要在文章中多穿插问答题，以及多插入自己的见解，例如：echo 认为 xxx\n 6.  把每一个栏目做好做精致，切勿急于求成，把观众留下\n 7.  每一篇文章末尾都要搜集相关问答\n 8.  在开篇设置悬念是一个很好的办法\n 9.  定期进行 “code review”\n 10. ai 审查 建议收藏：2024新版gpt/claude超详细论文润色指南「更新至24-08-03」_chatgpt润色-csdn博客\n\n\n# 如何拥有持续输出的能力\n\n 1. 对每一个栏目有一个总的理解，然后对该栏目需要发布哪些内容有个抽象的理解，然后对每一个抽象去做调研\n 2. 多看优质文章，学习他们的思路与表达方式\n 3. 学习英语，外网也有很多优质的文章\n 4. 写好自己的每一篇文章，每一篇文章都要做到最优",charsets:{cjk:!0},lastUpdated:"2024/09/17, 11:51:50",lastUpdatedTimestamp:172657391e4},{title:"归档",frontmatter:{archivesPage:!0,title:"归档",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-61d0fb85",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/16, 16:44:31",lastUpdatedTimestamp:1726505071e3},{title:"指南",frontmatter:{title:"指南",date:"2024-09-18T19:53:25.000Z",permalink:"/pages/bfab10/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80/05.%E6%8C%87%E5%8D%97.html",relativePath:"Netty 系统设计/05.一、前言/05.指南.md",key:"v-f3015bc0",path:"/pages/bfab10/",headers:[{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:2},{level:2,title:"阅读方法",slug:"阅读方法",normalizedTitle:"阅读方法",charIndex:80},{level:2,title:"学习资料",slug:"学习资料",normalizedTitle:"学习资料",charIndex:218}],headersStr:"前置知识 阅读方法 学习资料",content:"# 前置知识\n\n * 计算机网络，TCP，socket\n * 多线程\n * IO 模型：阻塞 IO，非阻塞IO\n * 对 Java 面向对象有一定理解\n\n\n# 阅读方法\n\n 1. 阅读「 基础知识」\n 2. 阅读「主线任务」\n 3. 阅读「深入 Netty核心」\n 4. 阅读「深入 Netty 内存管理」\n 5. 阅读「支线任务」\n 6. 上述中可能会有引用「基础知识」，请务必及时阅读\n 7. 首页底部可加入群聊，一起讨论\n\n\n# 学习资料\n\n * Netty: Home\n * 微信公众号：bin的技术小屋\n * yongshun/learn_netty_source_code: Netty 源码分析教程 (github.com)\n * 45 张图深度解析 Netty 架构与原理-腾讯云开发者社区-腾讯云 (tencent.com)\n * Netty 学习手册 (dongzl.github.io)\n * zouhuanli's blog – zouhuanli's blog. Java的Native化，道阻且长\n * God-Of-BigData/Netty/Netty源码解析-概述篇.md at master · wangzhiwubigdata/God-Of-BigData (github.com)\n * netty源码| ProcessOn免费在线作图,在线流程图,在线思维导图\n * Gregorius的博客 | Gregorius (gregoriusxu.github.io)\n * Netty 源码剖析与实战 (geekbang.org)\n * [Netty 核心原理剖析与 RPC 实践-完 (lianglianglee.com)](https://learn.lianglianglee.com/专栏/Netty 核心原理剖析与 RPC 实践-完)\n * Netty.docs: Related articles",normalizedContent:"# 前置知识\n\n * 计算机网络，tcp，socket\n * 多线程\n * io 模型：阻塞 io，非阻塞io\n * 对 java 面向对象有一定理解\n\n\n# 阅读方法\n\n 1. 阅读「 基础知识」\n 2. 阅读「主线任务」\n 3. 阅读「深入 netty核心」\n 4. 阅读「深入 netty 内存管理」\n 5. 阅读「支线任务」\n 6. 上述中可能会有引用「基础知识」，请务必及时阅读\n 7. 首页底部可加入群聊，一起讨论\n\n\n# 学习资料\n\n * netty: home\n * 微信公众号：bin的技术小屋\n * yongshun/learn_netty_source_code: netty 源码分析教程 (github.com)\n * 45 张图深度解析 netty 架构与原理-腾讯云开发者社区-腾讯云 (tencent.com)\n * netty 学习手册 (dongzl.github.io)\n * zouhuanli's blog – zouhuanli's blog. java的native化，道阻且长\n * god-of-bigdata/netty/netty源码解析-概述篇.md at master · wangzhiwubigdata/god-of-bigdata (github.com)\n * netty源码| processon免费在线作图,在线流程图,在线思维导图\n * gregorius的博客 | gregorius (gregoriusxu.github.io)\n * netty 源码剖析与实战 (geekbang.org)\n * [netty 核心原理剖析与 rpc 实践-完 (lianglianglee.com)](https://learn.lianglianglee.com/专栏/netty 核心原理剖析与 rpc 实践-完)\n * netty.docs: related articles",charsets:{cjk:!0},lastUpdated:"2024/09/19, 03:26:41",lastUpdatedTimestamp:1726716401e3},{title:"IO 多路复用详解",frontmatter:{title:"IO 多路复用详解",date:"2024-09-19T09:17:34.000Z",permalink:"/pages/c0367e/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/10.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/07.IO%20%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E8%AF%A6%E8%A7%A3.html",relativePath:"Netty 系统设计/10.二、基础知识/07.IO 多路复用详解.md",key:"v-f57141ac",path:"/pages/c0367e/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"select",slug:"select",normalizedTitle:"select",charIndex:851},{level:3,title:"API介绍",slug:"api介绍",normalizedTitle:"api介绍",charIndex:1965},{level:3,title:"性能开销",slug:"性能开销",normalizedTitle:"性能开销",charIndex:783},{level:2,title:"poll",slug:"poll",normalizedTitle:"poll",charIndex:3728},{level:2,title:"epoll",slug:"epoll",normalizedTitle:"epoll",charIndex:4392},{level:3,title:"Socket的创建",slug:"socket的创建",normalizedTitle:"socket的创建",charIndex:4780},{level:4,title:"进程中管理文件列表结构",slug:"进程中管理文件列表结构",normalizedTitle:"进程中管理文件列表结构",charIndex:5053},{level:4,title:"Socket内核结构",slug:"socket内核结构",normalizedTitle:"socket内核结构",charIndex:6318},{level:3,title:"阻塞IO中用户进程阻塞以及唤醒原理",slug:"阻塞io中用户进程阻塞以及唤醒原理",normalizedTitle:"阻塞io中用户进程阻塞以及唤醒原理",charIndex:8687},{level:3,title:"epoll_create创建epoll对象",slug:"epoll-create创建epoll对象",normalizedTitle:"epoll_create创建epoll对象",charIndex:11974},{level:3,title:"epoll_ctl向epoll对象中添加监听的Socket",slug:"epoll-ctl向epoll对象中添加监听的socket",normalizedTitle:"epoll_ctl向epoll对象中添加监听的socket",charIndex:13129},{level:3,title:"epoll_wait同步阻塞获取IO就绪的Socket",slug:"epoll-wait同步阻塞获取io就绪的socket",normalizedTitle:"epoll_wait同步阻塞获取io就绪的socket",charIndex:15516},{level:3,title:"再谈水平触发和边缘触发",slug:"再谈水平触发和边缘触发",normalizedTitle:"再谈水平触发和边缘触发",charIndex:16654},{level:3,title:"epoll对select，poll的优化总结",slug:"epoll对select-poll的优化总结",normalizedTitle:"epoll对select，poll的优化总结",charIndex:17690}],headersStr:"前言 select API介绍 性能开销 poll epoll Socket的创建 进程中管理文件列表结构 Socket内核结构 阻塞IO中用户进程阻塞以及唤醒原理 epoll_create创建epoll对象 epoll_ctl向epoll对象中添加监听的Socket epoll_wait同步阻塞获取IO就绪的Socket 再谈水平触发和边缘触发 epoll对select，poll的优化总结",content:'# 前言\n\n在非阻塞IO这一小节的开头，我们提到网络IO模型的演变都是围绕着---如何用尽可能少的线程去处理更多的连接这个核心需求开始展开的。\n\n本小节我们来谈谈IO多路复用模型，那么什么是多路？，什么又是复用呢？\n\n我们还是以这个核心需求来对这两个概念展开阐述：\n\n * 多路：我们的核心需求是要用尽可能少的线程来处理尽可能多的连接，这里的多路指的就是我们需要处理的众多连接。\n * 复用：核心需求要求我们使用尽可能少的线程，尽可能少的系统开销去处理尽可能多的连接（多路），那么这里的复用指的就是用有限的资源，比如用一个线程或者固定数量的线程去处理众多连接上的读写事件。换句话说，在阻塞IO模型中一个连接就需要分配一个独立的线程去专门处理这个连接上的读写，到了IO多路复用模型中，多个连接可以复用这一个独立的线程去处理这多个连接上的读写。\n\n好了，IO多路复用模型的概念解释清楚了，那么问题的关键是我们如何去实现这个复用，也就是如何让一个独立的线程去处理众多连接上的读写事件呢？\n\n这个问题其实在非阻塞IO模型中已经给出了它的答案，在非阻塞IO模型中，利用非阻塞的系统IO调用去不断的轮询众多连接的Socket接收缓冲区看是否有数据到来，如果有则处理，如果没有则继续轮询下一个Socket。这样就达到了用一个线程去处理众多连接上的读写事件了。\n\n但是非阻塞IO模型最大的问题就是需要不断的发起系统调用去轮询各个Socket中的接收缓冲区是否有数据到来，频繁的系统调用随之带来了大量的上下文切换开销。随着并发量的提升，这样也会导致非常严重的性能问题。\n\n那么如何避免频繁的系统调用同时又可以实现我们的核心需求呢？\n\n这就需要操作系统的内核来支持这样的操作，我们可以把频繁的轮询操作交给操作系统内核来替我们完成，这样就避免了在用户空间频繁的去使用系统调用来轮询所带来的性能开销。\n\n正如我们所想，操作系统内核也确实为我们提供了这样的功能实现，下面我们来一起看下操作系统对IO多路复用模型的实现。\n\n\n# select\n\nselect是操作系统内核提供给我们使用的一个系统调用，它解决了在非阻塞IO模型中需要不断的发起系统IO调用去轮询各个连接上的Socket接收缓冲区所带来的用户空间与内核空间不断切换的系统开销。\n\nselect系统调用将轮询的操作交给了内核来帮助我们完成，从而避免了在用户空间不断的发起轮询所带来的的系统性能开销。\n\n\n\n * 首先用户线程在发起select系统调用的时候会阻塞在select系统调用上。此时，用户线程从用户态切换到了内核态完成了一次上下文切换\n\n * 用户线程将需要监听的Socket对应的文件描述符fd数组通过select系统调用传递给内核。此时，用户线程将用户空间中的文件描述符fd数组拷贝到内核空间。\n\n这里的文件描述符数组其实是一个BitMap，BitMap下标为文件描述符fd，下标对应的值为：1表示该fd上有读写事件，0表示该fd上没有读写事件。\n\n\n\n文件描述符fd其实就是一个整数值，在Linux中一切皆文件，Socket也是一个文件。描述进程所有信息的数据结构task_struct中有一个属性struct files_struct *files，它最终指向了一个数组，数组里存放了进程打开的所有文件列表，文件信息封装在struct file结构体中，这个数组存放的类型就是struct file结构体，数组的下标则是我们常说的文件描述符fd。\n\n * 当用户线程调用完select后开始进入阻塞状态，内核开始轮询遍历fd数组，查看fd对应的Socket接收缓冲区中是否有数据到来。如果有数据到来，则将fd对应BitMap的值设置为1。如果没有数据到来，则保持值为0。\n\n注意\n\n注意这里内核会修改原始的fd数组！！\n\n * 内核遍历一遍fd数组后，如果发现有些fd上有IO数据到来，则将修改后的fd数组返回给用户线程。此时，会将fd数组从内核空间拷贝到用户空间。\n\n * 当内核将修改后的fd数组返回给用户线程后，用户线程解除阻塞，由用户线程开始遍历fd数组然后找出fd数组中值为1的Socket文件描述符。最后对这些Socket发起系统调用读取数据。\n\n笔记\n\nselect不会告诉用户线程具体哪些fd上有IO数据到来，只是在IO活跃的fd上打上标记，将打好标记的完整fd数组返回给用户线程，所以用户线程还需要遍历fd数组找出具体哪些fd上有IO数据到来。\n\n * 由于内核在遍历的过程中已经修改了fd数组，所以在用户线程遍历完fd数组后获取到IO就绪的Socket后，就需要重置fd数组，并重新调用select传入重置后的fd数组，让内核发起新的一轮遍历轮询。\n\n\n# API介绍\n\n当我们熟悉了select的原理后，就很容易理解内核给我们提供的select API了。\n\n int select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout)\n\n\n从select API中我们可以看到，select系统调用是在规定的超时时间内，监听（轮询）用户感兴趣的文件描述符集合上的可读,可写,异常三类事件。\n\n * maxfdp1 ： select传递给内核监听的文件描述符集合中数值最大的文件描述符+1，目的是用于限定内核遍历范围。比如：select监听的文件描述符集合为{0,1,2,3,4}，那么maxfdp1的值为5。\n * fd_set *readset： 对可读事件感兴趣的文件描述符集合。\n * fd_set *writeset： 对可写事件感兴趣的文件描述符集合。\n * fd_set *exceptset：对异常事件感兴趣的文件描述符集合。\n\n> 这里的fd_set就是我们前边提到的文件描述符数组，是一个BitMap结构。\n\n * const struct timeval *timeout：select系统调用超时时间，在这段时间内，内核如果没有发现有IO就绪的文件描述符，就直接返回。\n\n上小节提到，在内核遍历完fd数组后，发现有IO就绪的fd，则会将该fd对应的BitMap中的值设置为1，并将修改后的fd数组，返回给用户线程。\n\n在用户线程中需要重新遍历fd数组，找出IO就绪的fd出来，然后发起真正的读写调用。\n\n下面介绍下在用户线程中重新遍历fd数组的过程中，我们需要用到的API：\n\n * void FD_ZERO(fd_set *fdset)：清空指定的文件描述符集合，即让fd_set中不在包含任何文件描述符。\n * void FD_SET(int fd, fd_set *fdset)：将一个给定的文件描述符加入集合之中。\n\n> 每次调用select之前都要通过FD_ZERO和FD_SET重新设置文件描述符，因为文件描述符集合会在内核中被修改。\n\n * int FD_ISSET(int fd, fd_set *fdset)：检查集合中指定的文件描述符是否可以读写。用户线程遍历文件描述符集合,调用该方法检查相应的文件描述符是否IO就绪。\n * void FD_CLR(int fd, fd_set *fdset)：将一个给定的文件描述符从集合中删除\n\n\n# 性能开销\n\n虽然select解决了非阻塞IO模型中频繁发起系统调用的问题，但是在整个select工作过程中，我们还是看出了select有些不足的地方。\n\n * 在发起select系统调用以及返回时，用户线程各发生了一次用户态到内核态以及内核态到用户态的上下文切换开销。发生2次上下文切换\n * 在发起select系统调用以及返回时，用户线程在内核态需要将文件描述符集合从用户空间拷贝到内核空间。以及在内核修改完文件描述符集合后，又要将它从内核空间拷贝到用户空间。发生2次文件描述符集合的拷贝\n * 虽然由原来在用户空间发起轮询优化成了在内核空间发起轮询但select不会告诉用户线程到底是哪些Socket上发生了IO就绪事件，只是对IO就绪的Socket作了标记，用户线程依然要遍历文件描述符集合去查找具体IO就绪的Socket。时间复杂度依然为O(n)。\n\n> 大部分情况下，网络连接并不总是活跃的，如果select监听了大量的客户端连接，只有少数的连接活跃，然而使用轮询的这种方式会随着连接数的增大，效率会越来越低。\n\n * 内核会对原始的文件描述符集合进行修改。导致每次在用户空间重新发起select调用时，都需要对文件描述符集合进行重置。\n * BitMap结构的文件描述符集合，长度为固定的1024,所以只能监听0~1023的文件描述符。\n * select系统调用 不是线程安全的。\n\n以上select的不足所产生的性能开销都会随着并发量的增大而线性增长。\n\n很明显select也不能解决C10K问题，只适用于1000个左右的并发连接场景。\n\n\n# poll\n\npoll相当于是改进版的select，但是工作原理基本和select没有本质的区别。\n\nint poll(struct pollfd *fds, unsigned int nfds, int timeout)\n    \nstruct pollfd {\n    int   fd;         /* 文件描述符 */\n    short events;     /* 需要监听的事件 */\n    short revents;    /* 实际发生的事件 由内核修改设置 */\n};\n\n\nselect中使用的文件描述符集合是采用的固定长度为1024的BitMap结构的fd_set，而poll换成了一个pollfd结构没有固定长度的数组，这样就没有了最大描述符数量的限制（当然还会受到系统文件描述符限制）\n\npoll只是改进了select只能监听1024个文件描述符的数量限制，但是并没有在性能方面做出改进。和select上本质并没有多大差别。\n\n * 同样需要在内核空间和用户空间中对文件描述符集合进行轮询，查找出IO就绪的Socket的时间复杂度依然为O(n)。\n * 同样需要将包含大量文件描述符的集合整体在用户空间和内核空间之间来回复制，无论这些文件描述符是否就绪。他们的开销都会随着文件描述符数量的增加而线性增大。\n * select，poll在每次新增，删除需要监听的socket时，都需要将整个新的socket集合全量传至内核。\n\npoll同样不适用高并发的场景。依然无法解决C10K问题。\n\n\n# epoll\n\n通过上边对select,poll核心原理的介绍，我们看到select,poll的性能瓶颈主要体现在下面三个地方：\n\n * 因为内核不会保存我们要监听的socket集合，所以在每次调用select,poll的时候都需要传入，传出全量的socket文件描述符集合。这导致了大量的文件描述符在用户空间和内核空间频繁的来回复制。\n * 由于内核不会通知具体IO就绪的socket，只是在这些IO就绪的socket上打好标记，所以当select系统调用返回时，在用户空间还是需要完整遍历一遍socket文件描述符集合来获取具体IO就绪的socket。\n * 在内核空间中也是通过遍历的方式来得到IO就绪的socket。\n\n下面我们来看下epoll是如何解决这些问题的。在介绍epoll的核心原理之前，我们需要介绍下理解epoll工作过程所需要的一些核心基础知识。\n\n\n# Socket的创建\n\n服务端线程调用accept系统调用后开始阻塞，当有客户端连接上来并完成TCP三次握手后，内核会创建一个对应的Socket作为服务端与客户端通信的内核接口。\n\n在Linux内核的角度看来，一切皆是文件，Socket也不例外，当内核创建出Socket之后，会将这个Socket放到当前进程所打开的文件列表中管理起来。\n\n下面我们来看下进程管理这些打开的文件列表相关的内核数据结构是什么样的？在了解完这些数据结构后，我们会更加清晰的理解Socket在内核中所发挥的作用。并且对后面我们理解epoll的创建过程有很大的帮助。\n\n# 进程中管理文件列表结构\n\n\n\nstruct tast_struct是内核中用来表示进程的一个数据结构，它包含了进程的所有信息。本小节我们只列出和文件管理相关的属性。\n\n其中进程内打开的所有文件是通过一个数组fd_array来进行组织管理，数组的下标即为我们常提到的文件描述符，数组中存放的是对应的文件数据结构struct file。每打开一个文件，内核都会创建一个struct file与之对应，并在fd_array中找到一个空闲位置分配给它，数组中对应的下标，就是我们在用户空间用到的文件描述符。\n\n> 对于任何一个进程，默认情况下，文件描述符 0表示 stdin 标准输入，文件描述符 1表示stdout 标准输出，文件描述符2表示stderr 标准错误输出。\n\n进程中打开的文件列表fd_array定义在内核数据结构struct files_struct中，在struct fdtable结构中有一个指针struct fd **fd指向fd_array。\n\n由于本小节讨论的是内核网络系统部分的数据结构，所以这里拿Socket文件类型来举例说明：\n\n用于封装文件元信息的内核数据结构struct file中的private_data指针指向具体的Socket结构。\n\nstruct file中的file_operations属性定义了文件的操作函数，不同的文件类型，对应的file_operations是不同的，针对Socket文件类型，这里的file_operations指向socket_file_ops。\n\n> 我们在用户空间对Socket发起的读写等系统调用，进入内核首先会调用的是Socket对应的struct file中指向的socket_file_ops。比如：对Socket发起write写操作，在内核中首先被调用的就是socket_file_ops中定义的sock_write_iter。Socket发起read读操作内核中对应的则是sock_read_iter。\n\nstatic const struct file_operations socket_file_ops = {\n  .owner =  THIS_MODULE,\n  .llseek =  no_llseek,\n  .read_iter =  sock_read_iter,\n  .write_iter =  sock_write_iter,\n  .poll =    sock_poll,\n  .unlocked_ioctl = sock_ioctl,\n  .mmap =    sock_mmap,\n  .release =  sock_close,\n  .fasync =  sock_fasync,\n  .sendpage =  sock_sendpage,\n  .splice_write = generic_splice_sendpage,\n  .splice_read =  sock_splice_read,\n};\n\n\n# Socket内核结构\n\n\n\n在我们进行网络程序的编写时会首先创建一个Socket，然后基于这个Socket进行bind，listen，我们先将这个Socket称作为监听Socket。\n\n 1. 当我们调用accept后，内核会基于监听Socket创建出来一个新的Socket专门用于与客户端之间的网络通信。并将监听Socket中的Socket操作函数集合（inet_stream_ops）ops赋值到新的Socket的ops属性中。\n\nconst struct proto_ops inet_stream_ops = {\n  .bind = inet_bind,\n  .connect = inet_stream_connect,\n  .accept = inet_accept,\n  .poll = tcp_poll,\n  .listen = inet_listen,\n  .sendmsg = inet_sendmsg,\n  .recvmsg = inet_recvmsg,\n  ......\n}\n\n\n> 这里需要注意的是，监听的 socket和真正用来网络通信的 Socket，是两个 Socket，一个叫作监听 Socket，一个叫作已连接的Socket。\n\n 2. 接着内核会为已连接的Socket创建struct file并初始化，并把Socket文件操作函数集合（socket_file_ops）赋值给struct file中的f_ops指针。然后将struct socket中的file指针指向这个新分配申请的struct file结构体。\n\n> 内核会维护两个队列：\n> \n>  * 一个是已经完成TCP三次握手，连接状态处于established的连接队列。内核中为icsk_accept_queue。\n>  * 一个是还没有完成TCP三次握手，连接状态处于syn_rcvd的半连接队列。\n\n 3. 然后调用socket->ops->accept，从Socket内核结构图中我们可以看到其实调用的是inet_accept，该函数会在icsk_accept_queue中查找是否有已经建立好的连接，如果有的话，直接从icsk_accept_queue中获取已经创建好的struct sock。并将这个struct sock对象赋值给struct socket中的sock指针。\n\nstruct sock在struct socket中是一个非常核心的内核对象，正是在这里定义了我们在介绍网络包的接收发送流程中提到的接收队列，发送队列，等待队列，数据就绪回调函数指针，内核协议栈操作函数集合\n\nstruct sock在struct socket中是一个非常核心的内核对象，正是在这里定义了我们在介绍网络包的接收发送流程中提到的接收队列，发送队列，等待队列，数据就绪回调函数指针，内核协议栈操作函数集合\n\n * 根据创建Socket时发起的系统调用sock_create中的protocol参数(对于TCP协议这里的参数值为SOCK_STREAM)查找到对于 tcp 定义的操作方法实现集合 inet_stream_ops 和tcp_prot。并把它们分别设置到socket->ops和sock->sk_prot上。\n\n> 这里可以回看下本小节开头的《Socket内核结构图》捋一下他们之间的关系。\n\n> socket相关的操作接口定义在inet_stream_ops函数集合中，负责对上给用户提供接口。而socket与内核协议栈之间的操作接口定义在struct sock中的sk_prot指针上，这里指向tcp_prot协议操作函数集合。\n\nstruct proto tcp_prot = {\n  .name      = "TCP",\n  .owner      = THIS_MODULE,\n  .close      = tcp_close,\n  .connect    = tcp_v4_connect,\n  .disconnect    = tcp_disconnect,\n  .accept      = inet_csk_accept,\n  .keepalive    = tcp_set_keepalive,\n  .recvmsg    = tcp_recvmsg,\n  .sendmsg    = tcp_sendmsg,\n  .backlog_rcv    = tcp_v4_do_rcv,\n   ......\n}\n\n\n> 之前提到的对Socket发起的系统IO调用，在内核中首先会调用Socket的文件结构struct file中的file_operations文件操作集合，然后调用struct socket中的ops指向的inet_stream_opssocket操作函数，最终调用到struct sock中sk_prot指针指向的tcp_prot内核协议栈操作函数接口集合。\n\n\n\n * 将struct sock 对象中的sk_data_ready 函数指针设置为 sock_def_readable，在Socket数据就绪的时候内核会回调该函数。\n * struct sock中的等待队列中存放的是系统IO调用发生阻塞的进程fd，以及相应的回调函数。记住这个地方，后边介绍epoll的时候我们还会提到！\n\n 4. 当struct file，struct socket，struct sock这些核心的内核对象创建好之后，最后就是把socket对象对应的struct file放到进程打开的文件列表fd_array中。随后系统调用accept返回socket的文件描述符fd给用户程序。\n\n\n# 阻塞IO中用户进程阻塞以及唤醒原理\n\n在前边小节我们介绍阻塞IO的时候提到，当用户进程发起系统IO调用时，这里我们拿read举例，用户进程会在内核态查看对应Socket接收缓冲区是否有数据到来。\n\n * Socket接收缓冲区有数据，则拷贝数据到用户空间，系统调用返回。\n * Socket接收缓冲区没有数据，则用户进程让出CPU进入阻塞状态，当数据到达接收缓冲区时，用户进程会被唤醒，从阻塞状态进入就绪状态，等待CPU调度。\n\n本小节我们就来看下用户进程是如何阻塞在Socket上，又是如何在Socket上被唤醒的。理解这个过程很重要，对我们理解epoll的事件通知过程很有帮助\n\n * 首先我们在用户进程中对Socket进行read系统调用时，用户进程会从用户态转为内核态。\n * 在进程的struct task_struct结构找到fd_array，并根据Socket的文件描述符fd找到对应的struct file，调用struct file中的文件操作函数结合file_operations，read系统调用对应的是sock_read_iter。\n * 在sock_read_iter函数中找到struct file指向的struct socket，并调用socket->ops->recvmsg，这里我们知道调用的是inet_stream_ops集合中定义的inet_recvmsg。\n * 在inet_recvmsg中会找到struct sock，并调用sock->skprot->recvmsg,这里调用的是tcp_prot集合中定义的tcp_recvmsg函数。\n\n> 整个调用过程可以参考上边的《系统IO调用结构图》\n\n熟悉了内核函数调用栈后，我们来看下系统IO调用在tcp_recvmsg内核函数中是如何将用户进程给阻塞掉的\n\n\n\nint tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n  size_t len, int nonblock, int flags, int *addr_len)\n{\n    .................省略非核心代码...............\n   //访问sock对象中定义的接收队列\n  skb_queue_walk(&sk->sk_receive_queue, skb) {\n\n    .................省略非核心代码...............\n\n  //没有收到足够数据，调用sk_wait_data 阻塞当前进程\n  sk_wait_data(sk, &timeo);\n}\n\n\nint sk_wait_data(struct sock *sk, long *timeo)\n{\n //创建struct sock中等待队列上的元素wait_queue_t\n //将进程描述符和回调函数autoremove_wake_function关联到wait_queue_t中\n DEFINE_WAIT(wait);\n\n // 调用 sk_sleep 获取 sock 对象下的等待队列的头指针wait_queue_head_t\n // 调用prepare_to_wait将新创建的等待项wait_queue_t插入到等待队列中，并将进程状态设置为可打断 INTERRUPTIBLE\n prepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n set_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);\n\n // 通过调用schedule_timeout让出CPU，然后进行睡眠，导致一次上下文切换\n rc = sk_wait_event(sk, timeo, !skb_queue_empty(&sk->sk_receive_queue));\n ...\n\n\n * 首先会在DEFINE_WAIT中创建struct sock中等待队列上的等待类型wait_queue_t。\n\n#define DEFINE_WAIT(name) DEFINE_WAIT_FUNC(name, autoremove_wake_function)\n\n#define DEFINE_WAIT_FUNC(name, function)    \\\n wait_queue_t name = {      \\\n  .private = current,    \\\n  .func  = function,    \\\n  .task_list = LIST_HEAD_INIT((name).task_list), \\\n }\n\n\n等待类型wait_queue_t中的private用来关联阻塞在当前socket上的用户进程fd。func用来关联等待项上注册的回调函数。这里注册的是autoremove_wake_function。\n\n * 调用sk_sleep(sk)获取struct sock对象中的等待队列头指针wait_queue_head_t。\n * 调用prepare_to_wait将新创建的等待项wait_queue_t插入到等待队列中，并将进程设置为可打断 INTERRUPTIBL。\n * 调用sk_wait_event让出CPU，进程进入睡眠状态。\n\n用户进程的阻塞过程我们就介绍完了，关键是要理解记住struct sock中定义的等待队列上的等待类型wait_queue_t的结构。后面epoll的介绍中我们还会用到它。\n\n下面我们接着介绍当数据就绪后，用户进程是如何被唤醒的\n\n在本文开始介绍《网络包接收过程》这一小节中我们提到：\n\n * 当网络数据包到达网卡时，网卡通过DMA的方式将数据放到RingBuffer中。\n * 然后向CPU发起硬中断，在硬中断响应程序中创建sk_buffer，并将网络数据拷贝至sk_buffer中。\n * 随后发起软中断，内核线程ksoftirqd响应软中断，调用poll函数将sk_buffer送往内核协议栈做层层协议处理。\n * 在传输层tcp_rcv 函数中，去掉TCP头，根据四元组（源IP，源端口，目的IP，目的端口）查找对应的Socket。\n * 最后将sk_buffer放到Socket中的接收队列里。\n\n上边这些过程是内核接收网络数据的完整过程，下边我们来看下，当数据包接收完毕后，用户进程是如何被唤醒的。\n\n\n\n * 当软中断将sk_buffer放到Socket的接收队列上时，接着就会调用数据就绪函数回调指针sk_data_ready，前边我们提到，这个函数指针在初始化的时候指向了sock_def_readable函数。\n\n * 在sock_def_readable函数中会去获取socket->sock->sk_wq等待队列。在wake_up_common函数中从等待队列sk_wq中找出一个等待项wait_queue_t，回调注册在该等待项上的func回调函数（wait_queue_t->func）,创建等待项wait_queue_t是我们提到，这里注册的回调函数是autoremove_wake_function。\n\n> 即使是有多个进程都阻塞在同一个 socket 上，也只唤醒 1 个进程。其作用是为了避免惊群。\n\n * 在autoremove_wake_function函数中，根据等待项wait_queue_t上的private关联的阻塞进程fd调用try_to_wake_up唤醒阻塞在该Socket上的进程。\n\n> 记住wait_queue_t中的func函数指针，在epoll中这里会注册epoll的回调函数。\n\n现在理解epoll所需要的基础知识我们就介绍完了，唠叨了这么多，下面终于正式进入本小节的主题epoll了。\n\n\n# epoll_create创建epoll对象\n\nepoll_create是内核提供给我们创建epoll对象的一个系统调用，当我们在用户进程中调用epoll_create时，内核会为我们创建一个struct eventpoll对象，并且也有相应的struct file与之关联，同样需要把这个struct eventpoll对象所关联的struct file放入进程打开的文件列表fd_array中管理。\n\n> 熟悉了Socket的创建逻辑，epoll的创建逻辑也就不难理解了。\n\n> struct eventpoll对象关联的struct file中的file_operations 指针指向的是eventpoll_fops操作函数集合。\n\nstatic const struct file_operations eventpoll_fops = {\n     .release = ep_eventpoll_release;\n     .poll = ep_eventpoll_poll,\n}\n\n\n\n\nstruct eventpoll {\n\n    //等待队列，阻塞在epoll上的进程会放在这里\n    wait_queue_head_t wq;\n\n    //就绪队列，IO就绪的socket连接会放在这里\n    struct list_head rdllist;\n\n    //红黑树用来管理所有监听的socket连接\n    struct rb_root rbr;\n\n    ......\n}\n\n\n * wait_queue_head_t wq：epoll中的等待队列，队列里存放的是阻塞在epoll上的用户进程。在IO就绪的时候epoll可以通过这个队列找到这些阻塞的进程并唤醒它们，从而执行IO调用读写Socket上的数据。\n\n> 这里注意与Socket中的等待队列区分！！！\n\n * struct list_head rdllist：epoll中的就绪队列，队列里存放的是都是IO就绪的Socket，被唤醒的用户进程可以直接读取这个队列获取IO活跃的Socket。无需再次遍历整个Socket集合。\n\n> 这里正是epoll比select ，poll高效之处，select ，poll返回的是全部的socket连接，我们需要在用户空间再次遍历找出真正IO活跃的Socket连接。而epoll只是返回IO活跃的Socket连接。用户进程可以直接进行IO操作。\n\n * struct rb_root rbr : 由于红黑树在查找，插入，删除等综合性能方面是最优的，所以epoll内部使用一颗红黑树来管理海量的Socket连接。\n\n> select用数组管理连接，poll用链表管理连接。\n\n\n# epoll_ctl向epoll对象中添加监听的Socket\n\n当我们调用epoll_create在内核中创建出epoll对象struct eventpoll后，我们就可以利用epoll_ctl向epoll中添加我们需要管理的Socket连接了。\n\n 1. 首先要在epoll内核中创建一个表示Socket连接的数据结构struct epitem，而在epoll中为了综合性能的考虑，采用一颗红黑树来管理这些海量socket连接。所以struct epitem是一个红黑树节点。\n\n\n\nstruct epitem\n{\n    //指向所属epoll对象\n    struct eventpoll *ep; \n    //注册的感兴趣的事件,也就是用户空间的epoll_event     \n    struct epoll_event event; \n    //指向epoll对象中的就绪队列\n    struct list_head rdllink;  \n    //指向epoll中对应的红黑树节点\n    struct rb_node rbn;     \n    //指向epitem所表示的socket->file结构以及对应的fd\n    struct epoll_filefd ffd;                  \n}\n\n\n> 这里重点记住struct epitem结构中的rdllink以及epoll_filefd成员，后面我们会用到。\n\n 2. 在内核中创建完表示Socket连接的数据结构struct epitem后，我们就需要在Socket中的等待队列上创建等待项wait_queue_t并且注册epoll的回调函数ep_poll_callback。\n\n通过《阻塞IO中用户进程阻塞以及唤醒原理》小节的铺垫，我想大家已经猜到这一步的意义所在了吧！当时在等待项wait_queue_t中注册的是autoremove_wake_function回调函数。还记得吗？\n\n> epoll的回调函数ep_poll_callback正是epoll同步IO事件通知机制的核心所在，也是区别于select，poll采用内核轮询方式的根本性能差异所在。\n\n\n\n这里又出现了一个新的数据结构struct eppoll_entry，那它的作用是干什么的呢？大家可以结合上图先猜测下它的作用!\n\n我们知道socket->sock->sk_wq等待队列中的类型是wait_queue_t，我们需要在struct epitem所表示的socket的等待队列上注册epoll回调函数ep_poll_callback。\n\n这样当数据到达socket中的接收队列时，内核会回调sk_data_ready，在阻塞IO中用户进程阻塞以及唤醒原理这一小节中，我们知道这个sk_data_ready函数指针会指向sk_def_readable函数，在sk_def_readable中会回调注册在等待队列里的等待项wait_queue_t -> func回调函数ep_poll_callback。在ep_poll_callback中需要找到epitem，将IO就绪的epitem放入epoll中的就绪队列中。\n\n而socket等待队列中类型是wait_queue_t无法关联到epitem。所以就出现了struct eppoll_entry结构体，它的作用就是关联Socket等待队列中的等待项wait_queue_t和epitem。\n\nstruct eppoll_entry { \n    //指向关联的epitem\n    struct epitem *base; \n\n    // 关联监听socket中等待队列中的等待项 (private = null  func = ep_poll_callback)\n    wait_queue_t wait;   \n\n    // 监听socket中等待队列头指针\n    wait_queue_head_t *whead; \n    .........\n}; \n\n\n这样在ep_poll_callback回调函数中就可以根据Socket等待队列中的等待项wait，通过container_of宏找到eppoll_entry，继而找到epitem了。\n\n> container_of在Linux内核中是一个常用的宏，用于从包含在某个结构中的指针获得结构本身的指针，通俗地讲就是通过结构体变量中某个成员的首地址进而获得整个结构体变量的首地址。\n\n> 这里需要注意下这次等待项wait_queue_t中的private设置的是null，因为这里Socket是交给epoll来管理的，阻塞在Socket上的进程是也由epoll来唤醒。在等待项wait_queue_t注册的func是ep_poll_callback而不是autoremove_wake_function，阻塞进程并不需要autoremove_wake_function来唤醒，所以这里设置private为null\n\n 3. 当在Socket的等待队列中创建好等待项wait_queue_t并且注册了epoll的回调函数ep_poll_callback，然后又通过eppoll_entry关联了epitem后。剩下要做的就是将epitem插入到epoll中的红黑树struct rb_root rbr中。\n\n> 这里可以看到epoll另一个优化的地方，epoll将所有的socket连接通过内核中的红黑树来集中管理。每次添加或者删除socket连接都是增量添加删除，而不是像select，poll那样每次调用都是全量socket连接集合传入内核。避免了频繁大量的内存拷贝。\n\n\n# epoll_wait同步阻塞获取IO就绪的Socket\n\n 1. 用户程序调用epoll_wait后，内核首先会查找epoll中的就绪队列eventpoll->rdllist是否有IO就绪的epitem。epitem里封装了socket的信息。如果就绪队列中有就绪的epitem，就将就绪的socket信息封装到epoll_event返回。\n 2. 如果eventpoll->rdllist就绪队列中没有IO就绪的epitem，则会创建等待项wait_queue_t，将用户进程的fd关联到wait_queue_t->private上，并在等待项wait_queue_t->func上注册回调函数default_wake_function。最后将等待项添加到epoll中的等待队列中。用户进程让出CPU，进入阻塞状态。\n\n\n\n> 这里和阻塞IO模型中的阻塞原理是一样的，只不过在阻塞IO模型中注册到等待项wait_queue_t->func上的是autoremove_wake_function，并将等待项添加到socket中的等待队列中。这里注册的是default_wake_function，将等待项添加到epoll中的等待队列上。\n\n\n\n 3. 前边做了那么多的知识铺垫，下面终于到了epoll的整个工作流程了：\n\n\n\n * 当网络数据包在软中断中经过内核协议栈的处理到达socket的接收缓冲区时，紧接着会调用socket的数据就绪回调指针sk_data_ready，回调函数为sock_def_readable。在socket的等待队列中找出等待项，其中等待项中注册的回调函数为ep_poll_callback。\n * 在回调函数ep_poll_callback中，根据struct eppoll_entry中的struct wait_queue_t wait通过container_of宏找到eppoll_entry对象并通过它的base指针找到封装socket的数据结构struct epitem，并将它加入到epoll中的就绪队列rdllist中。\n * 随后查看epoll中的等待队列中是否有等待项，也就是说查看是否有进程阻塞在epoll_wait上等待IO就绪的socket。如果没有等待项，则软中断处理完成。\n * 如果有等待项，则回到注册在等待项中的回调函数default_wake_function,在回调函数中唤醒阻塞进程，并将就绪队列rdllist中的epitem的IO就绪socket信息封装到struct epoll_event中返回。\n * 用户进程拿到epoll_event获取IO就绪的socket，发起系统IO调用读取数据。\n\n\n# 再谈水平触发和边缘触发\n\n网上有大量的关于这两种模式的讲解，大部分讲的比较模糊，感觉只是强行从概念上进行描述，看完让人难以理解。所以在这里，笔者想结合上边epoll的工作过程，再次对这两种模式做下自己的解读，力求清晰的解释出这两种工作模式的异同。\n\n经过上边对epoll工作过程的详细解读，我们知道，当我们监听的socket上有数据到来时，软中断会执行epoll的回调函数ep_poll_callback,在回调函数中会将epoll中描述socket信息的数据结构epitem插入到epoll中的就绪队列rdllist中。随后用户进程从epoll的等待队列中被唤醒，epoll_wait将IO就绪的socket返回给用户进程，随即epoll_wait会清空rdllist。\n\n水平触发和边缘触发最关键的区别就在于当socket中的接收缓冲区还有数据可读时。epoll_wait是否会清空rdllist。\n\n * 水平触发：在这种模式下，用户线程调用epoll_wait获取到IO就绪的socket后，对Socket进行系统IO调用读取数据，假设socket中的数据只读了一部分没有全部读完，这时再次调用epoll_wait，epoll_wait会检查这些Socket中的接收缓冲区是否还有数据可读，如果还有数据可读，就将socket重新放回rdllist。所以当socket上的IO没有被处理完时，再次调用epoll_wait依然可以获得这些socket，用户进程可以接着处理socket上的IO事件。\n * 边缘触发： 在这种模式下，epoll_wait就会直接清空rdllist，不管socket上是否还有数据可读。所以在边缘触发模式下，当你没有来得及处理socket接收缓冲区的剩下可读数据时，再次调用epoll_wait，因为这时rdlist已经被清空了，socket不会再次从epoll_wait中返回，所以用户进程就不会再次获得这个socket了，也就无法在对它进行IO处理了。除非，这个socket上有新的IO数据到达，根据epoll的工作过程，该socket会被再次放入rdllist中。\n\n> 如果你在边缘触发模式下，处理了部分socket上的数据，那么想要处理剩下部分的数据，就只能等到这个socket上再次有网络数据到达。\n\n在Netty中实现的EpollSocketChannel默认的就是边缘触发模式。JDK的NIO默认是水平触发模式。\n\n\n# epoll对select，poll的优化总结\n\n * epoll在内核中通过红黑树管理海量的连接，所以在调用epoll_wait获取IO就绪的socket时，不需要传入监听的socket文件描述符。从而避免了海量的文件描述符集合在用户空间和内核空间中来回复制。\n\n> select，poll每次调用时都需要传递全量的文件描述符集合，导致大量频繁的拷贝操作。\n\n * epoll仅会通知IO就绪的socket。避免了在用户空间遍历的开销。\n\n> select，poll只会在IO就绪的socket上打好标记，依然是全量返回，所以在用户空间还需要用户程序在一次遍历全量集合找出具体IO就绪的socket。\n\n * epoll通过在socket的等待队列上注册回调函数ep_poll_callback通知用户程序IO就绪的socket。避免了在内核中轮询的开销。\n\n> 大部分情况下socket上并不总是IO活跃的，在面对海量连接的情况下，select，poll采用内核轮询的方式获取IO活跃的socket，无疑是性能低下的核心原因。\n\n根据以上epoll的性能优势，它是目前为止各大主流网络框架，以及反向代理中间件使用到的网络IO模型。\n\n利用epoll多路复用IO模型可以轻松的解决C10K问题。\n\nC100k的解决方案也还是基于C10K的方案，通过epoll 配合线程池，再加上 CPU、内存和网络接口的性能和容量提升。大部分情况下，C100K很自然就可以达到。\n\n甚至C1000K的解决方法，本质上还是构建在 epoll 的多路复用 I/O 模型上。只不过，除了 I/O 模型之外，还需要从应用程序到 Linux 内核、再到 CPU、内存和网络等各个层次的深度优化，特别是需要借助硬件，来卸载那些原来通过软件处理的大量功能（去掉大量的中断响应开销，以及内核协议栈处理的开销）。',normalizedContent:'# 前言\n\n在非阻塞io这一小节的开头，我们提到网络io模型的演变都是围绕着---如何用尽可能少的线程去处理更多的连接这个核心需求开始展开的。\n\n本小节我们来谈谈io多路复用模型，那么什么是多路？，什么又是复用呢？\n\n我们还是以这个核心需求来对这两个概念展开阐述：\n\n * 多路：我们的核心需求是要用尽可能少的线程来处理尽可能多的连接，这里的多路指的就是我们需要处理的众多连接。\n * 复用：核心需求要求我们使用尽可能少的线程，尽可能少的系统开销去处理尽可能多的连接（多路），那么这里的复用指的就是用有限的资源，比如用一个线程或者固定数量的线程去处理众多连接上的读写事件。换句话说，在阻塞io模型中一个连接就需要分配一个独立的线程去专门处理这个连接上的读写，到了io多路复用模型中，多个连接可以复用这一个独立的线程去处理这多个连接上的读写。\n\n好了，io多路复用模型的概念解释清楚了，那么问题的关键是我们如何去实现这个复用，也就是如何让一个独立的线程去处理众多连接上的读写事件呢？\n\n这个问题其实在非阻塞io模型中已经给出了它的答案，在非阻塞io模型中，利用非阻塞的系统io调用去不断的轮询众多连接的socket接收缓冲区看是否有数据到来，如果有则处理，如果没有则继续轮询下一个socket。这样就达到了用一个线程去处理众多连接上的读写事件了。\n\n但是非阻塞io模型最大的问题就是需要不断的发起系统调用去轮询各个socket中的接收缓冲区是否有数据到来，频繁的系统调用随之带来了大量的上下文切换开销。随着并发量的提升，这样也会导致非常严重的性能问题。\n\n那么如何避免频繁的系统调用同时又可以实现我们的核心需求呢？\n\n这就需要操作系统的内核来支持这样的操作，我们可以把频繁的轮询操作交给操作系统内核来替我们完成，这样就避免了在用户空间频繁的去使用系统调用来轮询所带来的性能开销。\n\n正如我们所想，操作系统内核也确实为我们提供了这样的功能实现，下面我们来一起看下操作系统对io多路复用模型的实现。\n\n\n# select\n\nselect是操作系统内核提供给我们使用的一个系统调用，它解决了在非阻塞io模型中需要不断的发起系统io调用去轮询各个连接上的socket接收缓冲区所带来的用户空间与内核空间不断切换的系统开销。\n\nselect系统调用将轮询的操作交给了内核来帮助我们完成，从而避免了在用户空间不断的发起轮询所带来的的系统性能开销。\n\n\n\n * 首先用户线程在发起select系统调用的时候会阻塞在select系统调用上。此时，用户线程从用户态切换到了内核态完成了一次上下文切换\n\n * 用户线程将需要监听的socket对应的文件描述符fd数组通过select系统调用传递给内核。此时，用户线程将用户空间中的文件描述符fd数组拷贝到内核空间。\n\n这里的文件描述符数组其实是一个bitmap，bitmap下标为文件描述符fd，下标对应的值为：1表示该fd上有读写事件，0表示该fd上没有读写事件。\n\n\n\n文件描述符fd其实就是一个整数值，在linux中一切皆文件，socket也是一个文件。描述进程所有信息的数据结构task_struct中有一个属性struct files_struct *files，它最终指向了一个数组，数组里存放了进程打开的所有文件列表，文件信息封装在struct file结构体中，这个数组存放的类型就是struct file结构体，数组的下标则是我们常说的文件描述符fd。\n\n * 当用户线程调用完select后开始进入阻塞状态，内核开始轮询遍历fd数组，查看fd对应的socket接收缓冲区中是否有数据到来。如果有数据到来，则将fd对应bitmap的值设置为1。如果没有数据到来，则保持值为0。\n\n注意\n\n注意这里内核会修改原始的fd数组！！\n\n * 内核遍历一遍fd数组后，如果发现有些fd上有io数据到来，则将修改后的fd数组返回给用户线程。此时，会将fd数组从内核空间拷贝到用户空间。\n\n * 当内核将修改后的fd数组返回给用户线程后，用户线程解除阻塞，由用户线程开始遍历fd数组然后找出fd数组中值为1的socket文件描述符。最后对这些socket发起系统调用读取数据。\n\n笔记\n\nselect不会告诉用户线程具体哪些fd上有io数据到来，只是在io活跃的fd上打上标记，将打好标记的完整fd数组返回给用户线程，所以用户线程还需要遍历fd数组找出具体哪些fd上有io数据到来。\n\n * 由于内核在遍历的过程中已经修改了fd数组，所以在用户线程遍历完fd数组后获取到io就绪的socket后，就需要重置fd数组，并重新调用select传入重置后的fd数组，让内核发起新的一轮遍历轮询。\n\n\n# api介绍\n\n当我们熟悉了select的原理后，就很容易理解内核给我们提供的select api了。\n\n int select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout)\n\n\n从select api中我们可以看到，select系统调用是在规定的超时时间内，监听（轮询）用户感兴趣的文件描述符集合上的可读,可写,异常三类事件。\n\n * maxfdp1 ： select传递给内核监听的文件描述符集合中数值最大的文件描述符+1，目的是用于限定内核遍历范围。比如：select监听的文件描述符集合为{0,1,2,3,4}，那么maxfdp1的值为5。\n * fd_set *readset： 对可读事件感兴趣的文件描述符集合。\n * fd_set *writeset： 对可写事件感兴趣的文件描述符集合。\n * fd_set *exceptset：对异常事件感兴趣的文件描述符集合。\n\n> 这里的fd_set就是我们前边提到的文件描述符数组，是一个bitmap结构。\n\n * const struct timeval *timeout：select系统调用超时时间，在这段时间内，内核如果没有发现有io就绪的文件描述符，就直接返回。\n\n上小节提到，在内核遍历完fd数组后，发现有io就绪的fd，则会将该fd对应的bitmap中的值设置为1，并将修改后的fd数组，返回给用户线程。\n\n在用户线程中需要重新遍历fd数组，找出io就绪的fd出来，然后发起真正的读写调用。\n\n下面介绍下在用户线程中重新遍历fd数组的过程中，我们需要用到的api：\n\n * void fd_zero(fd_set *fdset)：清空指定的文件描述符集合，即让fd_set中不在包含任何文件描述符。\n * void fd_set(int fd, fd_set *fdset)：将一个给定的文件描述符加入集合之中。\n\n> 每次调用select之前都要通过fd_zero和fd_set重新设置文件描述符，因为文件描述符集合会在内核中被修改。\n\n * int fd_isset(int fd, fd_set *fdset)：检查集合中指定的文件描述符是否可以读写。用户线程遍历文件描述符集合,调用该方法检查相应的文件描述符是否io就绪。\n * void fd_clr(int fd, fd_set *fdset)：将一个给定的文件描述符从集合中删除\n\n\n# 性能开销\n\n虽然select解决了非阻塞io模型中频繁发起系统调用的问题，但是在整个select工作过程中，我们还是看出了select有些不足的地方。\n\n * 在发起select系统调用以及返回时，用户线程各发生了一次用户态到内核态以及内核态到用户态的上下文切换开销。发生2次上下文切换\n * 在发起select系统调用以及返回时，用户线程在内核态需要将文件描述符集合从用户空间拷贝到内核空间。以及在内核修改完文件描述符集合后，又要将它从内核空间拷贝到用户空间。发生2次文件描述符集合的拷贝\n * 虽然由原来在用户空间发起轮询优化成了在内核空间发起轮询但select不会告诉用户线程到底是哪些socket上发生了io就绪事件，只是对io就绪的socket作了标记，用户线程依然要遍历文件描述符集合去查找具体io就绪的socket。时间复杂度依然为o(n)。\n\n> 大部分情况下，网络连接并不总是活跃的，如果select监听了大量的客户端连接，只有少数的连接活跃，然而使用轮询的这种方式会随着连接数的增大，效率会越来越低。\n\n * 内核会对原始的文件描述符集合进行修改。导致每次在用户空间重新发起select调用时，都需要对文件描述符集合进行重置。\n * bitmap结构的文件描述符集合，长度为固定的1024,所以只能监听0~1023的文件描述符。\n * select系统调用 不是线程安全的。\n\n以上select的不足所产生的性能开销都会随着并发量的增大而线性增长。\n\n很明显select也不能解决c10k问题，只适用于1000个左右的并发连接场景。\n\n\n# poll\n\npoll相当于是改进版的select，但是工作原理基本和select没有本质的区别。\n\nint poll(struct pollfd *fds, unsigned int nfds, int timeout)\n    \nstruct pollfd {\n    int   fd;         /* 文件描述符 */\n    short events;     /* 需要监听的事件 */\n    short revents;    /* 实际发生的事件 由内核修改设置 */\n};\n\n\nselect中使用的文件描述符集合是采用的固定长度为1024的bitmap结构的fd_set，而poll换成了一个pollfd结构没有固定长度的数组，这样就没有了最大描述符数量的限制（当然还会受到系统文件描述符限制）\n\npoll只是改进了select只能监听1024个文件描述符的数量限制，但是并没有在性能方面做出改进。和select上本质并没有多大差别。\n\n * 同样需要在内核空间和用户空间中对文件描述符集合进行轮询，查找出io就绪的socket的时间复杂度依然为o(n)。\n * 同样需要将包含大量文件描述符的集合整体在用户空间和内核空间之间来回复制，无论这些文件描述符是否就绪。他们的开销都会随着文件描述符数量的增加而线性增大。\n * select，poll在每次新增，删除需要监听的socket时，都需要将整个新的socket集合全量传至内核。\n\npoll同样不适用高并发的场景。依然无法解决c10k问题。\n\n\n# epoll\n\n通过上边对select,poll核心原理的介绍，我们看到select,poll的性能瓶颈主要体现在下面三个地方：\n\n * 因为内核不会保存我们要监听的socket集合，所以在每次调用select,poll的时候都需要传入，传出全量的socket文件描述符集合。这导致了大量的文件描述符在用户空间和内核空间频繁的来回复制。\n * 由于内核不会通知具体io就绪的socket，只是在这些io就绪的socket上打好标记，所以当select系统调用返回时，在用户空间还是需要完整遍历一遍socket文件描述符集合来获取具体io就绪的socket。\n * 在内核空间中也是通过遍历的方式来得到io就绪的socket。\n\n下面我们来看下epoll是如何解决这些问题的。在介绍epoll的核心原理之前，我们需要介绍下理解epoll工作过程所需要的一些核心基础知识。\n\n\n# socket的创建\n\n服务端线程调用accept系统调用后开始阻塞，当有客户端连接上来并完成tcp三次握手后，内核会创建一个对应的socket作为服务端与客户端通信的内核接口。\n\n在linux内核的角度看来，一切皆是文件，socket也不例外，当内核创建出socket之后，会将这个socket放到当前进程所打开的文件列表中管理起来。\n\n下面我们来看下进程管理这些打开的文件列表相关的内核数据结构是什么样的？在了解完这些数据结构后，我们会更加清晰的理解socket在内核中所发挥的作用。并且对后面我们理解epoll的创建过程有很大的帮助。\n\n# 进程中管理文件列表结构\n\n\n\nstruct tast_struct是内核中用来表示进程的一个数据结构，它包含了进程的所有信息。本小节我们只列出和文件管理相关的属性。\n\n其中进程内打开的所有文件是通过一个数组fd_array来进行组织管理，数组的下标即为我们常提到的文件描述符，数组中存放的是对应的文件数据结构struct file。每打开一个文件，内核都会创建一个struct file与之对应，并在fd_array中找到一个空闲位置分配给它，数组中对应的下标，就是我们在用户空间用到的文件描述符。\n\n> 对于任何一个进程，默认情况下，文件描述符 0表示 stdin 标准输入，文件描述符 1表示stdout 标准输出，文件描述符2表示stderr 标准错误输出。\n\n进程中打开的文件列表fd_array定义在内核数据结构struct files_struct中，在struct fdtable结构中有一个指针struct fd **fd指向fd_array。\n\n由于本小节讨论的是内核网络系统部分的数据结构，所以这里拿socket文件类型来举例说明：\n\n用于封装文件元信息的内核数据结构struct file中的private_data指针指向具体的socket结构。\n\nstruct file中的file_operations属性定义了文件的操作函数，不同的文件类型，对应的file_operations是不同的，针对socket文件类型，这里的file_operations指向socket_file_ops。\n\n> 我们在用户空间对socket发起的读写等系统调用，进入内核首先会调用的是socket对应的struct file中指向的socket_file_ops。比如：对socket发起write写操作，在内核中首先被调用的就是socket_file_ops中定义的sock_write_iter。socket发起read读操作内核中对应的则是sock_read_iter。\n\nstatic const struct file_operations socket_file_ops = {\n  .owner =  this_module,\n  .llseek =  no_llseek,\n  .read_iter =  sock_read_iter,\n  .write_iter =  sock_write_iter,\n  .poll =    sock_poll,\n  .unlocked_ioctl = sock_ioctl,\n  .mmap =    sock_mmap,\n  .release =  sock_close,\n  .fasync =  sock_fasync,\n  .sendpage =  sock_sendpage,\n  .splice_write = generic_splice_sendpage,\n  .splice_read =  sock_splice_read,\n};\n\n\n# socket内核结构\n\n\n\n在我们进行网络程序的编写时会首先创建一个socket，然后基于这个socket进行bind，listen，我们先将这个socket称作为监听socket。\n\n 1. 当我们调用accept后，内核会基于监听socket创建出来一个新的socket专门用于与客户端之间的网络通信。并将监听socket中的socket操作函数集合（inet_stream_ops）ops赋值到新的socket的ops属性中。\n\nconst struct proto_ops inet_stream_ops = {\n  .bind = inet_bind,\n  .connect = inet_stream_connect,\n  .accept = inet_accept,\n  .poll = tcp_poll,\n  .listen = inet_listen,\n  .sendmsg = inet_sendmsg,\n  .recvmsg = inet_recvmsg,\n  ......\n}\n\n\n> 这里需要注意的是，监听的 socket和真正用来网络通信的 socket，是两个 socket，一个叫作监听 socket，一个叫作已连接的socket。\n\n 2. 接着内核会为已连接的socket创建struct file并初始化，并把socket文件操作函数集合（socket_file_ops）赋值给struct file中的f_ops指针。然后将struct socket中的file指针指向这个新分配申请的struct file结构体。\n\n> 内核会维护两个队列：\n> \n>  * 一个是已经完成tcp三次握手，连接状态处于established的连接队列。内核中为icsk_accept_queue。\n>  * 一个是还没有完成tcp三次握手，连接状态处于syn_rcvd的半连接队列。\n\n 3. 然后调用socket->ops->accept，从socket内核结构图中我们可以看到其实调用的是inet_accept，该函数会在icsk_accept_queue中查找是否有已经建立好的连接，如果有的话，直接从icsk_accept_queue中获取已经创建好的struct sock。并将这个struct sock对象赋值给struct socket中的sock指针。\n\nstruct sock在struct socket中是一个非常核心的内核对象，正是在这里定义了我们在介绍网络包的接收发送流程中提到的接收队列，发送队列，等待队列，数据就绪回调函数指针，内核协议栈操作函数集合\n\nstruct sock在struct socket中是一个非常核心的内核对象，正是在这里定义了我们在介绍网络包的接收发送流程中提到的接收队列，发送队列，等待队列，数据就绪回调函数指针，内核协议栈操作函数集合\n\n * 根据创建socket时发起的系统调用sock_create中的protocol参数(对于tcp协议这里的参数值为sock_stream)查找到对于 tcp 定义的操作方法实现集合 inet_stream_ops 和tcp_prot。并把它们分别设置到socket->ops和sock->sk_prot上。\n\n> 这里可以回看下本小节开头的《socket内核结构图》捋一下他们之间的关系。\n\n> socket相关的操作接口定义在inet_stream_ops函数集合中，负责对上给用户提供接口。而socket与内核协议栈之间的操作接口定义在struct sock中的sk_prot指针上，这里指向tcp_prot协议操作函数集合。\n\nstruct proto tcp_prot = {\n  .name      = "tcp",\n  .owner      = this_module,\n  .close      = tcp_close,\n  .connect    = tcp_v4_connect,\n  .disconnect    = tcp_disconnect,\n  .accept      = inet_csk_accept,\n  .keepalive    = tcp_set_keepalive,\n  .recvmsg    = tcp_recvmsg,\n  .sendmsg    = tcp_sendmsg,\n  .backlog_rcv    = tcp_v4_do_rcv,\n   ......\n}\n\n\n> 之前提到的对socket发起的系统io调用，在内核中首先会调用socket的文件结构struct file中的file_operations文件操作集合，然后调用struct socket中的ops指向的inet_stream_opssocket操作函数，最终调用到struct sock中sk_prot指针指向的tcp_prot内核协议栈操作函数接口集合。\n\n\n\n * 将struct sock 对象中的sk_data_ready 函数指针设置为 sock_def_readable，在socket数据就绪的时候内核会回调该函数。\n * struct sock中的等待队列中存放的是系统io调用发生阻塞的进程fd，以及相应的回调函数。记住这个地方，后边介绍epoll的时候我们还会提到！\n\n 4. 当struct file，struct socket，struct sock这些核心的内核对象创建好之后，最后就是把socket对象对应的struct file放到进程打开的文件列表fd_array中。随后系统调用accept返回socket的文件描述符fd给用户程序。\n\n\n# 阻塞io中用户进程阻塞以及唤醒原理\n\n在前边小节我们介绍阻塞io的时候提到，当用户进程发起系统io调用时，这里我们拿read举例，用户进程会在内核态查看对应socket接收缓冲区是否有数据到来。\n\n * socket接收缓冲区有数据，则拷贝数据到用户空间，系统调用返回。\n * socket接收缓冲区没有数据，则用户进程让出cpu进入阻塞状态，当数据到达接收缓冲区时，用户进程会被唤醒，从阻塞状态进入就绪状态，等待cpu调度。\n\n本小节我们就来看下用户进程是如何阻塞在socket上，又是如何在socket上被唤醒的。理解这个过程很重要，对我们理解epoll的事件通知过程很有帮助\n\n * 首先我们在用户进程中对socket进行read系统调用时，用户进程会从用户态转为内核态。\n * 在进程的struct task_struct结构找到fd_array，并根据socket的文件描述符fd找到对应的struct file，调用struct file中的文件操作函数结合file_operations，read系统调用对应的是sock_read_iter。\n * 在sock_read_iter函数中找到struct file指向的struct socket，并调用socket->ops->recvmsg，这里我们知道调用的是inet_stream_ops集合中定义的inet_recvmsg。\n * 在inet_recvmsg中会找到struct sock，并调用sock->skprot->recvmsg,这里调用的是tcp_prot集合中定义的tcp_recvmsg函数。\n\n> 整个调用过程可以参考上边的《系统io调用结构图》\n\n熟悉了内核函数调用栈后，我们来看下系统io调用在tcp_recvmsg内核函数中是如何将用户进程给阻塞掉的\n\n\n\nint tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n  size_t len, int nonblock, int flags, int *addr_len)\n{\n    .................省略非核心代码...............\n   //访问sock对象中定义的接收队列\n  skb_queue_walk(&sk->sk_receive_queue, skb) {\n\n    .................省略非核心代码...............\n\n  //没有收到足够数据，调用sk_wait_data 阻塞当前进程\n  sk_wait_data(sk, &timeo);\n}\n\n\nint sk_wait_data(struct sock *sk, long *timeo)\n{\n //创建struct sock中等待队列上的元素wait_queue_t\n //将进程描述符和回调函数autoremove_wake_function关联到wait_queue_t中\n define_wait(wait);\n\n // 调用 sk_sleep 获取 sock 对象下的等待队列的头指针wait_queue_head_t\n // 调用prepare_to_wait将新创建的等待项wait_queue_t插入到等待队列中，并将进程状态设置为可打断 interruptible\n prepare_to_wait(sk_sleep(sk), &wait, task_interruptible);\n set_bit(sock_async_waitdata, &sk->sk_socket->flags);\n\n // 通过调用schedule_timeout让出cpu，然后进行睡眠，导致一次上下文切换\n rc = sk_wait_event(sk, timeo, !skb_queue_empty(&sk->sk_receive_queue));\n ...\n\n\n * 首先会在define_wait中创建struct sock中等待队列上的等待类型wait_queue_t。\n\n#define define_wait(name) define_wait_func(name, autoremove_wake_function)\n\n#define define_wait_func(name, function)    \\\n wait_queue_t name = {      \\\n  .private = current,    \\\n  .func  = function,    \\\n  .task_list = list_head_init((name).task_list), \\\n }\n\n\n等待类型wait_queue_t中的private用来关联阻塞在当前socket上的用户进程fd。func用来关联等待项上注册的回调函数。这里注册的是autoremove_wake_function。\n\n * 调用sk_sleep(sk)获取struct sock对象中的等待队列头指针wait_queue_head_t。\n * 调用prepare_to_wait将新创建的等待项wait_queue_t插入到等待队列中，并将进程设置为可打断 interruptibl。\n * 调用sk_wait_event让出cpu，进程进入睡眠状态。\n\n用户进程的阻塞过程我们就介绍完了，关键是要理解记住struct sock中定义的等待队列上的等待类型wait_queue_t的结构。后面epoll的介绍中我们还会用到它。\n\n下面我们接着介绍当数据就绪后，用户进程是如何被唤醒的\n\n在本文开始介绍《网络包接收过程》这一小节中我们提到：\n\n * 当网络数据包到达网卡时，网卡通过dma的方式将数据放到ringbuffer中。\n * 然后向cpu发起硬中断，在硬中断响应程序中创建sk_buffer，并将网络数据拷贝至sk_buffer中。\n * 随后发起软中断，内核线程ksoftirqd响应软中断，调用poll函数将sk_buffer送往内核协议栈做层层协议处理。\n * 在传输层tcp_rcv 函数中，去掉tcp头，根据四元组（源ip，源端口，目的ip，目的端口）查找对应的socket。\n * 最后将sk_buffer放到socket中的接收队列里。\n\n上边这些过程是内核接收网络数据的完整过程，下边我们来看下，当数据包接收完毕后，用户进程是如何被唤醒的。\n\n\n\n * 当软中断将sk_buffer放到socket的接收队列上时，接着就会调用数据就绪函数回调指针sk_data_ready，前边我们提到，这个函数指针在初始化的时候指向了sock_def_readable函数。\n\n * 在sock_def_readable函数中会去获取socket->sock->sk_wq等待队列。在wake_up_common函数中从等待队列sk_wq中找出一个等待项wait_queue_t，回调注册在该等待项上的func回调函数（wait_queue_t->func）,创建等待项wait_queue_t是我们提到，这里注册的回调函数是autoremove_wake_function。\n\n> 即使是有多个进程都阻塞在同一个 socket 上，也只唤醒 1 个进程。其作用是为了避免惊群。\n\n * 在autoremove_wake_function函数中，根据等待项wait_queue_t上的private关联的阻塞进程fd调用try_to_wake_up唤醒阻塞在该socket上的进程。\n\n> 记住wait_queue_t中的func函数指针，在epoll中这里会注册epoll的回调函数。\n\n现在理解epoll所需要的基础知识我们就介绍完了，唠叨了这么多，下面终于正式进入本小节的主题epoll了。\n\n\n# epoll_create创建epoll对象\n\nepoll_create是内核提供给我们创建epoll对象的一个系统调用，当我们在用户进程中调用epoll_create时，内核会为我们创建一个struct eventpoll对象，并且也有相应的struct file与之关联，同样需要把这个struct eventpoll对象所关联的struct file放入进程打开的文件列表fd_array中管理。\n\n> 熟悉了socket的创建逻辑，epoll的创建逻辑也就不难理解了。\n\n> struct eventpoll对象关联的struct file中的file_operations 指针指向的是eventpoll_fops操作函数集合。\n\nstatic const struct file_operations eventpoll_fops = {\n     .release = ep_eventpoll_release;\n     .poll = ep_eventpoll_poll,\n}\n\n\n\n\nstruct eventpoll {\n\n    //等待队列，阻塞在epoll上的进程会放在这里\n    wait_queue_head_t wq;\n\n    //就绪队列，io就绪的socket连接会放在这里\n    struct list_head rdllist;\n\n    //红黑树用来管理所有监听的socket连接\n    struct rb_root rbr;\n\n    ......\n}\n\n\n * wait_queue_head_t wq：epoll中的等待队列，队列里存放的是阻塞在epoll上的用户进程。在io就绪的时候epoll可以通过这个队列找到这些阻塞的进程并唤醒它们，从而执行io调用读写socket上的数据。\n\n> 这里注意与socket中的等待队列区分！！！\n\n * struct list_head rdllist：epoll中的就绪队列，队列里存放的是都是io就绪的socket，被唤醒的用户进程可以直接读取这个队列获取io活跃的socket。无需再次遍历整个socket集合。\n\n> 这里正是epoll比select ，poll高效之处，select ，poll返回的是全部的socket连接，我们需要在用户空间再次遍历找出真正io活跃的socket连接。而epoll只是返回io活跃的socket连接。用户进程可以直接进行io操作。\n\n * struct rb_root rbr : 由于红黑树在查找，插入，删除等综合性能方面是最优的，所以epoll内部使用一颗红黑树来管理海量的socket连接。\n\n> select用数组管理连接，poll用链表管理连接。\n\n\n# epoll_ctl向epoll对象中添加监听的socket\n\n当我们调用epoll_create在内核中创建出epoll对象struct eventpoll后，我们就可以利用epoll_ctl向epoll中添加我们需要管理的socket连接了。\n\n 1. 首先要在epoll内核中创建一个表示socket连接的数据结构struct epitem，而在epoll中为了综合性能的考虑，采用一颗红黑树来管理这些海量socket连接。所以struct epitem是一个红黑树节点。\n\n\n\nstruct epitem\n{\n    //指向所属epoll对象\n    struct eventpoll *ep; \n    //注册的感兴趣的事件,也就是用户空间的epoll_event     \n    struct epoll_event event; \n    //指向epoll对象中的就绪队列\n    struct list_head rdllink;  \n    //指向epoll中对应的红黑树节点\n    struct rb_node rbn;     \n    //指向epitem所表示的socket->file结构以及对应的fd\n    struct epoll_filefd ffd;                  \n}\n\n\n> 这里重点记住struct epitem结构中的rdllink以及epoll_filefd成员，后面我们会用到。\n\n 2. 在内核中创建完表示socket连接的数据结构struct epitem后，我们就需要在socket中的等待队列上创建等待项wait_queue_t并且注册epoll的回调函数ep_poll_callback。\n\n通过《阻塞io中用户进程阻塞以及唤醒原理》小节的铺垫，我想大家已经猜到这一步的意义所在了吧！当时在等待项wait_queue_t中注册的是autoremove_wake_function回调函数。还记得吗？\n\n> epoll的回调函数ep_poll_callback正是epoll同步io事件通知机制的核心所在，也是区别于select，poll采用内核轮询方式的根本性能差异所在。\n\n\n\n这里又出现了一个新的数据结构struct eppoll_entry，那它的作用是干什么的呢？大家可以结合上图先猜测下它的作用!\n\n我们知道socket->sock->sk_wq等待队列中的类型是wait_queue_t，我们需要在struct epitem所表示的socket的等待队列上注册epoll回调函数ep_poll_callback。\n\n这样当数据到达socket中的接收队列时，内核会回调sk_data_ready，在阻塞io中用户进程阻塞以及唤醒原理这一小节中，我们知道这个sk_data_ready函数指针会指向sk_def_readable函数，在sk_def_readable中会回调注册在等待队列里的等待项wait_queue_t -> func回调函数ep_poll_callback。在ep_poll_callback中需要找到epitem，将io就绪的epitem放入epoll中的就绪队列中。\n\n而socket等待队列中类型是wait_queue_t无法关联到epitem。所以就出现了struct eppoll_entry结构体，它的作用就是关联socket等待队列中的等待项wait_queue_t和epitem。\n\nstruct eppoll_entry { \n    //指向关联的epitem\n    struct epitem *base; \n\n    // 关联监听socket中等待队列中的等待项 (private = null  func = ep_poll_callback)\n    wait_queue_t wait;   \n\n    // 监听socket中等待队列头指针\n    wait_queue_head_t *whead; \n    .........\n}; \n\n\n这样在ep_poll_callback回调函数中就可以根据socket等待队列中的等待项wait，通过container_of宏找到eppoll_entry，继而找到epitem了。\n\n> container_of在linux内核中是一个常用的宏，用于从包含在某个结构中的指针获得结构本身的指针，通俗地讲就是通过结构体变量中某个成员的首地址进而获得整个结构体变量的首地址。\n\n> 这里需要注意下这次等待项wait_queue_t中的private设置的是null，因为这里socket是交给epoll来管理的，阻塞在socket上的进程是也由epoll来唤醒。在等待项wait_queue_t注册的func是ep_poll_callback而不是autoremove_wake_function，阻塞进程并不需要autoremove_wake_function来唤醒，所以这里设置private为null\n\n 3. 当在socket的等待队列中创建好等待项wait_queue_t并且注册了epoll的回调函数ep_poll_callback，然后又通过eppoll_entry关联了epitem后。剩下要做的就是将epitem插入到epoll中的红黑树struct rb_root rbr中。\n\n> 这里可以看到epoll另一个优化的地方，epoll将所有的socket连接通过内核中的红黑树来集中管理。每次添加或者删除socket连接都是增量添加删除，而不是像select，poll那样每次调用都是全量socket连接集合传入内核。避免了频繁大量的内存拷贝。\n\n\n# epoll_wait同步阻塞获取io就绪的socket\n\n 1. 用户程序调用epoll_wait后，内核首先会查找epoll中的就绪队列eventpoll->rdllist是否有io就绪的epitem。epitem里封装了socket的信息。如果就绪队列中有就绪的epitem，就将就绪的socket信息封装到epoll_event返回。\n 2. 如果eventpoll->rdllist就绪队列中没有io就绪的epitem，则会创建等待项wait_queue_t，将用户进程的fd关联到wait_queue_t->private上，并在等待项wait_queue_t->func上注册回调函数default_wake_function。最后将等待项添加到epoll中的等待队列中。用户进程让出cpu，进入阻塞状态。\n\n\n\n> 这里和阻塞io模型中的阻塞原理是一样的，只不过在阻塞io模型中注册到等待项wait_queue_t->func上的是autoremove_wake_function，并将等待项添加到socket中的等待队列中。这里注册的是default_wake_function，将等待项添加到epoll中的等待队列上。\n\n\n\n 3. 前边做了那么多的知识铺垫，下面终于到了epoll的整个工作流程了：\n\n\n\n * 当网络数据包在软中断中经过内核协议栈的处理到达socket的接收缓冲区时，紧接着会调用socket的数据就绪回调指针sk_data_ready，回调函数为sock_def_readable。在socket的等待队列中找出等待项，其中等待项中注册的回调函数为ep_poll_callback。\n * 在回调函数ep_poll_callback中，根据struct eppoll_entry中的struct wait_queue_t wait通过container_of宏找到eppoll_entry对象并通过它的base指针找到封装socket的数据结构struct epitem，并将它加入到epoll中的就绪队列rdllist中。\n * 随后查看epoll中的等待队列中是否有等待项，也就是说查看是否有进程阻塞在epoll_wait上等待io就绪的socket。如果没有等待项，则软中断处理完成。\n * 如果有等待项，则回到注册在等待项中的回调函数default_wake_function,在回调函数中唤醒阻塞进程，并将就绪队列rdllist中的epitem的io就绪socket信息封装到struct epoll_event中返回。\n * 用户进程拿到epoll_event获取io就绪的socket，发起系统io调用读取数据。\n\n\n# 再谈水平触发和边缘触发\n\n网上有大量的关于这两种模式的讲解，大部分讲的比较模糊，感觉只是强行从概念上进行描述，看完让人难以理解。所以在这里，笔者想结合上边epoll的工作过程，再次对这两种模式做下自己的解读，力求清晰的解释出这两种工作模式的异同。\n\n经过上边对epoll工作过程的详细解读，我们知道，当我们监听的socket上有数据到来时，软中断会执行epoll的回调函数ep_poll_callback,在回调函数中会将epoll中描述socket信息的数据结构epitem插入到epoll中的就绪队列rdllist中。随后用户进程从epoll的等待队列中被唤醒，epoll_wait将io就绪的socket返回给用户进程，随即epoll_wait会清空rdllist。\n\n水平触发和边缘触发最关键的区别就在于当socket中的接收缓冲区还有数据可读时。epoll_wait是否会清空rdllist。\n\n * 水平触发：在这种模式下，用户线程调用epoll_wait获取到io就绪的socket后，对socket进行系统io调用读取数据，假设socket中的数据只读了一部分没有全部读完，这时再次调用epoll_wait，epoll_wait会检查这些socket中的接收缓冲区是否还有数据可读，如果还有数据可读，就将socket重新放回rdllist。所以当socket上的io没有被处理完时，再次调用epoll_wait依然可以获得这些socket，用户进程可以接着处理socket上的io事件。\n * 边缘触发： 在这种模式下，epoll_wait就会直接清空rdllist，不管socket上是否还有数据可读。所以在边缘触发模式下，当你没有来得及处理socket接收缓冲区的剩下可读数据时，再次调用epoll_wait，因为这时rdlist已经被清空了，socket不会再次从epoll_wait中返回，所以用户进程就不会再次获得这个socket了，也就无法在对它进行io处理了。除非，这个socket上有新的io数据到达，根据epoll的工作过程，该socket会被再次放入rdllist中。\n\n> 如果你在边缘触发模式下，处理了部分socket上的数据，那么想要处理剩下部分的数据，就只能等到这个socket上再次有网络数据到达。\n\n在netty中实现的epollsocketchannel默认的就是边缘触发模式。jdk的nio默认是水平触发模式。\n\n\n# epoll对select，poll的优化总结\n\n * epoll在内核中通过红黑树管理海量的连接，所以在调用epoll_wait获取io就绪的socket时，不需要传入监听的socket文件描述符。从而避免了海量的文件描述符集合在用户空间和内核空间中来回复制。\n\n> select，poll每次调用时都需要传递全量的文件描述符集合，导致大量频繁的拷贝操作。\n\n * epoll仅会通知io就绪的socket。避免了在用户空间遍历的开销。\n\n> select，poll只会在io就绪的socket上打好标记，依然是全量返回，所以在用户空间还需要用户程序在一次遍历全量集合找出具体io就绪的socket。\n\n * epoll通过在socket的等待队列上注册回调函数ep_poll_callback通知用户程序io就绪的socket。避免了在内核中轮询的开销。\n\n> 大部分情况下socket上并不总是io活跃的，在面对海量连接的情况下，select，poll采用内核轮询的方式获取io活跃的socket，无疑是性能低下的核心原因。\n\n根据以上epoll的性能优势，它是目前为止各大主流网络框架，以及反向代理中间件使用到的网络io模型。\n\n利用epoll多路复用io模型可以轻松的解决c10k问题。\n\nc100k的解决方案也还是基于c10k的方案，通过epoll 配合线程池，再加上 cpu、内存和网络接口的性能和容量提升。大部分情况下，c100k很自然就可以达到。\n\n甚至c1000k的解决方法，本质上还是构建在 epoll 的多路复用 i/o 模型上。只不过，除了 i/o 模型之外，还需要从应用程序到 linux 内核、再到 cpu、内存和网络等各个层次的深度优化，特别是需要借助硬件，来卸载那些原来通过软件处理的大量功能（去掉大量的中断响应开销，以及内核协议栈处理的开销）。',charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"零拷贝详解",frontmatter:{title:"零拷贝详解",date:"2024-09-18T21:08:24.000Z",permalink:"/pages/3f7882/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/10.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/15.%E9%9B%B6%E6%8B%B7%E8%B4%9D%E8%AF%A6%E8%A7%A3.html",relativePath:"Netty 系统设计/10.二、基础知识/15.零拷贝详解.md",key:"v-80c18d30",path:"/pages/3f7882/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"Future 和 Promise",frontmatter:{title:"Future 和 Promise",date:"2024-09-18T21:09:42.000Z",permalink:"/pages/cb89e4/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/10.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/20.Future%20%E5%92%8C%20Promise.html",relativePath:"Netty 系统设计/10.二、基础知识/20.Future 和 Promise.md",key:"v-c0ed00cc",path:"/pages/cb89e4/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"TCP 拆包与粘包",frontmatter:{title:"TCP 拆包与粘包",date:"2024-09-18T21:15:00.000Z",permalink:"/pages/9b88cb/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/10.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/30.TCP%20%E6%8B%86%E5%8C%85%E4%B8%8E%E7%B2%98%E5%8C%85.html",relativePath:"Netty 系统设计/10.二、基础知识/30.TCP 拆包与粘包.md",key:"v-65e99d02",path:"/pages/9b88cb/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"心跳机制详解",frontmatter:{title:"心跳机制详解",date:"2024-09-18T21:15:29.000Z",permalink:"/pages/cab36c/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/10.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/40.%E5%BF%83%E8%B7%B3%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html",relativePath:"Netty 系统设计/10.二、基础知识/40.心跳机制详解.md",key:"v-049caca4",path:"/pages/cab36c/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"Netty 框架概述",frontmatter:{title:"Netty 框架概述",date:"2024-09-18T21:01:51.000Z",permalink:"/pages/b0bc66/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/20.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/01.Netty%20%E6%A1%86%E6%9E%B6%E6%A6%82%E8%BF%B0.html",relativePath:"Netty 系统设计/20.三、主线任务/01.Netty 框架概述.md",key:"v-2915e1b2",path:"/pages/b0bc66/",headers:[{level:2,title:"简约图",slug:"简约图",normalizedTitle:"简约图",charIndex:2},{level:2,title:"二级图",slug:"二级图",normalizedTitle:"二级图",charIndex:402},{level:2,title:"终极图",slug:"终极图",normalizedTitle:"终极图",charIndex:994}],headersStr:"简约图 二级图 终极图",content:"# 简约图\n\nNetty 的设计主要基于主从 Reactor 多线程模式，并做了一定的改进\n\n简单版本的 Netty 的模样如下：\n\n\n\n解释如下：\n\n 1. BossGroup 线程维护 Selector，ServerSocketChannel 注册到这个 Selector 上，只关注连接建立请求事件（相当于主 Reactor）\n 2. 当接收到来自客户端的连接建立请求事件的时候，通过 ServerSocketChannel.accept 方法获得对应的 SocketChannel，并封装成 NioSocketChannel 注册到 WorkerGroup 线程中的 Selector，每个 Selector 运行在一个线程中（相当于从 Reactor）\n 3. 当 WorkerGroup 线程中的 Selector 监听到自己感兴趣的 IO 事件后，就调用 Handler 进行处理\n\n\n# 二级图\n\n\n\n 1. 有两组线程池：BossGroup 和 WorkerGroup，BossGroup 中的线程（可以有多个，图中只画了一个）专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写\n 2. BossGroup 和 WorkerGroup 含有多个不断循环的执行事件处理的线程，每个线程都包含一个 Selector，用于监听注册在其上的 Channel\n 3. 每个 BossGroup 中的线程循环执行以下三个步骤：\n    1. 轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件）\n    2. 处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到 WorkerGroup 中某个线程上的 Selector 上\n    3. 再去以此循环处理任务队列中的下一个事件\n 4. 每个 WorkerGroup 中的线程循环执行以下三个步骤：\n    1. 轮训注册在其上的 NioSocketChannel 的 read/write 事件（OP_READ/OP_WRITE 事件）\n    2. 在对应的 NioSocketChannel 上处理 read/write 事件\n    3. 再去以此循环处理任务队列中的下一个事件\n\n\n# 终极图\n\n\n\n 1. Netty 抽象出两组线程池：BossGroup 和 WorkerGroup，也可以叫做 BossNioEventLoopGroup 和 WorkerNioEventLoopGroup。每个线程池中都有 NioEventLoop 线程。BossGroup 中的线程专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写。BossGroup 和 WorkerGroup 的类型都是 NioEventLoopGroup。\n 2. NioEventLoopGroup 相当于一个事件循环组，这个组中含有多个事件循环，每个事件循环就是一个 NioEventLoop\n 3. NioEventLoop 表示一个不断循环的执行事件处理的线程，每个 NioEventLoop 都包含一个 Selector，用于监听注册在其上的 Socket 网络连接（Channel）\n 4. NioEventLoopGroup 可以含有多个线程，即可以含有多个 NioEventLoop\n 5. 每个 BossNioEventLoop 中循环执行以下三个步骤\n    1. select：轮训注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件）\n    2. processSelectedKeys：处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到某个 WorkerNioEventLoop 上的 Selector 上\n    3. runAllTasks：再去以此循环处理任务队列中的其他任务\n 6. 每个 WorkerNioEventLoop 中循环执行以下三个步骤：\n    1. select：轮训注册在其上的 NioSocketChannel 的 read/write 事件（OP_READ/OP_WRITE 事件）\n    2. processSelectedKeys：在对应的 NioSocketChannel 上处理 read/write 事件\n    3. runAllTasks：再去以此循环处理任务队列中的其他任务\n 7. 在以上两个processSelectedKeys步骤中，会使用 Pipeline（管道），Pipeline 中引用了 Channel，即通过 Pipeline 可以获取到对应的 Channel，Pipeline 中维护了很多的处理器（拦截处理器、过滤处理器、自定义处理器等）。这里暂时不详细展开讲解 Pipeline。",normalizedContent:"# 简约图\n\nnetty 的设计主要基于主从 reactor 多线程模式，并做了一定的改进\n\n简单版本的 netty 的模样如下：\n\n\n\n解释如下：\n\n 1. bossgroup 线程维护 selector，serversocketchannel 注册到这个 selector 上，只关注连接建立请求事件（相当于主 reactor）\n 2. 当接收到来自客户端的连接建立请求事件的时候，通过 serversocketchannel.accept 方法获得对应的 socketchannel，并封装成 niosocketchannel 注册到 workergroup 线程中的 selector，每个 selector 运行在一个线程中（相当于从 reactor）\n 3. 当 workergroup 线程中的 selector 监听到自己感兴趣的 io 事件后，就调用 handler 进行处理\n\n\n# 二级图\n\n\n\n 1. 有两组线程池：bossgroup 和 workergroup，bossgroup 中的线程（可以有多个，图中只画了一个）专门负责和客户端建立连接，workergroup 中的线程专门负责处理连接上的读写\n 2. bossgroup 和 workergroup 含有多个不断循环的执行事件处理的线程，每个线程都包含一个 selector，用于监听注册在其上的 channel\n 3. 每个 bossgroup 中的线程循环执行以下三个步骤：\n    1. 轮训注册在其上的 serversocketchannel 的 accept 事件（op_accept 事件）\n    2. 处理 accept 事件，与客户端建立连接，生成一个 niosocketchannel，并将其注册到 workergroup 中某个线程上的 selector 上\n    3. 再去以此循环处理任务队列中的下一个事件\n 4. 每个 workergroup 中的线程循环执行以下三个步骤：\n    1. 轮训注册在其上的 niosocketchannel 的 read/write 事件（op_read/op_write 事件）\n    2. 在对应的 niosocketchannel 上处理 read/write 事件\n    3. 再去以此循环处理任务队列中的下一个事件\n\n\n# 终极图\n\n\n\n 1. netty 抽象出两组线程池：bossgroup 和 workergroup，也可以叫做 bossnioeventloopgroup 和 workernioeventloopgroup。每个线程池中都有 nioeventloop 线程。bossgroup 中的线程专门负责和客户端建立连接，workergroup 中的线程专门负责处理连接上的读写。bossgroup 和 workergroup 的类型都是 nioeventloopgroup。\n 2. nioeventloopgroup 相当于一个事件循环组，这个组中含有多个事件循环，每个事件循环就是一个 nioeventloop\n 3. nioeventloop 表示一个不断循环的执行事件处理的线程，每个 nioeventloop 都包含一个 selector，用于监听注册在其上的 socket 网络连接（channel）\n 4. nioeventloopgroup 可以含有多个线程，即可以含有多个 nioeventloop\n 5. 每个 bossnioeventloop 中循环执行以下三个步骤\n    1. select：轮训注册在其上的 serversocketchannel 的 accept 事件（op_accept 事件）\n    2. processselectedkeys：处理 accept 事件，与客户端建立连接，生成一个 niosocketchannel，并将其注册到某个 workernioeventloop 上的 selector 上\n    3. runalltasks：再去以此循环处理任务队列中的其他任务\n 6. 每个 workernioeventloop 中循环执行以下三个步骤：\n    1. select：轮训注册在其上的 niosocketchannel 的 read/write 事件（op_read/op_write 事件）\n    2. processselectedkeys：在对应的 niosocketchannel 上处理 read/write 事件\n    3. runalltasks：再去以此循环处理任务队列中的其他任务\n 7. 在以上两个processselectedkeys步骤中，会使用 pipeline（管道），pipeline 中引用了 channel，即通过 pipeline 可以获取到对应的 channel，pipeline 中维护了很多的处理器（拦截处理器、过滤处理器、自定义处理器等）。这里暂时不详细展开讲解 pipeline。",charsets:{cjk:!0},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"Bootstrap（client）源码解析",frontmatter:{title:"Bootstrap（client）源码解析",date:"2024-09-18T21:10:11.000Z",permalink:"/pages/9582d0/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/20.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/02.Bootstrap%EF%BC%88client%EF%BC%89%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html",relativePath:"Netty 系统设计/20.三、主线任务/02.Bootstrap（client）源码解析.md",key:"v-cc4a8e7a",path:"/pages/9582d0/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"Bootstrap",slug:"bootstrap",normalizedTitle:"bootstrap",charIndex:117},{level:2,title:"客户端部分",slug:"客户端部分",normalizedTitle:"客户端部分",charIndex:326},{level:3,title:"连接源码",slug:"连接源码",normalizedTitle:"连接源码",charIndex:336},{level:3,title:"NioSocketChannel 的初始化过程",slug:"niosocketchannel-的初始化过程",normalizedTitle:"niosocketchannel 的初始化过程",charIndex:1451},{level:4,title:"ChannelFactory 和 Channel 类型的确定",slug:"channelfactory-和-channel-类型的确定",normalizedTitle:"channelfactory 和 channel 类型的确定",charIndex:1662},{level:4,title:"Channel 实例化",slug:"channel-实例化",normalizedTitle:"channel 实例化",charIndex:3195},{level:3,title:"关于 unsafe 字段的初始化",slug:"关于-unsafe-字段的初始化",normalizedTitle:"关于 unsafe 字段的初始化",charIndex:5832},{level:3,title:"关于 pipeline 的初始化",slug:"关于-pipeline-的初始化",normalizedTitle:"关于 pipeline 的初始化",charIndex:7125},{level:3,title:"关于 EventLoop 初始化",slug:"关于-eventloop-初始化",normalizedTitle:"关于 eventloop 初始化",charIndex:9131},{level:3,title:"channel 的注册过程",slug:"channel-的注册过程",normalizedTitle:"channel 的注册过程",charIndex:11995},{level:3,title:"handler 的添加过程",slug:"handler-的添加过程",normalizedTitle:"handler 的添加过程",charIndex:14890},{level:3,title:"客户端连接分析",slug:"客户端连接分析",normalizedTitle:"客户端连接分析",charIndex:17553},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:5027},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:22576}],headersStr:"前言 Bootstrap 客户端部分 连接源码 NioSocketChannel 的初始化过程 ChannelFactory 和 Channel 类型的确定 Channel 实例化 关于 unsafe 字段的初始化 关于 pipeline 的初始化 关于 EventLoop 初始化 channel 的注册过程 handler 的添加过程 客户端连接分析 总结 参考资料",content:'# 前言\n\n这一章是 Netty 源码分析系列的第一章, 我打算在这一章中, 展示一下 Netty 的客户端和服务端的初始化和启动的流程, 给读者一个对 Netty 源码有一个大致的框架上的认识, 而不会深入每个功能模块. 本章会从 Bootstrap/ServerBootstrap 类 入手, 分析 Netty 程序的初始化和启动的流程.\n\n\n# Bootstrap\n\nBootstrap 是 Netty 提供的一个便利的工厂类, 我们可以通过它来完成 Netty 的客户端或服务器端的 Netty 初始化. 下面我以 Netty 源码例子中的 Echo 服务器作为例子, 从客户端和服务器端分别分析一下 Netty 的程序是如何启动的.\n\n\n# 客户端部分\n\n\n# 连接源码\n\n首先, 让我们从客户端方面的代码开始 下面是源码 example/src/main/java/io/netty/example/echo/EchoClient.java 的客户端部分的启动代码:\n\nEventLoopGroup group = new NioEventLoopGroup();\ntry {\n    Bootstrap b = new Bootstrap();\n    b.group(group)\n     .channel(NioSocketChannel.class)\n     .option(ChannelOption.TCP_NODELAY, true)\n     .handler(new ChannelInitializer<SocketChannel>() {\n         @Override\n         public void initChannel(SocketChannel ch) throws Exception {\n             ChannelPipeline p = ch.pipeline();\n             p.addLast(new EchoClientHandler());\n         }\n     });\n\n    // Start the client.\n    ChannelFuture f = b.connect(HOST, PORT).sync();\n\n    // Wait until the connection is closed.\n    f.channel().closeFuture().sync();\n} finally {\n    // Shut down the event loop to terminate all threads.\n    group.shutdownGracefully();\n}\n\n\n从上面的客户端代码虽然简单, 但是却展示了 Netty 客户端初始化时所需的所有内容：\n\n 1. EventLoopGroup：不论是服务器端还是客户端, 都必须指定 EventLoopGroup. 在这个例子中, 指定了 NioEventLoopGroup, 表示一个 NIO 的 EventLoopGroup\n 2. ChannelType: 指定 Channel 的类型。因为是客户端, 因此使用了 NioSocketChannel.\n 3. Handler: 设置数据的处理器\n\n下面我们深入代码，看一下客户端通过 Bootstrap 启动后，都做了哪些工作.\n\n\n# NioSocketChannel 的初始化过程\n\n在 Netty 中, Channel 是一个 Socket 的抽象, 它为用户提供了关于 Socket 状态(是否是连接还是断开) 以及对 Socket 的读写等操作. 每当 Netty 建立了一个连接后, 都会有一个对应的 Channel 实例。NioSocketChannel 的类层次结构如下：\n\n\n\n这一小节我们着重分析一下 Channel 的初始化过程.\n\n# ChannelFactory 和 Channel 类型的确定\n\n除了 TCP 协议以外, Netty 还支持很多其他的连接协议, 并且每种协议还有 NIO(异步 IO) 和 OIO(Old-IO, 即传统的阻塞 IO) 版本的区别. 不同协议不同的阻塞类型的连接都有不同的 Channel 类型与之对应下面是一些常用的 Channel 类型:\n\n * NioSocketChannel, 代表异步的客户端 TCP Socket 连接\n * NioServerSocketChannel, 异步的服务器端 TCP Socket 连接\n * NioDatagramChannel, 异步的 UDP 连接\n * NioSctpChannel, 异步的客户端 Sctp 连接\n * NioSctpServerChannel, 异步的 Sctp 服务器端连接\n * OioSocketChannel, 同步的客户端 TCP Socket 连接\n * OioServerSocketChannel, 同步的服务器端 TCP Socket 连接\n * OioDatagramChannel, 同步的 UDP 连接\n * OioSctpChannel, 同步的 Sctp 服务器端连接\n * OioSctpServerChannel, 同步的客户端 TCP Socket 连接\n\n那么我们是如何设置所需要的 Channel 的类型的呢? 答案是 channel() 方法的调用. 回想一下我们在客户端连接代码的初始化 Bootstrap 中, 会调用 channel() 方法, 传入 NioSocketChannel.class, 这个方法其实就是初始化了一个 BootstrapChannelFactory：\n\npublic B channel(Class<? extends C> channelClass) {\n    if (channelClass == null) {\n        throw new NullPointerException("channelClass");\n    }\n    return channelFactory(new BootstrapChannelFactory<C>(channelClass));\n}\n\n\n而 BootstrapChannelFactory 实现了 ChannelFactory 接口, 它提供了唯一的方法, 即 newChannel. ChannelFactory, 顾名思义, 就是产生 Channel 的工厂类. 进入到 BootstrapChannelFactory.newChannel 中, 我们看到其实现代码如下:\n\n@Override\npublic T newChannel() {\n\t// 删除 try 块\n    return clazz.newInstance();\n}\n\n\n根据上面代码的提示, 我们就可以确定:\n\n * Bootstrap 中的 ChannelFactory 的实现是 BootstrapChannelFactory\n * 生成的 Channel 的具体类型是 NioSocketChannel. Channel 的实例化过程, 其实就是调用的 ChannelFactory#newChannel 方法, 而实例化的 Channel 的具体的类型又是和在初始化 Bootstrap 时传入的 channel() 方法的参数相关. 因此对于我们这个例子中的客户端的 Bootstrap 而言, 生成的的 Channel 实例就是 NioSocketChannel\n\n# Channel 实例化\n\n前面我们已经知道了如何确定一个 Channel 的类型, 并且了解到 Channel 是通过工厂方法 ChannelFactory.newChannel() 来实例化的, 那么 ChannelFactory.newChannel() 方法在哪里调用呢？继续跟踪, 我们发现其调用链是:\n\nBootstrap.connect -> Bootstrap.doConnect -> AbstractBootstrap.initAndRegister\n\n\n在 AbstractBootstrap.initAndRegister 中就调用了 channelFactory().newChannel() 来获取一个新的 NioSocketChannel 实例, 其源码如下:\n\nfinal ChannelFuture initAndRegister() {\n\t// 去掉非关键代码\n    final Channel channel = channelFactory().newChannel();\n    init(channel);\n    ChannelFuture regFuture = group().register(channel);\n}\n\n\n在 newChannel 中, 通过类对象的 newInstance 来获取一个新 Channel 实例, 因而会调用 NioSocketChannel 的默认构造器. NioSocketChannel 默认构造器代码如下:\n\npublic NioSocketChannel() {\n    this(newSocket(DEFAULT_SELECTOR_PROVIDER));\n}\n\n\n这里的代码比较关键, 我们看到, 在这个构造器中, 会调用 newSocket 来打开一个新的 Java NIO SocketChannel:\n\nprivate static SocketChannel newSocket(SelectorProvider provider) {\n    ...\n    return provider.openSocketChannel();\n}\n\n\n接着会调用父类, 即 AbstractNioByteChannel 的构造器:\n\nAbstractNioByteChannel(Channel parent, SelectableChannel ch)\n\n\n并传入参数 parent 为 null, ch 为刚才使用 newSocket 创建的 Java NIO SocketChannel, 因此生成的 NioSocketChannel 的 parent channel 是空的.\n\nprotected AbstractNioByteChannel(Channel parent, SelectableChannel ch) {\n    super(parent, ch, SelectionKey.OP_READ);\n}\n\n\n接着会继续调用父类 AbstractNioChannel 的构造器, 并传入了参数 readInterestOp = SelectionKey.OP_READ:\n\nprotected AbstractNioChannel(Channel parent, SelectableChannel ch, int readInterestOp) {\n    super(parent);\n    this.ch = ch;\n    this.readInterestOp = readInterestOp;\n    // 省略 try 块\n    // 配置 Java NIO SocketChannel 为非阻塞的.\n    ch.configureBlocking(false);\n}\n\n\n然后继续调用父类 AbstractChannel 的构造器:\n\nprotected AbstractChannel(Channel parent) {\n    this.parent = parent;\n    unsafe = newUnsafe();\n    pipeline = new DefaultChannelPipeline(this);\n}\n\n\n到这里, 一个完整的 NioSocketChannel 就初始化完成了, 我们可以稍微总结一下构造一个 NioSocketChannel 所需要做的工作:\n\n * 调用 NioSocketChannel.newSocket(DEFAULT_SELECTOR_PROVIDER) 打开一个新的 Java NIO SocketChannel\n * AbstractChannel(Channel parent) 中初始化 AbstractChannel 的属性:\n * parent 属性置为 null\n * unsafe 通过 newUnsafe() 实例化一个 unsafe 对象, 它的类型是 AbstractNioByteChannel.NioByteUnsafe 内部类\n * pipeline 是 new DefaultChannelPipeline(this) 新创建的实例. 这里体现了:Each channel has its own pipeline and it is created automatically when a new channel is created.\n * AbstractNioChannel 中的属性:\n * SelectableChannel ch 被设置为 Java SocketChannel, 即 NioSocketChannel#newSocket 返回的 Java NIO SocketChannel.\n * readInterestOp 被设置为 SelectionKey.OP_READ\n * SelectableChannel ch 被配置为非阻塞的 ch.configureBlocking(false)\n * NioSocketChannel 中的属性:\n * SocketChannelConfig config = new NioSocketChannelConfig(this, socket.socket())\n\n\n# 关于 unsafe 字段的初始化\n\n我们简单地提到了, 在实例化 NioSocketChannel 的过程中, 会在父类 AbstractChannel 的构造器中, 调用 newUnsafe() 来获取一个 unsafe 实例. 那么 unsafe 是怎么初始化的呢? 它的作用是什么? 其实 unsafe 特别关键, 它封装了对 Java 底层 Socket 的操作, 因此实际上是沟通 Netty 上层和 Java 底层的重要的桥梁.\n\n那么我们就来看一下 Unsafe 接口所提供的方法吧:\n\ninterface Unsafe {\n    SocketAddress localAddress();\n    SocketAddress remoteAddress();\n    void register(EventLoop eventLoop, ChannelPromise promise);\n    void bind(SocketAddress localAddress, ChannelPromise promise);\n    void connect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise);\n    void disconnect(ChannelPromise promise);\n    void close(ChannelPromise promise);\n    void closeForcibly();\n    void deregister(ChannelPromise promise);\n    void beginRead();\n    void write(Object msg, ChannelPromise promise);\n    void flush();\n    ChannelPromise voidPromise();\n    ChannelOutboundBuffer outboundBuffer();\n}\n\n\n一看便知, 这些方法其实都会对应到相关的 Java 底层的 Socket 的操作. 回到 AbstractChannel 的构造方法中, 在这里调用了 newUnsafe() 获取一个新的 unsafe 对象, 而 newUnsafe 方法在 NioSocketChannel 中被重写了:\n\n@Override\nprotected AbstractNioUnsafe newUnsafe() {\n    return new NioSocketChannelUnsafe();\n}\n\n\nNioSocketChannel.newUnsafe 方法会返回一个 NioSocketChannelUnsafe 实例. 从这里我们就可以确定了, 在实例化的 NioSocketChannel 中的 unsafe 字段, 其实是一个 NioSocketChannelUnsafe 的实例.\n\n\n# 关于 pipeline 的初始化\n\n上面我们分析了一个 Channel (在这个例子中是 NioSocketChannel) 的大体初始化过程, 但是我们漏掉了一个关键的部分, 即 ChannelPipeline 的初始化. 根据 Each channel has its own pipeline and it is created automatically when a new channel is created., 我们知道, 在实例化一个 Channel 时, 必然伴随着实例化一个 ChannelPipeline. 而我们确实在 AbstractChannel 的构造器看到了 pipeline 字段被初始化为 DefaultChannelPipeline 的实例. 那么我们就来看一下, DefaultChannelPipeline 构造器做了哪些工作吧:\n\npublic DefaultChannelPipeline(AbstractChannel channel) {\n    if (channel == null) {\n        throw new NullPointerException("channel");\n    }\n    this.channel = channel;\n\n    tail = new TailContext(this);\n    head = new HeadContext(this);\n\n    head.next = tail;\n    tail.prev = head;\n}\n\n\n我们调用 DefaultChannelPipeline 的构造器, 传入了一个 channel, 而这个 channel 其实就是我们实例化的 NioSocketChannel, DefaultChannelPipeline 会将这个 NioSocketChannel 对象保存在 channel 字段中. DefaultChannelPipeline 中, 还有两个特殊的字段, 即 head 和 tail, 而这两个字段是一个双向链表的头和尾. 其实在 DefaultChannelPipeline 中, 维护了一个以 AbstractChannelHandlerContext 为节点的双向链表, 这个链表是 Netty 实现 Pipeline 机制的关键. 关于 DefaultChannelPipeline 中的双向链表以及它所起的作用, 我在这里暂时不表, 在 Netty 源码分析之 二 贯穿 Netty 的大动脉 ── ChannelPipeline 中会有详细的分析.\n\nHeadContext 的继承层次结构如下所示: [](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端)/HeadContext.png) TailContext 的继承层次结构如下所示: [](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端)/TailContext.png)\n\n我们可以看到, 链表中 head 是一个 ChannelOutboundHandler, 而 tail 则是一个 ChannelInboundHandler. 接着看一下 HeadContext 的构造器:\n\nHeadContext(DefaultChannelPipeline pipeline) {\n    super(pipeline, null, HEAD_NAME, false, true);\n    unsafe = pipeline.channel().unsafe();\n}\n\n\n它调用了父类 AbstractChannelHandlerContext 的构造器, 并传入参数 inbound = false, outbound = true. TailContext 的构造器与 HeadContext 的相反, 它调用了父类 AbstractChannelHandlerContext 的构造器, 并传入参数 inbound = true, outbound = false. 即 header 是一个 outboundHandler, 而 tail 是一个 inboundHandler, 关于这一点, 大家要特别注意, 因为在分析到 Netty Pipeline 时, 我们会反复用到 inbound 和 outbound 这两个属性.\n\n\n# 关于 EventLoop 初始化\n\n回到最开始的 EchoClient.java 代码中, 我们在一开始就实例化了一个 NioEventLoopGroup 对象, 因此我们就从它的构造器中追踪一下 EventLoop 的初始化过程. 首先来看一下 NioEventLoopGroup 的类继承层次:\n\nNioEventLoop 有几个重载的构造器, 不过内容都没有什么区别, 最终都是调用的父类 MultithreadEventLoopGroup 构造器:\n\nprotected MultithreadEventLoopGroup(int nThreads, ThreadFactory threadFactory, Object... args) {\n    super(nThreads == 0? DEFAULT_EVENT_LOOP_THREADS : nThreads, threadFactory, args);\n}\n\n\n其中有一点有意思的地方是, 如果我们传入的线程数 nThreads 是 0, 那么 Netty 会为我们设置默认的线程数 DEFAULT_EVENT_LOOP_THREADS, 而这个默认的线程数是怎么确定的呢? 其实很简单, 在静态代码块中, 会首先确定 DEFAULT_EVENT_LOOP_THREADS 的值:\n\nstatic {\n    DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt(\n            "io.netty.eventLoopThreads", Runtime.getRuntime().availableProcessors() * 2));\n}\n\n\nNetty 会首先从系统属性中获取 "io.netty.eventLoopThreads" 的值, 如果我们没有设置它的话, 那么就返回默认值: 处理器核心数 * 2.\n\n回到 MultithreadEventLoopGroup 构造器中, 这个构造器会继续调用父类 MultithreadEventExecutorGroup 的构造器:\n\nprotected MultithreadEventExecutorGroup(int nThreads, ThreadFactory threadFactory, Object... args) {\n    // 去掉了参数检查, 异常处理 等代码.\n    children = new SingleThreadEventExecutor[nThreads];\n    if (isPowerOfTwo(children.length)) {\n        chooser = new PowerOfTwoEventExecutorChooser();\n    } else {\n        chooser = new GenericEventExecutorChooser();\n    }\n\n    for (int i = 0; i < nThreads; i ++) {\n        children[i] = newChild(threadFactory, args);\n    }\n}\n\n\n根据代码, 我们就很清楚 MultithreadEventExecutorGroup 中的处理逻辑了:\n\n * 创建一个大小为 nThreads 的 SingleThreadEventExecutor 数组\n * 根据 nThreads 的大小, 创建不同的 Chooser, 即如果 nThreads 是 2 的幂, 则使用 PowerOfTwoEventExecutorChooser, 反之使用 GenericEventExecutorChooser. 不论使用哪个 Chooser, 它们的功能都是一样的, 即从 children 数组中选出一个合适的 EventExecutor 实例.\n * 调用 newChhild 方法初始化 children 数组.\n\n根据上面的代码, 我们知道, MultithreadEventExecutorGroup 内部维护了一个 EventExecutor 数组, Netty 的 EventLoopGroup 的实现机制其实就建立在 MultithreadEventExecutorGroup 之上. 每当 Netty 需要一个 EventLoop 时, 会调用 next() 方法获取一个可用的 EventLoop. 上面代码的最后一部分是 newChild 方法, 这个是一个抽象方法, 它的任务是实例化 EventLoop 对象. 我们跟踪一下它的代码, 可以发现, 这个方法在 NioEventLoopGroup 类中实现了, 其内容很简单:\n\n@Override\nprotected EventExecutor newChild(\n        ThreadFactory threadFactory, Object... args) throws Exception {\n    return new NioEventLoop(this, threadFactory, (SelectorProvider) args[0]);\n}\n\n\n其实就是实例化一个 NioEventLoop 对象, 然后返回它.\n\n最后总结一下整个 EventLoopGroup 的初始化过程吧:\n\n * EventLoopGroup(其实是 MultithreadEventExecutorGroup) 内部维护一个类型为 EventExecutor children 数组, 其大小是 nThreads, 这样就构成了一个线程池\n * 如果我们在实例化 NioEventLoopGroup 时, 如果指定线程池大小, 则 nThreads 就是指定的值, 反之是处理器核心数 * 2\n * MultithreadEventExecutorGroup 中会调用 newChild 抽象方法来初始化 children 数组\n * 抽象方法 newChild 是在 NioEventLoopGroup 中实现的, 它返回一个 NioEventLoop 实例.\n * NioEventLoop 属性:\n * SelectorProvider provider 属性: NioEventLoopGroup 构造器中通过 SelectorProvider.provider() 获取一个 SelectorProvider\n * Selector selector 属性: NioEventLoop 构造器中通过调用通过 selector = provider.openSelector() 获取一个 selector 对象.\n\n\n# channel 的注册过程\n\n在前面的分析中, 我们提到, channel 会在 Bootstrap.initAndRegister 中进行初始化, 但是这个方法还会将初始化好的 Channel 注册到 EventGroup 中. 接下来我们就来分析一下 Channel 注册的过程. 回顾一下 AbstractBootstrap.initAndRegister 方法:\n\nfinal ChannelFuture initAndRegister() {\n\t// 去掉非关键代码\n    final Channel channel = channelFactory().newChannel();\n    init(channel);\n    ChannelFuture regFuture = group().register(channel);\n}\n\n\n当 Channel 初始化后, 会紧接着调用 group().register() 方法来注册 Channel, 我们继续跟踪的话, 会发现其调用链如下: AbstractBootstrap.initAndRegister -> MultithreadEventLoopGroup.register -> SingleThreadEventLoop.register -> AbstractUnsafe.register 通过跟踪调用链, 最终我们发现是调用到了 unsafe 的 register 方法, 那么接下来我们就仔细看一下 AbstractUnsafe.register 方法中到底做了什么:\n\n@Override\npublic final void register(EventLoop eventLoop, final ChannelPromise promise) {\n\t// 省略条件判断和错误处理\n    AbstractChannel.this.eventLoop = eventLoop;\n    register0(promise);\n}\n\n\n首先, 将 eventLoop 赋值给 Channel 的 eventLoop 属性, 而我们知道这个 eventLoop 对象其实是 MultithreadEventLoopGroup.next() 方法获取的, 根据我们前面 关于 EventLoop 初始化 小节中, 我们可以确定 next() 方法返回的 eventLoop 对象是 NioEventLoop 实例. register 方法接着调用了 register0 方法:\n\nprivate void register0(ChannelPromise promise) {\n    boolean firstRegistration = neverRegistered;\n    doRegister();\n    neverRegistered = false;\n    registered = true;\n    safeSetSuccess(promise);\n    pipeline.fireChannelRegistered();\n    // Only fire a channelActive if the channel has never been registered. This prevents firing\n    // multiple channel actives if the channel is deregistered and re-registered.\n    if (firstRegistration && isActive()) {\n        pipeline.fireChannelActive();\n    }\n}\n\n\nregister0 又调用了 AbstractNioChannel.doRegister:\n\n@Override\nprotected void doRegister() throws Exception {\n\t// 省略错误处理\n    selectionKey = javaChannel().register(eventLoop().selector, 0, this);\n}\n\n\njavaChannel() 这个方法在前面我们已经知道了, 它返回的是一个 Java NIO SocketChannel, 这里我们将这个 SocketChannel 注册到与 eventLoop 关联的 selector 上了.\n\n我们总结一下 Channel 的注册过程:\n\n * 首先在 AbstractBootstrap.initAndRegister 中, 通过 group().register(channel), 调用 MultithreadEventLoopGroup.register 方法\n * 在 MultithreadEventLoopGroup.register 中, 通过 next() 获取一个可用的 SingleThreadEventLoop, 然后调用它的 register\n * 在 SingleThreadEventLoop.register 中, 通过 channel.unsafe().register(this, promise) 来获取 channel 的 unsafe() 底层操作对象, 然后调用它的 register.\n * 在 AbstractUnsafe.register 方法中, 调用 register0 方法注册 Channel\n * 在 AbstractUnsafe.register0 中, 调用 AbstractNioChannel.doRegister 方法\n * AbstractNioChannel.doRegister 方法通过 javaChannel().register(eventLoop().selector, 0, this) 将 Channel 对应的 Java NIO SockerChannel 注册到一个 eventLoop 的 Selector 中, 并且将当前 Channel 作为 attachment.\n\n总的来说, Channel 注册过程所做的工作就是将 Channel 与对应的 EventLoop 关联, 因此这也体现了, 在 Netty 中, 每个 Channel 都会关联一个特定的 EventLoop, 并且这个 Channel 中的所有 IO 操作都是在这个 EventLoop 中执行的; 当关联好 Channel 和 EventLoop 后, 会继续调用底层的 Java NIO SocketChannel 的 register 方法, 将底层的 Java NIO SocketChannel 注册到指定的 selector 中. 通过这两步, 就完成了 Netty Channel 的注册过程.\n\n\n# handler 的添加过程\n\nNetty 的一个强大和灵活之处就是基于 Pipeline 的自定义 handler 机制. 基于此, 我们可以像添加插件一样自由组合各种各样的 handler 来完成业务逻辑. 例如我们需要处理 HTTP 数据, 那么就可以在 pipeline 前添加一个 Http 的编解码的 Handler, 然后接着添加我们自己的业务逻辑的 handler, 这样网络上的数据流就向通过一个管道一样, 从不同的 handler 中流过并进行编解码, 最终在到达我们自定义的 handler 中. 既然说到这里, 有些读者朋友肯定会好奇, 既然这个 pipeline 机制是这么的强大, 那么它是怎么实现的呢? 不过我这里不打算详细展开 Netty 的 ChannelPipeline 的实现机制(具体的细节会在后续的章节中展示), 我在这一小节中, 从简单的入手, 展示一下我们自定义的 handler 是如何以及何时添加到 ChannelPipeline 中的. 首先让我们看一下如下的代码片段:\n\n...\n.handler(new ChannelInitializer<SocketChannel>() {\n     @Override\n     public void initChannel(SocketChannel ch) throws Exception {\n         ChannelPipeline p = ch.pipeline();\n         if (sslCtx != null) {\n             p.addLast(sslCtx.newHandler(ch.alloc(), HOST, PORT));\n         }\n         //p.addLast(new LoggingHandler(LogLevel.INFO));\n         p.addLast(new EchoClientHandler());\n     }\n });\n\n\n这个代码片段就是实现了 handler 的添加功能. 我们看到, Bootstrap.handler 方法接收一个 ChannelHandler, 而我们传递的是一个 派生于 ChannelInitializer 的匿名类, 它正好也实现了 ChannelHandler 接口. 我们来看一下, ChannelInitializer 类内到底有什么玄机:\n\n@Sharable\npublic abstract class ChannelInitializer<C extends Channel> extends ChannelInboundHandlerAdapter {\n\n    private static final InternalLogger logger = InternalLoggerFactory.getInstance(ChannelInitializer.class);\n    protected abstract void initChannel(C ch) throws Exception;\n\n    @Override\n    @SuppressWarnings("unchecked")\n    public final void channelRegistered(ChannelHandlerContext ctx) throws Exception {\n        initChannel((C) ctx.channel());\n        ctx.pipeline().remove(this);\n        ctx.fireChannelRegistered();\n    }\n    ...\n}\n\n\nChannelInitializer 是一个抽象类, 它有一个抽象的方法 initChannel, 我们正是实现了这个方法, 并在这个方法中添加的自定义的 handler 的. 那么 initChannel 是哪里被调用的呢? 答案是 ChannelInitializer.channelRegistered 方法中. 我们来关注一下 channelRegistered 方法. 从上面的源码中, 我们可以看到, 在 channelRegistered 方法中, 会调用 initChannel 方法, 将自定义的 handler 添加到 ChannelPipeline 中, 然后调用 ctx.pipeline().remove(this) 将自己从 ChannelPipeline 中删除. 上面的分析过程, 可以用如下图片展示: 一开始, ChannelPipeline 中只有三个 handler, head, tail 和我们添加的 ChannelInitializer. [](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端)/1477130291691.png) 接着 initChannel 方法调用后, 添加了自定义的 handler: [](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端)/1477130295919.png) 最后将 ChannelInitializer 删除: [](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端)/1477130299722.png)\n\n分析到这里, 我们已经简单了解了自定义的 handler 是如何添加到 ChannelPipeline 中的, 不过限于主题与篇幅的原因, 我没有在这里详细展开 ChannelPipeline 的底层机制, 我打算在下一篇 Netty 源码分析之 二 贯穿 Netty 的大动脉 ── ChannelPipeline 中对这个问题进行深入的探讨.\n\n\n# 客户端连接分析\n\n经过上面的各种分析后, 我们大致了解了 Netty 初始化时, 所做的工作, 那么接下来我们就直奔主题, 分析一下客户端是如何发起 TCP 连接的.\n\n首先, 客户端通过调用 Bootstrap 的 connect 方法进行连接. 在 connect 中, 会进行一些参数检查后, 最终调用的是 doConnect0 方法, 其实现如下:\n\nprivate static void doConnect0(\n        final ChannelFuture regFuture, final Channel channel,\n        final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) {\n\n    // This method is invoked before channelRegistered() is triggered.  Give user handlers a chance to set up\n    // the pipeline in its channelRegistered() implementation.\n    channel.eventLoop().execute(new Runnable() {\n        @Override\n        public void run() {\n            if (regFuture.isSuccess()) {\n                if (localAddress == null) {\n                    channel.connect(remoteAddress, promise);\n                } else {\n                    channel.connect(remoteAddress, localAddress, promise);\n                }\n                promise.addListener(ChannelFutureListener.CLOSE_ON_FAILURE);\n            } else {\n                promise.setFailure(regFuture.cause());\n            }\n        }\n    });\n}\n\n\n在 doConnect0 中, 会在 eventloop 线程中调用 Channel 的 connect 方法, 而这个 Channel 的具体类型是什么呢? 我们在 Channel 初始化这一小节中已经分析过了, 这里 channel 的类型就是 NioSocketChannel. 进行跟踪到 channel.connect 中, 我们发现它调用的是 DefaultChannelPipeline#connect, 而, pipeline 的 connect 代码如下:\n\n@Override\npublic ChannelFuture connect(SocketAddress remoteAddress) {\n    return tail.connect(remoteAddress);\n}\n\n\n而 tail 字段, 我们已经分析过了, 是一个 TailContext 的实例, 而 TailContext 又是 AbstractChannelHandlerContext 的子类, 并且没有实现 connect 方法, 因此这里调用的其实是 AbstractChannelHandlerContext.connect, 我们看一下这个方法的实现:\n\n@Override\npublic ChannelFuture connect(\n        final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) {\n\n\t// 删除的参数检查的代码\n    final AbstractChannelHandlerContext next = findContextOutbound();\n    EventExecutor executor = next.executor();\n    if (executor.inEventLoop()) {\n        next.invokeConnect(remoteAddress, localAddress, promise);\n    } else {\n        safeExecute(executor, new OneTimeTask() {\n            @Override\n            public void run() {\n                next.invokeConnect(remoteAddress, localAddress, promise);\n            }\n        }, promise, null);\n    }\n\n    return promise;\n}\n\n\n上面的代码中有一个关键的地方, 即 final AbstractChannelHandlerContext next = findContextOutbound(), 这里调用 findContextOutbound 方法, 从 DefaultChannelPipeline 内的双向链表的 tail 开始, 不断向前寻找第一个 outbound 为 true 的 AbstractChannelHandlerContext, 然后调用它的 invokeConnect 方法, 其代码如下:\n\nprivate void invokeConnect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) {\n    // 忽略 try 块\n    ((ChannelOutboundHandler) handler()).connect(this, remoteAddress, localAddress, promise);\n}\n\n\n还记得我们在 "关于 pipeline 的初始化" 这一小节分析的的内容吗? 我们提到, 在 DefaultChannelPipeline 的构造器中, 会实例化两个对象: head 和 tail, 并形成了双向链表的头和尾. head 是 HeadContext 的实例, 它实现了 ChannelOutboundHandler 接口, 并且它的 outbound 字段为 true. 因此在 findContextOutbound 中, 找到的 AbstractChannelHandlerContext 对象其实就是 head. 进而在 invokeConnect 方法中, 我们向上转换为 ChannelOutboundHandler 就是没问题的了. 而又因为 HeadContext 重写了 connect 方法, 因此实际上调用的是 HeadContext.connect. 我们接着跟踪到 HeadContext.connect, 其代码如下:\n\n@Override\npublic void connect(\n        ChannelHandlerContext ctx,\n        SocketAddress remoteAddress, SocketAddress localAddress,\n        ChannelPromise promise) throws Exception {\n    unsafe.connect(remoteAddress, localAddress, promise);\n}\n\n\n这个 connect 方法很简单, 仅仅调用了 unsafe 的 connect 方法. 而 unsafe 又是什么呢? 回顾一下 HeadContext 的构造器, 我们发现 unsafe 是 pipeline.channel().unsafe() 返回的, 而 Channel 的 unsafe 字段, 在这个例子中, 我们已经知道了, 其实是 AbstractNioByteChannel.NioByteUnsafe 内部类. 兜兜转转了一大圈, 我们找到了创建 Socket 连接的关键代码. 进行跟踪 NioByteUnsafe -> AbstractNioUnsafe.connect:\n\n@Override\npublic final void connect(\n        final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) {\n    boolean wasActive = isActive();\n    if (doConnect(remoteAddress, localAddress)) {\n        fulfillConnectPromise(promise, wasActive);\n    } else {\n        ...\n    }\n}\n\n\nAbstractNioUnsafe.connect 的实现如上代码所示, 在这个 connect 方法中, 调用了 doConnect 方法, 注意, 这个方法并不是 AbstractNioUnsafe 的方法, 而是 AbstractNioChannel 的抽象方法. doConnect 方法是在 NioSocketChannel 中实现的, 因此进入到 NioSocketChannel.doConnect 中:\n\n@Override\nprotected boolean doConnect(SocketAddress remoteAddress, SocketAddress localAddress) throws Exception {\n    if (localAddress != null) {\n        javaChannel().socket().bind(localAddress);\n    }\n\n    boolean success = false;\n    try {\n        boolean connected = javaChannel().connect(remoteAddress);\n        if (!connected) {\n            selectionKey().interestOps(SelectionKey.OP_CONNECT);\n        }\n        success = true;\n        return connected;\n    } finally {\n        if (!success) {\n            doClose();\n        }\n    }\n}\n\n\n我们终于看到的最关键的部分了, 庆祝一下! 上面的代码不用多说, 首先是获取 Java NIO SocketChannel, 即我们已经分析过的, 从 NioSocketChannel.newSocket 返回的 SocketChannel 对象; 然后是调用 SocketChannel.connect 方法完成 Java NIO 层面上的 Socket 的连接. 最后, 上面的代码流程可以用如下时序图直观地展示:\n\n\n\n\n# 总结\n\n\n# 参考资料\n\n * https://github.com/yongshun/learn_netty_source_code',normalizedContent:'# 前言\n\n这一章是 netty 源码分析系列的第一章, 我打算在这一章中, 展示一下 netty 的客户端和服务端的初始化和启动的流程, 给读者一个对 netty 源码有一个大致的框架上的认识, 而不会深入每个功能模块. 本章会从 bootstrap/serverbootstrap 类 入手, 分析 netty 程序的初始化和启动的流程.\n\n\n# bootstrap\n\nbootstrap 是 netty 提供的一个便利的工厂类, 我们可以通过它来完成 netty 的客户端或服务器端的 netty 初始化. 下面我以 netty 源码例子中的 echo 服务器作为例子, 从客户端和服务器端分别分析一下 netty 的程序是如何启动的.\n\n\n# 客户端部分\n\n\n# 连接源码\n\n首先, 让我们从客户端方面的代码开始 下面是源码 example/src/main/java/io/netty/example/echo/echoclient.java 的客户端部分的启动代码:\n\neventloopgroup group = new nioeventloopgroup();\ntry {\n    bootstrap b = new bootstrap();\n    b.group(group)\n     .channel(niosocketchannel.class)\n     .option(channeloption.tcp_nodelay, true)\n     .handler(new channelinitializer<socketchannel>() {\n         @override\n         public void initchannel(socketchannel ch) throws exception {\n             channelpipeline p = ch.pipeline();\n             p.addlast(new echoclienthandler());\n         }\n     });\n\n    // start the client.\n    channelfuture f = b.connect(host, port).sync();\n\n    // wait until the connection is closed.\n    f.channel().closefuture().sync();\n} finally {\n    // shut down the event loop to terminate all threads.\n    group.shutdowngracefully();\n}\n\n\n从上面的客户端代码虽然简单, 但是却展示了 netty 客户端初始化时所需的所有内容：\n\n 1. eventloopgroup：不论是服务器端还是客户端, 都必须指定 eventloopgroup. 在这个例子中, 指定了 nioeventloopgroup, 表示一个 nio 的 eventloopgroup\n 2. channeltype: 指定 channel 的类型。因为是客户端, 因此使用了 niosocketchannel.\n 3. handler: 设置数据的处理器\n\n下面我们深入代码，看一下客户端通过 bootstrap 启动后，都做了哪些工作.\n\n\n# niosocketchannel 的初始化过程\n\n在 netty 中, channel 是一个 socket 的抽象, 它为用户提供了关于 socket 状态(是否是连接还是断开) 以及对 socket 的读写等操作. 每当 netty 建立了一个连接后, 都会有一个对应的 channel 实例。niosocketchannel 的类层次结构如下：\n\n\n\n这一小节我们着重分析一下 channel 的初始化过程.\n\n# channelfactory 和 channel 类型的确定\n\n除了 tcp 协议以外, netty 还支持很多其他的连接协议, 并且每种协议还有 nio(异步 io) 和 oio(old-io, 即传统的阻塞 io) 版本的区别. 不同协议不同的阻塞类型的连接都有不同的 channel 类型与之对应下面是一些常用的 channel 类型:\n\n * niosocketchannel, 代表异步的客户端 tcp socket 连接\n * nioserversocketchannel, 异步的服务器端 tcp socket 连接\n * niodatagramchannel, 异步的 udp 连接\n * niosctpchannel, 异步的客户端 sctp 连接\n * niosctpserverchannel, 异步的 sctp 服务器端连接\n * oiosocketchannel, 同步的客户端 tcp socket 连接\n * oioserversocketchannel, 同步的服务器端 tcp socket 连接\n * oiodatagramchannel, 同步的 udp 连接\n * oiosctpchannel, 同步的 sctp 服务器端连接\n * oiosctpserverchannel, 同步的客户端 tcp socket 连接\n\n那么我们是如何设置所需要的 channel 的类型的呢? 答案是 channel() 方法的调用. 回想一下我们在客户端连接代码的初始化 bootstrap 中, 会调用 channel() 方法, 传入 niosocketchannel.class, 这个方法其实就是初始化了一个 bootstrapchannelfactory：\n\npublic b channel(class<? extends c> channelclass) {\n    if (channelclass == null) {\n        throw new nullpointerexception("channelclass");\n    }\n    return channelfactory(new bootstrapchannelfactory<c>(channelclass));\n}\n\n\n而 bootstrapchannelfactory 实现了 channelfactory 接口, 它提供了唯一的方法, 即 newchannel. channelfactory, 顾名思义, 就是产生 channel 的工厂类. 进入到 bootstrapchannelfactory.newchannel 中, 我们看到其实现代码如下:\n\n@override\npublic t newchannel() {\n\t// 删除 try 块\n    return clazz.newinstance();\n}\n\n\n根据上面代码的提示, 我们就可以确定:\n\n * bootstrap 中的 channelfactory 的实现是 bootstrapchannelfactory\n * 生成的 channel 的具体类型是 niosocketchannel. channel 的实例化过程, 其实就是调用的 channelfactory#newchannel 方法, 而实例化的 channel 的具体的类型又是和在初始化 bootstrap 时传入的 channel() 方法的参数相关. 因此对于我们这个例子中的客户端的 bootstrap 而言, 生成的的 channel 实例就是 niosocketchannel\n\n# channel 实例化\n\n前面我们已经知道了如何确定一个 channel 的类型, 并且了解到 channel 是通过工厂方法 channelfactory.newchannel() 来实例化的, 那么 channelfactory.newchannel() 方法在哪里调用呢？继续跟踪, 我们发现其调用链是:\n\nbootstrap.connect -> bootstrap.doconnect -> abstractbootstrap.initandregister\n\n\n在 abstractbootstrap.initandregister 中就调用了 channelfactory().newchannel() 来获取一个新的 niosocketchannel 实例, 其源码如下:\n\nfinal channelfuture initandregister() {\n\t// 去掉非关键代码\n    final channel channel = channelfactory().newchannel();\n    init(channel);\n    channelfuture regfuture = group().register(channel);\n}\n\n\n在 newchannel 中, 通过类对象的 newinstance 来获取一个新 channel 实例, 因而会调用 niosocketchannel 的默认构造器. niosocketchannel 默认构造器代码如下:\n\npublic niosocketchannel() {\n    this(newsocket(default_selector_provider));\n}\n\n\n这里的代码比较关键, 我们看到, 在这个构造器中, 会调用 newsocket 来打开一个新的 java nio socketchannel:\n\nprivate static socketchannel newsocket(selectorprovider provider) {\n    ...\n    return provider.opensocketchannel();\n}\n\n\n接着会调用父类, 即 abstractniobytechannel 的构造器:\n\nabstractniobytechannel(channel parent, selectablechannel ch)\n\n\n并传入参数 parent 为 null, ch 为刚才使用 newsocket 创建的 java nio socketchannel, 因此生成的 niosocketchannel 的 parent channel 是空的.\n\nprotected abstractniobytechannel(channel parent, selectablechannel ch) {\n    super(parent, ch, selectionkey.op_read);\n}\n\n\n接着会继续调用父类 abstractniochannel 的构造器, 并传入了参数 readinterestop = selectionkey.op_read:\n\nprotected abstractniochannel(channel parent, selectablechannel ch, int readinterestop) {\n    super(parent);\n    this.ch = ch;\n    this.readinterestop = readinterestop;\n    // 省略 try 块\n    // 配置 java nio socketchannel 为非阻塞的.\n    ch.configureblocking(false);\n}\n\n\n然后继续调用父类 abstractchannel 的构造器:\n\nprotected abstractchannel(channel parent) {\n    this.parent = parent;\n    unsafe = newunsafe();\n    pipeline = new defaultchannelpipeline(this);\n}\n\n\n到这里, 一个完整的 niosocketchannel 就初始化完成了, 我们可以稍微总结一下构造一个 niosocketchannel 所需要做的工作:\n\n * 调用 niosocketchannel.newsocket(default_selector_provider) 打开一个新的 java nio socketchannel\n * abstractchannel(channel parent) 中初始化 abstractchannel 的属性:\n * parent 属性置为 null\n * unsafe 通过 newunsafe() 实例化一个 unsafe 对象, 它的类型是 abstractniobytechannel.niobyteunsafe 内部类\n * pipeline 是 new defaultchannelpipeline(this) 新创建的实例. 这里体现了:each channel has its own pipeline and it is created automatically when a new channel is created.\n * abstractniochannel 中的属性:\n * selectablechannel ch 被设置为 java socketchannel, 即 niosocketchannel#newsocket 返回的 java nio socketchannel.\n * readinterestop 被设置为 selectionkey.op_read\n * selectablechannel ch 被配置为非阻塞的 ch.configureblocking(false)\n * niosocketchannel 中的属性:\n * socketchannelconfig config = new niosocketchannelconfig(this, socket.socket())\n\n\n# 关于 unsafe 字段的初始化\n\n我们简单地提到了, 在实例化 niosocketchannel 的过程中, 会在父类 abstractchannel 的构造器中, 调用 newunsafe() 来获取一个 unsafe 实例. 那么 unsafe 是怎么初始化的呢? 它的作用是什么? 其实 unsafe 特别关键, 它封装了对 java 底层 socket 的操作, 因此实际上是沟通 netty 上层和 java 底层的重要的桥梁.\n\n那么我们就来看一下 unsafe 接口所提供的方法吧:\n\ninterface unsafe {\n    socketaddress localaddress();\n    socketaddress remoteaddress();\n    void register(eventloop eventloop, channelpromise promise);\n    void bind(socketaddress localaddress, channelpromise promise);\n    void connect(socketaddress remoteaddress, socketaddress localaddress, channelpromise promise);\n    void disconnect(channelpromise promise);\n    void close(channelpromise promise);\n    void closeforcibly();\n    void deregister(channelpromise promise);\n    void beginread();\n    void write(object msg, channelpromise promise);\n    void flush();\n    channelpromise voidpromise();\n    channeloutboundbuffer outboundbuffer();\n}\n\n\n一看便知, 这些方法其实都会对应到相关的 java 底层的 socket 的操作. 回到 abstractchannel 的构造方法中, 在这里调用了 newunsafe() 获取一个新的 unsafe 对象, 而 newunsafe 方法在 niosocketchannel 中被重写了:\n\n@override\nprotected abstractniounsafe newunsafe() {\n    return new niosocketchannelunsafe();\n}\n\n\nniosocketchannel.newunsafe 方法会返回一个 niosocketchannelunsafe 实例. 从这里我们就可以确定了, 在实例化的 niosocketchannel 中的 unsafe 字段, 其实是一个 niosocketchannelunsafe 的实例.\n\n\n# 关于 pipeline 的初始化\n\n上面我们分析了一个 channel (在这个例子中是 niosocketchannel) 的大体初始化过程, 但是我们漏掉了一个关键的部分, 即 channelpipeline 的初始化. 根据 each channel has its own pipeline and it is created automatically when a new channel is created., 我们知道, 在实例化一个 channel 时, 必然伴随着实例化一个 channelpipeline. 而我们确实在 abstractchannel 的构造器看到了 pipeline 字段被初始化为 defaultchannelpipeline 的实例. 那么我们就来看一下, defaultchannelpipeline 构造器做了哪些工作吧:\n\npublic defaultchannelpipeline(abstractchannel channel) {\n    if (channel == null) {\n        throw new nullpointerexception("channel");\n    }\n    this.channel = channel;\n\n    tail = new tailcontext(this);\n    head = new headcontext(this);\n\n    head.next = tail;\n    tail.prev = head;\n}\n\n\n我们调用 defaultchannelpipeline 的构造器, 传入了一个 channel, 而这个 channel 其实就是我们实例化的 niosocketchannel, defaultchannelpipeline 会将这个 niosocketchannel 对象保存在 channel 字段中. defaultchannelpipeline 中, 还有两个特殊的字段, 即 head 和 tail, 而这两个字段是一个双向链表的头和尾. 其实在 defaultchannelpipeline 中, 维护了一个以 abstractchannelhandlercontext 为节点的双向链表, 这个链表是 netty 实现 pipeline 机制的关键. 关于 defaultchannelpipeline 中的双向链表以及它所起的作用, 我在这里暂时不表, 在 netty 源码分析之 二 贯穿 netty 的大动脉 ── channelpipeline 中会有详细的分析.\n\nheadcontext 的继承层次结构如下所示: [](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端)/headcontext.png) tailcontext 的继承层次结构如下所示: [](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端)/tailcontext.png)\n\n我们可以看到, 链表中 head 是一个 channeloutboundhandler, 而 tail 则是一个 channelinboundhandler. 接着看一下 headcontext 的构造器:\n\nheadcontext(defaultchannelpipeline pipeline) {\n    super(pipeline, null, head_name, false, true);\n    unsafe = pipeline.channel().unsafe();\n}\n\n\n它调用了父类 abstractchannelhandlercontext 的构造器, 并传入参数 inbound = false, outbound = true. tailcontext 的构造器与 headcontext 的相反, 它调用了父类 abstractchannelhandlercontext 的构造器, 并传入参数 inbound = true, outbound = false. 即 header 是一个 outboundhandler, 而 tail 是一个 inboundhandler, 关于这一点, 大家要特别注意, 因为在分析到 netty pipeline 时, 我们会反复用到 inbound 和 outbound 这两个属性.\n\n\n# 关于 eventloop 初始化\n\n回到最开始的 echoclient.java 代码中, 我们在一开始就实例化了一个 nioeventloopgroup 对象, 因此我们就从它的构造器中追踪一下 eventloop 的初始化过程. 首先来看一下 nioeventloopgroup 的类继承层次:\n\nnioeventloop 有几个重载的构造器, 不过内容都没有什么区别, 最终都是调用的父类 multithreadeventloopgroup 构造器:\n\nprotected multithreadeventloopgroup(int nthreads, threadfactory threadfactory, object... args) {\n    super(nthreads == 0? default_event_loop_threads : nthreads, threadfactory, args);\n}\n\n\n其中有一点有意思的地方是, 如果我们传入的线程数 nthreads 是 0, 那么 netty 会为我们设置默认的线程数 default_event_loop_threads, 而这个默认的线程数是怎么确定的呢? 其实很简单, 在静态代码块中, 会首先确定 default_event_loop_threads 的值:\n\nstatic {\n    default_event_loop_threads = math.max(1, systempropertyutil.getint(\n            "io.netty.eventloopthreads", runtime.getruntime().availableprocessors() * 2));\n}\n\n\nnetty 会首先从系统属性中获取 "io.netty.eventloopthreads" 的值, 如果我们没有设置它的话, 那么就返回默认值: 处理器核心数 * 2.\n\n回到 multithreadeventloopgroup 构造器中, 这个构造器会继续调用父类 multithreadeventexecutorgroup 的构造器:\n\nprotected multithreadeventexecutorgroup(int nthreads, threadfactory threadfactory, object... args) {\n    // 去掉了参数检查, 异常处理 等代码.\n    children = new singlethreadeventexecutor[nthreads];\n    if (ispoweroftwo(children.length)) {\n        chooser = new poweroftwoeventexecutorchooser();\n    } else {\n        chooser = new genericeventexecutorchooser();\n    }\n\n    for (int i = 0; i < nthreads; i ++) {\n        children[i] = newchild(threadfactory, args);\n    }\n}\n\n\n根据代码, 我们就很清楚 multithreadeventexecutorgroup 中的处理逻辑了:\n\n * 创建一个大小为 nthreads 的 singlethreadeventexecutor 数组\n * 根据 nthreads 的大小, 创建不同的 chooser, 即如果 nthreads 是 2 的幂, 则使用 poweroftwoeventexecutorchooser, 反之使用 genericeventexecutorchooser. 不论使用哪个 chooser, 它们的功能都是一样的, 即从 children 数组中选出一个合适的 eventexecutor 实例.\n * 调用 newchhild 方法初始化 children 数组.\n\n根据上面的代码, 我们知道, multithreadeventexecutorgroup 内部维护了一个 eventexecutor 数组, netty 的 eventloopgroup 的实现机制其实就建立在 multithreadeventexecutorgroup 之上. 每当 netty 需要一个 eventloop 时, 会调用 next() 方法获取一个可用的 eventloop. 上面代码的最后一部分是 newchild 方法, 这个是一个抽象方法, 它的任务是实例化 eventloop 对象. 我们跟踪一下它的代码, 可以发现, 这个方法在 nioeventloopgroup 类中实现了, 其内容很简单:\n\n@override\nprotected eventexecutor newchild(\n        threadfactory threadfactory, object... args) throws exception {\n    return new nioeventloop(this, threadfactory, (selectorprovider) args[0]);\n}\n\n\n其实就是实例化一个 nioeventloop 对象, 然后返回它.\n\n最后总结一下整个 eventloopgroup 的初始化过程吧:\n\n * eventloopgroup(其实是 multithreadeventexecutorgroup) 内部维护一个类型为 eventexecutor children 数组, 其大小是 nthreads, 这样就构成了一个线程池\n * 如果我们在实例化 nioeventloopgroup 时, 如果指定线程池大小, 则 nthreads 就是指定的值, 反之是处理器核心数 * 2\n * multithreadeventexecutorgroup 中会调用 newchild 抽象方法来初始化 children 数组\n * 抽象方法 newchild 是在 nioeventloopgroup 中实现的, 它返回一个 nioeventloop 实例.\n * nioeventloop 属性:\n * selectorprovider provider 属性: nioeventloopgroup 构造器中通过 selectorprovider.provider() 获取一个 selectorprovider\n * selector selector 属性: nioeventloop 构造器中通过调用通过 selector = provider.openselector() 获取一个 selector 对象.\n\n\n# channel 的注册过程\n\n在前面的分析中, 我们提到, channel 会在 bootstrap.initandregister 中进行初始化, 但是这个方法还会将初始化好的 channel 注册到 eventgroup 中. 接下来我们就来分析一下 channel 注册的过程. 回顾一下 abstractbootstrap.initandregister 方法:\n\nfinal channelfuture initandregister() {\n\t// 去掉非关键代码\n    final channel channel = channelfactory().newchannel();\n    init(channel);\n    channelfuture regfuture = group().register(channel);\n}\n\n\n当 channel 初始化后, 会紧接着调用 group().register() 方法来注册 channel, 我们继续跟踪的话, 会发现其调用链如下: abstractbootstrap.initandregister -> multithreadeventloopgroup.register -> singlethreadeventloop.register -> abstractunsafe.register 通过跟踪调用链, 最终我们发现是调用到了 unsafe 的 register 方法, 那么接下来我们就仔细看一下 abstractunsafe.register 方法中到底做了什么:\n\n@override\npublic final void register(eventloop eventloop, final channelpromise promise) {\n\t// 省略条件判断和错误处理\n    abstractchannel.this.eventloop = eventloop;\n    register0(promise);\n}\n\n\n首先, 将 eventloop 赋值给 channel 的 eventloop 属性, 而我们知道这个 eventloop 对象其实是 multithreadeventloopgroup.next() 方法获取的, 根据我们前面 关于 eventloop 初始化 小节中, 我们可以确定 next() 方法返回的 eventloop 对象是 nioeventloop 实例. register 方法接着调用了 register0 方法:\n\nprivate void register0(channelpromise promise) {\n    boolean firstregistration = neverregistered;\n    doregister();\n    neverregistered = false;\n    registered = true;\n    safesetsuccess(promise);\n    pipeline.firechannelregistered();\n    // only fire a channelactive if the channel has never been registered. this prevents firing\n    // multiple channel actives if the channel is deregistered and re-registered.\n    if (firstregistration && isactive()) {\n        pipeline.firechannelactive();\n    }\n}\n\n\nregister0 又调用了 abstractniochannel.doregister:\n\n@override\nprotected void doregister() throws exception {\n\t// 省略错误处理\n    selectionkey = javachannel().register(eventloop().selector, 0, this);\n}\n\n\njavachannel() 这个方法在前面我们已经知道了, 它返回的是一个 java nio socketchannel, 这里我们将这个 socketchannel 注册到与 eventloop 关联的 selector 上了.\n\n我们总结一下 channel 的注册过程:\n\n * 首先在 abstractbootstrap.initandregister 中, 通过 group().register(channel), 调用 multithreadeventloopgroup.register 方法\n * 在 multithreadeventloopgroup.register 中, 通过 next() 获取一个可用的 singlethreadeventloop, 然后调用它的 register\n * 在 singlethreadeventloop.register 中, 通过 channel.unsafe().register(this, promise) 来获取 channel 的 unsafe() 底层操作对象, 然后调用它的 register.\n * 在 abstractunsafe.register 方法中, 调用 register0 方法注册 channel\n * 在 abstractunsafe.register0 中, 调用 abstractniochannel.doregister 方法\n * abstractniochannel.doregister 方法通过 javachannel().register(eventloop().selector, 0, this) 将 channel 对应的 java nio sockerchannel 注册到一个 eventloop 的 selector 中, 并且将当前 channel 作为 attachment.\n\n总的来说, channel 注册过程所做的工作就是将 channel 与对应的 eventloop 关联, 因此这也体现了, 在 netty 中, 每个 channel 都会关联一个特定的 eventloop, 并且这个 channel 中的所有 io 操作都是在这个 eventloop 中执行的; 当关联好 channel 和 eventloop 后, 会继续调用底层的 java nio socketchannel 的 register 方法, 将底层的 java nio socketchannel 注册到指定的 selector 中. 通过这两步, 就完成了 netty channel 的注册过程.\n\n\n# handler 的添加过程\n\nnetty 的一个强大和灵活之处就是基于 pipeline 的自定义 handler 机制. 基于此, 我们可以像添加插件一样自由组合各种各样的 handler 来完成业务逻辑. 例如我们需要处理 http 数据, 那么就可以在 pipeline 前添加一个 http 的编解码的 handler, 然后接着添加我们自己的业务逻辑的 handler, 这样网络上的数据流就向通过一个管道一样, 从不同的 handler 中流过并进行编解码, 最终在到达我们自定义的 handler 中. 既然说到这里, 有些读者朋友肯定会好奇, 既然这个 pipeline 机制是这么的强大, 那么它是怎么实现的呢? 不过我这里不打算详细展开 netty 的 channelpipeline 的实现机制(具体的细节会在后续的章节中展示), 我在这一小节中, 从简单的入手, 展示一下我们自定义的 handler 是如何以及何时添加到 channelpipeline 中的. 首先让我们看一下如下的代码片段:\n\n...\n.handler(new channelinitializer<socketchannel>() {\n     @override\n     public void initchannel(socketchannel ch) throws exception {\n         channelpipeline p = ch.pipeline();\n         if (sslctx != null) {\n             p.addlast(sslctx.newhandler(ch.alloc(), host, port));\n         }\n         //p.addlast(new logginghandler(loglevel.info));\n         p.addlast(new echoclienthandler());\n     }\n });\n\n\n这个代码片段就是实现了 handler 的添加功能. 我们看到, bootstrap.handler 方法接收一个 channelhandler, 而我们传递的是一个 派生于 channelinitializer 的匿名类, 它正好也实现了 channelhandler 接口. 我们来看一下, channelinitializer 类内到底有什么玄机:\n\n@sharable\npublic abstract class channelinitializer<c extends channel> extends channelinboundhandleradapter {\n\n    private static final internallogger logger = internalloggerfactory.getinstance(channelinitializer.class);\n    protected abstract void initchannel(c ch) throws exception;\n\n    @override\n    @suppresswarnings("unchecked")\n    public final void channelregistered(channelhandlercontext ctx) throws exception {\n        initchannel((c) ctx.channel());\n        ctx.pipeline().remove(this);\n        ctx.firechannelregistered();\n    }\n    ...\n}\n\n\nchannelinitializer 是一个抽象类, 它有一个抽象的方法 initchannel, 我们正是实现了这个方法, 并在这个方法中添加的自定义的 handler 的. 那么 initchannel 是哪里被调用的呢? 答案是 channelinitializer.channelregistered 方法中. 我们来关注一下 channelregistered 方法. 从上面的源码中, 我们可以看到, 在 channelregistered 方法中, 会调用 initchannel 方法, 将自定义的 handler 添加到 channelpipeline 中, 然后调用 ctx.pipeline().remove(this) 将自己从 channelpipeline 中删除. 上面的分析过程, 可以用如下图片展示: 一开始, channelpipeline 中只有三个 handler, head, tail 和我们添加的 channelinitializer. [](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端)/1477130291691.png) 接着 initchannel 方法调用后, 添加了自定义的 handler: [](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端)/1477130295919.png) 最后将 channelinitializer 删除: [](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端)/1477130299722.png)\n\n分析到这里, 我们已经简单了解了自定义的 handler 是如何添加到 channelpipeline 中的, 不过限于主题与篇幅的原因, 我没有在这里详细展开 channelpipeline 的底层机制, 我打算在下一篇 netty 源码分析之 二 贯穿 netty 的大动脉 ── channelpipeline 中对这个问题进行深入的探讨.\n\n\n# 客户端连接分析\n\n经过上面的各种分析后, 我们大致了解了 netty 初始化时, 所做的工作, 那么接下来我们就直奔主题, 分析一下客户端是如何发起 tcp 连接的.\n\n首先, 客户端通过调用 bootstrap 的 connect 方法进行连接. 在 connect 中, 会进行一些参数检查后, 最终调用的是 doconnect0 方法, 其实现如下:\n\nprivate static void doconnect0(\n        final channelfuture regfuture, final channel channel,\n        final socketaddress remoteaddress, final socketaddress localaddress, final channelpromise promise) {\n\n    // this method is invoked before channelregistered() is triggered.  give user handlers a chance to set up\n    // the pipeline in its channelregistered() implementation.\n    channel.eventloop().execute(new runnable() {\n        @override\n        public void run() {\n            if (regfuture.issuccess()) {\n                if (localaddress == null) {\n                    channel.connect(remoteaddress, promise);\n                } else {\n                    channel.connect(remoteaddress, localaddress, promise);\n                }\n                promise.addlistener(channelfuturelistener.close_on_failure);\n            } else {\n                promise.setfailure(regfuture.cause());\n            }\n        }\n    });\n}\n\n\n在 doconnect0 中, 会在 eventloop 线程中调用 channel 的 connect 方法, 而这个 channel 的具体类型是什么呢? 我们在 channel 初始化这一小节中已经分析过了, 这里 channel 的类型就是 niosocketchannel. 进行跟踪到 channel.connect 中, 我们发现它调用的是 defaultchannelpipeline#connect, 而, pipeline 的 connect 代码如下:\n\n@override\npublic channelfuture connect(socketaddress remoteaddress) {\n    return tail.connect(remoteaddress);\n}\n\n\n而 tail 字段, 我们已经分析过了, 是一个 tailcontext 的实例, 而 tailcontext 又是 abstractchannelhandlercontext 的子类, 并且没有实现 connect 方法, 因此这里调用的其实是 abstractchannelhandlercontext.connect, 我们看一下这个方法的实现:\n\n@override\npublic channelfuture connect(\n        final socketaddress remoteaddress, final socketaddress localaddress, final channelpromise promise) {\n\n\t// 删除的参数检查的代码\n    final abstractchannelhandlercontext next = findcontextoutbound();\n    eventexecutor executor = next.executor();\n    if (executor.ineventloop()) {\n        next.invokeconnect(remoteaddress, localaddress, promise);\n    } else {\n        safeexecute(executor, new onetimetask() {\n            @override\n            public void run() {\n                next.invokeconnect(remoteaddress, localaddress, promise);\n            }\n        }, promise, null);\n    }\n\n    return promise;\n}\n\n\n上面的代码中有一个关键的地方, 即 final abstractchannelhandlercontext next = findcontextoutbound(), 这里调用 findcontextoutbound 方法, 从 defaultchannelpipeline 内的双向链表的 tail 开始, 不断向前寻找第一个 outbound 为 true 的 abstractchannelhandlercontext, 然后调用它的 invokeconnect 方法, 其代码如下:\n\nprivate void invokeconnect(socketaddress remoteaddress, socketaddress localaddress, channelpromise promise) {\n    // 忽略 try 块\n    ((channeloutboundhandler) handler()).connect(this, remoteaddress, localaddress, promise);\n}\n\n\n还记得我们在 "关于 pipeline 的初始化" 这一小节分析的的内容吗? 我们提到, 在 defaultchannelpipeline 的构造器中, 会实例化两个对象: head 和 tail, 并形成了双向链表的头和尾. head 是 headcontext 的实例, 它实现了 channeloutboundhandler 接口, 并且它的 outbound 字段为 true. 因此在 findcontextoutbound 中, 找到的 abstractchannelhandlercontext 对象其实就是 head. 进而在 invokeconnect 方法中, 我们向上转换为 channeloutboundhandler 就是没问题的了. 而又因为 headcontext 重写了 connect 方法, 因此实际上调用的是 headcontext.connect. 我们接着跟踪到 headcontext.connect, 其代码如下:\n\n@override\npublic void connect(\n        channelhandlercontext ctx,\n        socketaddress remoteaddress, socketaddress localaddress,\n        channelpromise promise) throws exception {\n    unsafe.connect(remoteaddress, localaddress, promise);\n}\n\n\n这个 connect 方法很简单, 仅仅调用了 unsafe 的 connect 方法. 而 unsafe 又是什么呢? 回顾一下 headcontext 的构造器, 我们发现 unsafe 是 pipeline.channel().unsafe() 返回的, 而 channel 的 unsafe 字段, 在这个例子中, 我们已经知道了, 其实是 abstractniobytechannel.niobyteunsafe 内部类. 兜兜转转了一大圈, 我们找到了创建 socket 连接的关键代码. 进行跟踪 niobyteunsafe -> abstractniounsafe.connect:\n\n@override\npublic final void connect(\n        final socketaddress remoteaddress, final socketaddress localaddress, final channelpromise promise) {\n    boolean wasactive = isactive();\n    if (doconnect(remoteaddress, localaddress)) {\n        fulfillconnectpromise(promise, wasactive);\n    } else {\n        ...\n    }\n}\n\n\nabstractniounsafe.connect 的实现如上代码所示, 在这个 connect 方法中, 调用了 doconnect 方法, 注意, 这个方法并不是 abstractniounsafe 的方法, 而是 abstractniochannel 的抽象方法. doconnect 方法是在 niosocketchannel 中实现的, 因此进入到 niosocketchannel.doconnect 中:\n\n@override\nprotected boolean doconnect(socketaddress remoteaddress, socketaddress localaddress) throws exception {\n    if (localaddress != null) {\n        javachannel().socket().bind(localaddress);\n    }\n\n    boolean success = false;\n    try {\n        boolean connected = javachannel().connect(remoteaddress);\n        if (!connected) {\n            selectionkey().interestops(selectionkey.op_connect);\n        }\n        success = true;\n        return connected;\n    } finally {\n        if (!success) {\n            doclose();\n        }\n    }\n}\n\n\n我们终于看到的最关键的部分了, 庆祝一下! 上面的代码不用多说, 首先是获取 java nio socketchannel, 即我们已经分析过的, 从 niosocketchannel.newsocket 返回的 socketchannel 对象; 然后是调用 socketchannel.connect 方法完成 java nio 层面上的 socket 的连接. 最后, 上面的代码流程可以用如下时序图直观地展示:\n\n\n\n\n# 总结\n\n\n# 参考资料\n\n * https://github.com/yongshun/learn_netty_source_code',charsets:{cjk:!0},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"Bootstrap（server）源码解析",frontmatter:{title:"Bootstrap（server）源码解析",date:"2024-09-18T21:59:55.000Z",permalink:"/pages/b2a14a/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/20.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/03.Bootstrap%EF%BC%88server%EF%BC%89%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html",relativePath:"Netty 系统设计/20.三、主线任务/03.Bootstrap（server）源码解析.md",key:"v-74249d63",path:"/pages/b2a14a/",headers:[{level:2,title:"服务器端",slug:"服务器端",normalizedTitle:"服务器端",charIndex:2},{level:3,title:"Channel 的初始化过程",slug:"channel-的初始化过程",normalizedTitle:"channel 的初始化过程",charIndex:2329},{level:4,title:"Channel 类型的确定",slug:"channel-类型的确定",normalizedTitle:"channel 类型的确定",charIndex:2573},{level:4,title:"NioServerSocketChannel 的实例化过程",slug:"nioserversocketchannel-的实例化过程",normalizedTitle:"nioserversocketchannel 的实例化过程",charIndex:3449},{level:3,title:"ChannelPipeline 初始化",slug:"channelpipeline-初始化",normalizedTitle:"channelpipeline 初始化",charIndex:5917},{level:3,title:"Channel 的注册",slug:"channel-的注册",normalizedTitle:"channel 的注册",charIndex:5988},{level:3,title:"关于 bossGroup 与 workerGroup",slug:"关于-bossgroup-与-workergroup",normalizedTitle:"关于 bossgroup 与 workergroup",charIndex:6044},{level:3,title:"handler 的添加过程",slug:"handler-的添加过程",normalizedTitle:"handler 的添加过程",charIndex:10073},{level:2,title:"后记",slug:"后记",normalizedTitle:"后记",charIndex:13550}],headersStr:"服务器端 Channel 的初始化过程 Channel 类型的确定 NioServerSocketChannel 的实例化过程 ChannelPipeline 初始化 Channel 的注册 关于 bossGroup 与 workerGroup handler 的添加过程 后记",content:'# 服务器端\n\n在分析客户端的代码时, 我们已经对 Bootstrap 启动 Netty 有了一个大致的认识, 那么接下来分析服务器端时, 就会相对简单一些了. 首先还是来看一下服务器端的启动代码:\n\npublic final class EchoServer {\n\n    static final boolean SSL = System.getProperty("ssl") != null;\n    static final int PORT = Integer.parseInt(System.getProperty("port", "8007"));\n\n    public static void main(String[] args) throws Exception {\n        // Configure SSL.\n        final SslContext sslCtx;\n        if (SSL) {\n            SelfSignedCertificate ssc = new SelfSignedCertificate();\n            sslCtx = SslContextBuilder.forServer(ssc.certificate(), ssc.privateKey()).build();\n        } else {\n            sslCtx = null;\n        }\n\n        // Configure the server.\n        EventLoopGroup bossGroup = new NioEventLoopGroup(1);\n        EventLoopGroup workerGroup = new NioEventLoopGroup();\n        try {\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)\n             .channel(NioServerSocketChannel.class)\n             .option(ChannelOption.SO_BACKLOG, 100)\n             .handler(new LoggingHandler(LogLevel.INFO))\n             .childHandler(new ChannelInitializer<SocketChannel>() {\n                 @Override\n                 public void initChannel(SocketChannel ch) throws Exception {\n                     ChannelPipeline p = ch.pipeline();\n                     if (sslCtx != null) {\n                         p.addLast(sslCtx.newHandler(ch.alloc()));\n                     }\n                     //p.addLast(new LoggingHandler(LogLevel.INFO));\n                     p.addLast(new EchoServerHandler());\n                 }\n             });\n\n            // Start the server.\n            ChannelFuture f = b.bind(PORT).sync();\n\n            // Wait until the server socket is closed.\n            f.channel().closeFuture().sync();\n        } finally {\n            // Shut down all event loops to terminate all threads.\n            bossGroup.shutdownGracefully();\n            workerGroup.shutdownGracefully();\n        }\n    }\n}\n\n\n和客户端的代码相比, 没有很大的差别, 基本上也是进行了如下几个部分的初始化:\n\n 1. EventLoopGroup: 不论是服务器端还是客户端, 都必须指定 EventLoopGroup. 在这个例子中, 指定了 NioEventLoopGroup, 表示一个 NIO 的 EventLoopGroup, 不过服务器端需要指定两个 EventLoopGroup, 一个是 bossGroup, 用于处理客户端的连接请求; 另一个是 workerGroup, 用于处理与各个客户端连接的 IO 操作\n 2. ChannelType: 指定 Channel 的类型. 因为是服务器端, 因此使用了 NioServerSocketChannel\n 3. Handler: 设置数据的处理器\n\n\n# Channel 的初始化过程\n\n我们在分析客户端的 Channel 初始化过程时, 已经提到, Channel 是对 Java 底层 Socket 连接的抽象, 并且知道了客户端的 Channel 的具体类型是 NioSocketChannel, 那么自然的, 服务器端的 Channel 类型就是 NioServerSocketChannel 了. 那么接下来我们按照分析客户端的流程对服务器端的代码也同样地分析一遍, 这样也方便我们对比一下服务器端和客户端有哪些不一样的地方.\n\n# Channel 类型的确定\n\n同样的分析套路, 我们已经知道了, 在客户端中, Channel 的类型其实是在初始化时, 通过 Bootstrap.channel() 方法设置的, 服务器端自然也不例外\n\n在服务器端, 我们调用了 ServerBootstarap.channel(NioServerSocketChannel.class), 传递了一个 NioServerSocketChannel Class 对象. 这样的话, 按照和分析客户端代码一样的流程, 我们就可以确定, NioServerSocketChannel 的实例化是通过 BootstrapChannelFactory 工厂类来完成的, 而 BootstrapChannelFactory 中的 clazz 字段被设置为了 NioServerSocketChannel.class, 因此当调用 BootstrapChannelFactory.newChannel() 时:\n\n@Override\npublic T newChannel() {\n\t// 删除 try 块\n    return clazz.newInstance();\n}\n\n\n就获取到了一个 NioServerSocketChannel 的实例.\n\n最后我们也来总结一下:\n\n * ServerBootstrap 中的 ChannelFactory 的实现是 BootstrapChannelFactory\n * 生成的 Channel 的具体类型是 NioServerSocketChannel. Channel 的实例化过程, 其实就是调用的 ChannelFactory.newChannel 方法, 而实例化的 Channel 的具体的类型又是和在初始化 ServerBootstrap 时传入的 channel() 方法的参数相关. 因此对于我们这个例子中的服务器端的 ServerBootstrap 而言, 生成的的 Channel 实例就是 NioServerSocketChannel\n\n# NioServerSocketChannel 的实例化过程\n\n首先还是来看一下 NioServerSocketChannel 的实例化过程. 下面是 NioServerSocketChannel 的类层次结构图:\n\n\n\n首先, 我们来看一下它的默认的构造器. 和 NioSocketChannel 类似, 构造器都是调用了 newSocket 来打开一个 Java 的 NIO Socket, 不过需要注意的是, 客户端的 newSocket 调用的是 openSocketChannel, 而服务器端的 newSocket 调用的是 openServerSocketChannel. 顾名思义, 一个是客户端的 Java SocketChannel, 一个是服务器端的 Java ServerSocketChannel.\n\npublic NioServerSocketChannel() {\n    this(newSocket(DEFAULT_SELECTOR_PROVIDER));\n}\n\nprivate static ServerSocketChannel newSocket(SelectorProvider provider) {\n    return provider.openServerSocketChannel();\n}\n\n\n接下来会调用重载的构造器:\n\npublic NioServerSocketChannel(ServerSocketChannel channel) {\n    super(null, channel, SelectionKey.OP_ACCEPT);\n    config = new NioServerSocketChannelConfig(this, javaChannel().socket());\n}\n\n\n这个构造其中, 调用父类构造器时, 传入的参数是 SelectionKey.OP_ACCEPT. 作为对比, 我们回想一下, 在客户端的 Channel 初始化时, 传入的参数是 SelectionKey.OP_READ\n\n有 Java NIO Socket 开发经验的朋友就知道了, Java NIO 是一种 Reactor 模式, 我们通过 selector 来实现 I/O 的多路复用复用. 在一开始时, 服务器端需要监听客户端的连接请求, 因此在这里我们设置了 SelectionKey.OP_ACCEPT, 即通知 selector 我们对客户端的连接请求感兴趣.\n\n接着和客户端的分析一下, 会逐级地调用父类的构造器 NioServerSocketChannel <- AbstractNioMessageChannel <- AbstractNioChannel <- AbstractChannel.\n\n同样的, 在 AbstractChannel 中会实例化一个 unsafe 和 pipeline:\n\nprotected AbstractChannel(Channel parent) {\n    this.parent = parent;\n    unsafe = newUnsafe();\n    pipeline = new DefaultChannelPipeline(this);\n}\n\n\n不过, 这里有一点需要注意的是, 客户端的 unsafe 是一个 AbstractNioByteChannel#NioByteUnsafe 的实例, 而在服务器端时, 因为 AbstractNioMessageChannel 重写了 newUnsafe 方法:\n\n@Override\nprotected AbstractNioUnsafe newUnsafe() {\n    return new NioMessageUnsafe();\n}\n\n\n因此在服务器端, unsafe 字段其实是一个 AbstractNioMessageChannel#AbstractNioUnsafe 的实例. 我们来总结一下, 在 NioServerSocketChannsl 实例化过程中, 所需要做的工作:\n\n * 调用 NioServerSocketChannel.newSocket(DEFAULT_SELECTOR_PROVIDER) 打开一个新的 Java NIO ServerSocketChannel\n * AbstractChannel(Channel parent) 中初始化 AbstractChannel 的属性:\n * parent 属性置为 null\n * unsafe 通过 newUnsafe() 实例化一个 unsafe 对象, 它的类型是 AbstractNioMessageChannel#AbstractNioUnsafe 内部类\n * pipeline 是 new DefaultChannelPipeline(this) 新创建的实例.\n * AbstractNioChannel 中的属性:\n * SelectableChannel ch 被设置为 Java ServerSocketChannel, 即 NioServerSocketChannel#newSocket 返回的 Java NIO ServerSocketChannel.\n * readInterestOp 被设置为 SelectionKey.OP_ACCEPT\n * SelectableChannel ch 被配置为非阻塞的 ch.configureBlocking(false)\n * NioServerSocketChannel 中的属性:\n * ServerSocketChannelConfig config = new NioServerSocketChannelConfig(this, javaChannel().socket())\n\n\n# ChannelPipeline 初始化\n\n服务器端和客户端的 ChannelPipeline 的初始化一致, 因此就不再单独分析了.\n\n\n# Channel 的注册\n\n服务器端和客户端的 Channel 的注册过程一致, 因此就不再单独分析了.\n\n\n# 关于 bossGroup 与 workerGroup\n\n在客户端的时候, 我们只提供了一个 EventLoopGroup 对象, 而在服务器端的初始化时, 我们设置了两个 EventLoopGroup, 一个是 bossGroup, 另一个是 workerGroup. 那么这两个 EventLoopGroup 都是干什么用的呢? 其实呢, bossGroup 是用于服务端 的 accept 的, 即用于处理客户端的连接请求. 我们可以把 Netty 比作一个饭店, bossGroup 就像一个像一个前台接待, 当客户来到饭店吃时, 接待员就会引导顾客就坐, 为顾客端茶送水等. 而 workerGroup, 其实就是实际上干活的啦, 它们负责客户端连接通道的 IO 操作: 当接待员 招待好顾客后, 就可以稍做休息, 而此时后厨里的厨师们(workerGroup)就开始忙碌地准备饭菜了. 关于 bossGroup 与 workerGroup 的关系, 我们可以用如下图来展示:\n\n\n\n首先, 服务器端 bossGroup 不断地监听是否有客户端的连接, 当发现有一个新的客户端连接到来时, bossGroup 就会为此连接初始化各项资源, 然后从 workerGroup 中选出一个 EventLoop 绑定到此客户端连接中. 那么接下来的服务器与客户端的交互过程就全部在此分配的 EventLoop 中了.\n\n口说无凭, 我们还是以源码说话吧. 首先在 ServerBootstrap 初始化时, 调用了 b.group(bossGroup, workerGroup) 设置了两个 EventLoopGroup, 我们跟踪进去看一下:\n\npublic ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) {\n    super.group(parentGroup);\n    ...\n    this.childGroup = childGroup;\n    return this;\n}\n\n\n显然, 这个方法初始化了两个字段, 一个是 group = parentGroup, 它是在 super.group(parentGroup) 中初始化的, 另一个是 childGroup = childGroup. 接着我们启动程序调用了 b.bind 方法来监听一个本地端口. bind 方法会触发如下的调用链:\n\nAbstractBootstrap.bind -> AbstractBootstrap.doBind -> AbstractBootstrap.initAndRegister\n\n\nAbstractBootstrap.initAndRegister 是我们的老朋友了, 我们在分析客户端程序时, 和它打过很多交到了, 我们再来回顾一下这个方法吧:\n\nfinal ChannelFuture initAndRegister() {\n    final Channel channel = channelFactory().newChannel();\n    ... 省略异常判断\n    init(channel);\n    ChannelFuture regFuture = group().register(channel);\n    return regFuture;\n}\n\n\n这里 group() 方法返回的是上面我们提到的 bossGroup, 而这里的 channel 我们也已经分析过了, 它是一个是一个 NioServerSocketChannel 实例, 因此我们可以知道, group().register(channel) 将 bossGroup 和 NioServerSocketChannel 关联起来了. 那么 workerGroup 是在哪里与 NioSocketChannel 关联的呢? 我们继续看 init(channel) 方法:\n\n@Override\nvoid init(Channel channel) throws Exception {\n    ...\n    ChannelPipeline p = channel.pipeline();\n\n    final EventLoopGroup currentChildGroup = childGroup;\n    final ChannelHandler currentChildHandler = childHandler;\n    final Entry<ChannelOption<?>, Object>[] currentChildOptions;\n    final Entry<AttributeKey<?>, Object>[] currentChildAttrs;\n\n    p.addLast(new ChannelInitializer<Channel>() {\n        @Override\n        public void initChannel(Channel ch) throws Exception {\n            ChannelPipeline pipeline = ch.pipeline();\n            ChannelHandler handler = handler();\n            if (handler != null) {\n                pipeline.addLast(handler);\n            }\n            pipeline.addLast(new ServerBootstrapAcceptor(\n                    currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs));\n        }\n    });\n}\n\n\ninit 方法在 ServerBootstrap 中重写了, 从上面的代码片段中我们看到, 它为 pipeline 中添加了一个 ChannelInitializer, 而这个 ChannelInitializer 中添加了一个关键的 ServerBootstrapAcceptor handler. 关于 handler 的添加与初始化的过程, 我们留待下一小节中分析, 我们现在关注一下 ServerBootstrapAcceptor 类. ServerBootstrapAcceptor 中重写了 channelRead 方法, 其主要代码如下:\n\n@Override\n@SuppressWarnings("unchecked")\npublic void channelRead(ChannelHandlerContext ctx, Object msg) {\n    final Channel child = (Channel) msg;\n    child.pipeline().addLast(childHandler);\n    ...\n    childGroup.register(child).addListener(...);\n}\n\n\nServerBootstrapAcceptor 中的 childGroup 是构造此对象是传入的 currentChildGroup, 即我们的 workerGroup, 而 Channel 是一个 NioSocketChannel 的实例, 因此这里的 childGroup.register 就是将 workerGroup 中的摸个 EventLoop 和 NioSocketChannel 关联了. 既然这样, 那么现在的问题是, ServerBootstrapAcceptor.channelRead 方法是怎么被调用的呢? 其实当一个 client 连接到 server 时, Java 底层的 NIO ServerSocketChannel 会有一个 SelectionKey.OP_ACCEPT 就绪事件, 接着就会调用到 NioServerSocketChannel.doReadMessages:\n\n@Override\nprotected int doReadMessages(List<Object> buf) throws Exception {\n    SocketChannel ch = javaChannel().accept();\n    ... 省略异常处理\n    buf.add(new NioSocketChannel(this, ch));\n    return 1;\n}\n\n\n在 doReadMessages 中, 通过 javaChannel().accept() 获取到客户端新连接的 SocketChannel, 接着就实例化一个 NioSocketChannel, 并且传入 NioServerSocketChannel 对象(即 this), 由此可知, 我们创建的这个 NioSocketChannel 的父 Channel 就是 NioServerSocketChannel 实例 . 接下来就经由 Netty 的 ChannelPipeline 机制, 将读取事件逐级发送到各个 handler 中, 于是就会触发前面我们提到的 ServerBootstrapAcceptor.channelRead 方法啦.\n\n\n# handler 的添加过程\n\n服务器端的 handler 的添加过程和客户端的有点区别, 和 EventLoopGroup 一样, 服务器端的 handler 也有两个, 一个是通过 handler() 方法设置 handler 字段, 另一个是通过 childHandler() 设置 childHandler 字段. 通过前面的 bossGroup 和 workerGroup 的分析, 其实我们在这里可以大胆地猜测: handler 字段与 accept 过程有关, 即这个 handler 负责处理客户端的连接请求; 而 childHandler 就是负责和客户端的连接的 IO 交互. 那么实际上是不是这样的呢? 来, 我们继续通过代码证明.\n\n在 关于 bossGroup 与 workerGroup 小节中, 我们提到, ServerBootstrap 重写了 init 方法, 在这个方法中添加了 handler:\n\n@Override\nvoid init(Channel channel) throws Exception {\n    ...\n    ChannelPipeline p = channel.pipeline();\n\n    final EventLoopGroup currentChildGroup = childGroup;\n    final ChannelHandler currentChildHandler = childHandler;\n    final Entry<ChannelOption<?>, Object>[] currentChildOptions;\n    final Entry<AttributeKey<?>, Object>[] currentChildAttrs;\n\n    p.addLast(new ChannelInitializer<Channel>() {\n        @Override\n        public void initChannel(Channel ch) throws Exception {\n            ChannelPipeline pipeline = ch.pipeline();\n            ChannelHandler handler = handler();\n            if (handler != null) {\n                pipeline.addLast(handler);\n            }\n            pipeline.addLast(new ServerBootstrapAcceptor(\n                    currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs));\n        }\n    });\n}\n\n\n上面代码的 initChannel 方法中, 首先通过 handler() 方法获取一个 handler, 如果获取的 handler 不为空,则添加到 pipeline 中. 然后接着, 添加了一个 ServerBootstrapAcceptor 实例. 那么这里 handler() 方法返回的是哪个对象呢? 其实它返回的是 handler 字段, 而这个字段就是我们在服务器端的启动代码中设置的:\n\nb.group(bossGroup, workerGroup)\n ...\n .handler(new LoggingHandler(LogLevel.INFO))\n\n\n那么这个时候, pipeline 中的 handler 情况如下: [](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (服务器端)/1477377831087.png)\n\n根据我们原来分析客户端的经验, 我们指定, 当 channel 绑定到 eventLoop 后(在这里是 NioServerSocketChannel 绑定到 bossGroup)中时, 会在 pipeline 中发出 fireChannelRegistered 事件, 接着就会触发 ChannelInitializer.initChannel 方法的调用. 因此在绑定完成后, 此时的 pipeline 的内如如下: [](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (服务器端)/1477378182291.png)\n\n前面我们在分析 bossGroup 和 workerGroup 时, 已经知道了在 ServerBootstrapAcceptor.channelRead 中会为新建的 Channel 设置 handler 并注册到一个 eventLoop 中, 即:\n\n@Override\n@SuppressWarnings("unchecked")\npublic void channelRead(ChannelHandlerContext ctx, Object msg) {\n    final Channel child = (Channel) msg;\n    child.pipeline().addLast(childHandler);\n    ...\n    childGroup.register(child).addListener(...);\n}\n\n\n而这里的 childHandler 就是我们在服务器端启动代码中设置的 handler:\n\nb.group(bossGroup, workerGroup)\n ...\n .childHandler(new ChannelInitializer<SocketChannel>() {\n     @Override\n     public void initChannel(SocketChannel ch) throws Exception {\n         ChannelPipeline p = ch.pipeline();\n         if (sslCtx != null) {\n             p.addLast(sslCtx.newHandler(ch.alloc()));\n         }\n         //p.addLast(new LoggingHandler(LogLevel.INFO));\n         p.addLast(new EchoServerHandler());\n     }\n });\n\n\n后续的步骤就没有什么好说的了, 当这个客户端连接 Channel 注册后, 就会触发 ChannelInitializer.initChannel 方法的调用, 此后的客户端的 ChannelPipeline 状态如下:\n\n\n\n最后我们来总结一下服务器端的 handler 与 childHandler 的区别与联系:\n\n * 在服务器 NioServerSocketChannel 的 pipeline 中添加的是 handler 与 ServerBootstrapAcceptor.\n * 当有新的客户端连接请求时, ServerBootstrapAcceptor.channelRead 中负责新建此连接的 NioSocketChannel 并添加 childHandler 到 NioSocketChannel 对应的 pipeline 中, 并将此 channel 绑定到 workerGroup 中的某个 eventLoop 中.\n * handler 是在 accept 阶段起作用, 它处理客户端的连接请求.\n * childHandler 是在客户端连接建立以后起作用, 它负责客户端连接的 IO 交互.\n\n下面我们用一幅图来总结一下服务器端的 handler 添加流程:\n\n\n\n\n# 后记\n\n这是 Netty 源码分析 系列教程的第一篇, 按我的计划, 这一篇文章是一个简述性质的, 即这里会涉及到 Netty 各个功能模块, 但是我只是简单地提了一下, 而没有深入地探索它们内部的实现机理. 之所以这样做, 第一, 是因为如果一上来就从细节分析, 那么未免会陷入各种琐碎的细节中难以自拔; 第二, 我想给读者展示一个一个完整的 Netty 的运行流程, 让读者从一个整体上对 Netty 有一个感性的认识. 此篇文章涉及的模块比较多, 面比较广, 因此写起来难免有一点跳跃, 并且我感觉写着写着见见有点不知所云, 逻辑混乱了, 汗. 唉, 还是感觉自己功力不够, hold 不住. 接下来的几篇文章, 我会根据 Netty 的各个模块深入分析一下, 希望以后的文章能够组织的调理更加清晰一些.',normalizedContent:'# 服务器端\n\n在分析客户端的代码时, 我们已经对 bootstrap 启动 netty 有了一个大致的认识, 那么接下来分析服务器端时, 就会相对简单一些了. 首先还是来看一下服务器端的启动代码:\n\npublic final class echoserver {\n\n    static final boolean ssl = system.getproperty("ssl") != null;\n    static final int port = integer.parseint(system.getproperty("port", "8007"));\n\n    public static void main(string[] args) throws exception {\n        // configure ssl.\n        final sslcontext sslctx;\n        if (ssl) {\n            selfsignedcertificate ssc = new selfsignedcertificate();\n            sslctx = sslcontextbuilder.forserver(ssc.certificate(), ssc.privatekey()).build();\n        } else {\n            sslctx = null;\n        }\n\n        // configure the server.\n        eventloopgroup bossgroup = new nioeventloopgroup(1);\n        eventloopgroup workergroup = new nioeventloopgroup();\n        try {\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)\n             .channel(nioserversocketchannel.class)\n             .option(channeloption.so_backlog, 100)\n             .handler(new logginghandler(loglevel.info))\n             .childhandler(new channelinitializer<socketchannel>() {\n                 @override\n                 public void initchannel(socketchannel ch) throws exception {\n                     channelpipeline p = ch.pipeline();\n                     if (sslctx != null) {\n                         p.addlast(sslctx.newhandler(ch.alloc()));\n                     }\n                     //p.addlast(new logginghandler(loglevel.info));\n                     p.addlast(new echoserverhandler());\n                 }\n             });\n\n            // start the server.\n            channelfuture f = b.bind(port).sync();\n\n            // wait until the server socket is closed.\n            f.channel().closefuture().sync();\n        } finally {\n            // shut down all event loops to terminate all threads.\n            bossgroup.shutdowngracefully();\n            workergroup.shutdowngracefully();\n        }\n    }\n}\n\n\n和客户端的代码相比, 没有很大的差别, 基本上也是进行了如下几个部分的初始化:\n\n 1. eventloopgroup: 不论是服务器端还是客户端, 都必须指定 eventloopgroup. 在这个例子中, 指定了 nioeventloopgroup, 表示一个 nio 的 eventloopgroup, 不过服务器端需要指定两个 eventloopgroup, 一个是 bossgroup, 用于处理客户端的连接请求; 另一个是 workergroup, 用于处理与各个客户端连接的 io 操作\n 2. channeltype: 指定 channel 的类型. 因为是服务器端, 因此使用了 nioserversocketchannel\n 3. handler: 设置数据的处理器\n\n\n# channel 的初始化过程\n\n我们在分析客户端的 channel 初始化过程时, 已经提到, channel 是对 java 底层 socket 连接的抽象, 并且知道了客户端的 channel 的具体类型是 niosocketchannel, 那么自然的, 服务器端的 channel 类型就是 nioserversocketchannel 了. 那么接下来我们按照分析客户端的流程对服务器端的代码也同样地分析一遍, 这样也方便我们对比一下服务器端和客户端有哪些不一样的地方.\n\n# channel 类型的确定\n\n同样的分析套路, 我们已经知道了, 在客户端中, channel 的类型其实是在初始化时, 通过 bootstrap.channel() 方法设置的, 服务器端自然也不例外\n\n在服务器端, 我们调用了 serverbootstarap.channel(nioserversocketchannel.class), 传递了一个 nioserversocketchannel class 对象. 这样的话, 按照和分析客户端代码一样的流程, 我们就可以确定, nioserversocketchannel 的实例化是通过 bootstrapchannelfactory 工厂类来完成的, 而 bootstrapchannelfactory 中的 clazz 字段被设置为了 nioserversocketchannel.class, 因此当调用 bootstrapchannelfactory.newchannel() 时:\n\n@override\npublic t newchannel() {\n\t// 删除 try 块\n    return clazz.newinstance();\n}\n\n\n就获取到了一个 nioserversocketchannel 的实例.\n\n最后我们也来总结一下:\n\n * serverbootstrap 中的 channelfactory 的实现是 bootstrapchannelfactory\n * 生成的 channel 的具体类型是 nioserversocketchannel. channel 的实例化过程, 其实就是调用的 channelfactory.newchannel 方法, 而实例化的 channel 的具体的类型又是和在初始化 serverbootstrap 时传入的 channel() 方法的参数相关. 因此对于我们这个例子中的服务器端的 serverbootstrap 而言, 生成的的 channel 实例就是 nioserversocketchannel\n\n# nioserversocketchannel 的实例化过程\n\n首先还是来看一下 nioserversocketchannel 的实例化过程. 下面是 nioserversocketchannel 的类层次结构图:\n\n\n\n首先, 我们来看一下它的默认的构造器. 和 niosocketchannel 类似, 构造器都是调用了 newsocket 来打开一个 java 的 nio socket, 不过需要注意的是, 客户端的 newsocket 调用的是 opensocketchannel, 而服务器端的 newsocket 调用的是 openserversocketchannel. 顾名思义, 一个是客户端的 java socketchannel, 一个是服务器端的 java serversocketchannel.\n\npublic nioserversocketchannel() {\n    this(newsocket(default_selector_provider));\n}\n\nprivate static serversocketchannel newsocket(selectorprovider provider) {\n    return provider.openserversocketchannel();\n}\n\n\n接下来会调用重载的构造器:\n\npublic nioserversocketchannel(serversocketchannel channel) {\n    super(null, channel, selectionkey.op_accept);\n    config = new nioserversocketchannelconfig(this, javachannel().socket());\n}\n\n\n这个构造其中, 调用父类构造器时, 传入的参数是 selectionkey.op_accept. 作为对比, 我们回想一下, 在客户端的 channel 初始化时, 传入的参数是 selectionkey.op_read\n\n有 java nio socket 开发经验的朋友就知道了, java nio 是一种 reactor 模式, 我们通过 selector 来实现 i/o 的多路复用复用. 在一开始时, 服务器端需要监听客户端的连接请求, 因此在这里我们设置了 selectionkey.op_accept, 即通知 selector 我们对客户端的连接请求感兴趣.\n\n接着和客户端的分析一下, 会逐级地调用父类的构造器 nioserversocketchannel <- abstractniomessagechannel <- abstractniochannel <- abstractchannel.\n\n同样的, 在 abstractchannel 中会实例化一个 unsafe 和 pipeline:\n\nprotected abstractchannel(channel parent) {\n    this.parent = parent;\n    unsafe = newunsafe();\n    pipeline = new defaultchannelpipeline(this);\n}\n\n\n不过, 这里有一点需要注意的是, 客户端的 unsafe 是一个 abstractniobytechannel#niobyteunsafe 的实例, 而在服务器端时, 因为 abstractniomessagechannel 重写了 newunsafe 方法:\n\n@override\nprotected abstractniounsafe newunsafe() {\n    return new niomessageunsafe();\n}\n\n\n因此在服务器端, unsafe 字段其实是一个 abstractniomessagechannel#abstractniounsafe 的实例. 我们来总结一下, 在 nioserversocketchannsl 实例化过程中, 所需要做的工作:\n\n * 调用 nioserversocketchannel.newsocket(default_selector_provider) 打开一个新的 java nio serversocketchannel\n * abstractchannel(channel parent) 中初始化 abstractchannel 的属性:\n * parent 属性置为 null\n * unsafe 通过 newunsafe() 实例化一个 unsafe 对象, 它的类型是 abstractniomessagechannel#abstractniounsafe 内部类\n * pipeline 是 new defaultchannelpipeline(this) 新创建的实例.\n * abstractniochannel 中的属性:\n * selectablechannel ch 被设置为 java serversocketchannel, 即 nioserversocketchannel#newsocket 返回的 java nio serversocketchannel.\n * readinterestop 被设置为 selectionkey.op_accept\n * selectablechannel ch 被配置为非阻塞的 ch.configureblocking(false)\n * nioserversocketchannel 中的属性:\n * serversocketchannelconfig config = new nioserversocketchannelconfig(this, javachannel().socket())\n\n\n# channelpipeline 初始化\n\n服务器端和客户端的 channelpipeline 的初始化一致, 因此就不再单独分析了.\n\n\n# channel 的注册\n\n服务器端和客户端的 channel 的注册过程一致, 因此就不再单独分析了.\n\n\n# 关于 bossgroup 与 workergroup\n\n在客户端的时候, 我们只提供了一个 eventloopgroup 对象, 而在服务器端的初始化时, 我们设置了两个 eventloopgroup, 一个是 bossgroup, 另一个是 workergroup. 那么这两个 eventloopgroup 都是干什么用的呢? 其实呢, bossgroup 是用于服务端 的 accept 的, 即用于处理客户端的连接请求. 我们可以把 netty 比作一个饭店, bossgroup 就像一个像一个前台接待, 当客户来到饭店吃时, 接待员就会引导顾客就坐, 为顾客端茶送水等. 而 workergroup, 其实就是实际上干活的啦, 它们负责客户端连接通道的 io 操作: 当接待员 招待好顾客后, 就可以稍做休息, 而此时后厨里的厨师们(workergroup)就开始忙碌地准备饭菜了. 关于 bossgroup 与 workergroup 的关系, 我们可以用如下图来展示:\n\n\n\n首先, 服务器端 bossgroup 不断地监听是否有客户端的连接, 当发现有一个新的客户端连接到来时, bossgroup 就会为此连接初始化各项资源, 然后从 workergroup 中选出一个 eventloop 绑定到此客户端连接中. 那么接下来的服务器与客户端的交互过程就全部在此分配的 eventloop 中了.\n\n口说无凭, 我们还是以源码说话吧. 首先在 serverbootstrap 初始化时, 调用了 b.group(bossgroup, workergroup) 设置了两个 eventloopgroup, 我们跟踪进去看一下:\n\npublic serverbootstrap group(eventloopgroup parentgroup, eventloopgroup childgroup) {\n    super.group(parentgroup);\n    ...\n    this.childgroup = childgroup;\n    return this;\n}\n\n\n显然, 这个方法初始化了两个字段, 一个是 group = parentgroup, 它是在 super.group(parentgroup) 中初始化的, 另一个是 childgroup = childgroup. 接着我们启动程序调用了 b.bind 方法来监听一个本地端口. bind 方法会触发如下的调用链:\n\nabstractbootstrap.bind -> abstractbootstrap.dobind -> abstractbootstrap.initandregister\n\n\nabstractbootstrap.initandregister 是我们的老朋友了, 我们在分析客户端程序时, 和它打过很多交到了, 我们再来回顾一下这个方法吧:\n\nfinal channelfuture initandregister() {\n    final channel channel = channelfactory().newchannel();\n    ... 省略异常判断\n    init(channel);\n    channelfuture regfuture = group().register(channel);\n    return regfuture;\n}\n\n\n这里 group() 方法返回的是上面我们提到的 bossgroup, 而这里的 channel 我们也已经分析过了, 它是一个是一个 nioserversocketchannel 实例, 因此我们可以知道, group().register(channel) 将 bossgroup 和 nioserversocketchannel 关联起来了. 那么 workergroup 是在哪里与 niosocketchannel 关联的呢? 我们继续看 init(channel) 方法:\n\n@override\nvoid init(channel channel) throws exception {\n    ...\n    channelpipeline p = channel.pipeline();\n\n    final eventloopgroup currentchildgroup = childgroup;\n    final channelhandler currentchildhandler = childhandler;\n    final entry<channeloption<?>, object>[] currentchildoptions;\n    final entry<attributekey<?>, object>[] currentchildattrs;\n\n    p.addlast(new channelinitializer<channel>() {\n        @override\n        public void initchannel(channel ch) throws exception {\n            channelpipeline pipeline = ch.pipeline();\n            channelhandler handler = handler();\n            if (handler != null) {\n                pipeline.addlast(handler);\n            }\n            pipeline.addlast(new serverbootstrapacceptor(\n                    currentchildgroup, currentchildhandler, currentchildoptions, currentchildattrs));\n        }\n    });\n}\n\n\ninit 方法在 serverbootstrap 中重写了, 从上面的代码片段中我们看到, 它为 pipeline 中添加了一个 channelinitializer, 而这个 channelinitializer 中添加了一个关键的 serverbootstrapacceptor handler. 关于 handler 的添加与初始化的过程, 我们留待下一小节中分析, 我们现在关注一下 serverbootstrapacceptor 类. serverbootstrapacceptor 中重写了 channelread 方法, 其主要代码如下:\n\n@override\n@suppresswarnings("unchecked")\npublic void channelread(channelhandlercontext ctx, object msg) {\n    final channel child = (channel) msg;\n    child.pipeline().addlast(childhandler);\n    ...\n    childgroup.register(child).addlistener(...);\n}\n\n\nserverbootstrapacceptor 中的 childgroup 是构造此对象是传入的 currentchildgroup, 即我们的 workergroup, 而 channel 是一个 niosocketchannel 的实例, 因此这里的 childgroup.register 就是将 workergroup 中的摸个 eventloop 和 niosocketchannel 关联了. 既然这样, 那么现在的问题是, serverbootstrapacceptor.channelread 方法是怎么被调用的呢? 其实当一个 client 连接到 server 时, java 底层的 nio serversocketchannel 会有一个 selectionkey.op_accept 就绪事件, 接着就会调用到 nioserversocketchannel.doreadmessages:\n\n@override\nprotected int doreadmessages(list<object> buf) throws exception {\n    socketchannel ch = javachannel().accept();\n    ... 省略异常处理\n    buf.add(new niosocketchannel(this, ch));\n    return 1;\n}\n\n\n在 doreadmessages 中, 通过 javachannel().accept() 获取到客户端新连接的 socketchannel, 接着就实例化一个 niosocketchannel, 并且传入 nioserversocketchannel 对象(即 this), 由此可知, 我们创建的这个 niosocketchannel 的父 channel 就是 nioserversocketchannel 实例 . 接下来就经由 netty 的 channelpipeline 机制, 将读取事件逐级发送到各个 handler 中, 于是就会触发前面我们提到的 serverbootstrapacceptor.channelread 方法啦.\n\n\n# handler 的添加过程\n\n服务器端的 handler 的添加过程和客户端的有点区别, 和 eventloopgroup 一样, 服务器端的 handler 也有两个, 一个是通过 handler() 方法设置 handler 字段, 另一个是通过 childhandler() 设置 childhandler 字段. 通过前面的 bossgroup 和 workergroup 的分析, 其实我们在这里可以大胆地猜测: handler 字段与 accept 过程有关, 即这个 handler 负责处理客户端的连接请求; 而 childhandler 就是负责和客户端的连接的 io 交互. 那么实际上是不是这样的呢? 来, 我们继续通过代码证明.\n\n在 关于 bossgroup 与 workergroup 小节中, 我们提到, serverbootstrap 重写了 init 方法, 在这个方法中添加了 handler:\n\n@override\nvoid init(channel channel) throws exception {\n    ...\n    channelpipeline p = channel.pipeline();\n\n    final eventloopgroup currentchildgroup = childgroup;\n    final channelhandler currentchildhandler = childhandler;\n    final entry<channeloption<?>, object>[] currentchildoptions;\n    final entry<attributekey<?>, object>[] currentchildattrs;\n\n    p.addlast(new channelinitializer<channel>() {\n        @override\n        public void initchannel(channel ch) throws exception {\n            channelpipeline pipeline = ch.pipeline();\n            channelhandler handler = handler();\n            if (handler != null) {\n                pipeline.addlast(handler);\n            }\n            pipeline.addlast(new serverbootstrapacceptor(\n                    currentchildgroup, currentchildhandler, currentchildoptions, currentchildattrs));\n        }\n    });\n}\n\n\n上面代码的 initchannel 方法中, 首先通过 handler() 方法获取一个 handler, 如果获取的 handler 不为空,则添加到 pipeline 中. 然后接着, 添加了一个 serverbootstrapacceptor 实例. 那么这里 handler() 方法返回的是哪个对象呢? 其实它返回的是 handler 字段, 而这个字段就是我们在服务器端的启动代码中设置的:\n\nb.group(bossgroup, workergroup)\n ...\n .handler(new logginghandler(loglevel.info))\n\n\n那么这个时候, pipeline 中的 handler 情况如下: [](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (服务器端)/1477377831087.png)\n\n根据我们原来分析客户端的经验, 我们指定, 当 channel 绑定到 eventloop 后(在这里是 nioserversocketchannel 绑定到 bossgroup)中时, 会在 pipeline 中发出 firechannelregistered 事件, 接着就会触发 channelinitializer.initchannel 方法的调用. 因此在绑定完成后, 此时的 pipeline 的内如如下: [](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (服务器端)/1477378182291.png)\n\n前面我们在分析 bossgroup 和 workergroup 时, 已经知道了在 serverbootstrapacceptor.channelread 中会为新建的 channel 设置 handler 并注册到一个 eventloop 中, 即:\n\n@override\n@suppresswarnings("unchecked")\npublic void channelread(channelhandlercontext ctx, object msg) {\n    final channel child = (channel) msg;\n    child.pipeline().addlast(childhandler);\n    ...\n    childgroup.register(child).addlistener(...);\n}\n\n\n而这里的 childhandler 就是我们在服务器端启动代码中设置的 handler:\n\nb.group(bossgroup, workergroup)\n ...\n .childhandler(new channelinitializer<socketchannel>() {\n     @override\n     public void initchannel(socketchannel ch) throws exception {\n         channelpipeline p = ch.pipeline();\n         if (sslctx != null) {\n             p.addlast(sslctx.newhandler(ch.alloc()));\n         }\n         //p.addlast(new logginghandler(loglevel.info));\n         p.addlast(new echoserverhandler());\n     }\n });\n\n\n后续的步骤就没有什么好说的了, 当这个客户端连接 channel 注册后, 就会触发 channelinitializer.initchannel 方法的调用, 此后的客户端的 channelpipeline 状态如下:\n\n\n\n最后我们来总结一下服务器端的 handler 与 childhandler 的区别与联系:\n\n * 在服务器 nioserversocketchannel 的 pipeline 中添加的是 handler 与 serverbootstrapacceptor.\n * 当有新的客户端连接请求时, serverbootstrapacceptor.channelread 中负责新建此连接的 niosocketchannel 并添加 childhandler 到 niosocketchannel 对应的 pipeline 中, 并将此 channel 绑定到 workergroup 中的某个 eventloop 中.\n * handler 是在 accept 阶段起作用, 它处理客户端的连接请求.\n * childhandler 是在客户端连接建立以后起作用, 它负责客户端连接的 io 交互.\n\n下面我们用一幅图来总结一下服务器端的 handler 添加流程:\n\n\n\n\n# 后记\n\n这是 netty 源码分析 系列教程的第一篇, 按我的计划, 这一篇文章是一个简述性质的, 即这里会涉及到 netty 各个功能模块, 但是我只是简单地提了一下, 而没有深入地探索它们内部的实现机理. 之所以这样做, 第一, 是因为如果一上来就从细节分析, 那么未免会陷入各种琐碎的细节中难以自拔; 第二, 我想给读者展示一个一个完整的 netty 的运行流程, 让读者从一个整体上对 netty 有一个感性的认识. 此篇文章涉及的模块比较多, 面比较广, 因此写起来难免有一点跳跃, 并且我感觉写着写着见见有点不知所云, 逻辑混乱了, 汗. 唉, 还是感觉自己功力不够, hold 不住. 接下来的几篇文章, 我会根据 netty 的各个模块深入分析一下, 希望以后的文章能够组织的调理更加清晰一些.',charsets:{cjk:!0},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"Channel 源码解析",frontmatter:{title:"Channel 源码解析",date:"2024-09-18T21:11:11.000Z",permalink:"/pages/8d1ba9/Channel"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/20.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/05.Channel%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html",relativePath:"Netty 系统设计/20.三、主线任务/05.Channel 源码解析.md",key:"v-5763dc5c",path:"/pages/8d1ba9/Channel/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:2},{level:2,title:"NIO 和 IO 的对比",slug:"nio-和-io-的对比",normalizedTitle:"nio 和 io 的对比",charIndex:96},{level:4,title:"基于 Stream 与 基于 Buffer",slug:"基于-stream-与-基于-buffer",normalizedTitle:"基于 stream 与 基于 buffer",charIndex:269},{level:4,title:"阻塞 和 非阻塞",slug:"阻塞-和-非阻塞",normalizedTitle:"阻塞 和 非阻塞",charIndex:640},{level:4,title:"selector",slug:"selector",normalizedTitle:"selector",charIndex:232},{level:2,title:"Java NIO Channel",slug:"java-nio-channel",normalizedTitle:"java nio channel",charIndex:1144},{level:3,title:"FileChannel",slug:"filechannel",normalizedTitle:"filechannel",charIndex:1404},{level:4,title:"打开 FileChannel",slug:"打开-filechannel",normalizedTitle:"打开 filechannel",charIndex:2196},{level:4,title:"从 FileChannel 中读取数据",slug:"从-filechannel-中读取数据",normalizedTitle:"从 filechannel 中读取数据",charIndex:2335},{level:4,title:"写入数据",slug:"写入数据",normalizedTitle:"写入数据",charIndex:2439},{level:4,title:"关闭",slug:"关闭",normalizedTitle:"关闭",charIndex:2682},{level:4,title:"设置 position",slug:"设置-position",normalizedTitle:"设置 position",charIndex:2741},{level:4,title:"文件大小",slug:"文件大小",normalizedTitle:"文件大小",charIndex:2816},{level:4,title:"截断文件",slug:"截断文件",normalizedTitle:"截断文件",charIndex:2910},{level:4,title:"强制写入",slug:"强制写入",normalizedTitle:"强制写入",charIndex:2962},{level:3,title:"SocketChannel",slug:"socketchannel",normalizedTitle:"socketchannel",charIndex:1450},{level:4,title:"打开 SocketChannel",slug:"打开-socketchannel",normalizedTitle:"打开 socketchannel",charIndex:3205},{level:4,title:"关闭",slug:"关闭-2",normalizedTitle:"关闭",charIndex:2682},{level:4,title:"读取数据",slug:"读取数据",normalizedTitle:"读取数据",charIndex:389},{level:4,title:"写入数据",slug:"写入数据-2",normalizedTitle:"写入数据",charIndex:2439},{level:4,title:"非阻塞模式",slug:"非阻塞模式",normalizedTitle:"非阻塞模式",charIndex:745},{level:5,title:"连接",slug:"连接",normalizedTitle:"连接",charIndex:1082},{level:5,title:"读写",slug:"读写",normalizedTitle:"读写",charIndex:1320},{level:3,title:"ServerSocketChannel",slug:"serversocketchannel",normalizedTitle:"serversocketchannel",charIndex:1474},{level:4,title:"打开 关闭",slug:"打开-关闭",normalizedTitle:"打开 关闭",charIndex:4528},{level:4,title:"监听连接",slug:"监听连接",normalizedTitle:"监听连接",charIndex:4640},{level:4,title:"非阻塞模式",slug:"非阻塞模式-2",normalizedTitle:"非阻塞模式",charIndex:745},{level:3,title:"DatagramChannel",slug:"datagramchannel",normalizedTitle:"datagramchannel",charIndex:1424},{level:4,title:"打开",slug:"打开",normalizedTitle:"打开",charIndex:2196},{level:4,title:"读取数据",slug:"读取数据-2",normalizedTitle:"读取数据",charIndex:389},{level:4,title:"发送数据",slug:"发送数据",normalizedTitle:"发送数据",charIndex:5579},{level:4,title:"连接到指定地址",slug:"连接到指定地址",normalizedTitle:"连接到指定地址",charIndex:5868},{level:2,title:"Java NIO Buffer",slug:"java-nio-buffer",normalizedTitle:"java nio buffer",charIndex:6041},{level:3,title:"NIO Buffer 的基本使用",slug:"nio-buffer-的基本使用",normalizedTitle:"nio buffer 的基本使用",charIndex:6419},{level:3,title:"Buffer 属性",slug:"buffer-属性",normalizedTitle:"buffer 属性",charIndex:7302},{level:4,title:"Capacity",slug:"capacity",normalizedTitle:"capacity",charIndex:7445},{level:4,title:"Position",slug:"position",normalizedTitle:"position",charIndex:7584},{level:4,title:"limit",slug:"limit",normalizedTitle:"limit",charIndex:7358},{level:4,title:"例子:",slug:"例子",normalizedTitle:"例子:",charIndex:7989},{level:3,title:"分配 Buffer",slug:"分配-buffer",normalizedTitle:"分配 buffer",charIndex:8811},{level:3,title:"关于 Direct Buffer 和 Non-Direct Buffer 的区别",slug:"关于-direct-buffer-和-non-direct-buffer-的区别",normalizedTitle:"关于 direct buffer 和 non-direct buffer 的区别",charIndex:9111},{level:3,title:"写入数据到 Buffer",slug:"写入数据到-buffer",normalizedTitle:"写入数据到 buffer",charIndex:9761},{level:3,title:"从 Buffer 中读取数据",slug:"从-buffer-中读取数据",normalizedTitle:"从 buffer 中读取数据",charIndex:1354},{level:3,title:"重置 position",slug:"重置-position",normalizedTitle:"重置 position",charIndex:9970},{level:3,title:"关于 mark()和 reset()",slug:"关于-mark-和-reset",normalizedTitle:"关于 mark()和 reset()",charIndex:10949},{level:3,title:"flip, rewind 和 clear 的区别",slug:"flip-rewind-和-clear-的区别",normalizedTitle:"flip, rewind 和 clear 的区别",charIndex:11848},{level:4,title:"flip",slug:"flip",normalizedTitle:"flip",charIndex:1883},{level:4,title:"rewind",slug:"rewind",normalizedTitle:"rewind",charIndex:9990},{level:4,title:"clear",slug:"clear",normalizedTitle:"clear",charIndex:1998},{level:5,title:"例子:",slug:"例子-2",normalizedTitle:"例子:",charIndex:7989},{level:3,title:"Buffer 的比较",slug:"buffer-的比较",normalizedTitle:"buffer 的比较",charIndex:13210},{level:2,title:"Selector",slug:"selector-2",normalizedTitle:"selector",charIndex:83},{level:3,title:"创建选择器",slug:"创建选择器",normalizedTitle:"创建选择器",charIndex:13777},{level:3,title:"将 Channel 注册到选择器中",slug:"将-channel-注册到选择器中",normalizedTitle:"将 channel 注册到选择器中",charIndex:13862},{level:3,title:"关于 SelectionKey",slug:"关于-selectionkey",normalizedTitle:"关于 selectionkey",charIndex:15041},{level:4,title:"interest set",slug:"interest-set",normalizedTitle:"interest set",charIndex:14768},{level:4,title:"ready set",slug:"ready-set",normalizedTitle:"ready set",charIndex:15209},{level:4,title:"Channel 和 Selector",slug:"channel-和-selector",normalizedTitle:"channel 和 selector",charIndex:15896},{level:4,title:"Attaching Object",slug:"attaching-object",normalizedTitle:"attaching object",charIndex:16059},{level:3,title:"通过 Selector 选择 Channel",slug:"通过-selector-选择-channel",normalizedTitle:"通过 selector 选择 channel",charIndex:16284},{level:3,title:"获取可操作的 Channel",slug:"获取可操作的-channel",normalizedTitle:"获取可操作的 channel",charIndex:16469},{level:3,title:"Selector 的基本使用流程",slug:"selector-的基本使用流程",normalizedTitle:"selector 的基本使用流程",charIndex:17631},{level:3,title:"关闭 Selector",slug:"关闭-selector",normalizedTitle:"关闭 selector",charIndex:18154},{level:3,title:"完整的 Selector 例子",slug:"完整的-selector-例子",normalizedTitle:"完整的 selector 例子",charIndex:18258},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:21483},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:21490}],headersStr:"简介 NIO 和 IO 的对比 基于 Stream 与 基于 Buffer 阻塞 和 非阻塞 selector Java NIO Channel FileChannel 打开 FileChannel 从 FileChannel 中读取数据 写入数据 关闭 设置 position 文件大小 截断文件 强制写入 SocketChannel 打开 SocketChannel 关闭 读取数据 写入数据 非阻塞模式 连接 读写 ServerSocketChannel 打开 关闭 监听连接 非阻塞模式 DatagramChannel 打开 读取数据 发送数据 连接到指定地址 Java NIO Buffer NIO Buffer 的基本使用 Buffer 属性 Capacity Position limit 例子: 分配 Buffer 关于 Direct Buffer 和 Non-Direct Buffer 的区别 写入数据到 Buffer 从 Buffer 中读取数据 重置 position 关于 mark()和 reset() flip, rewind 和 clear 的区别 flip rewind clear 例子: Buffer 的比较 Selector 创建选择器 将 Channel 注册到选择器中 关于 SelectionKey interest set ready set Channel 和 Selector Attaching Object 通过 Selector 选择 Channel 获取可操作的 Channel Selector 的基本使用流程 关闭 Selector 完整的 Selector 例子 总结 参考资料",content:'# 简介\n\nJava NIO 是由 Java 1.4 引进的异步 IO Java NIO 由以下几个核心部分组成:\n\n * Channel\n * Buffer\n * Selector\n\n\n# NIO 和 IO 的对比\n\nIO 和 NIO 的区别主要体现在三个方面:\n\n * IO 基于流(Stream oriented)，而 NIO 基于 Buffer (Buffer oriented)\n * IO 操作是阻塞的, 而 NIO 操作是非阻塞的\n * IO 没有 selector 概念, 而 NIO 有 selector 概念.\n\n# 基于 Stream 与 基于 Buffer\n\n传统的 IO 是面向字节流或字符流的, 而在 NIO 中, 我们抛弃了传统的 IO 流, 而是引入了 Channel 和 Buffer 的概念.\n\n在 NIO 中, 我只能从 Channel 中读取数据到 Buffer 中或将数据从 Buffer 中写入到 Channel\n\n那么什么是 基于流 呢? 在一般的 Java IO 操作中, 我们以流式的方式顺序地从一个 Stream 中读取一个或多个字节, 因此我们也就不能随意改变读取指针的位置\n\n而 基于 Buffer 就显得有点不同了，我们首先需要从 Channel 中读取数据到 Buffer 中, 当 Buffer 中有数据后，我们就可以对这些数据进行操作了，不像 IO 那样是顺序操作，NIO 中我们可以随意地读取任意位置的数据\n\n# 阻塞 和 非阻塞\n\nJava 提供的各种 Stream 操作都是阻塞的, 例如我们调用一个 read 方法读取一个文件的内容, 那么调用 read 的线程会被阻塞住, 直到 read 操作完成\n\n而 NIO 的非阻塞模式允许我们非阻塞地进行 IO 操作，例如我们需要从网络中读取数据, 在 NIO 的非阻塞模式中，当我们调用 read 方法时，如果此时有数据，则 read 读取并返回；如果此时没有数据， 则 read 直接返回，而不会阻塞当前线程。\n\n# selector\n\nselector 是 NIO 中才有的概念, 它是 Java NIO 之所以可以非阻塞地进行 IO 操作的关键\n\n通过 Selector，一个线程可以监听多个 Channel 的 IO 事件，当我们向一个 Selector 中注册了 Channel 后，Selector 内部的机制就可以自动地为我们不断地查询（select）这些注册的 Channel 是否有已就绪的 IO 事件（例如可读, 可写, 网络连接完成等）。通过这样的 Selector 机制，我们就可以很简单地使用一个线程高效地管理多个 Channel 了\n\n\n# Java NIO Channel\n\n通常来说，所有的 NIO 的 I/O 操作都是从 Channel 开始的。一个 channel 类似于一个 stream java Stream 和 NIO Channel 对比\n\n * 我们可以在同一个 Channel 中执行读和写操作，然而同一个 Stream 仅仅支持读或写.\n * Channel 可以异步地读写，而 Stream 是阻塞的同步读写\n * Channel 总是从 Buffer 中读取数据，或将数据写入到 Buffer 中\n\nChannel 类型有：\n\n * FileChannel，文件操作\n * DatagramChannel，UDP 操作\n * SocketChannel，TCP 操作\n * ServerSocketChannel，TCP 操作，使用在服务器端\n\n这些通道涵盖了 UDP 和 TCP网络 IO以及文件 IO 基本的 Channel 使用例子：\n\npublic static void main( String[] args ) throws Exception\n{\n    RandomAccessFile aFile = new RandomAccessFile("/Users/echo/settings.xml", "rw");\n    FileChannel inChannel = aFile.getChannel();\n\n    ByteBuffer buf = ByteBuffer.allocate(48);\n\n    int bytesRead = inChannel.read(buf);\n    while (bytesRead != -1) {\n        buf.flip();\n\n        while(buf.hasRemaining()){\n            System.out.print((char) buf.get());\n        }\n\n        buf.clear();\n        bytesRead = inChannel.read(buf);\n    }\n    aFile.close();\n}\n\n\n\n# FileChannel\n\nFileChannel 是操作文件的 Channel, 我们可以通过 FileChannel 从一个文件中读取数据, 也可以将数据写入到文件中\n\n注意, FileChannel 不能设置为非阻塞模式\n\n# 打开 FileChannel\n\nRandomAccessFile aFile = new RandomAccessFile("data/nio-data.txt", "rw");\nFileChannel  inChannel = aFile.getChannel();\n\n\n# 从 FileChannel 中读取数据\n\nByteBuffer buf = ByteBuffer.allocate(48);\nint bytesRead = inChannel.read(buf);\n\n\n# 写入数据\n\nString newData = "New String to write to file..." + System.currentTimeMillis();\n\nByteBuffer buf = ByteBuffer.allocate(48);\nbuf.clear();\nbuf.put(newData.getBytes());\n\nbuf.flip();\n\nwhile(buf.hasRemaining()) {\n    channel.write(buf);\n}\n\n\n# 关闭\n\n当我们对 FileChannel 的操作完成后, 必须将其关闭\n\nchannel.close(); \n\n\n# 设置 position\n\nlong pos channel.position();\nchannel.position(pos + 123);\n\n\n# 文件大小\n\n我们可以通过 channel.size()获取关联到这个 Channel 中的文件的大小. 注意, 这里返回的是文件的大小, 而不是 Channel 中剩余的元素个数.\n\n# 截断文件\n\nchannel.truncate(1024);\n\n\n将文件的大小截断为1024字节.\n\n# 强制写入\n\n我们可以强制将缓存的未写入的数据写入到文件中:\n\nchannel.force(true);\n\n\n\n# SocketChannel\n\nSocketChannel 是一个客户端用来进行 TCP 连接的 Channel. 创建一个 SocketChannel 的方法有两种:\n\n * 打开一个 SocketChannel, 然后将其连接到某个服务器中\n * 当一个 ServerSocketChannel 接受到连接请求时, 会返回一个 SocketChannel 对象.\n\n# 打开 SocketChannel\n\nSocketChannel socketChannel = SocketChannel.open();\nsocketChannel.connect(new InetSocketAddress("http://example.com", 80));\n\n\n# 关闭\n\nsocketChannel.close(); \n\n\n# 读取数据\n\nByteBuffer buf = ByteBuffer.allocate(48);\nint bytesRead = socketChannel.read(buf);\n\n\n如果 read()返回 -1, 那么表示连接中断了.\n\n# 写入数据\n\nString newData = "New String to write to file..." + System.currentTimeMillis();\n\nByteBuffer buf = ByteBuffer.allocate(48);\nbuf.clear();\nbuf.put(newData.getBytes());\n\nbuf.flip();\n\nwhile(buf.hasRemaining()) {\n    channel.write(buf);\n}\n\n\n# 非阻塞模式\n\n我们可以设置 SocketChannel 为异步模式, 这样我们的 connect, read, write 都是异步的了.\n\n# 连接\n\nsocketChannel.configureBlocking(false);\nsocketChannel.connect(new InetSocketAddress("http://example.com", 80));\n\nwhile(! socketChannel.finishConnect() ){\n    //wait, or do something else...    \n}\n\n\n在异步模式中, 或许连接还没有建立, connect 方法就返回了, 因此我们需要检查当前是否是连接到了主机, 因此通过一个 while 循环来判断.\n\n# 读写\n\n在异步模式下, 读写的方式是一样的. 在读取时, 因为是异步的, 因此我们必须检查 read 的返回值, 来判断当前是否读取到了数据.\n\n\n# ServerSocketChannel\n\nServerSocketChannel 顾名思义, 是用在服务器为端的, 可以监听客户端的 TCP 连接, 例如:\n\nServerSocketChannel serverSocketChannel = ServerSocketChannel.open();\nserverSocketChannel.socket().bind(new InetSocketAddress(9999));\nwhile(true){\n    SocketChannel socketChannel =\n            serverSocketChannel.accept();\n\n    //do something with socketChannel...\n}\n\n\n# 打开 关闭\n\nServerSocketChannel serverSocketChannel = ServerSocketChannel.open();\n\n\nserverSocketChannel.close();\n\n\n# 监听连接\n\n我们可以使用ServerSocketChannel.accept()方法来监听客户端的 TCP 连接请求, accept()方法会阻塞, 直到有连接到来, 当有连接时, 这个方法会返回一个 SocketChannel 对象:\n\nwhile(true){\n    SocketChannel socketChannel =\n            serverSocketChannel.accept();\n\n    //do something with socketChannel...\n}\n\n\n# 非阻塞模式\n\n在非阻塞模式下, accept()是非阻塞的, 因此如果此时没有连接到来, 那么 accept()方法会返回null:\n\nServerSocketChannel serverSocketChannel = ServerSocketChannel.open();\n\nserverSocketChannel.socket().bind(new InetSocketAddress(9999));\nserverSocketChannel.configureBlocking(false);\n\nwhile(true){\n    SocketChannel socketChannel =\n            serverSocketChannel.accept();\n\n    if(socketChannel != null){\n        //do something with socketChannel...\n        }\n}\n\n\n\n# DatagramChannel\n\nDatagramChannel 是用来处理 UDP 连接的.\n\n# 打开\n\nDatagramChannel channel = DatagramChannel.open();\nchannel.socket().bind(new InetSocketAddress(9999));\n\n\n# 读取数据\n\nByteBuffer buf = ByteBuffer.allocate(48);\nbuf.clear();\n\nchannel.receive(buf);\n\n\n# 发送数据\n\nString newData = "New String to write to file..."\n                    + System.currentTimeMillis();\n    \nByteBuffer buf = ByteBuffer.allocate(48);\nbuf.clear();\nbuf.put(newData.getBytes());\nbuf.flip();\n\nint bytesSent = channel.send(buf, new InetSocketAddress("example.com", 80));\n\n\n# 连接到指定地址\n\n因为 UDP 是非连接的, 因此这个的 connect 并不是向 TCP 一样真正意义上的连接, 而是它会讲 DatagramChannel 锁住, 因此我们仅仅可以从指定的地址中读取或写入数据.\n\nchannel.connect(new InetSocketAddress("example.com", 80));\n\n\n\n# Java NIO Buffer\n\n当我们需要与 NIO Channel 进行交互时, 我们就需要使用到 NIO Buffer, 即数据从 Buffer读取到 Channel 中, 并且从 Channel 中写入到 Buffer 中. 实际上, 一个 Buffer 其实就是一块内存区域, 我们可以在这个内存区域中进行数据的读写. NIO Buffer 其实是这样的内存块的一个封装, 并提供了一些操作方法让我们能够方便地进行数据的读写. Buffer 类型有:\n\n * ByteBuffer\n * CharBuffer\n * DoubleBuffer\n * FloatBuffer\n * IntBuffer\n * LongBuffer\n * ShortBuffer\n\n这些 Buffer 覆盖了能从 IO 中传输的所有的 Java 基本数据类型.\n\n\n# NIO Buffer 的基本使用\n\n使用 NIO Buffer 的步骤如下:\n\n * 将数据写入到 Buffer 中.\n * 调用 Buffer.flip() 方法, 将 NIO Buffer 转换为读模式.\n * 从 Buffer 中读取数据\n * 调用 Buffer.clear() 或 Buffer.compact()方法, 将 Buffer 转换为写模式.\n\n当我们将数据写入到 Buffer 中时, Buffer 会记录我们已经写了多少的数据, 当我们需要从 Buffer 中读取数据时, 必须调用 Buffer.flip()将 Buffer 切换为读模式. 一旦读取了所有的 Buffer 数据, 那么我们必须清理 Buffer, 让其从新可写, 清理 Buffer 可以调用 Buffer.clear() 或 Buffer.compact(). 例如:\n\npublic class Test {\n    public static void main(String[] args) {\n        IntBuffer intBuffer = IntBuffer.allocate(2);\n        intBuffer.put(12345678);\n        intBuffer.put(2);\n        intBuffer.flip();\n        System.err.println(intBuffer.get());\n        System.err.println(intBuffer.get());\n    }\n}\n\n\n上述中, 我们分配两个单位大小的 IntBuffer, 因此它可以写入两个 int 值. 我们使用 put 方法将 int 值写入, 然后使用 flip 方法将 buffer 转换为读模式, 然后连续使用 get 方法从 buffer 中获取这两个 int 值. 每当调用一次 get 方法读取数据时, buffer 的读指针都会向前移动一个单位长度(在这里是一个 int 长度)\n\n\n# Buffer 属性\n\n一个 Buffer 有三个属性:\n\n * capacity\n * position\n * limit\n\n其中 position 和 limit 的含义与 Buffer 处于读模式或写模式有关, 而 capacity 的含义与 Buffer 所处的模式无关.\n\n# Capacity\n\n一个内存块会有一个固定的大小, 即容量(capacity), 我们最多写入capacity 个单位的数据到 Buffer 中, 例如一个 DoubleBuffer, 其 Capacity 是100, 那么我们最多可以写入100个 double 数据.\n\n# Position\n\n当从一个 Buffer 中写入数据时, 我们是从 Buffer 的一个确定的位置(position)开始写入的. 在最初的状态时, position 的值是0. 每当我们写入了一个单位的数据后, position 就会递增一. 当我们从 Buffer 中读取数据时, 我们也是从某个特定的位置开始读取的. 当我们调用了 filp()方法将 Buffer 从写模式转换到读模式时, position 的值会自动被设置为0, 每当我们读取一个单位的数据, position 的值递增1. position 表示了读写操作的位置指针.\n\n# limit\n\nlimit - position 表示此时还可以写入/读取多少单位的数据. 例如在写模式, 如果此时 limit 是10, position 是2, 则表示已经写入了2个单位的数据, 还可以写入 10 - 2 = 8 个单位的数据.\n\n# 例子:\n\npublic class Test {\n    public static void main(String args[]) {\n        IntBuffer intBuffer = IntBuffer.allocate(10);\n        intBuffer.put(10);\n        intBuffer.put(101);\n        System.err.println("Write mode: ");\n        System.err.println("\\tCapacity: " + intBuffer.capacity());\n        System.err.println("\\tPosition: " + intBuffer.position());\n        System.err.println("\\tLimit: " + intBuffer.limit());\n\n        intBuffer.flip();\n        System.err.println("Read mode: ");\n        System.err.println("\\tCapacity: " + intBuffer.capacity());\n        System.err.println("\\tPosition: " + intBuffer.position());\n        System.err.println("\\tLimit: " + intBuffer.limit());\n    }\n}\n\n\n这里我们首先写入两个 int 值, 此时 capacity = 10, position = 2, limit = 10. 然后我们调用 flip 转换为读模式, 此时 capacity = 10, position = 0, limit = 2;\n\n\n# 分配 Buffer\n\n为了获取一个 Buffer 对象, 我们首先需要分配内存空间. 每个类型的 Buffer 都有一个 allocate()方法, 我们可以通过这个方法分配 Buffer:\n\nByteBuffer buf = ByteBuffer.allocate(48);\n\n\n这里我们分配了48 * sizeof(Byte)字节的内存空间.\n\nCharBuffer buf = CharBuffer.allocate(1024);\n\n\n这里我们分配了大小为1024个字符的 Buffer, 即 这个 Buffer 可以存储1024 个 Char, 其大小为 1024 * 2 个字节.\n\n\n# 关于 Direct Buffer 和 Non-Direct Buffer 的区别\n\nDirect Buffer:\n\n * 所分配的内存不在 JVM 堆上, 不受 GC 的管理.(但是 Direct Buffer 的 Java 对象是由 GC 管理的, 因此当发生 GC, 对象被回收时, Direct Buffer 也会被释放)\n * 因为 Direct Buffer 不在 JVM 堆上分配, 因此 Direct Buffer 对应用程序的内存占用的影响就不那么明显(实际上还是占用了这么多内存, 但是 JVM 不好统计到非 JVM 管理的内存.)\n * 申请和释放 Direct Buffer 的开销比较大. 因此正确的使用 Direct Buffer 的方式是在初始化时申请一个 Buffer, 然后不断复用此 buffer, 在程序结束后才释放此 buffer.\n * 使用 Direct Buffer 时, 当进行一些底层的系统 IO 操作时, 效率会比较高, 因为此时 JVM 不需要拷贝 buffer 中的内存到中间临时缓冲区中.\n\nNon-Direct Buffer:\n\n * 直接在 JVM 堆上进行内存的分配, 本质上是 byte[] 数组的封装.\n * 因为 Non-Direct Buffer 在 JVM 堆中, 因此当进行操作系统底层 IO 操作中时, 会将此 buffer 的内存复制到中间临时缓冲区中. 因此 Non-Direct Buffer 的效率就较低.\n\n\n# 写入数据到 Buffer\n\nint bytesRead = inChannel.read(buf); //read into buffer.\nbuf.put(127);\n\n\n\n# 从 Buffer 中读取数据\n\n//read from buffer into channel.\nint bytesWritten = inChannel.write(buf);\nbyte aByte = buf.get();\n\n\n\n# 重置 position\n\nBuffer.rewind()方法可以重置 position 的值为0, 因此我们可以重新读取/写入 Buffer 了. 如果是读模式, 则重置的是读模式的 position, 如果是写模式, 则重置的是写模式的 position. 例如:\n\npublic class Test {\n    public static void main(String[] args) {\n        IntBuffer intBuffer = IntBuffer.allocate(2);\n        intBuffer.put(1);\n        intBuffer.put(2);\n        System.err.println("position: " + intBuffer.position());\n\n        intBuffer.rewind();\n        System.err.println("position: " + intBuffer.position());\n        intBuffer.put(1);\n        intBuffer.put(2);\n        System.err.println("position: " + intBuffer.position());\n\n        \n        intBuffer.flip();\n        System.err.println("position: " + intBuffer.position());\n        intBuffer.get();\n        intBuffer.get();\n        System.err.println("position: " + intBuffer.position());\n\n        intBuffer.rewind();\n        System.err.println("position: " + intBuffer.position());\n    }\n}\n\n\nrewind() 主要针对于读模式. 在读模式时, 读取到 limit 后, 可以调用 rewind() 方法, 将读 position 置为0.\n\n\n# 关于 mark()和 reset()\n\n我们可以通过调用 Buffer.mark()将当前的 position 的值保存起来, 随后可以通过调用 Buffer.reset()方法将 position 的值回复回来. 例如:\n\npublic class Test {\n    public static void main(String[] args) {\n        IntBuffer intBuffer = IntBuffer.allocate(2);\n        intBuffer.put(1);\n        intBuffer.put(2);\n        intBuffer.flip();\n        System.err.println(intBuffer.get());\n        System.err.println("position: " + intBuffer.position());\n        intBuffer.mark();\n        System.err.println(intBuffer.get());\n\n        System.err.println("position: " + intBuffer.position());\n        intBuffer.reset();\n        System.err.println("position: " + intBuffer.position());\n        System.err.println(intBuffer.get());\n    }\n}\n\n\n这里我们写入两个 int 值, 然后首先读取了一个值. 此时读 position 的值为1. 接着我们调用 mark() 方法将当前的 position 保存起来(在读模式, 因此保存的是读的 position), 然后再次读取, 此时 position 就是2了. 接着使用 reset() 恢复原来的读 position, 因此读 position 就为1, 可以再次读取数据.\n\n\n# flip, rewind 和 clear 的区别\n\n# flip\n\n方法源码:\n\npublic final Buffer flip() {\n    limit = position;\n    position = 0;\n    mark = -1;\n    return this;\n}\n\n\nBuffer 的读/写模式共用一个 position 和 limit 变量. 当从写模式变为读模式时, 原先的 写 position 就变成了读模式的 limit.\n\n# rewind\n\n方法源码\n\npublic final Buffer rewind() {\n    position = 0;\n    mark = -1;\n    return this;\n}\n\n\nrewind, 即倒带, 这个方法仅仅是将 position 置为0.\n\n# clear\n\n方法源码:\n\npublic final Buffer clear() {\n    position = 0;\n    limit = capacity;\n    mark = -1;\n    return this;\n}\n\n\n根据源码我们可以知道, clear 将 positin 设置为0, 将 limit 设置为 capacity. clear 方法使用场景:\n\n * 在一个已经写满数据的 buffer 中, 调用 clear, 可以从头读取 buffer 的数据.\n * 为了将一个 buffer 填充满数据, 可以调用 clear, 然后一直写入, 直到达到 limit.\n\n# 例子:\n\nIntBuffer intBuffer = IntBuffer.allocate(2);\nintBuffer.flip();\nSystem.err.println("position: " + intBuffer.position());\nSystem.err.println("limit: " + intBuffer.limit());\nSystem.err.println("capacity: " + intBuffer.capacity());\n\n// 这里不能读, 因为 limit == position == 0, 没有数据.\n//System.err.println(intBuffer.get());\n\nintBuffer.clear();\nSystem.err.println("position: " + intBuffer.position());\nSystem.err.println("limit: " + intBuffer.limit());\nSystem.err.println("capacity: " + intBuffer.capacity());\n\n// 这里可以读取数据了, 因为 clear 后, limit == capacity == 2, position == 0,\n// 即使我们没有写入任何的数据到 buffer 中.\nSystem.err.println(intBuffer.get()); // 读取到0\nSystem.err.println(intBuffer.get()); // 读取到0\n\n\n\n# Buffer 的比较\n\n我们可以通过 equals() 或 compareTo() 方法比较两个 Buffer, 当且仅当如下条件满足时, 两个 Buffer 是相等的:\n\n * 两个 Buffer 是相同类型的\n * 两个 Buffer 的剩余的数据个数是相同的\n * 两个 Buffer 的剩余的数据都是相同的.\n\n通过上述条件我们可以发现, 比较两个 Buffer 时, 并不是 Buffer 中的每个元素都进行比较, 而是比较 Buffer 中剩余的元素.\n\n\n# Selector\n\nSelector 允许一个单一的线程来操作多个 Channel. 如果我们的应用程序中使用了多个 Channel, 那么使用 Selector 很方便的实现这样的目的, 但是因为在一个线程中使用了多个 Channel, 因此也会造成了每个 Channel 传输效率的降低. 使用 Selector 的图解如下:\n\n\n\n为了使用 Selector, 我们首先需要将 Channel 注册到 Selector 中, 随后调用 Selector 的 select() 方法, 这个方法会阻塞, 直到注册在 Selector 中的 Channel 发送可读写事件. 当这个方法返回后, 当前的这个线程就可以处理 Channel 的事件了.\n\n\n# 创建选择器\n\n通过 Selector.open()方法, 我们可以创建一个选择器:\n\nSelector selector = Selector.open();\n\n\n\n# 将 Channel 注册到选择器中\n\n为了使用选择器管理 Channel, 我们需要将 Channel 注册到选择器中:\n\nchannel.configureBlocking(false);\n\nSelectionKey key = channel.register(selector, SelectionKey.OP_READ);\n\n\n> 注意, 如果一个 Channel 要注册到 Selector 中, 那么这个 Channel 必须是非阻塞的, 即channel.configureBlocking(false); 因为 Channel 必须要是非阻塞的, 因此 FileChannel 是不能够使用选择器的, 因为 FileChannel 都是阻塞的.\n\n注意到, 在使用 Channel.register()方法时, 第二个参数指定了我们对 Channel 的什么类型的事件感兴趣, 这些事件有:\n\n * Connect, 即连接事件(TCP 连接), 对应于SelectionKey.OP_CONNECT\n * Accept, 即确认事件, 对应于SelectionKey.OP_ACCEPT\n * Read, 即读事件, 对应于SelectionKey.OP_READ, 表示 buffer 可读.\n * Write, 即写事件, 对应于SelectionKey.OP_WRITE, 表示 buffer 可写.\n\n一个 Channel发出一个事件也可以称为** 对于某个事件, Channel 准备好了**. 因此一个 Channel 成功连接到了另一个服务器也可以被称为** connect ready**. 我们可以使用或运算**|**来组合多个事件, 例如:\n\nint interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE;    \n\n\n注意, 一个 Channel 仅仅可以被注册到一个 Selector 一次, 如果将 Channel 注册到 Selector 多次, 那么其实就是相当于更新 SelectionKey 的 interest set. 例如:\n\nchannel.register(selector, SelectionKey.OP_READ);\nchannel.register(selector, SelectionKey.OP_READ | SelectionKey.OP_WRITE);\n\n\n上面的 channel 注册到同一个 Selector 两次了, 那么第二次的注册其实就是相当于更新这个 Channel 的 interest set 为 SelectionKey.OP_READ | SelectionKey.OP_WRITE.\n\n\n# 关于 SelectionKey\n\n如上所示, 当我们使用 register 注册一个 Channel 时, 会返回一个 SelectionKey 对象, 这个对象包含了如下内容:\n\n * interest set, 即我们感兴趣的事件集, 即在调用 register 注册 channel 时所设置的 interest set.\n * ready set\n * channel\n * selector\n * attached object, 可选的附加对象\n\n# interest set\n\n我们可以通过如下方式获取 interest set:\n\nint interestSet = selectionKey.interestOps();\n\nboolean isInterestedInAccept  = interestSet & SelectionKey.OP_ACCEPT;\nboolean isInterestedInConnect = interestSet & SelectionKey.OP_CONNECT;\nboolean isInterestedInRead    = interestSet & SelectionKey.OP_READ;\nboolean isInterestedInWrite   = interestSet & SelectionKey.OP_WRITE;    \n\n\n# ready set\n\n代表了 Channel 所准备好了的操作. 我们可以像判断 interest set 一样操作 Ready set, 但是我们还可以使用如下方法进行判断:\n\nint readySet = selectionKey.readyOps();\n\nselectionKey.isAcceptable();\nselectionKey.isConnectable();\nselectionKey.isReadable();\nselectionKey.isWritable();\n\n\n# Channel 和 Selector\n\n我们可以通过 SelectionKey 获取相对应的 Channel 和 Selector:\n\nChannel  channel  = selectionKey.channel();\nSelector selector = selectionKey.selector();  \n\n\n# Attaching Object\n\n我们可以在selectionKey中附加一个对象:\n\nselectionKey.attach(theObject);\nObject attachedObj = selectionKey.attachment();\n\n\n或者在注册时直接附加:\n\nSelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject);\n\n\n\n# 通过 Selector 选择 Channel\n\n我们可以通过 Selector.select()方法获取对某件事件准备好了的 Channel, 即如果我们在注册 Channel 时, 对其的可写事件感兴趣, 那么当 select()返回时, 我们就可以获取 Channel 了.\n\n> 注意, select()方法返回的值表示有多少个 Channel 可操作.\n\n\n# 获取可操作的 Channel\n\n如果 select()方法返回值表示有多个 Channel 准备好了, 那么我们可以通过 Selected key set 访问这个 Channel:\n\nSet<SelectionKey> selectedKeys = selector.selectedKeys();\n\nIterator<SelectionKey> keyIterator = selectedKeys.iterator();\n\nwhile(keyIterator.hasNext()) {\n    \n    SelectionKey key = keyIterator.next();\n\n    if(key.isAcceptable()) {\n        // a connection was accepted by a ServerSocketChannel.\n\n    } else if (key.isConnectable()) {\n        // a connection was established with a remote server.\n\n    } else if (key.isReadable()) {\n        // a channel is ready for reading\n\n    } else if (key.isWritable()) {\n        // a channel is ready for writing\n    }\n\n    keyIterator.remove();\n}\n\n\n注意, 在每次迭代时, 我们都调用 "keyIterator.remove()" 将这个 key 从迭代器中删除, 因为 select() 方法仅仅是简单地将就绪的 IO 操作放到 selectedKeys 集合中, 因此如果我们从 selectedKeys 获取到一个 key, 但是没有将它删除, 那么下一次 select 时, 这个 key 所对应的 IO 事件还在 selectedKeys 中.\n\n例如此时我们收到 OP_ACCEPT 通知, 然后我们进行相关处理, 但是并没有将这个 Key 从 SelectedKeys 中删除, 那么下一次 select() 返回时 我们还可以在 SelectedKeys 中获取到 OP_ACCEPT 的 key.\n\n注意, 我们可以动态更改 SekectedKeys 中的 key 的 interest set.\n\n例如在 OP_ACCEPT 中, 我们可以将 interest set 更新为 OP_READ, 这样 Selector 就会将这个 Channel 的 读 IO 就绪事件包含进来了.\n\n\n# Selector 的基本使用流程\n\n 1. 通过 Selector.open() 打开一个 Selector.\n 2. 将 Channel 注册到 Selector 中, 并设置需要监听的事件(interest set)\n 3. 不断重复:\n\n * 调用 select() 方法\n * 调用 selector.selectedKeys() 获取 selected keys\n * 迭代每个 selected key:\n * *从 selected key 中获取 对应的 Channel 和附加信息(如果有的话)\n * *判断是哪些 IO 事件已经就绪了, 然后处理它们. 如果是 OP_ACCEPT 事件, 则调用 "SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept()" 获取 SocketChannel, 并将它设置为 非阻塞的, 然后将这个 Channel 注册到 Selector 中.\n * *根据需要更改 selected key 的监听事件.\n * *将已经处理过的 key 从 selected keys 集合中删除.\n\n\n# 关闭 Selector\n\n当调用了 Selector.close()方法时, 我们其实是关闭了 Selector 本身并且将所有的 SelectionKey 失效, 但是并不会关闭 Channel.\n\n\n# 完整的 Selector 例子\n\npublic class NioEchoServer {\n    private static final int BUF_SIZE = 256;\n    private static final int TIMEOUT = 3000;\n\n    public static void main(String args[]) throws Exception {\n        // 打开服务端 Socket\n        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();\n\n        // 打开 Selector\n        Selector selector = Selector.open();\n\n        // 服务端 Socket 监听8080端口, 并配置为非阻塞模式\n        serverSocketChannel.socket().bind(new InetSocketAddress(8080));\n        serverSocketChannel.configureBlocking(false);\n\n        // 将 channel 注册到 selector 中.\n        // 通常我们都是先注册一个 OP_ACCEPT 事件, 然后在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ\n        // 注册到 Selector 中.\n        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);\n\n        while (true) {\n            // 通过调用 select 方法, 阻塞地等待 channel I/O 可操作\n            if (selector.select(TIMEOUT) == 0) {\n                System.out.print(".");\n                continue;\n            }\n\n            // 获取 I/O 操作就绪的 SelectionKey, 通过 SelectionKey 可以知道哪些 Channel 的哪类 I/O 操作已经就绪.\n            Iterator<SelectionKey> keyIterator = selector.selectedKeys().iterator();\n\n            while (keyIterator.hasNext()) {\n\n                // 当获取一个 SelectionKey 后, 就要将它删除, 表示我们已经对这个 IO 事件进行了处理.\n                keyIterator.remove();\n\n                SelectionKey key = keyIterator.next();\n\n                if (key.isAcceptable()) {\n                    // 当 OP_ACCEPT 事件到来时, 我们就有从 ServerSocketChannel 中获取一个 SocketChannel,\n                    // 代表客户端的连接\n                    // 注意, 在 OP_ACCEPT 事件中, 从 key.channel() 返回的 Channel 是 ServerSocketChannel.\n                    // 而在 OP_WRITE 和 OP_READ 中, 从 key.channel() 返回的是 SocketChannel.\n                    SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept();\n                    clientChannel.configureBlocking(false);\n                    //在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ 注册到 Selector 中.\n                    // 注意, 这里我们如果没有设置 OP_READ 的话, 即 interest set 仍然是 OP_CONNECT 的话, 那么 select 方法会一直直接返回.\n                    clientChannel.register(key.selector(), OP_READ, ByteBuffer.allocate(BUF_SIZE));\n                }\n\n                if (key.isReadable()) {\n                    SocketChannel clientChannel = (SocketChannel) key.channel();\n                    ByteBuffer buf = (ByteBuffer) key.attachment();\n                    long bytesRead = clientChannel.read(buf);\n                    if (bytesRead == -1) {\n                        clientChannel.close();\n                    } else if (bytesRead > 0) {\n                        key.interestOps(OP_READ | SelectionKey.OP_WRITE);\n                        System.out.println("Get data length: " + bytesRead);\n                    }\n                }\n\n                if (key.isValid() && key.isWritable()) {\n                    ByteBuffer buf = (ByteBuffer) key.attachment();\n                    buf.flip();\n                    SocketChannel clientChannel = (SocketChannel) key.channel();\n\n                    clientChannel.write(buf);\n\n                    if (!buf.hasRemaining()) {\n                        key.interestOps(OP_READ);\n                    }\n                    buf.compact();\n                }\n            }\n        }\n    }\n}\n\n\n\n# 总结\n\n\n# 参考资料\n\n * https://github.com/yongshun/learn_netty_source_code',normalizedContent:'# 简介\n\njava nio 是由 java 1.4 引进的异步 io java nio 由以下几个核心部分组成:\n\n * channel\n * buffer\n * selector\n\n\n# nio 和 io 的对比\n\nio 和 nio 的区别主要体现在三个方面:\n\n * io 基于流(stream oriented)，而 nio 基于 buffer (buffer oriented)\n * io 操作是阻塞的, 而 nio 操作是非阻塞的\n * io 没有 selector 概念, 而 nio 有 selector 概念.\n\n# 基于 stream 与 基于 buffer\n\n传统的 io 是面向字节流或字符流的, 而在 nio 中, 我们抛弃了传统的 io 流, 而是引入了 channel 和 buffer 的概念.\n\n在 nio 中, 我只能从 channel 中读取数据到 buffer 中或将数据从 buffer 中写入到 channel\n\n那么什么是 基于流 呢? 在一般的 java io 操作中, 我们以流式的方式顺序地从一个 stream 中读取一个或多个字节, 因此我们也就不能随意改变读取指针的位置\n\n而 基于 buffer 就显得有点不同了，我们首先需要从 channel 中读取数据到 buffer 中, 当 buffer 中有数据后，我们就可以对这些数据进行操作了，不像 io 那样是顺序操作，nio 中我们可以随意地读取任意位置的数据\n\n# 阻塞 和 非阻塞\n\njava 提供的各种 stream 操作都是阻塞的, 例如我们调用一个 read 方法读取一个文件的内容, 那么调用 read 的线程会被阻塞住, 直到 read 操作完成\n\n而 nio 的非阻塞模式允许我们非阻塞地进行 io 操作，例如我们需要从网络中读取数据, 在 nio 的非阻塞模式中，当我们调用 read 方法时，如果此时有数据，则 read 读取并返回；如果此时没有数据， 则 read 直接返回，而不会阻塞当前线程。\n\n# selector\n\nselector 是 nio 中才有的概念, 它是 java nio 之所以可以非阻塞地进行 io 操作的关键\n\n通过 selector，一个线程可以监听多个 channel 的 io 事件，当我们向一个 selector 中注册了 channel 后，selector 内部的机制就可以自动地为我们不断地查询（select）这些注册的 channel 是否有已就绪的 io 事件（例如可读, 可写, 网络连接完成等）。通过这样的 selector 机制，我们就可以很简单地使用一个线程高效地管理多个 channel 了\n\n\n# java nio channel\n\n通常来说，所有的 nio 的 i/o 操作都是从 channel 开始的。一个 channel 类似于一个 stream java stream 和 nio channel 对比\n\n * 我们可以在同一个 channel 中执行读和写操作，然而同一个 stream 仅仅支持读或写.\n * channel 可以异步地读写，而 stream 是阻塞的同步读写\n * channel 总是从 buffer 中读取数据，或将数据写入到 buffer 中\n\nchannel 类型有：\n\n * filechannel，文件操作\n * datagramchannel，udp 操作\n * socketchannel，tcp 操作\n * serversocketchannel，tcp 操作，使用在服务器端\n\n这些通道涵盖了 udp 和 tcp网络 io以及文件 io 基本的 channel 使用例子：\n\npublic static void main( string[] args ) throws exception\n{\n    randomaccessfile afile = new randomaccessfile("/users/echo/settings.xml", "rw");\n    filechannel inchannel = afile.getchannel();\n\n    bytebuffer buf = bytebuffer.allocate(48);\n\n    int bytesread = inchannel.read(buf);\n    while (bytesread != -1) {\n        buf.flip();\n\n        while(buf.hasremaining()){\n            system.out.print((char) buf.get());\n        }\n\n        buf.clear();\n        bytesread = inchannel.read(buf);\n    }\n    afile.close();\n}\n\n\n\n# filechannel\n\nfilechannel 是操作文件的 channel, 我们可以通过 filechannel 从一个文件中读取数据, 也可以将数据写入到文件中\n\n注意, filechannel 不能设置为非阻塞模式\n\n# 打开 filechannel\n\nrandomaccessfile afile = new randomaccessfile("data/nio-data.txt", "rw");\nfilechannel  inchannel = afile.getchannel();\n\n\n# 从 filechannel 中读取数据\n\nbytebuffer buf = bytebuffer.allocate(48);\nint bytesread = inchannel.read(buf);\n\n\n# 写入数据\n\nstring newdata = "new string to write to file..." + system.currenttimemillis();\n\nbytebuffer buf = bytebuffer.allocate(48);\nbuf.clear();\nbuf.put(newdata.getbytes());\n\nbuf.flip();\n\nwhile(buf.hasremaining()) {\n    channel.write(buf);\n}\n\n\n# 关闭\n\n当我们对 filechannel 的操作完成后, 必须将其关闭\n\nchannel.close(); \n\n\n# 设置 position\n\nlong pos channel.position();\nchannel.position(pos + 123);\n\n\n# 文件大小\n\n我们可以通过 channel.size()获取关联到这个 channel 中的文件的大小. 注意, 这里返回的是文件的大小, 而不是 channel 中剩余的元素个数.\n\n# 截断文件\n\nchannel.truncate(1024);\n\n\n将文件的大小截断为1024字节.\n\n# 强制写入\n\n我们可以强制将缓存的未写入的数据写入到文件中:\n\nchannel.force(true);\n\n\n\n# socketchannel\n\nsocketchannel 是一个客户端用来进行 tcp 连接的 channel. 创建一个 socketchannel 的方法有两种:\n\n * 打开一个 socketchannel, 然后将其连接到某个服务器中\n * 当一个 serversocketchannel 接受到连接请求时, 会返回一个 socketchannel 对象.\n\n# 打开 socketchannel\n\nsocketchannel socketchannel = socketchannel.open();\nsocketchannel.connect(new inetsocketaddress("http://example.com", 80));\n\n\n# 关闭\n\nsocketchannel.close(); \n\n\n# 读取数据\n\nbytebuffer buf = bytebuffer.allocate(48);\nint bytesread = socketchannel.read(buf);\n\n\n如果 read()返回 -1, 那么表示连接中断了.\n\n# 写入数据\n\nstring newdata = "new string to write to file..." + system.currenttimemillis();\n\nbytebuffer buf = bytebuffer.allocate(48);\nbuf.clear();\nbuf.put(newdata.getbytes());\n\nbuf.flip();\n\nwhile(buf.hasremaining()) {\n    channel.write(buf);\n}\n\n\n# 非阻塞模式\n\n我们可以设置 socketchannel 为异步模式, 这样我们的 connect, read, write 都是异步的了.\n\n# 连接\n\nsocketchannel.configureblocking(false);\nsocketchannel.connect(new inetsocketaddress("http://example.com", 80));\n\nwhile(! socketchannel.finishconnect() ){\n    //wait, or do something else...    \n}\n\n\n在异步模式中, 或许连接还没有建立, connect 方法就返回了, 因此我们需要检查当前是否是连接到了主机, 因此通过一个 while 循环来判断.\n\n# 读写\n\n在异步模式下, 读写的方式是一样的. 在读取时, 因为是异步的, 因此我们必须检查 read 的返回值, 来判断当前是否读取到了数据.\n\n\n# serversocketchannel\n\nserversocketchannel 顾名思义, 是用在服务器为端的, 可以监听客户端的 tcp 连接, 例如:\n\nserversocketchannel serversocketchannel = serversocketchannel.open();\nserversocketchannel.socket().bind(new inetsocketaddress(9999));\nwhile(true){\n    socketchannel socketchannel =\n            serversocketchannel.accept();\n\n    //do something with socketchannel...\n}\n\n\n# 打开 关闭\n\nserversocketchannel serversocketchannel = serversocketchannel.open();\n\n\nserversocketchannel.close();\n\n\n# 监听连接\n\n我们可以使用serversocketchannel.accept()方法来监听客户端的 tcp 连接请求, accept()方法会阻塞, 直到有连接到来, 当有连接时, 这个方法会返回一个 socketchannel 对象:\n\nwhile(true){\n    socketchannel socketchannel =\n            serversocketchannel.accept();\n\n    //do something with socketchannel...\n}\n\n\n# 非阻塞模式\n\n在非阻塞模式下, accept()是非阻塞的, 因此如果此时没有连接到来, 那么 accept()方法会返回null:\n\nserversocketchannel serversocketchannel = serversocketchannel.open();\n\nserversocketchannel.socket().bind(new inetsocketaddress(9999));\nserversocketchannel.configureblocking(false);\n\nwhile(true){\n    socketchannel socketchannel =\n            serversocketchannel.accept();\n\n    if(socketchannel != null){\n        //do something with socketchannel...\n        }\n}\n\n\n\n# datagramchannel\n\ndatagramchannel 是用来处理 udp 连接的.\n\n# 打开\n\ndatagramchannel channel = datagramchannel.open();\nchannel.socket().bind(new inetsocketaddress(9999));\n\n\n# 读取数据\n\nbytebuffer buf = bytebuffer.allocate(48);\nbuf.clear();\n\nchannel.receive(buf);\n\n\n# 发送数据\n\nstring newdata = "new string to write to file..."\n                    + system.currenttimemillis();\n    \nbytebuffer buf = bytebuffer.allocate(48);\nbuf.clear();\nbuf.put(newdata.getbytes());\nbuf.flip();\n\nint bytessent = channel.send(buf, new inetsocketaddress("example.com", 80));\n\n\n# 连接到指定地址\n\n因为 udp 是非连接的, 因此这个的 connect 并不是向 tcp 一样真正意义上的连接, 而是它会讲 datagramchannel 锁住, 因此我们仅仅可以从指定的地址中读取或写入数据.\n\nchannel.connect(new inetsocketaddress("example.com", 80));\n\n\n\n# java nio buffer\n\n当我们需要与 nio channel 进行交互时, 我们就需要使用到 nio buffer, 即数据从 buffer读取到 channel 中, 并且从 channel 中写入到 buffer 中. 实际上, 一个 buffer 其实就是一块内存区域, 我们可以在这个内存区域中进行数据的读写. nio buffer 其实是这样的内存块的一个封装, 并提供了一些操作方法让我们能够方便地进行数据的读写. buffer 类型有:\n\n * bytebuffer\n * charbuffer\n * doublebuffer\n * floatbuffer\n * intbuffer\n * longbuffer\n * shortbuffer\n\n这些 buffer 覆盖了能从 io 中传输的所有的 java 基本数据类型.\n\n\n# nio buffer 的基本使用\n\n使用 nio buffer 的步骤如下:\n\n * 将数据写入到 buffer 中.\n * 调用 buffer.flip() 方法, 将 nio buffer 转换为读模式.\n * 从 buffer 中读取数据\n * 调用 buffer.clear() 或 buffer.compact()方法, 将 buffer 转换为写模式.\n\n当我们将数据写入到 buffer 中时, buffer 会记录我们已经写了多少的数据, 当我们需要从 buffer 中读取数据时, 必须调用 buffer.flip()将 buffer 切换为读模式. 一旦读取了所有的 buffer 数据, 那么我们必须清理 buffer, 让其从新可写, 清理 buffer 可以调用 buffer.clear() 或 buffer.compact(). 例如:\n\npublic class test {\n    public static void main(string[] args) {\n        intbuffer intbuffer = intbuffer.allocate(2);\n        intbuffer.put(12345678);\n        intbuffer.put(2);\n        intbuffer.flip();\n        system.err.println(intbuffer.get());\n        system.err.println(intbuffer.get());\n    }\n}\n\n\n上述中, 我们分配两个单位大小的 intbuffer, 因此它可以写入两个 int 值. 我们使用 put 方法将 int 值写入, 然后使用 flip 方法将 buffer 转换为读模式, 然后连续使用 get 方法从 buffer 中获取这两个 int 值. 每当调用一次 get 方法读取数据时, buffer 的读指针都会向前移动一个单位长度(在这里是一个 int 长度)\n\n\n# buffer 属性\n\n一个 buffer 有三个属性:\n\n * capacity\n * position\n * limit\n\n其中 position 和 limit 的含义与 buffer 处于读模式或写模式有关, 而 capacity 的含义与 buffer 所处的模式无关.\n\n# capacity\n\n一个内存块会有一个固定的大小, 即容量(capacity), 我们最多写入capacity 个单位的数据到 buffer 中, 例如一个 doublebuffer, 其 capacity 是100, 那么我们最多可以写入100个 double 数据.\n\n# position\n\n当从一个 buffer 中写入数据时, 我们是从 buffer 的一个确定的位置(position)开始写入的. 在最初的状态时, position 的值是0. 每当我们写入了一个单位的数据后, position 就会递增一. 当我们从 buffer 中读取数据时, 我们也是从某个特定的位置开始读取的. 当我们调用了 filp()方法将 buffer 从写模式转换到读模式时, position 的值会自动被设置为0, 每当我们读取一个单位的数据, position 的值递增1. position 表示了读写操作的位置指针.\n\n# limit\n\nlimit - position 表示此时还可以写入/读取多少单位的数据. 例如在写模式, 如果此时 limit 是10, position 是2, 则表示已经写入了2个单位的数据, 还可以写入 10 - 2 = 8 个单位的数据.\n\n# 例子:\n\npublic class test {\n    public static void main(string args[]) {\n        intbuffer intbuffer = intbuffer.allocate(10);\n        intbuffer.put(10);\n        intbuffer.put(101);\n        system.err.println("write mode: ");\n        system.err.println("\\tcapacity: " + intbuffer.capacity());\n        system.err.println("\\tposition: " + intbuffer.position());\n        system.err.println("\\tlimit: " + intbuffer.limit());\n\n        intbuffer.flip();\n        system.err.println("read mode: ");\n        system.err.println("\\tcapacity: " + intbuffer.capacity());\n        system.err.println("\\tposition: " + intbuffer.position());\n        system.err.println("\\tlimit: " + intbuffer.limit());\n    }\n}\n\n\n这里我们首先写入两个 int 值, 此时 capacity = 10, position = 2, limit = 10. 然后我们调用 flip 转换为读模式, 此时 capacity = 10, position = 0, limit = 2;\n\n\n# 分配 buffer\n\n为了获取一个 buffer 对象, 我们首先需要分配内存空间. 每个类型的 buffer 都有一个 allocate()方法, 我们可以通过这个方法分配 buffer:\n\nbytebuffer buf = bytebuffer.allocate(48);\n\n\n这里我们分配了48 * sizeof(byte)字节的内存空间.\n\ncharbuffer buf = charbuffer.allocate(1024);\n\n\n这里我们分配了大小为1024个字符的 buffer, 即 这个 buffer 可以存储1024 个 char, 其大小为 1024 * 2 个字节.\n\n\n# 关于 direct buffer 和 non-direct buffer 的区别\n\ndirect buffer:\n\n * 所分配的内存不在 jvm 堆上, 不受 gc 的管理.(但是 direct buffer 的 java 对象是由 gc 管理的, 因此当发生 gc, 对象被回收时, direct buffer 也会被释放)\n * 因为 direct buffer 不在 jvm 堆上分配, 因此 direct buffer 对应用程序的内存占用的影响就不那么明显(实际上还是占用了这么多内存, 但是 jvm 不好统计到非 jvm 管理的内存.)\n * 申请和释放 direct buffer 的开销比较大. 因此正确的使用 direct buffer 的方式是在初始化时申请一个 buffer, 然后不断复用此 buffer, 在程序结束后才释放此 buffer.\n * 使用 direct buffer 时, 当进行一些底层的系统 io 操作时, 效率会比较高, 因为此时 jvm 不需要拷贝 buffer 中的内存到中间临时缓冲区中.\n\nnon-direct buffer:\n\n * 直接在 jvm 堆上进行内存的分配, 本质上是 byte[] 数组的封装.\n * 因为 non-direct buffer 在 jvm 堆中, 因此当进行操作系统底层 io 操作中时, 会将此 buffer 的内存复制到中间临时缓冲区中. 因此 non-direct buffer 的效率就较低.\n\n\n# 写入数据到 buffer\n\nint bytesread = inchannel.read(buf); //read into buffer.\nbuf.put(127);\n\n\n\n# 从 buffer 中读取数据\n\n//read from buffer into channel.\nint byteswritten = inchannel.write(buf);\nbyte abyte = buf.get();\n\n\n\n# 重置 position\n\nbuffer.rewind()方法可以重置 position 的值为0, 因此我们可以重新读取/写入 buffer 了. 如果是读模式, 则重置的是读模式的 position, 如果是写模式, 则重置的是写模式的 position. 例如:\n\npublic class test {\n    public static void main(string[] args) {\n        intbuffer intbuffer = intbuffer.allocate(2);\n        intbuffer.put(1);\n        intbuffer.put(2);\n        system.err.println("position: " + intbuffer.position());\n\n        intbuffer.rewind();\n        system.err.println("position: " + intbuffer.position());\n        intbuffer.put(1);\n        intbuffer.put(2);\n        system.err.println("position: " + intbuffer.position());\n\n        \n        intbuffer.flip();\n        system.err.println("position: " + intbuffer.position());\n        intbuffer.get();\n        intbuffer.get();\n        system.err.println("position: " + intbuffer.position());\n\n        intbuffer.rewind();\n        system.err.println("position: " + intbuffer.position());\n    }\n}\n\n\nrewind() 主要针对于读模式. 在读模式时, 读取到 limit 后, 可以调用 rewind() 方法, 将读 position 置为0.\n\n\n# 关于 mark()和 reset()\n\n我们可以通过调用 buffer.mark()将当前的 position 的值保存起来, 随后可以通过调用 buffer.reset()方法将 position 的值回复回来. 例如:\n\npublic class test {\n    public static void main(string[] args) {\n        intbuffer intbuffer = intbuffer.allocate(2);\n        intbuffer.put(1);\n        intbuffer.put(2);\n        intbuffer.flip();\n        system.err.println(intbuffer.get());\n        system.err.println("position: " + intbuffer.position());\n        intbuffer.mark();\n        system.err.println(intbuffer.get());\n\n        system.err.println("position: " + intbuffer.position());\n        intbuffer.reset();\n        system.err.println("position: " + intbuffer.position());\n        system.err.println(intbuffer.get());\n    }\n}\n\n\n这里我们写入两个 int 值, 然后首先读取了一个值. 此时读 position 的值为1. 接着我们调用 mark() 方法将当前的 position 保存起来(在读模式, 因此保存的是读的 position), 然后再次读取, 此时 position 就是2了. 接着使用 reset() 恢复原来的读 position, 因此读 position 就为1, 可以再次读取数据.\n\n\n# flip, rewind 和 clear 的区别\n\n# flip\n\n方法源码:\n\npublic final buffer flip() {\n    limit = position;\n    position = 0;\n    mark = -1;\n    return this;\n}\n\n\nbuffer 的读/写模式共用一个 position 和 limit 变量. 当从写模式变为读模式时, 原先的 写 position 就变成了读模式的 limit.\n\n# rewind\n\n方法源码\n\npublic final buffer rewind() {\n    position = 0;\n    mark = -1;\n    return this;\n}\n\n\nrewind, 即倒带, 这个方法仅仅是将 position 置为0.\n\n# clear\n\n方法源码:\n\npublic final buffer clear() {\n    position = 0;\n    limit = capacity;\n    mark = -1;\n    return this;\n}\n\n\n根据源码我们可以知道, clear 将 positin 设置为0, 将 limit 设置为 capacity. clear 方法使用场景:\n\n * 在一个已经写满数据的 buffer 中, 调用 clear, 可以从头读取 buffer 的数据.\n * 为了将一个 buffer 填充满数据, 可以调用 clear, 然后一直写入, 直到达到 limit.\n\n# 例子:\n\nintbuffer intbuffer = intbuffer.allocate(2);\nintbuffer.flip();\nsystem.err.println("position: " + intbuffer.position());\nsystem.err.println("limit: " + intbuffer.limit());\nsystem.err.println("capacity: " + intbuffer.capacity());\n\n// 这里不能读, 因为 limit == position == 0, 没有数据.\n//system.err.println(intbuffer.get());\n\nintbuffer.clear();\nsystem.err.println("position: " + intbuffer.position());\nsystem.err.println("limit: " + intbuffer.limit());\nsystem.err.println("capacity: " + intbuffer.capacity());\n\n// 这里可以读取数据了, 因为 clear 后, limit == capacity == 2, position == 0,\n// 即使我们没有写入任何的数据到 buffer 中.\nsystem.err.println(intbuffer.get()); // 读取到0\nsystem.err.println(intbuffer.get()); // 读取到0\n\n\n\n# buffer 的比较\n\n我们可以通过 equals() 或 compareto() 方法比较两个 buffer, 当且仅当如下条件满足时, 两个 buffer 是相等的:\n\n * 两个 buffer 是相同类型的\n * 两个 buffer 的剩余的数据个数是相同的\n * 两个 buffer 的剩余的数据都是相同的.\n\n通过上述条件我们可以发现, 比较两个 buffer 时, 并不是 buffer 中的每个元素都进行比较, 而是比较 buffer 中剩余的元素.\n\n\n# selector\n\nselector 允许一个单一的线程来操作多个 channel. 如果我们的应用程序中使用了多个 channel, 那么使用 selector 很方便的实现这样的目的, 但是因为在一个线程中使用了多个 channel, 因此也会造成了每个 channel 传输效率的降低. 使用 selector 的图解如下:\n\n\n\n为了使用 selector, 我们首先需要将 channel 注册到 selector 中, 随后调用 selector 的 select() 方法, 这个方法会阻塞, 直到注册在 selector 中的 channel 发送可读写事件. 当这个方法返回后, 当前的这个线程就可以处理 channel 的事件了.\n\n\n# 创建选择器\n\n通过 selector.open()方法, 我们可以创建一个选择器:\n\nselector selector = selector.open();\n\n\n\n# 将 channel 注册到选择器中\n\n为了使用选择器管理 channel, 我们需要将 channel 注册到选择器中:\n\nchannel.configureblocking(false);\n\nselectionkey key = channel.register(selector, selectionkey.op_read);\n\n\n> 注意, 如果一个 channel 要注册到 selector 中, 那么这个 channel 必须是非阻塞的, 即channel.configureblocking(false); 因为 channel 必须要是非阻塞的, 因此 filechannel 是不能够使用选择器的, 因为 filechannel 都是阻塞的.\n\n注意到, 在使用 channel.register()方法时, 第二个参数指定了我们对 channel 的什么类型的事件感兴趣, 这些事件有:\n\n * connect, 即连接事件(tcp 连接), 对应于selectionkey.op_connect\n * accept, 即确认事件, 对应于selectionkey.op_accept\n * read, 即读事件, 对应于selectionkey.op_read, 表示 buffer 可读.\n * write, 即写事件, 对应于selectionkey.op_write, 表示 buffer 可写.\n\n一个 channel发出一个事件也可以称为** 对于某个事件, channel 准备好了**. 因此一个 channel 成功连接到了另一个服务器也可以被称为** connect ready**. 我们可以使用或运算**|**来组合多个事件, 例如:\n\nint interestset = selectionkey.op_read | selectionkey.op_write;    \n\n\n注意, 一个 channel 仅仅可以被注册到一个 selector 一次, 如果将 channel 注册到 selector 多次, 那么其实就是相当于更新 selectionkey 的 interest set. 例如:\n\nchannel.register(selector, selectionkey.op_read);\nchannel.register(selector, selectionkey.op_read | selectionkey.op_write);\n\n\n上面的 channel 注册到同一个 selector 两次了, 那么第二次的注册其实就是相当于更新这个 channel 的 interest set 为 selectionkey.op_read | selectionkey.op_write.\n\n\n# 关于 selectionkey\n\n如上所示, 当我们使用 register 注册一个 channel 时, 会返回一个 selectionkey 对象, 这个对象包含了如下内容:\n\n * interest set, 即我们感兴趣的事件集, 即在调用 register 注册 channel 时所设置的 interest set.\n * ready set\n * channel\n * selector\n * attached object, 可选的附加对象\n\n# interest set\n\n我们可以通过如下方式获取 interest set:\n\nint interestset = selectionkey.interestops();\n\nboolean isinterestedinaccept  = interestset & selectionkey.op_accept;\nboolean isinterestedinconnect = interestset & selectionkey.op_connect;\nboolean isinterestedinread    = interestset & selectionkey.op_read;\nboolean isinterestedinwrite   = interestset & selectionkey.op_write;    \n\n\n# ready set\n\n代表了 channel 所准备好了的操作. 我们可以像判断 interest set 一样操作 ready set, 但是我们还可以使用如下方法进行判断:\n\nint readyset = selectionkey.readyops();\n\nselectionkey.isacceptable();\nselectionkey.isconnectable();\nselectionkey.isreadable();\nselectionkey.iswritable();\n\n\n# channel 和 selector\n\n我们可以通过 selectionkey 获取相对应的 channel 和 selector:\n\nchannel  channel  = selectionkey.channel();\nselector selector = selectionkey.selector();  \n\n\n# attaching object\n\n我们可以在selectionkey中附加一个对象:\n\nselectionkey.attach(theobject);\nobject attachedobj = selectionkey.attachment();\n\n\n或者在注册时直接附加:\n\nselectionkey key = channel.register(selector, selectionkey.op_read, theobject);\n\n\n\n# 通过 selector 选择 channel\n\n我们可以通过 selector.select()方法获取对某件事件准备好了的 channel, 即如果我们在注册 channel 时, 对其的可写事件感兴趣, 那么当 select()返回时, 我们就可以获取 channel 了.\n\n> 注意, select()方法返回的值表示有多少个 channel 可操作.\n\n\n# 获取可操作的 channel\n\n如果 select()方法返回值表示有多个 channel 准备好了, 那么我们可以通过 selected key set 访问这个 channel:\n\nset<selectionkey> selectedkeys = selector.selectedkeys();\n\niterator<selectionkey> keyiterator = selectedkeys.iterator();\n\nwhile(keyiterator.hasnext()) {\n    \n    selectionkey key = keyiterator.next();\n\n    if(key.isacceptable()) {\n        // a connection was accepted by a serversocketchannel.\n\n    } else if (key.isconnectable()) {\n        // a connection was established with a remote server.\n\n    } else if (key.isreadable()) {\n        // a channel is ready for reading\n\n    } else if (key.iswritable()) {\n        // a channel is ready for writing\n    }\n\n    keyiterator.remove();\n}\n\n\n注意, 在每次迭代时, 我们都调用 "keyiterator.remove()" 将这个 key 从迭代器中删除, 因为 select() 方法仅仅是简单地将就绪的 io 操作放到 selectedkeys 集合中, 因此如果我们从 selectedkeys 获取到一个 key, 但是没有将它删除, 那么下一次 select 时, 这个 key 所对应的 io 事件还在 selectedkeys 中.\n\n例如此时我们收到 op_accept 通知, 然后我们进行相关处理, 但是并没有将这个 key 从 selectedkeys 中删除, 那么下一次 select() 返回时 我们还可以在 selectedkeys 中获取到 op_accept 的 key.\n\n注意, 我们可以动态更改 sekectedkeys 中的 key 的 interest set.\n\n例如在 op_accept 中, 我们可以将 interest set 更新为 op_read, 这样 selector 就会将这个 channel 的 读 io 就绪事件包含进来了.\n\n\n# selector 的基本使用流程\n\n 1. 通过 selector.open() 打开一个 selector.\n 2. 将 channel 注册到 selector 中, 并设置需要监听的事件(interest set)\n 3. 不断重复:\n\n * 调用 select() 方法\n * 调用 selector.selectedkeys() 获取 selected keys\n * 迭代每个 selected key:\n * *从 selected key 中获取 对应的 channel 和附加信息(如果有的话)\n * *判断是哪些 io 事件已经就绪了, 然后处理它们. 如果是 op_accept 事件, 则调用 "socketchannel clientchannel = ((serversocketchannel) key.channel()).accept()" 获取 socketchannel, 并将它设置为 非阻塞的, 然后将这个 channel 注册到 selector 中.\n * *根据需要更改 selected key 的监听事件.\n * *将已经处理过的 key 从 selected keys 集合中删除.\n\n\n# 关闭 selector\n\n当调用了 selector.close()方法时, 我们其实是关闭了 selector 本身并且将所有的 selectionkey 失效, 但是并不会关闭 channel.\n\n\n# 完整的 selector 例子\n\npublic class nioechoserver {\n    private static final int buf_size = 256;\n    private static final int timeout = 3000;\n\n    public static void main(string args[]) throws exception {\n        // 打开服务端 socket\n        serversocketchannel serversocketchannel = serversocketchannel.open();\n\n        // 打开 selector\n        selector selector = selector.open();\n\n        // 服务端 socket 监听8080端口, 并配置为非阻塞模式\n        serversocketchannel.socket().bind(new inetsocketaddress(8080));\n        serversocketchannel.configureblocking(false);\n\n        // 将 channel 注册到 selector 中.\n        // 通常我们都是先注册一个 op_accept 事件, 然后在 op_accept 到来时, 再将这个 channel 的 op_read\n        // 注册到 selector 中.\n        serversocketchannel.register(selector, selectionkey.op_accept);\n\n        while (true) {\n            // 通过调用 select 方法, 阻塞地等待 channel i/o 可操作\n            if (selector.select(timeout) == 0) {\n                system.out.print(".");\n                continue;\n            }\n\n            // 获取 i/o 操作就绪的 selectionkey, 通过 selectionkey 可以知道哪些 channel 的哪类 i/o 操作已经就绪.\n            iterator<selectionkey> keyiterator = selector.selectedkeys().iterator();\n\n            while (keyiterator.hasnext()) {\n\n                // 当获取一个 selectionkey 后, 就要将它删除, 表示我们已经对这个 io 事件进行了处理.\n                keyiterator.remove();\n\n                selectionkey key = keyiterator.next();\n\n                if (key.isacceptable()) {\n                    // 当 op_accept 事件到来时, 我们就有从 serversocketchannel 中获取一个 socketchannel,\n                    // 代表客户端的连接\n                    // 注意, 在 op_accept 事件中, 从 key.channel() 返回的 channel 是 serversocketchannel.\n                    // 而在 op_write 和 op_read 中, 从 key.channel() 返回的是 socketchannel.\n                    socketchannel clientchannel = ((serversocketchannel) key.channel()).accept();\n                    clientchannel.configureblocking(false);\n                    //在 op_accept 到来时, 再将这个 channel 的 op_read 注册到 selector 中.\n                    // 注意, 这里我们如果没有设置 op_read 的话, 即 interest set 仍然是 op_connect 的话, 那么 select 方法会一直直接返回.\n                    clientchannel.register(key.selector(), op_read, bytebuffer.allocate(buf_size));\n                }\n\n                if (key.isreadable()) {\n                    socketchannel clientchannel = (socketchannel) key.channel();\n                    bytebuffer buf = (bytebuffer) key.attachment();\n                    long bytesread = clientchannel.read(buf);\n                    if (bytesread == -1) {\n                        clientchannel.close();\n                    } else if (bytesread > 0) {\n                        key.interestops(op_read | selectionkey.op_write);\n                        system.out.println("get data length: " + bytesread);\n                    }\n                }\n\n                if (key.isvalid() && key.iswritable()) {\n                    bytebuffer buf = (bytebuffer) key.attachment();\n                    buf.flip();\n                    socketchannel clientchannel = (socketchannel) key.channel();\n\n                    clientchannel.write(buf);\n\n                    if (!buf.hasremaining()) {\n                        key.interestops(op_read);\n                    }\n                    buf.compact();\n                }\n            }\n        }\n    }\n}\n\n\n\n# 总结\n\n\n# 参考资料\n\n * https://github.com/yongshun/learn_netty_source_code',charsets:{cjk:!0},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"EventLoop 源码解析",frontmatter:{title:"EventLoop 源码解析",date:"2024-09-18T21:10:31.000Z",permalink:"/pages/397456/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/20.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/10.EventLoop%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html",relativePath:"Netty 系统设计/20.三、主线任务/10.EventLoop 源码解析.md",key:"v-a2fb4e08",path:"/pages/397456/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"NioEventLoopGroup",slug:"nioeventloopgroup",normalizedTitle:"nioeventloopgroup",charIndex:99},{level:3,title:"关于 Reactor 的线程模型",slug:"关于-reactor-的线程模型",normalizedTitle:"关于 reactor 的线程模型",charIndex:353},{level:3,title:"NioEventLoopGroup 与 Reactor 线程模型的对应",slug:"nioeventloopgroup-与-reactor-线程模型的对应",normalizedTitle:"nioeventloopgroup 与 reactor 线程模型的对应",charIndex:1359},{level:4,title:"单线程模型",slug:"单线程模型",normalizedTitle:"单线程模型",charIndex:417},{level:4,title:"多线程模型",slug:"多线程模型",normalizedTitle:"多线程模型",charIndex:426},{level:4,title:"主从多线程模型",slug:"主从多线程模型",normalizedTitle:"主从多线程模型",charIndex:435},{level:3,title:"NioEventLoopGroup 类层次结构",slug:"nioeventloopgroup-类层次结构",normalizedTitle:"nioeventloopgroup 类层次结构",charIndex:3693},{level:3,title:"NioEventLoopGroup 实例化过程",slug:"nioeventloopgroup-实例化过程",normalizedTitle:"nioeventloopgroup 实例化过程",charIndex:3845},{level:2,title:"NioEventLoop",slug:"nioeventloop",normalizedTitle:"nioeventloop",charIndex:99},{level:3,title:"NioEventLoop 类层次结构",slug:"nioeventloop-类层次结构",normalizedTitle:"nioeventloop 类层次结构",charIndex:4801},{level:3,title:"NioEventLoop 的实例化过程",slug:"nioeventloop-的实例化过程",normalizedTitle:"nioeventloop 的实例化过程",charIndex:5551},{level:3,title:"EventLoop 与 Channel 的关联",slug:"eventloop-与-channel-的关联",normalizedTitle:"eventloop 与 channel 的关联",charIndex:6692},{level:3,title:"EventLoop 的启动",slug:"eventloop-的启动",normalizedTitle:"eventloop 的启动",charIndex:7524},{level:2,title:"Netty 的 IO 处理循环",slug:"netty-的-io-处理循环",normalizedTitle:"netty 的 io 处理循环",charIndex:10329},{level:3,title:"thread 的 run 循环",slug:"thread-的-run-循环",normalizedTitle:"thread 的 run 循环",charIndex:16320},{level:3,title:"IO 事件的轮询",slug:"io-事件的轮询",normalizedTitle:"io 事件的轮询",charIndex:18376},{level:3,title:"IO 事件的处理",slug:"io-事件的处理",normalizedTitle:"io 事件的处理",charIndex:20224},{level:4,title:"OP_READ 处理",slug:"op-read-处理",normalizedTitle:"op_read 处理",charIndex:25877},{level:4,title:"OP_WRITE 处理",slug:"op-write-处理",normalizedTitle:"op_write 处理",charIndex:28079},{level:4,title:"OP_CONNECT 处理",slug:"op-connect-处理",normalizedTitle:"op_connect 处理",charIndex:28324},{level:2,title:"Netty 的任务队列机制",slug:"netty-的任务队列机制",normalizedTitle:"netty 的任务队列机制",charIndex:29052},{level:3,title:"Task 的添加",slug:"task-的添加",normalizedTitle:"task 的添加",charIndex:29230},{level:4,title:"普通 Runnable 任务",slug:"普通-runnable-任务",normalizedTitle:"普通 runnable 任务",charIndex:29242},{level:4,title:"schedule 任务",slug:"schedule-任务",normalizedTitle:"schedule 任务",charIndex:30417},{level:3,title:"任务的执行",slug:"任务的执行",normalizedTitle:"任务的执行",charIndex:31982},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:10165},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:33666}],headersStr:"前言 NioEventLoopGroup 关于 Reactor 的线程模型 NioEventLoopGroup 与 Reactor 线程模型的对应 单线程模型 多线程模型 主从多线程模型 NioEventLoopGroup 类层次结构 NioEventLoopGroup 实例化过程 NioEventLoop NioEventLoop 类层次结构 NioEventLoop 的实例化过程 EventLoop 与 Channel 的关联 EventLoop 的启动 Netty 的 IO 处理循环 thread 的 run 循环 IO 事件的轮询 IO 事件的处理 OP_READ 处理 OP_WRITE 处理 OP_CONNECT 处理 Netty 的任务队列机制 Task 的添加 普通 Runnable 任务 schedule 任务 任务的执行 总结 参考资料",content:'# 前言\n\n这一章是 Netty 源码分析 的第三章, 我将在这一章中大家一起探究一下 Netty 的 EventLoop 的底层原理, 让大家对 Netty 的线程模型有更加深入的了解.\n\n\n# NioEventLoopGroup\n\n在 Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端) 章节中我们已经知道了, 一个 Netty 程序启动时, 至少要指定一个 EventLoopGroup(如果使用到的是 NIO, 那么通常是 NioEventLoopGroup), 那么这个 NioEventLoopGroup 在 Netty 中到底扮演着什么角色呢? 我们知道, Netty 是 Reactor 模型的一个实现, 那么首先从 Reactor 的线程模型开始吧.\n\n\n# 关于 Reactor 的线程模型\n\n首先我们来看一下 Reactor 的线程模型. Reactor 的线程模型有三种:\n\n * 单线程模型\n * 多线程模型\n * 主从多线程模型\n\n首先来看一下 单线程模型:\n\n\n\n所谓单线程, 即 acceptor 处理和 handler 处理都在一个线程中处理. 这个模型的坏处显而易见: 当其中某个 handler 阻塞时, 会导致其他所有的 client 的 handler 都得不到执行, 并且更严重的是, handler 的阻塞也会导致整个服务不能接收新的 client 请求(因为 acceptor 也被阻塞了). 因为有这么多的缺陷, 因此单线程Reactor 模型用的比较少.\n\n那么什么是 多线程模型 呢? Reactor 的多线程模型与单线程模型的区别就是 acceptor 是一个单独的线程处理, 并且有一组特定的 NIO 线程来负责各个客户端连接的 IO 操作. Reactor 多线程模型如下:\n\n\n\nReactor 多线程模型 有如下特点:\n\n * 有专门一个线程, 即 Acceptor 线程用于监听客户端的TCP连接请求.\n * 客户端连接的 IO 操作都是由一个特定的 NIO 线程池负责. 每个客户端连接都与一个特定的 NIO 线程绑定, 因此在这个客户端连接中的所有 IO 操作都是在同一个线程中完成的.\n * 客户端连接有很多, 但是 NIO 线程数是比较少的, 因此一个 NIO 线程可以同时绑定到多个客户端连接中.\n\n接下来我们再来看一下 Reactor 的主从多线程模型. 一般情况下, Reactor 的多线程模式已经可以很好的工作了, 但是我们考虑一下如下情况: 如果我们的服务器需要同时处理大量的客户端连接请求或我们需要在客户端连接时, 进行一些权限的检查, 那么单线程的 Acceptor 很有可能就处理不过来, 造成了大量的客户端不能连接到服务器. Reactor 的主从多线程模型就是在这样的情况下提出来的, 它的特点是: 服务器端接收客户端的连接请求不再是一个线程, 而是由一个独立的线程池组成. 它的线程模型如下:\n\n\n\n可以看到, Reactor 的主从多线程模型和 Reactor 多线程模型很类似, 只不过 Reactor 的主从多线程模型的 acceptor 使用了线程池来处理大量的客户端请求.\n\n\n# NioEventLoopGroup 与 Reactor 线程模型的对应\n\n我们介绍了三种 Reactor 的线程模型, 那么它们和 NioEventLoopGroup 又有什么关系呢? 其实, 不同的设置 NioEventLoopGroup 的方式就对应了不同的 Reactor 的线程模型.\n\n# 单线程模型\n\n来看一下下面的例子:\n\nEventLoopGroup bossGroup = new NioEventLoopGroup(1);\nServerBootstrap b = new ServerBootstrap();\nb.group(bossGroup)\n .channel(NioServerSocketChannel.class)\n ...\n\n\n注意, 我们实例化了一个 NioEventLoopGroup, 构造器参数是1, 表示 NioEventLoopGroup 的线程池大小是1. 然后接着我们调用 b.group(bossGroup) 设置了服务器端的 EventLoopGroup. 有些朋友可能会有疑惑: 我记得在启动服务器端的 Netty 程序时, 是需要设置 bossGroup 和 workerGroup 的, 为什么这里就只有一个 bossGroup? 其实很简单, ServerBootstrap 重写了 group 方法:\n\n@Override\npublic ServerBootstrap group(EventLoopGroup group) {\n    return group(group, group);\n}\n\n\n因此当传入一个 group 时, 那么 bossGroup 和 workerGroup 就是同一个 NioEventLoopGroup 了. 这时候呢, 因为 bossGroup 和 workerGroup 就是同一个 NioEventLoopGroup, 并且这个 NioEventLoopGroup 只有一个线程, 这样就会导致 Netty 中的 acceptor 和后续的所有客户端连接的 IO 操作都是在一个线程中处理的. 那么对应到 Reactor 的线程模型中, 我们这样设置 NioEventLoopGroup 时, 就相当于 Reactor 单线程模型.\n\n# 多线程模型\n\n同理, 再来看一下下面的例子:\n\nEventLoopGroup bossGroup = new NioEventLoopGroup(1);\nEventLoopGroup workerGroup = new NioEventLoopGroup();\nServerBootstrap b = new ServerBootstrap();\nb.group(bossGroup, workerGroup)\n .channel(NioServerSocketChannel.class)\n ...\n\n\nbossGroup 中只有一个线程, 而 workerGroup 中的线程是 CPU 核心数乘以2, 因此对应的到 Reactor 线程模型中, 我们知道, 这样设置的 NioEventLoopGroup 其实就是 Reactor 多线程模型.\n\n# 主从多线程模型\n\n相信读者朋友都想到了, 实现主从线程模型的例子如下:\n\nEventLoopGroup bossGroup = new NioEventLoopGroup(4);\nEventLoopGroup workerGroup = new NioEventLoopGroup();\nServerBootstrap b = new ServerBootstrap();\nb.group(bossGroup, workerGroup)\n .channel(NioServerSocketChannel.class)\n ...\n\n\nbossGroup 线程池中的线程数我们设置为4, 而 workerGroup 中的线程是 CPU 核心数乘以2, 因此对应的到 Reactor 线程模型中, 我们知道, 这样设置的 NioEventLoopGroup 其实就是 Reactor 主从多线程模型.\n\n----------------------------------------\n\n根据 @labmem 的提示, Netty 的服务器端的 acceptor 阶段, 没有使用到多线程, 因此上面的 主从多线程模型 在 Netty 的服务器端是不存在的\n\n:: warning\n\n服务器端的 ServerSocketChannel 只绑定到了 bossGroup 中的一个线程, 因此在调用 Java NIO 的 Selector.select 处理客户端的连接请求时, 实际上是在一个线程中的, 所以对只有一个服务的应用来说, bossGroup 设置多个线程是没有什么作用的, 反而还会造成资源浪费.\n\n:::\n\n经 Google, Netty 中的 bossGroup 为什么使用线程池的原因大家众所纷纭, 不过我在 stackoverflow 上找到一个比较靠谱的答案:\n\n> the creator of Netty says multiple boss threads are useful if we share NioEventLoopGroup between different server bootstraps, but I don\'t see the reason for it. 因此上面的 主从多线程模型 分析是有问题, 抱歉.\n\n\n# NioEventLoopGroup 类层次结构\n\n[](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 三 我就是大名鼎鼎的 EventLoop/NioEventLoopGroup.png)\n\n\n# NioEventLoopGroup 实例化过程\n\n在前面 Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端) 章节中, 我们已经简单地介绍了一下 NioEventLoopGroup 的初始化过程, 这里再回顾一下:\n\n\n\n即:\n\n * EventLoopGroup(其实是MultithreadEventExecutorGroup) 内部维护一个类型为 EventExecutor children 数组, 其大小是 nThreads, 这样就构成了一个线程池\n * 如果我们在实例化 NioEventLoopGroup 时, 如果指定线程池大小, 则 nThreads 就是指定的值, 反之是处理器核心数 * 2\n * MultithreadEventExecutorGroup 中会调用 newChild 抽象方法来初始化 children 数组\n * 抽象方法 newChild 是在 NioEventLoopGroup 中实现的, 它返回一个 NioEventLoop 实例.\n * NioEventLoop 属性:\n * SelectorProvider provider 属性: NioEventLoopGroup 构造器中通过 SelectorProvider.provider() 获取一个 SelectorProvider\n * Selector selector 属性: NioEventLoop 构造器中通过调用通过 selector = provider.openSelector() 获取一个 selector 对象.\n\n\n# NioEventLoop\n\nNioEventLoop 继承于 SingleThreadEventLoop, 而 SingleThreadEventLoop 又继承于 SingleThreadEventExecutor. SingleThreadEventExecutor 是 Netty 中对本地线程的抽象, 它内部有一个 Thread thread 属性, 存储了一个本地 Java 线程. 因此我们可以认为, 一个 NioEventLoop 其实和一个特定的线程绑定, 并且在其生命周期内, 绑定的线程都不会再改变.\n\n\n# NioEventLoop 类层次结构\n\n[](https://github.com/yongshun/learn_netty_source_code/blob/master/Netty 源码分析之 三 我就是大名鼎鼎的 EventLoop/NioEventLoop.png)\n\nNioEventLoop 的类层次结构图还是比较复杂的, 不过我们只需要关注几个重要的点即可. 首先 NioEventLoop 的继承链如下:\n\nNioEventLoop -> SingleThreadEventLoop -> SingleThreadEventExecutor -> AbstractScheduledEventExecutor\n\n\n在 AbstractScheduledEventExecutor 中, Netty 实现了 NioEventLoop 的 schedule 功能, 即我们可以通过调用一个 NioEventLoop 实例的 schedule 方法来运行一些定时任务. 而在 SingleThreadEventLoop 中, 又实现了任务队列的功能, 通过它, 我们可以调用一个 NioEventLoop 实例的 execute 方法来向任务队列中添加一个 task, 并由 NioEventLoop 进行调度执行.\n\n通常来说, NioEventLoop 肩负着两种任务, 第一个是作为 IO 线程, 执行与 Channel 相关的 IO 操作, 包括 调用 select 等待就绪的 IO 事件、读写数据与数据的处理等; 而第二个任务是作为任务队列, 执行 taskQueue 中的任务, 例如用户调用 eventLoop.schedule 提交的定时任务也是这个线程执行的.\n\n\n# NioEventLoop 的实例化过程\n\n\n\n从上图可以看到, SingleThreadEventExecutor 有一个名为 thread 的 Thread 类型字段, 这个字段就代表了与 SingleThreadEventExecutor 关联的本地线程. 下面是这个构造器的代码:\n\nprotected SingleThreadEventExecutor(\n        EventExecutorGroup parent, ThreadFactory threadFactory, boolean addTaskWakesUp) {\n    this.parent = parent;\n    this.addTaskWakesUp = addTaskWakesUp;\n\n    thread = threadFactory.newThread(new Runnable() {\n        @Override\n        public void run() {\n            boolean success = false;\n            updateLastExecutionTime();\n            try {\n                SingleThreadEventExecutor.this.run();\n                success = true;\n            } catch (Throwable t) {\n                logger.warn("Unexpected exception from an event executor: ", t);\n            } finally {\n                // 省略清理代码\n                ...\n            }\n        }\n    });\n    threadProperties = new DefaultThreadProperties(thread);\n    taskQueue = newTaskQueue();\n}\n\n\n在 SingleThreadEventExecutor 构造器中, 通过 threadFactory.newThread 创建了一个新的 Java 线程. 在这个线程中所做的事情主要就是调用 SingleThreadEventExecutor.this.run() 方法, 而因为 NioEventLoop 实现了这个方法, 因此根据多态性, 其实调用的是 NioEventLoop.run() 方法\n\n\n# EventLoop 与 Channel 的关联\n\nNetty 中, 每个 Channel 都有且仅有一个 EventLoop 与之关联, 它们的关联过程如下:\n\n\n\n从上图中我们可以看到, 当调用了 AbstractChannel#AbstractUnsafe.register 后, 就完成了 Channel 和 EventLoop 的关联. register 实现如下:\n\n@Override\npublic final void register(EventLoop eventLoop, final ChannelPromise promise) {\n    // 删除条件检查.\n    ...\n    AbstractChannel.this.eventLoop = eventLoop;\n\n    if (eventLoop.inEventLoop()) {\n        register0(promise);\n    } else {\n        try {\n            eventLoop.execute(new OneTimeTask() {\n                @Override\n                public void run() {\n                    register0(promise);\n                }\n            });\n        } catch (Throwable t) {\n            ...\n        }\n    }\n}\n\n\n在 AbstractChannel#AbstractUnsafe.register 中, 会将一个 EventLoop 赋值给 AbstractChannel 内部的 eventLoop 字段, 到这里就完成了 EventLoop 与 Channel 的关联过程.\n\n\n# EventLoop 的启动\n\n在前面我们已经知道了, NioEventLoop 本身就是一个 SingleThreadEventExecutor, 因此 NioEventLoop 的启动, 其实就是 NioEventLoop 所绑定的本地 Java 线程的启动. 依照这个思想, 我们只要找到在哪里调用了 SingleThreadEventExecutor 的 thread 字段的 start() 方法就可以知道是在哪里启动的这个线程了. 从代码中搜索, thread.start() 被封装到 SingleThreadEventExecutor.startThread() 方法中了:\n\nprivate void startThread() {\n    if (STATE_UPDATER.get(this) == ST_NOT_STARTED) {\n        if (STATE_UPDATER.compareAndSet(this, ST_NOT_STARTED, ST_STARTED)) {\n            thread.start();\n        }\n    }\n}\n\n\nSTATE_UPDATER 是 SingleThreadEventExecutor 内部维护的一个属性, 它的作用是标识当前的 thread 的状态. 在初始的时候, STATE_UPDATER == ST_NOT_STARTED, 因此第一次调用 startThread() 方法时, 就会进入到 if 语句内, 进而调用到 thread.start(). 而这个关键的 startThread() 方法又是在哪里调用的呢? 经过方法调用关系搜索, 我们发现, startThread 是在 SingleThreadEventExecutor.execute 方法中调用的:\n\n@Override\npublic void execute(Runnable task) {\n    if (task == null) {\n        throw new NullPointerException("task");\n    }\n\n    boolean inEventLoop = inEventLoop();\n    if (inEventLoop) {\n        addTask(task);\n    } else {\n        startThread(); // 调用 startThread 方法, 启动 EventLoop 线程.\n        addTask(task);\n        if (isShutdown() && removeTask(task)) {\n            reject();\n        }\n    }\n\n    if (!addTaskWakesUp && wakesUpForTask(task)) {\n        wakeup(inEventLoop);\n    }\n}\n\n\n既然如此, 那现在我们的工作就变为了寻找 在哪里第一次调用了 SingleThreadEventExecutor.execute() 方法. 如果留心的读者可能已经注意到了, 我们在 EventLoop 与 Channel 的关联 这一小节时, 有提到到在注册 channel 的过程中, 会在 AbstractChannel#AbstractUnsafe.register 中调用 eventLoop.execute 方法, 在 EventLoop 中进行 Channel 注册代码的执行, AbstractChannel#AbstractUnsafe.register 部分代码如下:\n\nif (eventLoop.inEventLoop()) {\n    register0(promise);\n} else {\n    try {\n        eventLoop.execute(new OneTimeTask() {\n            @Override\n            public void run() {\n                register0(promise);\n            }\n        });\n    } catch (Throwable t) {\n        ...\n    }\n}\n\n\n很显然, 一路从 Bootstrap.bind 方法跟踪到 AbstractChannel#AbstractUnsafe.register 方法, 整个代码都是在主线程中运行的, 因此上面的 eventLoop.inEventLoop() 就为 false, 于是进入到 else 分支, 在这个分支中调用了 eventLoop.execute. eventLoop 是一个 NioEventLoop 的实例, 而 NioEventLoop 没有实现 execute 方法, 因此调用的是 SingleThreadEventExecutor.execute:\n\n@Override\npublic void execute(Runnable task) {\n    ...\n    boolean inEventLoop = inEventLoop();\n    if (inEventLoop) {\n        addTask(task);\n    } else {\n        startThread();\n        addTask(task);\n        if (isShutdown() && removeTask(task)) {\n            reject();\n        }\n    }\n\n    if (!addTaskWakesUp && wakesUpForTask(task)) {\n        wakeup(inEventLoop);\n    }\n}\n\n\n我们已经分析过了, inEventLoop == false, 因此执行到 else 分支, 在这里就调用了 startThread() 方法来启动 SingleThreadEventExecutor 内部关联的 Java 本地线程了. 总结一句话, 当 EventLoop.execute 第一次被调用时, 就会触发 startThread() 的调用, 进而导致了 EventLoop 所对应的 Java 线程的启动. 我们将 EventLoop 与 Channel 的关联 小节中的时序图补全后, 就得到了 EventLoop 启动过程的时序图:\n\n\n\n\n# Netty 的 IO 处理循环\n\n笔记\n\n在 Netty 中, 一个 EventLoop 需要负责两个工作,\n\n * 第一个是作为 IO 线程, 负责相应的 IO 操作\n * 第二个是作为任务线程, 执行 taskQueue 中的任务\n\n接下来我们先从 IO 操纵方面入手, 看一下 TCP 数据是如何从 Java NIO Socket 传递到我们的 handler 中的\n\nNetty 是 Reactor 模型的一个实现, 并且是基于 Java NIO 的, 那么从 Java NIO 的前生今世 之四 NIO Selector 详解 中我们知道, Netty 中必然有一个 Selector 线程, 用于不断调用 Java NIO 的 Selector.select 方法, 查询当前是否有就绪的 IO 事件. 回顾一下在 Java NIO 中所讲述的 Selector 的使用流程:\n\n 1. 通过 Selector.open() 打开一个 Selector.\n 2. 将 Channel 注册到 Selector 中, 并设置需要监听的事件(interest set)\n 3. 不断重复:\n    1. 调用 select() 方法\n    2. 调用 selector.selectedKeys() 获取 selected keys\n    3. 迭代每个 selected key:\n       1. 从 selected key 中获取 对应的 Channel 和附加信息(如果有的话)\n       2. 判断是哪些 IO 事件已经就绪了, 然后处理它们. 如果是 OP_ACCEPT 事件, 则调用 "SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept()" 获取 SocketChannel, 并将它设置为 非阻塞的, 然后将这个 Channel 注册到 Selector 中.\n       3. 根据需要更改 selected key 的监听事件.\n       4. 将已经处理过的 key 从 selected keys 集合中删除.\n\n上面的使用流程用代码来体现就是:\n\n/**\n * @author echo\n * @version 1.0\n * @created 2024/8/1 13:13\n */\npublic class NioEchoServer {\n    private static final int BUF_SIZE = 256;\n    private static final int TIMEOUT = 3000;\n\n    public static void main(String args[]) throws Exception {\n        // 打开服务端 Socket\n        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();\n\n        // 打开 Selector\n        Selector selector = Selector.open();\n\n        // 服务端 Socket 监听8080端口, 并配置为非阻塞模式\n        serverSocketChannel.socket().bind(new InetSocketAddress(8080));\n        serverSocketChannel.configureBlocking(false);\n\n        // 将 channel 注册到 selector 中.\n        // 通常我们都是先注册一个 OP_ACCEPT 事件, 然后在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ\n        // 注册到 Selector 中.\n        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);\n\n        while (true) {\n            // 通过调用 select 方法, 阻塞地等待 channel I/O 可操作\n            if (selector.select(TIMEOUT) == 0) {\n                System.out.print(".");\n                continue;\n            }\n\n            // 获取 I/O 操作就绪的 SelectionKey, 通过 SelectionKey 可以知道哪些 Channel 的哪类 I/O 操作已经就绪.\n            Iterator<SelectionKey> keyIterator = selector.selectedKeys().iterator();\n\n            while (keyIterator.hasNext()) {\n\n                // 当获取一个 SelectionKey 后, 就要将它删除, 表示我们已经对这个 IO 事件进行了处理.\n                keyIterator.remove();\n\n                SelectionKey key = keyIterator.next();\n\n                if (key.isAcceptable()) {\n                    // 当 OP_ACCEPT 事件到来时, 我们就有从 ServerSocketChannel 中获取一个 SocketChannel,\n                    // 代表客户端的连接\n                    // 注意, 在 OP_ACCEPT 事件中, 从 key.channel() 返回的 Channel 是 ServerSocketChannel.\n                    // 而在 OP_WRITE 和 OP_READ 中, 从 key.channel() 返回的是 SocketChannel.\n                    SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept();\n                    clientChannel.configureBlocking(false);\n                    //在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ 注册到 Selector 中.\n                    // 注意, 这里我们如果没有设置 OP_READ 的话, 即 interest set 仍然是 OP_CONNECT 的话, 那么 select 方法会一直直接返回.\n                    clientChannel.register(key.selector(), OP_READ, ByteBuffer.allocate(BUF_SIZE));\n                }\n\n                if (key.isReadable()) {\n                    SocketChannel clientChannel = (SocketChannel) key.channel();\n                    ByteBuffer buf = (ByteBuffer) key.attachment();\n                    long bytesRead = clientChannel.read(buf);\n                    if (bytesRead == -1) {\n                        clientChannel.close();\n                    } else if (bytesRead > 0) {\n                        key.interestOps(OP_READ | SelectionKey.OP_WRITE);\n                        System.out.println("Get data length: " + bytesRead);\n                    }\n                }\n\n                if (key.isValid() && key.isWritable()) {\n                    ByteBuffer buf = (ByteBuffer) key.attachment();\n                    buf.flip();\n                    SocketChannel clientChannel = (SocketChannel) key.channel();\n\n                    clientChannel.write(buf);\n\n                    if (!buf.hasRemaining()) {\n                        key.interestOps(OP_READ);\n                    }\n                    buf.compact();\n                }\n            }\n        }\n    }\n}\n\n\n还记得不, 上面操作的第一步 通过 Selector.open() 打开一个 Selector 我们已经在第一章的 Channel 实例化 这一小节中已经提到了, Netty 中是通过调用 SelectorProvider.openSocketChannel() 来打开一个新的 Java NIO SocketChannel:\n\nprivate static SocketChannel newSocket(SelectorProvider provider) {\n    ...\n    return provider.openSocketChannel();\n}\n\n\n第二步 将 Channel 注册到 Selector 中, 并设置需要监听的事件(interest set) 的操作我们在第一章 channel 的注册过程 中也分析过了, 我们在来回顾一下, 在客户端的 Channel 注册过程中, 会有如下调用链:\n\nBootstrap.initAndRegister -> \n\tAbstractBootstrap.initAndRegister -> \n\t\tMultithreadEventLoopGroup.register -> \n\t\t\tSingleThreadEventLoop.register -> \n\t\t\t\tAbstractUnsafe.register ->\n\t\t\t\t\tAbstractUnsafe.register0 ->\n\t\t\t\t\t\tAbstractNioChannel.doRegister\n\n\n在 AbstractUnsafe.register 方法中调用了 register0 方法:\n\n@Override\npublic final void register(EventLoop eventLoop, final ChannelPromise promise) {\n\t// 省略条件判断和错误处理\n    AbstractChannel.this.eventLoop = eventLoop;\n    register0(promise);\n}\n\n\nregister0 方法代码如下:\n\nprivate void register0(ChannelPromise promise) {\n    boolean firstRegistration = neverRegistered;\n    doRegister();\n    neverRegistered = false;\n    registered = true;\n    safeSetSuccess(promise);\n    pipeline.fireChannelRegistered();\n    // Only fire a channelActive if the channel has never been registered. This prevents firing\n    // multiple channel actives if the channel is deregistered and re-registered.\n    if (firstRegistration && isActive()) {\n        pipeline.fireChannelActive();\n    }\n}\n\n\nregister0 又调用了 AbstractNioChannel.doRegister:\n\n@Override\nprotected void doRegister() throws Exception {\n\t// 省略错误处理\n    selectionKey = javaChannel().register(eventLoop().selector, 0, this);\n}\n\n\n在这里 javaChannel() 返回的是一个 Java NIO SocketChannel 对象, 我们将此 SocketChannel 注册到前面第一步获取的 Selector 中.\n\n那么接下来的第三步的循环是在哪里实现的呢? 第三步的操作就是我们今天分析的关键, 下面我会一步一步向读者展示出来.\n\n\n# thread 的 run 循环\n\n在 EventLoop 的启动 一小节中, 我们已经了解到了, 当 EventLoop.execute 第一次被调用时, 就会触发 startThread() 的调用, 进而导致了 EventLoop 所对应的 Java 线程的启动. 接着我们来更深入一些, 来看一下此线程启动后都会做什么东东吧. 下面是此线程的 run() 方法, 我已经把一些异常处理和收尾工作的代码都去掉了. 这个 run 方法可以说是十分简单, 主要就是调用了 SingleThreadEventExecutor.this.run() 方法. 而 SingleThreadEventExecutor.run() 是一个抽象方法, 它的实现在 NioEventLoop 中.\n\nthread = threadFactory.newThread(new Runnable() {\n    @Override\n    public void run() {\n        boolean success = false;\n        updateLastExecutionTime();\n        try {\n            SingleThreadEventExecutor.this.run();\n            success = true;\n        } catch (Throwable t) {\n            logger.warn("Unexpected exception from an event executor: ", t);\n        } finally {\n            ...\n        }\n    }\n});\n\n\n继续跟踪到 NioEventLoop.run() 方法, 其源码如下:\n\n@Override\nprotected void run() {\n    for (;;) {\n        boolean oldWakenUp = wakenUp.getAndSet(false);\n        try {\n            if (hasTasks()) {\n                selectNow();\n            } else {\n                select(oldWakenUp);\n                if (wakenUp.get()) {\n                    selector.wakeup();\n                }\n            }\n\n            cancelledKeys = 0;\n            needsToSelectAgain = false;\n            final int ioRatio = this.ioRatio;\n            if (ioRatio == 100) {\n                processSelectedKeys();\n                runAllTasks();\n            } else {\n                final long ioStartTime = System.nanoTime();\n\n                processSelectedKeys();\n\n                final long ioTime = System.nanoTime() - ioStartTime;\n                runAllTasks(ioTime * (100 - ioRatio) / ioRatio);\n            }\n\n            if (isShuttingDown()) {\n                closeAll();\n                if (confirmShutdown()) {\n                    break;\n                }\n            }\n        } catch (Throwable t) {\n            ...\n        }\n    }\n}\n\n\n啊哈, 看到了上面代码的 for(;😉 所构成的死循环了没? 原来 NioEventLoop 事件循环的核心就是这里! 现在我们把上面所提到的 Selector 使用步骤的第三步的部分也找到了. 这个 run 方法可以说是 Netty NIO 的核心, 属于重中之重, 把它分析明白了, 那么对 Netty 的事件循环机制也就了解了大部分了. 让我们一鼓作气, 继续分析下去吧~\n\n\n# IO 事件的轮询\n\n首先, 在 run 方法中, 第一步是调用 hasTasks() 方法来判断当前任务队列中是否有任务:\n\nprotected boolean hasTasks() {\n    assert inEventLoop();\n    return !taskQueue.isEmpty();\n}\n\n\n这个方法很简单, 仅仅是检查了一下 taskQueue 是否为空. 至于 taskQueue 是什么呢, 其实它就是存放一系列的需要由此 EventLoop 所执行的任务列表. 关于 taskQueue, 我们这里暂时不表, 等到后面再来详细分析它. 当 taskQueue 不为空时, 就执行到了 if 分支中的 selectNow() 方法. 然而当 taskQueue 为空时, 执行的是 select(oldWakenUp) 方法. 那么 selectNow() 和 select(oldWakenUp) 之间有什么区别呢? 来看一下, selectNow() 的源码如下:\n\nvoid selectNow() throws IOException {\n    try {\n        selector.selectNow();\n    } finally {\n        // restore wakup state if needed\n        if (wakenUp.get()) {\n            selector.wakeup();\n        }\n    }\n}\n\n\n首先调用了 selector.selectNow() 方法, 这里 selector 是什么大家还有印象不? 我们在第一章 Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端) 时对它有过介绍, 这个 selector 字段正是 Java NIO 中的多路复用器 Selector. 那么这里 selector.selectNow() 就很好理解了, selectNow() 方法会检查当前是否有就绪的 IO 事件, 如果有, 则返回就绪 IO 事件的个数; 如果没有, 则返回0. 注意, selectNow() 是立即返回的, 不会阻塞当前线程. 当 selectNow() 调用后, finally 语句块中会检查 wakenUp 变量是否为 true, 当为 true 时, 调用 selector.wakeup() 唤醒 select() 的阻塞调用.\n\n看了 if 分支的 selectNow 方法后, 我们再来看一下 else 分支的 select(oldWakenUp) 方法. 其实 else 分支的 select(oldWakenUp) 方法的处理逻辑比较复杂, 而我们这里的目的暂时不是分析这个方法调用的具体工作, 因此我这里长话短说, 只列出我们我们关注的内如:\n\nprivate void select(boolean oldWakenUp) throws IOException {\n    Selector selector = this.selector;\n    try {\n        ...\n        int selectedKeys = selector.select(timeoutMillis);\n        ...\n    } catch (CancelledKeyException e) {\n        ...\n    }\n}\n\n\n在这个 select 方法中, 调用了 selector.select(timeoutMillis), 而这个调用是会阻塞住当前线程的, timeoutMillis 是阻塞的超时时间. 到来这里, 我们可以看到, 当 hasTasks() 为真时, 调用的的 selectNow() 方法是不会阻塞当前线程的, 而当 hasTasks() 为假时, 调用的 select(oldWakenUp) 是会阻塞当前线程的. 这其实也很好理解: 当 taskQueue 中没有任务时, 那么 Netty 可以阻塞地等待 IO 就绪事件; 而当 taskQueue 中有任务时, 我们自然地希望所提交的任务可以尽快地执行, 因此 Netty 会调用非阻塞的 selectNow() 方法, 以保证 taskQueue 中的任务尽快可以执行.\n\n\n# IO 事件的处理\n\n在 NioEventLoop.run() 方法中, 第一步是通过 select/selectNow 调用查询当前是否有就绪的 IO 事件. 那么当有 IO 事件就绪时, 第二步自然就是处理这些 IO 事件啦. 首先让我们来看一下 NioEventLoop.run 中循环的剩余部分:\n\nfinal int ioRatio = this.ioRatio;\nif (ioRatio == 100) {\n    processSelectedKeys();\n    runAllTasks();\n} else {\n    final long ioStartTime = System.nanoTime();\n\n    processSelectedKeys();\n\n    final long ioTime = System.nanoTime() - ioStartTime;\n    runAllTasks(ioTime * (100 - ioRatio) / ioRatio);\n}\n\n\n上面列出的代码中, 有两个关键的调用, 第一个是 processSelectedKeys() 调用, 根据字面意思, 我们可以猜出这个方法肯定是查询就绪的 IO 事件, 然后处理它; 第二个调用是 runAllTasks(), 这个方法我们也可以一眼就看出来它的功能就是运行 taskQueue 中的任务. 这里的代码还有一个十分有意思的地方, 即 ioRatio. 那什么是 ioRatio呢? 它表示的是此线程分配给 IO 操作所占的时间比(即运行 processSelectedKeys 耗时在整个循环中所占用的时间). 例如 ioRatio 默认是 50, 则表示 IO 操作和执行 task 的所占用的线程执行时间比是 1 : 1. 当知道了 IO 操作耗时和它所占用的时间比, 那么执行 task 的时间就可以很方便的计算出来了:\n\n设 IO 操作耗时为 ioTime, ioTime 占的时间比例为 ioRatio, 则:\n\tioTime / ioRatio = taskTime / taskRatio\n\ttaskRatio = 100 - ioRatio\n\t=> taskTime = ioTime * (100 - ioRatio) / ioRatio\n\n\n根据上面的公式, 当我们设置 ioRate = 70 时, 则表示 IO 运行耗时占比为70%, 即假设某次循环一共耗时为 100ms, 那么根据公式, 我们知道 processSelectedKeys() 方法调用所耗时大概为70ms(即 IO 耗时), 而 runAllTasks() 耗时大概为 30ms(即执行 task 耗时). 当 ioRatio 为 100 时, Netty 就不考虑 IO 耗时的占比, 而是分别调用 processSelectedKeys()、runAllTasks(); 而当 ioRatio 不为 100时, 则执行到 else 分支, 在这个分支中, 首先记录下 processSelectedKeys() 所执行的时间(即 IO 操作的耗时), 然后根据公式, 计算出执行 task 所占用的时间, 然后以此为参数, 调用 runAllTasks().\n\n我们这里先分析一下 processSelectedKeys() 方法调用, runAllTasks() 我们留到下一节再分析. processSelectedKeys() 方法的源码如下:\n\nprivate void processSelectedKeys() {\n    if (selectedKeys != null) {\n        processSelectedKeysOptimized(selectedKeys.flip());\n    } else {\n        processSelectedKeysPlain(selector.selectedKeys());\n    }\n}\n\n\n这个方法中, 会根据 selectedKeys 字段是否为空, 而分别调用 processSelectedKeysOptimized 或 processSelectedKeysPlain. selectedKeys 字段是在调用 openSelector() 方法时, 根据 JVM 平台的不同, 而有设置不同的值, 在我所调试这个值是不为 null 的. 其实 processSelectedKeysOptimized 方法 processSelectedKeysPlain 没有太大的区别, 为了简单起见, 我们以 processSelectedKeysOptimized 为例分析一下源码的工作流程吧.\n\nprivate void processSelectedKeysOptimized(SelectionKey[] selectedKeys) {\n    for (int i = 0;; i ++) {\n        final SelectionKey k = selectedKeys[i];\n        if (k == null) {\n            break;\n        }\n        selectedKeys[i] = null;\n\n        final Object a = k.attachment();\n\n        if (a instanceof AbstractNioChannel) {\n            processSelectedKey(k, (AbstractNioChannel) a);\n        } else {\n            @SuppressWarnings("unchecked")\n            NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;\n            processSelectedKey(k, task);\n        }\n        ...\n    }\n}\n\n\n其实你别看它代码挺多的, 但是关键的点就两个: 迭代 selectedKeys 获取就绪的 IO 事件, 然后为每个事件都调用 processSelectedKey 来处理它. 这里正好完美对应上了我们提到的 Selector 的使用流程中的第三步里操作. 还有一点需要注意的是, 我们可以调用 selectionKey.attach(object) 给一个 selectionKey 设置一个附加的字段, 然后可以通过 Object attachedObj = selectionKey.attachment() 获取它. 上面代代码正是通过了 k.attachment() 来获取一个附加在 selectionKey 中的对象, 那么这个对象是什么呢? 它又是在哪里设置的呢? 我们再来回忆一下 SocketChannel 是如何注册到 Selector 中的: 在客户端的 Channel 注册过程中, 会有如下调用链:\n\nBootstrap.initAndRegister -> \n\tAbstractBootstrap.initAndRegister -> \n\t\tMultithreadEventLoopGroup.register -> \n\t\t\tSingleThreadEventLoop.register -> \n\t\t\t\tAbstractUnsafe.register ->\n\t\t\t\t\tAbstractUnsafe.register0 ->\n\t\t\t\t\t\tAbstractNioChannel.doRegister\n\n\n最后的 AbstractNioChannel.doRegister 方法会调用 SocketChannel.register 方法注册一个 SocketChannel 到指定的 Selector:\n\n@Override\nprotected void doRegister() throws Exception {\n\t// 省略错误处理\n    selectionKey = javaChannel().register(eventLoop().selector, 0, this);\n}\n\n\n特别注意一下 register 的第三个参数, 这个参数是设置 selectionKey 的附加对象的, 和调用 selectionKey.attach(object) 的效果一样. 而调用 register 所传递的第三个参数是 this, 它其实就是一个 NioSocketChannel 的实例. 那么这里就很清楚了, 我们在将 SocketChannel 注册到 Selector 中时, 将 SocketChannel 所对应的 NioSocketChannel 以附加字段的方式添加到了selectionKey 中. 再回到 processSelectedKeysOptimized 方法中, 当我们获取到附加的对象后, 我们就调用 processSelectedKey 来处理这个 IO 事件:\n\nfinal Object a = k.attachment();\n\nif (a instanceof AbstractNioChannel) {\n    processSelectedKey(k, (AbstractNioChannel) a);\n} else {\n    @SuppressWarnings("unchecked")\n    NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;\n    processSelectedKey(k, task);\n}\n\n\nprocessSelectedKey 方法源码如下:\n\nprivate static void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {\n    final NioUnsafe unsafe = ch.unsafe();\n    ...\n    try {\n        int readyOps = k.readyOps();\n        \n        // 可读事件\n        if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {\n            unsafe.read();\n            if (!ch.isOpen()) {\n                // Connection already closed - no need to handle write.\n                return;\n            }\n        }\n\t\t\n\t\t// 可写事件\n        if ((readyOps & SelectionKey.OP_WRITE) != 0) {\n            // Call forceFlush which will also take care of clear the OP_WRITE once there is nothing left to write\n            ch.unsafe().forceFlush();\n        }\n        \n        // 连接建立事件\n        if ((readyOps & SelectionKey.OP_CONNECT) != 0) {\n            // remove OP_CONNECT as otherwise Selector.select(..) will always return without blocking\n            // See https://github.com/netty/netty/issues/924\n            int ops = k.interestOps();\n            ops &= ~SelectionKey.OP_CONNECT;\n            k.interestOps(ops);\n\n            unsafe.finishConnect();\n        }\n    } catch (CancelledKeyException ignored) {\n        unsafe.close(unsafe.voidPromise());\n    }\n}\n\n\n这个代码是不是很熟悉啊? 完全是 Java NIO 的 Selector 的那一套处理流程嘛! processSelectedKey 中处理了三个事件, 分别是:\n\n * OP_READ, 可读事件, 即 Channel 中收到了新数据可供上层读取.\n * OP_WRITE, 可写事件, 即上层可以向 Channel 写入数据.\n * OP_CONNECT, 连接建立事件, 即 TCP 连接已经建立, Channel 处于 active 状态.\n\n下面我们分别根据这三个事件来看一下 Netty 是怎么处理的吧.\n\n# OP_READ 处理\n\n当就绪的 IO 事件是 OP_READ, 代码会调用 unsafe.read() 方法, 即:\n\n// 可读事件\nif ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {\n    unsafe.read();\n    if (!ch.isOpen()) {\n        // Connection already closed - no need to handle write.\n        return;\n    }\n}\n\n\nunsafe 这个字段, 我们已经和它打了太多的交道了, 在第一章 Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端) 中我们已经对它进行过浓墨重彩地分析了, 最后我们确定了它是一个 NioSocketChannelUnsafe 实例, 负责的是 Channel 的底层 IO 操作. 我们可以利用 Intellij IDEA 提供的 Go To Implementations 功能, 寻找到这个方法的实现. 最后我们发现这个方法没有在 NioSocketChannelUnsafe 中实现, 而是在它的父类 AbstractNioByteChannel 实现的, 它的实现源码如下:\n\n@Override\npublic final void read() {\n    ...\n    ByteBuf byteBuf = null;\n    int messages = 0;\n    boolean close = false;\n    try {\n        int totalReadAmount = 0;\n        boolean readPendingReset = false;\n        do {\n            byteBuf = allocHandle.allocate(allocator);\n            int writable = byteBuf.writableBytes();\n            int localReadAmount = doReadBytes(byteBuf);\n\n            // 检查读取结果.\n            ...\n\n            pipeline.fireChannelRead(byteBuf);\n            byteBuf = null;\n\n            ...\n\n            totalReadAmount += localReadAmount;\n        \n            // 检查是否是配置了自动读取, 如果不是, 则立即退出循环.\n            ...\n        } while (++ messages < maxMessagesPerRead);\n\n        pipeline.fireChannelReadComplete();\n        allocHandle.record(totalReadAmount);\n\n        if (close) {\n            closeOnRead(pipeline);\n            close = false;\n        }\n    } catch (Throwable t) {\n        handleReadException(pipeline, byteBuf, t, close);\n    } finally {\n    }\n}\n\n\nread() 源码比较长, 我为了篇幅起见, 删除了部分代码, 只留下了主干. 不过我建议读者朋友们自己一定要看一下 read() 源码, 这对理解 Netty 的 EventLoop 十分有帮助. 上面 read 方法其实归纳起来, 可以认为做了如下工作:\n\n 1. 分配 ByteBuf\n 2. 从 SocketChannel 中读取数据\n 3. 调用 pipeline.fireChannelRead 发送一个 inbound 事件.\n\n前面两点没什么好说的, 第三点 pipeline.fireChannelRead 读者朋友们看到了有没有会心一笑地感觉呢? 反正我看到这里时是有的. pipeline.fireChannelRead 正好就是我们在第二章 Netty 源码分析之 二 贯穿Netty 的大动脉 ── ChannelPipeline (二) 中分析的 inbound 事件起点. 当调用了 pipeline.fireIN_EVT() 后, 那么就产生了一个 inbound 事件, 此事件会以 head -> customContext -> tail 的方向依次流经 ChannelPipeline 中的各个 handler. 调用了 pipeline.fireChannelRead 后, 就是 ChannelPipeline 中所需要做的工作了, 这些我们已经在第二章中有过详细讨论, 这里就展开了.\n\n# OP_WRITE 处理\n\nOP_WRITE 可写事件代码如下. 这里代码比较简单, 没有详细分析的必要了.\n\nif ((readyOps & SelectionKey.OP_WRITE) != 0) {\n    // Call forceFlush which will also take care of clear the OP_WRITE once there is nothing left to write\n    ch.unsafe().forceFlush();\n}\n\n\n# OP_CONNECT 处理\n\n最后一个事件是 OP_CONNECT, 即 TCP 连接已建立事件.\n\nif ((readyOps & SelectionKey.OP_CONNECT) != 0) {\n    // remove OP_CONNECT as otherwise Selector.select(..) will always return without blocking\n    // See https://github.com/netty/netty/issues/924\n    int ops = k.interestOps();\n    ops &= ~SelectionKey.OP_CONNECT;\n    k.interestOps(ops);\n\n    unsafe.finishConnect();\n}\n\n\nOP_CONNECT 事件的处理中, 只做了两件事情:\n\n 1. 正如代码中的注释所言, 我们需要将 OP_CONNECT 从就绪事件集中清除, 不然会一直有 OP_CONNECT 事件.\n 2. 调用 unsafe.finishConnect() 通知上层连接已建立\n\nunsafe.finishConnect() 调用最后会调用到 pipeline().fireChannelActive(), 产生一个 inbound 事件, 通知 pipeline 中的各个 handler TCP 通道已建立(即 ChannelInboundHandler.channelActive 方法会被调用)\n\n到了这里, 我们整个 NioEventLoop 的 IO 操作部分已经了解完了, 接下来的一节我们要重点分析一下 Netty 的任务队列机制.\n\n\n# Netty 的任务队列机制\n\n我们已经提到过, 在Netty 中, 一个 NioEventLoop 通常需要肩负起两种任务, 第一个是作为 IO 线程, 处理 IO 操作; 第二个就是作为任务线程, 处理 taskQueue 中的任务. 这一节的重点就是分析一下 NioEventLoop 的任务队列机制的.\n\n\n# Task 的添加\n\n# 普通 Runnable 任务\n\nNioEventLoop 继承于 SingleThreadEventExecutor, 而 SingleThreadEventExecutor 中有一个 Queue taskQueue 字段, 用于存放添加的 Task. 在 Netty 中, 每个 Task 都使用一个实现了 Runnable 接口的实例来表示. 例如当我们需要将一个 Runnable 添加到 taskQueue 中时, 我们可以进行如下操作:\n\nEventLoop eventLoop = channel.eventLoop();\neventLoop.execute(new Runnable() {\n    @Override\n    public void run() {\n        System.out.println("Hello, Netty!");\n    }\n});\n\n\n当调用 execute 后, 实际上是调用到了 SingleThreadEventExecutor.execute() 方法, 它的实现如下:\n\n@Override\npublic void execute(Runnable task) {\n    if (task == null) {\n        throw new NullPointerException("task");\n    }\n\n    boolean inEventLoop = inEventLoop();\n    if (inEventLoop) {\n        addTask(task);\n    } else {\n        startThread();\n        addTask(task);\n        if (isShutdown() && removeTask(task)) {\n            reject();\n        }\n    }\n\n    if (!addTaskWakesUp && wakesUpForTask(task)) {\n        wakeup(inEventLoop);\n    }\n}\n\n\n而添加任务的 addTask 方法的源码如下:\n\nprotected void addTask(Runnable task) {\n    if (task == null) {\n        throw new NullPointerException("task");\n    }\n    if (isShutdown()) {\n        reject();\n    }\n    taskQueue.add(task);\n}\n\n\n因此实际上, taskQueue 是存放着待执行的任务的队列.\n\n# schedule 任务\n\n除了通过 execute 添加普通的 Runnable 任务外, 我们还可以通过调用 eventLoop.scheduleXXX 之类的方法来添加一个定时任务. EventLoop 中实现任务队列的功能在超类 SingleThreadEventExecutor 实现的, 而 schedule 功能的实现是在 SingleThreadEventExecutor 的父类, 即 AbstractScheduledEventExecutor 中实现的. 在 AbstractScheduledEventExecutor 中, 有以 scheduledTaskQueue 字段:\n\nQueue<ScheduledFutureTask<?>> scheduledTaskQueue;\n\n\nscheduledTaskQueue 是一个队列(Queue), 其中存放的元素是 ScheduledFutureTask. 而 ScheduledFutureTask 我们很容易猜到, 它是对 Schedule 任务的一个抽象. 我们来看一下 AbstractScheduledEventExecutor 所实现的 schedule 方法吧:\n\n@Override\npublic  ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit) {\n    ObjectUtil.checkNotNull(command, "command");\n    ObjectUtil.checkNotNull(unit, "unit");\n    if (delay < 0) {\n        throw new IllegalArgumentException(\n                String.format("delay: %d (expected: >= 0)", delay));\n    }\n    return schedule(new ScheduledFutureTask<Void>(\n            this, command, null, ScheduledFutureTask.deadlineNanos(unit.toNanos(delay))));\n}\n\n\n这是其中一个重载的 schedule, 当一个 Runnable 传递进来后, 会被封装为一个 ScheduledFutureTask 对象, 这个对象会记录下这个 Runnable 在何时运行、已何种频率运行等信息. 当构建了 ScheduledFutureTask 后, 会继续调用 另一个重载的 schedule 方法:\n\n<V> ScheduledFuture<V> schedule(final ScheduledFutureTask<V> task) {\n    if (inEventLoop()) {\n        scheduledTaskQueue().add(task);\n    } else {\n        execute(new OneTimeTask() {\n            @Override\n            public void run() {\n                scheduledTaskQueue().add(task);\n            }\n        });\n    }\n\n    return task;\n}\n\n\n在这个方法中, ScheduledFutureTask 对象就会被添加到 scheduledTaskQueue 中了\n\n\n# 任务的执行\n\n当一个任务被添加到 taskQueue 后, 它是怎么被 EventLoop 执行的呢? 让我们回到 NioEventLoop.run() 方法中, 在这个方法里, 会分别调用 processSelectedKeys() 和 runAllTasks() 方法, 来进行 IO 事件的处理和 task 的处理. processSelectedKeys() 方法我们已经分析过了, 下面我们来看一下 runAllTasks() 中到底有什么名堂吧. runAllTasks 方法有两个重载的方法, 一个是无参数的, 另一个有一个参数的. 首先来看一下无参数的 runAllTasks:\n\nprotected boolean runAllTasks() {\n    fetchFromScheduledTaskQueue();\n    Runnable task = pollTask();\n    if (task == null) {\n        return false;\n    }\n\n    for (;;) {\n        try {\n            task.run();\n        } catch (Throwable t) {\n            logger.warn("A task raised an exception.", t);\n        }\n\n        task = pollTask();\n        if (task == null) {\n            lastExecutionTime = ScheduledFutureTask.nanoTime();\n            return true;\n        }\n    }\n}\n\n\n我们前面已经提到过, EventLoop 可以通过调用 EventLoop.execute 来将一个 Runnable 提交到 taskQueue 中, 也可以通过调用 EventLoop.schedule 来提交一个 schedule 任务到 scheduledTaskQueue 中. 在此方法的一开始调用的 fetchFromScheduledTaskQueue() 其实就是将 scheduledTaskQueue 中已经可以执行的(即定时时间已到的 schedule 任务) 拿出来并添加到 taskQueue 中, 作为可执行的 task 等待被调度执行. 它的源码如下:\n\nprivate void fetchFromScheduledTaskQueue() {\n    if (hasScheduledTasks()) {\n        long nanoTime = AbstractScheduledEventExecutor.nanoTime();\n        for (;;) {\n            Runnable scheduledTask = pollScheduledTask(nanoTime);\n            if (scheduledTask == null) {\n                break;\n            }\n            taskQueue.add(scheduledTask);\n        }\n    }\n}\n\n\n接下来 runAllTasks() 方法就会不断调用 task = pollTask() 从 taskQueue 中获取一个可执行的 task, 然后调用它的 run() 方法来运行此 task.\n\n> 注意, 因为 EventLoop 既需要执行 IO 操作, 又需要执行 task, 因此我们在调用 EventLoop.execute 方法提交任务时, 不要提交耗时任务, 更不能提交一些会造成阻塞的任务, 不然会导致我们的 IO 线程得不到调度, 影响整个程序的并发量.\n\n\n# 总结\n\n\n# 参考资料',normalizedContent:'# 前言\n\n这一章是 netty 源码分析 的第三章, 我将在这一章中大家一起探究一下 netty 的 eventloop 的底层原理, 让大家对 netty 的线程模型有更加深入的了解.\n\n\n# nioeventloopgroup\n\n在 netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端) 章节中我们已经知道了, 一个 netty 程序启动时, 至少要指定一个 eventloopgroup(如果使用到的是 nio, 那么通常是 nioeventloopgroup), 那么这个 nioeventloopgroup 在 netty 中到底扮演着什么角色呢? 我们知道, netty 是 reactor 模型的一个实现, 那么首先从 reactor 的线程模型开始吧.\n\n\n# 关于 reactor 的线程模型\n\n首先我们来看一下 reactor 的线程模型. reactor 的线程模型有三种:\n\n * 单线程模型\n * 多线程模型\n * 主从多线程模型\n\n首先来看一下 单线程模型:\n\n\n\n所谓单线程, 即 acceptor 处理和 handler 处理都在一个线程中处理. 这个模型的坏处显而易见: 当其中某个 handler 阻塞时, 会导致其他所有的 client 的 handler 都得不到执行, 并且更严重的是, handler 的阻塞也会导致整个服务不能接收新的 client 请求(因为 acceptor 也被阻塞了). 因为有这么多的缺陷, 因此单线程reactor 模型用的比较少.\n\n那么什么是 多线程模型 呢? reactor 的多线程模型与单线程模型的区别就是 acceptor 是一个单独的线程处理, 并且有一组特定的 nio 线程来负责各个客户端连接的 io 操作. reactor 多线程模型如下:\n\n\n\nreactor 多线程模型 有如下特点:\n\n * 有专门一个线程, 即 acceptor 线程用于监听客户端的tcp连接请求.\n * 客户端连接的 io 操作都是由一个特定的 nio 线程池负责. 每个客户端连接都与一个特定的 nio 线程绑定, 因此在这个客户端连接中的所有 io 操作都是在同一个线程中完成的.\n * 客户端连接有很多, 但是 nio 线程数是比较少的, 因此一个 nio 线程可以同时绑定到多个客户端连接中.\n\n接下来我们再来看一下 reactor 的主从多线程模型. 一般情况下, reactor 的多线程模式已经可以很好的工作了, 但是我们考虑一下如下情况: 如果我们的服务器需要同时处理大量的客户端连接请求或我们需要在客户端连接时, 进行一些权限的检查, 那么单线程的 acceptor 很有可能就处理不过来, 造成了大量的客户端不能连接到服务器. reactor 的主从多线程模型就是在这样的情况下提出来的, 它的特点是: 服务器端接收客户端的连接请求不再是一个线程, 而是由一个独立的线程池组成. 它的线程模型如下:\n\n\n\n可以看到, reactor 的主从多线程模型和 reactor 多线程模型很类似, 只不过 reactor 的主从多线程模型的 acceptor 使用了线程池来处理大量的客户端请求.\n\n\n# nioeventloopgroup 与 reactor 线程模型的对应\n\n我们介绍了三种 reactor 的线程模型, 那么它们和 nioeventloopgroup 又有什么关系呢? 其实, 不同的设置 nioeventloopgroup 的方式就对应了不同的 reactor 的线程模型.\n\n# 单线程模型\n\n来看一下下面的例子:\n\neventloopgroup bossgroup = new nioeventloopgroup(1);\nserverbootstrap b = new serverbootstrap();\nb.group(bossgroup)\n .channel(nioserversocketchannel.class)\n ...\n\n\n注意, 我们实例化了一个 nioeventloopgroup, 构造器参数是1, 表示 nioeventloopgroup 的线程池大小是1. 然后接着我们调用 b.group(bossgroup) 设置了服务器端的 eventloopgroup. 有些朋友可能会有疑惑: 我记得在启动服务器端的 netty 程序时, 是需要设置 bossgroup 和 workergroup 的, 为什么这里就只有一个 bossgroup? 其实很简单, serverbootstrap 重写了 group 方法:\n\n@override\npublic serverbootstrap group(eventloopgroup group) {\n    return group(group, group);\n}\n\n\n因此当传入一个 group 时, 那么 bossgroup 和 workergroup 就是同一个 nioeventloopgroup 了. 这时候呢, 因为 bossgroup 和 workergroup 就是同一个 nioeventloopgroup, 并且这个 nioeventloopgroup 只有一个线程, 这样就会导致 netty 中的 acceptor 和后续的所有客户端连接的 io 操作都是在一个线程中处理的. 那么对应到 reactor 的线程模型中, 我们这样设置 nioeventloopgroup 时, 就相当于 reactor 单线程模型.\n\n# 多线程模型\n\n同理, 再来看一下下面的例子:\n\neventloopgroup bossgroup = new nioeventloopgroup(1);\neventloopgroup workergroup = new nioeventloopgroup();\nserverbootstrap b = new serverbootstrap();\nb.group(bossgroup, workergroup)\n .channel(nioserversocketchannel.class)\n ...\n\n\nbossgroup 中只有一个线程, 而 workergroup 中的线程是 cpu 核心数乘以2, 因此对应的到 reactor 线程模型中, 我们知道, 这样设置的 nioeventloopgroup 其实就是 reactor 多线程模型.\n\n# 主从多线程模型\n\n相信读者朋友都想到了, 实现主从线程模型的例子如下:\n\neventloopgroup bossgroup = new nioeventloopgroup(4);\neventloopgroup workergroup = new nioeventloopgroup();\nserverbootstrap b = new serverbootstrap();\nb.group(bossgroup, workergroup)\n .channel(nioserversocketchannel.class)\n ...\n\n\nbossgroup 线程池中的线程数我们设置为4, 而 workergroup 中的线程是 cpu 核心数乘以2, 因此对应的到 reactor 线程模型中, 我们知道, 这样设置的 nioeventloopgroup 其实就是 reactor 主从多线程模型.\n\n----------------------------------------\n\n根据 @labmem 的提示, netty 的服务器端的 acceptor 阶段, 没有使用到多线程, 因此上面的 主从多线程模型 在 netty 的服务器端是不存在的\n\n:: warning\n\n服务器端的 serversocketchannel 只绑定到了 bossgroup 中的一个线程, 因此在调用 java nio 的 selector.select 处理客户端的连接请求时, 实际上是在一个线程中的, 所以对只有一个服务的应用来说, bossgroup 设置多个线程是没有什么作用的, 反而还会造成资源浪费.\n\n:::\n\n经 google, netty 中的 bossgroup 为什么使用线程池的原因大家众所纷纭, 不过我在 stackoverflow 上找到一个比较靠谱的答案:\n\n> the creator of netty says multiple boss threads are useful if we share nioeventloopgroup between different server bootstraps, but i don\'t see the reason for it. 因此上面的 主从多线程模型 分析是有问题, 抱歉.\n\n\n# nioeventloopgroup 类层次结构\n\n[](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 三 我就是大名鼎鼎的 eventloop/nioeventloopgroup.png)\n\n\n# nioeventloopgroup 实例化过程\n\n在前面 netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端) 章节中, 我们已经简单地介绍了一下 nioeventloopgroup 的初始化过程, 这里再回顾一下:\n\n\n\n即:\n\n * eventloopgroup(其实是multithreadeventexecutorgroup) 内部维护一个类型为 eventexecutor children 数组, 其大小是 nthreads, 这样就构成了一个线程池\n * 如果我们在实例化 nioeventloopgroup 时, 如果指定线程池大小, 则 nthreads 就是指定的值, 反之是处理器核心数 * 2\n * multithreadeventexecutorgroup 中会调用 newchild 抽象方法来初始化 children 数组\n * 抽象方法 newchild 是在 nioeventloopgroup 中实现的, 它返回一个 nioeventloop 实例.\n * nioeventloop 属性:\n * selectorprovider provider 属性: nioeventloopgroup 构造器中通过 selectorprovider.provider() 获取一个 selectorprovider\n * selector selector 属性: nioeventloop 构造器中通过调用通过 selector = provider.openselector() 获取一个 selector 对象.\n\n\n# nioeventloop\n\nnioeventloop 继承于 singlethreadeventloop, 而 singlethreadeventloop 又继承于 singlethreadeventexecutor. singlethreadeventexecutor 是 netty 中对本地线程的抽象, 它内部有一个 thread thread 属性, 存储了一个本地 java 线程. 因此我们可以认为, 一个 nioeventloop 其实和一个特定的线程绑定, 并且在其生命周期内, 绑定的线程都不会再改变.\n\n\n# nioeventloop 类层次结构\n\n[](https://github.com/yongshun/learn_netty_source_code/blob/master/netty 源码分析之 三 我就是大名鼎鼎的 eventloop/nioeventloop.png)\n\nnioeventloop 的类层次结构图还是比较复杂的, 不过我们只需要关注几个重要的点即可. 首先 nioeventloop 的继承链如下:\n\nnioeventloop -> singlethreadeventloop -> singlethreadeventexecutor -> abstractscheduledeventexecutor\n\n\n在 abstractscheduledeventexecutor 中, netty 实现了 nioeventloop 的 schedule 功能, 即我们可以通过调用一个 nioeventloop 实例的 schedule 方法来运行一些定时任务. 而在 singlethreadeventloop 中, 又实现了任务队列的功能, 通过它, 我们可以调用一个 nioeventloop 实例的 execute 方法来向任务队列中添加一个 task, 并由 nioeventloop 进行调度执行.\n\n通常来说, nioeventloop 肩负着两种任务, 第一个是作为 io 线程, 执行与 channel 相关的 io 操作, 包括 调用 select 等待就绪的 io 事件、读写数据与数据的处理等; 而第二个任务是作为任务队列, 执行 taskqueue 中的任务, 例如用户调用 eventloop.schedule 提交的定时任务也是这个线程执行的.\n\n\n# nioeventloop 的实例化过程\n\n\n\n从上图可以看到, singlethreadeventexecutor 有一个名为 thread 的 thread 类型字段, 这个字段就代表了与 singlethreadeventexecutor 关联的本地线程. 下面是这个构造器的代码:\n\nprotected singlethreadeventexecutor(\n        eventexecutorgroup parent, threadfactory threadfactory, boolean addtaskwakesup) {\n    this.parent = parent;\n    this.addtaskwakesup = addtaskwakesup;\n\n    thread = threadfactory.newthread(new runnable() {\n        @override\n        public void run() {\n            boolean success = false;\n            updatelastexecutiontime();\n            try {\n                singlethreadeventexecutor.this.run();\n                success = true;\n            } catch (throwable t) {\n                logger.warn("unexpected exception from an event executor: ", t);\n            } finally {\n                // 省略清理代码\n                ...\n            }\n        }\n    });\n    threadproperties = new defaultthreadproperties(thread);\n    taskqueue = newtaskqueue();\n}\n\n\n在 singlethreadeventexecutor 构造器中, 通过 threadfactory.newthread 创建了一个新的 java 线程. 在这个线程中所做的事情主要就是调用 singlethreadeventexecutor.this.run() 方法, 而因为 nioeventloop 实现了这个方法, 因此根据多态性, 其实调用的是 nioeventloop.run() 方法\n\n\n# eventloop 与 channel 的关联\n\nnetty 中, 每个 channel 都有且仅有一个 eventloop 与之关联, 它们的关联过程如下:\n\n\n\n从上图中我们可以看到, 当调用了 abstractchannel#abstractunsafe.register 后, 就完成了 channel 和 eventloop 的关联. register 实现如下:\n\n@override\npublic final void register(eventloop eventloop, final channelpromise promise) {\n    // 删除条件检查.\n    ...\n    abstractchannel.this.eventloop = eventloop;\n\n    if (eventloop.ineventloop()) {\n        register0(promise);\n    } else {\n        try {\n            eventloop.execute(new onetimetask() {\n                @override\n                public void run() {\n                    register0(promise);\n                }\n            });\n        } catch (throwable t) {\n            ...\n        }\n    }\n}\n\n\n在 abstractchannel#abstractunsafe.register 中, 会将一个 eventloop 赋值给 abstractchannel 内部的 eventloop 字段, 到这里就完成了 eventloop 与 channel 的关联过程.\n\n\n# eventloop 的启动\n\n在前面我们已经知道了, nioeventloop 本身就是一个 singlethreadeventexecutor, 因此 nioeventloop 的启动, 其实就是 nioeventloop 所绑定的本地 java 线程的启动. 依照这个思想, 我们只要找到在哪里调用了 singlethreadeventexecutor 的 thread 字段的 start() 方法就可以知道是在哪里启动的这个线程了. 从代码中搜索, thread.start() 被封装到 singlethreadeventexecutor.startthread() 方法中了:\n\nprivate void startthread() {\n    if (state_updater.get(this) == st_not_started) {\n        if (state_updater.compareandset(this, st_not_started, st_started)) {\n            thread.start();\n        }\n    }\n}\n\n\nstate_updater 是 singlethreadeventexecutor 内部维护的一个属性, 它的作用是标识当前的 thread 的状态. 在初始的时候, state_updater == st_not_started, 因此第一次调用 startthread() 方法时, 就会进入到 if 语句内, 进而调用到 thread.start(). 而这个关键的 startthread() 方法又是在哪里调用的呢? 经过方法调用关系搜索, 我们发现, startthread 是在 singlethreadeventexecutor.execute 方法中调用的:\n\n@override\npublic void execute(runnable task) {\n    if (task == null) {\n        throw new nullpointerexception("task");\n    }\n\n    boolean ineventloop = ineventloop();\n    if (ineventloop) {\n        addtask(task);\n    } else {\n        startthread(); // 调用 startthread 方法, 启动 eventloop 线程.\n        addtask(task);\n        if (isshutdown() && removetask(task)) {\n            reject();\n        }\n    }\n\n    if (!addtaskwakesup && wakesupfortask(task)) {\n        wakeup(ineventloop);\n    }\n}\n\n\n既然如此, 那现在我们的工作就变为了寻找 在哪里第一次调用了 singlethreadeventexecutor.execute() 方法. 如果留心的读者可能已经注意到了, 我们在 eventloop 与 channel 的关联 这一小节时, 有提到到在注册 channel 的过程中, 会在 abstractchannel#abstractunsafe.register 中调用 eventloop.execute 方法, 在 eventloop 中进行 channel 注册代码的执行, abstractchannel#abstractunsafe.register 部分代码如下:\n\nif (eventloop.ineventloop()) {\n    register0(promise);\n} else {\n    try {\n        eventloop.execute(new onetimetask() {\n            @override\n            public void run() {\n                register0(promise);\n            }\n        });\n    } catch (throwable t) {\n        ...\n    }\n}\n\n\n很显然, 一路从 bootstrap.bind 方法跟踪到 abstractchannel#abstractunsafe.register 方法, 整个代码都是在主线程中运行的, 因此上面的 eventloop.ineventloop() 就为 false, 于是进入到 else 分支, 在这个分支中调用了 eventloop.execute. eventloop 是一个 nioeventloop 的实例, 而 nioeventloop 没有实现 execute 方法, 因此调用的是 singlethreadeventexecutor.execute:\n\n@override\npublic void execute(runnable task) {\n    ...\n    boolean ineventloop = ineventloop();\n    if (ineventloop) {\n        addtask(task);\n    } else {\n        startthread();\n        addtask(task);\n        if (isshutdown() && removetask(task)) {\n            reject();\n        }\n    }\n\n    if (!addtaskwakesup && wakesupfortask(task)) {\n        wakeup(ineventloop);\n    }\n}\n\n\n我们已经分析过了, ineventloop == false, 因此执行到 else 分支, 在这里就调用了 startthread() 方法来启动 singlethreadeventexecutor 内部关联的 java 本地线程了. 总结一句话, 当 eventloop.execute 第一次被调用时, 就会触发 startthread() 的调用, 进而导致了 eventloop 所对应的 java 线程的启动. 我们将 eventloop 与 channel 的关联 小节中的时序图补全后, 就得到了 eventloop 启动过程的时序图:\n\n\n\n\n# netty 的 io 处理循环\n\n笔记\n\n在 netty 中, 一个 eventloop 需要负责两个工作,\n\n * 第一个是作为 io 线程, 负责相应的 io 操作\n * 第二个是作为任务线程, 执行 taskqueue 中的任务\n\n接下来我们先从 io 操纵方面入手, 看一下 tcp 数据是如何从 java nio socket 传递到我们的 handler 中的\n\nnetty 是 reactor 模型的一个实现, 并且是基于 java nio 的, 那么从 java nio 的前生今世 之四 nio selector 详解 中我们知道, netty 中必然有一个 selector 线程, 用于不断调用 java nio 的 selector.select 方法, 查询当前是否有就绪的 io 事件. 回顾一下在 java nio 中所讲述的 selector 的使用流程:\n\n 1. 通过 selector.open() 打开一个 selector.\n 2. 将 channel 注册到 selector 中, 并设置需要监听的事件(interest set)\n 3. 不断重复:\n    1. 调用 select() 方法\n    2. 调用 selector.selectedkeys() 获取 selected keys\n    3. 迭代每个 selected key:\n       1. 从 selected key 中获取 对应的 channel 和附加信息(如果有的话)\n       2. 判断是哪些 io 事件已经就绪了, 然后处理它们. 如果是 op_accept 事件, 则调用 "socketchannel clientchannel = ((serversocketchannel) key.channel()).accept()" 获取 socketchannel, 并将它设置为 非阻塞的, 然后将这个 channel 注册到 selector 中.\n       3. 根据需要更改 selected key 的监听事件.\n       4. 将已经处理过的 key 从 selected keys 集合中删除.\n\n上面的使用流程用代码来体现就是:\n\n/**\n * @author echo\n * @version 1.0\n * @created 2024/8/1 13:13\n */\npublic class nioechoserver {\n    private static final int buf_size = 256;\n    private static final int timeout = 3000;\n\n    public static void main(string args[]) throws exception {\n        // 打开服务端 socket\n        serversocketchannel serversocketchannel = serversocketchannel.open();\n\n        // 打开 selector\n        selector selector = selector.open();\n\n        // 服务端 socket 监听8080端口, 并配置为非阻塞模式\n        serversocketchannel.socket().bind(new inetsocketaddress(8080));\n        serversocketchannel.configureblocking(false);\n\n        // 将 channel 注册到 selector 中.\n        // 通常我们都是先注册一个 op_accept 事件, 然后在 op_accept 到来时, 再将这个 channel 的 op_read\n        // 注册到 selector 中.\n        serversocketchannel.register(selector, selectionkey.op_accept);\n\n        while (true) {\n            // 通过调用 select 方法, 阻塞地等待 channel i/o 可操作\n            if (selector.select(timeout) == 0) {\n                system.out.print(".");\n                continue;\n            }\n\n            // 获取 i/o 操作就绪的 selectionkey, 通过 selectionkey 可以知道哪些 channel 的哪类 i/o 操作已经就绪.\n            iterator<selectionkey> keyiterator = selector.selectedkeys().iterator();\n\n            while (keyiterator.hasnext()) {\n\n                // 当获取一个 selectionkey 后, 就要将它删除, 表示我们已经对这个 io 事件进行了处理.\n                keyiterator.remove();\n\n                selectionkey key = keyiterator.next();\n\n                if (key.isacceptable()) {\n                    // 当 op_accept 事件到来时, 我们就有从 serversocketchannel 中获取一个 socketchannel,\n                    // 代表客户端的连接\n                    // 注意, 在 op_accept 事件中, 从 key.channel() 返回的 channel 是 serversocketchannel.\n                    // 而在 op_write 和 op_read 中, 从 key.channel() 返回的是 socketchannel.\n                    socketchannel clientchannel = ((serversocketchannel) key.channel()).accept();\n                    clientchannel.configureblocking(false);\n                    //在 op_accept 到来时, 再将这个 channel 的 op_read 注册到 selector 中.\n                    // 注意, 这里我们如果没有设置 op_read 的话, 即 interest set 仍然是 op_connect 的话, 那么 select 方法会一直直接返回.\n                    clientchannel.register(key.selector(), op_read, bytebuffer.allocate(buf_size));\n                }\n\n                if (key.isreadable()) {\n                    socketchannel clientchannel = (socketchannel) key.channel();\n                    bytebuffer buf = (bytebuffer) key.attachment();\n                    long bytesread = clientchannel.read(buf);\n                    if (bytesread == -1) {\n                        clientchannel.close();\n                    } else if (bytesread > 0) {\n                        key.interestops(op_read | selectionkey.op_write);\n                        system.out.println("get data length: " + bytesread);\n                    }\n                }\n\n                if (key.isvalid() && key.iswritable()) {\n                    bytebuffer buf = (bytebuffer) key.attachment();\n                    buf.flip();\n                    socketchannel clientchannel = (socketchannel) key.channel();\n\n                    clientchannel.write(buf);\n\n                    if (!buf.hasremaining()) {\n                        key.interestops(op_read);\n                    }\n                    buf.compact();\n                }\n            }\n        }\n    }\n}\n\n\n还记得不, 上面操作的第一步 通过 selector.open() 打开一个 selector 我们已经在第一章的 channel 实例化 这一小节中已经提到了, netty 中是通过调用 selectorprovider.opensocketchannel() 来打开一个新的 java nio socketchannel:\n\nprivate static socketchannel newsocket(selectorprovider provider) {\n    ...\n    return provider.opensocketchannel();\n}\n\n\n第二步 将 channel 注册到 selector 中, 并设置需要监听的事件(interest set) 的操作我们在第一章 channel 的注册过程 中也分析过了, 我们在来回顾一下, 在客户端的 channel 注册过程中, 会有如下调用链:\n\nbootstrap.initandregister -> \n\tabstractbootstrap.initandregister -> \n\t\tmultithreadeventloopgroup.register -> \n\t\t\tsinglethreadeventloop.register -> \n\t\t\t\tabstractunsafe.register ->\n\t\t\t\t\tabstractunsafe.register0 ->\n\t\t\t\t\t\tabstractniochannel.doregister\n\n\n在 abstractunsafe.register 方法中调用了 register0 方法:\n\n@override\npublic final void register(eventloop eventloop, final channelpromise promise) {\n\t// 省略条件判断和错误处理\n    abstractchannel.this.eventloop = eventloop;\n    register0(promise);\n}\n\n\nregister0 方法代码如下:\n\nprivate void register0(channelpromise promise) {\n    boolean firstregistration = neverregistered;\n    doregister();\n    neverregistered = false;\n    registered = true;\n    safesetsuccess(promise);\n    pipeline.firechannelregistered();\n    // only fire a channelactive if the channel has never been registered. this prevents firing\n    // multiple channel actives if the channel is deregistered and re-registered.\n    if (firstregistration && isactive()) {\n        pipeline.firechannelactive();\n    }\n}\n\n\nregister0 又调用了 abstractniochannel.doregister:\n\n@override\nprotected void doregister() throws exception {\n\t// 省略错误处理\n    selectionkey = javachannel().register(eventloop().selector, 0, this);\n}\n\n\n在这里 javachannel() 返回的是一个 java nio socketchannel 对象, 我们将此 socketchannel 注册到前面第一步获取的 selector 中.\n\n那么接下来的第三步的循环是在哪里实现的呢? 第三步的操作就是我们今天分析的关键, 下面我会一步一步向读者展示出来.\n\n\n# thread 的 run 循环\n\n在 eventloop 的启动 一小节中, 我们已经了解到了, 当 eventloop.execute 第一次被调用时, 就会触发 startthread() 的调用, 进而导致了 eventloop 所对应的 java 线程的启动. 接着我们来更深入一些, 来看一下此线程启动后都会做什么东东吧. 下面是此线程的 run() 方法, 我已经把一些异常处理和收尾工作的代码都去掉了. 这个 run 方法可以说是十分简单, 主要就是调用了 singlethreadeventexecutor.this.run() 方法. 而 singlethreadeventexecutor.run() 是一个抽象方法, 它的实现在 nioeventloop 中.\n\nthread = threadfactory.newthread(new runnable() {\n    @override\n    public void run() {\n        boolean success = false;\n        updatelastexecutiontime();\n        try {\n            singlethreadeventexecutor.this.run();\n            success = true;\n        } catch (throwable t) {\n            logger.warn("unexpected exception from an event executor: ", t);\n        } finally {\n            ...\n        }\n    }\n});\n\n\n继续跟踪到 nioeventloop.run() 方法, 其源码如下:\n\n@override\nprotected void run() {\n    for (;;) {\n        boolean oldwakenup = wakenup.getandset(false);\n        try {\n            if (hastasks()) {\n                selectnow();\n            } else {\n                select(oldwakenup);\n                if (wakenup.get()) {\n                    selector.wakeup();\n                }\n            }\n\n            cancelledkeys = 0;\n            needstoselectagain = false;\n            final int ioratio = this.ioratio;\n            if (ioratio == 100) {\n                processselectedkeys();\n                runalltasks();\n            } else {\n                final long iostarttime = system.nanotime();\n\n                processselectedkeys();\n\n                final long iotime = system.nanotime() - iostarttime;\n                runalltasks(iotime * (100 - ioratio) / ioratio);\n            }\n\n            if (isshuttingdown()) {\n                closeall();\n                if (confirmshutdown()) {\n                    break;\n                }\n            }\n        } catch (throwable t) {\n            ...\n        }\n    }\n}\n\n\n啊哈, 看到了上面代码的 for(;😉 所构成的死循环了没? 原来 nioeventloop 事件循环的核心就是这里! 现在我们把上面所提到的 selector 使用步骤的第三步的部分也找到了. 这个 run 方法可以说是 netty nio 的核心, 属于重中之重, 把它分析明白了, 那么对 netty 的事件循环机制也就了解了大部分了. 让我们一鼓作气, 继续分析下去吧~\n\n\n# io 事件的轮询\n\n首先, 在 run 方法中, 第一步是调用 hastasks() 方法来判断当前任务队列中是否有任务:\n\nprotected boolean hastasks() {\n    assert ineventloop();\n    return !taskqueue.isempty();\n}\n\n\n这个方法很简单, 仅仅是检查了一下 taskqueue 是否为空. 至于 taskqueue 是什么呢, 其实它就是存放一系列的需要由此 eventloop 所执行的任务列表. 关于 taskqueue, 我们这里暂时不表, 等到后面再来详细分析它. 当 taskqueue 不为空时, 就执行到了 if 分支中的 selectnow() 方法. 然而当 taskqueue 为空时, 执行的是 select(oldwakenup) 方法. 那么 selectnow() 和 select(oldwakenup) 之间有什么区别呢? 来看一下, selectnow() 的源码如下:\n\nvoid selectnow() throws ioexception {\n    try {\n        selector.selectnow();\n    } finally {\n        // restore wakup state if needed\n        if (wakenup.get()) {\n            selector.wakeup();\n        }\n    }\n}\n\n\n首先调用了 selector.selectnow() 方法, 这里 selector 是什么大家还有印象不? 我们在第一章 netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端) 时对它有过介绍, 这个 selector 字段正是 java nio 中的多路复用器 selector. 那么这里 selector.selectnow() 就很好理解了, selectnow() 方法会检查当前是否有就绪的 io 事件, 如果有, 则返回就绪 io 事件的个数; 如果没有, 则返回0. 注意, selectnow() 是立即返回的, 不会阻塞当前线程. 当 selectnow() 调用后, finally 语句块中会检查 wakenup 变量是否为 true, 当为 true 时, 调用 selector.wakeup() 唤醒 select() 的阻塞调用.\n\n看了 if 分支的 selectnow 方法后, 我们再来看一下 else 分支的 select(oldwakenup) 方法. 其实 else 分支的 select(oldwakenup) 方法的处理逻辑比较复杂, 而我们这里的目的暂时不是分析这个方法调用的具体工作, 因此我这里长话短说, 只列出我们我们关注的内如:\n\nprivate void select(boolean oldwakenup) throws ioexception {\n    selector selector = this.selector;\n    try {\n        ...\n        int selectedkeys = selector.select(timeoutmillis);\n        ...\n    } catch (cancelledkeyexception e) {\n        ...\n    }\n}\n\n\n在这个 select 方法中, 调用了 selector.select(timeoutmillis), 而这个调用是会阻塞住当前线程的, timeoutmillis 是阻塞的超时时间. 到来这里, 我们可以看到, 当 hastasks() 为真时, 调用的的 selectnow() 方法是不会阻塞当前线程的, 而当 hastasks() 为假时, 调用的 select(oldwakenup) 是会阻塞当前线程的. 这其实也很好理解: 当 taskqueue 中没有任务时, 那么 netty 可以阻塞地等待 io 就绪事件; 而当 taskqueue 中有任务时, 我们自然地希望所提交的任务可以尽快地执行, 因此 netty 会调用非阻塞的 selectnow() 方法, 以保证 taskqueue 中的任务尽快可以执行.\n\n\n# io 事件的处理\n\n在 nioeventloop.run() 方法中, 第一步是通过 select/selectnow 调用查询当前是否有就绪的 io 事件. 那么当有 io 事件就绪时, 第二步自然就是处理这些 io 事件啦. 首先让我们来看一下 nioeventloop.run 中循环的剩余部分:\n\nfinal int ioratio = this.ioratio;\nif (ioratio == 100) {\n    processselectedkeys();\n    runalltasks();\n} else {\n    final long iostarttime = system.nanotime();\n\n    processselectedkeys();\n\n    final long iotime = system.nanotime() - iostarttime;\n    runalltasks(iotime * (100 - ioratio) / ioratio);\n}\n\n\n上面列出的代码中, 有两个关键的调用, 第一个是 processselectedkeys() 调用, 根据字面意思, 我们可以猜出这个方法肯定是查询就绪的 io 事件, 然后处理它; 第二个调用是 runalltasks(), 这个方法我们也可以一眼就看出来它的功能就是运行 taskqueue 中的任务. 这里的代码还有一个十分有意思的地方, 即 ioratio. 那什么是 ioratio呢? 它表示的是此线程分配给 io 操作所占的时间比(即运行 processselectedkeys 耗时在整个循环中所占用的时间). 例如 ioratio 默认是 50, 则表示 io 操作和执行 task 的所占用的线程执行时间比是 1 : 1. 当知道了 io 操作耗时和它所占用的时间比, 那么执行 task 的时间就可以很方便的计算出来了:\n\n设 io 操作耗时为 iotime, iotime 占的时间比例为 ioratio, 则:\n\tiotime / ioratio = tasktime / taskratio\n\ttaskratio = 100 - ioratio\n\t=> tasktime = iotime * (100 - ioratio) / ioratio\n\n\n根据上面的公式, 当我们设置 iorate = 70 时, 则表示 io 运行耗时占比为70%, 即假设某次循环一共耗时为 100ms, 那么根据公式, 我们知道 processselectedkeys() 方法调用所耗时大概为70ms(即 io 耗时), 而 runalltasks() 耗时大概为 30ms(即执行 task 耗时). 当 ioratio 为 100 时, netty 就不考虑 io 耗时的占比, 而是分别调用 processselectedkeys()、runalltasks(); 而当 ioratio 不为 100时, 则执行到 else 分支, 在这个分支中, 首先记录下 processselectedkeys() 所执行的时间(即 io 操作的耗时), 然后根据公式, 计算出执行 task 所占用的时间, 然后以此为参数, 调用 runalltasks().\n\n我们这里先分析一下 processselectedkeys() 方法调用, runalltasks() 我们留到下一节再分析. processselectedkeys() 方法的源码如下:\n\nprivate void processselectedkeys() {\n    if (selectedkeys != null) {\n        processselectedkeysoptimized(selectedkeys.flip());\n    } else {\n        processselectedkeysplain(selector.selectedkeys());\n    }\n}\n\n\n这个方法中, 会根据 selectedkeys 字段是否为空, 而分别调用 processselectedkeysoptimized 或 processselectedkeysplain. selectedkeys 字段是在调用 openselector() 方法时, 根据 jvm 平台的不同, 而有设置不同的值, 在我所调试这个值是不为 null 的. 其实 processselectedkeysoptimized 方法 processselectedkeysplain 没有太大的区别, 为了简单起见, 我们以 processselectedkeysoptimized 为例分析一下源码的工作流程吧.\n\nprivate void processselectedkeysoptimized(selectionkey[] selectedkeys) {\n    for (int i = 0;; i ++) {\n        final selectionkey k = selectedkeys[i];\n        if (k == null) {\n            break;\n        }\n        selectedkeys[i] = null;\n\n        final object a = k.attachment();\n\n        if (a instanceof abstractniochannel) {\n            processselectedkey(k, (abstractniochannel) a);\n        } else {\n            @suppresswarnings("unchecked")\n            niotask<selectablechannel> task = (niotask<selectablechannel>) a;\n            processselectedkey(k, task);\n        }\n        ...\n    }\n}\n\n\n其实你别看它代码挺多的, 但是关键的点就两个: 迭代 selectedkeys 获取就绪的 io 事件, 然后为每个事件都调用 processselectedkey 来处理它. 这里正好完美对应上了我们提到的 selector 的使用流程中的第三步里操作. 还有一点需要注意的是, 我们可以调用 selectionkey.attach(object) 给一个 selectionkey 设置一个附加的字段, 然后可以通过 object attachedobj = selectionkey.attachment() 获取它. 上面代代码正是通过了 k.attachment() 来获取一个附加在 selectionkey 中的对象, 那么这个对象是什么呢? 它又是在哪里设置的呢? 我们再来回忆一下 socketchannel 是如何注册到 selector 中的: 在客户端的 channel 注册过程中, 会有如下调用链:\n\nbootstrap.initandregister -> \n\tabstractbootstrap.initandregister -> \n\t\tmultithreadeventloopgroup.register -> \n\t\t\tsinglethreadeventloop.register -> \n\t\t\t\tabstractunsafe.register ->\n\t\t\t\t\tabstractunsafe.register0 ->\n\t\t\t\t\t\tabstractniochannel.doregister\n\n\n最后的 abstractniochannel.doregister 方法会调用 socketchannel.register 方法注册一个 socketchannel 到指定的 selector:\n\n@override\nprotected void doregister() throws exception {\n\t// 省略错误处理\n    selectionkey = javachannel().register(eventloop().selector, 0, this);\n}\n\n\n特别注意一下 register 的第三个参数, 这个参数是设置 selectionkey 的附加对象的, 和调用 selectionkey.attach(object) 的效果一样. 而调用 register 所传递的第三个参数是 this, 它其实就是一个 niosocketchannel 的实例. 那么这里就很清楚了, 我们在将 socketchannel 注册到 selector 中时, 将 socketchannel 所对应的 niosocketchannel 以附加字段的方式添加到了selectionkey 中. 再回到 processselectedkeysoptimized 方法中, 当我们获取到附加的对象后, 我们就调用 processselectedkey 来处理这个 io 事件:\n\nfinal object a = k.attachment();\n\nif (a instanceof abstractniochannel) {\n    processselectedkey(k, (abstractniochannel) a);\n} else {\n    @suppresswarnings("unchecked")\n    niotask<selectablechannel> task = (niotask<selectablechannel>) a;\n    processselectedkey(k, task);\n}\n\n\nprocessselectedkey 方法源码如下:\n\nprivate static void processselectedkey(selectionkey k, abstractniochannel ch) {\n    final niounsafe unsafe = ch.unsafe();\n    ...\n    try {\n        int readyops = k.readyops();\n        \n        // 可读事件\n        if ((readyops & (selectionkey.op_read | selectionkey.op_accept)) != 0 || readyops == 0) {\n            unsafe.read();\n            if (!ch.isopen()) {\n                // connection already closed - no need to handle write.\n                return;\n            }\n        }\n\t\t\n\t\t// 可写事件\n        if ((readyops & selectionkey.op_write) != 0) {\n            // call forceflush which will also take care of clear the op_write once there is nothing left to write\n            ch.unsafe().forceflush();\n        }\n        \n        // 连接建立事件\n        if ((readyops & selectionkey.op_connect) != 0) {\n            // remove op_connect as otherwise selector.select(..) will always return without blocking\n            // see https://github.com/netty/netty/issues/924\n            int ops = k.interestops();\n            ops &= ~selectionkey.op_connect;\n            k.interestops(ops);\n\n            unsafe.finishconnect();\n        }\n    } catch (cancelledkeyexception ignored) {\n        unsafe.close(unsafe.voidpromise());\n    }\n}\n\n\n这个代码是不是很熟悉啊? 完全是 java nio 的 selector 的那一套处理流程嘛! processselectedkey 中处理了三个事件, 分别是:\n\n * op_read, 可读事件, 即 channel 中收到了新数据可供上层读取.\n * op_write, 可写事件, 即上层可以向 channel 写入数据.\n * op_connect, 连接建立事件, 即 tcp 连接已经建立, channel 处于 active 状态.\n\n下面我们分别根据这三个事件来看一下 netty 是怎么处理的吧.\n\n# op_read 处理\n\n当就绪的 io 事件是 op_read, 代码会调用 unsafe.read() 方法, 即:\n\n// 可读事件\nif ((readyops & (selectionkey.op_read | selectionkey.op_accept)) != 0 || readyops == 0) {\n    unsafe.read();\n    if (!ch.isopen()) {\n        // connection already closed - no need to handle write.\n        return;\n    }\n}\n\n\nunsafe 这个字段, 我们已经和它打了太多的交道了, 在第一章 netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端) 中我们已经对它进行过浓墨重彩地分析了, 最后我们确定了它是一个 niosocketchannelunsafe 实例, 负责的是 channel 的底层 io 操作. 我们可以利用 intellij idea 提供的 go to implementations 功能, 寻找到这个方法的实现. 最后我们发现这个方法没有在 niosocketchannelunsafe 中实现, 而是在它的父类 abstractniobytechannel 实现的, 它的实现源码如下:\n\n@override\npublic final void read() {\n    ...\n    bytebuf bytebuf = null;\n    int messages = 0;\n    boolean close = false;\n    try {\n        int totalreadamount = 0;\n        boolean readpendingreset = false;\n        do {\n            bytebuf = allochandle.allocate(allocator);\n            int writable = bytebuf.writablebytes();\n            int localreadamount = doreadbytes(bytebuf);\n\n            // 检查读取结果.\n            ...\n\n            pipeline.firechannelread(bytebuf);\n            bytebuf = null;\n\n            ...\n\n            totalreadamount += localreadamount;\n        \n            // 检查是否是配置了自动读取, 如果不是, 则立即退出循环.\n            ...\n        } while (++ messages < maxmessagesperread);\n\n        pipeline.firechannelreadcomplete();\n        allochandle.record(totalreadamount);\n\n        if (close) {\n            closeonread(pipeline);\n            close = false;\n        }\n    } catch (throwable t) {\n        handlereadexception(pipeline, bytebuf, t, close);\n    } finally {\n    }\n}\n\n\nread() 源码比较长, 我为了篇幅起见, 删除了部分代码, 只留下了主干. 不过我建议读者朋友们自己一定要看一下 read() 源码, 这对理解 netty 的 eventloop 十分有帮助. 上面 read 方法其实归纳起来, 可以认为做了如下工作:\n\n 1. 分配 bytebuf\n 2. 从 socketchannel 中读取数据\n 3. 调用 pipeline.firechannelread 发送一个 inbound 事件.\n\n前面两点没什么好说的, 第三点 pipeline.firechannelread 读者朋友们看到了有没有会心一笑地感觉呢? 反正我看到这里时是有的. pipeline.firechannelread 正好就是我们在第二章 netty 源码分析之 二 贯穿netty 的大动脉 ── channelpipeline (二) 中分析的 inbound 事件起点. 当调用了 pipeline.firein_evt() 后, 那么就产生了一个 inbound 事件, 此事件会以 head -> customcontext -> tail 的方向依次流经 channelpipeline 中的各个 handler. 调用了 pipeline.firechannelread 后, 就是 channelpipeline 中所需要做的工作了, 这些我们已经在第二章中有过详细讨论, 这里就展开了.\n\n# op_write 处理\n\nop_write 可写事件代码如下. 这里代码比较简单, 没有详细分析的必要了.\n\nif ((readyops & selectionkey.op_write) != 0) {\n    // call forceflush which will also take care of clear the op_write once there is nothing left to write\n    ch.unsafe().forceflush();\n}\n\n\n# op_connect 处理\n\n最后一个事件是 op_connect, 即 tcp 连接已建立事件.\n\nif ((readyops & selectionkey.op_connect) != 0) {\n    // remove op_connect as otherwise selector.select(..) will always return without blocking\n    // see https://github.com/netty/netty/issues/924\n    int ops = k.interestops();\n    ops &= ~selectionkey.op_connect;\n    k.interestops(ops);\n\n    unsafe.finishconnect();\n}\n\n\nop_connect 事件的处理中, 只做了两件事情:\n\n 1. 正如代码中的注释所言, 我们需要将 op_connect 从就绪事件集中清除, 不然会一直有 op_connect 事件.\n 2. 调用 unsafe.finishconnect() 通知上层连接已建立\n\nunsafe.finishconnect() 调用最后会调用到 pipeline().firechannelactive(), 产生一个 inbound 事件, 通知 pipeline 中的各个 handler tcp 通道已建立(即 channelinboundhandler.channelactive 方法会被调用)\n\n到了这里, 我们整个 nioeventloop 的 io 操作部分已经了解完了, 接下来的一节我们要重点分析一下 netty 的任务队列机制.\n\n\n# netty 的任务队列机制\n\n我们已经提到过, 在netty 中, 一个 nioeventloop 通常需要肩负起两种任务, 第一个是作为 io 线程, 处理 io 操作; 第二个就是作为任务线程, 处理 taskqueue 中的任务. 这一节的重点就是分析一下 nioeventloop 的任务队列机制的.\n\n\n# task 的添加\n\n# 普通 runnable 任务\n\nnioeventloop 继承于 singlethreadeventexecutor, 而 singlethreadeventexecutor 中有一个 queue taskqueue 字段, 用于存放添加的 task. 在 netty 中, 每个 task 都使用一个实现了 runnable 接口的实例来表示. 例如当我们需要将一个 runnable 添加到 taskqueue 中时, 我们可以进行如下操作:\n\neventloop eventloop = channel.eventloop();\neventloop.execute(new runnable() {\n    @override\n    public void run() {\n        system.out.println("hello, netty!");\n    }\n});\n\n\n当调用 execute 后, 实际上是调用到了 singlethreadeventexecutor.execute() 方法, 它的实现如下:\n\n@override\npublic void execute(runnable task) {\n    if (task == null) {\n        throw new nullpointerexception("task");\n    }\n\n    boolean ineventloop = ineventloop();\n    if (ineventloop) {\n        addtask(task);\n    } else {\n        startthread();\n        addtask(task);\n        if (isshutdown() && removetask(task)) {\n            reject();\n        }\n    }\n\n    if (!addtaskwakesup && wakesupfortask(task)) {\n        wakeup(ineventloop);\n    }\n}\n\n\n而添加任务的 addtask 方法的源码如下:\n\nprotected void addtask(runnable task) {\n    if (task == null) {\n        throw new nullpointerexception("task");\n    }\n    if (isshutdown()) {\n        reject();\n    }\n    taskqueue.add(task);\n}\n\n\n因此实际上, taskqueue 是存放着待执行的任务的队列.\n\n# schedule 任务\n\n除了通过 execute 添加普通的 runnable 任务外, 我们还可以通过调用 eventloop.schedulexxx 之类的方法来添加一个定时任务. eventloop 中实现任务队列的功能在超类 singlethreadeventexecutor 实现的, 而 schedule 功能的实现是在 singlethreadeventexecutor 的父类, 即 abstractscheduledeventexecutor 中实现的. 在 abstractscheduledeventexecutor 中, 有以 scheduledtaskqueue 字段:\n\nqueue<scheduledfuturetask<?>> scheduledtaskqueue;\n\n\nscheduledtaskqueue 是一个队列(queue), 其中存放的元素是 scheduledfuturetask. 而 scheduledfuturetask 我们很容易猜到, 它是对 schedule 任务的一个抽象. 我们来看一下 abstractscheduledeventexecutor 所实现的 schedule 方法吧:\n\n@override\npublic  scheduledfuture<?> schedule(runnable command, long delay, timeunit unit) {\n    objectutil.checknotnull(command, "command");\n    objectutil.checknotnull(unit, "unit");\n    if (delay < 0) {\n        throw new illegalargumentexception(\n                string.format("delay: %d (expected: >= 0)", delay));\n    }\n    return schedule(new scheduledfuturetask<void>(\n            this, command, null, scheduledfuturetask.deadlinenanos(unit.tonanos(delay))));\n}\n\n\n这是其中一个重载的 schedule, 当一个 runnable 传递进来后, 会被封装为一个 scheduledfuturetask 对象, 这个对象会记录下这个 runnable 在何时运行、已何种频率运行等信息. 当构建了 scheduledfuturetask 后, 会继续调用 另一个重载的 schedule 方法:\n\n<v> scheduledfuture<v> schedule(final scheduledfuturetask<v> task) {\n    if (ineventloop()) {\n        scheduledtaskqueue().add(task);\n    } else {\n        execute(new onetimetask() {\n            @override\n            public void run() {\n                scheduledtaskqueue().add(task);\n            }\n        });\n    }\n\n    return task;\n}\n\n\n在这个方法中, scheduledfuturetask 对象就会被添加到 scheduledtaskqueue 中了\n\n\n# 任务的执行\n\n当一个任务被添加到 taskqueue 后, 它是怎么被 eventloop 执行的呢? 让我们回到 nioeventloop.run() 方法中, 在这个方法里, 会分别调用 processselectedkeys() 和 runalltasks() 方法, 来进行 io 事件的处理和 task 的处理. processselectedkeys() 方法我们已经分析过了, 下面我们来看一下 runalltasks() 中到底有什么名堂吧. runalltasks 方法有两个重载的方法, 一个是无参数的, 另一个有一个参数的. 首先来看一下无参数的 runalltasks:\n\nprotected boolean runalltasks() {\n    fetchfromscheduledtaskqueue();\n    runnable task = polltask();\n    if (task == null) {\n        return false;\n    }\n\n    for (;;) {\n        try {\n            task.run();\n        } catch (throwable t) {\n            logger.warn("a task raised an exception.", t);\n        }\n\n        task = polltask();\n        if (task == null) {\n            lastexecutiontime = scheduledfuturetask.nanotime();\n            return true;\n        }\n    }\n}\n\n\n我们前面已经提到过, eventloop 可以通过调用 eventloop.execute 来将一个 runnable 提交到 taskqueue 中, 也可以通过调用 eventloop.schedule 来提交一个 schedule 任务到 scheduledtaskqueue 中. 在此方法的一开始调用的 fetchfromscheduledtaskqueue() 其实就是将 scheduledtaskqueue 中已经可以执行的(即定时时间已到的 schedule 任务) 拿出来并添加到 taskqueue 中, 作为可执行的 task 等待被调度执行. 它的源码如下:\n\nprivate void fetchfromscheduledtaskqueue() {\n    if (hasscheduledtasks()) {\n        long nanotime = abstractscheduledeventexecutor.nanotime();\n        for (;;) {\n            runnable scheduledtask = pollscheduledtask(nanotime);\n            if (scheduledtask == null) {\n                break;\n            }\n            taskqueue.add(scheduledtask);\n        }\n    }\n}\n\n\n接下来 runalltasks() 方法就会不断调用 task = polltask() 从 taskqueue 中获取一个可执行的 task, 然后调用它的 run() 方法来运行此 task.\n\n> 注意, 因为 eventloop 既需要执行 io 操作, 又需要执行 task, 因此我们在调用 eventloop.execute 方法提交任务时, 不要提交耗时任务, 更不能提交一些会造成阻塞的任务, 不然会导致我们的 io 线程得不到调度, 影响整个程序的并发量.\n\n\n# 总结\n\n\n# 参考资料',charsets:{cjk:!0},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"ChannelHandler 源码解析",frontmatter:{title:"ChannelHandler 源码解析",date:"2024-09-18T21:11:52.000Z",permalink:"/pages/ce1f78/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/20.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/20.ChannelHandler%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html",relativePath:"Netty 系统设计/20.三、主线任务/20.ChannelHandler 源码解析.md",key:"v-190cf2c0",path:"/pages/ce1f78/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"ChannelPipeline 源码解析",frontmatter:{title:"ChannelPipeline 源码解析",date:"2024-09-18T21:12:21.000Z",permalink:"/pages/4234c0/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/20.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/30.ChannelPipeline%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html",relativePath:"Netty 系统设计/20.三、主线任务/30.ChannelPipeline 源码解析.md",key:"v-6a692486",path:"/pages/4234c0/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"Channel 与 ChannelPipeline",slug:"channel-与-channelpipeline",normalizedTitle:"channel 与 channelpipeline",charIndex:79},{level:2,title:"ChannelPipeline 的初始化再探",slug:"channelpipeline-的初始化再探",normalizedTitle:"channelpipeline 的初始化再探",charIndex:1559},{level:3,title:"ChannelPipeline 实例化过程",slug:"channelpipeline-实例化过程",normalizedTitle:"channelpipeline 实例化过程",charIndex:1727},{level:3,title:"链表的节点 ChannelHandlerContext",slug:"链表的节点-channelhandlercontext",normalizedTitle:"链表的节点 channelhandlercontext",charIndex:3435},{level:3,title:"ChannelInitializer 的添加",slug:"channelinitializer-的添加",normalizedTitle:"channelinitializer 的添加",charIndex:3467},{level:2,title:"自定义 ChannelHandler 的添加过程",slug:"自定义-channelhandler-的添加过程",normalizedTitle:"自定义 channelhandler 的添加过程",charIndex:7886},{level:2,title:"ChannelHandler 的名字",slug:"channelhandler-的名字",normalizedTitle:"channelhandler 的名字",charIndex:5385},{level:3,title:"自动生成 handler 的名字",slug:"自动生成-handler-的名字",normalizedTitle:"自动生成 handler 的名字",charIndex:13490},{level:2,title:"关于 Pipeline 的事件传输机制",slug:"关于-pipeline-的事件传输机制",normalizedTitle:"关于 pipeline 的事件传输机制",charIndex:14994},{level:3,title:"Outbound 操作",slug:"outbound-操作",normalizedTitle:"outbound 操作",charIndex:19837},{level:3,title:"Inbound 事件",slug:"inbound-事件",normalizedTitle:"inbound 事件",charIndex:18389},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:26577},{level:2,title:"总结",slug:"总结-2",normalizedTitle:"总结",charIndex:26577},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:27644}],headersStr:"前言 Channel 与 ChannelPipeline ChannelPipeline 的初始化再探 ChannelPipeline 实例化过程 链表的节点 ChannelHandlerContext ChannelInitializer 的添加 自定义 ChannelHandler 的添加过程 ChannelHandler 的名字 自动生成 handler 的名字 关于 Pipeline 的事件传输机制 Outbound 操作 Inbound 事件 总结 总结 参考资料",content:'# 前言\n\n这篇是 Netty 源码分析 的第二篇, 在这篇文章中, 我会为读者详细地分析 Netty 中的 ChannelPipeline 机制.\n\n\n# Channel 与 ChannelPipeline\n\n相信大家都知道了, 在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应, 它们的组成关系如下:\n\n\n\n通过上图我们可以看到, 一个 Channel 包含了一个 ChannelPipeline, 而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表. 这个链表的头是 HeadContext, 链表的尾是 TailContext, 并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler. 上面的图示给了我们一个对 ChannelPipeline 的直观认识, 但是实际上 Netty 实现的 Channel 是否真的是这样的呢? 我们继续用源码说话.\n\n在第一章 Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 中, 我们已经知道了一个 Channel 的初始化的基本过程, 下面我们再回顾一下. 下面的代码是 AbstractChannel 构造器:\n\nprotected AbstractChannel(Channel parent) {\n    this.parent = parent;\n    unsafe = newUnsafe();\n    pipeline = new DefaultChannelPipeline(this);\n}\n\n\nAbstractChannel 有一个 pipeline 字段, 在构造器中会初始化它为 DefaultChannelPipeline的实例. 这里的代码就印证了一点: 每个 Channel 都有一个 ChannelPipeline. 接着我们跟踪一下 DefaultChannelPipeline 的初始化过程. 首先进入到 DefaultChannelPipeline 构造器中:\n\npublic DefaultChannelPipeline(AbstractChannel channel) {\n    if (channel == null) {\n        throw new NullPointerException("channel");\n    }\n    this.channel = channel;\n\n    tail = new TailContext(this);\n    head = new HeadContext(this);\n\n    head.next = tail;\n    tail.prev = head;\n}\n\n\n在 DefaultChannelPipeline 构造器中, 首先将与之关联的 Channel 保存到字段 channel 中, 然后实例化两个 ChannelHandlerContext, 一个是 HeadContext 实例 head, 另一个是 TailContext 实例 tail. 接着将 head 和 tail 互相指向, 构成一个双向链表. 特别注意到, 我们在开始的示意图中, head 和 tail 并没有包含 ChannelHandler, 这是因为 HeadContext 和 TailContext 继承于 AbstractChannelHandlerContext 的同时也实现了 ChannelHandler 接口了, 因此它们有 Context 和 Handler 的双重属性.\n\n\n# ChannelPipeline 的初始化再探\n\n在第一章的时候, 我们已经对 ChannelPipeline 的初始化有了一个大致的了解, 不过当时重点毕竟不在 ChannelPipeline 这里, 因此没有深入地分析它的初始化过程. 那么下面我们就来看一下具体的 ChannelPipeline 的初始化都做了哪些工作吧.\n\n\n# ChannelPipeline 实例化过程\n\n我们再来回顾一下, 在实例化一个 Channel 时, 会伴随着一个 ChannelPipeline 的实例化, 并且此 Channel 会与这个 ChannelPipeline 相互关联, 这一点可以通过NioSocketChannel 的父类 AbstractChannel 的构造器予以佐证:\n\nprotected AbstractChannel(Channel parent) {\n    this.parent = parent;\n    unsafe = newUnsafe();\n    pipeline = new DefaultChannelPipeline(this);\n}\n\n\n当实例化一个 Channel(这里以 EchoClient 为例, 那么 Channel 就是 NioSocketChannel), 其 pipeline 字段就是我们新创建的 DefaultChannelPipeline 对象, 那么我们就来看一下 DefaultChannelPipeline 的构造方法吧:\n\npublic DefaultChannelPipeline(AbstractChannel channel) {\n    if (channel == null) {\n        throw new NullPointerException("channel");\n    }\n    this.channel = channel;\n\n    tail = new TailContext(this);\n    head = new HeadContext(this);\n\n    head.next = tail;\n    tail.prev = head;\n}\n\n\n可以看到, 在 DefaultChannelPipeline 的构造方法中, 将传入的 channel 赋值给字段 this.channel, 接着又实例化了两个特殊的字段: tail 与 head. 这两个字段是一个双向链表的头和尾. 其实在 DefaultChannelPipeline 中, 维护了一个以 AbstractChannelHandlerContext 为节点的双向链表, 这个链表是 Netty 实现 Pipeline 机制的关键. 再回顾一下 head 和 tail 的类层次结构:\n\n\n\n\n\n从类层次结构图中可以很清楚地看到, head 实现了 ChannelInboundHandler, 而 tail 实现了 ChannelOutboundHandler 接口, 并且它们都实现了 ChannelHandlerContext 接口, 因此可以说 head 和 tail 既是一个 ChannelHandler, 又是一个 ChannelHandlerContext.\n\n接着看一下 HeadContext 的构造器:\n\nHeadContext(DefaultChannelPipeline pipeline) {\n    super(pipeline, null, HEAD_NAME, false, true);\n    unsafe = pipeline.channel().unsafe();\n}\n\n\n它调用了父类 AbstractChannelHandlerContext 的构造器, 并传入参数 inbound = false, outbound = true. TailContext 的构造器与 HeadContext 的相反, 它调用了父类 AbstractChannelHandlerContext 的构造器, 并传入参数 inbound = true, outbound = false. 即 header 是一个 outboundHandler, 而 tail 是一个inboundHandler, 关于这一点, 大家要特别注意, 因为在后面的分析中, 我们会反复用到 inbound 和 outbound 这两个属性.\n\n\n# 链表的节点 ChannelHandlerContext\n\n\n# ChannelInitializer 的添加\n\n上面一小节中, 我们已经分析了 Channel 的组成, 其中我们了解到, 最开始的时候 ChannelPipeline 中含有两个 ChannelHandlerContext(同时也是 ChannelHandler), 但是这个 Pipeline并不能实现什么特殊的功能, 因为我们还没有给它添加自定义的 ChannelHandler. 通常来说, 我们在初始化 Bootstrap, 会添加我们自定义的 ChannelHandler, 就以我们熟悉的 EchoClient 来举例吧:\n\nBootstrap b = new Bootstrap();\nb.group(group)\n .channel(NioSocketChannel.class)\n .option(ChannelOption.TCP_NODELAY, true)\n .handler(new ChannelInitializer<SocketChannel>() {\n     @Override\n     public void initChannel(SocketChannel ch) throws Exception {\n         ChannelPipeline p = ch.pipeline();\n         p.addLast(new EchoClientHandler());\n     }\n });\n\n\n上面代码的初始化过程, 相信大家都不陌生. 在调用 handler 时, 传入了 ChannelInitializer 对象, 它提供了一个 initChannel 方法供我们初始化 ChannelHandler. 那么这个初始化过程是怎样的呢? 下面我们就来揭开它的神秘面纱.\n\nChannelInitializer 实现了 ChannelHandler, 那么它是在什么时候添加到 ChannelPipeline 中的呢? 进行了一番搜索后, 我们发现它是在 Bootstrap.init 方法中添加到 ChannelPipeline 中的. 其代码如下:\n\n@Override\n@SuppressWarnings("unchecked")\nvoid init(Channel channel) throws Exception {\n    ChannelPipeline p = channel.pipeline();\n    p.addLast(handler());\n    ...\n}\n\n\n上面的代码将 handler() 返回的 ChannelHandler 添加到 Pipeline 中, 而 handler() 返回的是handler 其实就是我们在初始化 Bootstrap 调用 handler 设置的 ChannelInitializer 实例, 因此这里就是将 ChannelInitializer 插入到了 Pipeline 的末端. 此时 Pipeline 的结构如下图所示:\n\n\n\n有朋友可能就有疑惑了, 我明明插入的是一个 ChannelInitializer 实例, 为什么在 ChannelPipeline 中的双向链表中的元素却是一个 ChannelHandlerContext? 为了解答这个问题, 我们继续在代码中寻找答案吧. 我们刚才提到, 在 Bootstrap.init 中会调用 p.addLast() 方法, 将 ChannelInitializer 插入到链表末端:\n\n@Override\npublic ChannelPipeline addLast(EventExecutorGroup group, final String name, ChannelHandler handler) {\n    synchronized (this) {\n        checkDuplicateName(name); // 检查此 handler 是否有重复的名字\n\n        AbstractChannelHandlerContext newCtx = new DefaultChannelHandlerContext(this, group, name, handler);\n        addLast0(name, newCtx);\n    }\n\n    return this;\n}\n\n\naddLast 有很多重载的方法, 我们关注这个比较重要的方法就可以了. 上面的 addLast 方法中, 首先检查这个 ChannelHandler 的名字是否是重复的, 如果不重复的话, 则为这个 Handler 创建一个对应的 DefaultChannelHandlerContext 实例, 并与之关联起来(Context 中有一个 handler 属性保存着对应的 Handler 实例). 判断此 Handler 是否重名的方法很简单: Netty 中有一个 name2ctx Map 字段, key 是 handler 的名字, 而 value 则是 handler 本身. 因此通过如下代码就可以判断一个 handler 是否重名了:\n\nprivate void checkDuplicateName(String name) {\n    if (name2ctx.containsKey(name)) {\n        throw new IllegalArgumentException("Duplicate handler name: " + name);\n    }\n}\n\n\n为了添加一个 handler 到 pipeline 中, 必须把此 handler 包装成 ChannelHandlerContext. 因此在上面的代码中我们可以看到新实例化了一个 newCtx 对象, 并将 handler 作为参数传递到构造方法中. 那么我们来看一下实例化的 DefaultChannelHandlerContext 到底有什么玄机吧. 首先看它的构造器:\n\nDefaultChannelHandlerContext(\n        DefaultChannelPipeline pipeline, EventExecutorGroup group, String name, ChannelHandler handler) {\n    super(pipeline, group, name, isInbound(handler), isOutbound(handler));\n    if (handler == null) {\n        throw new NullPointerException("handler");\n    }\n    this.handler = handler;\n}\n\n\nDefaultChannelHandlerContext 的构造器中, 调用了两个很有意思的方法: isInbound 与 isOutbound, 这两个方法是做什么的呢?\n\nprivate static boolean isInbound(ChannelHandler handler) {\n    return handler instanceof ChannelInboundHandler;\n}\n\nprivate static boolean isOutbound(ChannelHandler handler) {\n    return handler instanceof ChannelOutboundHandler;\n}\n\n\n从源码中可以看到, 当一个 handler 实现了 ChannelInboundHandler 接口, 则 isInbound 返回真; 相似地, 当一个 handler 实现了 ChannelOutboundHandler 接口, 则 isOutbound 就返回真. 而这两个 boolean 变量会传递到父类 AbstractChannelHandlerContext 中, 并初始化父类的两个字段: inbound 与 outbound. 那么这里的 ChannelInitializer 所对应的 DefaultChannelHandlerContext 的 inbound 与 inbound 字段分别是什么呢? 那就看一下 ChannelInitializer 到底实现了哪个接口不就行了? 如下是 ChannelInitializer 的类层次结构图：\n\n\n\n可以清楚地看到, ChannelInitializer 仅仅实现了 ChannelInboundHandler 接口, 因此这里实例化的 DefaultChannelHandlerContext 的 inbound = true, outbound = false. 不就是 inbound 和 outbound 两个字段嘛, 为什么需要这么大费周章地分析一番? 其实这两个字段关系到 pipeline 的事件的流向与分类, 因此是十分关键的, 不过我在这里先卖个关子, 后面我们再来详细分析这两个字段所起的作用. 在这里, 读者只需要记住, ChannelInitializer 所对应的 DefaultChannelHandlerContext 的 inbound = true, outbound = false 即可.\n\n当创建好 Context 后, 就将这个 Context 插入到 Pipeline 的双向链表中:\n\nprivate void addLast0(final String name, AbstractChannelHandlerContext newCtx) {\n    checkMultiplicity(newCtx);\n\n    AbstractChannelHandlerContext prev = tail.prev;\n    newCtx.prev = prev;\n    newCtx.next = tail;\n    prev.next = newCtx;\n    tail.prev = newCtx;\n\n    name2ctx.put(name, newCtx);\n\n    callHandlerAdded(newCtx);\n}\n\n\n显然, 这个代码就是典型的双向链表的插入操作了. 当调用了 addLast 方法后, Netty 就会将此 handler 添加到双向链表中 tail 元素之前的位置.\n\n\n# 自定义 ChannelHandler 的添加过程\n\n在上一小节中, 我们已经分析了一个 ChannelInitializer 如何插入到 Pipeline 中的, 接下来就来探讨一下 ChannelInitializer 在哪里被调用, ChannelInitializer 的作用, 以及我们自定义的 ChannelHandler 是如何插入到 Pipeline 中的.\n\n在 Netty 源码分析之 一 揭开 Bootstrap 神秘的红盖头 (客户端) 一章的 channel 的注册过程 小节中, 我们已经分析过 Channel 的注册过程了, 这里我们再简单地复习一下:\n\n * 首先在 AbstractBootstrap.initAndRegister中, 通过 group().register(channel), 调用 MultithreadEventLoopGroup.register 方法\n * 在MultithreadEventLoopGroup.register 中, 通过 next() 获取一个可用的 SingleThreadEventLoop, 然后调用它的 register\n * 在 SingleThreadEventLoop.register 中, 通过 channel.unsafe().register(this, promise) 来获取 channel 的 unsafe() 底层操作对象, 然后调用它的 register.\n * 在 AbstractUnsafe.register 方法中, 调用 register0 方法注册 Channel\n * 在 AbstractUnsafe.register0 中, 调用 AbstractNioChannel#doRegister 方法\n * AbstractNioChannel.doRegister 方法通过 javaChannel().register(eventLoop().selector, 0, this) 将 Channel 对应的 Java NIO SockerChannel 注册到一个 eventLoop 的 Selector 中, 并且将当前 Channel 作为 attachment.\n\n而我们自定义 ChannelHandler 的添加过程, 发生在 AbstractUnsafe.register0 中, 在这个方法中调用了 pipeline.fireChannelRegistered() 方法, 其实现如下:\n\n@Override\npublic ChannelPipeline fireChannelRegistered() {\n    head.fireChannelRegistered();\n    return this;\n}\n\n\n上面的代码很简单, 就是调用了 head.fireChannelRegistered() 方法而已.\n\n> 关于上面代码的 head.fireXXX 的调用形式, 是 Netty 中 Pipeline 传递事件的常用方式, 我们以后会经常看到.\n\n还记得 head 的 类层次结构图不, head 是一个 AbstractChannelHandlerContext 实例, 并且它没有重写 fireChannelRegistered 方法, 因此 head.fireChannelRegistered 其实是调用的 AbstractChannelHandlerContext.fireChannelRegistered:\n\n@Override\npublic ChannelHandlerContext fireChannelRegistered() {\n    final AbstractChannelHandlerContext next = findContextInbound();\n    EventExecutor executor = next.executor();\n    if (executor.inEventLoop()) {\n        next.invokeChannelRegistered();\n    } else {\n        executor.execute(new OneTimeTask() {\n            @Override\n            public void run() {\n                next.invokeChannelRegistered();\n            }\n        });\n    }\n    return this;\n}\n\n\n这个方法的第一句是调用 findContextInbound 获取一个 Context, 那么它返回的 Context 到底是什么呢? 我们跟进代码中看一下:\n\nprivate AbstractChannelHandlerContext findContextInbound() {\n    AbstractChannelHandlerContext ctx = this;\n    do {\n        ctx = ctx.next;\n    } while (!ctx.inbound);\n    return ctx;\n}\n\n\n很显然, 这个代码会从 head 开始遍历 Pipeline 的双向链表, 然后找到第一个属性 inbound 为 true 的 ChannelHandlerContext 实例. 想起来了没? 我们在前面分析 ChannelInitializer 时, 花了大量的笔墨来分析了 inbound 和 outbound 属性, 你看现在这里就用上了. 回想一下, ChannelInitializer 实现了 ChannelInboudHandler, 因此它所对应的 ChannelHandlerContext 的 inbound 属性就是 true, 因此这里返回就是 ChannelInitializer 实例所对应的 ChannelHandlerContext. 即:\n\n\n\n当获取到 inbound 的 Context 后, 就调用它的 invokeChannelRegistered 方法:\n\nprivate void invokeChannelRegistered() {\n    try {\n        ((ChannelInboundHandler) handler()).channelRegistered(this);\n    } catch (Throwable t) {\n        notifyHandlerException(t);\n    }\n}\n\n\n我们已经强调过了, 每个 ChannelHandler 都与一个 ChannelHandlerContext 关联, 我们可以通过 ChannelHandlerContext 获取到对应的 ChannelHandler. 因此很显然了, 这里 handler() 返回的, 其实就是一开始我们实例化的 ChannelInitializer 对象, 并接着调用了 ChannelInitializer.channelRegistered 方法. 看到这里, 读者朋友是否会觉得有点眼熟呢？ChannelInitializer.channelRegistered 这个方法我们在第一章的时候已经大量地接触了, 但是我们并没有深入地分析这个方法的调用过程, 那么在这里读者朋友应该对它的调用有了更加深入的了解了吧\n\n那么这个方法中又有什么玄机呢? 继续看代码:\n\n@Override\n@SuppressWarnings("unchecked")\npublic final void channelRegistered(ChannelHandlerContext ctx) throws Exception {\n    initChannel((C) ctx.channel());\n    ctx.pipeline().remove(this);\n    ctx.fireChannelRegistered();\n}\n\n\ninitChannel 这个方法我们很熟悉了吧, 它就是我们在初始化 Bootstrap 时, 调用 handler 方法传入的匿名内部类所实现的方法:\n\nbootstrap.handler(new ChannelInitializer<SocketChannel>() {\n     @Override\n     public void initChannel(SocketChannel ch) throws Exception {\n         ChannelPipeline p = ch.pipeline();\n         p.addLast(new EchoClientHandler());\n     }\n });\n\n\n因此当调用了这个方法后, 我们自定义的 ChannelHandler 就插入到 Pipeline 了, 此时的 Pipeline 如下图所示:\n\n\n\n当添加了自定义的 ChannelHandler 后, 会删除 ChannelInitializer 这个 ChannelHandler, 即 "ctx.pipeline().remove(this)", 因此最后的 Pipeline 如下:\n\n\n\n好了, 到了这里, 我们的 自定义 ChannelHandler 的添加过程 也分析的查不多了.\n\n\n# ChannelHandler 的名字\n\n我们注意到, pipeline.addXXX 都有一个重载的方法, 例如 addLast, 它有一个重载的版本是:\n\nChannelPipeline addLast(String name, ChannelHandler handler);\n\n\n第一个参数指定了所添加的 handler 的名字(更准确地说是 ChannelHandlerContext 的名字, 不过我们通常是以 handler 作为叙述的对象, 因此说成 handler 的名字便于理解). 那么 handler 的名字有什么用呢? 如果我们不设置name, 那么 handler 会有怎样的名字? 为了解答这些疑惑, 老规矩, 依然是从源码中找到答案. 我们还是以 addLast 方法为例:\n\n@Override\npublic ChannelPipeline addLast(String name, ChannelHandler handler) {\n    return addLast(null, name, handler);\n}\n\n\n这个方法会调用重载的 addLast 方法:\n\n@Override\npublic ChannelPipeline addLast(EventExecutorGroup group, final String name, ChannelHandler handler) {\n    synchronized (this) {\n        checkDuplicateName(name);\n\n        AbstractChannelHandlerContext newCtx = new DefaultChannelHandlerContext(this, group, name, handler);\n        addLast0(name, newCtx);\n    }\n\n    return this;\n}\n\n\n第一个参数被设置为 null, 我们不关系它. 第二参数就是这个 handler 的名字. 看代码可知, 在添加一个 handler 之前, 需要调用 checkDuplicateName 方法来确定此 handler 的名字是否和已添加的 handler 的名字重复. 而这个 checkDuplicateName 方法我们在前面已经有提到, 这里再回顾一下:\n\nprivate void checkDuplicateName(String name) {\n    if (name2ctx.containsKey(name)) {\n        throw new IllegalArgumentException("Duplicate handler name: " + name);\n    }\n}\n\n\nNetty 判断一个 handler 的名字是否重复的依据很简单: DefaultChannelPipeline 中有一个 类型为 Map<String, AbstractChannelHandlerContext> 的 name2ctx 字段, 它的 key 是一个 handler 的名字, 而 value 则是这个 handler 所对应的 ChannelHandlerContext. 每当新添加一个 handler 时, 就会 put 到 name2ctx 中. 因此检查 name2ctx 中是否包含这个 name 即可. 当没有重名的 handler 时, 就为这个 handler 生成一个关联的 DefaultChannelHandlerContext 对象, 然后就将 name 和 newCtx 作为 key-value 对 放到 name2Ctx 中.\n\n\n# 自动生成 handler 的名字\n\n如果我们调用的是如下的 addLast 方法\n\nChannelPipeline addLast(ChannelHandler... handlers);\n\n\n那么 Netty 会调用 generateName 为我们的 handler 自动生成一个名字:\n\nprivate String generateName(ChannelHandler handler) {\n    WeakHashMap<Class<?>, String> cache = nameCaches[(int) (Thread.currentThread().getId() % nameCaches.length)];\n    Class<?> handlerType = handler.getClass();\n    String name;\n    synchronized (cache) {\n        name = cache.get(handlerType);\n        if (name == null) {\n            name = generateName0(handlerType);\n            cache.put(handlerType, name);\n        }\n    }\n\n    synchronized (this) {\n        // It\'s not very likely for a user to put more than one handler of the same type, but make sure to avoid\n        // any name conflicts.  Note that we don\'t cache the names generated here.\n        if (name2ctx.containsKey(name)) {\n            String baseName = name.substring(0, name.length() - 1); // Strip the trailing \'0\'.\n            for (int i = 1;; i ++) {\n                String newName = baseName + i;\n                if (!name2ctx.containsKey(newName)) {\n                    name = newName;\n                    break;\n                }\n            }\n        }\n    }\n\n    return name;\n}\n\n\n而 generateName 会接着调用 generateName0 来实际产生一个 handler 的名字:\n\nprivate static String generateName0(Class<?> handlerType) {\n    return StringUtil.simpleClassName(handlerType) + "#0";\n}\n\n\n自动生成的名字的规则很简单, 就是 handler 的简单类名加上 "#0", 因此我们的 EchoClientHandler 的名字就是 "EchoClientHandler#0", 这一点也可以通过调试窗口佐证:\n\n\n\n\n# 关于 Pipeline 的事件传输机制\n\n前面章节中, 我们知道 AbstractChannelHandlerContext 中有 inbound 和 outbound 两个 boolean 变量, 分别用于标识 Context 所对应的 handler 的类型, 即:\n\n * inbound 为真时, 表示对应的 ChannelHandler 实现了 ChannelInboundHandler 方法.\n * outbound 为真时, 表示对应的 ChannelHandler 实现了 ChannelOutboundHandler 方法.\n\n读者朋友肯定很疑惑了吧: 那究竟这两个字段有什么作用呢? 其实这还要从 ChannelPipeline 的传输的事件类型说起. Netty 的事件可以分为 Inbound 和 Outbound 事件.\n\n如下是从 Netty 官网上拷贝的一个图示:\n\n                                             I/O Request\n                                        via Channel or\n                                    ChannelHandlerContext\n                                                  |\n+---------------------------------------------------+---------------+\n|                           ChannelPipeline         |               |\n|                                                  \\|/              |\n|    +---------------------+            +-----------+----------+    |\n|    | Inbound Handler  N  |            | Outbound Handler  1  |    |\n|    +----------+----------+            +-----------+----------+    |\n|              /|\\                                  |               |\n|               |                                  \\|/              |\n|    +----------+----------+            +-----------+----------+    |\n|    | Inbound Handler N-1 |            | Outbound Handler  2  |    |\n|    +----------+----------+            +-----------+----------+    |\n|              /|\\                                  .               |\n|               .                                   .               |\n| ChannelHandlerContext.fireIN_EVT() ChannelHandlerContext.OUT_EVT()|\n|        [ method call]                       [method call]         |\n|               .                                   .               |\n|               .                                  \\|/              |\n|    +----------+----------+            +-----------+----------+    |\n|    | Inbound Handler  2  |            | Outbound Handler M-1 |    |\n|    +----------+----------+            +-----------+----------+    |\n|              /|\\                                  |               |\n|               |                                  \\|/              |\n|    +----------+----------+            +-----------+----------+    |\n|    | Inbound Handler  1  |            | Outbound Handler  M  |    |\n|    +----------+----------+            +-----------+----------+    |\n|              /|\\                                  |               |\n+---------------+-----------------------------------+---------------+\n              |                                  \\|/\n+---------------+-----------------------------------+---------------+\n|               |                                   |               |\n|       [ Socket.read() ]                    [ Socket.write() ]     |\n|                                                                   |\n|  Netty Internal I/O Threads (Transport Implementation)            |\n+-------------------------------------------------------------------+\n\n\n从上图可以看出, inbound 事件和 outbound 事件的流向是不一样的, inbound 事件的流行是从下至上, 而 outbound 刚好相反, 是从上到下. 并且 inbound 的传递方式是通过调用相应的 ChannelHandlerContext.fireIN_EVT() 方法, 而 outbound 方法的的传递方式是通过调用 ChannelHandlerContext.OUT_EVT() 方法. 例如 ChannelHandlerContext.fireChannelRegistered() 调用会发送一个 ChannelRegistered 的 inbound 给下一个ChannelHandlerContext, 而 ChannelHandlerContext.bind 调用会发送一个 bind 的 outbound 事件给 下一个 ChannelHandlerContext.\n\nInbound 事件传播方法有:\n\nChannelHandlerContext.fireChannelRegistered()\nChannelHandlerContext.fireChannelActive()\nChannelHandlerContext.fireChannelRead(Object)\nChannelHandlerContext.fireChannelReadComplete()\nChannelHandlerContext.fireExceptionCaught(Throwable)\nChannelHandlerContext.fireUserEventTriggered(Object)\nChannelHandlerContext.fireChannelWritabilityChanged()\nChannelHandlerContext.fireChannelInactive()\nChannelHandlerContext.fireChannelUnregistered()\n\n\nOubound 事件传输方法有:\n\nChannelHandlerContext.bind(SocketAddress, ChannelPromise)\nChannelHandlerContext.connect(SocketAddress, SocketAddress, ChannelPromise)\nChannelHandlerContext.write(Object, ChannelPromise)\nChannelHandlerContext.flush()\nChannelHandlerContext.read()\nChannelHandlerContext.disconnect(ChannelPromise)\nChannelHandlerContext.close(ChannelPromise)\n\n\n注意, 如果我们捕获了一个事件, 并且想让这个事件继续传递下去, 那么需要调用 Context 相应的传播方法. 例如:\n\npublic class MyInboundHandler extends ChannelInboundHandlerAdapter {\n    @Override\n    public void channelActive(ChannelHandlerContext ctx) {\n        System.out.println("Connected!");\n        ctx.fireChannelActive();\n    }\n}\n\npublic clas MyOutboundHandler extends ChannelOutboundHandlerAdapter {\n    @Override\n    public void close(ChannelHandlerContext ctx, ChannelPromise promise) {\n        System.out.println("Closing ..");\n        ctx.close(promise);\n    }\n}\n\n\n上面的例子中, MyInboundHandler 收到了一个 channelActive 事件, 它在处理后, 如果希望将事件继续传播下去, 那么需要接着调用 ctx.fireChannelActive().\n\n\n# Outbound 操作\n\nOutbound 事件都是请求事件(request event), 即请求某件事情的发生, 然后通过 Outbound 事件进行通知. Outbound 事件的传播方向是 tail -> customContext -> head.\n\n我们接下来以 connect 事件为例, 分析一下 Outbound 事件的传播机制. 首先, 当用户调用了 Bootstrap.connect 方法时, 就会触发一个 Connect 请求事件, 此调用会触发如下调用链:\n\nBootstrap.connect -> Bootstrap.doConnect -> Bootstrap.doConnect0 -> AbstractChannel.connect\n\n\n继续跟踪的话, 我们就发现, AbstractChannel.connect 其实由调用了 DefaultChannelPipeline.connect 方法:\n\n@Override\npublic ChannelFuture connect(SocketAddress remoteAddress, ChannelPromise promise) {\n\treturn pipeline.connect(remoteAddress, promise);\n}\n\n\n而 pipeline.connect 的实现如下:\n\n@Override\npublic ChannelFuture connect(SocketAddress remoteAddress, ChannelPromise promise) {\n    return tail.connect(remoteAddress, promise);\n}\n\n\n可以看到, 当 outbound 事件(这里是 connect 事件)传递到 Pipeline 后, 它其实是以 tail 为起点开始传播的. 而 tail.connect 其实调用的是 AbstractChannelHandlerContext.connect 方法:\n\n@Override\npublic ChannelFuture connect(\n        final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) {\n    ...\n    final AbstractChannelHandlerContext next = findContextOutbound();\n    EventExecutor executor = next.executor();\n    ...\n    next.invokeConnect(remoteAddress, localAddress, promise);\n    ...\n    return promise;\n}\n\n\nfindContextOutbound() 顾名思义, 它的作用是以当前 Context 为起点, 向 Pipeline 中的 Context 双向链表的前端寻找第一个 outbound 属性为真的 Context(即关联着 ChannelOutboundHandler 的 Context), 然后返回. 它的实现如下:\n\nprivate AbstractChannelHandlerContext findContextOutbound() {\n    AbstractChannelHandlerContext ctx = this;\n    do {\n        ctx = ctx.prev;\n    } while (!ctx.outbound);\n    return ctx;\n}\n\n\n当我们找到了一个 outbound 的 Context 后, 就调用它的 invokeConnect 方法, 这个方法中会调用 Context 所关联着的 ChannelHandler 的 connect 方法:\n\nprivate void invokeConnect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) {\n    try {\n        ((ChannelOutboundHandler) handler()).connect(this, remoteAddress, localAddress, promise);\n    } catch (Throwable t) {\n        notifyOutboundHandlerException(t, promise);\n    }\n}\n\n\n如果用户没有重写 ChannelHandler 的 connect 方法, 那么会调用 ChannelOutboundHandlerAdapter 所实现的方法:\n\n@Override\npublic void connect(ChannelHandlerContext ctx, SocketAddress remoteAddress,\n        SocketAddress localAddress, ChannelPromise promise) throws Exception {\n    ctx.connect(remoteAddress, localAddress, promise);\n}\n\n\n我们看到, ChannelOutboundHandlerAdapter.connect 仅仅调用了 ctx.connect, 而这个调用又回到了:\n\nContext.connect -> Connect.findContextOutbound -> next.invokeConnect -> handler.connect -> Context.connect\n\n\n这样的循环中, 直到 connect 事件传递到DefaultChannelPipeline 的双向链表的头节点, 即 head 中. 为什么会传递到 head 中呢? 回想一下, head 实现了 ChannelOutboundHandler, 因此它的 outbound 属性是 true. 因为 head 本身既是一个 ChannelHandlerContext, 又实现了 ChannelOutboundHandler 接口, 因此当 connect 消息传递到 head 后, 会将消息转递到对应的 ChannelHandler 中处理, 而恰好, head 的 handler() 返回的就是 head 本身:\n\n@Override\npublic ChannelHandler handler() {\n    return this;\n}\n\n\n因此最终 connect 事件是在 head 中处理的. head 的 connect 事件处理方法如下:\n\n@Override\npublic void connect(\n        ChannelHandlerContext ctx,\n        SocketAddress remoteAddress, SocketAddress localAddress,\n        ChannelPromise promise) throws Exception {\n    unsafe.connect(remoteAddress, localAddress, promise);\n}\n\n\n到这里, 整个 Connect 请求事件就结束了. 下面以一幅图来描述一个整个 Connect 请求事件的处理过程:\n\n\n\n我们仅仅以 Connect 请求事件为例, 分析了 Outbound 事件的传播过程, 但是其实所有的 outbound 的事件传播都遵循着一样的传播规律, 读者可以试着分析一下其他的 outbound 事件, 体会一下它们的传播过程.\n\n\n# Inbound 事件\n\nInbound 事件和 Outbound 事件的处理过程有点镜像. Inbound 事件是一个通知事件, 即某件事已经发生了, 然后通过 Inbound 事件进行通知. Inbound 通常发生在 Channel 的状态的改变或 IO 事件就绪. Inbound 的特点是它传播方向是 head -> customContext -> tail.\n\n既然上面我们分析了 Connect 这个 Outbound 事件, 那么接着分析 Connect 事件后会发生什么 Inbound 事件, 并最终找到 Outbound 和 Inbound 事件之间的联系.\n\n当 Connect 这个 Outbound 传播到 unsafe 后, 其实是在 AbstractNioUnsafe.connect 方法中进行处理的:\n\n@Override\npublic final void connect(\n        final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) {\n    ...\n    if (doConnect(remoteAddress, localAddress)) {\n        fulfillConnectPromise(promise, wasActive);\n    } else {\n        ...\n    }\n    ...\n}\n\n\n在 AbstractNioUnsafe.connect 中, 首先调用 doConnect 方法进行实际上的 Socket 连接, 当连接上后, 会调用 fulfillConnectPromise 方法:\n\nprivate void fulfillConnectPromise(ChannelPromise promise, boolean wasActive) {\n    ...\n    // Regardless if the connection attempt was cancelled, channelActive() event should be triggered,\n    // because what happened is what happened.\n    if (!wasActive && isActive()) {\n        pipeline().fireChannelActive();\n    }\n    ...\n}\n\n\n我们看到, 在 fulfillConnectPromise 中, 会通过调用 pipeline().fireChannelActive() 将通道激活的消息(即 Socket 连接成功)发送出去. 而这里, 当调用 pipeline.fireXXX 后, 就是 Inbound 事件的起点. 因此当调用了 pipeline().fireChannelActive() 后, 就产生了一个 ChannelActive Inbound 事件, 我们就从这里开始看看这个 Inbound 事件是怎么传播的吧.\n\n@Override\npublic ChannelPipeline fireChannelActive() {\n    head.fireChannelActive();\n\n    if (channel.config().isAutoRead()) {\n        channel.read();\n    }\n\n    return this;\n}\n\n\n哈哈, 果然, 在 fireChannelActive 方法中, 调用的是 head.fireChannelActive, 因此可以证明了, Inbound 事件在 Pipeline 中传输的起点是 head. 那么, 在 head.fireChannelActive() 中又做了什么呢?\n\n@Override\npublic ChannelHandlerContext fireChannelActive() {\n    final AbstractChannelHandlerContext next = findContextInbound();\n    EventExecutor executor = next.executor();\n    ...\n    next.invokeChannelActive();\n    ...\n    return this;\n}\n\n\n上面的代码应该很熟悉了吧. 回想一下在 Outbound 事件(例如 Connect 事件)的传输过程中时, 我们也有类似的操作:\n\n * 首先调用 findContextInbound, 从 Pipeline 的双向链表中中找到第一个属性 inbound 为真的 Context, 然后返回\n * 调用这个 Context 的 invokeChannelActive\n\ninvokeChannelActive 方法如下:\n\nprivate void invokeChannelActive() {\n    try {\n        ((ChannelInboundHandler) handler()).channelActive(this);\n    } catch (Throwable t) {\n        notifyHandlerException(t);\n    }\n}\n\n\n这个方法和 Outbound 的对应方法(例如 invokeConnect) 如出一辙. 同 Outbound 一样, 如果用户没有重写 channelActive 方法, 那么会调用 ChannelInboundHandlerAdapter 的 channelActive 方法:\n\n@Override\npublic void channelActive(ChannelHandlerContext ctx) throws Exception {\n    ctx.fireChannelActive();\n}\n\n\n同样地, 在 ChannelInboundHandlerAdapter.channelActive 中, 仅仅调用了 ctx.fireChannelActive 方法, 因此就会有如下循环:\n\nContext.fireChannelActive -> Connect.findContextInbound -> nextContext.invokeChannelActive -> nextHandler.channelActive -> nextContext.fireChannelActive\n\n\n这样的循环中. 同理, tail 本身 既实现了 ChannelInboundHandler 接口, 又实现了 ChannelHandlerContext 接口, 因此当 channelActive 消息传递到 tail 后, 会将消息转递到对应的 ChannelHandler 中处理, 而恰好, tail 的 handler() 返回的就是 tail 本身:\n\n@Override\npublic ChannelHandler handler() {\n    return this;\n}\n\n\n因此 channelActive Inbound 事件最终是在 tail 中处理的, 我们看一下它的处理方法:\n\n@Override\npublic void channelActive(ChannelHandlerContext ctx) throws Exception { }\n\n\nTailContext.channelActive 方法是空的. 如果读者自行查看 TailContext 的 Inbound 处理方法时, 会发现, 它们的实现都是空的. 可见, 如果是 Inbound, 当用户没有实现自定义的处理器时, 那么默认是不处理的.\n\n用一幅图来总结一下 Inbound 的传输过程吧:\n\n\n\n\n# 总结\n\n对于 Outbound事件:\n\n * Outbound 事件是请求事件(由 Connect 发起一个请求, 并最终由 unsafe 处理这个请求)\n * Outbound 事件的发起者是 Channel\n * Outbound 事件的处理者是 unsafe\n * Outbound 事件在 Pipeline 中的传输方向是 tail -> head.\n * 在 ChannelHandler 中处理事件时, 如果这个 Handler 不是最后一个 Hnalder, 则需要调用 ctx.xxx (例如 ctx.connect) 将此事件继续传播下去. 如果不这样做, 那么此事件的传播会提前终止.\n * Outbound 事件流: Context.OUT_EVT -> Connect.findContextOutbound -> nextContext.invokeOUT_EVT -> nextHandler.OUT_EVT -> nextContext.OUT_EVT\n\n对于 Inbound 事件:\n\n * Inbound 事件是通知事件, 当某件事情已经就绪后, 通知上层.\n * Inbound 事件发起者是 unsafe\n * Inbound 事件的处理者是 Channel, 如果用户没有实现自定义的处理方法, 那么Inbound 事件默认的处理者是 TailContext, 并且其处理方法是空实现.\n * Inbound 事件在 Pipeline 中传输方向是 head -> tail\n * 在 ChannelHandler 中处理事件时, 如果这个 Handler 不是最后一个 Hnalder, 则需要调用 ctx.fireIN_EVT (例如 ctx.fireChannelActive) 将此事件继续传播下去. 如果不这样做, 那么此事件的传播会提前终止.\n * Outbound 事件流: Context.fireIN_EVT -> Connect.findContextInbound -> nextContext.invokeIN_EVT -> nextHandler.IN_EVT -> nextContext.fireIN_EVT\n\noutbound 和 inbound 事件十分的镜像, 并且 Context 与 Handler 直接的调用关系是否容易混淆, 因此读者在阅读这里的源码时, 需要特别的注意.\n\n\n# 总结\n\n\n# 参考资料',normalizedContent:'# 前言\n\n这篇是 netty 源码分析 的第二篇, 在这篇文章中, 我会为读者详细地分析 netty 中的 channelpipeline 机制.\n\n\n# channel 与 channelpipeline\n\n相信大家都知道了, 在 netty 中每个 channel 都有且仅有一个 channelpipeline 与之对应, 它们的组成关系如下:\n\n\n\n通过上图我们可以看到, 一个 channel 包含了一个 channelpipeline, 而 channelpipeline 中又维护了一个由 channelhandlercontext 组成的双向链表. 这个链表的头是 headcontext, 链表的尾是 tailcontext, 并且每个 channelhandlercontext 中又关联着一个 channelhandler. 上面的图示给了我们一个对 channelpipeline 的直观认识, 但是实际上 netty 实现的 channel 是否真的是这样的呢? 我们继续用源码说话.\n\n在第一章 netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 中, 我们已经知道了一个 channel 的初始化的基本过程, 下面我们再回顾一下. 下面的代码是 abstractchannel 构造器:\n\nprotected abstractchannel(channel parent) {\n    this.parent = parent;\n    unsafe = newunsafe();\n    pipeline = new defaultchannelpipeline(this);\n}\n\n\nabstractchannel 有一个 pipeline 字段, 在构造器中会初始化它为 defaultchannelpipeline的实例. 这里的代码就印证了一点: 每个 channel 都有一个 channelpipeline. 接着我们跟踪一下 defaultchannelpipeline 的初始化过程. 首先进入到 defaultchannelpipeline 构造器中:\n\npublic defaultchannelpipeline(abstractchannel channel) {\n    if (channel == null) {\n        throw new nullpointerexception("channel");\n    }\n    this.channel = channel;\n\n    tail = new tailcontext(this);\n    head = new headcontext(this);\n\n    head.next = tail;\n    tail.prev = head;\n}\n\n\n在 defaultchannelpipeline 构造器中, 首先将与之关联的 channel 保存到字段 channel 中, 然后实例化两个 channelhandlercontext, 一个是 headcontext 实例 head, 另一个是 tailcontext 实例 tail. 接着将 head 和 tail 互相指向, 构成一个双向链表. 特别注意到, 我们在开始的示意图中, head 和 tail 并没有包含 channelhandler, 这是因为 headcontext 和 tailcontext 继承于 abstractchannelhandlercontext 的同时也实现了 channelhandler 接口了, 因此它们有 context 和 handler 的双重属性.\n\n\n# channelpipeline 的初始化再探\n\n在第一章的时候, 我们已经对 channelpipeline 的初始化有了一个大致的了解, 不过当时重点毕竟不在 channelpipeline 这里, 因此没有深入地分析它的初始化过程. 那么下面我们就来看一下具体的 channelpipeline 的初始化都做了哪些工作吧.\n\n\n# channelpipeline 实例化过程\n\n我们再来回顾一下, 在实例化一个 channel 时, 会伴随着一个 channelpipeline 的实例化, 并且此 channel 会与这个 channelpipeline 相互关联, 这一点可以通过niosocketchannel 的父类 abstractchannel 的构造器予以佐证:\n\nprotected abstractchannel(channel parent) {\n    this.parent = parent;\n    unsafe = newunsafe();\n    pipeline = new defaultchannelpipeline(this);\n}\n\n\n当实例化一个 channel(这里以 echoclient 为例, 那么 channel 就是 niosocketchannel), 其 pipeline 字段就是我们新创建的 defaultchannelpipeline 对象, 那么我们就来看一下 defaultchannelpipeline 的构造方法吧:\n\npublic defaultchannelpipeline(abstractchannel channel) {\n    if (channel == null) {\n        throw new nullpointerexception("channel");\n    }\n    this.channel = channel;\n\n    tail = new tailcontext(this);\n    head = new headcontext(this);\n\n    head.next = tail;\n    tail.prev = head;\n}\n\n\n可以看到, 在 defaultchannelpipeline 的构造方法中, 将传入的 channel 赋值给字段 this.channel, 接着又实例化了两个特殊的字段: tail 与 head. 这两个字段是一个双向链表的头和尾. 其实在 defaultchannelpipeline 中, 维护了一个以 abstractchannelhandlercontext 为节点的双向链表, 这个链表是 netty 实现 pipeline 机制的关键. 再回顾一下 head 和 tail 的类层次结构:\n\n\n\n\n\n从类层次结构图中可以很清楚地看到, head 实现了 channelinboundhandler, 而 tail 实现了 channeloutboundhandler 接口, 并且它们都实现了 channelhandlercontext 接口, 因此可以说 head 和 tail 既是一个 channelhandler, 又是一个 channelhandlercontext.\n\n接着看一下 headcontext 的构造器:\n\nheadcontext(defaultchannelpipeline pipeline) {\n    super(pipeline, null, head_name, false, true);\n    unsafe = pipeline.channel().unsafe();\n}\n\n\n它调用了父类 abstractchannelhandlercontext 的构造器, 并传入参数 inbound = false, outbound = true. tailcontext 的构造器与 headcontext 的相反, 它调用了父类 abstractchannelhandlercontext 的构造器, 并传入参数 inbound = true, outbound = false. 即 header 是一个 outboundhandler, 而 tail 是一个inboundhandler, 关于这一点, 大家要特别注意, 因为在后面的分析中, 我们会反复用到 inbound 和 outbound 这两个属性.\n\n\n# 链表的节点 channelhandlercontext\n\n\n# channelinitializer 的添加\n\n上面一小节中, 我们已经分析了 channel 的组成, 其中我们了解到, 最开始的时候 channelpipeline 中含有两个 channelhandlercontext(同时也是 channelhandler), 但是这个 pipeline并不能实现什么特殊的功能, 因为我们还没有给它添加自定义的 channelhandler. 通常来说, 我们在初始化 bootstrap, 会添加我们自定义的 channelhandler, 就以我们熟悉的 echoclient 来举例吧:\n\nbootstrap b = new bootstrap();\nb.group(group)\n .channel(niosocketchannel.class)\n .option(channeloption.tcp_nodelay, true)\n .handler(new channelinitializer<socketchannel>() {\n     @override\n     public void initchannel(socketchannel ch) throws exception {\n         channelpipeline p = ch.pipeline();\n         p.addlast(new echoclienthandler());\n     }\n });\n\n\n上面代码的初始化过程, 相信大家都不陌生. 在调用 handler 时, 传入了 channelinitializer 对象, 它提供了一个 initchannel 方法供我们初始化 channelhandler. 那么这个初始化过程是怎样的呢? 下面我们就来揭开它的神秘面纱.\n\nchannelinitializer 实现了 channelhandler, 那么它是在什么时候添加到 channelpipeline 中的呢? 进行了一番搜索后, 我们发现它是在 bootstrap.init 方法中添加到 channelpipeline 中的. 其代码如下:\n\n@override\n@suppresswarnings("unchecked")\nvoid init(channel channel) throws exception {\n    channelpipeline p = channel.pipeline();\n    p.addlast(handler());\n    ...\n}\n\n\n上面的代码将 handler() 返回的 channelhandler 添加到 pipeline 中, 而 handler() 返回的是handler 其实就是我们在初始化 bootstrap 调用 handler 设置的 channelinitializer 实例, 因此这里就是将 channelinitializer 插入到了 pipeline 的末端. 此时 pipeline 的结构如下图所示:\n\n\n\n有朋友可能就有疑惑了, 我明明插入的是一个 channelinitializer 实例, 为什么在 channelpipeline 中的双向链表中的元素却是一个 channelhandlercontext? 为了解答这个问题, 我们继续在代码中寻找答案吧. 我们刚才提到, 在 bootstrap.init 中会调用 p.addlast() 方法, 将 channelinitializer 插入到链表末端:\n\n@override\npublic channelpipeline addlast(eventexecutorgroup group, final string name, channelhandler handler) {\n    synchronized (this) {\n        checkduplicatename(name); // 检查此 handler 是否有重复的名字\n\n        abstractchannelhandlercontext newctx = new defaultchannelhandlercontext(this, group, name, handler);\n        addlast0(name, newctx);\n    }\n\n    return this;\n}\n\n\naddlast 有很多重载的方法, 我们关注这个比较重要的方法就可以了. 上面的 addlast 方法中, 首先检查这个 channelhandler 的名字是否是重复的, 如果不重复的话, 则为这个 handler 创建一个对应的 defaultchannelhandlercontext 实例, 并与之关联起来(context 中有一个 handler 属性保存着对应的 handler 实例). 判断此 handler 是否重名的方法很简单: netty 中有一个 name2ctx map 字段, key 是 handler 的名字, 而 value 则是 handler 本身. 因此通过如下代码就可以判断一个 handler 是否重名了:\n\nprivate void checkduplicatename(string name) {\n    if (name2ctx.containskey(name)) {\n        throw new illegalargumentexception("duplicate handler name: " + name);\n    }\n}\n\n\n为了添加一个 handler 到 pipeline 中, 必须把此 handler 包装成 channelhandlercontext. 因此在上面的代码中我们可以看到新实例化了一个 newctx 对象, 并将 handler 作为参数传递到构造方法中. 那么我们来看一下实例化的 defaultchannelhandlercontext 到底有什么玄机吧. 首先看它的构造器:\n\ndefaultchannelhandlercontext(\n        defaultchannelpipeline pipeline, eventexecutorgroup group, string name, channelhandler handler) {\n    super(pipeline, group, name, isinbound(handler), isoutbound(handler));\n    if (handler == null) {\n        throw new nullpointerexception("handler");\n    }\n    this.handler = handler;\n}\n\n\ndefaultchannelhandlercontext 的构造器中, 调用了两个很有意思的方法: isinbound 与 isoutbound, 这两个方法是做什么的呢?\n\nprivate static boolean isinbound(channelhandler handler) {\n    return handler instanceof channelinboundhandler;\n}\n\nprivate static boolean isoutbound(channelhandler handler) {\n    return handler instanceof channeloutboundhandler;\n}\n\n\n从源码中可以看到, 当一个 handler 实现了 channelinboundhandler 接口, 则 isinbound 返回真; 相似地, 当一个 handler 实现了 channeloutboundhandler 接口, 则 isoutbound 就返回真. 而这两个 boolean 变量会传递到父类 abstractchannelhandlercontext 中, 并初始化父类的两个字段: inbound 与 outbound. 那么这里的 channelinitializer 所对应的 defaultchannelhandlercontext 的 inbound 与 inbound 字段分别是什么呢? 那就看一下 channelinitializer 到底实现了哪个接口不就行了? 如下是 channelinitializer 的类层次结构图：\n\n\n\n可以清楚地看到, channelinitializer 仅仅实现了 channelinboundhandler 接口, 因此这里实例化的 defaultchannelhandlercontext 的 inbound = true, outbound = false. 不就是 inbound 和 outbound 两个字段嘛, 为什么需要这么大费周章地分析一番? 其实这两个字段关系到 pipeline 的事件的流向与分类, 因此是十分关键的, 不过我在这里先卖个关子, 后面我们再来详细分析这两个字段所起的作用. 在这里, 读者只需要记住, channelinitializer 所对应的 defaultchannelhandlercontext 的 inbound = true, outbound = false 即可.\n\n当创建好 context 后, 就将这个 context 插入到 pipeline 的双向链表中:\n\nprivate void addlast0(final string name, abstractchannelhandlercontext newctx) {\n    checkmultiplicity(newctx);\n\n    abstractchannelhandlercontext prev = tail.prev;\n    newctx.prev = prev;\n    newctx.next = tail;\n    prev.next = newctx;\n    tail.prev = newctx;\n\n    name2ctx.put(name, newctx);\n\n    callhandleradded(newctx);\n}\n\n\n显然, 这个代码就是典型的双向链表的插入操作了. 当调用了 addlast 方法后, netty 就会将此 handler 添加到双向链表中 tail 元素之前的位置.\n\n\n# 自定义 channelhandler 的添加过程\n\n在上一小节中, 我们已经分析了一个 channelinitializer 如何插入到 pipeline 中的, 接下来就来探讨一下 channelinitializer 在哪里被调用, channelinitializer 的作用, 以及我们自定义的 channelhandler 是如何插入到 pipeline 中的.\n\n在 netty 源码分析之 一 揭开 bootstrap 神秘的红盖头 (客户端) 一章的 channel 的注册过程 小节中, 我们已经分析过 channel 的注册过程了, 这里我们再简单地复习一下:\n\n * 首先在 abstractbootstrap.initandregister中, 通过 group().register(channel), 调用 multithreadeventloopgroup.register 方法\n * 在multithreadeventloopgroup.register 中, 通过 next() 获取一个可用的 singlethreadeventloop, 然后调用它的 register\n * 在 singlethreadeventloop.register 中, 通过 channel.unsafe().register(this, promise) 来获取 channel 的 unsafe() 底层操作对象, 然后调用它的 register.\n * 在 abstractunsafe.register 方法中, 调用 register0 方法注册 channel\n * 在 abstractunsafe.register0 中, 调用 abstractniochannel#doregister 方法\n * abstractniochannel.doregister 方法通过 javachannel().register(eventloop().selector, 0, this) 将 channel 对应的 java nio sockerchannel 注册到一个 eventloop 的 selector 中, 并且将当前 channel 作为 attachment.\n\n而我们自定义 channelhandler 的添加过程, 发生在 abstractunsafe.register0 中, 在这个方法中调用了 pipeline.firechannelregistered() 方法, 其实现如下:\n\n@override\npublic channelpipeline firechannelregistered() {\n    head.firechannelregistered();\n    return this;\n}\n\n\n上面的代码很简单, 就是调用了 head.firechannelregistered() 方法而已.\n\n> 关于上面代码的 head.firexxx 的调用形式, 是 netty 中 pipeline 传递事件的常用方式, 我们以后会经常看到.\n\n还记得 head 的 类层次结构图不, head 是一个 abstractchannelhandlercontext 实例, 并且它没有重写 firechannelregistered 方法, 因此 head.firechannelregistered 其实是调用的 abstractchannelhandlercontext.firechannelregistered:\n\n@override\npublic channelhandlercontext firechannelregistered() {\n    final abstractchannelhandlercontext next = findcontextinbound();\n    eventexecutor executor = next.executor();\n    if (executor.ineventloop()) {\n        next.invokechannelregistered();\n    } else {\n        executor.execute(new onetimetask() {\n            @override\n            public void run() {\n                next.invokechannelregistered();\n            }\n        });\n    }\n    return this;\n}\n\n\n这个方法的第一句是调用 findcontextinbound 获取一个 context, 那么它返回的 context 到底是什么呢? 我们跟进代码中看一下:\n\nprivate abstractchannelhandlercontext findcontextinbound() {\n    abstractchannelhandlercontext ctx = this;\n    do {\n        ctx = ctx.next;\n    } while (!ctx.inbound);\n    return ctx;\n}\n\n\n很显然, 这个代码会从 head 开始遍历 pipeline 的双向链表, 然后找到第一个属性 inbound 为 true 的 channelhandlercontext 实例. 想起来了没? 我们在前面分析 channelinitializer 时, 花了大量的笔墨来分析了 inbound 和 outbound 属性, 你看现在这里就用上了. 回想一下, channelinitializer 实现了 channelinboudhandler, 因此它所对应的 channelhandlercontext 的 inbound 属性就是 true, 因此这里返回就是 channelinitializer 实例所对应的 channelhandlercontext. 即:\n\n\n\n当获取到 inbound 的 context 后, 就调用它的 invokechannelregistered 方法:\n\nprivate void invokechannelregistered() {\n    try {\n        ((channelinboundhandler) handler()).channelregistered(this);\n    } catch (throwable t) {\n        notifyhandlerexception(t);\n    }\n}\n\n\n我们已经强调过了, 每个 channelhandler 都与一个 channelhandlercontext 关联, 我们可以通过 channelhandlercontext 获取到对应的 channelhandler. 因此很显然了, 这里 handler() 返回的, 其实就是一开始我们实例化的 channelinitializer 对象, 并接着调用了 channelinitializer.channelregistered 方法. 看到这里, 读者朋友是否会觉得有点眼熟呢？channelinitializer.channelregistered 这个方法我们在第一章的时候已经大量地接触了, 但是我们并没有深入地分析这个方法的调用过程, 那么在这里读者朋友应该对它的调用有了更加深入的了解了吧\n\n那么这个方法中又有什么玄机呢? 继续看代码:\n\n@override\n@suppresswarnings("unchecked")\npublic final void channelregistered(channelhandlercontext ctx) throws exception {\n    initchannel((c) ctx.channel());\n    ctx.pipeline().remove(this);\n    ctx.firechannelregistered();\n}\n\n\ninitchannel 这个方法我们很熟悉了吧, 它就是我们在初始化 bootstrap 时, 调用 handler 方法传入的匿名内部类所实现的方法:\n\nbootstrap.handler(new channelinitializer<socketchannel>() {\n     @override\n     public void initchannel(socketchannel ch) throws exception {\n         channelpipeline p = ch.pipeline();\n         p.addlast(new echoclienthandler());\n     }\n });\n\n\n因此当调用了这个方法后, 我们自定义的 channelhandler 就插入到 pipeline 了, 此时的 pipeline 如下图所示:\n\n\n\n当添加了自定义的 channelhandler 后, 会删除 channelinitializer 这个 channelhandler, 即 "ctx.pipeline().remove(this)", 因此最后的 pipeline 如下:\n\n\n\n好了, 到了这里, 我们的 自定义 channelhandler 的添加过程 也分析的查不多了.\n\n\n# channelhandler 的名字\n\n我们注意到, pipeline.addxxx 都有一个重载的方法, 例如 addlast, 它有一个重载的版本是:\n\nchannelpipeline addlast(string name, channelhandler handler);\n\n\n第一个参数指定了所添加的 handler 的名字(更准确地说是 channelhandlercontext 的名字, 不过我们通常是以 handler 作为叙述的对象, 因此说成 handler 的名字便于理解). 那么 handler 的名字有什么用呢? 如果我们不设置name, 那么 handler 会有怎样的名字? 为了解答这些疑惑, 老规矩, 依然是从源码中找到答案. 我们还是以 addlast 方法为例:\n\n@override\npublic channelpipeline addlast(string name, channelhandler handler) {\n    return addlast(null, name, handler);\n}\n\n\n这个方法会调用重载的 addlast 方法:\n\n@override\npublic channelpipeline addlast(eventexecutorgroup group, final string name, channelhandler handler) {\n    synchronized (this) {\n        checkduplicatename(name);\n\n        abstractchannelhandlercontext newctx = new defaultchannelhandlercontext(this, group, name, handler);\n        addlast0(name, newctx);\n    }\n\n    return this;\n}\n\n\n第一个参数被设置为 null, 我们不关系它. 第二参数就是这个 handler 的名字. 看代码可知, 在添加一个 handler 之前, 需要调用 checkduplicatename 方法来确定此 handler 的名字是否和已添加的 handler 的名字重复. 而这个 checkduplicatename 方法我们在前面已经有提到, 这里再回顾一下:\n\nprivate void checkduplicatename(string name) {\n    if (name2ctx.containskey(name)) {\n        throw new illegalargumentexception("duplicate handler name: " + name);\n    }\n}\n\n\nnetty 判断一个 handler 的名字是否重复的依据很简单: defaultchannelpipeline 中有一个 类型为 map<string, abstractchannelhandlercontext> 的 name2ctx 字段, 它的 key 是一个 handler 的名字, 而 value 则是这个 handler 所对应的 channelhandlercontext. 每当新添加一个 handler 时, 就会 put 到 name2ctx 中. 因此检查 name2ctx 中是否包含这个 name 即可. 当没有重名的 handler 时, 就为这个 handler 生成一个关联的 defaultchannelhandlercontext 对象, 然后就将 name 和 newctx 作为 key-value 对 放到 name2ctx 中.\n\n\n# 自动生成 handler 的名字\n\n如果我们调用的是如下的 addlast 方法\n\nchannelpipeline addlast(channelhandler... handlers);\n\n\n那么 netty 会调用 generatename 为我们的 handler 自动生成一个名字:\n\nprivate string generatename(channelhandler handler) {\n    weakhashmap<class<?>, string> cache = namecaches[(int) (thread.currentthread().getid() % namecaches.length)];\n    class<?> handlertype = handler.getclass();\n    string name;\n    synchronized (cache) {\n        name = cache.get(handlertype);\n        if (name == null) {\n            name = generatename0(handlertype);\n            cache.put(handlertype, name);\n        }\n    }\n\n    synchronized (this) {\n        // it\'s not very likely for a user to put more than one handler of the same type, but make sure to avoid\n        // any name conflicts.  note that we don\'t cache the names generated here.\n        if (name2ctx.containskey(name)) {\n            string basename = name.substring(0, name.length() - 1); // strip the trailing \'0\'.\n            for (int i = 1;; i ++) {\n                string newname = basename + i;\n                if (!name2ctx.containskey(newname)) {\n                    name = newname;\n                    break;\n                }\n            }\n        }\n    }\n\n    return name;\n}\n\n\n而 generatename 会接着调用 generatename0 来实际产生一个 handler 的名字:\n\nprivate static string generatename0(class<?> handlertype) {\n    return stringutil.simpleclassname(handlertype) + "#0";\n}\n\n\n自动生成的名字的规则很简单, 就是 handler 的简单类名加上 "#0", 因此我们的 echoclienthandler 的名字就是 "echoclienthandler#0", 这一点也可以通过调试窗口佐证:\n\n\n\n\n# 关于 pipeline 的事件传输机制\n\n前面章节中, 我们知道 abstractchannelhandlercontext 中有 inbound 和 outbound 两个 boolean 变量, 分别用于标识 context 所对应的 handler 的类型, 即:\n\n * inbound 为真时, 表示对应的 channelhandler 实现了 channelinboundhandler 方法.\n * outbound 为真时, 表示对应的 channelhandler 实现了 channeloutboundhandler 方法.\n\n读者朋友肯定很疑惑了吧: 那究竟这两个字段有什么作用呢? 其实这还要从 channelpipeline 的传输的事件类型说起. netty 的事件可以分为 inbound 和 outbound 事件.\n\n如下是从 netty 官网上拷贝的一个图示:\n\n                                             i/o request\n                                        via channel or\n                                    channelhandlercontext\n                                                  |\n+---------------------------------------------------+---------------+\n|                           channelpipeline         |               |\n|                                                  \\|/              |\n|    +---------------------+            +-----------+----------+    |\n|    | inbound handler  n  |            | outbound handler  1  |    |\n|    +----------+----------+            +-----------+----------+    |\n|              /|\\                                  |               |\n|               |                                  \\|/              |\n|    +----------+----------+            +-----------+----------+    |\n|    | inbound handler n-1 |            | outbound handler  2  |    |\n|    +----------+----------+            +-----------+----------+    |\n|              /|\\                                  .               |\n|               .                                   .               |\n| channelhandlercontext.firein_evt() channelhandlercontext.out_evt()|\n|        [ method call]                       [method call]         |\n|               .                                   .               |\n|               .                                  \\|/              |\n|    +----------+----------+            +-----------+----------+    |\n|    | inbound handler  2  |            | outbound handler m-1 |    |\n|    +----------+----------+            +-----------+----------+    |\n|              /|\\                                  |               |\n|               |                                  \\|/              |\n|    +----------+----------+            +-----------+----------+    |\n|    | inbound handler  1  |            | outbound handler  m  |    |\n|    +----------+----------+            +-----------+----------+    |\n|              /|\\                                  |               |\n+---------------+-----------------------------------+---------------+\n              |                                  \\|/\n+---------------+-----------------------------------+---------------+\n|               |                                   |               |\n|       [ socket.read() ]                    [ socket.write() ]     |\n|                                                                   |\n|  netty internal i/o threads (transport implementation)            |\n+-------------------------------------------------------------------+\n\n\n从上图可以看出, inbound 事件和 outbound 事件的流向是不一样的, inbound 事件的流行是从下至上, 而 outbound 刚好相反, 是从上到下. 并且 inbound 的传递方式是通过调用相应的 channelhandlercontext.firein_evt() 方法, 而 outbound 方法的的传递方式是通过调用 channelhandlercontext.out_evt() 方法. 例如 channelhandlercontext.firechannelregistered() 调用会发送一个 channelregistered 的 inbound 给下一个channelhandlercontext, 而 channelhandlercontext.bind 调用会发送一个 bind 的 outbound 事件给 下一个 channelhandlercontext.\n\ninbound 事件传播方法有:\n\nchannelhandlercontext.firechannelregistered()\nchannelhandlercontext.firechannelactive()\nchannelhandlercontext.firechannelread(object)\nchannelhandlercontext.firechannelreadcomplete()\nchannelhandlercontext.fireexceptioncaught(throwable)\nchannelhandlercontext.fireusereventtriggered(object)\nchannelhandlercontext.firechannelwritabilitychanged()\nchannelhandlercontext.firechannelinactive()\nchannelhandlercontext.firechannelunregistered()\n\n\noubound 事件传输方法有:\n\nchannelhandlercontext.bind(socketaddress, channelpromise)\nchannelhandlercontext.connect(socketaddress, socketaddress, channelpromise)\nchannelhandlercontext.write(object, channelpromise)\nchannelhandlercontext.flush()\nchannelhandlercontext.read()\nchannelhandlercontext.disconnect(channelpromise)\nchannelhandlercontext.close(channelpromise)\n\n\n注意, 如果我们捕获了一个事件, 并且想让这个事件继续传递下去, 那么需要调用 context 相应的传播方法. 例如:\n\npublic class myinboundhandler extends channelinboundhandleradapter {\n    @override\n    public void channelactive(channelhandlercontext ctx) {\n        system.out.println("connected!");\n        ctx.firechannelactive();\n    }\n}\n\npublic clas myoutboundhandler extends channeloutboundhandleradapter {\n    @override\n    public void close(channelhandlercontext ctx, channelpromise promise) {\n        system.out.println("closing ..");\n        ctx.close(promise);\n    }\n}\n\n\n上面的例子中, myinboundhandler 收到了一个 channelactive 事件, 它在处理后, 如果希望将事件继续传播下去, 那么需要接着调用 ctx.firechannelactive().\n\n\n# outbound 操作\n\noutbound 事件都是请求事件(request event), 即请求某件事情的发生, 然后通过 outbound 事件进行通知. outbound 事件的传播方向是 tail -> customcontext -> head.\n\n我们接下来以 connect 事件为例, 分析一下 outbound 事件的传播机制. 首先, 当用户调用了 bootstrap.connect 方法时, 就会触发一个 connect 请求事件, 此调用会触发如下调用链:\n\nbootstrap.connect -> bootstrap.doconnect -> bootstrap.doconnect0 -> abstractchannel.connect\n\n\n继续跟踪的话, 我们就发现, abstractchannel.connect 其实由调用了 defaultchannelpipeline.connect 方法:\n\n@override\npublic channelfuture connect(socketaddress remoteaddress, channelpromise promise) {\n\treturn pipeline.connect(remoteaddress, promise);\n}\n\n\n而 pipeline.connect 的实现如下:\n\n@override\npublic channelfuture connect(socketaddress remoteaddress, channelpromise promise) {\n    return tail.connect(remoteaddress, promise);\n}\n\n\n可以看到, 当 outbound 事件(这里是 connect 事件)传递到 pipeline 后, 它其实是以 tail 为起点开始传播的. 而 tail.connect 其实调用的是 abstractchannelhandlercontext.connect 方法:\n\n@override\npublic channelfuture connect(\n        final socketaddress remoteaddress, final socketaddress localaddress, final channelpromise promise) {\n    ...\n    final abstractchannelhandlercontext next = findcontextoutbound();\n    eventexecutor executor = next.executor();\n    ...\n    next.invokeconnect(remoteaddress, localaddress, promise);\n    ...\n    return promise;\n}\n\n\nfindcontextoutbound() 顾名思义, 它的作用是以当前 context 为起点, 向 pipeline 中的 context 双向链表的前端寻找第一个 outbound 属性为真的 context(即关联着 channeloutboundhandler 的 context), 然后返回. 它的实现如下:\n\nprivate abstractchannelhandlercontext findcontextoutbound() {\n    abstractchannelhandlercontext ctx = this;\n    do {\n        ctx = ctx.prev;\n    } while (!ctx.outbound);\n    return ctx;\n}\n\n\n当我们找到了一个 outbound 的 context 后, 就调用它的 invokeconnect 方法, 这个方法中会调用 context 所关联着的 channelhandler 的 connect 方法:\n\nprivate void invokeconnect(socketaddress remoteaddress, socketaddress localaddress, channelpromise promise) {\n    try {\n        ((channeloutboundhandler) handler()).connect(this, remoteaddress, localaddress, promise);\n    } catch (throwable t) {\n        notifyoutboundhandlerexception(t, promise);\n    }\n}\n\n\n如果用户没有重写 channelhandler 的 connect 方法, 那么会调用 channeloutboundhandleradapter 所实现的方法:\n\n@override\npublic void connect(channelhandlercontext ctx, socketaddress remoteaddress,\n        socketaddress localaddress, channelpromise promise) throws exception {\n    ctx.connect(remoteaddress, localaddress, promise);\n}\n\n\n我们看到, channeloutboundhandleradapter.connect 仅仅调用了 ctx.connect, 而这个调用又回到了:\n\ncontext.connect -> connect.findcontextoutbound -> next.invokeconnect -> handler.connect -> context.connect\n\n\n这样的循环中, 直到 connect 事件传递到defaultchannelpipeline 的双向链表的头节点, 即 head 中. 为什么会传递到 head 中呢? 回想一下, head 实现了 channeloutboundhandler, 因此它的 outbound 属性是 true. 因为 head 本身既是一个 channelhandlercontext, 又实现了 channeloutboundhandler 接口, 因此当 connect 消息传递到 head 后, 会将消息转递到对应的 channelhandler 中处理, 而恰好, head 的 handler() 返回的就是 head 本身:\n\n@override\npublic channelhandler handler() {\n    return this;\n}\n\n\n因此最终 connect 事件是在 head 中处理的. head 的 connect 事件处理方法如下:\n\n@override\npublic void connect(\n        channelhandlercontext ctx,\n        socketaddress remoteaddress, socketaddress localaddress,\n        channelpromise promise) throws exception {\n    unsafe.connect(remoteaddress, localaddress, promise);\n}\n\n\n到这里, 整个 connect 请求事件就结束了. 下面以一幅图来描述一个整个 connect 请求事件的处理过程:\n\n\n\n我们仅仅以 connect 请求事件为例, 分析了 outbound 事件的传播过程, 但是其实所有的 outbound 的事件传播都遵循着一样的传播规律, 读者可以试着分析一下其他的 outbound 事件, 体会一下它们的传播过程.\n\n\n# inbound 事件\n\ninbound 事件和 outbound 事件的处理过程有点镜像. inbound 事件是一个通知事件, 即某件事已经发生了, 然后通过 inbound 事件进行通知. inbound 通常发生在 channel 的状态的改变或 io 事件就绪. inbound 的特点是它传播方向是 head -> customcontext -> tail.\n\n既然上面我们分析了 connect 这个 outbound 事件, 那么接着分析 connect 事件后会发生什么 inbound 事件, 并最终找到 outbound 和 inbound 事件之间的联系.\n\n当 connect 这个 outbound 传播到 unsafe 后, 其实是在 abstractniounsafe.connect 方法中进行处理的:\n\n@override\npublic final void connect(\n        final socketaddress remoteaddress, final socketaddress localaddress, final channelpromise promise) {\n    ...\n    if (doconnect(remoteaddress, localaddress)) {\n        fulfillconnectpromise(promise, wasactive);\n    } else {\n        ...\n    }\n    ...\n}\n\n\n在 abstractniounsafe.connect 中, 首先调用 doconnect 方法进行实际上的 socket 连接, 当连接上后, 会调用 fulfillconnectpromise 方法:\n\nprivate void fulfillconnectpromise(channelpromise promise, boolean wasactive) {\n    ...\n    // regardless if the connection attempt was cancelled, channelactive() event should be triggered,\n    // because what happened is what happened.\n    if (!wasactive && isactive()) {\n        pipeline().firechannelactive();\n    }\n    ...\n}\n\n\n我们看到, 在 fulfillconnectpromise 中, 会通过调用 pipeline().firechannelactive() 将通道激活的消息(即 socket 连接成功)发送出去. 而这里, 当调用 pipeline.firexxx 后, 就是 inbound 事件的起点. 因此当调用了 pipeline().firechannelactive() 后, 就产生了一个 channelactive inbound 事件, 我们就从这里开始看看这个 inbound 事件是怎么传播的吧.\n\n@override\npublic channelpipeline firechannelactive() {\n    head.firechannelactive();\n\n    if (channel.config().isautoread()) {\n        channel.read();\n    }\n\n    return this;\n}\n\n\n哈哈, 果然, 在 firechannelactive 方法中, 调用的是 head.firechannelactive, 因此可以证明了, inbound 事件在 pipeline 中传输的起点是 head. 那么, 在 head.firechannelactive() 中又做了什么呢?\n\n@override\npublic channelhandlercontext firechannelactive() {\n    final abstractchannelhandlercontext next = findcontextinbound();\n    eventexecutor executor = next.executor();\n    ...\n    next.invokechannelactive();\n    ...\n    return this;\n}\n\n\n上面的代码应该很熟悉了吧. 回想一下在 outbound 事件(例如 connect 事件)的传输过程中时, 我们也有类似的操作:\n\n * 首先调用 findcontextinbound, 从 pipeline 的双向链表中中找到第一个属性 inbound 为真的 context, 然后返回\n * 调用这个 context 的 invokechannelactive\n\ninvokechannelactive 方法如下:\n\nprivate void invokechannelactive() {\n    try {\n        ((channelinboundhandler) handler()).channelactive(this);\n    } catch (throwable t) {\n        notifyhandlerexception(t);\n    }\n}\n\n\n这个方法和 outbound 的对应方法(例如 invokeconnect) 如出一辙. 同 outbound 一样, 如果用户没有重写 channelactive 方法, 那么会调用 channelinboundhandleradapter 的 channelactive 方法:\n\n@override\npublic void channelactive(channelhandlercontext ctx) throws exception {\n    ctx.firechannelactive();\n}\n\n\n同样地, 在 channelinboundhandleradapter.channelactive 中, 仅仅调用了 ctx.firechannelactive 方法, 因此就会有如下循环:\n\ncontext.firechannelactive -> connect.findcontextinbound -> nextcontext.invokechannelactive -> nexthandler.channelactive -> nextcontext.firechannelactive\n\n\n这样的循环中. 同理, tail 本身 既实现了 channelinboundhandler 接口, 又实现了 channelhandlercontext 接口, 因此当 channelactive 消息传递到 tail 后, 会将消息转递到对应的 channelhandler 中处理, 而恰好, tail 的 handler() 返回的就是 tail 本身:\n\n@override\npublic channelhandler handler() {\n    return this;\n}\n\n\n因此 channelactive inbound 事件最终是在 tail 中处理的, 我们看一下它的处理方法:\n\n@override\npublic void channelactive(channelhandlercontext ctx) throws exception { }\n\n\ntailcontext.channelactive 方法是空的. 如果读者自行查看 tailcontext 的 inbound 处理方法时, 会发现, 它们的实现都是空的. 可见, 如果是 inbound, 当用户没有实现自定义的处理器时, 那么默认是不处理的.\n\n用一幅图来总结一下 inbound 的传输过程吧:\n\n\n\n\n# 总结\n\n对于 outbound事件:\n\n * outbound 事件是请求事件(由 connect 发起一个请求, 并最终由 unsafe 处理这个请求)\n * outbound 事件的发起者是 channel\n * outbound 事件的处理者是 unsafe\n * outbound 事件在 pipeline 中的传输方向是 tail -> head.\n * 在 channelhandler 中处理事件时, 如果这个 handler 不是最后一个 hnalder, 则需要调用 ctx.xxx (例如 ctx.connect) 将此事件继续传播下去. 如果不这样做, 那么此事件的传播会提前终止.\n * outbound 事件流: context.out_evt -> connect.findcontextoutbound -> nextcontext.invokeout_evt -> nexthandler.out_evt -> nextcontext.out_evt\n\n对于 inbound 事件:\n\n * inbound 事件是通知事件, 当某件事情已经就绪后, 通知上层.\n * inbound 事件发起者是 unsafe\n * inbound 事件的处理者是 channel, 如果用户没有实现自定义的处理方法, 那么inbound 事件默认的处理者是 tailcontext, 并且其处理方法是空实现.\n * inbound 事件在 pipeline 中传输方向是 head -> tail\n * 在 channelhandler 中处理事件时, 如果这个 handler 不是最后一个 hnalder, 则需要调用 ctx.firein_evt (例如 ctx.firechannelactive) 将此事件继续传播下去. 如果不这样做, 那么此事件的传播会提前终止.\n * outbound 事件流: context.firein_evt -> connect.findcontextinbound -> nextcontext.invokein_evt -> nexthandler.in_evt -> nextcontext.firein_evt\n\noutbound 和 inbound 事件十分的镜像, 并且 context 与 handler 直接的调用关系是否容易混淆, 因此读者在阅读这里的源码时, 需要特别的注意.\n\n\n# 总结\n\n\n# 参考资料',charsets:{cjk:!0},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"ByteBuf 源码解析",frontmatter:{title:"ByteBuf 源码解析",date:"2024-09-18T21:12:36.000Z",permalink:"/pages/43eb30/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/20.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/40.Bytebuf%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html",relativePath:"Netty 系统设计/20.三、主线任务/40.Bytebuf 源码解析.md",key:"v-3da569d0",path:"/pages/43eb30/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 16:19:18",lastUpdatedTimestamp:1726676358e3},{title:"第一课：透过 Netty 看 IO 模型",frontmatter:{title:"第一课：透过 Netty 看 IO 模型",date:"2024-09-18T21:07:35.000Z",permalink:"/pages/65674f/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/25.%E5%9B%9B%E3%80%81%E6%B7%B1%E5%85%A5%20Netty%20%E6%A0%B8%E5%BF%83/05.%E7%AC%AC%E4%B8%80%E8%AF%BE%EF%BC%9A%E9%80%8F%E8%BF%87%20Netty%20%E7%9C%8B%20IO%20%E6%A8%A1%E5%9E%8B.html",relativePath:"Netty 系统设计/25.四、深入 Netty 核心/05.第一课：透过 Netty 看 IO 模型.md",key:"v-598109b1",path:"/pages/65674f/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"网络包接收流程",slug:"网络包接收流程",normalizedTitle:"网络包接收流程",charIndex:285},{level:3,title:"性能开销",slug:"性能开销",normalizedTitle:"性能开销",charIndex:1665},{level:2,title:"网络包发送流程",slug:"网络包发送流程",normalizedTitle:"网络包发送流程",charIndex:1932},{level:3,title:"性能开销",slug:"性能开销-2",normalizedTitle:"性能开销",charIndex:1665},{level:2,title:"再谈(阻塞，非阻塞)与(同步，异步)",slug:"再谈-阻塞-非阻塞-与-同步-异步",normalizedTitle:"再谈(阻塞，非阻塞)与(同步，异步)",charIndex:5667},{level:3,title:"阻塞与非阻塞",slug:"阻塞与非阻塞",normalizedTitle:"阻塞与非阻塞",charIndex:5881},{level:4,title:"阻塞",slug:"阻塞",normalizedTitle:"阻塞",charIndex:1585},{level:4,title:"非阻塞",slug:"非阻塞",normalizedTitle:"非阻塞",charIndex:5673},{level:3,title:"同步与异步",slug:"同步与异步",normalizedTitle:"同步与异步",charIndex:5891},{level:4,title:"同步",slug:"同步",normalizedTitle:"同步",charIndex:5679},{level:4,title:"异步",slug:"异步",normalizedTitle:"异步",charIndex:43},{level:2,title:"IO模型",slug:"io模型",normalizedTitle:"io模型",charIndex:234},{level:3,title:"阻塞IO（BIO）",slug:"阻塞io-bio",normalizedTitle:"阻塞io（bio）",charIndex:7434},{level:4,title:"阻塞读",slug:"阻塞读",normalizedTitle:"阻塞读",charIndex:7533},{level:4,title:"阻塞写",slug:"阻塞写",normalizedTitle:"阻塞写",charIndex:7786},{level:4,title:"阻塞IO模型",slug:"阻塞io模型",normalizedTitle:"阻塞io模型",charIndex:7511},{level:4,title:"适用场景",slug:"适用场景",normalizedTitle:"适用场景",charIndex:8370},{level:3,title:"非阻塞IO（NIO）",slug:"非阻塞io-nio",normalizedTitle:"非阻塞io（nio）",charIndex:8500},{level:4,title:"非阻塞读",slug:"非阻塞读",normalizedTitle:"非阻塞读",charIndex:8760},{level:4,title:"非阻塞写",slug:"非阻塞写",normalizedTitle:"非阻塞写",charIndex:9019},{level:4,title:"非阻塞IO模型",slug:"非阻塞io模型",normalizedTitle:"非阻塞io模型",charIndex:9246},{level:4,title:"适用场景",slug:"适用场景-2",normalizedTitle:"适用场景",charIndex:8370},{level:3,title:"IO 多路复用",slug:"io-多路复用",normalizedTitle:"io 多路复用",charIndex:9706},{level:3,title:"信号驱动IO",slug:"信号驱动io",normalizedTitle:"信号驱动io",charIndex:7346},{level:3,title:"异步IO（AIO）",slug:"异步io-aio",normalizedTitle:"异步io（aio）",charIndex:10728},{level:2,title:"IO 线程模型",slug:"io-线程模型",normalizedTitle:"io 线程模型",charIndex:11371},{level:3,title:"Reactor",slug:"reactor",normalizedTitle:"reactor",charIndex:11618},{level:4,title:"单Reactor单线程",slug:"单reactor单线程",normalizedTitle:"单reactor单线程",charIndex:11879},{level:4,title:"单Reactor多线程",slug:"单reactor多线程",normalizedTitle:"单reactor多线程",charIndex:12242},{level:4,title:"主从Reactor多线程",slug:"主从reactor多线程",normalizedTitle:"主从reactor多线程",charIndex:12524},{level:3,title:"Proactor",slug:"proactor",normalizedTitle:"proactor",charIndex:13281},{level:3,title:"Reactor与Proactor对比",slug:"reactor与proactor对比",normalizedTitle:"reactor与proactor对比",charIndex:14412},{level:2,title:"Netty的IO模型",slug:"netty的io模型",normalizedTitle:"netty的io模型",charIndex:14711},{level:3,title:"配置单Reactor单线程",slug:"配置单reactor单线程",normalizedTitle:"配置单reactor单线程",charIndex:16273},{level:3,title:"配置多Reactor线程",slug:"配置多reactor线程",normalizedTitle:"配置多reactor线程",charIndex:16440},{level:3,title:"配置主从Reactor多线程",slug:"配置主从reactor多线程",normalizedTitle:"配置主从reactor多线程",charIndex:16605},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:5929},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:17089}],headersStr:"前言 网络包接收流程 性能开销 网络包发送流程 性能开销 再谈(阻塞，非阻塞)与(同步，异步) 阻塞与非阻塞 阻塞 非阻塞 同步与异步 同步 异步 IO模型 阻塞IO（BIO） 阻塞读 阻塞写 阻塞IO模型 适用场景 非阻塞IO（NIO） 非阻塞读 非阻塞写 非阻塞IO模型 适用场景 IO 多路复用 信号驱动IO 异步IO（AIO） IO 线程模型 Reactor 单Reactor单线程 单Reactor多线程 主从Reactor多线程 Proactor Reactor与Proactor对比 Netty的IO模型 配置单Reactor单线程 配置多Reactor线程 配置主从Reactor多线程 总结 参考资料",content:"# 前言\n\n从今天开始我们来聊聊Netty的那些事儿，我们都知道Netty是一个高性能异步事件驱动的网络框架。\n\n它的设计异常优雅简洁，扩展性高，稳定性强。拥有非常详细完整的用户文档。\n\n同时内置了很多非常有用的模块基本上做到了开箱即用，用户只需要编写短短几行代码，就可以快速构建出一个具有高吞吐，低延时，更少的资源消耗，高性能（非必要的内存拷贝最小化）等特征的高并发网络应用程序。\n\n本文我们来探讨下支持Netty具有高吞吐，低延时特征的基石----netty的网络IO模型。\n\n由Netty的网络IO模型开始，我们来正式揭开本系列Netty源码解析的序幕：\n\n\n# 网络包接收流程\n\n\n\n * 当网络数据帧通过网络传输到达网卡时，网卡会将网络数据帧通过DMA的方式放到环形缓冲区RingBuffer中\n\n笔记\n\nRingBuffer是网卡在启动的时候分配和初始化的环形缓冲队列。当RingBuffer满的时候，新来的数据包就会被丢弃。我们可以通过ifconfig命令查看网卡收发数据包的情况。其中overruns数据项表示当RingBuffer满时，被丢弃的数据包。如果发现出现丢包情况，可以通过ethtool命令来增大RingBuffer长度。\n\n * 当DMA操作完成时，网卡会向CPU发起一个硬中断，告诉CPU有网络数据到达。CPU调用网卡驱动注册的硬中断响应程序。网卡硬中断响应程序会为网络数据帧创建内核数据结构sk_buffer，并将网络数据帧拷贝到sk_buffer中。然后发起软中断请求，通知内核有新的网络数据帧到达。\n\n笔记\n\nsk_buff缓冲区，是一个维护网络帧结构的双向链表，链表中的每一个元素都是一个网络帧。虽然 TCP/IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而无需进行数据复制。\n\n * 内核线程ksoftirqd发现有软中断请求到来，随后调用网卡驱动注册的poll函数，poll函数将sk_buffer中的网络数据包送到内核协议栈中注册的ip_rcv函数中。\n\n笔记\n\n每个CPU会绑定一个ksoftirqd内核线程专门用来处理软中断响应。2个 CPU 时，就会有 ksoftirqd/0 和 ksoftirqd/1这两个内核线程。\n\n笔记\n\n这里有个事情需要注意下： 网卡接收到数据后，当DMA拷贝完成时，向CPU发出硬中断，这时哪个CPU上响应了这个硬中断，那么在网卡硬中断响应程序中发出的软中断请求也会在这个CPU绑定的ksoftirqd线程中响应。所以如果发现Linux软中断，CPU消耗都集中在一个核上的话，那么就需要调整硬中断的CPU亲和性，来将硬中断打散到不通的CPU核上去。\n\n * 在ip_rcv函数中也就是上图中的网络层，取出数据包的IP头，判断该数据包下一跳的走向，如果数据包是发送给本机的，则取出传输层的协议类型（TCP或者UDP)，并去掉数据包的IP头，将数据包交给上图中得传输层处理。\n\n笔记\n\n传输层的处理函数：TCP协议对应内核协议栈中注册的tcp_rcv函数，UDP协议对应内核协议栈中注册的udp_rcv函数。\n\n * 当我们采用的是TCP协议时，数据包到达传输层时，会在内核协议栈中的tcp_rcv函数处理，在tcp_rcv函数中去掉TCP头，根据四元组（源IP，源端口，目的IP，目的端口）查找对应的Socket，如果找到对应的Socket则将网络数据包中的传输数据拷贝到Socket中的接收缓冲区中。如果没有找到，则发送一个目标不可达的icmp包。\n\n内核在接收网络数据包时所做的工作我们就介绍完了，现在我们把视角放到应用层，当我们程序通过系统调用read读取Socket接收缓冲区中的数据时，如果接收缓冲区中没有数据，那么应用程序就会在系统调用上阻塞，直到Socket接收缓冲区有数据，然后CPU将内核空间（Socket接收缓冲区）的数据拷贝到用户空间，最后系统调用read返回，应用程序读取数据\n\n\n# 性能开销\n\n从内核处理网络数据包接收的整个过程来看，内核帮我们做了非常之多的工作，最终我们的应用程序才能读取到网络数据。\n\n随着而来的也带来了很多的性能开销，结合前面介绍的网络数据包接收过程我们来看下网络数据包接收的过程中都有哪些性能开销：\n\n * 应用程序通过系统调用从用户态转为内核态的开销以及系统调用返回时从内核态转为用户态的开销。\n * 网络数据从内核空间通过CPU拷贝到用户空间的开销。\n * 内核线程ksoftirqd响应软中断的开销。\n * CPU响应硬中断的开销。\n * DMA拷贝网络数据包到内存中的开销。\n\n\n# 网络包发送流程\n\n\n\n * 当我们在应用程序中调用send系统调用发送数据时，由于是系统调用所以线程会发生一次用户态到内核态的转换，在内核中首先根据fd将真正的Socket找出，这个Socket对象中记录着各种协议栈的函数地址，然后构造struct msghdr对象，将用户需要发送的数据全部封装在这个struct msghdr结构体中。\n * 调用内核协议栈函数inet_sendmsg，发送流程进入内核协议栈处理。在进入到内核协议栈之后，内核会找到Socket上的具体协议的发送函数。\n\n> 比如：我们使用的是TCP协议，对应的TCP协议发送函数是tcp_sendmsg，如果是UDP协议的话，对应的发送函数为udp_sendmsg。\n\n * 在TCP协议的发送函数tcp_sendmsg中，创建内核数据结构sk_buffer,将struct msghdr结构体中的发送数据拷贝到sk_buffer中。调用tcp_write_queue_tail函数获取Socket发送队列中的队尾元素，将新创建的sk_buffer添加到Socket发送队列的尾部。\n\n> Socket的发送队列是由sk_buffer组成的一个双向链表。\n\n> 发送流程走到这里，用户要发送的数据总算是从用户空间拷贝到了内核中，这时虽然发送数据已经拷贝到了内核Socket中的发送队列中，但并不代表内核会开始发送，因为TCP协议的流量控制和拥塞控制，用户要发送的数据包并不一定会立马被发送出去，需要符合TCP协议的发送条件。如果没有达到发送条件，那么本次send系统调用就会直接返回。\n\n * 如果符合发送条件，则开始调用tcp_write_xmit内核函数。在这个函数中，会循环获取Socket发送队列中待发送的sk_buffer，然后进行拥塞控制以及滑动窗口的管理。\n * 将从Socket发送队列中获取到的sk_buffer重新拷贝一份，设置sk_buffer副本中的TCP HEADER。\n\n> sk_buffer 内部其实包含了网络协议中所有的 header。在设置 TCP HEADER的时候，只是把指针指向 sk_buffer的合适位置。后面再设置 IP HEADER的时候，在把指针移动一下就行，避免频繁的内存申请和拷贝，效率很高。\n\n\n\n> **为什么不直接使用Socket发送队列中的sk_buffer而是需要拷贝一份呢？**因为TCP协议是支持丢包重传的，在没有收到对端的ACK之前，这个sk_buffer是不能删除的。内核每次调用网卡发送数据的时候，实际上传递的是sk_buffer的拷贝副本，当网卡把数据发送出去后，sk_buffer拷贝副本会被释放。当收到对端的ACK之后，Socket发送队列中的sk_buffer才会被真正删除。\n\n * 当设置完TCP头后，内核协议栈传输层的事情就做完了，下面通过调用ip_queue_xmit内核函数，正式来到内核协议栈网络层的处理。\n   \n   > 通过route命令可以查看本机路由配置。\n   \n   > 如果你使用 iptables配置了一些规则，那么这里将检测是否命中规则。如果你设置了非常复杂的 netfilter 规则，在这个函数里将会导致你的线程 CPU 开销会极大增加。\n\n * * 将sk_buffer中的指针移动到IP头位置上，设置IP头。\n   * 执行netfilters过滤。过滤通过之后，如果数据大于 MTU的话，则执行分片。\n   * 检查Socket中是否有缓存路由表，如果没有的话，则查找路由项，并缓存到Socket中。接着在把路由表设置到sk_buffer中。\n\n * 内核协议栈网络层的事情处理完后，现在发送流程进入了到了邻居子系统，邻居子系统位于内核协议栈中的网络层和网络接口层之间，用于发送ARP请求获取MAC地址，然后将sk_buffer中的指针移动到MAC头位置，填充MAC头。\n\n * 经过邻居子系统的处理，现在sk_buffer中已经封装了一个完整的数据帧，随后内核将sk_buffer交给网络设备子系统进行处理。网络设备子系统主要做以下几项事情：\n\n * * 选择发送队列（RingBuffer）。因为网卡拥有多个发送队列，所以在发送前需要选择一个发送队列。\n   * 将sk_buffer添加到发送队列中。\n   * 循环从发送队列（RingBuffer）中取出sk_buffer，调用内核函数sch_direct_xmit发送数据，其中会调用网卡驱动程序来发送数据。\n\n> 以上过程全部是用户线程的内核态在执行，占用的CPU时间是系统态时间(sy)，当分配给用户线程的CPU quota用完的时候，会触发NET_TX_SOFTIRQ类型的软中断，内核线程ksoftirqd会响应这个软中断，并执行NET_TX_SOFTIRQ类型的软中断注册的回调函数net_tx_action，在回调函数中会执行到驱动程序函数 dev_hard_start_xmit来发送数据。\n\n> 注意：当触发NET_TX_SOFTIRQ软中断来发送数据时，后边消耗的 CPU 就都显示在 si这里了，不会消耗用户进程的系统态时间（sy）了。\n\n> 从这里可以看到网络包的发送过程和接受过程是不同的，在介绍网络包的接受过程时，我们提到是通过触发NET_RX_SOFTIRQ类型的软中断在内核线程ksoftirqd中执行内核网络协议栈接受数据。而在网络数据包的发送过程中是用户线程的内核态在执行内核网络协议栈，只有当线程的CPU quota用尽时，才触发NET_TX_SOFTIRQ软中断来发送数据。\n\n> 在整个网络包的发送和接受过程中，NET_TX_SOFTIRQ类型的软中断只会在发送网络包时并且当用户线程的CPU quota用尽时，才会触发。剩下的接受过程中触发的软中断类型以及发送完数据触发的软中断类型均为NET_RX_SOFTIRQ。所以这就是你在服务器上查看 /proc/softirqs，一般 NET_RX都要比 NET_TX大很多的的原因\n\n * 现在发送流程终于到了网卡真实发送数据的阶段，前边我们讲到无论是用户线程的内核态还是触发NET_TX_SOFTIRQ类型的软中断在发送数据的时候最终会调用到网卡的驱动程序函数dev_hard_start_xmit来发送数据。在网卡驱动程序函数dev_hard_start_xmit中会将sk_buffer映射到网卡可访问的内存 DMA 区域，最终网卡驱动程序通过DMA的方式将数据帧通过物理网卡发送出去。\n * 当数据发送完毕后，还有最后一项重要的工作，就是清理工作。数据发送完毕后，网卡设备会向CPU发送一个硬中断，CPU调用网卡驱动程序注册的硬中断响应程序，在硬中断响应中触发NET_RX_SOFTIRQ类型的软中断，在软中断的回调函数igb_poll中清理释放 sk_buffer，清理网卡发送队列（RingBuffer），解除 DMA 映射。\n\n> 无论硬中断是因为有数据要接收，还是说发送完成通知，从硬中断触发的软中断都是 NET_RX_SOFTIRQ。\n\n> 这里释放清理的只是sk_buffer的副本，真正的sk_buffer现在还是存放在Socket的发送队列中。前面在传输层处理的时候我们提到过，因为传输层需要保证可靠性，所以 sk_buffer其实还没有删除。它得等收到对方的 ACK 之后才会真正删除。\n\n\n# 性能开销\n\n前边我们提到了在网络包接收过程中涉及到的性能开销，现在介绍完了网络包的发送过程，我们来看下在数据包发送过程中的性能开销：\n\n * 和接收数据一样，应用程序在调用系统调用send的时候会从用户态转为内核态以及发送完数据后，系统调用返回时从内核态转为用户态的开销。\n\n * 用户线程内核态CPU quota用尽时触发NET_TX_SOFTIRQ类型软中断，内核响应软中断的开销。\n\n * 网卡发送完数据，向CPU发送硬中断，CPU响应硬中断的开销。以及在硬中断中发送NET_RX_SOFTIRQ软中断执行具体的内存清理动作。内核响应软中断的开销。\n\n * 内存拷贝的开销。我们来回顾下在数据包发送的过程中都发生了哪些内存拷贝：\n\n * * 在内核协议栈的传输层中，TCP协议对应的发送函数tcp_sendmsg会申请sk_buffer，将用户要发送的数据拷贝到sk_buffer中。\n   * 在发送流程从传输层到网络层的时候，会拷贝一个sk_buffer副本出来，将这个sk_buffer副本向下传递。原始sk_buffer保留在Socket发送队列中，等待网络对端ACK，对端ACK后删除Socket发送队列中的sk_buffer。对端没有发送ACK，则重新从Socket发送队列中发送，实现TCP协议的可靠传输。\n   * 在网络层，如果发现要发送的数据大于MTU，则会进行分片操作，申请额外的sk_buffer，并将原来的sk_buffer拷贝到多个小的sk_buffer中。\n\n\n# 再谈(阻塞，非阻塞)与(同步，异步)\n\n在我们聊完网络数据的接收和发送过程后，我们来谈下IO中特别容易混淆的概念：阻塞与同步，非阻塞与异步。\n\n网上各种博文还有各种书籍中有大量的关于这两个概念的解释，但是笔者觉得还是不够形象化，只是对概念的生硬解释，如果硬套概念的话，其实感觉阻塞与同步，非阻塞与异步还是没啥区别，时间长了，还是比较模糊容易混淆。\n\n所以笔者在这里尝试换一种更加形象化，更加容易理解记忆的方式来清晰地解释下什么是阻塞与非阻塞，什么是同步与异步。\n\n经过前边对网络数据包接收流程的介绍，在这里我们可以将整个流程总结为两个阶段：\n\n\n\n * 数据准备阶段： 在这个阶段，网络数据包到达网卡，通过DMA的方式将数据包拷贝到内存中，然后经过硬中断，软中断，接着通过内核线程ksoftirqd经过内核协议栈的处理，最终将数据发送到内核Socket的接收缓冲区中。\n * 数据拷贝阶段： 当数据到达内核Socket的接收缓冲区中时，此时数据存在于内核空间中，需要将数据拷贝到用户空间中，才能够被应用程序读取。\n\n\n# 阻塞与非阻塞\n\n阻塞与非阻塞的区别主要发生在第一阶段：数据准备阶段。\n\n当应用程序发起系统调用read时，线程从用户态转为内核态，读取内核Socket的接收缓冲区中的网络数据。\n\n# 阻塞\n\n如果这时内核Socket的接收缓冲区没有数据，那么线程就会一直等待，直到Socket接收缓冲区有数据为止。随后将数据从内核空间拷贝到用户空间，系统调用read返回。\n\n\n\n从图中我们可以看出：阻塞的特点是在第一阶段和第二阶段都会等待。\n\n# 非阻塞\n\n阻塞和非阻塞主要的区分是在第一阶段：数据准备阶段。\n\n * 在第一阶段，当Socket的接收缓冲区中没有数据的时候，阻塞模式下应用线程会一直等待。非阻塞模式下应用线程不会等待，系统调用直接返回错误标志EWOULDBLOCK。\n * 当Socket的接收缓冲区中有数据的时候，阻塞和非阻塞的表现是一样的，都会进入第二阶段等待数据从内核空间拷贝到用户空间，然后系统调用返回。\n\n\n\n从上图中，我们可以看出：非阻塞的特点是第一阶段不会等待，但是在第二阶段还是会等待。\n\n\n# 同步与异步\n\n同步与异步主要的区别发生在第二阶段：数据拷贝阶段。\n\n前边我们提到在数据拷贝阶段主要是将数据从内核空间拷贝到用户空间。然后应用程序才可以读取数据。\n\n当内核Socket的接收缓冲区有数据到达时，进入第二阶段。\n\n# 同步\n\n同步模式在数据准备好后，是由用户线程的内核态来执行第二阶段。所以应用程序会在第二阶段发生阻塞，直到数据从内核空间拷贝到用户空间，系统调用才会返回。\n\nLinux下的 epoll和Mac 下的 kqueue都属于同步 IO。\n\n\n\n# 异步\n\n异步模式下是由内核来执行第二阶段的数据拷贝操作，当内核执行完第二阶段，会通知用户线程IO操作已经完成，并将数据回调给用户线程。所以在异步模式下 数据准备阶段和数据拷贝阶段均是由内核来完成，不会对应用程序造成任何阻塞。\n\n基于以上特征，我们可以看到异步模式需要内核的支持，比较依赖操作系统底层的支持。\n\n在目前流行的操作系统中，只有Windows 中的 IOCP才真正属于异步 IO，实现的也非常成熟。但Windows很少用来作为服务器使用。\n\n而常用来作为服务器使用的Linux，异步IO机制实现的不够成熟，与NIO相比性能提升的也不够明显。\n\n但Linux kernel 在5.1版本由Facebook的大神Jens Axboe引入了新的异步IO库io_uring 改善了原来Linux native AIO的一些性能问题。性能相比Epoll以及之前原生的AIO提高了不少，值得关注。\n\n\n\n\n# IO模型\n\n在进行网络IO操作时，用什么样的IO模型来读写数据将在很大程度上决定了网络框架的IO性能。所以IO模型的选择是构建一个高性能网络框架的基础。\n\n在《UNIX 网络编程》一书中介绍了五种IO模型：阻塞IO,非阻塞IO,IO多路复用,信号驱动IO,异步IO，每一种IO模型的出现都是对前一种的升级优化。\n\n下面我们就来分别介绍下这五种IO模型各自都解决了什么问题，适用于哪些场景，各自的优缺点是什么？\n\n\n# 阻塞IO（BIO）\n\n\n\n经过前一小节对阻塞这个概念的介绍，相信大家可以很容易理解阻塞IO的概念和过程。\n\n既然这小节我们谈的是IO，那么下边我们来看下在阻塞IO模型下，网络数据的读写过程。\n\n# 阻塞读\n\n当用户线程发起read系统调用，用户线程从用户态切换到内核态，在内核中去查看Socket接收缓冲区是否有数据到来。\n\n * Socket接收缓冲区中有数据，则用户线程在内核态将内核空间中的数据拷贝到用户空间，系统IO调用返回。\n * Socket接收缓冲区中无数据，则用户线程让出CPU，进入阻塞状态。当数据到达Socket接收缓冲区后，内核唤醒阻塞状态中的用户线程进入就绪状态，随后经过CPU的调度获取到CPU quota进入运行状态，将内核空间的数据拷贝到用户空间，随后系统调用返回。\n\n# 阻塞写\n\n当用户线程发起send系统调用时，用户线程从用户态切换到内核态，将发送数据从用户空间拷贝到内核空间中的Socket发送缓冲区中。\n\n * 当Socket发送缓冲区能够容纳下发送数据时，用户线程会将全部的发送数据写入Socket缓冲区，然后执行在《网络包发送流程》这小节介绍的后续流程，然后返回。\n * 当Socket发送缓冲区空间不够，无法容纳下全部发送数据时，用户线程让出CPU,进入阻塞状态，直到Socket发送缓冲区能够容纳下全部发送数据时，内核唤醒用户线程，执行后续发送流程。\n\n阻塞IO模型下的写操作做事风格比较硬刚，非得要把全部的发送数据写入发送缓冲区才肯善罢甘休。\n\n# 阻塞IO模型\n\n\n\n由于阻塞IO的读写特点，所以导致在阻塞IO模型下，每个请求都需要被一个独立的线程处理。一个线程在同一时刻只能与一个连接绑定。来一个请求，服务端就需要创建一个线程用来处理请求。\n\n当客户端请求的并发量突然增大时，服务端在一瞬间就会创建出大量的线程，而创建线程是需要系统资源开销的，这样一来就会一瞬间占用大量的系统资源。\n\n如果客户端创建好连接后，但是一直不发数据，通常大部分情况下，网络连接也并不总是有数据可读，那么在空闲的这段时间内，服务端线程就会一直处于阻塞状态，无法干其他的事情。CPU也无法得到充分的发挥，同时还会导致大量线程切换的开销。\n\n# 适用场景\n\n基于以上阻塞IO模型的特点，该模型只适用于连接数少，并发度低的业务场景。\n\n比如公司内部的一些管理系统，通常请求数在100个左右，使用阻塞IO模型还是非常适合的。而且性能还不输NIO。\n\n该模型在C10K之前，是普遍被采用的一种IO模型。\n\n\n# 非阻塞IO（NIO）\n\n阻塞IO模型最大的问题就是一个线程只能处理一个连接，如果这个连接上没有数据的话，那么这个线程就只能阻塞在系统IO调用上，不能干其他的事情。这对系统资源来说，是一种极大的浪费。同时大量的线程上下文切换，也是一个巨大的系统开销。\n\n所以为了解决这个问题，我们就需要用尽可能少的线程去处理更多的连接。，网络IO模型的演变也是根据这个需求来一步一步演进的。\n\n基于这个需求，第一种解决方案非阻塞IO就出现了。我们在上一小节中介绍了非阻塞的概念，现在我们来看下网络读写操作在非阻塞IO下的特点：\n\n\n\n# 非阻塞读\n\n当用户线程发起非阻塞read系统调用时，用户线程从用户态转为内核态，在内核中去查看Socket接收缓冲区是否有数据到来。\n\n * Socket接收缓冲区中无数据，系统调用立马返回，并带有一个 EWOULDBLOCK 或 EAGAIN错误，这个阶段用户线程不会阻塞，也不会让出CPU，而是会继续轮训直到Socket接收缓冲区中有数据为止。\n * Socket接收缓冲区中有数据，用户线程在内核态会将内核空间中的数据拷贝到用户空间，注意这个数据拷贝阶段，应用程序是阻塞的，当数据拷贝完成，系统调用返回。\n\n# 非阻塞写\n\n前边我们在介绍阻塞写的时候提到阻塞写的风格特别的硬朗，头比较铁非要把全部发送数据一次性都写到Socket的发送缓冲区中才返回，如果发送缓冲区中没有足够的空间容纳，那么就一直阻塞死等，特别的刚。\n\n相比较而言非阻塞写的特点就比较佛系，当发送缓冲区中没有足够的空间容纳全部发送数据时，非阻塞写的特点是能写多少写多少，写不下了，就立即返回。并将写入到发送缓冲区的字节数返回给应用程序，方便用户线程不断的轮训尝试将剩下的数据写入发送缓冲区中。\n\n# 非阻塞IO模型\n\n\n\n基于以上非阻塞IO的特点，我们就不必像阻塞IO那样为每个请求分配一个线程去处理连接上的读写了。\n\n我们可以利用一个线程或者很少的线程，去不断地轮询每个Socket的接收缓冲区是否有数据到达，如果没有数据，不必阻塞线程，而是接着去轮询下一个Socket接收缓冲区，直到轮询到数据后，处理连接上的读写，或者交给业务线程池去处理，轮询线程则继续轮询其他的Socket接收缓冲区。\n\n这样一个非阻塞IO模型就实现了我们在本小节开始提出的需求：我们需要用尽可能少的线程去处理更多的连接\n\n# 适用场景\n\n虽然非阻塞IO模型与阻塞IO模型相比，减少了很大一部分的资源消耗和系统开销。\n\n但是它仍然有很大的性能问题，因为在非阻塞IO模型下，需要用户线程去不断地发起系统调用去轮训Socket接收缓冲区，这就需要用户线程不断地从用户态切换到内核态，内核态切换到用户态。随着并发量的增大，这个上下文切换的开销也是巨大的。\n\n所以单纯的非阻塞IO模型还是无法适用于高并发的场景。只能适用于C10K以下的场景。\n\n\n# IO 多路复用\n\n\n# 信号驱动IO\n\n信号驱动IO.png\n\n大家对这个装备肯定不会陌生，当我们去一些美食城吃饭的时候，点完餐付了钱，老板会给我们一个信号器。然后我们带着这个信号器可以去找餐桌，或者干些其他的事情。当信号器亮了的时候，这时代表饭餐已经做好，我们可以去窗口取餐了。\n\n这个典型的生活场景和我们要介绍的信号驱动IO模型就很像。\n\n在信号驱动IO模型下，用户进程操作通过系统调用 sigaction 函数发起一个 IO 请求，在对应的socket注册一个信号回调，此时不阻塞用户进程，进程会继续工作。当内核数据就绪时，内核就为该进程生成一个 SIGIO 信号，通过信号回调通知进程进行相关 IO 操作。\n\n> 这里需要注意的是：信号驱动式 IO 模型依然是同步IO，因为它虽然可以在等待数据的时候不被阻塞，也不会频繁的轮询，但是当数据就绪，内核信号通知后，用户进程依然要自己去读取数据，在数据拷贝阶段发生阻塞。\n\n> 信号驱动 IO模型 相比于前三种 IO 模型，实现了在等待数据就绪时，进程不被阻塞，主循环可以继续工作，所以理论上性能更佳。\n\n但是实际上，使用TCP协议通信时，信号驱动IO模型几乎不会被采用。原因如下：\n\n * 信号IO 在大量 IO 操作时可能会因为信号队列溢出导致没法通知\n * SIGIO 信号是一种 Unix 信号，信号没有附加信息，如果一个信号源有多种产生信号的原因，信号接收者就无法确定究竟发生了什么。而 TCP socket 生产的信号事件有七种之多，这样应用程序收到 SIGIO，根本无从区分处理。\n\n但信号驱动IO模型可以用在 UDP通信上，因为UDP 只有一个数据请求事件，这也就意味着在正常情况下 UDP 进程只要捕获 SIGIO 信号，就调用 read 系统调用读取到达的数据。如果出现异常，就返回一个异常错误。\n\n----------------------------------------\n\n这里插句题外话，大家觉不觉得阻塞IO模型在生活中的例子就像是我们在食堂排队打饭。你自己需要排队去打饭同时打饭师傅在配菜的过程中你需要等待。\n\n阻塞IO.png\n\nIO多路复用模型就像是我们在饭店门口排队等待叫号。叫号器就好比select,poll,epoll可以统一管理全部顾客的吃饭就绪事件，客户好比是socket连接，谁可以去吃饭了，叫号器就通知谁。\n\nIO多路复用.png\n\n\n# 异步IO（AIO）\n\n以上介绍的四种IO模型均为同步IO，它们都会阻塞在第二阶段数据拷贝阶段。\n\n通过在前边小节《同步与异步》中的介绍，相信大家很容易就会理解异步IO模型，在异步IO模型下，IO操作在数据准备阶段和数据拷贝阶段均是由内核来完成，不会对应用程序造成任何阻塞。应用进程只需要在指定的数组中引用数据即可。\n\n异步 IO 与信号驱动 IO 的主要区别在于：信号驱动 IO 由内核通知何时可以开始一个 IO 操作，而异步 IO由内核通知 IO 操作何时已经完成。\n\n举个生活中的例子：异步IO模型就像我们去一个高档饭店里的包间吃饭，我们只需要坐在包间里面，点完餐（类比异步IO调用）之后，我们就什么也不需要管，该喝酒喝酒，该聊天聊天，饭餐做好后服务员（类比内核）会自己给我们送到包间（类比用户空间）来。整个过程没有任何阻塞。\n\n异步IO.png\n\n异步IO的系统调用需要操作系统内核来支持，目前只有Window中的IOCP实现了非常成熟的异步IO机制。\n\n而Linux系统对异步IO机制实现的不够成熟，且与NIO的性能相比提升也不明显。\n\n> 但Linux kernel 在5.1版本由Facebook的大神Jens Axboe引入了新的异步IO库io_uring 改善了原来Linux native AIO的一些性能问题。性能相比Epoll以及之前原生的AIO提高了不少，值得关注。\n\n再加上信号驱动IO模型不适用TCP协议，所以目前大部分采用的还是IO多路复用模型。\n\n\n# IO 线程模型\n\n在前边内容的介绍中，我们详述了网络数据包的接收和发送过程，并通过介绍5种IO模型了解了内核是如何读取网络数据并通知给用户线程的。\n\n前边的内容都是以内核空间的视角来剖析网络数据的收发模型，本小节我们站在用户空间的视角来看下如果对网络数据进行收发。\n\n相对内核来讲，用户空间的IO线程模型相对就简单一些。这些用户空间的IO线程模型都是在讨论当多线程一起配合工作时谁负责接收连接，谁负责响应IO 读写、谁负责计算、谁负责发送和接收，仅仅是用户IO线程的不同分工模式罢了。\n\n\n# Reactor\n\nReactor是利用NIO对IO线程进行不同的分工：\n\n * 使用前边我们提到的IO多路复用模型比如select,poll,epoll,kqueue,进行IO事件的注册和监听。\n * 将监听到就绪的IO事件分发dispatch到各个具体的处理Handler中进行相应的IO事件处理。\n\n通过IO多路复用技术就可以不断的监听IO事件，不断的分发dispatch，就像一个反应堆一样，看起来像不断的产生IO事件，因此我们称这种模式为Reactor模型。\n\n下面我们来看下Reactor模型的三种分类：\n\n# 单Reactor单线程\n\n单Reactor单线程\n\nReactor模型是依赖IO多路复用技术实现监听IO事件，从而源源不断的产生IO就绪事件，在Linux系统下我们使用epoll来进行IO多路复用，我们以Linux系统为例：\n\n * 单Reactor意味着只有一个epoll对象，用来监听所有的事件，比如连接事件，读写事件。\n * 单线程意味着只有一个线程来执行epoll_wait获取IO就绪的Socket，然后对这些就绪的Socket执行读写，以及后边的业务处理也依然是这个线程。\n\n单Reactor单线程模型就好比我们开了一个很小很小的小饭馆，作为老板的我们需要一个人干所有的事情，包括：迎接顾客（accept事件），为顾客介绍菜单等待顾客点菜(IO请求)，做菜（业务处理），上菜（IO响应），送客（断开连接）。\n\n# 单Reactor多线程\n\n随着客人的增多（并发请求），显然饭馆里的事情只有我们一个人干（单线程）肯定是忙不过来的，这时候我们就需要多招聘一些员工（多线程）来帮着一起干上述的事情。\n\n于是就有了单Reactor多线程模型：\n\n单Reactor多线程\n\n * 这种模式下，也是只有一个epoll对象来监听所有的IO事件，一个线程来调用epoll_wait获取IO就绪的Socket。\n * 但是当IO就绪事件产生时，这些IO事件对应处理的业务Handler，我们是通过线程池来执行。这样相比单Reactor单线程模型提高了执行效率，充分发挥了多核CPU的优势。\n\n# 主从Reactor多线程\n\n做任何事情都要区分事情的优先级，我们应该优先高效的去做优先级更高的事情，而不是一股脑不分优先级的全部去做。\n\n当我们的小饭馆客人越来越多（并发量越来越大），我们就需要扩大饭店的规模，在这个过程中我们发现，迎接客人是饭店最重要的工作，我们要先把客人迎接进来，不能让客人一看人多就走掉，只要客人进来了，哪怕菜做的慢一点也没关系。\n\n于是，主从Reactor多线程模型就产生了：\n\n主从Reactor多线程\n\n * 我们由原来的单Reactor变为了多Reactor。主Reactor用来优先专门做优先级最高的事情，也就是迎接客人（处理连接事件），对应的处理Handler就是图中的acceptor。\n * 当创建好连接，建立好对应的socket后，在acceptor中将要监听的read事件注册到从Reactor中，由从Reactor来监听socket上的读写事件。\n * 最终将读写的业务逻辑处理交给线程池处理。\n\n> 注意：这里向从Reactor注册的只是read事件，并没有注册write事件，因为read事件是由epoll内核触发的，而write事件则是由用户业务线程触发的（什么时候发送数据是由具体业务线程决定的），所以write事件理应是由用户业务线程去注册。\n\n> 用户线程注册write事件的时机是只有当用户发送的数据无法一次性全部写入buffer时，才会去注册write事件，等待buffer重新可写时，继续写入剩下的发送数据、如果用户线程可以一股脑的将发送数据全部写入buffer，那么也就无需注册write事件到从Reactor中。\n\n主从Reactor多线程模型是现在大部分主流网络框架中采用的一种IO线程模型。我们本系列的主题Netty就是用的这种模型。\n\n\n# Proactor\n\nProactor是基于AIO对IO线程进行分工的一种模型。前边我们介绍了异步IO模型，它是操作系统内核支持的一种全异步编程模型，在数据准备阶段和数据拷贝阶段全程无阻塞。\n\nProactorIO线程模型将IO事件的监听，IO操作的执行，IO结果的dispatch统统交给内核来做。\n\nproactor.png\n\nProactor模型组件介绍：\n\n * completion handler 为用户程序定义的异步IO操作回调函数，在异步IO操作完成时会被内核回调并通知IO结果。\n * Completion Event Queue 异步IO操作完成后，会产生对应的IO完成事件，将IO完成事件放入该队列中。\n * Asynchronous Operation Processor 负责异步IO的执行。执行完成后产生IO完成事件放入Completion Event Queue 队列中。\n * Proactor 是一个事件循环派发器，负责从Completion Event Queue中获取IO完成事件，并回调与IO完成事件关联的completion handler。\n * Initiator 初始化异步操作（asynchronous operation）并通过Asynchronous Operation Processor将completion handler和proactor注册到内核。\n\nProactor模型执行过程：\n\n * 用户线程发起aio_read，并告诉内核用户空间中的读缓冲区地址，以便内核完成IO操作将结果放入用户空间的读缓冲区，用户线程直接可以读取结果（无任何阻塞）。\n * Initiator 初始化aio_read异步读取操作（asynchronous operation）,并将completion handler注册到内核。\n\n> 在Proactor中我们关心的IO完成事件：内核已经帮我们读好数据并放入我们指定的读缓冲区，用户线程可以直接读取。在Reactor中我们关心的是IO就绪事件：数据已经到来，但是需要用户线程自己去读取。\n\n * 此时用户线程就可以做其他事情了，无需等待IO结果。而内核与此同时开始异步执行IO操作。当IO操作完成时会产生一个completion event事件，将这个IO完成事件放入completion event queue中。\n * Proactor从completion event queue中取出completion event，并回调与IO完成事件关联的completion handler。\n * 在completion handler中完成业务逻辑处理。\n\n\n# Reactor与Proactor对比\n\nReactor是基于NIO实现的一种IO线程模型，Proactor是基于AIO实现的IO线程模型。\n\nReactor关心的是IO就绪事件，Proactor关心的是IO完成事件。\n\n在Proactor中，用户程序需要向内核传递用户空间的读缓冲区地址。Reactor则不需要。这也就导致了在Proactor中每个并发操作都要求有独立的缓存区，在内存上有一定的开销。\n\nProactor 的实现逻辑复杂，编码成本较 Reactor要高很多。\n\nProactor 在处理高耗时 IO时的性能要高于 Reactor，但对于低耗时 IO的执行效率提升并不明显。\n\n\n# Netty的IO模型\n\n在我们介绍完网络数据包在内核中的收发过程以及五种IO模型和两种IO线程模型后，现在我们来看下netty中的IO模型是什么样的。\n\n在我们介绍Reactor IO线程模型的时候提到有三种Reactor模型：单Reactor单线程，单Reactor多线程，主从Reactor多线程。\n\n这三种Reactor模型在netty中都是支持的，但是我们常用的是主从Reactor多线程模型。\n\n而我们之前介绍的三种Reactor只是一种模型，是一种设计思想。实际上各种网络框架在实现中并不是严格按照模型来实现的，会有一些小的不同，但大体设计思想上是一样的。\n\n下面我们来看下netty中的主从Reactor多线程模型是什么样子的？\n\n\n\n * Reactor在netty中是以group的形式出现的，netty中将Reactor分为两组，一组是MainReactorGroup也就是我们在编码中常常看到的EventLoopGroup bossGroup,另一组是SubReactorGroup也就是我们在编码中常常看到的EventLoopGroup workerGroup。\n * MainReactorGroup中通常只有一个Reactor，专门负责做最重要的事情，也就是监听连接accept事件。当有连接事件产生时，在对应的处理handler acceptor中创建初始化相应的NioSocketChannel（代表一个Socket连接）。然后以负载均衡的方式在SubReactorGroup中选取一个Reactor，注册上去，监听Read事件。\n\n> MainReactorGroup中只有一个Reactor的原因是，通常我们服务端程序只会绑定监听一个端口，如果要绑定监听多个端口，就会配置多个Reactor。\n\n * SubReactorGroup中有多个Reactor，具体Reactor的个数可以由系统参数 -D io.netty.eventLoopThreads指定。默认的Reactor的个数为CPU核数 * 2。SubReactorGroup中的Reactor主要负责监听读写事件，每一个Reactor负责监听一组socket连接。将全量的连接分摊在多个Reactor中。\n * 一个Reactor分配一个IO线程，这个IO线程负责从Reactor中获取IO就绪事件，执行IO调用获取IO数据，执行PipeLine。\n\n> Socket连接在创建后就被固定的分配给一个Reactor，所以一个Socket连接也只会被一个固定的IO线程执行，每个Socket连接分配一个独立的PipeLine实例，用来编排这个Socket连接上的IO处理逻辑。这种无锁串行化的设计的目的是为了防止多线程并发执行同一个socket连接上的IO逻辑处理，防止出现线程安全问题。同时使系统吞吐量达到最大化\n\n> 由于每个Reactor中只有一个IO线程，这个IO线程既要执行IO活跃Socket连接对应的PipeLine中的ChannelHandler，又要从Reactor中获取IO就绪事件，执行IO调用。所以PipeLine中ChannelHandler中执行的逻辑不能耗时太长，尽量将耗时的业务逻辑处理放入单独的业务线程池中处理，否则会影响其他连接的IO读写，从而近一步影响整个服务程序的IO吞吐。\n\n * 当IO请求在业务线程中完成相应的业务逻辑处理后，在业务线程中利用持有的ChannelHandlerContext引用将响应数据在PipeLine中反向传播，最终写回给客户端。\n\nnetty中的IO模型我们介绍完了，下面我们来简单介绍下在netty中是如何支持前边提到的三种Reactor模型的。\n\n\n# 配置单Reactor单线程\n\nEventLoopGroup eventGroup = new NioEventLoopGroup(1);\nServerBootstrap serverBootstrap = new ServerBootstrap(); \nserverBootstrap.group(eventGroup);\n\n\n\n# 配置多Reactor线程\n\nEventLoopGroup eventGroup = new NioEventLoopGroup();\nServerBootstrap serverBootstrap = new ServerBootstrap(); \nserverBootstrap.group(eventGroup);\n\n\n\n# 配置主从Reactor多线程\n\nEventLoopGroup bossGroup = new NioEventLoopGroup(1); \nEventLoopGroup workerGroup = new NioEventLoopGroup();\nServerBootstrap serverBootstrap = new ServerBootstrap(); \nserverBootstrap.group(bossGroup, workerGroup);\n\n\n\n# 总结\n\n本文是一篇信息量比较大的文章，用了25张图，22336个字从内核如何处理网络数据包的收发过程开始展开，随后又在内核角度介绍了经常容易混淆的阻塞与非阻塞，同步与异步的概念。以这个作为铺垫，我们通过一个C10K的问题，引出了五种IO模型，随后在IO多路复用中以技术演进的形式介绍了select,poll,epoll的原理和它们综合的对比。最后我们介绍了两种IO线程模型以及netty中的Reactor模型。\n\n感谢大家听我唠叨到这里，哈哈，现在大家可以揉揉眼，伸个懒腰，好好休息一下了。\n\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/zAh1yD5IfwuoYdrZ1tGf5Q",normalizedContent:"# 前言\n\n从今天开始我们来聊聊netty的那些事儿，我们都知道netty是一个高性能异步事件驱动的网络框架。\n\n它的设计异常优雅简洁，扩展性高，稳定性强。拥有非常详细完整的用户文档。\n\n同时内置了很多非常有用的模块基本上做到了开箱即用，用户只需要编写短短几行代码，就可以快速构建出一个具有高吞吐，低延时，更少的资源消耗，高性能（非必要的内存拷贝最小化）等特征的高并发网络应用程序。\n\n本文我们来探讨下支持netty具有高吞吐，低延时特征的基石----netty的网络io模型。\n\n由netty的网络io模型开始，我们来正式揭开本系列netty源码解析的序幕：\n\n\n# 网络包接收流程\n\n\n\n * 当网络数据帧通过网络传输到达网卡时，网卡会将网络数据帧通过dma的方式放到环形缓冲区ringbuffer中\n\n笔记\n\nringbuffer是网卡在启动的时候分配和初始化的环形缓冲队列。当ringbuffer满的时候，新来的数据包就会被丢弃。我们可以通过ifconfig命令查看网卡收发数据包的情况。其中overruns数据项表示当ringbuffer满时，被丢弃的数据包。如果发现出现丢包情况，可以通过ethtool命令来增大ringbuffer长度。\n\n * 当dma操作完成时，网卡会向cpu发起一个硬中断，告诉cpu有网络数据到达。cpu调用网卡驱动注册的硬中断响应程序。网卡硬中断响应程序会为网络数据帧创建内核数据结构sk_buffer，并将网络数据帧拷贝到sk_buffer中。然后发起软中断请求，通知内核有新的网络数据帧到达。\n\n笔记\n\nsk_buff缓冲区，是一个维护网络帧结构的双向链表，链表中的每一个元素都是一个网络帧。虽然 tcp/ip 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而无需进行数据复制。\n\n * 内核线程ksoftirqd发现有软中断请求到来，随后调用网卡驱动注册的poll函数，poll函数将sk_buffer中的网络数据包送到内核协议栈中注册的ip_rcv函数中。\n\n笔记\n\n每个cpu会绑定一个ksoftirqd内核线程专门用来处理软中断响应。2个 cpu 时，就会有 ksoftirqd/0 和 ksoftirqd/1这两个内核线程。\n\n笔记\n\n这里有个事情需要注意下： 网卡接收到数据后，当dma拷贝完成时，向cpu发出硬中断，这时哪个cpu上响应了这个硬中断，那么在网卡硬中断响应程序中发出的软中断请求也会在这个cpu绑定的ksoftirqd线程中响应。所以如果发现linux软中断，cpu消耗都集中在一个核上的话，那么就需要调整硬中断的cpu亲和性，来将硬中断打散到不通的cpu核上去。\n\n * 在ip_rcv函数中也就是上图中的网络层，取出数据包的ip头，判断该数据包下一跳的走向，如果数据包是发送给本机的，则取出传输层的协议类型（tcp或者udp)，并去掉数据包的ip头，将数据包交给上图中得传输层处理。\n\n笔记\n\n传输层的处理函数：tcp协议对应内核协议栈中注册的tcp_rcv函数，udp协议对应内核协议栈中注册的udp_rcv函数。\n\n * 当我们采用的是tcp协议时，数据包到达传输层时，会在内核协议栈中的tcp_rcv函数处理，在tcp_rcv函数中去掉tcp头，根据四元组（源ip，源端口，目的ip，目的端口）查找对应的socket，如果找到对应的socket则将网络数据包中的传输数据拷贝到socket中的接收缓冲区中。如果没有找到，则发送一个目标不可达的icmp包。\n\n内核在接收网络数据包时所做的工作我们就介绍完了，现在我们把视角放到应用层，当我们程序通过系统调用read读取socket接收缓冲区中的数据时，如果接收缓冲区中没有数据，那么应用程序就会在系统调用上阻塞，直到socket接收缓冲区有数据，然后cpu将内核空间（socket接收缓冲区）的数据拷贝到用户空间，最后系统调用read返回，应用程序读取数据\n\n\n# 性能开销\n\n从内核处理网络数据包接收的整个过程来看，内核帮我们做了非常之多的工作，最终我们的应用程序才能读取到网络数据。\n\n随着而来的也带来了很多的性能开销，结合前面介绍的网络数据包接收过程我们来看下网络数据包接收的过程中都有哪些性能开销：\n\n * 应用程序通过系统调用从用户态转为内核态的开销以及系统调用返回时从内核态转为用户态的开销。\n * 网络数据从内核空间通过cpu拷贝到用户空间的开销。\n * 内核线程ksoftirqd响应软中断的开销。\n * cpu响应硬中断的开销。\n * dma拷贝网络数据包到内存中的开销。\n\n\n# 网络包发送流程\n\n\n\n * 当我们在应用程序中调用send系统调用发送数据时，由于是系统调用所以线程会发生一次用户态到内核态的转换，在内核中首先根据fd将真正的socket找出，这个socket对象中记录着各种协议栈的函数地址，然后构造struct msghdr对象，将用户需要发送的数据全部封装在这个struct msghdr结构体中。\n * 调用内核协议栈函数inet_sendmsg，发送流程进入内核协议栈处理。在进入到内核协议栈之后，内核会找到socket上的具体协议的发送函数。\n\n> 比如：我们使用的是tcp协议，对应的tcp协议发送函数是tcp_sendmsg，如果是udp协议的话，对应的发送函数为udp_sendmsg。\n\n * 在tcp协议的发送函数tcp_sendmsg中，创建内核数据结构sk_buffer,将struct msghdr结构体中的发送数据拷贝到sk_buffer中。调用tcp_write_queue_tail函数获取socket发送队列中的队尾元素，将新创建的sk_buffer添加到socket发送队列的尾部。\n\n> socket的发送队列是由sk_buffer组成的一个双向链表。\n\n> 发送流程走到这里，用户要发送的数据总算是从用户空间拷贝到了内核中，这时虽然发送数据已经拷贝到了内核socket中的发送队列中，但并不代表内核会开始发送，因为tcp协议的流量控制和拥塞控制，用户要发送的数据包并不一定会立马被发送出去，需要符合tcp协议的发送条件。如果没有达到发送条件，那么本次send系统调用就会直接返回。\n\n * 如果符合发送条件，则开始调用tcp_write_xmit内核函数。在这个函数中，会循环获取socket发送队列中待发送的sk_buffer，然后进行拥塞控制以及滑动窗口的管理。\n * 将从socket发送队列中获取到的sk_buffer重新拷贝一份，设置sk_buffer副本中的tcp header。\n\n> sk_buffer 内部其实包含了网络协议中所有的 header。在设置 tcp header的时候，只是把指针指向 sk_buffer的合适位置。后面再设置 ip header的时候，在把指针移动一下就行，避免频繁的内存申请和拷贝，效率很高。\n\n\n\n> **为什么不直接使用socket发送队列中的sk_buffer而是需要拷贝一份呢？**因为tcp协议是支持丢包重传的，在没有收到对端的ack之前，这个sk_buffer是不能删除的。内核每次调用网卡发送数据的时候，实际上传递的是sk_buffer的拷贝副本，当网卡把数据发送出去后，sk_buffer拷贝副本会被释放。当收到对端的ack之后，socket发送队列中的sk_buffer才会被真正删除。\n\n * 当设置完tcp头后，内核协议栈传输层的事情就做完了，下面通过调用ip_queue_xmit内核函数，正式来到内核协议栈网络层的处理。\n   \n   > 通过route命令可以查看本机路由配置。\n   \n   > 如果你使用 iptables配置了一些规则，那么这里将检测是否命中规则。如果你设置了非常复杂的 netfilter 规则，在这个函数里将会导致你的线程 cpu 开销会极大增加。\n\n * * 将sk_buffer中的指针移动到ip头位置上，设置ip头。\n   * 执行netfilters过滤。过滤通过之后，如果数据大于 mtu的话，则执行分片。\n   * 检查socket中是否有缓存路由表，如果没有的话，则查找路由项，并缓存到socket中。接着在把路由表设置到sk_buffer中。\n\n * 内核协议栈网络层的事情处理完后，现在发送流程进入了到了邻居子系统，邻居子系统位于内核协议栈中的网络层和网络接口层之间，用于发送arp请求获取mac地址，然后将sk_buffer中的指针移动到mac头位置，填充mac头。\n\n * 经过邻居子系统的处理，现在sk_buffer中已经封装了一个完整的数据帧，随后内核将sk_buffer交给网络设备子系统进行处理。网络设备子系统主要做以下几项事情：\n\n * * 选择发送队列（ringbuffer）。因为网卡拥有多个发送队列，所以在发送前需要选择一个发送队列。\n   * 将sk_buffer添加到发送队列中。\n   * 循环从发送队列（ringbuffer）中取出sk_buffer，调用内核函数sch_direct_xmit发送数据，其中会调用网卡驱动程序来发送数据。\n\n> 以上过程全部是用户线程的内核态在执行，占用的cpu时间是系统态时间(sy)，当分配给用户线程的cpu quota用完的时候，会触发net_tx_softirq类型的软中断，内核线程ksoftirqd会响应这个软中断，并执行net_tx_softirq类型的软中断注册的回调函数net_tx_action，在回调函数中会执行到驱动程序函数 dev_hard_start_xmit来发送数据。\n\n> 注意：当触发net_tx_softirq软中断来发送数据时，后边消耗的 cpu 就都显示在 si这里了，不会消耗用户进程的系统态时间（sy）了。\n\n> 从这里可以看到网络包的发送过程和接受过程是不同的，在介绍网络包的接受过程时，我们提到是通过触发net_rx_softirq类型的软中断在内核线程ksoftirqd中执行内核网络协议栈接受数据。而在网络数据包的发送过程中是用户线程的内核态在执行内核网络协议栈，只有当线程的cpu quota用尽时，才触发net_tx_softirq软中断来发送数据。\n\n> 在整个网络包的发送和接受过程中，net_tx_softirq类型的软中断只会在发送网络包时并且当用户线程的cpu quota用尽时，才会触发。剩下的接受过程中触发的软中断类型以及发送完数据触发的软中断类型均为net_rx_softirq。所以这就是你在服务器上查看 /proc/softirqs，一般 net_rx都要比 net_tx大很多的的原因\n\n * 现在发送流程终于到了网卡真实发送数据的阶段，前边我们讲到无论是用户线程的内核态还是触发net_tx_softirq类型的软中断在发送数据的时候最终会调用到网卡的驱动程序函数dev_hard_start_xmit来发送数据。在网卡驱动程序函数dev_hard_start_xmit中会将sk_buffer映射到网卡可访问的内存 dma 区域，最终网卡驱动程序通过dma的方式将数据帧通过物理网卡发送出去。\n * 当数据发送完毕后，还有最后一项重要的工作，就是清理工作。数据发送完毕后，网卡设备会向cpu发送一个硬中断，cpu调用网卡驱动程序注册的硬中断响应程序，在硬中断响应中触发net_rx_softirq类型的软中断，在软中断的回调函数igb_poll中清理释放 sk_buffer，清理网卡发送队列（ringbuffer），解除 dma 映射。\n\n> 无论硬中断是因为有数据要接收，还是说发送完成通知，从硬中断触发的软中断都是 net_rx_softirq。\n\n> 这里释放清理的只是sk_buffer的副本，真正的sk_buffer现在还是存放在socket的发送队列中。前面在传输层处理的时候我们提到过，因为传输层需要保证可靠性，所以 sk_buffer其实还没有删除。它得等收到对方的 ack 之后才会真正删除。\n\n\n# 性能开销\n\n前边我们提到了在网络包接收过程中涉及到的性能开销，现在介绍完了网络包的发送过程，我们来看下在数据包发送过程中的性能开销：\n\n * 和接收数据一样，应用程序在调用系统调用send的时候会从用户态转为内核态以及发送完数据后，系统调用返回时从内核态转为用户态的开销。\n\n * 用户线程内核态cpu quota用尽时触发net_tx_softirq类型软中断，内核响应软中断的开销。\n\n * 网卡发送完数据，向cpu发送硬中断，cpu响应硬中断的开销。以及在硬中断中发送net_rx_softirq软中断执行具体的内存清理动作。内核响应软中断的开销。\n\n * 内存拷贝的开销。我们来回顾下在数据包发送的过程中都发生了哪些内存拷贝：\n\n * * 在内核协议栈的传输层中，tcp协议对应的发送函数tcp_sendmsg会申请sk_buffer，将用户要发送的数据拷贝到sk_buffer中。\n   * 在发送流程从传输层到网络层的时候，会拷贝一个sk_buffer副本出来，将这个sk_buffer副本向下传递。原始sk_buffer保留在socket发送队列中，等待网络对端ack，对端ack后删除socket发送队列中的sk_buffer。对端没有发送ack，则重新从socket发送队列中发送，实现tcp协议的可靠传输。\n   * 在网络层，如果发现要发送的数据大于mtu，则会进行分片操作，申请额外的sk_buffer，并将原来的sk_buffer拷贝到多个小的sk_buffer中。\n\n\n# 再谈(阻塞，非阻塞)与(同步，异步)\n\n在我们聊完网络数据的接收和发送过程后，我们来谈下io中特别容易混淆的概念：阻塞与同步，非阻塞与异步。\n\n网上各种博文还有各种书籍中有大量的关于这两个概念的解释，但是笔者觉得还是不够形象化，只是对概念的生硬解释，如果硬套概念的话，其实感觉阻塞与同步，非阻塞与异步还是没啥区别，时间长了，还是比较模糊容易混淆。\n\n所以笔者在这里尝试换一种更加形象化，更加容易理解记忆的方式来清晰地解释下什么是阻塞与非阻塞，什么是同步与异步。\n\n经过前边对网络数据包接收流程的介绍，在这里我们可以将整个流程总结为两个阶段：\n\n\n\n * 数据准备阶段： 在这个阶段，网络数据包到达网卡，通过dma的方式将数据包拷贝到内存中，然后经过硬中断，软中断，接着通过内核线程ksoftirqd经过内核协议栈的处理，最终将数据发送到内核socket的接收缓冲区中。\n * 数据拷贝阶段： 当数据到达内核socket的接收缓冲区中时，此时数据存在于内核空间中，需要将数据拷贝到用户空间中，才能够被应用程序读取。\n\n\n# 阻塞与非阻塞\n\n阻塞与非阻塞的区别主要发生在第一阶段：数据准备阶段。\n\n当应用程序发起系统调用read时，线程从用户态转为内核态，读取内核socket的接收缓冲区中的网络数据。\n\n# 阻塞\n\n如果这时内核socket的接收缓冲区没有数据，那么线程就会一直等待，直到socket接收缓冲区有数据为止。随后将数据从内核空间拷贝到用户空间，系统调用read返回。\n\n\n\n从图中我们可以看出：阻塞的特点是在第一阶段和第二阶段都会等待。\n\n# 非阻塞\n\n阻塞和非阻塞主要的区分是在第一阶段：数据准备阶段。\n\n * 在第一阶段，当socket的接收缓冲区中没有数据的时候，阻塞模式下应用线程会一直等待。非阻塞模式下应用线程不会等待，系统调用直接返回错误标志ewouldblock。\n * 当socket的接收缓冲区中有数据的时候，阻塞和非阻塞的表现是一样的，都会进入第二阶段等待数据从内核空间拷贝到用户空间，然后系统调用返回。\n\n\n\n从上图中，我们可以看出：非阻塞的特点是第一阶段不会等待，但是在第二阶段还是会等待。\n\n\n# 同步与异步\n\n同步与异步主要的区别发生在第二阶段：数据拷贝阶段。\n\n前边我们提到在数据拷贝阶段主要是将数据从内核空间拷贝到用户空间。然后应用程序才可以读取数据。\n\n当内核socket的接收缓冲区有数据到达时，进入第二阶段。\n\n# 同步\n\n同步模式在数据准备好后，是由用户线程的内核态来执行第二阶段。所以应用程序会在第二阶段发生阻塞，直到数据从内核空间拷贝到用户空间，系统调用才会返回。\n\nlinux下的 epoll和mac 下的 kqueue都属于同步 io。\n\n\n\n# 异步\n\n异步模式下是由内核来执行第二阶段的数据拷贝操作，当内核执行完第二阶段，会通知用户线程io操作已经完成，并将数据回调给用户线程。所以在异步模式下 数据准备阶段和数据拷贝阶段均是由内核来完成，不会对应用程序造成任何阻塞。\n\n基于以上特征，我们可以看到异步模式需要内核的支持，比较依赖操作系统底层的支持。\n\n在目前流行的操作系统中，只有windows 中的 iocp才真正属于异步 io，实现的也非常成熟。但windows很少用来作为服务器使用。\n\n而常用来作为服务器使用的linux，异步io机制实现的不够成熟，与nio相比性能提升的也不够明显。\n\n但linux kernel 在5.1版本由facebook的大神jens axboe引入了新的异步io库io_uring 改善了原来linux native aio的一些性能问题。性能相比epoll以及之前原生的aio提高了不少，值得关注。\n\n\n\n\n# io模型\n\n在进行网络io操作时，用什么样的io模型来读写数据将在很大程度上决定了网络框架的io性能。所以io模型的选择是构建一个高性能网络框架的基础。\n\n在《unix 网络编程》一书中介绍了五种io模型：阻塞io,非阻塞io,io多路复用,信号驱动io,异步io，每一种io模型的出现都是对前一种的升级优化。\n\n下面我们就来分别介绍下这五种io模型各自都解决了什么问题，适用于哪些场景，各自的优缺点是什么？\n\n\n# 阻塞io（bio）\n\n\n\n经过前一小节对阻塞这个概念的介绍，相信大家可以很容易理解阻塞io的概念和过程。\n\n既然这小节我们谈的是io，那么下边我们来看下在阻塞io模型下，网络数据的读写过程。\n\n# 阻塞读\n\n当用户线程发起read系统调用，用户线程从用户态切换到内核态，在内核中去查看socket接收缓冲区是否有数据到来。\n\n * socket接收缓冲区中有数据，则用户线程在内核态将内核空间中的数据拷贝到用户空间，系统io调用返回。\n * socket接收缓冲区中无数据，则用户线程让出cpu，进入阻塞状态。当数据到达socket接收缓冲区后，内核唤醒阻塞状态中的用户线程进入就绪状态，随后经过cpu的调度获取到cpu quota进入运行状态，将内核空间的数据拷贝到用户空间，随后系统调用返回。\n\n# 阻塞写\n\n当用户线程发起send系统调用时，用户线程从用户态切换到内核态，将发送数据从用户空间拷贝到内核空间中的socket发送缓冲区中。\n\n * 当socket发送缓冲区能够容纳下发送数据时，用户线程会将全部的发送数据写入socket缓冲区，然后执行在《网络包发送流程》这小节介绍的后续流程，然后返回。\n * 当socket发送缓冲区空间不够，无法容纳下全部发送数据时，用户线程让出cpu,进入阻塞状态，直到socket发送缓冲区能够容纳下全部发送数据时，内核唤醒用户线程，执行后续发送流程。\n\n阻塞io模型下的写操作做事风格比较硬刚，非得要把全部的发送数据写入发送缓冲区才肯善罢甘休。\n\n# 阻塞io模型\n\n\n\n由于阻塞io的读写特点，所以导致在阻塞io模型下，每个请求都需要被一个独立的线程处理。一个线程在同一时刻只能与一个连接绑定。来一个请求，服务端就需要创建一个线程用来处理请求。\n\n当客户端请求的并发量突然增大时，服务端在一瞬间就会创建出大量的线程，而创建线程是需要系统资源开销的，这样一来就会一瞬间占用大量的系统资源。\n\n如果客户端创建好连接后，但是一直不发数据，通常大部分情况下，网络连接也并不总是有数据可读，那么在空闲的这段时间内，服务端线程就会一直处于阻塞状态，无法干其他的事情。cpu也无法得到充分的发挥，同时还会导致大量线程切换的开销。\n\n# 适用场景\n\n基于以上阻塞io模型的特点，该模型只适用于连接数少，并发度低的业务场景。\n\n比如公司内部的一些管理系统，通常请求数在100个左右，使用阻塞io模型还是非常适合的。而且性能还不输nio。\n\n该模型在c10k之前，是普遍被采用的一种io模型。\n\n\n# 非阻塞io（nio）\n\n阻塞io模型最大的问题就是一个线程只能处理一个连接，如果这个连接上没有数据的话，那么这个线程就只能阻塞在系统io调用上，不能干其他的事情。这对系统资源来说，是一种极大的浪费。同时大量的线程上下文切换，也是一个巨大的系统开销。\n\n所以为了解决这个问题，我们就需要用尽可能少的线程去处理更多的连接。，网络io模型的演变也是根据这个需求来一步一步演进的。\n\n基于这个需求，第一种解决方案非阻塞io就出现了。我们在上一小节中介绍了非阻塞的概念，现在我们来看下网络读写操作在非阻塞io下的特点：\n\n\n\n# 非阻塞读\n\n当用户线程发起非阻塞read系统调用时，用户线程从用户态转为内核态，在内核中去查看socket接收缓冲区是否有数据到来。\n\n * socket接收缓冲区中无数据，系统调用立马返回，并带有一个 ewouldblock 或 eagain错误，这个阶段用户线程不会阻塞，也不会让出cpu，而是会继续轮训直到socket接收缓冲区中有数据为止。\n * socket接收缓冲区中有数据，用户线程在内核态会将内核空间中的数据拷贝到用户空间，注意这个数据拷贝阶段，应用程序是阻塞的，当数据拷贝完成，系统调用返回。\n\n# 非阻塞写\n\n前边我们在介绍阻塞写的时候提到阻塞写的风格特别的硬朗，头比较铁非要把全部发送数据一次性都写到socket的发送缓冲区中才返回，如果发送缓冲区中没有足够的空间容纳，那么就一直阻塞死等，特别的刚。\n\n相比较而言非阻塞写的特点就比较佛系，当发送缓冲区中没有足够的空间容纳全部发送数据时，非阻塞写的特点是能写多少写多少，写不下了，就立即返回。并将写入到发送缓冲区的字节数返回给应用程序，方便用户线程不断的轮训尝试将剩下的数据写入发送缓冲区中。\n\n# 非阻塞io模型\n\n\n\n基于以上非阻塞io的特点，我们就不必像阻塞io那样为每个请求分配一个线程去处理连接上的读写了。\n\n我们可以利用一个线程或者很少的线程，去不断地轮询每个socket的接收缓冲区是否有数据到达，如果没有数据，不必阻塞线程，而是接着去轮询下一个socket接收缓冲区，直到轮询到数据后，处理连接上的读写，或者交给业务线程池去处理，轮询线程则继续轮询其他的socket接收缓冲区。\n\n这样一个非阻塞io模型就实现了我们在本小节开始提出的需求：我们需要用尽可能少的线程去处理更多的连接\n\n# 适用场景\n\n虽然非阻塞io模型与阻塞io模型相比，减少了很大一部分的资源消耗和系统开销。\n\n但是它仍然有很大的性能问题，因为在非阻塞io模型下，需要用户线程去不断地发起系统调用去轮训socket接收缓冲区，这就需要用户线程不断地从用户态切换到内核态，内核态切换到用户态。随着并发量的增大，这个上下文切换的开销也是巨大的。\n\n所以单纯的非阻塞io模型还是无法适用于高并发的场景。只能适用于c10k以下的场景。\n\n\n# io 多路复用\n\n\n# 信号驱动io\n\n信号驱动io.png\n\n大家对这个装备肯定不会陌生，当我们去一些美食城吃饭的时候，点完餐付了钱，老板会给我们一个信号器。然后我们带着这个信号器可以去找餐桌，或者干些其他的事情。当信号器亮了的时候，这时代表饭餐已经做好，我们可以去窗口取餐了。\n\n这个典型的生活场景和我们要介绍的信号驱动io模型就很像。\n\n在信号驱动io模型下，用户进程操作通过系统调用 sigaction 函数发起一个 io 请求，在对应的socket注册一个信号回调，此时不阻塞用户进程，进程会继续工作。当内核数据就绪时，内核就为该进程生成一个 sigio 信号，通过信号回调通知进程进行相关 io 操作。\n\n> 这里需要注意的是：信号驱动式 io 模型依然是同步io，因为它虽然可以在等待数据的时候不被阻塞，也不会频繁的轮询，但是当数据就绪，内核信号通知后，用户进程依然要自己去读取数据，在数据拷贝阶段发生阻塞。\n\n> 信号驱动 io模型 相比于前三种 io 模型，实现了在等待数据就绪时，进程不被阻塞，主循环可以继续工作，所以理论上性能更佳。\n\n但是实际上，使用tcp协议通信时，信号驱动io模型几乎不会被采用。原因如下：\n\n * 信号io 在大量 io 操作时可能会因为信号队列溢出导致没法通知\n * sigio 信号是一种 unix 信号，信号没有附加信息，如果一个信号源有多种产生信号的原因，信号接收者就无法确定究竟发生了什么。而 tcp socket 生产的信号事件有七种之多，这样应用程序收到 sigio，根本无从区分处理。\n\n但信号驱动io模型可以用在 udp通信上，因为udp 只有一个数据请求事件，这也就意味着在正常情况下 udp 进程只要捕获 sigio 信号，就调用 read 系统调用读取到达的数据。如果出现异常，就返回一个异常错误。\n\n----------------------------------------\n\n这里插句题外话，大家觉不觉得阻塞io模型在生活中的例子就像是我们在食堂排队打饭。你自己需要排队去打饭同时打饭师傅在配菜的过程中你需要等待。\n\n阻塞io.png\n\nio多路复用模型就像是我们在饭店门口排队等待叫号。叫号器就好比select,poll,epoll可以统一管理全部顾客的吃饭就绪事件，客户好比是socket连接，谁可以去吃饭了，叫号器就通知谁。\n\nio多路复用.png\n\n\n# 异步io（aio）\n\n以上介绍的四种io模型均为同步io，它们都会阻塞在第二阶段数据拷贝阶段。\n\n通过在前边小节《同步与异步》中的介绍，相信大家很容易就会理解异步io模型，在异步io模型下，io操作在数据准备阶段和数据拷贝阶段均是由内核来完成，不会对应用程序造成任何阻塞。应用进程只需要在指定的数组中引用数据即可。\n\n异步 io 与信号驱动 io 的主要区别在于：信号驱动 io 由内核通知何时可以开始一个 io 操作，而异步 io由内核通知 io 操作何时已经完成。\n\n举个生活中的例子：异步io模型就像我们去一个高档饭店里的包间吃饭，我们只需要坐在包间里面，点完餐（类比异步io调用）之后，我们就什么也不需要管，该喝酒喝酒，该聊天聊天，饭餐做好后服务员（类比内核）会自己给我们送到包间（类比用户空间）来。整个过程没有任何阻塞。\n\n异步io.png\n\n异步io的系统调用需要操作系统内核来支持，目前只有window中的iocp实现了非常成熟的异步io机制。\n\n而linux系统对异步io机制实现的不够成熟，且与nio的性能相比提升也不明显。\n\n> 但linux kernel 在5.1版本由facebook的大神jens axboe引入了新的异步io库io_uring 改善了原来linux native aio的一些性能问题。性能相比epoll以及之前原生的aio提高了不少，值得关注。\n\n再加上信号驱动io模型不适用tcp协议，所以目前大部分采用的还是io多路复用模型。\n\n\n# io 线程模型\n\n在前边内容的介绍中，我们详述了网络数据包的接收和发送过程，并通过介绍5种io模型了解了内核是如何读取网络数据并通知给用户线程的。\n\n前边的内容都是以内核空间的视角来剖析网络数据的收发模型，本小节我们站在用户空间的视角来看下如果对网络数据进行收发。\n\n相对内核来讲，用户空间的io线程模型相对就简单一些。这些用户空间的io线程模型都是在讨论当多线程一起配合工作时谁负责接收连接，谁负责响应io 读写、谁负责计算、谁负责发送和接收，仅仅是用户io线程的不同分工模式罢了。\n\n\n# reactor\n\nreactor是利用nio对io线程进行不同的分工：\n\n * 使用前边我们提到的io多路复用模型比如select,poll,epoll,kqueue,进行io事件的注册和监听。\n * 将监听到就绪的io事件分发dispatch到各个具体的处理handler中进行相应的io事件处理。\n\n通过io多路复用技术就可以不断的监听io事件，不断的分发dispatch，就像一个反应堆一样，看起来像不断的产生io事件，因此我们称这种模式为reactor模型。\n\n下面我们来看下reactor模型的三种分类：\n\n# 单reactor单线程\n\n单reactor单线程\n\nreactor模型是依赖io多路复用技术实现监听io事件，从而源源不断的产生io就绪事件，在linux系统下我们使用epoll来进行io多路复用，我们以linux系统为例：\n\n * 单reactor意味着只有一个epoll对象，用来监听所有的事件，比如连接事件，读写事件。\n * 单线程意味着只有一个线程来执行epoll_wait获取io就绪的socket，然后对这些就绪的socket执行读写，以及后边的业务处理也依然是这个线程。\n\n单reactor单线程模型就好比我们开了一个很小很小的小饭馆，作为老板的我们需要一个人干所有的事情，包括：迎接顾客（accept事件），为顾客介绍菜单等待顾客点菜(io请求)，做菜（业务处理），上菜（io响应），送客（断开连接）。\n\n# 单reactor多线程\n\n随着客人的增多（并发请求），显然饭馆里的事情只有我们一个人干（单线程）肯定是忙不过来的，这时候我们就需要多招聘一些员工（多线程）来帮着一起干上述的事情。\n\n于是就有了单reactor多线程模型：\n\n单reactor多线程\n\n * 这种模式下，也是只有一个epoll对象来监听所有的io事件，一个线程来调用epoll_wait获取io就绪的socket。\n * 但是当io就绪事件产生时，这些io事件对应处理的业务handler，我们是通过线程池来执行。这样相比单reactor单线程模型提高了执行效率，充分发挥了多核cpu的优势。\n\n# 主从reactor多线程\n\n做任何事情都要区分事情的优先级，我们应该优先高效的去做优先级更高的事情，而不是一股脑不分优先级的全部去做。\n\n当我们的小饭馆客人越来越多（并发量越来越大），我们就需要扩大饭店的规模，在这个过程中我们发现，迎接客人是饭店最重要的工作，我们要先把客人迎接进来，不能让客人一看人多就走掉，只要客人进来了，哪怕菜做的慢一点也没关系。\n\n于是，主从reactor多线程模型就产生了：\n\n主从reactor多线程\n\n * 我们由原来的单reactor变为了多reactor。主reactor用来优先专门做优先级最高的事情，也就是迎接客人（处理连接事件），对应的处理handler就是图中的acceptor。\n * 当创建好连接，建立好对应的socket后，在acceptor中将要监听的read事件注册到从reactor中，由从reactor来监听socket上的读写事件。\n * 最终将读写的业务逻辑处理交给线程池处理。\n\n> 注意：这里向从reactor注册的只是read事件，并没有注册write事件，因为read事件是由epoll内核触发的，而write事件则是由用户业务线程触发的（什么时候发送数据是由具体业务线程决定的），所以write事件理应是由用户业务线程去注册。\n\n> 用户线程注册write事件的时机是只有当用户发送的数据无法一次性全部写入buffer时，才会去注册write事件，等待buffer重新可写时，继续写入剩下的发送数据、如果用户线程可以一股脑的将发送数据全部写入buffer，那么也就无需注册write事件到从reactor中。\n\n主从reactor多线程模型是现在大部分主流网络框架中采用的一种io线程模型。我们本系列的主题netty就是用的这种模型。\n\n\n# proactor\n\nproactor是基于aio对io线程进行分工的一种模型。前边我们介绍了异步io模型，它是操作系统内核支持的一种全异步编程模型，在数据准备阶段和数据拷贝阶段全程无阻塞。\n\nproactorio线程模型将io事件的监听，io操作的执行，io结果的dispatch统统交给内核来做。\n\nproactor.png\n\nproactor模型组件介绍：\n\n * completion handler 为用户程序定义的异步io操作回调函数，在异步io操作完成时会被内核回调并通知io结果。\n * completion event queue 异步io操作完成后，会产生对应的io完成事件，将io完成事件放入该队列中。\n * asynchronous operation processor 负责异步io的执行。执行完成后产生io完成事件放入completion event queue 队列中。\n * proactor 是一个事件循环派发器，负责从completion event queue中获取io完成事件，并回调与io完成事件关联的completion handler。\n * initiator 初始化异步操作（asynchronous operation）并通过asynchronous operation processor将completion handler和proactor注册到内核。\n\nproactor模型执行过程：\n\n * 用户线程发起aio_read，并告诉内核用户空间中的读缓冲区地址，以便内核完成io操作将结果放入用户空间的读缓冲区，用户线程直接可以读取结果（无任何阻塞）。\n * initiator 初始化aio_read异步读取操作（asynchronous operation）,并将completion handler注册到内核。\n\n> 在proactor中我们关心的io完成事件：内核已经帮我们读好数据并放入我们指定的读缓冲区，用户线程可以直接读取。在reactor中我们关心的是io就绪事件：数据已经到来，但是需要用户线程自己去读取。\n\n * 此时用户线程就可以做其他事情了，无需等待io结果。而内核与此同时开始异步执行io操作。当io操作完成时会产生一个completion event事件，将这个io完成事件放入completion event queue中。\n * proactor从completion event queue中取出completion event，并回调与io完成事件关联的completion handler。\n * 在completion handler中完成业务逻辑处理。\n\n\n# reactor与proactor对比\n\nreactor是基于nio实现的一种io线程模型，proactor是基于aio实现的io线程模型。\n\nreactor关心的是io就绪事件，proactor关心的是io完成事件。\n\n在proactor中，用户程序需要向内核传递用户空间的读缓冲区地址。reactor则不需要。这也就导致了在proactor中每个并发操作都要求有独立的缓存区，在内存上有一定的开销。\n\nproactor 的实现逻辑复杂，编码成本较 reactor要高很多。\n\nproactor 在处理高耗时 io时的性能要高于 reactor，但对于低耗时 io的执行效率提升并不明显。\n\n\n# netty的io模型\n\n在我们介绍完网络数据包在内核中的收发过程以及五种io模型和两种io线程模型后，现在我们来看下netty中的io模型是什么样的。\n\n在我们介绍reactor io线程模型的时候提到有三种reactor模型：单reactor单线程，单reactor多线程，主从reactor多线程。\n\n这三种reactor模型在netty中都是支持的，但是我们常用的是主从reactor多线程模型。\n\n而我们之前介绍的三种reactor只是一种模型，是一种设计思想。实际上各种网络框架在实现中并不是严格按照模型来实现的，会有一些小的不同，但大体设计思想上是一样的。\n\n下面我们来看下netty中的主从reactor多线程模型是什么样子的？\n\n\n\n * reactor在netty中是以group的形式出现的，netty中将reactor分为两组，一组是mainreactorgroup也就是我们在编码中常常看到的eventloopgroup bossgroup,另一组是subreactorgroup也就是我们在编码中常常看到的eventloopgroup workergroup。\n * mainreactorgroup中通常只有一个reactor，专门负责做最重要的事情，也就是监听连接accept事件。当有连接事件产生时，在对应的处理handler acceptor中创建初始化相应的niosocketchannel（代表一个socket连接）。然后以负载均衡的方式在subreactorgroup中选取一个reactor，注册上去，监听read事件。\n\n> mainreactorgroup中只有一个reactor的原因是，通常我们服务端程序只会绑定监听一个端口，如果要绑定监听多个端口，就会配置多个reactor。\n\n * subreactorgroup中有多个reactor，具体reactor的个数可以由系统参数 -d io.netty.eventloopthreads指定。默认的reactor的个数为cpu核数 * 2。subreactorgroup中的reactor主要负责监听读写事件，每一个reactor负责监听一组socket连接。将全量的连接分摊在多个reactor中。\n * 一个reactor分配一个io线程，这个io线程负责从reactor中获取io就绪事件，执行io调用获取io数据，执行pipeline。\n\n> socket连接在创建后就被固定的分配给一个reactor，所以一个socket连接也只会被一个固定的io线程执行，每个socket连接分配一个独立的pipeline实例，用来编排这个socket连接上的io处理逻辑。这种无锁串行化的设计的目的是为了防止多线程并发执行同一个socket连接上的io逻辑处理，防止出现线程安全问题。同时使系统吞吐量达到最大化\n\n> 由于每个reactor中只有一个io线程，这个io线程既要执行io活跃socket连接对应的pipeline中的channelhandler，又要从reactor中获取io就绪事件，执行io调用。所以pipeline中channelhandler中执行的逻辑不能耗时太长，尽量将耗时的业务逻辑处理放入单独的业务线程池中处理，否则会影响其他连接的io读写，从而近一步影响整个服务程序的io吞吐。\n\n * 当io请求在业务线程中完成相应的业务逻辑处理后，在业务线程中利用持有的channelhandlercontext引用将响应数据在pipeline中反向传播，最终写回给客户端。\n\nnetty中的io模型我们介绍完了，下面我们来简单介绍下在netty中是如何支持前边提到的三种reactor模型的。\n\n\n# 配置单reactor单线程\n\neventloopgroup eventgroup = new nioeventloopgroup(1);\nserverbootstrap serverbootstrap = new serverbootstrap(); \nserverbootstrap.group(eventgroup);\n\n\n\n# 配置多reactor线程\n\neventloopgroup eventgroup = new nioeventloopgroup();\nserverbootstrap serverbootstrap = new serverbootstrap(); \nserverbootstrap.group(eventgroup);\n\n\n\n# 配置主从reactor多线程\n\neventloopgroup bossgroup = new nioeventloopgroup(1); \neventloopgroup workergroup = new nioeventloopgroup();\nserverbootstrap serverbootstrap = new serverbootstrap(); \nserverbootstrap.group(bossgroup, workergroup);\n\n\n\n# 总结\n\n本文是一篇信息量比较大的文章，用了25张图，22336个字从内核如何处理网络数据包的收发过程开始展开，随后又在内核角度介绍了经常容易混淆的阻塞与非阻塞，同步与异步的概念。以这个作为铺垫，我们通过一个c10k的问题，引出了五种io模型，随后在io多路复用中以技术演进的形式介绍了select,poll,epoll的原理和它们综合的对比。最后我们介绍了两种io线程模型以及netty中的reactor模型。\n\n感谢大家听我唠叨到这里，哈哈，现在大家可以揉揉眼，伸个懒腰，好好休息一下了。\n\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/zah1yd5ifwuoydrz1tgf5q",charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"第二课：Reactor在 Netty 中的实现（创建篇）",frontmatter:{title:"第二课：Reactor在 Netty 中的实现（创建篇）",date:"2024-09-19T09:55:47.000Z",permalink:"/pages/440035/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/25.%E5%9B%9B%E3%80%81%E6%B7%B1%E5%85%A5%20Netty%20%E6%A0%B8%E5%BF%83/10.%E7%AC%AC%E4%BA%8C%E8%AF%BE%EF%BC%9AReactor%E5%9C%A8Netty%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%88%E5%88%9B%E5%BB%BA%E7%AF%87%EF%BC%89.html",relativePath:"Netty 系统设计/25.四、深入 Netty 核心/10.第二课：Reactor在Netty中的实现（创建篇）.md",key:"v-e0e338cc",path:"/pages/440035/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:3,title:"Netty 服务端代码模板",slug:"netty-服务端代码模板",normalizedTitle:"netty 服务端代码模板",charIndex:1410},{level:2,title:"Netty 对 IO 模型的支持",slug:"netty-对-io-模型的支持",normalizedTitle:"netty 对 io 模型的支持",charIndex:6105},{level:3,title:"EventLoop",slug:"eventloop",normalizedTitle:"eventloop",charIndex:1724},{level:3,title:"EventLoopGroup",slug:"eventloopgroup",normalizedTitle:"eventloopgroup",charIndex:1724},{level:3,title:"ServerSocketChannel",slug:"serversocketchannel",normalizedTitle:"serversocketchannel",charIndex:2063},{level:3,title:"SocketChannel",slug:"socketchannel",normalizedTitle:"socketchannel",charIndex:714},{level:3,title:"多种NIO的实现",slug:"多种nio的实现",normalizedTitle:"多种nio的实现",charIndex:7721},{level:2,title:"创建主从Reactor线程组",slug:"创建主从reactor线程组",normalizedTitle:"创建主从reactor线程组",charIndex:1701},{level:3,title:"MultithreadEventLoopGroup",slug:"multithreadeventloopgroup",normalizedTitle:"multithreadeventloopgroup",charIndex:9225},{level:3,title:"MultithreadEventExecutorGroup",slug:"multithreadeventexecutorgroup",normalizedTitle:"multithreadeventexecutorgroup",charIndex:11319},{level:4,title:"1. 创建用于启动Reactor线程的executor",slug:"_1-创建用于启动reactor线程的executor",normalizedTitle:"1. 创建用于启动reactor线程的executor",charIndex:15297},{level:5,title:"ThreadPerTaskExecutor",slug:"threadpertaskexecutor",normalizedTitle:"threadpertaskexecutor",charIndex:14326},{level:4,title:"2. 创建Reactor",slug:"_2-创建reactor",normalizedTitle:"2. 创建reactor",charIndex:16259},{level:5,title:"newChild",slug:"newchild",normalizedTitle:"newchild",charIndex:13342},{level:5,title:"NioEventLoop",slug:"nioeventloop",normalizedTitle:"nioeventloop",charIndex:1755},{level:5,title:"openSelector",slug:"openselector",normalizedTitle:"openselector",charIndex:18605},{level:5,title:"SelectorProvider",slug:"selectorprovider",normalizedTitle:"selectorprovider",charIndex:9384},{level:5,title:"Netty对JDK NIO 原生Selector的优化",slug:"netty对jdk-nio-原生selector的优化",normalizedTitle:"netty对jdk nio 原生selector的优化",charIndex:22498},{level:5,title:"newTaskQueue",slug:"newtaskqueue",normalizedTitle:"newtaskqueue",charIndex:18292},{level:5,title:"Reactor对应的NioEventLoop类型继承结构",slug:"reactor对应的nioeventloop类型继承结构",normalizedTitle:"reactor对应的nioeventloop类型继承结构",charIndex:25165},{level:6,title:"SingleThreadEventLoop",slug:"singlethreadeventloop",normalizedTitle:"singlethreadeventloop",charIndex:17668},{level:6,title:"SingleThreadEventExecutor",slug:"singlethreadeventexecutor",normalizedTitle:"singlethreadeventexecutor",charIndex:25271},{level:4,title:"3. 创建Channel到Reactor的绑定策略",slug:"_3-创建channel到reactor的绑定策略",normalizedTitle:"3. 创建channel到reactor的绑定策略",charIndex:27948},{level:5,title:"DefaultEventExecutorChooserFactory",slug:"defaulteventexecutorchooserfactory",normalizedTitle:"defaulteventexecutorchooserfactory",charIndex:13514},{level:4,title:"4. 向Reactor线程组中所有的Reactor注册terminated回调函数",slug:"_4-向reactor线程组中所有的reactor注册terminated回调函数",normalizedTitle:"4. 向reactor线程组中所有的reactor注册terminated回调函数",charIndex:30999},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:32801},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:32992}],headersStr:"前言 Netty 服务端代码模板 Netty 对 IO 模型的支持 EventLoop EventLoopGroup ServerSocketChannel SocketChannel 多种NIO的实现 创建主从Reactor线程组 MultithreadEventLoopGroup MultithreadEventExecutorGroup 1. 创建用于启动Reactor线程的executor ThreadPerTaskExecutor 2. 创建Reactor newChild NioEventLoop openSelector SelectorProvider Netty对JDK NIO 原生Selector的优化 newTaskQueue Reactor对应的NioEventLoop类型继承结构 SingleThreadEventLoop SingleThreadEventExecutor 3. 创建Channel到Reactor的绑定策略 DefaultEventExecutorChooserFactory 4. 向Reactor线程组中所有的Reactor注册terminated回调函数 总结 参考资料",content:'# 前言\n\n在上篇文章《透过 Netty 看 IO 模型》中我们花了大量的篇幅来从内核角度详细讲述了五种IO模型的演进过程以及ReactorIO线程模型的底层基石IO多路复用技术在内核中的实现原理。\n\n最后我们引出了netty中使用的主从Reactor IO线程模型。\n\n\n\n通过上篇文章的介绍，我们已经清楚了在IO调用的过程中内核帮我们搞了哪些事情，那么俗话说的好内核领进门，修行在netty，netty在用户空间又帮我们搞了哪些事情?\n\n那么从本文开始，笔者将从源码角度来带大家看下上图中的Reactor IO线程模型在Netty中是如何实现的。\n\n本文作为Reactor在Netty中实现系列文章中的开篇文章，笔者先来为大家介绍Reactor的骨架是如何创建出来的。\n\n在上篇文章中我们提到Netty采用的是主从Reactor多线程的模型，但是它在实现上又与Doug Lea在Scalable IO in Java论文中提到的经典主从Reactor多线程模型有所差异。\n\n\n\nNetty中的Reactor是以Group的形式出现的，主从Reactor在Netty中就是主从Reactor组，每个Reactor Group中会有多个Reactor用来执行具体的IO任务。当然在netty中Reactor不只用来执行IO任务，这个我们后面再说。\n\n * Main Reactor Group中的Reactor数量取决于服务端要监听的端口个数，通常我们的服务端程序只会监听一个端口，所以Main Reactor Group只会有一个Main Reactor线程来处理最重要的事情：绑定端口地址，接收客户端连接，为客户端创建对应的SocketChannel，将客户端SocketChannel分配给一个固定的Sub Reactor。也就是上篇文章笔者为大家举的例子，饭店最重要的工作就是先把客人迎接进来。“我家大门常打开，开放怀抱等你，拥抱过就有了默契你会爱上这里......”\n * Sub Reactor Group里有多个Reactor线程，Reactor线程的个数可以通过系统参数-D io.netty.eventLoopThreads指定。默认的Reactor的个数为CPU核数 * 2。Sub Reactor线程主要用来轮询客户端SocketChannel上的IO就绪事件，处理IO就绪事件，执行异步任务。Sub Reactor Group做的事情就是上篇饭店例子中服务员的工作，客人进来了要为客人分配座位，端茶送水，做菜上菜。“不管远近都是客人，请不用客气，相约好了在一起，我们欢迎您......”\n\n> 一个客户端SocketChannel只能分配给一个固定的Sub Reactor。一个Sub Reactor负责处理多个客户端SocketChannel，这样可以将服务端承载的全量客户端连接分摊到多个Sub Reactor中处理，同时也能保证客户端SocketChannel上的IO处理的线程安全性。\n\n由于文章篇幅的关系，作为Reactor在netty中实现的第一篇我们主要来介绍主从Reactor Group的创建流程，骨架脉络先搭好。\n\n下面我们来看一段Netty服务端代码的编写模板，从代码模板的流程中我们来解析下主从Reactor的创建流程以及在这个过程中所涉及到的Netty核心类。\n\n\n# Netty 服务端代码模板\n\n/**\n * Echoes back any received data from a client.\n */\npublic final class EchoServer {\n    static final int PORT = Integer.parseInt(System.getProperty("port", "8007"));\n\n    public static void main(String[] args) throws Exception {\n        // Configure the server.\n        //创建主从Reactor线程组\n        EventLoopGroup bossGroup = new NioEventLoopGroup(1);\n        EventLoopGroup workerGroup = new NioEventLoopGroup();\n        final EchoServerHandler serverHandler = new EchoServerHandler();\n        try {\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)//配置主从Reactor\n             .channel(NioServerSocketChannel.class)//配置主Reactor中的channel类型\n             .option(ChannelOption.SO_BACKLOG, 100)//设置主Reactor中channel的option选项\n             .handler(new LoggingHandler(LogLevel.INFO))//设置主Reactor中Channel->pipline->handler\n             .childHandler(new ChannelInitializer<SocketChannel>() {//设置从Reactor中注册channel的pipeline\n                 @Override\n                 public void initChannel(SocketChannel ch) throws Exception {\n                     ChannelPipeline p = ch.pipeline();\n                     //p.addLast(new LoggingHandler(LogLevel.INFO));\n                     p.addLast(serverHandler);\n                 }\n             });\n\n            // Start the server. 绑定端口启动服务，开始监听accept事件\n            ChannelFuture f = b.bind(PORT).sync();\n            // Wait until the server socket is closed.\n            f.channel().closeFuture().sync();\n        } finally {\n            // Shut down all event loops to terminate all threads.\n            bossGroup.shutdownGracefully();\n            workerGroup.shutdownGracefully();\n        }\n    }\n}\n\n\n 1. 首先我们要创建Netty最核心的部分 -> 创建主从Reactor Group，在Netty中EventLoopGroup就是Reactor Group的实现类。对应的EventLoop就是Reactor的实现类。\n    \n      //创建主从Reactor线程组\n      EventLoopGroup bossGroup = new NioEventLoopGroup(1);\n      EventLoopGroup workerGroup = new NioEventLoopGroup();\n    \n\n 2. 创建用于IO处理的ChannelHandler，实现相应IO事件的回调函数，编写对应的IO处理逻辑。注意这里只是简单示例哈，详细的IO事件处理，笔者会单独开一篇文章专门讲述。\n    \n    final EchoServerHandler serverHandler = new EchoServerHandler();\n    \n    /**\n     * Handler implementation for the echo server.\n     */\n    @Sharable\n    public class EchoServerHandler extends ChannelInboundHandlerAdapter {\n    \n        @Override\n        public void channelRead(ChannelHandlerContext ctx, Object msg) {\n            ................省略IO处理逻辑................\n            ctx.write(msg);\n        }\n    \n        @Override\n        public void channelReadComplete(ChannelHandlerContext ctx) {\n            \n            ctx.flush();\n        }\n    \n        @Override\n        public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {\n            // Close the connection when an exception is raised.\n            cause.printStackTrace();\n            ctx.close();\n        }\n    }\n    \n\n 3. 创建ServerBootstrapNetty服务端启动类，并在启动类中配置启动Netty服务端所需要的一些必备信息。\n    \n    > 在上篇文章介绍Socket内核结构小节中我们提到，在编写服务端网络程序时，我们首先要创建一个Socket用于listen和bind端口地址，我们把这个叫做监听Socket,这里对应的就是NioServerSocketChannel.class。当客户端连接完成三次握手，系统调用accept函数会基于监听Socket创建出来一个新的Socket专门用于与客户端之间的网络通信我们称为客户端连接Socket,这里对应的就是NioSocketChannel.class\n    \n    > netty有两种Channel类型：一种是服务端用于监听绑定端口地址的NioServerSocketChannel,一种是用于客户端通信的NioSocketChannel。每种Channel类型实例都会对应一个PipeLine用于编排对应channel实例上的IO事件处理逻辑。PipeLine中组织的就是ChannelHandler用于编写特定的IO处理逻辑。\n    \n    > 注意serverBootstrap.handler设置的是服务端NioServerSocketChannel PipeLine中的ChannelHandler。\n    \n    > ServerBootstrap启动类方法带有child前缀的均是设置客户端NioSocketChannel属性的。\n    \n    > ChannelInitializer是用于当SocketChannel成功注册到绑定的Reactor上后，用于初始化该SocketChannel的Pipeline。它的initChannel方法会在注册成功后执行。这里只是捎带提一下，让大家有个初步印象，后面我会专门介绍。\n    \n    * serverBootstrap.childHandler(ChannelHandler childHandler)用于设置客户端NioSocketChannel中对应Pipieline中的ChannelHandler。我们通常配置的编码解码器就是在这里。\n    * serverBootstrap.option(ChannelOption.SO_BACKLOG, 100)设置服务端ServerSocketChannel中的SocketOption。关于SocketOption的选项我们后边的文章再聊，本文主要聚焦在Netty Main Reactor Group的创建及工作流程。\n    * serverBootstrap.handler(....)设置服务端NioServerSocketChannel中对应Pipieline中的ChannelHandler。\n    * 通过serverBootstrap.group(bossGroup, workerGroup)为Netty服务端配置主从Reactor Group实例。\n    * 通过serverBootstrap.channel(NioServerSocketChannel.class)配置Netty服务端的ServerSocketChannel用于绑定端口地址以及创建客户端SocketChannel。Netty中的NioServerSocketChannel.class就是对JDK NIO中ServerSocketChannel的封装。而用于表示客户端连接的NioSocketChannel是对JDK NIO SocketChannel封装。\n\n 4. ChannelFuture f = serverBootstrap.bind(PORT).sync()这一步会是下篇文章要重点分析的主题Main Reactor Group的启动，绑定端口地址，开始监听客户端连接事件（OP_ACCEPT）。本文我们只关注创建流程\n\n 5. f.channel().closeFuture().sync()等待服务端NioServerSocketChannel关闭。Netty服务端到这里正式启动，并准备好接受客户端连接的准备。\n\n 6. shutdownGracefully优雅关闭主从Reactor线程组里的所有Reactor线程。\n\n\n# Netty 对 IO 模型的支持\n\n在上篇文章中我们介绍了五种IO模型，Netty中支持BIO,NIO,AIO以及多种操作系统下的IO多路复用技术实现。\n\n在Netty中切换这几种IO模型也是非常的方便，下面我们来看下Netty如何对这几种IO模型进行支持的。\n\n首先我们介绍下几个与IO模型相关的重要接口：\n\n\n# EventLoop\n\nEventLoop就是Netty中的Reactor，可以说它就是Netty的引擎，负责Channel上IO就绪事件的监听，IO就绪事件的处理，异步任务的执行驱动着整个Netty的运转。\n\n不同IO模型下，EventLoop有着不同的实现，我们只需要切换不同的实现类就可以完成对NettyIO模型的切换。\n\nBIO                         NIO            AIO\nThreadPerChannelEventLoop   NioEventLoop   AioEventLoop\n\n在NIO模型下Netty会自动根据操作系统以及版本的不同选择对应的IO多路复用技术实现。比如Linux 2.6版本以上用的是Epoll，2.6版本以下用的是Poll，Mac下采用的是Kqueue。\n\n\n# EventLoopGroup\n\nNetty中的Reactor是以Group的形式出现的，EventLoopGroup正是Reactor组的接口定义，负责管理Reactor，Netty中的Channel就是通过EventLoopGroup注册到具体的Reactor上的。\n\nNetty的IO线程模型是主从Reactor多线程模型，主从Reactor线程组在Netty源码中对应的其实就是两个EventLoopGroup实例。\n\n不同的IO模型也有对应的实现：\n\nBIO                              NIO                 AIO\nThreadPerChannelEventLoopGroup   NioEventLoopGroup   AioEventLoopGroup\n\n\n# ServerSocketChannel\n\n用于Netty服务端使用的ServerSocketChannel，对应于上篇文章提到的监听Socket，负责绑定监听端口地址，接收客户端连接并创建用于与客户端通信的SocketChannel。\n\n不同的IO模型下的实现：\n\nBIO                      NIO                      AIO\nOioServerSocketChannel   NioServerSocketChannel   AioServerSocketChannel\n\n\n# SocketChannel\n\n用于与客户端通信的SocketChannel，对应于上篇文章提到的客户端连接Socket，当客户端完成三次握手后，由系统调用accept函数根据监听Socket创建。\n\n不同的IO模型下的实现：\n\nBIO                NIO                AIO\nOioSocketChannel   NioSocketChannel   AioSocketChannel\n\n我们看到在不同IO模型的实现中，Netty这些围绕IO模型的核心类只是前缀的不同：\n\n * BIO对应的前缀为Oio，表示old io，现在已经废弃不推荐使用。\n * NIO对应的前缀为Nio，正是Netty推荐也是我们常用的非阻塞IO模型。\n * AIO对应的前缀为Aio，由于Linux下的异步IO机制实现的并不成熟，性能提升表现上也不明显，现已被删除。\n\n我们只需要将IO模型的这些核心接口对应的实现类前缀改为对应IO模型的前缀，就可以轻松在Netty中完成对IO模型的切换。\n\n\n\n\n# 多种NIO的实现\n\nCOMMON                   LINUX                      MAC\nNioEventLoopGroup        EpollEventLoopGroup        KQueueEventLoopGroup\nNioEventLoop             EpollEventLoop             KQueueEventLoop\nNioServerSocketChannel   EpollServerSocketChannel   KQueueServerSocketChannel\nNioSocketChannel         EpollSocketChannel         KQueueSocketChannel\n\n我们通常在使用NIO模型的时候会使用Common列下的这些IO模型核心类，Common类也会根据操作系统的不同自动选择JDK在对应平台下的IO多路复用技术的实现。\n\n而Netty自身也根据操作系统的不同提供了自己对IO多路复用技术的实现，比JDK的实现性能更优。比如：\n\n * JDK的 NIO 默认实现是水平触发，Netty 是边缘触发(默认)和水平触发可切换。。\n * Netty 实现的垃圾回收更少、性能更好。\n\n我们编写Netty服务端程序的时候也可以根据操作系统的不同，采用Netty自身的实现来进一步优化程序。做法也很简单，直接将上图中红框里的实现类替换成Netty的自身实现类即可完成切换。\n\n----------------------------------------\n\n经过以上对Netty服务端代码编写模板以及IO模型相关核心类的简单介绍，我们对Netty的创建流程有了一个简单粗略的总体认识，下面我们来深入剖析下创建流程过程中的每一个步骤以及这个过程中涉及到的核心类实现。\n\n以下源码解析部分我们均采用Common列下NIO相关的实现进行解析。\n\n\n# 创建主从Reactor线程组\n\n在 Netty 服务端程序编写模板的开始，我们首先会创建两个Reactor线程组：\n\n\n\n * 一个是主Reactor线程组bossGroup用于监听客户端连接，创建客户端连接NioSocketChannel，并将创建好的客户端连接NioSocketChannel注册到从Reactor线程组中一个固定的Reactor上。\n * 一个是从Reactor线程组workerGroup，workerGroup中的Reactor负责监听绑定在其上的客户端连接NioSocketChannel上的IO就绪事件，并处理IO就绪事件，执行异步任务。\n\n//创建主从Reactor线程组\nEventLoopGroup bossGroup = new NioEventLoopGroup(1);\nEventLoopGroup workerGroup = new NioEventLoopGroup();\n\n\nNetty中Reactor线程组的实现类为NioEventLoopGroup，在创建bossGroup和workerGroup的时候用到了NioEventLoopGroup的两个构造函数：\n\n * 带nThreads参数的构造函数public NioEventLoopGroup(int nThreads)。\n * 不带nThreads参数的默认构造函数public NioEventLoopGroup()\n\npublic class NioEventLoopGroup extends MultithreadEventLoopGroup {\n\n    /**\n     * Create a new instance using the default number of threads, the default {@link ThreadFactory} and\n     * the {@link SelectorProvider} which is returned by {@link SelectorProvider#provider()}.\n     */\n    public NioEventLoopGroup() {\n        this(0);\n    }\n\n    /**\n     * Create a new instance using the specified number of threads, {@link ThreadFactory} and the\n     * {@link SelectorProvider} which is returned by {@link SelectorProvider#provider()}.\n     */\n    public NioEventLoopGroup(int nThreads) {\n        this(nThreads, (Executor) null);\n    }\n\n    ......................省略...........................\n}\n\n\n> nThreads参数表示当前要创建的Reactor线程组内包含多少个Reactor线程。不指定nThreads参数的话采用默认的Reactor线程个数，用0表示。\n\n最终会调用到构造函数\n\npublic NioEventLoopGroup(int nThreads, Executor executor, final SelectorProvider selectorProvider,\nfinal SelectStrategyFactory selectStrategyFactory) {\n\tsuper(nThreads, executor, selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject());\n}\n\n\n下面简单介绍下构造函数中这几个参数的作用，后面我们在讲解本文主线的过程中还会提及这几个参数，到时在详细介绍，这里只是让大家有个初步印象，不必做过多的纠缠。\n\n * Executor executor：负责启动Reactor线程进而Reactor才可以开始工作。\n\n> Reactor线程组NioEventLoopGroup负责创建Reactor线程，在创建的时候会将executor传入。\n\n * RejectedExecutionHandler： 当向Reactor添加异步任务添加失败时，采用的拒绝策略。Reactor的任务不只是监听IO活跃事件和IO任务的处理，还包括对异步任务的处理。这里大家只需有个这样的概念，后面笔者会专门详细介绍。\n * SelectorProvider selectorProvider： Reactor中的IO模型为IO多路复用模型，对应于JDK NIO中的实现为java.nio.channels.Selector（就是我们上篇文章中提到的select,poll,epoll），每个Reator中都包含一个Selector，用于轮询注册在该Reactor上的所有Channel上的IO事件。SelectorProvider就是用来创建Selector的。\n * SelectStrategyFactory selectStrategyFactory： Reactor最重要的事情就是轮询注册其上的Channel上的IO就绪事件，这里的SelectStrategyFactory用于指定轮询策略，默认为DefaultSelectStrategyFactory.INSTANCE。\n\n最终会将这些参数交给NioEventLoopGroup的父类构造器，下面我们来看下NioEventLoopGroup类的继承结构：\n\n\n\nNioEventLoopGroup类的继承结构乍一看比较复杂，大家不要慌，笔者会随着主线的深入慢慢地介绍这些父类接口，我们现在重点关注Mutithread前缀的类。\n\n我们知道NioEventLoopGroup是Netty中的Reactor线程组的实现，既然是线程组那么肯定是负责管理和创建多个Reactor线程的，所以Mutithread前缀的类定义的行为自然是对Reactor线程组内多个Reactor线程的创建和管理工作。\n\n\n# MultithreadEventLoopGroup\n\npublic abstract class MultithreadEventLoopGroup extends MultithreadEventExecutorGroup implements EventLoopGroup {\n\n    private static final InternalLogger logger = InternalLoggerFactory.getInstance(MultithreadEventLoopGroup.class);\n    //默认Reactor个数\n    private static final int DEFAULT_EVENT_LOOP_THREADS;\n\n    static {\n        DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt(\n                "io.netty.eventLoopThreads", NettyRuntime.availableProcessors() * 2));\n\n        if (logger.isDebugEnabled()) {\n            logger.debug("-Dio.netty.eventLoopThreads: {}", DEFAULT_EVENT_LOOP_THREADS);\n        }\n    }\n\n    /**\n     * @see MultithreadEventExecutorGroup#MultithreadEventExecutorGroup(int, Executor, Object...)\n     */\n    protected MultithreadEventLoopGroup(int nThreads, Executor executor, Object... args) {\n        super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, executor, args);\n    }\n\n    ...................省略.....................\n}\n\n\nMultithreadEventLoopGroup类主要的功能就是用来确定Reactor线程组内Reactor的个数。\n\n默认的Reactor的个数存放于字段DEFAULT_EVENT_LOOP_THREADS中。\n\n从static {}静态代码块中我们可以看出默认Reactor的个数的获取逻辑：\n\n * 可以通过系统变量 -D io.netty.eventLoopThreads"指定。\n * 如果不指定，那么默认的就是NettyRuntime.availableProcessors() * 2\n\n当nThread参数设置为0采用默认设置时，Reactor线程组内的Reactor个数则设置为DEFAULT_EVENT_LOOP_THREADS。\n\n\n# MultithreadEventExecutorGroup\n\nMultithreadEventExecutorGroup这里就是本小节的核心，主要用来定义创建和管理Reactor的行为。\n\npublic abstract class MultithreadEventExecutorGroup extends AbstractEventExecutorGroup {\n\n    //Reactor线程组中的Reactor集合\n    private final EventExecutor[] children;\n    private final Set<EventExecutor> readonlyChildren;\n    //从Reactor group中选择一个特定的Reactor的选择策略 用于channel注册绑定到一个固定的Reactor上\n    private final EventExecutorChooserFactory.EventExecutorChooser chooser;\n\n    /**\n     * Create a new instance.\n     *\n     * @param nThreads          the number of threads that will be used by this instance.\n     * @param executor          the Executor to use, or {@code null} if the default should be used.\n     * @param args              arguments which will passed to each {@link #newChild(Executor, Object...)} call\n     */\n    protected MultithreadEventExecutorGroup(int nThreads, Executor executor, Object... args) {\n        this(nThreads, executor, DefaultEventExecutorChooserFactory.INSTANCE, args);\n    }\n\n    ............................省略................................\n}\n\n\n首先介绍一个新的构造器参数EventExecutorChooserFactory chooserFactory。当客户端连接完成三次握手后，Main Reactor会创建客户端连接NioSocketChannel，并将其绑定到Sub Reactor Group中的一个固定Reactor，那么具体要绑定到哪个具体的Sub Reactor上呢？这个绑定策略就是由chooserFactory来创建的。默认为DefaultEventExecutorChooserFactory。\n\n下面就是本小节的主题Reactor线程组的创建过程：\n\n    protected MultithreadEventExecutorGroup(int nThreads, Executor executor,\n                                            EventExecutorChooserFactory chooserFactory, Object... args) {\n        if (nThreads <= 0) {\n            throw new IllegalArgumentException(String.format("nThreads: %d (expected: > 0)", nThreads));\n        }\n\n        if (executor == null) {\n            //用于创建Reactor线程\n            executor = new ThreadPerTaskExecutor(newDefaultThreadFactory());\n        }\n\n        children = new EventExecutor[nThreads];\n        //循环创建reaactor group 中的 Reactor\n        for (int i = 0; i < nThreads; i ++) {\n            boolean success = false;\n            try {\n                //创建reactor\n                children[i] = newChild(executor, args);\n                success = true;\n            } catch (Exception e) {\n                throw new IllegalStateException("failed to create a child event loop", e);\n            } finally {\n                     ................省略................\n                }\n            }\n        }\n        //创建channel到Reactor的绑定策略\n        chooser = chooserFactory.newChooser(children);\n\n         ................省略................\n\n        Set<EventExecutor> childrenSet = new LinkedHashSet<EventExecutor>(children.length);\n        Collections.addAll(childrenSet, children);\n        readonlyChildren = Collections.unmodifiableSet(childrenSet);\n    }\n\n\n# 1. 创建用于启动Reactor线程的executor\n\n在Netty Reactor Group中的单个Reactor的IO线程模型为上篇文章提到的单Reactor单线程模型，一个Reactor线程负责轮询注册其上的所有Channel中的IO就绪事件，处理IO事件，执行Netty中的异步任务等工作。正是这个Reactor线程驱动着整个Netty的运转，可谓是Netty的核心引擎。\n\n\n\n而这里的executor就是负责启动Reactor线程的，从创建源码中我们可以看到executor的类型为ThreadPerTaskExecutor。\n\n# ThreadPerTaskExecutor\n\npublic final class ThreadPerTaskExecutor implements Executor {\n    private final ThreadFactory threadFactory;\n\n    public ThreadPerTaskExecutor(ThreadFactory threadFactory) {\n        this.threadFactory = ObjectUtil.checkNotNull(threadFactory, "threadFactory");\n    }\n\n    @Override\n    public void execute(Runnable command) {\n        threadFactory.newThread(command).start();\n    }\n}\n\n\n我们看到ThreadPerTaskExecutor做的事情很简单，从它的命名前缀ThreadPerTask我们就可以猜出它的工作方式，就是来一个任务就创建一个线程执行。而创建的这个线程正是netty的核心引擎Reactor线程。\n\n在Reactor线程启动的时候，Netty会将Reactor线程要做的事情封装成Runnable，丢给exexutor启动。\n\n而Reactor线程的核心就是一个死循环不停的轮询IO就绪事件，处理IO事件，执行异步任务。一刻也不停歇，堪称996典范。\n\n这里向大家先卖个关子，"Reactor线程是何时启动的呢？？"\n\n# 2. 创建Reactor\n\nReactor线程组NioEventLoopGroup包含多个Reactor，存放于private final EventExecutor[] children数组中。\n\n所以下面的事情就是创建nThread个Reactor，并存放于EventExecutor[] children字段中，\n\n我们来看下用于创建Reactor的newChild(executor, args)方法：\n\n# newChild\n\nnewChild方法是MultithreadEventExecutorGroup中的一个抽象方法，提供给具体子类实现。\n\nprotected abstract EventExecutor newChild(Executor executor, Object... args) throws Exception;\n\n\n这里我们解析的是NioEventLoopGroup，我们来看下newChild在该类中的实现：\n\npublic class NioEventLoopGroup extends MultithreadEventLoopGroup {\n    @Override\n    protected EventLoop newChild(Executor executor, Object... args) throws Exception {\n        EventLoopTaskQueueFactory queueFactory = args.length == 4 ? (EventLoopTaskQueueFactory) args[3] : null;\n        return new NioEventLoop(this, executor, (SelectorProvider) args[0],\n            ((SelectStrategyFactory) args[1]).newSelectStrategy(), (RejectedExecutionHandler) args[2], queueFactory);\n    }\n}\n\n\n前边提到的众多构造器参数，这里会通过可变参数Object... args传入到Reactor类NioEventLoop的构造器中。\n\n这里介绍下新的参数EventLoopTaskQueueFactory queueFactory，前边提到Netty中的Reactor主要工作是轮询注册其上的所有Channel上的IO就绪事件，处理IO就绪事件。除了这些主要的工作外，Netty为了极致的压榨Reactor的性能，还会让它做一些异步任务的执行工作。既然要执行异步任务，那么Reactor中就需要一个队列来保存任务。\n\n这里的EventLoopTaskQueueFactory就是用来创建这样的一个队列来保存Reactor中待执行的异步任务。\n\n可以把Reactor理解成为一个单线程的线程池，类似于JDK中的SingleThreadExecutor，仅用一个线程来执行轮询IO就绪事件，处理IO就绪事件，执行异步任务。同时待执行的异步任务保存在Reactor里的taskQueue中。\n\n# NioEventLoop\n\npublic final class NioEventLoop extends SingleThreadEventLoop {\n    //用于创建JDK NIO Selector,ServerSocketChannel\n    private final SelectorProvider provider;\n    //Selector轮询策略 决定什么时候轮询，什么时候处理IO事件，什么时候执行异步任务\n    private final SelectStrategy selectStrategy;\n    /**\n     * The NIO {@link Selector}.\n     */\n    private Selector selector;\n    private Selector unwrappedSelector;\n\n    NioEventLoop(NioEventLoopGroup parent, Executor executor, SelectorProvider selectorProvider,\n                 SelectStrategy strategy, RejectedExecutionHandler rejectedExecutionHandler,\n                 EventLoopTaskQueueFactory queueFactory) {\n        super(parent, executor, false, newTaskQueue(queueFactory), newTaskQueue(queueFactory),\n                rejectedExecutionHandler);\n        this.provider = ObjectUtil.checkNotNull(selectorProvider, "selectorProvider");\n        this.selectStrategy = ObjectUtil.checkNotNull(strategy, "selectStrategy");\n        final SelectorTuple selectorTuple = openSelector();\n        this.selector = selectorTuple.selector;\n        this.unwrappedSelector = selectorTuple.unwrappedSelector;\n    }\n}\n\n\n这里就正式开始了Reactor的创建过程，我们知道Reactor的核心是采用的IO多路复用模型来对客户端连接上的IO事件进行监听，所以最重要的事情是创建Selector(JDK NIO 中IO多路复用技术的实现)。\n\n> 可以把Selector理解为我们上篇文章介绍的Select,poll,epoll，它是JDK NIO对操作系统内核提供的这些IO多路复用技术的封装。\n\n# openSelector\n\nopenSelector是NioEventLoop类中用于创建IO多路复用的Selector，并对创建出来的JDK NIO 原生的Selector进行性能优化。\n\n首先会通过SelectorProvider#openSelector创建JDK NIO原生的Selector。\n\nprivate SelectorTuple openSelector() {\n    final Selector unwrappedSelector;\n    try {\n        //通过JDK NIO SelectorProvider创建Selector\n        unwrappedSelector = provider.openSelector();\n    } catch (IOException e) {\n    \tthrow new ChannelException("failed to open a new selector", e);\n    }\n\n    ..................省略.............\n}\n\n\nSelectorProvider会根据操作系统的不同选择JDK在不同操作系统版本下的对应Selector的实现。Linux下会选择Epoll，Mac下会选择Kqueue。\n\n下面我们就来看下SelectorProvider是如何做到自动适配不同操作系统下IO多路复用实现的\n\n# SelectorProvider\n\npublic NioEventLoopGroup(ThreadFactory threadFactory) {\n\tthis(0, threadFactory, SelectorProvider.provider());\n}\n\n\nSelectorProvider是在前面介绍的NioEventLoopGroup类构造函数中通过调用SelectorProvider.provider()被加载，并通过NioEventLoopGroup#newChild方法中的可变长参数Object... args传递到NioEventLoop中的private final SelectorProvider provider字段中。\n\nSelectorProvider的加载过程：\n\npublic abstract class SelectorProvider {\n\n    public static SelectorProvider provider() {\n        synchronized (lock) {\n            if (provider != null)\n                return provider;\n            return AccessController.doPrivileged(\n                new PrivilegedAction<SelectorProvider>() {\n                    public SelectorProvider run() {\n                            if (loadProviderFromProperty())\n                                return provider;\n                            if (loadProviderAsService())\n                                return provider;\n                            provider = sun.nio.ch.DefaultSelectorProvider.create();\n                            return provider;\n                        }\n                    });\n        }\n    }\n}\n\n\n从SelectorProvider加载源码中我们可以看出，SelectorProvider的加载方式有三种，优先级如下：\n\n 1. 通过系统变量-D java.nio.channels.spi.SelectorProvider指定SelectorProvider的自定义实现类全限定名。通过应用程序类加载器(Application Classloader)加载。\n\nprivate static boolean loadProviderFromProperty() {\n    String cn = System.getProperty("java.nio.channels.spi.SelectorProvider");\n    if (cn == null)\n    \treturn false;\n    try {\n        Class<?> c = Class.forName(cn, true,\n        ClassLoader.getSystemClassLoader());\n        provider = (SelectorProvider)c.newInstance();\n        return true;\n    } \n    .................省略.............\n}\n\n\n 2. 通过SPI方式加载。在工程目录META-INF/services下定义名为java.nio.channels.spi.SelectorProvider的SPI文件，文件中第一个定义的SelectorProvider实现类全限定名就会被加载。\n\nprivate static boolean loadProviderAsService() {\n\n        ServiceLoader<SelectorProvider> sl =\n            ServiceLoader.load(SelectorProvider.class,\n                               ClassLoader.getSystemClassLoader());\n        Iterator<SelectorProvider> i = sl.iterator();\n        for (;;) {\n            try {\n                if (!i.hasNext())\n                    return false;\n                provider = i.next();\n                return true;\n            } catch (ServiceConfigurationError sce) {\n                if (sce.getCause() instanceof SecurityException) {\n                    // Ignore the security exception, try the next provider\n                    continue;\n                }\n                throw sce;\n            }\n        }\n    }\n\n\n 3. 如果以上两种方式均未被定义，那么就采用SelectorProvider系统默认实现sun.nio.ch.DefaultSelectorProvider。笔者当前使用的操作系统是MacOS，从源码中我们可以看到自动适配了KQueue实现。\n\npublic class DefaultSelectorProvider {\n    private DefaultSelectorProvider() {\n    }\n\n    public static SelectorProvider create() {\n        return new KQueueSelectorProvider();\n    }\n}\n\n\n下面我们接着回到io.netty.channel.nio.NioEventLoop#openSelector的主线上来。\n\n# Netty对JDK NIO 原生Selector的优化\n\n建议查看 https://mp.weixin.qq.com/s/IuIsUtpiye13L8ZyHWvzXA\n\n# newTaskQueue\n\nNioEventLoop(NioEventLoopGroup parent, Executor executor, SelectorProvider selectorProvider,SelectStrategy strategy, RejectedExecutionHandler rejectedExecutionHandler,EventLoopTaskQueueFactory queueFactory) {\t\t  \tsuper(parent, executor, false, newTaskQueue(queueFactory), newTaskQueue(queueFactory),rejectedExecutionHandler);\n    this.provider = ObjectUtil.checkNotNull(selectorProvider, "selectorProvider");\n    this.selectStrategy = ObjectUtil.checkNotNull(strategy, "selectStrategy");\n    final SelectorTuple selectorTuple = openSelector();\n    //通过用SelectedSelectionKeySet装饰后的unwrappedSelector\n    this.selector = selectorTuple.selector;\n    //Netty优化过的JDK NIO远程Selector\n    this.unwrappedSelector = selectorTuple.unwrappedSelector;\n}\n\n\n我们继续回到创建Reactor的主线上，到目前为止Reactor的核心Selector就创建好了，前边我们提到Reactor除了需要监听IO就绪事件以及处理IO就绪事件外，还需要执行一些异步任务，当外部线程向Reactor提交异步任务后，Reactor就需要一个队列来保存这些异步任务，等待Reactor线程执行。\n\n下面我们来看下Reactor中任务队列的创建过程：\n\n    //任务队列大小，默认是无界队列\n    protected static final int DEFAULT_MAX_PENDING_TASKS = Math.max(16,\n            SystemPropertyUtil.getInt("io.netty.eventLoop.maxPendingTasks", Integer.MAX_VALUE));\n\n    private static Queue<Runnable> newTaskQueue(\n            EventLoopTaskQueueFactory queueFactory) {\n        if (queueFactory == null) {\n            return newTaskQueue0(DEFAULT_MAX_PENDING_TASKS);\n        }\n        return queueFactory.newTaskQueue(DEFAULT_MAX_PENDING_TASKS);\n    }\n\n    private static Queue<Runnable> newTaskQueue0(int maxPendingTasks) {\n        // This event loop never calls takeTask()\n        return maxPendingTasks == Integer.MAX_VALUE ? PlatformDependent.<Runnable>newMpscQueue()\n                : PlatformDependent.<Runnable>newMpscQueue(maxPendingTasks);\n    }  \n\n\n * 在NioEventLoop的父类SingleThreadEventLoop中提供了一个静态变量DEFAULT_MAX_PENDING_TASKS用来指定Reactor任务队列的大小。可以通过系统变量-D io.netty.eventLoop.maxPendingTasks进行设置，默认为Integer.MAX_VALUE，表示任务队列默认为无界队列。\n\n * 根据DEFAULT_MAX_PENDING_TASKS变量的设定，来决定创建无界任务队列还是有界任务队列。\n\n//创建无界任务队列\nPlatformDependent.<Runnable>newMpscQueue()\n//创建有界任务队列\nPlatformDependent.<Runnable>newMpscQueue(maxPendingTasks)\n\npublic static <T> Queue<T> newMpscQueue() {\n\treturn Mpsc.newMpscQueue();\n}\n\npublic static <T> Queue<T> newMpscQueue(final int maxCapacity) {\n\treturn Mpsc.newMpscQueue(maxCapacity);\n}\n\n\n> Reactor内的异步任务队列的类型为MpscQueue,它是由JCTools提供的一个高性能无锁队列，从命名前缀Mpsc可以看出，它适用于多生产者单消费者的场景，它支持多个生产者线程安全的访问队列，同一时刻只允许一个消费者线程读取队列中的元素。\n\n> 我们知道Netty中的Reactor可以线程安全的处理注册其上的多个SocketChannel上的IO数据，保证Reactor线程安全的核心原因正是因为这个MpscQueue，它可以支持多个业务线程在处理完业务逻辑后，线程安全的向MpscQueue添加异步写任务，然后由单个Reactor线程来执行这些写任务。既然是单线程执行，那肯定是线程安全的了。\n\n# Reactor对应的NioEventLoop类型继承结构\n\n\n\nNioEventLoop的继承结构也是比较复杂，这里我们只关注在Reactor创建过程中涉及的到两个父类SingleThreadEventLoop,SingleThreadEventExecutor。\n\n剩下的继承体系，我们在后边随着Netty源码的深入在慢慢介绍。\n\n前边我们提到，其实Reactor我们可以看作是一个单线程的线程池，只有一个线程用来执行IO就绪事件的监听，IO事件的处理，异步任务的执行。用MpscQueue来存储待执行的异步任务。\n\n命名前缀为SingleThread的父类都是对Reactor这些行为的分层定义。也是本小节要介绍的对象\n\n# SingleThreadEventLoop\n\nReactor负责执行的异步任务分为三类：\n\n * 普通任务：这是Netty最主要执行的异步任务，存放在普通任务队列taskQueue中。在NioEventLoop构造函数中创建。\n * 定时任务： 存放在优先级队列中。后续我们介绍。\n * 尾部任务： 存放于尾部任务队列tailTasks中，尾部任务一般不常用，在普通任务执行完后 Reactor线程会执行尾部任务。**使用场景：**比如对Netty 的运行状态做一些统计数据，例如任务循环的耗时、占用物理内存的大小等等都可以向尾部队列添加一个收尾任务完成统计数据的实时更新。\n\nSingleThreadEventLoop负责对尾部任务队列tailTasks进行管理。并且提供Channel向Reactor注册的行为。\n\npublic abstract class SingleThreadEventLoop extends SingleThreadEventExecutor implements EventLoop {\n\n    //任务队列大小，默认是无界队列\n    protected static final int DEFAULT_MAX_PENDING_TASKS = Math.max(16,\n            SystemPropertyUtil.getInt("io.netty.eventLoop.maxPendingTasks", Integer.MAX_VALUE));\n    \n    //尾部任务队列\n    private final Queue<Runnable> tailTasks;\n\n    protected SingleThreadEventLoop(EventLoopGroup parent, Executor executor,\n                                    boolean addTaskWakesUp, Queue<Runnable> taskQueue, Queue<Runnable> tailTaskQueue,\n                                    RejectedExecutionHandler rejectedExecutionHandler) {\n        super(parent, executor, addTaskWakesUp, taskQueue, rejectedExecutionHandler);\n        //尾部队列 执行一些统计任务 不常用\n        tailTasks = ObjectUtil.checkNotNull(tailTaskQueue, "tailTaskQueue");\n    }\n\n    @Override\n    public ChannelFuture register(Channel channel) {\n        //注册channel到绑定的Reactor上\n        return register(new DefaultChannelPromise(channel, this));\n    }\n}\n\n\n# SingleThreadEventExecutor\n\nSingleThreadEventExecutor主要负责对普通任务队列的管理，以及异步任务的执行，Reactor线程的启停。\n\npublic abstract class SingleThreadEventExecutor extends AbstractScheduledEventExecutor implements OrderedEventExecutor {\n\n    protected SingleThreadEventExecutor(EventExecutorGroup parent, Executor executor,\n                                        boolean addTaskWakesUp, Queue<Runnable> taskQueue, RejectedExecutionHandler rejectedHandler) {\n        //parent为Reactor所属的NioEventLoopGroup Reactor线程组\n        super(parent);\n        //向Reactor添加任务时，是否唤醒Selector停止轮询IO就绪事件，马上执行异步任务\n        this.addTaskWakesUp = addTaskWakesUp;\n        //Reactor异步任务队列的大小\n        this.maxPendingTasks = DEFAULT_MAX_PENDING_EXECUTOR_TASKS;\n        //用于启动Reactor线程的executor -> ThreadPerTaskExecutor\n        this.executor = ThreadExecutorMap.apply(executor, this);\n        //普通任务队列\n        this.taskQueue = ObjectUtil.checkNotNull(taskQueue, "taskQueue");\n        //任务队列满时的拒绝策略\n        this.rejectedExecutionHandler = ObjectUtil.checkNotNull(rejectedHandler, "rejectedHandler");\n    }\n}\n\n\n到现在为止，一个完整的Reactor架构就被创建出来了。\n\n\n\n# 3. 创建Channel到Reactor的绑定策略\n\n到这一步，Reactor线程组NioEventLoopGroup里边的所有Reactor就已经全部创建完毕。\n\n无论是Netty服务端NioServerSocketChannel关注的OP_ACCEPT事件也好，还是Netty客户端NioSocketChannel关注的OP_READ和OP_WRITE事件也好，都需要先注册到Reactor上，Reactor才能监听Channel上关注的IO事件实现IO多路复用。\n\nNioEventLoopGroup（Reactor线程组）里边有众多的Reactor，那么以上提到的这些Channel究竟应该注册到哪个Reactor上呢？这就需要一个绑定的策略来平均分配。\n\n还记得我们前边介绍MultithreadEventExecutorGroup类的时候提到的构造器参数EventExecutorChooserFactory吗？\n\n这时候它就派上用场了，它主要用来创建Channel到Reactor的绑定策略。默认为DefaultEventExecutorChooserFactory.INSTANCE。\n\npublic abstract class MultithreadEventExecutorGroup extends AbstractEventExecutorGroup {\n   //从Reactor集合中选择一个特定的Reactor的绑定策略 用于channel注册绑定到一个固定的Reactor上\n    private final EventExecutorChooserFactory.EventExecutorChooser chooser;\n\n    chooser = chooserFactory.newChooser(children);\n}\n\n\n下面我们来看下具体的绑定策略：\n\n# DefaultEventExecutorChooserFactory\n\npublic final class DefaultEventExecutorChooserFactory implements EventExecutorChooserFactory {\n\n    public static final DefaultEventExecutorChooserFactory INSTANCE = new DefaultEventExecutorChooserFactory();\n\n    private DefaultEventExecutorChooserFactory() { }\n\n    @Override\n    public EventExecutorChooser newChooser(EventExecutor[] executors) {\n        if (isPowerOfTwo(executors.length)) {\n            return new PowerOfTwoEventExecutorChooser(executors);\n        } else {\n            return new GenericEventExecutorChooser(executors);\n        }\n    }\n\n    private static boolean isPowerOfTwo(int val) {\n        return (val & -val) == val;\n    }\n    ...................省略.................\n}\n\n\n我们看到在newChooser方法绑定策略有两个分支，不同之处在于需要判断Reactor线程组中的Reactor个数是否为2的次幂。\n\nNetty中的绑定策略就是采用round-robin轮询的方式来挨个选择Reactor进行绑定。\n\n采用round-robin的方式进行负载均衡，我们一般会用round % reactor.length取余的方式来挨个平均的定位到对应的Reactor上。\n\n如果Reactor的个数reactor.length恰好是2的次幂，那么就可以用位操作&运算round & reactor.length -1来代替%运算round % reactor.length，因为位运算的性能更高。具体为什么&运算能够代替%运算，笔者会在后面讲述时间轮的时候为大家详细证明，这里大家只需记住这个公式，我们还是聚焦本文的主线。\n\n了解了优化原理，我们在看代码实现就很容易理解了。\n\n利用%运算的方式Math.abs(idx.getAndIncrement() % executors.length)来进行绑定。\n\n    private static final class GenericEventExecutorChooser implements EventExecutorChooser {\n        private final AtomicLong idx = new AtomicLong();\n        private final EventExecutor[] executors;\n\n        GenericEventExecutorChooser(EventExecutor[] executors) {\n            this.executors = executors;\n        }\n\n        @Override\n        public EventExecutor next() {\n            return executors[(int) Math.abs(idx.getAndIncrement() % executors.length)];\n        }\n    }\n\n\n利用&运算的方式idx.getAndIncrement() & executors.length - 1来进行绑定。\n\n    private static final class PowerOfTwoEventExecutorChooser implements EventExecutorChooser {\n        private final AtomicInteger idx = new AtomicInteger();\n        private final EventExecutor[] executors;\n\n        PowerOfTwoEventExecutorChooser(EventExecutor[] executors) {\n            this.executors = executors;\n        }\n\n        @Override\n        public EventExecutor next() {\n            return executors[idx.getAndIncrement() & executors.length - 1];\n        }\n    }\n\n\n> 又一次被Netty对性能的极致追求所折服~~~~\n\n# 4. 向Reactor线程组中所有的Reactor注册terminated回调函数\n\n当Reactor线程组NioEventLoopGroup中所有的Reactor已经创建完毕，Channel到Reactor的绑定策略也创建完毕后，我们就来到了创建NioEventGroup的最后一步。\n\n俗话说的好，有创建就有启动，有启动就有关闭，这里会创建Reactor关闭的回调函数terminationListener，在Reactor关闭时回调。\n\nterminationListener回调的逻辑很简单：\n\n * 通过AtomicInteger terminatedChildren变量记录已经关闭的Reactor个数，用来判断NioEventLoopGroup中的Reactor是否已经全部关闭。\n * 如果所有Reactor均已关闭，设置NioEventLoopGroup中的terminationFuture为success。表示Reactor线程组关闭成功。\n\n       //记录关闭的Reactor个数，当Reactor全部关闭后，才可以认为关闭成功\n        private final AtomicInteger terminatedChildren = new AtomicInteger();\n        //关闭future\n        private final Promise<?> terminationFuture = new DefaultPromise(GlobalEventExecutor.INSTANCE);\n\n        final FutureListener<Object> terminationListener = new FutureListener<Object>() {\n            @Override\n            public void operationComplete(Future<Object> future) throws Exception {\n                if (terminatedChildren.incrementAndGet() == children.length) {\n                    //当所有Reactor关闭后 才认为是关闭成功\n                    terminationFuture.setSuccess(null);\n                }\n            }\n        };\n        \n        //为所有Reactor添加terminationListener\n        for (EventExecutor e: children) {\n            e.terminationFuture().addListener(terminationListener);\n        }\n\n\n----------------------------------------\n\n我们在回到文章开头Netty服务端代码模板\n\npublic final class EchoServer {\n    static final int PORT = Integer.parseInt(System.getProperty("port", "8007"));\n\n    public static void main(String[] args) throws Exception {\n        // Configure the server.\n        //创建主从Reactor线程组\n        EventLoopGroup bossGroup = new NioEventLoopGroup(1);\n        EventLoopGroup workerGroup = new NioEventLoopGroup();\n\n        ...........省略............\n    }\n}\n\n\n现在Netty的主从Reactor线程组就已经创建完毕，此时Netty服务端的骨架已经搭建完毕，骨架如下：\n\n\n\n\n# 总结\n\n本文介绍了首先介绍了Netty对各种IO模型的支持以及如何轻松切换各种IO模型。\n\n还花了大量的篇幅介绍Netty服务端的核心引擎主从Reactor线程组的创建过程。在这个过程中，我们还提到了Netty对各种细节进行的优化，展现了Netty对性能极致的追求。\n\n好了，Netty服务端的骨架已经搭好，剩下的事情就该绑定端口地址然后接收连接了，我们下篇文章再见~~~\n\n\n# 参考资料\n\nhttps://mp.weixin.qq.com/s/IuIsUtpiye13L8ZyHWvzXA',normalizedContent:'# 前言\n\n在上篇文章《透过 netty 看 io 模型》中我们花了大量的篇幅来从内核角度详细讲述了五种io模型的演进过程以及reactorio线程模型的底层基石io多路复用技术在内核中的实现原理。\n\n最后我们引出了netty中使用的主从reactor io线程模型。\n\n\n\n通过上篇文章的介绍，我们已经清楚了在io调用的过程中内核帮我们搞了哪些事情，那么俗话说的好内核领进门，修行在netty，netty在用户空间又帮我们搞了哪些事情?\n\n那么从本文开始，笔者将从源码角度来带大家看下上图中的reactor io线程模型在netty中是如何实现的。\n\n本文作为reactor在netty中实现系列文章中的开篇文章，笔者先来为大家介绍reactor的骨架是如何创建出来的。\n\n在上篇文章中我们提到netty采用的是主从reactor多线程的模型，但是它在实现上又与doug lea在scalable io in java论文中提到的经典主从reactor多线程模型有所差异。\n\n\n\nnetty中的reactor是以group的形式出现的，主从reactor在netty中就是主从reactor组，每个reactor group中会有多个reactor用来执行具体的io任务。当然在netty中reactor不只用来执行io任务，这个我们后面再说。\n\n * main reactor group中的reactor数量取决于服务端要监听的端口个数，通常我们的服务端程序只会监听一个端口，所以main reactor group只会有一个main reactor线程来处理最重要的事情：绑定端口地址，接收客户端连接，为客户端创建对应的socketchannel，将客户端socketchannel分配给一个固定的sub reactor。也就是上篇文章笔者为大家举的例子，饭店最重要的工作就是先把客人迎接进来。“我家大门常打开，开放怀抱等你，拥抱过就有了默契你会爱上这里......”\n * sub reactor group里有多个reactor线程，reactor线程的个数可以通过系统参数-d io.netty.eventloopthreads指定。默认的reactor的个数为cpu核数 * 2。sub reactor线程主要用来轮询客户端socketchannel上的io就绪事件，处理io就绪事件，执行异步任务。sub reactor group做的事情就是上篇饭店例子中服务员的工作，客人进来了要为客人分配座位，端茶送水，做菜上菜。“不管远近都是客人，请不用客气，相约好了在一起，我们欢迎您......”\n\n> 一个客户端socketchannel只能分配给一个固定的sub reactor。一个sub reactor负责处理多个客户端socketchannel，这样可以将服务端承载的全量客户端连接分摊到多个sub reactor中处理，同时也能保证客户端socketchannel上的io处理的线程安全性。\n\n由于文章篇幅的关系，作为reactor在netty中实现的第一篇我们主要来介绍主从reactor group的创建流程，骨架脉络先搭好。\n\n下面我们来看一段netty服务端代码的编写模板，从代码模板的流程中我们来解析下主从reactor的创建流程以及在这个过程中所涉及到的netty核心类。\n\n\n# netty 服务端代码模板\n\n/**\n * echoes back any received data from a client.\n */\npublic final class echoserver {\n    static final int port = integer.parseint(system.getproperty("port", "8007"));\n\n    public static void main(string[] args) throws exception {\n        // configure the server.\n        //创建主从reactor线程组\n        eventloopgroup bossgroup = new nioeventloopgroup(1);\n        eventloopgroup workergroup = new nioeventloopgroup();\n        final echoserverhandler serverhandler = new echoserverhandler();\n        try {\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)//配置主从reactor\n             .channel(nioserversocketchannel.class)//配置主reactor中的channel类型\n             .option(channeloption.so_backlog, 100)//设置主reactor中channel的option选项\n             .handler(new logginghandler(loglevel.info))//设置主reactor中channel->pipline->handler\n             .childhandler(new channelinitializer<socketchannel>() {//设置从reactor中注册channel的pipeline\n                 @override\n                 public void initchannel(socketchannel ch) throws exception {\n                     channelpipeline p = ch.pipeline();\n                     //p.addlast(new logginghandler(loglevel.info));\n                     p.addlast(serverhandler);\n                 }\n             });\n\n            // start the server. 绑定端口启动服务，开始监听accept事件\n            channelfuture f = b.bind(port).sync();\n            // wait until the server socket is closed.\n            f.channel().closefuture().sync();\n        } finally {\n            // shut down all event loops to terminate all threads.\n            bossgroup.shutdowngracefully();\n            workergroup.shutdowngracefully();\n        }\n    }\n}\n\n\n 1. 首先我们要创建netty最核心的部分 -> 创建主从reactor group，在netty中eventloopgroup就是reactor group的实现类。对应的eventloop就是reactor的实现类。\n    \n      //创建主从reactor线程组\n      eventloopgroup bossgroup = new nioeventloopgroup(1);\n      eventloopgroup workergroup = new nioeventloopgroup();\n    \n\n 2. 创建用于io处理的channelhandler，实现相应io事件的回调函数，编写对应的io处理逻辑。注意这里只是简单示例哈，详细的io事件处理，笔者会单独开一篇文章专门讲述。\n    \n    final echoserverhandler serverhandler = new echoserverhandler();\n    \n    /**\n     * handler implementation for the echo server.\n     */\n    @sharable\n    public class echoserverhandler extends channelinboundhandleradapter {\n    \n        @override\n        public void channelread(channelhandlercontext ctx, object msg) {\n            ................省略io处理逻辑................\n            ctx.write(msg);\n        }\n    \n        @override\n        public void channelreadcomplete(channelhandlercontext ctx) {\n            \n            ctx.flush();\n        }\n    \n        @override\n        public void exceptioncaught(channelhandlercontext ctx, throwable cause) {\n            // close the connection when an exception is raised.\n            cause.printstacktrace();\n            ctx.close();\n        }\n    }\n    \n\n 3. 创建serverbootstrapnetty服务端启动类，并在启动类中配置启动netty服务端所需要的一些必备信息。\n    \n    > 在上篇文章介绍socket内核结构小节中我们提到，在编写服务端网络程序时，我们首先要创建一个socket用于listen和bind端口地址，我们把这个叫做监听socket,这里对应的就是nioserversocketchannel.class。当客户端连接完成三次握手，系统调用accept函数会基于监听socket创建出来一个新的socket专门用于与客户端之间的网络通信我们称为客户端连接socket,这里对应的就是niosocketchannel.class\n    \n    > netty有两种channel类型：一种是服务端用于监听绑定端口地址的nioserversocketchannel,一种是用于客户端通信的niosocketchannel。每种channel类型实例都会对应一个pipeline用于编排对应channel实例上的io事件处理逻辑。pipeline中组织的就是channelhandler用于编写特定的io处理逻辑。\n    \n    > 注意serverbootstrap.handler设置的是服务端nioserversocketchannel pipeline中的channelhandler。\n    \n    > serverbootstrap启动类方法带有child前缀的均是设置客户端niosocketchannel属性的。\n    \n    > channelinitializer是用于当socketchannel成功注册到绑定的reactor上后，用于初始化该socketchannel的pipeline。它的initchannel方法会在注册成功后执行。这里只是捎带提一下，让大家有个初步印象，后面我会专门介绍。\n    \n    * serverbootstrap.childhandler(channelhandler childhandler)用于设置客户端niosocketchannel中对应pipieline中的channelhandler。我们通常配置的编码解码器就是在这里。\n    * serverbootstrap.option(channeloption.so_backlog, 100)设置服务端serversocketchannel中的socketoption。关于socketoption的选项我们后边的文章再聊，本文主要聚焦在netty main reactor group的创建及工作流程。\n    * serverbootstrap.handler(....)设置服务端nioserversocketchannel中对应pipieline中的channelhandler。\n    * 通过serverbootstrap.group(bossgroup, workergroup)为netty服务端配置主从reactor group实例。\n    * 通过serverbootstrap.channel(nioserversocketchannel.class)配置netty服务端的serversocketchannel用于绑定端口地址以及创建客户端socketchannel。netty中的nioserversocketchannel.class就是对jdk nio中serversocketchannel的封装。而用于表示客户端连接的niosocketchannel是对jdk nio socketchannel封装。\n\n 4. channelfuture f = serverbootstrap.bind(port).sync()这一步会是下篇文章要重点分析的主题main reactor group的启动，绑定端口地址，开始监听客户端连接事件（op_accept）。本文我们只关注创建流程\n\n 5. f.channel().closefuture().sync()等待服务端nioserversocketchannel关闭。netty服务端到这里正式启动，并准备好接受客户端连接的准备。\n\n 6. shutdowngracefully优雅关闭主从reactor线程组里的所有reactor线程。\n\n\n# netty 对 io 模型的支持\n\n在上篇文章中我们介绍了五种io模型，netty中支持bio,nio,aio以及多种操作系统下的io多路复用技术实现。\n\n在netty中切换这几种io模型也是非常的方便，下面我们来看下netty如何对这几种io模型进行支持的。\n\n首先我们介绍下几个与io模型相关的重要接口：\n\n\n# eventloop\n\neventloop就是netty中的reactor，可以说它就是netty的引擎，负责channel上io就绪事件的监听，io就绪事件的处理，异步任务的执行驱动着整个netty的运转。\n\n不同io模型下，eventloop有着不同的实现，我们只需要切换不同的实现类就可以完成对nettyio模型的切换。\n\nbio                         nio            aio\nthreadperchanneleventloop   nioeventloop   aioeventloop\n\n在nio模型下netty会自动根据操作系统以及版本的不同选择对应的io多路复用技术实现。比如linux 2.6版本以上用的是epoll，2.6版本以下用的是poll，mac下采用的是kqueue。\n\n\n# eventloopgroup\n\nnetty中的reactor是以group的形式出现的，eventloopgroup正是reactor组的接口定义，负责管理reactor，netty中的channel就是通过eventloopgroup注册到具体的reactor上的。\n\nnetty的io线程模型是主从reactor多线程模型，主从reactor线程组在netty源码中对应的其实就是两个eventloopgroup实例。\n\n不同的io模型也有对应的实现：\n\nbio                              nio                 aio\nthreadperchanneleventloopgroup   nioeventloopgroup   aioeventloopgroup\n\n\n# serversocketchannel\n\n用于netty服务端使用的serversocketchannel，对应于上篇文章提到的监听socket，负责绑定监听端口地址，接收客户端连接并创建用于与客户端通信的socketchannel。\n\n不同的io模型下的实现：\n\nbio                      nio                      aio\noioserversocketchannel   nioserversocketchannel   aioserversocketchannel\n\n\n# socketchannel\n\n用于与客户端通信的socketchannel，对应于上篇文章提到的客户端连接socket，当客户端完成三次握手后，由系统调用accept函数根据监听socket创建。\n\n不同的io模型下的实现：\n\nbio                nio                aio\noiosocketchannel   niosocketchannel   aiosocketchannel\n\n我们看到在不同io模型的实现中，netty这些围绕io模型的核心类只是前缀的不同：\n\n * bio对应的前缀为oio，表示old io，现在已经废弃不推荐使用。\n * nio对应的前缀为nio，正是netty推荐也是我们常用的非阻塞io模型。\n * aio对应的前缀为aio，由于linux下的异步io机制实现的并不成熟，性能提升表现上也不明显，现已被删除。\n\n我们只需要将io模型的这些核心接口对应的实现类前缀改为对应io模型的前缀，就可以轻松在netty中完成对io模型的切换。\n\n\n\n\n# 多种nio的实现\n\ncommon                   linux                      mac\nnioeventloopgroup        epolleventloopgroup        kqueueeventloopgroup\nnioeventloop             epolleventloop             kqueueeventloop\nnioserversocketchannel   epollserversocketchannel   kqueueserversocketchannel\nniosocketchannel         epollsocketchannel         kqueuesocketchannel\n\n我们通常在使用nio模型的时候会使用common列下的这些io模型核心类，common类也会根据操作系统的不同自动选择jdk在对应平台下的io多路复用技术的实现。\n\n而netty自身也根据操作系统的不同提供了自己对io多路复用技术的实现，比jdk的实现性能更优。比如：\n\n * jdk的 nio 默认实现是水平触发，netty 是边缘触发(默认)和水平触发可切换。。\n * netty 实现的垃圾回收更少、性能更好。\n\n我们编写netty服务端程序的时候也可以根据操作系统的不同，采用netty自身的实现来进一步优化程序。做法也很简单，直接将上图中红框里的实现类替换成netty的自身实现类即可完成切换。\n\n----------------------------------------\n\n经过以上对netty服务端代码编写模板以及io模型相关核心类的简单介绍，我们对netty的创建流程有了一个简单粗略的总体认识，下面我们来深入剖析下创建流程过程中的每一个步骤以及这个过程中涉及到的核心类实现。\n\n以下源码解析部分我们均采用common列下nio相关的实现进行解析。\n\n\n# 创建主从reactor线程组\n\n在 netty 服务端程序编写模板的开始，我们首先会创建两个reactor线程组：\n\n\n\n * 一个是主reactor线程组bossgroup用于监听客户端连接，创建客户端连接niosocketchannel，并将创建好的客户端连接niosocketchannel注册到从reactor线程组中一个固定的reactor上。\n * 一个是从reactor线程组workergroup，workergroup中的reactor负责监听绑定在其上的客户端连接niosocketchannel上的io就绪事件，并处理io就绪事件，执行异步任务。\n\n//创建主从reactor线程组\neventloopgroup bossgroup = new nioeventloopgroup(1);\neventloopgroup workergroup = new nioeventloopgroup();\n\n\nnetty中reactor线程组的实现类为nioeventloopgroup，在创建bossgroup和workergroup的时候用到了nioeventloopgroup的两个构造函数：\n\n * 带nthreads参数的构造函数public nioeventloopgroup(int nthreads)。\n * 不带nthreads参数的默认构造函数public nioeventloopgroup()\n\npublic class nioeventloopgroup extends multithreadeventloopgroup {\n\n    /**\n     * create a new instance using the default number of threads, the default {@link threadfactory} and\n     * the {@link selectorprovider} which is returned by {@link selectorprovider#provider()}.\n     */\n    public nioeventloopgroup() {\n        this(0);\n    }\n\n    /**\n     * create a new instance using the specified number of threads, {@link threadfactory} and the\n     * {@link selectorprovider} which is returned by {@link selectorprovider#provider()}.\n     */\n    public nioeventloopgroup(int nthreads) {\n        this(nthreads, (executor) null);\n    }\n\n    ......................省略...........................\n}\n\n\n> nthreads参数表示当前要创建的reactor线程组内包含多少个reactor线程。不指定nthreads参数的话采用默认的reactor线程个数，用0表示。\n\n最终会调用到构造函数\n\npublic nioeventloopgroup(int nthreads, executor executor, final selectorprovider selectorprovider,\nfinal selectstrategyfactory selectstrategyfactory) {\n\tsuper(nthreads, executor, selectorprovider, selectstrategyfactory, rejectedexecutionhandlers.reject());\n}\n\n\n下面简单介绍下构造函数中这几个参数的作用，后面我们在讲解本文主线的过程中还会提及这几个参数，到时在详细介绍，这里只是让大家有个初步印象，不必做过多的纠缠。\n\n * executor executor：负责启动reactor线程进而reactor才可以开始工作。\n\n> reactor线程组nioeventloopgroup负责创建reactor线程，在创建的时候会将executor传入。\n\n * rejectedexecutionhandler： 当向reactor添加异步任务添加失败时，采用的拒绝策略。reactor的任务不只是监听io活跃事件和io任务的处理，还包括对异步任务的处理。这里大家只需有个这样的概念，后面笔者会专门详细介绍。\n * selectorprovider selectorprovider： reactor中的io模型为io多路复用模型，对应于jdk nio中的实现为java.nio.channels.selector（就是我们上篇文章中提到的select,poll,epoll），每个reator中都包含一个selector，用于轮询注册在该reactor上的所有channel上的io事件。selectorprovider就是用来创建selector的。\n * selectstrategyfactory selectstrategyfactory： reactor最重要的事情就是轮询注册其上的channel上的io就绪事件，这里的selectstrategyfactory用于指定轮询策略，默认为defaultselectstrategyfactory.instance。\n\n最终会将这些参数交给nioeventloopgroup的父类构造器，下面我们来看下nioeventloopgroup类的继承结构：\n\n\n\nnioeventloopgroup类的继承结构乍一看比较复杂，大家不要慌，笔者会随着主线的深入慢慢地介绍这些父类接口，我们现在重点关注mutithread前缀的类。\n\n我们知道nioeventloopgroup是netty中的reactor线程组的实现，既然是线程组那么肯定是负责管理和创建多个reactor线程的，所以mutithread前缀的类定义的行为自然是对reactor线程组内多个reactor线程的创建和管理工作。\n\n\n# multithreadeventloopgroup\n\npublic abstract class multithreadeventloopgroup extends multithreadeventexecutorgroup implements eventloopgroup {\n\n    private static final internallogger logger = internalloggerfactory.getinstance(multithreadeventloopgroup.class);\n    //默认reactor个数\n    private static final int default_event_loop_threads;\n\n    static {\n        default_event_loop_threads = math.max(1, systempropertyutil.getint(\n                "io.netty.eventloopthreads", nettyruntime.availableprocessors() * 2));\n\n        if (logger.isdebugenabled()) {\n            logger.debug("-dio.netty.eventloopthreads: {}", default_event_loop_threads);\n        }\n    }\n\n    /**\n     * @see multithreadeventexecutorgroup#multithreadeventexecutorgroup(int, executor, object...)\n     */\n    protected multithreadeventloopgroup(int nthreads, executor executor, object... args) {\n        super(nthreads == 0 ? default_event_loop_threads : nthreads, executor, args);\n    }\n\n    ...................省略.....................\n}\n\n\nmultithreadeventloopgroup类主要的功能就是用来确定reactor线程组内reactor的个数。\n\n默认的reactor的个数存放于字段default_event_loop_threads中。\n\n从static {}静态代码块中我们可以看出默认reactor的个数的获取逻辑：\n\n * 可以通过系统变量 -d io.netty.eventloopthreads"指定。\n * 如果不指定，那么默认的就是nettyruntime.availableprocessors() * 2\n\n当nthread参数设置为0采用默认设置时，reactor线程组内的reactor个数则设置为default_event_loop_threads。\n\n\n# multithreadeventexecutorgroup\n\nmultithreadeventexecutorgroup这里就是本小节的核心，主要用来定义创建和管理reactor的行为。\n\npublic abstract class multithreadeventexecutorgroup extends abstracteventexecutorgroup {\n\n    //reactor线程组中的reactor集合\n    private final eventexecutor[] children;\n    private final set<eventexecutor> readonlychildren;\n    //从reactor group中选择一个特定的reactor的选择策略 用于channel注册绑定到一个固定的reactor上\n    private final eventexecutorchooserfactory.eventexecutorchooser chooser;\n\n    /**\n     * create a new instance.\n     *\n     * @param nthreads          the number of threads that will be used by this instance.\n     * @param executor          the executor to use, or {@code null} if the default should be used.\n     * @param args              arguments which will passed to each {@link #newchild(executor, object...)} call\n     */\n    protected multithreadeventexecutorgroup(int nthreads, executor executor, object... args) {\n        this(nthreads, executor, defaulteventexecutorchooserfactory.instance, args);\n    }\n\n    ............................省略................................\n}\n\n\n首先介绍一个新的构造器参数eventexecutorchooserfactory chooserfactory。当客户端连接完成三次握手后，main reactor会创建客户端连接niosocketchannel，并将其绑定到sub reactor group中的一个固定reactor，那么具体要绑定到哪个具体的sub reactor上呢？这个绑定策略就是由chooserfactory来创建的。默认为defaulteventexecutorchooserfactory。\n\n下面就是本小节的主题reactor线程组的创建过程：\n\n    protected multithreadeventexecutorgroup(int nthreads, executor executor,\n                                            eventexecutorchooserfactory chooserfactory, object... args) {\n        if (nthreads <= 0) {\n            throw new illegalargumentexception(string.format("nthreads: %d (expected: > 0)", nthreads));\n        }\n\n        if (executor == null) {\n            //用于创建reactor线程\n            executor = new threadpertaskexecutor(newdefaultthreadfactory());\n        }\n\n        children = new eventexecutor[nthreads];\n        //循环创建reaactor group 中的 reactor\n        for (int i = 0; i < nthreads; i ++) {\n            boolean success = false;\n            try {\n                //创建reactor\n                children[i] = newchild(executor, args);\n                success = true;\n            } catch (exception e) {\n                throw new illegalstateexception("failed to create a child event loop", e);\n            } finally {\n                     ................省略................\n                }\n            }\n        }\n        //创建channel到reactor的绑定策略\n        chooser = chooserfactory.newchooser(children);\n\n         ................省略................\n\n        set<eventexecutor> childrenset = new linkedhashset<eventexecutor>(children.length);\n        collections.addall(childrenset, children);\n        readonlychildren = collections.unmodifiableset(childrenset);\n    }\n\n\n# 1. 创建用于启动reactor线程的executor\n\n在netty reactor group中的单个reactor的io线程模型为上篇文章提到的单reactor单线程模型，一个reactor线程负责轮询注册其上的所有channel中的io就绪事件，处理io事件，执行netty中的异步任务等工作。正是这个reactor线程驱动着整个netty的运转，可谓是netty的核心引擎。\n\n\n\n而这里的executor就是负责启动reactor线程的，从创建源码中我们可以看到executor的类型为threadpertaskexecutor。\n\n# threadpertaskexecutor\n\npublic final class threadpertaskexecutor implements executor {\n    private final threadfactory threadfactory;\n\n    public threadpertaskexecutor(threadfactory threadfactory) {\n        this.threadfactory = objectutil.checknotnull(threadfactory, "threadfactory");\n    }\n\n    @override\n    public void execute(runnable command) {\n        threadfactory.newthread(command).start();\n    }\n}\n\n\n我们看到threadpertaskexecutor做的事情很简单，从它的命名前缀threadpertask我们就可以猜出它的工作方式，就是来一个任务就创建一个线程执行。而创建的这个线程正是netty的核心引擎reactor线程。\n\n在reactor线程启动的时候，netty会将reactor线程要做的事情封装成runnable，丢给exexutor启动。\n\n而reactor线程的核心就是一个死循环不停的轮询io就绪事件，处理io事件，执行异步任务。一刻也不停歇，堪称996典范。\n\n这里向大家先卖个关子，"reactor线程是何时启动的呢？？"\n\n# 2. 创建reactor\n\nreactor线程组nioeventloopgroup包含多个reactor，存放于private final eventexecutor[] children数组中。\n\n所以下面的事情就是创建nthread个reactor，并存放于eventexecutor[] children字段中，\n\n我们来看下用于创建reactor的newchild(executor, args)方法：\n\n# newchild\n\nnewchild方法是multithreadeventexecutorgroup中的一个抽象方法，提供给具体子类实现。\n\nprotected abstract eventexecutor newchild(executor executor, object... args) throws exception;\n\n\n这里我们解析的是nioeventloopgroup，我们来看下newchild在该类中的实现：\n\npublic class nioeventloopgroup extends multithreadeventloopgroup {\n    @override\n    protected eventloop newchild(executor executor, object... args) throws exception {\n        eventlooptaskqueuefactory queuefactory = args.length == 4 ? (eventlooptaskqueuefactory) args[3] : null;\n        return new nioeventloop(this, executor, (selectorprovider) args[0],\n            ((selectstrategyfactory) args[1]).newselectstrategy(), (rejectedexecutionhandler) args[2], queuefactory);\n    }\n}\n\n\n前边提到的众多构造器参数，这里会通过可变参数object... args传入到reactor类nioeventloop的构造器中。\n\n这里介绍下新的参数eventlooptaskqueuefactory queuefactory，前边提到netty中的reactor主要工作是轮询注册其上的所有channel上的io就绪事件，处理io就绪事件。除了这些主要的工作外，netty为了极致的压榨reactor的性能，还会让它做一些异步任务的执行工作。既然要执行异步任务，那么reactor中就需要一个队列来保存任务。\n\n这里的eventlooptaskqueuefactory就是用来创建这样的一个队列来保存reactor中待执行的异步任务。\n\n可以把reactor理解成为一个单线程的线程池，类似于jdk中的singlethreadexecutor，仅用一个线程来执行轮询io就绪事件，处理io就绪事件，执行异步任务。同时待执行的异步任务保存在reactor里的taskqueue中。\n\n# nioeventloop\n\npublic final class nioeventloop extends singlethreadeventloop {\n    //用于创建jdk nio selector,serversocketchannel\n    private final selectorprovider provider;\n    //selector轮询策略 决定什么时候轮询，什么时候处理io事件，什么时候执行异步任务\n    private final selectstrategy selectstrategy;\n    /**\n     * the nio {@link selector}.\n     */\n    private selector selector;\n    private selector unwrappedselector;\n\n    nioeventloop(nioeventloopgroup parent, executor executor, selectorprovider selectorprovider,\n                 selectstrategy strategy, rejectedexecutionhandler rejectedexecutionhandler,\n                 eventlooptaskqueuefactory queuefactory) {\n        super(parent, executor, false, newtaskqueue(queuefactory), newtaskqueue(queuefactory),\n                rejectedexecutionhandler);\n        this.provider = objectutil.checknotnull(selectorprovider, "selectorprovider");\n        this.selectstrategy = objectutil.checknotnull(strategy, "selectstrategy");\n        final selectortuple selectortuple = openselector();\n        this.selector = selectortuple.selector;\n        this.unwrappedselector = selectortuple.unwrappedselector;\n    }\n}\n\n\n这里就正式开始了reactor的创建过程，我们知道reactor的核心是采用的io多路复用模型来对客户端连接上的io事件进行监听，所以最重要的事情是创建selector(jdk nio 中io多路复用技术的实现)。\n\n> 可以把selector理解为我们上篇文章介绍的select,poll,epoll，它是jdk nio对操作系统内核提供的这些io多路复用技术的封装。\n\n# openselector\n\nopenselector是nioeventloop类中用于创建io多路复用的selector，并对创建出来的jdk nio 原生的selector进行性能优化。\n\n首先会通过selectorprovider#openselector创建jdk nio原生的selector。\n\nprivate selectortuple openselector() {\n    final selector unwrappedselector;\n    try {\n        //通过jdk nio selectorprovider创建selector\n        unwrappedselector = provider.openselector();\n    } catch (ioexception e) {\n    \tthrow new channelexception("failed to open a new selector", e);\n    }\n\n    ..................省略.............\n}\n\n\nselectorprovider会根据操作系统的不同选择jdk在不同操作系统版本下的对应selector的实现。linux下会选择epoll，mac下会选择kqueue。\n\n下面我们就来看下selectorprovider是如何做到自动适配不同操作系统下io多路复用实现的\n\n# selectorprovider\n\npublic nioeventloopgroup(threadfactory threadfactory) {\n\tthis(0, threadfactory, selectorprovider.provider());\n}\n\n\nselectorprovider是在前面介绍的nioeventloopgroup类构造函数中通过调用selectorprovider.provider()被加载，并通过nioeventloopgroup#newchild方法中的可变长参数object... args传递到nioeventloop中的private final selectorprovider provider字段中。\n\nselectorprovider的加载过程：\n\npublic abstract class selectorprovider {\n\n    public static selectorprovider provider() {\n        synchronized (lock) {\n            if (provider != null)\n                return provider;\n            return accesscontroller.doprivileged(\n                new privilegedaction<selectorprovider>() {\n                    public selectorprovider run() {\n                            if (loadproviderfromproperty())\n                                return provider;\n                            if (loadproviderasservice())\n                                return provider;\n                            provider = sun.nio.ch.defaultselectorprovider.create();\n                            return provider;\n                        }\n                    });\n        }\n    }\n}\n\n\n从selectorprovider加载源码中我们可以看出，selectorprovider的加载方式有三种，优先级如下：\n\n 1. 通过系统变量-d java.nio.channels.spi.selectorprovider指定selectorprovider的自定义实现类全限定名。通过应用程序类加载器(application classloader)加载。\n\nprivate static boolean loadproviderfromproperty() {\n    string cn = system.getproperty("java.nio.channels.spi.selectorprovider");\n    if (cn == null)\n    \treturn false;\n    try {\n        class<?> c = class.forname(cn, true,\n        classloader.getsystemclassloader());\n        provider = (selectorprovider)c.newinstance();\n        return true;\n    } \n    .................省略.............\n}\n\n\n 2. 通过spi方式加载。在工程目录meta-inf/services下定义名为java.nio.channels.spi.selectorprovider的spi文件，文件中第一个定义的selectorprovider实现类全限定名就会被加载。\n\nprivate static boolean loadproviderasservice() {\n\n        serviceloader<selectorprovider> sl =\n            serviceloader.load(selectorprovider.class,\n                               classloader.getsystemclassloader());\n        iterator<selectorprovider> i = sl.iterator();\n        for (;;) {\n            try {\n                if (!i.hasnext())\n                    return false;\n                provider = i.next();\n                return true;\n            } catch (serviceconfigurationerror sce) {\n                if (sce.getcause() instanceof securityexception) {\n                    // ignore the security exception, try the next provider\n                    continue;\n                }\n                throw sce;\n            }\n        }\n    }\n\n\n 3. 如果以上两种方式均未被定义，那么就采用selectorprovider系统默认实现sun.nio.ch.defaultselectorprovider。笔者当前使用的操作系统是macos，从源码中我们可以看到自动适配了kqueue实现。\n\npublic class defaultselectorprovider {\n    private defaultselectorprovider() {\n    }\n\n    public static selectorprovider create() {\n        return new kqueueselectorprovider();\n    }\n}\n\n\n下面我们接着回到io.netty.channel.nio.nioeventloop#openselector的主线上来。\n\n# netty对jdk nio 原生selector的优化\n\n建议查看 https://mp.weixin.qq.com/s/iuisutpiye13l8zyhwvzxa\n\n# newtaskqueue\n\nnioeventloop(nioeventloopgroup parent, executor executor, selectorprovider selectorprovider,selectstrategy strategy, rejectedexecutionhandler rejectedexecutionhandler,eventlooptaskqueuefactory queuefactory) {\t\t  \tsuper(parent, executor, false, newtaskqueue(queuefactory), newtaskqueue(queuefactory),rejectedexecutionhandler);\n    this.provider = objectutil.checknotnull(selectorprovider, "selectorprovider");\n    this.selectstrategy = objectutil.checknotnull(strategy, "selectstrategy");\n    final selectortuple selectortuple = openselector();\n    //通过用selectedselectionkeyset装饰后的unwrappedselector\n    this.selector = selectortuple.selector;\n    //netty优化过的jdk nio远程selector\n    this.unwrappedselector = selectortuple.unwrappedselector;\n}\n\n\n我们继续回到创建reactor的主线上，到目前为止reactor的核心selector就创建好了，前边我们提到reactor除了需要监听io就绪事件以及处理io就绪事件外，还需要执行一些异步任务，当外部线程向reactor提交异步任务后，reactor就需要一个队列来保存这些异步任务，等待reactor线程执行。\n\n下面我们来看下reactor中任务队列的创建过程：\n\n    //任务队列大小，默认是无界队列\n    protected static final int default_max_pending_tasks = math.max(16,\n            systempropertyutil.getint("io.netty.eventloop.maxpendingtasks", integer.max_value));\n\n    private static queue<runnable> newtaskqueue(\n            eventlooptaskqueuefactory queuefactory) {\n        if (queuefactory == null) {\n            return newtaskqueue0(default_max_pending_tasks);\n        }\n        return queuefactory.newtaskqueue(default_max_pending_tasks);\n    }\n\n    private static queue<runnable> newtaskqueue0(int maxpendingtasks) {\n        // this event loop never calls taketask()\n        return maxpendingtasks == integer.max_value ? platformdependent.<runnable>newmpscqueue()\n                : platformdependent.<runnable>newmpscqueue(maxpendingtasks);\n    }  \n\n\n * 在nioeventloop的父类singlethreadeventloop中提供了一个静态变量default_max_pending_tasks用来指定reactor任务队列的大小。可以通过系统变量-d io.netty.eventloop.maxpendingtasks进行设置，默认为integer.max_value，表示任务队列默认为无界队列。\n\n * 根据default_max_pending_tasks变量的设定，来决定创建无界任务队列还是有界任务队列。\n\n//创建无界任务队列\nplatformdependent.<runnable>newmpscqueue()\n//创建有界任务队列\nplatformdependent.<runnable>newmpscqueue(maxpendingtasks)\n\npublic static <t> queue<t> newmpscqueue() {\n\treturn mpsc.newmpscqueue();\n}\n\npublic static <t> queue<t> newmpscqueue(final int maxcapacity) {\n\treturn mpsc.newmpscqueue(maxcapacity);\n}\n\n\n> reactor内的异步任务队列的类型为mpscqueue,它是由jctools提供的一个高性能无锁队列，从命名前缀mpsc可以看出，它适用于多生产者单消费者的场景，它支持多个生产者线程安全的访问队列，同一时刻只允许一个消费者线程读取队列中的元素。\n\n> 我们知道netty中的reactor可以线程安全的处理注册其上的多个socketchannel上的io数据，保证reactor线程安全的核心原因正是因为这个mpscqueue，它可以支持多个业务线程在处理完业务逻辑后，线程安全的向mpscqueue添加异步写任务，然后由单个reactor线程来执行这些写任务。既然是单线程执行，那肯定是线程安全的了。\n\n# reactor对应的nioeventloop类型继承结构\n\n\n\nnioeventloop的继承结构也是比较复杂，这里我们只关注在reactor创建过程中涉及的到两个父类singlethreadeventloop,singlethreadeventexecutor。\n\n剩下的继承体系，我们在后边随着netty源码的深入在慢慢介绍。\n\n前边我们提到，其实reactor我们可以看作是一个单线程的线程池，只有一个线程用来执行io就绪事件的监听，io事件的处理，异步任务的执行。用mpscqueue来存储待执行的异步任务。\n\n命名前缀为singlethread的父类都是对reactor这些行为的分层定义。也是本小节要介绍的对象\n\n# singlethreadeventloop\n\nreactor负责执行的异步任务分为三类：\n\n * 普通任务：这是netty最主要执行的异步任务，存放在普通任务队列taskqueue中。在nioeventloop构造函数中创建。\n * 定时任务： 存放在优先级队列中。后续我们介绍。\n * 尾部任务： 存放于尾部任务队列tailtasks中，尾部任务一般不常用，在普通任务执行完后 reactor线程会执行尾部任务。**使用场景：**比如对netty 的运行状态做一些统计数据，例如任务循环的耗时、占用物理内存的大小等等都可以向尾部队列添加一个收尾任务完成统计数据的实时更新。\n\nsinglethreadeventloop负责对尾部任务队列tailtasks进行管理。并且提供channel向reactor注册的行为。\n\npublic abstract class singlethreadeventloop extends singlethreadeventexecutor implements eventloop {\n\n    //任务队列大小，默认是无界队列\n    protected static final int default_max_pending_tasks = math.max(16,\n            systempropertyutil.getint("io.netty.eventloop.maxpendingtasks", integer.max_value));\n    \n    //尾部任务队列\n    private final queue<runnable> tailtasks;\n\n    protected singlethreadeventloop(eventloopgroup parent, executor executor,\n                                    boolean addtaskwakesup, queue<runnable> taskqueue, queue<runnable> tailtaskqueue,\n                                    rejectedexecutionhandler rejectedexecutionhandler) {\n        super(parent, executor, addtaskwakesup, taskqueue, rejectedexecutionhandler);\n        //尾部队列 执行一些统计任务 不常用\n        tailtasks = objectutil.checknotnull(tailtaskqueue, "tailtaskqueue");\n    }\n\n    @override\n    public channelfuture register(channel channel) {\n        //注册channel到绑定的reactor上\n        return register(new defaultchannelpromise(channel, this));\n    }\n}\n\n\n# singlethreadeventexecutor\n\nsinglethreadeventexecutor主要负责对普通任务队列的管理，以及异步任务的执行，reactor线程的启停。\n\npublic abstract class singlethreadeventexecutor extends abstractscheduledeventexecutor implements orderedeventexecutor {\n\n    protected singlethreadeventexecutor(eventexecutorgroup parent, executor executor,\n                                        boolean addtaskwakesup, queue<runnable> taskqueue, rejectedexecutionhandler rejectedhandler) {\n        //parent为reactor所属的nioeventloopgroup reactor线程组\n        super(parent);\n        //向reactor添加任务时，是否唤醒selector停止轮询io就绪事件，马上执行异步任务\n        this.addtaskwakesup = addtaskwakesup;\n        //reactor异步任务队列的大小\n        this.maxpendingtasks = default_max_pending_executor_tasks;\n        //用于启动reactor线程的executor -> threadpertaskexecutor\n        this.executor = threadexecutormap.apply(executor, this);\n        //普通任务队列\n        this.taskqueue = objectutil.checknotnull(taskqueue, "taskqueue");\n        //任务队列满时的拒绝策略\n        this.rejectedexecutionhandler = objectutil.checknotnull(rejectedhandler, "rejectedhandler");\n    }\n}\n\n\n到现在为止，一个完整的reactor架构就被创建出来了。\n\n\n\n# 3. 创建channel到reactor的绑定策略\n\n到这一步，reactor线程组nioeventloopgroup里边的所有reactor就已经全部创建完毕。\n\n无论是netty服务端nioserversocketchannel关注的op_accept事件也好，还是netty客户端niosocketchannel关注的op_read和op_write事件也好，都需要先注册到reactor上，reactor才能监听channel上关注的io事件实现io多路复用。\n\nnioeventloopgroup（reactor线程组）里边有众多的reactor，那么以上提到的这些channel究竟应该注册到哪个reactor上呢？这就需要一个绑定的策略来平均分配。\n\n还记得我们前边介绍multithreadeventexecutorgroup类的时候提到的构造器参数eventexecutorchooserfactory吗？\n\n这时候它就派上用场了，它主要用来创建channel到reactor的绑定策略。默认为defaulteventexecutorchooserfactory.instance。\n\npublic abstract class multithreadeventexecutorgroup extends abstracteventexecutorgroup {\n   //从reactor集合中选择一个特定的reactor的绑定策略 用于channel注册绑定到一个固定的reactor上\n    private final eventexecutorchooserfactory.eventexecutorchooser chooser;\n\n    chooser = chooserfactory.newchooser(children);\n}\n\n\n下面我们来看下具体的绑定策略：\n\n# defaulteventexecutorchooserfactory\n\npublic final class defaulteventexecutorchooserfactory implements eventexecutorchooserfactory {\n\n    public static final defaulteventexecutorchooserfactory instance = new defaulteventexecutorchooserfactory();\n\n    private defaulteventexecutorchooserfactory() { }\n\n    @override\n    public eventexecutorchooser newchooser(eventexecutor[] executors) {\n        if (ispoweroftwo(executors.length)) {\n            return new poweroftwoeventexecutorchooser(executors);\n        } else {\n            return new genericeventexecutorchooser(executors);\n        }\n    }\n\n    private static boolean ispoweroftwo(int val) {\n        return (val & -val) == val;\n    }\n    ...................省略.................\n}\n\n\n我们看到在newchooser方法绑定策略有两个分支，不同之处在于需要判断reactor线程组中的reactor个数是否为2的次幂。\n\nnetty中的绑定策略就是采用round-robin轮询的方式来挨个选择reactor进行绑定。\n\n采用round-robin的方式进行负载均衡，我们一般会用round % reactor.length取余的方式来挨个平均的定位到对应的reactor上。\n\n如果reactor的个数reactor.length恰好是2的次幂，那么就可以用位操作&运算round & reactor.length -1来代替%运算round % reactor.length，因为位运算的性能更高。具体为什么&运算能够代替%运算，笔者会在后面讲述时间轮的时候为大家详细证明，这里大家只需记住这个公式，我们还是聚焦本文的主线。\n\n了解了优化原理，我们在看代码实现就很容易理解了。\n\n利用%运算的方式math.abs(idx.getandincrement() % executors.length)来进行绑定。\n\n    private static final class genericeventexecutorchooser implements eventexecutorchooser {\n        private final atomiclong idx = new atomiclong();\n        private final eventexecutor[] executors;\n\n        genericeventexecutorchooser(eventexecutor[] executors) {\n            this.executors = executors;\n        }\n\n        @override\n        public eventexecutor next() {\n            return executors[(int) math.abs(idx.getandincrement() % executors.length)];\n        }\n    }\n\n\n利用&运算的方式idx.getandincrement() & executors.length - 1来进行绑定。\n\n    private static final class poweroftwoeventexecutorchooser implements eventexecutorchooser {\n        private final atomicinteger idx = new atomicinteger();\n        private final eventexecutor[] executors;\n\n        poweroftwoeventexecutorchooser(eventexecutor[] executors) {\n            this.executors = executors;\n        }\n\n        @override\n        public eventexecutor next() {\n            return executors[idx.getandincrement() & executors.length - 1];\n        }\n    }\n\n\n> 又一次被netty对性能的极致追求所折服~~~~\n\n# 4. 向reactor线程组中所有的reactor注册terminated回调函数\n\n当reactor线程组nioeventloopgroup中所有的reactor已经创建完毕，channel到reactor的绑定策略也创建完毕后，我们就来到了创建nioeventgroup的最后一步。\n\n俗话说的好，有创建就有启动，有启动就有关闭，这里会创建reactor关闭的回调函数terminationlistener，在reactor关闭时回调。\n\nterminationlistener回调的逻辑很简单：\n\n * 通过atomicinteger terminatedchildren变量记录已经关闭的reactor个数，用来判断nioeventloopgroup中的reactor是否已经全部关闭。\n * 如果所有reactor均已关闭，设置nioeventloopgroup中的terminationfuture为success。表示reactor线程组关闭成功。\n\n       //记录关闭的reactor个数，当reactor全部关闭后，才可以认为关闭成功\n        private final atomicinteger terminatedchildren = new atomicinteger();\n        //关闭future\n        private final promise<?> terminationfuture = new defaultpromise(globaleventexecutor.instance);\n\n        final futurelistener<object> terminationlistener = new futurelistener<object>() {\n            @override\n            public void operationcomplete(future<object> future) throws exception {\n                if (terminatedchildren.incrementandget() == children.length) {\n                    //当所有reactor关闭后 才认为是关闭成功\n                    terminationfuture.setsuccess(null);\n                }\n            }\n        };\n        \n        //为所有reactor添加terminationlistener\n        for (eventexecutor e: children) {\n            e.terminationfuture().addlistener(terminationlistener);\n        }\n\n\n----------------------------------------\n\n我们在回到文章开头netty服务端代码模板\n\npublic final class echoserver {\n    static final int port = integer.parseint(system.getproperty("port", "8007"));\n\n    public static void main(string[] args) throws exception {\n        // configure the server.\n        //创建主从reactor线程组\n        eventloopgroup bossgroup = new nioeventloopgroup(1);\n        eventloopgroup workergroup = new nioeventloopgroup();\n\n        ...........省略............\n    }\n}\n\n\n现在netty的主从reactor线程组就已经创建完毕，此时netty服务端的骨架已经搭建完毕，骨架如下：\n\n\n\n\n# 总结\n\n本文介绍了首先介绍了netty对各种io模型的支持以及如何轻松切换各种io模型。\n\n还花了大量的篇幅介绍netty服务端的核心引擎主从reactor线程组的创建过程。在这个过程中，我们还提到了netty对各种细节进行的优化，展现了netty对性能极致的追求。\n\n好了，netty服务端的骨架已经搭好，剩下的事情就该绑定端口地址然后接收连接了，我们下篇文章再见~~~\n\n\n# 参考资料\n\nhttps://mp.weixin.qq.com/s/iuisutpiye13l8zyhwvzxa',charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"第三课：Netty 核心引擎 Reactor 的运转架构",frontmatter:{title:"第三课：Netty 核心引擎 Reactor 的运转架构",date:"2024-09-19T10:57:06.000Z",permalink:"/pages/bb668a/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/25.%E5%9B%9B%E3%80%81%E6%B7%B1%E5%85%A5%20Netty%20%E6%A0%B8%E5%BF%83/12.%E7%AC%AC%E4%B8%89%E8%AF%BE%EF%BC%9ANetty%20%E6%A0%B8%E5%BF%83%E5%BC%95%E6%93%8E%20Reactor%20%E7%9A%84%E8%BF%90%E8%BD%AC%E6%9E%B6%E6%9E%84.html",relativePath:"Netty 系统设计/25.四、深入 Netty 核心/12.第三课：Netty 核心引擎 Reactor 的运转架构.md",key:"v-983a4cec",path:"/pages/bb668a/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:3,title:"前文回顾",slug:"前文回顾",normalizedTitle:"前文回顾",charIndex:267},{level:2,title:"Reactor线程的整个运行框架",slug:"reactor线程的整个运行框架",normalizedTitle:"reactor线程的整个运行框架",charIndex:1826},{level:2,title:"1. Reactor 线程轮询 IO 就绪事件",slug:"_1-reactor-线程轮询-io-就绪事件",normalizedTitle:"1. reactor 线程轮询 io 就绪事件",charIndex:6844},{level:3,title:"1.1 轮询策略",slug:"_1-1-轮询策略",normalizedTitle:"1.1 轮询策略",charIndex:8578},{level:3,title:"1.2 轮询逻辑",slug:"_1-2-轮询逻辑",normalizedTitle:"1.2 轮询逻辑",charIndex:12258},{level:4,title:"1.2.1 Reactor的轮询超时时间",slug:"_1-2-1-reactor的轮询超时时间",normalizedTitle:"1.2.1 reactor的轮询超时时间",charIndex:13716},{level:4,title:"1.2.2 Reactor开始轮询IO就绪事件",slug:"_1-2-2-reactor开始轮询io就绪事件",normalizedTitle:"1.2.2 reactor开始轮询io就绪事件",charIndex:15367},{level:2,title:"2. Reactor 处理 IO 与处理异步任务的时间比例分配",slug:"_2-reactor-处理-io-与处理异步任务的时间比例分配",normalizedTitle:"2. reactor 处理 io 与处理异步任务的时间比例分配",charIndex:22359},{level:2,title:"3. Reactor线程处理IO就绪事件",slug:"_3-reactor线程处理io就绪事件",normalizedTitle:"3. reactor线程处理io就绪事件",charIndex:24405},{level:3,title:"3.1 processSelectedKeysPlain",slug:"_3-1-processselectedkeysplain",normalizedTitle:"3.1 processselectedkeysplain",charIndex:25963},{level:4,title:"3.1.1 获取 IO 就绪的 Channel",slug:"_3-1-1-获取-io-就绪的-channel",normalizedTitle:"3.1.1 获取 io 就绪的 channel",charIndex:27653},{level:4,title:"3.1.2 处理Channel上的IO事件",slug:"_3-1-2-处理channel上的io事件",normalizedTitle:"3.1.2 处理channel上的io事件",charIndex:29294},{level:5,title:"3.1.2.1 处理 Connect 事件",slug:"_3-1-2-1-处理-connect-事件",normalizedTitle:"3.1.2.1 处理 connect 事件",charIndex:33311},{level:5,title:"3.1.2.2 处理Write事件",slug:"_3-1-2-2-处理write事件",normalizedTitle:"3.1.2.2 处理write事件",charIndex:33888},{level:5,title:"3.1.2.3 处理Read事件或者Accept事件",slug:"_3-1-2-3-处理read事件或者accept事件",normalizedTitle:"3.1.2.3 处理read事件或者accept事件",charIndex:34243},{level:4,title:"3.1.3 从 Selector 中移除失效的 SelectionKey",slug:"_3-1-3-从-selector-中移除失效的-selectionkey",normalizedTitle:"3.1.3 从 selector 中移除失效的 selectionkey",charIndex:34590},{level:3,title:"3.2 processSelectedKeysOptimized",slug:"_3-2-processselectedkeysoptimized",normalizedTitle:"3.2 processselectedkeysoptimized",charIndex:38606},{level:2,title:"4. Reactor线程处理异步任务",slug:"_4-reactor线程处理异步任务",normalizedTitle:"4. reactor线程处理异步任务",charIndex:40519},{level:3,title:"4.1 runAllTasks()",slug:"_4-1-runalltasks",normalizedTitle:"4.1 runalltasks()",charIndex:40894},{level:4,title:"4.1.1 fetchFromScheduledTaskQueue",slug:"_4-1-1-fetchfromscheduledtaskqueue",normalizedTitle:"4.1.1 fetchfromscheduledtaskqueue",charIndex:41716},{level:4,title:"4.1.2 runAllTasksFrom",slug:"_4-1-2-runalltasksfrom",normalizedTitle:"4.1.2 runalltasksfrom",charIndex:43767},{level:4,title:"4.1.3 afterRunningAllTasks",slug:"_4-1-3-afterrunningalltasks",normalizedTitle:"4.1.3 afterrunningalltasks",charIndex:44627},{level:3,title:"4.2 runAllTasks(long timeoutNanos)",slug:"_4-2-runalltasks-long-timeoutnanos",normalizedTitle:"4.2 runalltasks(long timeoutnanos)",charIndex:45019},{level:2,title:"5. 解决 JDK Epoll 空轮询 BUG",slug:"_5-解决-jdk-epoll-空轮询-bug",normalizedTitle:"5. 解决 jdk epoll 空轮询 bug",charIndex:46658},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:49229},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:49516}],headersStr:"前言 前文回顾 Reactor线程的整个运行框架 1. Reactor 线程轮询 IO 就绪事件 1.1 轮询策略 1.2 轮询逻辑 1.2.1 Reactor的轮询超时时间 1.2.2 Reactor开始轮询IO就绪事件 2. Reactor 处理 IO 与处理异步任务的时间比例分配 3. Reactor线程处理IO就绪事件 3.1 processSelectedKeysPlain 3.1.1 获取 IO 就绪的 Channel 3.1.2 处理Channel上的IO事件 3.1.2.1 处理 Connect 事件 3.1.2.2 处理Write事件 3.1.2.3 处理Read事件或者Accept事件 3.1.3 从 Selector 中移除失效的 SelectionKey 3.2 processSelectedKeysOptimized 4. Reactor线程处理异步任务 4.1 runAllTasks() 4.1.1 fetchFromScheduledTaskQueue 4.1.2 runAllTasksFrom 4.1.3 afterRunningAllTasks 4.2 runAllTasks(long timeoutNanos) 5. 解决 JDK Epoll 空轮询 BUG 总结 参考资料",content:'# 前言\n\n\n\n本文笔者来为大家介绍下Netty的核心引擎Reactor的运转架构，希望通过本文的介绍能够让大家对Reactor是如何驱动着整个Netty框架的运转有一个全面的认识。也为我们后续进一步介绍Netty关于处理网络请求的整个生命周期的相关内容做一个前置知识的铺垫，方便大家后续理解。\n\n那么在开始本文正式的内容之前，笔者先来带着大家回顾下前边文章介绍的关于Netty整个框架如何搭建的相关内容，没有看过笔者前边几篇文章的读者朋友也没关系，这些并不会影响到本文的阅读，只不过涉及到相关细节的部分，大家可以在回看下。\n\n\n# 前文回顾\n\n在一文中，我们介绍了Netty服务端的核心引擎主从Reactor线程组的创建过程以及相关核心组件里的重要属性。在这个过程中，我们还提到了Netty对各种细节进行的优化，比如针对JDK NIO 原生Selector做的一些优化，展现了Netty对性能极致的追求。最终我们创建出了如下结构的Reactor。\n\n\n\n在上篇文章《详细图解Netty Reactor启动全流程》中，我们完整地介绍了Netty服务端启动的整个流程，并介绍了在启动过程中涉及到的ServerBootstrap相关的属性以及配置方式。用于接收连接的服务端NioServerSocketChannel的创建和初始化过程以及其类的继承结构。其中重点介绍了NioServerSocketChannel向Reactor的注册过程以及Reactor线程的启动时机和pipeline的初始化时机。最后介绍了NioServerSocketChannel绑定端口地址的整个流程。在这个过程中我们了解了Netty的这些核心组件是如何串联起来的。\n\n当Netty启动完毕后，我们得到了如下的框架结构：\n\n\n\n主 Reactor 线程组中管理的是 NioServerSocketChannel 用于接收客户端连接，并在自己的 pipeline 中的 ServerBootstrapAcceptor 里初始化接收到的客户端连接，随后会将初始化好的客户端连接注册到从 Reactor 线程组中\n\n从Reactor线程组主要负责监听处理注册其上的所有客户端连接的 IO 就绪事件\n\n其中一个 Channel 只能分配给一个固定的 Reactor。一个 Reactor 负责处理多个Channel上的IO就绪事件，这样可以将服务端承载的全量客户端连接分摊到多个Reactor中处理，同时也能保证Channel上IO处理的线程安全性。Reactor与Channel之间的对应关系如下图所示：\n\n\n\n以上内容就是对笔者前边几篇文章的相关内容回顾，大家能回忆起来更好，回忆不起来也没关系，一点也不影响大家理解本文的内容。如果对相关细节感兴趣的同学，可以在阅读完本文之后，在去回看下。\n\n我们言归正传，正式开始本文的内容，笔者接下来会为大家介绍这些核心组件是如何相互配合从而驱动着整个Netty Reactor框架运转的。\n\n----------------------------------------\n\n当Netty Reactor框架启动完毕后，接下来第一件事情也是最重要的事情就是如何来高效的接收客户端的连接。\n\n那么在探讨 Netty 服务端如何接收连接之前，我们需要弄清楚 Reactor线程 的运行机制，它是如何监听并处理 Channel 上的IO就绪事件的。\n\n本文相当于是后续我们介绍 Reactor线程 监听处理ACCEPT事件，Read事件，Write事件 的前置篇，本文专注于讲述 Reactor线程 的整个运行框架。理解了本文的内容，对理解后面 Reactor线程 如何处理IO事件会大有帮助。\n\n我们在Netty框架的 创建阶段 和 启动阶段 无数次的提到了 Reactor线程 ，那么在本文要介绍的 运行阶段 就该这个 Reactor线程 来大显神威了。\n\n经过前边文章的介绍，我们了解到 Netty 中的 Reactor线程 主要干三件事情：\n\n * 轮询：轮询注册在 Reactor 上的所有 Channel 感兴趣的 IO就绪事件\n * 处理 ：处理Channel 上的 IO就绪事件\n * 执行：执行 Netty 中的异步任务\n\n正是这三个部分组成了 Reactor 的运行框架，那么我们现在来看下这个运行框架具体是怎么运转的\n\n\n# Reactor线程的整个运行框架\n\n大家还记不记得笔者在《透过 Netty 看 IO 模型》一文中提到的，IO模型的演变是围绕着"如何用尽可能少的线程去管理尽可能多的连接"这一主题进行的\n\n笔记\n\nNetty 的 IO模型 是通过 JDK NIO Selector 实现的 IO多路复用模型，而 Netty 的 IO线程模型 为 `主从Reactor线程模型\n\n根据《透过 Netty 看 IO 模型》一文中介绍的 IO多路复用模型 我们很容易就能理解到 Netty 会使用一个用户态的 Reactor线程 去不断的通过 Selector 在内核态去轮训 Channel 上的 IO就绪事件\n\n说白了 Reactor线程 其实执行的就是一个 死循环，在 死循环 中不断的通过 Selector 去轮训 IO就绪事件，如果发生 IO就绪事件 则从Selector系统调用中返回并处理IO就绪事件，如果没有发生IO就绪事件则一直阻塞在Selector系统调用上，直到满足Selector唤醒条件\n\n以下三个条件中只要满足任意一个条件，Reactor 线程就会被从 Selector 上唤醒：\n\n * 当 Selector 轮询到有 IO 活跃事件发生时\n * 当 Reactor 线程需要执行的 定时任务 到达任务执行时间 deadline 时\n * 当有 异步任务 提交给 Reactor 时，Reactor 线程需要从 Selector 上被唤醒，这样才能及时的去执行 异步任务\n\n> 这里可以看出 Netty 对Reactor线程的压榨还是比较狠的，反正现在也没有IO就绪事件需要去处理，不能让Reactor线程在这里白白等着，要立即唤醒它，转去处理提交过来的异步任务以及定时任务。Reactor线程堪称996典范一刻不停歇地运作着。\n\n\n\n在了解了 Reactor线程 的大概运行框架后，我们接下来就到源码中去看下它的核心运转框架是如何实现出来的\n\n由于这块源码比较庞大繁杂，所以笔者先把它的运行框架提取出来，方便大家整体的理解整个运行过程的全貌\n\n\n\n上图所展示的就是 Reactor 整个工作体系的全貌，主要分为如下几个重要的工作模块：\n\n 1. Reactor 线程在 Selector 上阻塞获取 IO 就绪事件。在这个模块中首先会去检查当前是否有异步任务需要执行，如果有异步需要执行，那么不管当前有没有 IO 就绪事件都不能阻塞在 Selector 上，随后会去非阻塞的轮询一下 Selector 上是否有 IO 就绪事件，如果有，正好可以和异步任务一起执行。优先处理 IO 就绪事件，在执行异步任务。\n 2. 如果当前没有异步任务需要执行，那么 Reactor 线程会接着查看是否有定时任务需要执行，如果有则在 Selector 上阻塞直到定时任务的到期时间 deadline，或者满足其他唤醒条件被唤醒。如果没有定时任务需要执行，Reactor 线程则会在 Selector 上一直阻塞直到满足唤醒条件。\n 3. 当 Reactor 线程满足唤醒条件被唤醒后，首先会去判断当前是因为有 IO 就绪事件被唤醒还是因为有异步任务需要执行被唤醒或者是两者都有。随后 Reactor 线程就会去处理 IO 就绪事件和执行异步任务\n 4. 最后 Reactor 线程返回循环起点不断的重复上述三个步骤。\n\n以上就是 Reactor 线程运行的整个核心逻辑，下面是笔者根据上述核心逻辑，将 Reactor 的整体代码设计框架提取出来，大家可以结合上边的 Reactor 工作流程图，从总体上先感受下整个源码实现框架，能够把 Reactor 的核心处理步骤和代码中相应的处理模块对应起来即可，这里不需要读懂每一行代码，要以逻辑处理模块为单位理解。后面笔者会将这些一个一个的逻辑处理模块在单独拎出来为大家详细介绍。\n\n@Override\nprotected void run() {\n    //记录轮询次数 用于解决 JDK epoll 的空轮训 bug\n    int selectCnt = 0;\n    for (;;) {\n        try {\n            //轮询结果\n            int strategy;\n            try {\n                //根据轮询策略获取轮询结果 这里的 hasTasks() 主要检查的是普通队列和尾部队列中是否有异步任务等待执行\n                strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks());\n                switch (strategy) {\n                    case SelectStrategy.CONTINUE:\n                        continue;\n                    case SelectStrategy.BUSY_WAIT:\n                        // NIO不支持自旋（BUSY_WAIT）\n                    case SelectStrategy.SELECT:\n                        //核心逻辑是有任务需要执行，则 Reactor 线程立马执行异步任务，如果没有异步任务执行，则进行轮询IO事件\n                    default:\n                }\n            } catch (IOException e) {\n                ................省略...............\n            }\n\n            //执行到这里说明满足了唤醒条件，Reactor线程从selector上被唤醒开始处理IO就绪事件和执行异步任务\n                /**\n                 * Reactor线程需要保证及时的执行异步任务，只要有异步任务提交，就需要退出轮询。\n                 * 有IO事件就优先处理IO事件，然后处理异步任务\n                 * */\n\n            selectCnt++;\n            //主要用于从 IO 就绪的 SelectedKeys 集合中剔除已经失效的 selectKey\n            needsToSelectAgain = false;\n            //调整 Reactor 线程执行 IO事件和执行异步任务的CPU时间比例 默认50，表示执行IO事件和异步任务的时间比例是一比一\n            final int ioRatio = this.ioRatio;\n           \t// 这里主要处理IO就绪事件，以及执行异步任务,需要优先处理IO就绪事件，然后根据ioRatio设置的处理IO事件CPU用时与异步任务CPU用时比例，,来决定执行多长时间的异步任务\n            //判断是否触发JDK Epoll BUG 触发空轮询\n            if (ranTasks || strategy > 0) {\n                if (selectCnt > MIN_PREMATURE_SELECTOR_RETURNS && logger.isDebugEnabled()) {\n                    logger.debug("Selector.select() returned prematurely {} times in a row for Selector {}.",selectCnt - 1, selector);\n                }\n                selectCnt = 0;\n            } else if (unexpectedSelectorWakeup(selectCnt)) { // Unexpected wakeup (unusual case)\n                //既没有IO就绪事件，也没有异步任务，Reactor线程从Selector上被异常唤醒 触发JDK Epoll空轮训BUG\n                //重新构建Selector,selectCnt归零\n                selectCnt = 0;\n            }\n        } catch (CancelledKeyException e) {\n            ................省略...............\n        } catch (Error e) {\n            ................省略...............\n        } catch (Throwable t) {\n            ................省略...............\n        } finally {\n            ................省略...............\n        }\n    }\n}\n\n\n从上面提取出来的Reactor的源码实现框架中，我们可以看出Reactor线程主要做了下面几个事情：\n\n 1. 通过JDK NIO Selector轮询注册在Reactor上的所有Channel感兴趣的IO事件。对于NioServerSocketChannel来说因为它主要负责接收客户端连接所以监听的是OP_ACCEPT事件，对于客户端NioSocketChannel来说因为它主要负责处理连接上的读写事件所以监听的是OP_READ和OP_WRITE事件。\n\n注意\n\n这里需要注意的是 netty 只会自动注册OP_READ事件，而 OP_WRITE 事件是在当Socket写入缓冲区以满无法继续写入发送数据时由用户自己注册\n\n 2. 如果有异步任务需要执行，则立马停止轮询操作，转去执行异步任务。这里分为两种情况：\n\n> 这里第二种情况下只会执行64个异步任务，目的是为了防止过度执行异步任务，耽误了最重要的事情轮询IO事件。\n\n * 既有IO就绪事件发生，也有异步任务需要执行。则优先处理IO就绪事件，然后根据ioRatio设置的执行时间比例决定执行多长时间的异步任务。这里Reactor线程需要控制异步任务的执行时间，因为Reactor线程的核心是处理IO就绪事件，不能因为异步任务的执行而耽误了最重要的事情。\n\n * 没有IO就绪事件发生，但是有异步任务或者定时任务到期需要执行。则只执行异步任务，尽可能的去压榨Reactor线程。没有IO就绪事件发生也不能闲着。\n\n 3. 在最后Netty会判断本次Reactor线程的唤醒是否是由于触发了 JDK epoll 空轮询 BUG导致的，如果触发了该BUG，则重建Selector。绕过 JDK BUG，达到解决问题的目的。\n\n> 正常情况下Reactor线程从Selector中被唤醒有两种情况：\n> \n>  * 轮询到有IO就绪事件发生\n>  * 有异步任务或者定时任务需要执行。而 JDK epoll 空轮询 BUG会在上述两种情况都没有发生的时候，Reactor线程 会意外的从Selector中被唤醒，导致CPU空转\n\n> JDK epoll 空轮询 BUG：https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6670302\n\n好了，Reactor线程的总体运行结构框架我们现在已经了解了，下面我们来深入到这些核心处理模块中来各个击破它们\n\n\n# 1. Reactor 线程轮询 IO 就绪事件\n\n在《Reactor在Netty中的实现(创建篇)》一文中，笔者在讲述主从Reactor线程组NioEventLoopGroup的创建过程的时候，提到一个构造器参数SelectStrategyFactory。\n\npublic NioEventLoopGroup(\n    int nThreads, Executor executor, final SelectorProvider selectorProvider) {\n    this(nThreads, executor, selectorProvider, DefaultSelectStrategyFactory.INSTANCE);\n}\n\npublic NioEventLoopGroup(int nThreads, Executor executor, final SelectorProvider selectorProvider,\n                         final SelectStrategyFactory selectStrategyFactory) {\n    super(nThreads, executor, selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject());\n}\n\n\nReactor线程最重要的一件事情就是轮询IO就绪事件，SelectStrategyFactory就是用于指定轮询策略的，默认实现为DefaultSelectStrategyFactory.INSTANCE。\n\n而在Reactor线程开启轮询的一开始，就是用这个selectStrategy去计算一个轮询策略strategy，后续会根据这个strategy进行不同的逻辑处理。\n\n@Override\nprotected void run() {\n    //记录轮询次数 用于解决JDK epoll的空轮训bug\n    int selectCnt = 0;\n    for (;;) {\n        try {\n            //轮询结果\n            int strategy;\n            try {\n                //根据轮询策略获取轮询结果 这里的hasTasks()主要检查的是普通队列和尾部队列中是否有异步任务等待执行\n                strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks());\n                switch (strategy) {\n                    case SelectStrategy.CONTINUE:\n                        continue;\n                    case SelectStrategy.BUSY_WAIT:\n                        // NIO不支持自旋（BUSY_WAIT）\n                    case SelectStrategy.SELECT:\n                        //核心逻辑是有任务需要执行，则Reactor线程立马执行异步任务，如果没有异步任务执行，则进行轮询IO事件\n                    default:\n                }\n            } catch (IOException e) {\n                ................省略...............\n            }\n\n            ................省略...............\n}\n\n\n下面我们来看这个轮询策略strategy具体的计算逻辑是什么样的？\n\n\n# 1.1 轮询策略\n\n\n\npublic interface SelectStrategy {\n    /**\n     * Indicates a blocking select should follow.\n     */\n    int SELECT = -1;\n    /**\n     * Indicates the IO loop should be retried, no blocking select to follow directly.\n     */\n    int CONTINUE = -2;\n    /**\n     * Indicates the IO loop to poll for new events without blocking.\n     */\n    int BUSY_WAIT = -3;\n\n    int calculateStrategy(IntSupplier selectSupplier, boolean hasTasks) throws Exception;\n}\n\n\n我们首先来看下 Netty 中定义的这三种轮询策略：\n\n * SelectStrategy.SELECT：此时没有任何异步任务需要执行，Reactor线程可以安心的阻塞在Selector上等待IO就绪事件的来临。\n * SelectStrategy.CONTINUE：重新开启一轮IO轮询。\n * SelectStrategy.BUSY_WAIT： Reactor线程进行自旋轮询，由于NIO 不支持自旋操作，所以这里直接跳到SelectStrategy.SELECT 策略\n\n----------------------------------------\n\n下面我们来看下 轮询策略 的计算逻辑 calculateStrategy：\n\nfinal class DefaultSelectStrategy implements SelectStrategy {\n    static final SelectStrategy INSTANCE = new DefaultSelectStrategy();\n\n    private DefaultSelectStrategy() { }\n\n    @Override\n    public int calculateStrategy(IntSupplier selectSupplier, boolean hasTasks) throws Exception {\n        /**\n         * Reactor线程要保证及时的执行异步任务\n         * 1：如果有异步任务等待执行，则马上执行selectNow()非阻塞轮询一次IO就绪事件\n         * 2：没有异步任务，则跳到switch select分支\n         * */\n        return hasTasks ? selectSupplier.get() : SelectStrategy.SELECT;\n    }\n}\n\n\n * 在 Reactor线程 的轮询工作开始之前，需要首先判断下当前是否有异步任务需要执行。判断依据就是查看Reactor中的异步任务队列taskQueue和用于统计信息任务用的尾部队列tailTask是否有异步任务。\n\n@Override\nprotected boolean hasTasks() {\n    return super.hasTasks() || !tailTasks.isEmpty();\n}\n\nprotected boolean hasTasks() {\n    assert inEventLoop();\n    return !taskQueue.isEmpty();\n}\n\n\n * 如果 Reactor 中有 异步任务 需要执行，那么Reactor线程需要立即执行，不能阻塞在Selector上。在返回前需要再顺带调用selectNow()非阻塞查看一下当前是否有IO就绪事件发生。如果有，那么正好可以和异步任务一起被处理，如果没有，则及时地处理异步任务。\n\n笔记\n\n这里Netty要表达的语义是：首先Reactor线程需要优先保证IO就绪事件的处理，然后在保证异步任务的及时执行。如果当前没有IO就绪事件但是有异步任务需要执行时，Reactor线程就要去及时执行异步任务而不是继续阻塞在Selector上等待IO就绪事件。\n\nprivate final IntSupplier selectNowSupplier = new IntSupplier() {\n    @Override\n    public int get() throws Exception {\n        return selectNow();\n    }\n};\n\nint selectNow() throws IOException {\n    //非阻塞\n    return selector.selectNow();\n}\n\n\n * 如果当前Reactor线程没有异步任务需要执行，那么calculateStrategy方法直接返回SelectStrategy.SELECT也就是SelectStrategy接口中定义的常量-1。当calculateStrategy方法通过selectNow()返回非零数值时，表示此时有IO就绪的Channel，返回的数值表示有多少个IO就绪的Channel。\n\n@Override\nprotected void run() {\n    //记录轮询次数 用于解决JDK epoll的空轮训bug\n    int selectCnt = 0;\n    for (;;) {\n        try {\n            //轮询结果\n            int strategy;\n            try {\n                //根据轮询策略获取轮询结果 这里的hasTasks()主要检查的是普通队列和尾部队列中是否有异步任务等待执行\n                strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks());\n                switch (strategy) {\n                    case SelectStrategy.CONTINUE:\n                        continue;\n                    case SelectStrategy.BUSY_WAIT:\n                        // NIO不支持自旋（BUSY_WAIT）\n                    case SelectStrategy.SELECT:\n                        //核心逻辑是有任务需要执行，则Reactor线程立马执行异步任务，如果没有异步任务执行，则进行轮询IO事件\n                    default:\n                }\n            } catch (IOException e) {\n                ................省略...............\n            }\n\n             ................处理IO就绪事件以及执行异步任务...............\n}\n\n\n从默认的轮询策略我们可以看出 selectStrategy.calculateStrategy 只会返回三种情况：\n\n\n\n * 返回 -1： switch逻辑分支进入SelectStrategy.SELECT分支，表示此时Reactor中没有异步任务需要执行，Reactor线程可以安心的阻塞在Selector上等待IO就绪事件发生。\n * 返回 0： switch逻辑分支进入default分支，表示此时Reactor中没有IO就绪事件但是有异步任务需要执行，流程通过default分支直接进入了处理异步任务的逻辑部分。\n * 返回 > 0：switch逻辑分支进入default分支，表示此时Reactor中既有IO就绪事件发生也有异步任务需要执行，流程通过default分支直接进入了处理IO就绪事件和执行异步任务逻辑部分。\n\n现在Reactor的流程处理逻辑走向我们清楚了，那么接下来我们把重点放在SelectStrategy.SELECT分支中的轮询逻辑上。这块是Reactor监听IO就绪事件的核心。\n\n\n# 1.2 轮询逻辑\n\n\n\ncase SelectStrategy.SELECT:\n//当前没有异步任务执行，Reactor线程可以放心的阻塞等待IO就绪事件\n\n//从定时任务队列中取出即将快要执行的定时任务 deadline\nlong curDeadlineNanos = nextScheduledTaskDeadlineNanos();\nif (curDeadlineNanos == -1L) {\n    // -1代表当前定时任务队列中没有定时任务\n    curDeadlineNanos = NONE; // nothing on the calendar\n}\n\n//最早执行定时任务的 deadline 作为 select 的阻塞时间，意思是到了定时任务的执行时间\n//不管有无 IO 就绪事件，必须唤醒 selector，从而使 reactor 线程执行定时任务\nnextWakeupNanos.set(curDeadlineNanos);\ntry {\n    if (!hasTasks()) {\n        //再次检查普通任务队列中是否有异步任务\n        //没有的话开始select阻塞轮询IO就绪事件\n        strategy = select(curDeadlineNanos);\n    }\n} finally {\n    // 执行到这里说明Reactor已经从Selector上被唤醒了\n    // 设置Reactor的状态为苏醒状态AWAKE\n    // lazySet优化不必要的volatile操作，不使用内存屏障，不保证写操作的可见性（单线程不需要保证）\n    nextWakeupNanos.lazySet(AWAKE);\n}\n\n\n流程走到这里，说明现在Reactor上没有任何事情可做，可以安心的阻塞在Selector上等待IO就绪事件到来。\n\n那么Reactor线程到底应该在Selector上阻塞多久呢？？\n\n在回答这个问题之前，我们在回顾下《Reactor在Netty中的实现(创建篇)》一文中在讲述Reactor的创建时提到，Reactor线程除了要轮询Channel上的IO就绪事件，以及处理IO就绪事件外，还有一个任务就是负责执行Netty框架中的异步任务。\n\n\n\n而Netty框架中的 异步任务 分为三类：\n\n * 存放在普通任务队列taskQueue中的普通异步任务。\n * 存放在尾部队列tailTasks中的用于执行统计任务等收尾动作的尾部任务。\n * 还有一种就是这里即将提到的定时任务。存放在Reactor中的定时任务队列scheduledTaskQueue中。\n\n从ReactorNioEventLoop类中的继承结构我们也可以看出，Reactor具备执行定时任务的能力。\n\n\n\n既然Reactor需要执行定时任务，那么它就不能一直阻塞在Selector上无限等待IO就绪事件。\n\n那么我们回到本小节一开始提到的问题上，为了保证Reactor能够及时地执行定时任务，Reactor线程需要在即将要执行的的第一个定时任务deadline到达之前被唤醒。\n\n笔记\n\nRedis 的文件事件与时间事件协同机制，以及Nginx中的处理机制，和上述“根据定时任务到达时间执行阻塞”有异曲同工之妙\n\n所以在Reactor线程开始轮询IO就绪事件之前，我们需要首先计算出来Reactor线程在Selector上的阻塞超时时间。\n\n# 1.2.1 Reactor的轮询超时时间\n\n首先我们需要从Reactor的定时任务队列scheduledTaskQueue中取出即将快要执行的定时任务deadline。将这个deadline作为Reactor线程在Selector上轮询的超时时间。这样可以保证在定时任务即将要执行时，Reactor 现在可以及时的从 Selector 上被唤醒。\n\n    private static final long AWAKE = -1L;\n    private static final long NONE = Long.MAX_VALUE;\n\n    // nextWakeupNanos is:\n    //    AWAKE            when EL is awake\n    //    NONE             when EL is waiting with no wakeup scheduled\n    //    other value T    when EL is waiting with wakeup scheduled at time T\n    private final AtomicLong nextWakeupNanos = new AtomicLong(AWAKE);\n\n      long curDeadlineNanos = nextScheduledTaskDeadlineNanos();\n      if (curDeadlineNanos == -1L) {\n            // -1代表当前定时任务队列中没有定时任务\n            curDeadlineNanos = NONE; // nothing on the calendar\n      }\n\n      nextWakeupNanos.set(curDeadlineNanos);\npublic abstract class AbstractScheduledEventExecutor extends AbstractEventExecutor {\n\n    PriorityQueue<ScheduledFutureTask<?>> scheduledTaskQueue;\n\n    protected final long nextScheduledTaskDeadlineNanos() {\n        ScheduledFutureTask<?> scheduledTask = peekScheduledTask();\n        return scheduledTask != null ? scheduledTask.deadlineNanos() : -1;\n    }\n\n    final ScheduledFutureTask<?> peekScheduledTask() {\n        Queue<ScheduledFutureTask<?>> scheduledTaskQueue = this.scheduledTaskQueue;\n        return scheduledTaskQueue != null ? scheduledTaskQueue.peek() : null;\n    }\n\n}\n\n\nnextScheduledTaskDeadlineNanos方法会返回当前Reactor定时任务队列中最近的一个定时任务deadline时间点，如果定时任务队列中没有定时任务，则返回-1。\n\nNioEventLoop中nextWakeupNanos变量用来存放Reactor从Selector上被唤醒的时间点，设置为最近需要被执行定时任务的deadline，如果当前并没有定时任务需要执行，那么就设置为Long.MAX_VALUE一直阻塞，直到有IO就绪事件到达或者有异步任务需要执行。\n\n# 1.2.2 Reactor开始轮询IO就绪事件\n\nif (!hasTasks()) {\n    //再次检查普通任务队列中是否有异步任务， 没有的话  开始select阻塞轮询IO就绪事件\n    strategy = select(curDeadlineNanos);\n}\n\n\n在 Reactor线程 开始阻塞轮询IO就绪事件之前还需要再次检查一下是否有异步任务需要执行。\n\n如果此时恰巧有 异步任务 提交，就需要停止IO就绪事件的轮询，转去执行异步任务。如果没有异步任务，则正式开始轮询IO就绪事件。\n\nprivate int select(long deadlineNanos) throws IOException {\n    if (deadlineNanos == NONE) {\n        //无定时任务，无普通任务执行时，开始轮询IO就绪事件，没有就一直阻塞 直到唤醒条件成立\n        return selector.select();\n    }\n\n    long timeoutMillis = deadlineToDelayNanos(deadlineNanos + 995000L) / 1000000L;\n\n    return timeoutMillis <= 0 ? selector.selectNow() : selector.select(timeoutMillis);\n}\n\n\n如果deadlineNanos == NONE，经过上小节的介绍，我们知道NONE表示当前Reactor中并没有定时任务，所以可以安心的阻塞在Selector上等待IO就绪事件到来。\n\nselector.select()调用是一个阻塞调用，如果没有IO就绪事件，Reactor线程就会一直阻塞在这里直到IO就绪事件到来。这里占时不考虑前边提到的JDK NIO Epoll的空轮询BUG.\n\n读到这里那么问题来了，此时Reactor线程正阻塞在selector.select()调用上等待IO就绪事件的到来，如果此时正好有异步任务被提交到Reactor中需要执行，并且此时无任何IO就绪事件，而Reactor线程由于没有IO就绪事件到来，会继续在这里阻塞，那么如何去执行异步任务呢？\n\n----------------------------------------\n\n解铃还须系铃人，既然异步任务在被提交后希望立马得到执行，那么就在提交异步任务的时候去唤醒Reactor线程。\n\n//addTaskWakesUp = true 表示 当且仅当只有调用 addTask 方法时 才会唤醒Reactor线程\n//addTaskWakesUp = false 表示 并不是只有 addTask 方法才能唤醒 Reactor 还有其他方法可以唤醒 Reactor 默认设置 false\nprivate final boolean addTaskWakesUp;\n\nprivate void execute(Runnable task, boolean immediate) {\n    boolean inEventLoop = inEventLoop();\n    addTask(task);\n    if (!inEventLoop) {\n        //如果当前线程不是Reactor线程，则启动Reactor线程\n        //这里可以看出Reactor线程的启动是通过 向NioEventLoop添加异步任务时启动的\n        startThread();\n        .....................省略...................\n    }\n\n    if (!addTaskWakesUp && immediate) {\n        //io.netty.channel.nio.NioEventLoop.wakeup\n        wakeup(inEventLoop);\n    }\n}\n\n\n对于execute方法我想大家一定不会陌生，在上篇文章《详细图解Netty Reactor启动全流程》中我们在介绍Reactor线程的启动时介绍过该方法。\n\n在启动过程中涉及到的重要操作 Register操作，Bind操作 都需要封装成 异步任务 通过该方法提交到 Reactor 中执行。\n\n这里我们将重点放在 execute方法 后半段 wakeup 逻辑部分。\n\n我们先介绍下和wakeup逻辑相关的两个参数 boolean immediate 和 boolean addTaskWakesUp\n\n * immediate：表示提交的task是否需要被立即执行。Netty 中只要你提交的任务类型不是LazyRunnable类型的任务，都是需要立即执行的。immediate = true\n * addTaskWakesUp： true 表示当且仅当只有调用addTask方法时才会唤醒Reactor线程。调用别的方法并不会唤醒Reactor线程。在初始化NioEventLoop时会设置为false，表示并不是只有addTask方法才能唤醒Reactor线程 还有其他方法可以唤醒Reactor线程，比如这里的execute方法就会唤醒Reactor线程。\n\n针对 execute 方法中的这个唤醒条件!addTaskWakesUp && immediate，netty 这里要表达的语义是：当immediate参数为true的时候表示该异步任务需要立即执行，addTaskWakesUp 默认设置为false 表示不仅只有addTask方法可以唤醒Reactor，还有其他方法比如这里的execute方法也可以唤醒。但是当设置为true时，语义就变为只有addTask才可以唤醒Reactor，即使execute方法里的immediate = true也不能唤醒Reactor，因为执行的是execute方法而不是addTask方法。\n\nprivate static final long AWAKE = -1L;\nprivate final AtomicLong nextWakeupNanos = new AtomicLong(AWAKE);\n\nprotected void wakeup(boolean inEventLoop) {\n    if (!inEventLoop && nextWakeupNanos.getAndSet(AWAKE) != AWAKE) {\n        //将Reactor线程从Selector上唤醒\n        selector.wakeup();\n    }\n}\n\n\n当nextWakeupNanos = AWAKE时表示当前Reactor正处于苏醒状态，既然是苏醒状态也就没有必要去执行selector.wakeup()重复唤醒Reactor了，同时也能省去这一次的系统调用开销。\n\n在《1.2小节 轮询逻辑》开始介绍的源码实现框架里 Reactor 被唤醒之后执行代码会进入 finally{...} 语句块中，在那里会将nextWakeupNanos设置为AWAKE。\n\ntry {\n    if (!hasTasks()) {\n        strategy = select(curDeadlineNanos);\n    }\n} finally {\n    // 执行到这里说明Reactor已经从Selector上被唤醒了\n    // 设置Reactor的状态为苏醒状态AWAKE\n    // lazySet优化不必要的volatile操作，不使用内存屏障，不保证写操作的可见性（单线程不需要保证）\n    nextWakeupNanos.lazySet(AWAKE);\n}\n\n\n笔记\n\n这里Netty用了一个AtomicLong类型的变量nextWakeupNanos，既能表示当前Reactor线程的状态，又能表示Reactor线程的阻塞超时时间。我们在日常开发中也可以学习下这种技巧。\n\n----------------------------------------\n\n我们继续回到 Reactor线程 轮询 IO就绪事件 的主线上\n\nprivate int select(long deadlineNanos) throws IOException {\n    if (deadlineNanos == NONE) {\n        //无定时任务，无普通任务执行时，开始轮询IO就绪事件，没有就一直阻塞 直到唤醒条件成立\n        return selector.select();\n    }\n\n    long timeoutMillis = deadlineToDelayNanos(deadlineNanos + 995000L) / 1000000L;\n\n    return timeoutMillis <= 0 ? selector.selectNow() : selector.select(timeoutMillis);\n}\n\n\n当deadlineNanos不为NONE，表示此时Reactor有定时任务需要执行，Reactor线程需要阻塞在Selector上等待IO就绪事件直到最近的一个定时任务执行时间点deadline到达。\n\n这里的deadlineNanos表示的就是Reactor中最近的一个定时任务执行时间点deadline，单位是纳秒。指的是一个绝对时间。\n\n而我们需要计算的是Reactor线程阻塞在Selector的超时时间timeoutMillis，单位是毫秒，指的是一个相对时间。\n\n\n\n所以在Reactor线程开始阻塞在Selector上之前，我们需要将这个单位为纳秒的绝对时间deadlineNanos转化为单位为毫秒的相对时间timeoutMillis。\n\nprivate int select(long deadlineNanos) throws IOException {\n    if (deadlineNanos == NONE) {\n        //无定时任务，无普通任务执行时，开始轮询IO就绪事件，没有就一直阻塞 直到唤醒条件成立\n        return selector.select();\n    }\n\n    long timeoutMillis = deadlineToDelayNanos(deadlineNanos + 995000L) / 1000000L;\n\n    return timeoutMillis <= 0 ? selector.selectNow() : selector.select(timeoutMillis);\n}\n\n\n这里大家可能会好奇，通过deadlineToDelayNanos方法计算timeoutMillis的时候，为什么要给deadlineNanos在加上0.995毫秒呢？？\n\n大家想象一下这样的场景，当最近的一个定时任务的deadline即将在5微秒内到达，那么这时将纳秒转换成毫秒计算出的timeoutMillis会是0。\n\n而在Netty中timeoutMillis = 0要表达的语义是：定时任务执行时间已经到达deadline时间点，需要被执行。\n\n而现实情况是定时任务还有5微秒才能够到达deadline，所以对于这种情况，需要在deadlineNanos在加上0.995毫秒凑成1毫秒不能让其为0。\n\n笔记\n\n所以从这里我们可以看出，Reactor在有定时任务的情况下，至少要阻塞1毫秒。\n\npublic abstract class AbstractScheduledEventExecutor extends AbstractEventExecutor {\n\n    protected static long deadlineToDelayNanos(long deadlineNanos) {\n        return ScheduledFutureTask.deadlineToDelayNanos(deadlineNanos);\n    }\n}\nfinal class ScheduledFutureTask<V> extends PromiseTask<V> implements ScheduledFuture<V>, PriorityQueueNode {\n\n    static long deadlineToDelayNanos(long deadlineNanos) {\n        return deadlineNanos == 0L ? 0L : Math.max(0L, deadlineNanos - nanoTime());\n    }\n\n    //启动时间点\n    private static final long START_TIME = System.nanoTime();\n\n    static long nanoTime() {\n        return System.nanoTime() - START_TIME;\n    }\n\n    static long deadlineNanos(long delay) {\n        //计算定时任务执行deadline  去除启动时间\n        long deadlineNanos = nanoTime() + delay;\n        // Guard against overflow\n        return deadlineNanos < 0 ? Long.MAX_VALUE : deadlineNanos;\n    }\n\n}\n\n\n这里需要注意一下，在创建定时任务时会通过deadlineNanos方法计算定时任务的执行deadline，deadline的计算逻辑是当前时间点+任务延时delay-系统启动时间。这里需要扣除系统启动的时间。\n\n所以这里在通过deadline计算延时delay（也就是timeout）的时候需要在加上系统启动的时间 : deadlineNanos - nanoTime()\n\n当通过deadlineToDelayNanos计算出的timeoutMillis <= 0时，表示Reactor目前有临近的定时任务需要执行，这时候就需要立马返回，不能阻塞在Selector上影响定时任务的执行。当然在返回执行定时任务前，需要在顺手通过selector.selectNow()非阻塞轮询一下Channel上是否有IO就绪事件到达，防止耽误IO事件的处理。真是操碎了心~~\n\n当timeoutMillis > 0时，Reactor线程就可以安心的阻塞在Selector上等待IO事件的到来，直到timeoutMillis超时时间到达。\n\ntimeoutMillis <= 0 ? selector.selectNow() : selector.select(timeoutMillis)\n\n\n当注册在Reactor上的Channel中有IO事件到来时，Reactor线程就会从selector.select(timeoutMillis)调用中唤醒，立即去处理IO就绪事件。\n\n这里假设一种极端情况，如果最近的一个定时任务的deadline是在未来很远的一个时间点，这样就会使timeoutMillis的时间非常非常久，那么Reactor岂不是会一直阻塞在Selector上造成 Netty 无法工作？\n\n笔者觉得大家现在心里应该已经有了答案，我们在《1.2.2 Reactor开始轮询IO就绪事件》小节一开始介绍过，当Reactor正在Selector上阻塞时，如果此时用户线程向 Reactor 提交了异步任务，Reactor线程会通过execute方法被唤醒。\n\n----------------------------------------\n\n流程到这里，Reactor中最重要也是最核心的逻辑：轮询Channel上的 IO就绪事件 的处理流程我们就讲解完了。\n\n当 Reactor 轮询到有IO活跃事件或者有异步任务需要执行时，就会从Selector上被唤醒，下面就到了该介Reactor 被唤醒之后是如何处理IO就绪事件以及如何执行异步任务的时候了\n\nNetty毕竟是一个网络框架，所以它会优先去处理Channel上的IO事件，基于这个事实，所以Netty不会容忍 异步任务 被无限制的执行从而影响IO吞吐\n\nNetty通过ioRatio变量来调配Reactor线程在处理IO事件和执行异步任务之间的CPU时间分配比例\n\n下面我们就来看下这个执行时间比例的分配逻辑是什么样的\n\n\n# 2. Reactor 处理 IO 与处理异步任务的时间比例分配\n\n无论什么时候，当有 IO就绪事件 到来时，Reactor都需要保证IO事件被及时完整的处理完，而ioRatio主要限制的是执行异步任务所需用时，防止 Reactor线程 处理 异步任务 时间过长而导致I/O 事件得不到及时地处理\n\n\n\n//调整Reactor线程执行IO事件和执行异步任务的CPU时间比例 默认50，表示执行IO事件和异步任务的时间比例是一比一\nfinal int ioRatio = this.ioRatio;\nboolean ranTasks;\nif (ioRatio == 100) { //先一股脑执行IO事件，在一股脑执行异步任务（无时间限制）\n    try {\n        if (strategy > 0) {\n            //如果有IO就绪事件 则处理IO就绪事件\n            processSelectedKeys();\n        }\n    } finally {\n        // Ensure we always run tasks.\n        //处理所有异步任务\n        ranTasks = runAllTasks();\n    }\n} else if (strategy > 0) {//先执行IO事件 用时ioTime  执行异步任务只能用时ioTime * (100 - ioRatio) / ioRatio\n    final long ioStartTime = System.nanoTime();\n    try {\n        processSelectedKeys();\n    } finally {\n        // Ensure we always run tasks.\n        final long ioTime = System.nanoTime() - ioStartTime;\n        // 限定在超时时间内 处理有限的异步任务 防止Reactor线程处理异步任务时间过长而导致 I/O 事件阻塞\n        ranTasks = runAllTasks(ioTime * (100 - ioRatio) / ioRatio);\n    }\n} else { //没有IO就绪事件处理，则只执行异步任务 最多执行64个 防止Reactor线程处理异步任务时间过长而导致 I/O 事件阻塞\n    ranTasks = runAllTasks(0); // This will run the minimum number of tasks\n}\n\n\n * 当ioRatio = 100时，表示无需考虑执行时间的限制，当有IO就绪事件时（strategy > 0）Reactor线程需要优先处理IO就绪事件，处理完IO事件后，执行所有的异步任务包括：普通任务，尾部任务，定时任务。无时间限制。\n\n> strategy的数值表示IO就绪的Channel个数。它是前边介绍的io.netty.channel.nio.NioEventLoop#select方法的返回值。\n\n * 当ioRatio设置的值不为100时，默认为50。需要先统计出执行IO事件的用时ioTime，根据ioTime * (100 - ioRatio) / ioRatio计算出，后面执行异步任务的限制时间。也就是说Reactor线程需要在这个限定的时间内，执行有限的异步任务，防止Reactor线程由于处理异步任务时间过长而导致I/O 事件得不到及时地处理。\n\n> 默认情况下，执行IO事件用时和执行异步任务用时比例设置的是一比一。ioRatio设置的越高，则Reactor线程执行异步任务的时间占比越小。\n\n要想得到Reactor线程执行异步任务所需的时间限制，必须知道执行IO事件的用时ioTime然后在根据ioRatio计算出执行异步任务的时间限制。\n\n那如果此时并没有IO就绪事件需要Reactor线程处理的话，这种情况下我们无法得到ioTime，那怎么得到执行异步任务的限制时间呢？？\n\n在这种特殊情况下，Netty只允许Reactor线程最多执行64个异步任务，然后就结束执行。转去继续轮训IO就绪事件。核心目的还是防止Reactor线程由于处理异步任务时间过长而导致I/O 事件得不到及时地处理。\n\n> 默认情况下，当Reactor有异步任务需要处理但是没有IO就绪事件时，Netty只会允许Reactor线程执行最多64个异步任务。\n\n----------------------------------------\n\n现在我们对Reactor处理IO事件和异步任务的整体框架已经了解了，下面我们就来分别介绍下Reactor线程在处理IO事件和异步任务的具体逻辑是什么样的？\n\n\n# 3. Reactor线程处理IO就绪事件\n\n//该字段为持有selector对象selectedKeys的引用，当IO事件就绪时，直接从这里获取\nprivate SelectedSelectionKeySet selectedKeys;\n\nprivate void processSelectedKeys() {\n    //是否采用netty优化后的selectedKey集合类型 是由变量DISABLE_KEY_SET_OPTIMIZATION决定的 默认为false\n    if (selectedKeys != null) {\n        processSelectedKeysOptimized();\n    } else {\n        processSelectedKeysPlain(selector.selectedKeys());\n    }\n}\n\n\n看到这段代码大家眼熟吗？？\n\n----------------------------------------\n\n不知大家还记不记得我们在《Reactor在Netty中的实现(创建篇)》 一文中介绍Reactor NioEventLoop类在创建Selector的过程中提到，出于对JDK NIO Selector中selectedKeys 集合的插入和遍历操作性能的考虑Netty将自己用数组实现的SelectedSelectionKeySet 集合替换掉了JDK NIO Selector中selectedKeys的HashSet实现\n\npublic abstract class SelectorImpl extends AbstractSelector {\n\n    // The set of keys with data ready for an operation\n    // //IO就绪的SelectionKey（里面包裹着channel）\n    protected Set<SelectionKey> selectedKeys;\n\n    // The set of keys registered with this Selector\n    //注册在该Selector上的所有SelectionKey（里面包裹着channel）\n    protected HashSet<SelectionKey> keys;\n\n    ...............省略...................\n}\n\n\nNetty中通过优化开关DISABLE_KEY_SET_OPTIMIZATION控制是否对JDK NIO Selector进行优化。默认是需要优化。\n\n在优化开关开启的情况下，Netty会将创建的SelectedSelectionKeySet 集合保存在NioEventLoop的private SelectedSelectionKeySet selectedKeys字段中，方便Reactor线程直接从这里获取IO就绪的SelectionKey。\n\n在优化开关关闭的情况下，Netty会直接采用JDK NIO Selector的默认实现。此时NioEventLoop的selectedKeys字段就会为null。\n\n> 忘记这段的同学可以在回顾下《Reactor在Netty中的实现(创建篇)》一文中关于Reactor的创建过程。\n\n经过对前边内容的回顾，我们看到了在Reactor处理IO就绪事件的逻辑也分为两个部分，一个是经过Netty优化的，一个是采用JDK 原生的。\n\n我们先来看采用JDK 原生的Selector的处理方式，理解了这种方式，在看Netty优化的方式会更加容易。\n\n\n# 3.1 processSelectedKeysPlain\n\n我们在《Reactor在Netty中的实现(创建篇)》一文中介绍JDK NIO Selector的工作过程时讲过，当注册在Selector上的Channel发生IO就绪事件时，Selector会将IO就绪的SelectionKey插入到Set<SelectionKey> selectedKeys集合中。\n\n这时Reactor线程会从java.nio.channels.Selector#select(long)调用中返回。随后调用java.nio.channels.Selector#selectedKeys获取IO就绪的SelectionKey集合。\n\n所以Reactor线程在调用processSelectedKeysPlain方法处理IO就绪事件之前需要调用selector.selectedKeys()去获取所有IO就绪的SelectionKeys。\n\nprocessSelectedKeysPlain(selector.selectedKeys())\n    private void processSelectedKeysPlain(Set<SelectionKey> selectedKeys) {\n    if (selectedKeys.isEmpty()) {\n        return;\n    }\n\n    Iterator<SelectionKey> i = selectedKeys.iterator();\n    for (;;) {\n        final SelectionKey k = i.next();\n        final Object a = k.attachment();\n        //注意每次迭代末尾的keyIterator.remove()调用。Selector不会自己从已选择键集中移除SelectionKey实例。\n        //必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中。\n        i.remove();\n\n        if (a instanceof AbstractNioChannel) {\n            processSelectedKey(k, (AbstractNioChannel) a);\n        } else {\n            @SuppressWarnings("unchecked")\n            NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;\n            processSelectedKey(k, task);\n        }\n\n        if (!i.hasNext()) {\n            break;\n        }\n\n        //目的是再次进入for循环 移除失效的selectKey(socketChannel可能从selector上移除)\n        if (needsToSelectAgain) {\n            selectAgain();\n            selectedKeys = selector.selectedKeys();\n\n            // Create the iterator again to avoid ConcurrentModificationException\n            if (selectedKeys.isEmpty()) {\n                break;\n            } else {\n                i = selectedKeys.iterator();\n            }\n        }\n    }\n}\n\n\n# 3.1.1 获取 IO 就绪的 Channel\n\nSet<SelectionKey> selectedKeys 集合里面装的全部是IO就绪的SelectionKey，注意，此时Set<SelectionKey> selectedKeys的实现类型为HashSet类型。因为我们这里首先介绍的是JDK NIO 原生实现。\n\n通过获取HashSet的迭代器，开始逐个处理IO就绪的Channel。\n\nIterator<SelectionKey> i = selectedKeys.iterator();\nfinal SelectionKey k = i.next();\nfinal Object a = k.attachment();\n\n\n大家还记得这个SelectionKey中的attachment属性里存放的是什么吗？？\n\n在上篇文章《详细图解Netty Reactor启动全流程》中我们在讲NioServerSocketChannel向Main Reactor注册的时候，通过this指针将自己作为SelectionKey的attachment属性注册到Selector中。这一步完成了Netty自定义Channel和JDK NIO Channel的绑定。\n\n\n\npublic abstract class AbstractNioChannel extends AbstractChannel {\n\n    //channel注册到Selector后获得的SelectKey\n    volatile SelectionKey selectionKey;\n\n    @Override\n    protected void doRegister() throws Exception {\n        boolean selected = false;\n        for (;;) {\n            try {\n                selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this);\n                return;\n            } catch (CancelledKeyException e) {\n                ...............省略....................\n            }\n        }\n    }\n\n}\n\n\n而我们也提到SelectionKey就相当于是Channel在Selector中的一种表示，当Channel上有IO就绪事件时，Selector会将Channel对应的SelectionKey返回给Reactor线程，我们可以通过返回的这个SelectionKey里的attachment属性获取到对应的Netty自定义Channel。\n\n> 对于客户端连接事件（OP_ACCEPT）活跃时，这里的Channel类型为NioServerSocketChannel。对于客户端读写事件（Read，Write）活跃时，这里的Channel类型为NioSocketChannel。\n\n当我们通过k.attachment()获取到Netty自定义的Channel时，就需要把这个Channel对应的SelectionKey从Selector的就绪集合Set<SelectionKey> selectedKeys中删除。因为Selector自己不会主动删除已经处理完的SelectionKey，需要调用者自己主动删除，这样当这个Channel再次IO就绪时，Selector会再次将这个Channel对应的SelectionKey放入就绪集合Set<SelectionKey> selectedKeys中。\n\ni.remove();\n\n\n# 3.1.2 处理Channel上的IO事件\n\nif (a instanceof AbstractNioChannel) {\n    processSelectedKey(k, (AbstractNioChannel) a);\n} else {\n    @SuppressWarnings("unchecked")\n    NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;\n    processSelectedKey(k, task);\n}\n\n\n从这里我们可以看出 Netty 向SelectionKey中的attachment属性附加的对象分为两种：\n\n * 一种是我们熟悉的Channel，无论是服务端使用的NioServerSocketChannel还是客户端使用的NioSocketChannel都属于AbstractNioChannel。Channel上的IO事件是由Netty框架负责处理，也是本小节我们要重点介绍的\n * 另一种就是NioTask，这种类型是Netty提供给用户可以自定义一些当Channel上发生IO就绪事件时的自定义处理。\n\npublic interface NioTask<C extends SelectableChannel> {\n    /**\n     * Invoked when the {@link SelectableChannel} has been selected by the {@link Selector}.\n     */\n    void channelReady(C ch, SelectionKey key) throws Exception;\n\n    /**\n     * Invoked when the {@link SelectionKey} of the specified {@link SelectableChannel} has been cancelled and thus\n     * this {@link NioTask} will not be notified anymore.\n     *\n     * @param cause the cause of the unregistration. {@code null} if a user called {@link SelectionKey#cancel()} or\n     *              the event loop has been shut down.\n     */\n    void channelUnregistered(C ch, Throwable cause) throws Exception;\n}\n\n\n注意\n\nNioTask和Channel其实本质上是一样的都是负责处理Channel上的IO就绪事件，只不过一个是用户自定义处理，一个是Netty框架处理。这里我们重点关注Channel的IO处理逻辑\n\n----------------------------------------\n\nprivate void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {\n    //获取Channel的底层操作类Unsafe\n    final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();\n    if (!k.isValid()) {\n        ......如果SelectionKey已经失效则关闭对应的Channel......\n    }\n\n    try {\n        //获取IO就绪事件\n        int readyOps = k.readyOps();\n        //处理Connect事件\n        if ((readyOps & SelectionKey.OP_CONNECT) != 0) {\n            int ops = k.interestOps();\n            //移除对Connect事件的监听，否则Selector会一直通知\n            ops &= ~SelectionKey.OP_CONNECT;\n            k.interestOps(ops);\n            //触发channelActive事件处理Connect事件\n            unsafe.finishConnect();\n        }\n\n        //处理Write事件\n        if ((readyOps & SelectionKey.OP_WRITE) != 0) {\n            ch.unsafe().forceFlush();\n        }\n\n        //处理Read事件或者Accept事件\n        if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {\n            unsafe.read();\n        }\n    } catch (CancelledKeyException ignored) {\n        unsafe.close(unsafe.voidPromise());\n    }\n}\n\n\n * 首先我们需要获取IO就绪Channel底层的操作类Unsafe，用于对具体IO就绪事件的处理。\n\n> 这里可以看出，Netty对IO就绪事件的处理全部封装在Unsafe类中。比如：对OP_ACCEPT事件的具体处理逻辑是封装在NioServerSocketChannel中的UnSafe类中。对OP_READ或者OP_WRITE事件的处理是封装在NioSocketChannel中的Unsafe类中。\n\n * 从Selectionkey中获取具体IO就绪事件 readyOps。\n\nSelectonKey中关于IO事件的集合有两个。一个是interestOps,用于记录Channel感兴趣的IO事件，在Channel向Selector注册完毕后，通过pipeline中的HeadContext节点的ChannelActive事件回调中添加。下面这段代码就是在ChannelActive事件回调中Channel在向Selector注册自己感兴趣的IO事件。\n\npublic abstract class AbstractNioChannel extends AbstractChannel {\n    @Override\n    protected void doBeginRead() throws Exception {\n        // Channel.read() or ChannelHandlerContext.read() was called\n        final SelectionKey selectionKey = this.selectionKey;\n        if (!selectionKey.isValid()) {\n            return;\n        }\n\n        readPending = true;\n\n        final int interestOps = selectionKey.interestOps();\n        /**\n         * 1：ServerSocketChannel 初始化时 readInterestOp设置的是OP_ACCEPT事件\n         * 2：SocketChannel 初始化时 readInterestOp设置的是OP_READ事件\n         * */\n        if ((interestOps & readInterestOp) == 0) {\n            //注册监听OP_ACCEPT或者OP_READ事件\n            selectionKey.interestOps(interestOps | readInterestOp);\n        }\n    }\n}\n\n\n另一个就是这里的readyOps，用于记录在Channel感兴趣的IO事件中具体哪些IO事件就绪了。\n\nNetty中将各种事件的集合用一个int型变量来保存。\n\n * 用&操作判断，某个事件是否在事件集合中：(readyOps & SelectionKey.OP_CONNECT) != 0，这里就是判断Channel是否对Connect事件感兴趣。\n * 用|操作向事件集合中添加事件：interestOps | readInterestOp\n * 从事件集合中删除某个事件，是通过先将要删除事件取反~，然后在和事件集合做&操作：ops &= ~SelectionKey.OP_CONNECT\n\nNetty这种对空间的极致利用思想，很值得我们平时在日常开发中学习~~\n\n----------------------------------------\n\n现在我们已经知道哪些Channel现在处于IO就绪状态，并且知道了具体哪些类型的IO事件已经就绪。\n\n下面就该针对Channel上的不同IO就绪事件做出相应的处理了\n\n# 3.1.2.1 处理 Connect 事件\n\nNetty客户端向服务端发起连接，并向客户端的Reactor注册Connect事件，当连接建立成功后，客户端的NioSocketChannel就会产生Connect就绪事件，通过前面内容我们讲的Reactor的运行框架，最终流程会走到这里。\n\nif ((readyOps & SelectionKey.OP_CONNECT) != 0) {\n    int ops = k.interestOps();\n    ops &= ~SelectionKey.OP_CONNECT;\n    k.interestOps(ops);\n    //触发channelActive事件\n    unsafe.finishConnect();\n}\n\n\n如果IO就绪的事件是Connect事件，那么就调用对应客户端NioSocketChannel中的Unsafe操作类中的finishConnect方法处理Connect事件。这时会在Netty客户端NioSocketChannel中的pipeline中传播ChannelActive事件。\n\n最后需要将OP_CONNECT事件从客户端NioSocketChannel所关心的事件集合interestOps中删除。否则Selector会一直通知Connect事件就绪\n\n# 3.1.2.2 处理Write事件\n\n关于Reactor线程处理Netty中的Write事件的流程，笔者后续会专门用一篇文章来为大家介绍。本文我们重点关注Reactor线程的整体运行框架。\n\nif ((readyOps & SelectionKey.OP_WRITE) != 0) {\n    ch.unsafe().forceFlush();\n}\n\n\n这里大家只需要记住，OP_WRITE事件的注册是由用户来完成的，当Socket发送缓冲区已满无法继续写入数据时，用户会向Reactor注册OP_WRITE事件，等到Socket发送缓冲区变得可写时，Reactor会收到OP_WRITE事件活跃通知，随后在这里调用客户端NioSocketChannel中的forceFlush方法将剩余数据发送出去。\n\n# 3.1.2.3 处理Read事件或者Accept事件\n\nif ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {\n    unsafe.read();\n}\n\n\n这里可以看出Netty中处理Read事件和Accept事件都是由对应Channel中的Unsafe操作类中的read方法处理。\n\n服务端NioServerSocketChannel中的Read方法处理的是Accept事件，客户端NioSocketChannel中的Read方法处理的是Read事件。\n\n> 这里大家只需记住各个IO事件在对应Channel中的处理入口，后续文章我们会详细分析这些入口函数。\n\n# 3.1.3 从 Selector 中移除失效的 SelectionKey\n\n//用于及时从selectedKeys中清除失效的selectKey 比如 socketChannel从selector上被用户移除\nprivate boolean needsToSelectAgain;\n\n//目的是再次进入for循环 移除失效的selectKey(socketChannel可能被用户从selector上移除)\nif (needsToSelectAgain) {\n    selectAgain();\n    selectedKeys = selector.selectedKeys();\n\n    // Create the iterator again to avoid ConcurrentModificationException\n    if (selectedKeys.isEmpty()) {\n        break;\n    } else {\n        i = selectedKeys.iterator();\n    }\n}\n\n\n在前边介绍Reactor运行框架的时候，我们看到在每次Reactor线程轮询结束，准备处理IO就绪事件以及异步任务的时候，都会将needsToSelectAgain设置为false。\n\n那么这个needsToSelectAgain究竟是干嘛的？以及为什么我们需要去“Select Again”呢?\n\n首先我们来看下在什么情况下会将needsToSelectAgain这个变量设置为true，通过这个设置的过程，我们是否能够从中找到一些线索？\n\n我们知道Channel可以将自己注册到Selector上，那么当然也可以将自己从Selector上取消移除。\n\n在上篇文章中我们也花了大量的篇幅讲解了这个注册的过程，现在我们来看下Channel的取消注册。\n\npublic abstract class AbstractNioChannel extends AbstractChannel {\n\n   //channel注册到Selector后获得的SelectKey\n    volatile SelectionKey selectionKey;\n\n    @Override\n    protected void doDeregister() throws Exception {\n        eventLoop().cancel(selectionKey());\n    }\n\n    protected SelectionKey selectionKey() {\n        assert selectionKey != null;\n        return selectionKey;\n    }\n}\n\n\nChannel取消注册的过程很简单，直接调用NioChannel的doDeregister方法，Channel绑定的Reactor会将其从Selector中取消并停止监听Channel上的IO事件。\n\npublic final class NioEventLoop extends SingleThreadEventLoop {\n\n    //记录Selector上移除socketChannel的个数 达到256个 则需要将无效的selectKey从SelectedKeys集合中清除掉\n    private int cancelledKeys;\n\n    private static final int CLEANUP_INTERVAL = 256;\n\n    /**\n     * 将socketChannel从selector中移除 取消监听IO事件\n     * */\n    void cancel(SelectionKey key) {\n        key.cancel();\n        cancelledKeys ++;\n        // 当从selector中移除的socketChannel数量达到256个，设置needsToSelectAgain为true\n        // 在io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain 中重新做一次轮询，将失效的selectKey移除，\n        // 以保证selectKeySet的有效性\n        if (cancelledKeys >= CLEANUP_INTERVAL) {\n            cancelledKeys = 0;\n            needsToSelectAgain = true;\n        }\n    }\n}\n\n\n * 调用JDK NIO SelectionKey的API cancel方法，将Channel从Selector中取消掉。SelectionKey#cancel方法调用完毕后，此时调用SelectionKey#isValid将会返回false。SelectionKey#cancel方法调用后，Selector会将要取消的这个SelectionKey加入到Selector中的cancelledKeys集合中。\n\npublic abstract class AbstractSelector extends Selector {\n\n    private final Set<SelectionKey> cancelledKeys = new HashSet<SelectionKey>();\n\n    void cancel(SelectionKey k) {                      \n        synchronized (cancelledKeys) {\n            cancelledKeys.add(k);\n        }\n    }\n}\n\n\n * 当Channel对应的SelectionKey取消完毕后，Channel取消计数器cancelledKeys会加1，当cancelledKeys = 256时，将needsToSelectAgain设置为true。\n * 随后在Selector的**下一次轮询过程中，会将cancelledKeys集合中的SelectionKey从Selector中所有的KeySet中移除**。这里的KeySet包括Selector用于存放就绪SelectionKey的selectedKeys集合，以及用于存放所有注册的Channel对应的SelectionKey的keys集合。\n\npublic abstract class SelectorImpl extends AbstractSelector {\n\n    protected Set<SelectionKey> selectedKeys = new HashSet();\n    protected HashSet<SelectionKey> keys = new HashSet();\n    \n     .....................省略...............\n}\n\n\n----------------------------------------\n\n我们看到Reactor线程中对needsToSelectAgain的判断是在processSelectedKeysPlain方法处理IO就绪的SelectionKey的循环体中进行判断的。\n\n之所以这里特别提到needsToSelectAgain判断的位置，是要让大家注意到此时Reactor正在处理本次轮询的IO就绪事件。\n\n而前边也说了，当调用SelectionKey#cancel方法后，需要等到下次轮询的过程中Selector才会将这些取消的SelectionKey从Selector中的所有KeySet集合中移除，当然这里也包括就绪集合selectedKeys。\n\n当在本次轮询期间，假如大量的Channel从Selector中取消，Selector中的就绪集合selectedKeys中依然会保存这些Channel对应SelectionKey直到下次轮询。那么当然会影响本次轮询结果selectedKeys的有效性。\n\n所以为了保证Selector中所有KeySet的有效性，需要在Channel取消个数达到256时，触发一次selectNow，目的是清除无效的SelectionKey。\n\n    private void selectAgain() {\n        needsToSelectAgain = false;\n        try {\n            selector.selectNow();\n        } catch (Throwable t) {\n            logger.warn("Failed to update SelectionKeys.", t);\n        }\n    }\n\n\n----------------------------------------\n\n到这里，我们就对JDK 原生 Selector的处理方式processSelectedKeysPlain方法就介绍完了，其实 对IO就绪事件的处理逻辑都是一样的，在我们理解了processSelectedKeysPlain方法后，processSelectedKeysOptimized方法对IO就绪事件的处理，我们理解起来就非常轻松了。\n\n\n# 3.2 processSelectedKeysOptimized\n\nNetty默认会采用优化过的Selector对IO就绪事件的处理。但是处理逻辑是大同小异的。下面我们主要介绍一下这两个方法的不同之处。\n\n    private void processSelectedKeysOptimized() {\n        // 在openSelector的时候将JDK中selector实现类中得selectedKeys和publicSelectKeys字段类型\n        // 由原来的HashSet类型替换为 Netty优化后的数组实现的SelectedSelectionKeySet类型\n        for (int i = 0; i < selectedKeys.size; ++i) {\n            final SelectionKey k = selectedKeys.keys[i];\n            // 对应迭代器中得remove   selector不会自己清除selectedKey\n            selectedKeys.keys[i] = null;\n\n            final Object a = k.attachment();\n\n            if (a instanceof AbstractNioChannel) {\n                processSelectedKey(k, (AbstractNioChannel) a);\n            } else {\n                @SuppressWarnings("unchecked")\n                NioTask<SelectableChannel> task = (NioTask<SelectableChannel>) a;\n                processSelectedKey(k, task);\n            }\n\n            if (needsToSelectAgain) {\n\n                selectedKeys.reset(i + 1);\n\n                selectAgain();\n                i = -1;\n            }\n        }\n    }\n\n\n * JDK NIO 原生 Selector存放IO就绪的SelectionKey的集合为HashSet类型的selectedKeys。而Netty为了优化对selectedKeys 集合的遍历效率采用了自己实现的SelectedSelectionKeySet类型，从而用对数组的遍历代替用HashSet的迭代器遍历。\n\n * Selector会在每次轮询到IO就绪事件时，将IO就绪的Channel对应的SelectionKey插入到selectedKeys集合，但是Selector只管向selectedKeys集合放入IO就绪的SelectionKey，当SelectionKey被处理完毕后，Selector是不会自己主动将其从selectedKeys集合中移除的，典型的管杀不管埋。所以需要Netty自己在遍历到IO就绪的 SelectionKey后，将其删除。\n\n * * 在processSelectedKeysPlain中是直接将其从迭代器中删除。\n   * 在processSelectedKeysOptimized中将其在数组中对应的位置置为Null，方便垃圾回收。\n\n * 在最后清除无效的SelectionKey时，在processSelectedKeysPlain中由于采用的是JDK NIO 原生的Selector，所以只需要执行SelectAgain就可以，Selector会自动清除无效Key。但是在processSelectedKeysOptimized中由于是Netty自己实现的优化类型，所以需要Netty自己将SelectedSelectionKeySet数组中的SelectionKey全部清除，最后在执行SelectAgain。\n\n----------------------------------------\n\n好了，到这里，我们就将Reactor线程如何处理IO就绪事件的整个过程讲述完了，下面我们就该到了介绍Reactor线程如何处理Netty框架中的异步任务了。\n\n\n# 4. Reactor线程处理异步任务\n\nNetty关于处理异步任务的方法有两个：\n\n * 一个是无超时时间限制的runAllTasks()方法。当ioRatio设置为100时，Reactor线程会先一股脑的处理IO就绪事件，然后在一股脑的执行异步任务，并没有时间的限制。\n * 另一个是有超时时间限制的runAllTasks(long timeoutNanos)方法。当ioRatio != 100时，Reactor线程执行异步任务会有时间限制，优先一股脑的处理完IO就绪事件统计出执行IO任务耗时ioTime。根据公式ioTime * (100 - ioRatio) / ioRatio)计算出Reactor线程执行异步任务的超时时间。在超时时间限定范围内，执行有限的异步任务。\n\n\n\n下面我们来分别看下这两个执行异步任务的方法处理逻辑：\n\n\n# 4.1 runAllTasks()\n\nprotected boolean runAllTasks() {\n    assert inEventLoop();\n    boolean fetchedAll;\n    boolean ranAtLeastOne = false;\n\n    do {\n        //将到达执行时间的定时任务转存到普通任务队列taskQueue中，统一由Reactor线程从taskQueue中取出执行\n        fetchedAll = fetchFromScheduledTaskQueue();\n        if (runAllTasksFrom(taskQueue)) {\n            ranAtLeastOne = true;\n        }\n    } while (!fetchedAll); // keep on processing until we fetched all scheduled tasks.\n\n    if (ranAtLeastOne) {\n        lastExecutionTime = ScheduledFutureTask.nanoTime();\n    }\n    //执行尾部队列任务\n    afterRunningAllTasks();\n    return ranAtLeastOne;\n}\n\n\nReactor线程执行异步任务的核心逻辑就是：\n\n * 先将到期的定时任务一股脑的从定时任务队列scheduledTaskQueue中取出并转存到普通任务队列taskQueue中。\n * 由Reactor线程统一从普通任务队列taskQueue中取出任务执行。\n * 在Reactor线程执行完定时任务和普通任务后，开始执行存储于尾部任务队列tailTasks中的尾部任务。\n\n下面我们来分别看下上述几个核心步骤的实现：\n\n# 4.1.1 fetchFromScheduledTaskQueue\n\n/**\n * 从定时任务队列中取出达到deadline执行时间的定时任务\n * 将定时任务 转存到 普通任务队列taskQueue中，统一由Reactor线程从taskQueue中取出执行\n *\n * */\nprivate boolean fetchFromScheduledTaskQueue() {\n    if (scheduledTaskQueue == null || scheduledTaskQueue.isEmpty()) {\n        return true;\n    }\n    long nanoTime = AbstractScheduledEventExecutor.nanoTime();\n    for (;;) {\n        //从定时任务队列中取出到达执行deadline的定时任务  deadline <= nanoTime\n        Runnable scheduledTask = pollScheduledTask(nanoTime);\n        if (scheduledTask == null) {\n            return true;\n        }\n        if (!taskQueue.offer(scheduledTask)) {\n            // taskQueue没有空间容纳 则在将定时任务重新塞进定时任务队列中等待下次执行\n            scheduledTaskQueue.add((ScheduledFutureTask<?>) scheduledTask);\n            return false;\n        }\n    }\n}\n\n\n 1. 获取当前要执行异步任务的时间点nanoTime\n\nfinal class ScheduledFutureTask<V> extends PromiseTask<V> implements ScheduledFuture<V>, PriorityQueueNode {\n    private static final long START_TIME = System.nanoTime();\n\n    static long nanoTime() {\n        return System.nanoTime() - START_TIME;\n    }\n}\n\n\n 2. 从定时任务队列中找出deadline <= nanoTime的异步任务。也就是说找出所有到期的定时任务。\n\n    protected final Runnable pollScheduledTask(long nanoTime) {\n        assert inEventLoop();\n\n        //从定时队列中取出要执行的定时任务  deadline <= nanoTime\n        ScheduledFutureTask<?> scheduledTask = peekScheduledTask();\n        if (scheduledTask == null || scheduledTask.deadlineNanos() - nanoTime > 0) {\n            return null;\n        }\n        //符合取出条件 则取出\n        scheduledTaskQueue.remove();\n        scheduledTask.setConsumed();\n        return scheduledTask;\n    }\n\n\n 3. 将到期的定时任务插入到普通任务队列taskQueue中，如果taskQueue已经没有空间容纳新的任务，则将定时任务重新塞进定时任务队列中等待下次拉取。\n\n            if (!taskQueue.offer(scheduledTask)) {\n                scheduledTaskQueue.add((ScheduledFutureTask<?>) scheduledTask);\n                return false;\n            }\n\n\n 4. fetchFromScheduledTaskQueue方法的返回值为true时表示到期的定时任务已经全部拉取出来并转存到普通任务队列中。返回值为false时表示到期的定时任务只拉取出来一部分，因为这时普通任务队列已经满了，当执行完普通任务时，还需要在进行一次拉取。\n\n当到期的定时任务从定时任务队列中拉取完毕或者当普通任务队列已满时，这时就会停止拉取，开始执行普通任务队列中的异步任务。\n\n# 4.1.2 runAllTasksFrom\n\nprotected final boolean runAllTasksFrom(Queue<Runnable> taskQueue) {\n    Runnable task = pollTaskFrom(taskQueue);\n    if (task == null) {\n        return false;\n    }\n    for (;;) {\n        safeExecute(task);\n        task = pollTaskFrom(taskQueue);\n        if (task == null) {\n            return true;\n        }\n    }\n}\n\n\n * 首先runAllTasksFrom 方法的返回值表示是否执行了至少一个异步任务。后面会赋值给ranAtLeastOne变量，这个返回值我们后续会用到。\n * 从普通任务队列中拉取异步任务。\n\nprotected static Runnable pollTaskFrom(Queue<Runnable> taskQueue) {\n    for (;;) {\n        Runnable task = taskQueue.poll();\n        if (task != WAKEUP_TASK) {\n            return task;\n        }\n    }\n}\n\n\n * Reactor线程执行异步任务。\n\nprotected static void safeExecute(Runnable task) {\n    try {\n        task.run();\n    } catch (Throwable t) {\n        logger.warn("A task raised an exception. Task: {}", task, t);\n    }\n}\n\n\n# 4.1.3 afterRunningAllTasks\n\nif (ranAtLeastOne) {\n    lastExecutionTime = ScheduledFutureTask.nanoTime();\n}\n//执行尾部队列任务\nafterRunningAllTasks();\nreturn ranAtLeastOne;\n\n\n如果Reactor线程执行了至少一个异步任务，那么设置lastExecutionTime，并将ranAtLeastOne标识返回。这里的ranAtLeastOne标识就是runAllTasksFrom方法的返回值。\n\n最后执行收尾任务，也就是执行尾部任务队列中的尾部任务\n\n@Override\nprotected void afterRunningAllTasks() {\n    runAllTasksFrom(tailTasks);\n}\n\n\n\n# 4.2 runAllTasks(long timeoutNanos)\n\n\n\n这里在处理异步任务的核心逻辑还是和之前一样的，只不过就是多了对超时时间的控制。\n\nprotected boolean runAllTasks(long timeoutNanos) {\n    fetchFromScheduledTaskQueue();\n    Runnable task = pollTask();\n    if (task == null) {\n        //普通队列中没有任务时  执行队尾队列的任务\n        afterRunningAllTasks();\n        return false;\n    }\n\n    //异步任务执行超时deadline\n    final long deadline = timeoutNanos > 0 ? ScheduledFutureTask.nanoTime() + timeoutNanos : 0;\n    long runTasks = 0;\n    long lastExecutionTime;\n    for (;;) {\n        safeExecute(task);\n        runTasks ++;\n        //每运行64个异步任务 检查一下 是否达到 执行deadline\n        if ((runTasks & 0x3F) == 0) {\n            lastExecutionTime = ScheduledFutureTask.nanoTime();\n            if (lastExecutionTime >= deadline) {\n                //到达异步任务执行超时deadline，停止执行异步任务\n                break;\n            }\n        }\n\n        task = pollTask();\n        if (task == null) {\n            lastExecutionTime = ScheduledFutureTask.nanoTime();\n            break;\n        }\n    }\n\n    afterRunningAllTasks();\n    this.lastExecutionTime = lastExecutionTime;\n    return true;\n}\n\n\n * 首先还是通过fetchFromScheduledTaskQueue 方法从Reactor中的定时任务队列中拉取到期的定时任务，转存到普通任务队列中。当普通任务队列已满或者到期定时任务全部拉取完毕时，停止拉取。\n * 将ScheduledFutureTask.nanoTime() + timeoutNanos作为Reactor线程执行异步任务的超时时间点deadline。\n * 由于系统调用System.nanoTime()需要一定的系统开销，所以每执行完64个异步任务的时候才会去检查一下执行时间是否到达了deadline。如果到达了执行截止时间deadline则退出停止执行异步任务。如果没有到达deadline则继续从普通任务队列中取出任务循环执行下去。\n\n> 从这个细节又可以看出Netty对性能的考量还是相当讲究的\n\n----------------------------------------\n\n流程走到这里，我们就对Reactor的整个运行框架以及如何轮询IO就绪事件，如何处理IO就绪事件，如何执行异步任务的具体实现逻辑就剖析完了。\n\n下面还有一个小小的尾巴，就是Netty是如何解决文章开头提到的JDK NIO Epoll 的空轮询BUG的，让我们一起来看下吧~~~\n\n\n# 5. 解决 JDK Epoll 空轮询 BUG\n\n前边提到，由于JDK NIO Epoll的空轮询BUG存在，这样会导致Reactor线程在没有任何事情可做的情况下被意外唤醒，导致CPU空转。\n\n其实Netty也没有从根本上解决这个JDK BUG，而是选择巧妙的绕过这个BUG。\n\n下面我们来看下Netty是如何做到的。\n\n\n\nif (ranTasks || strategy > 0) {\n    if (selectCnt > MIN_PREMATURE_SELECTOR_RETURNS && logger.isDebugEnabled()) {\n        logger.debug("Selector.select() returned prematurely {} times in a row for Selector {}.",\n                     selectCnt - 1, selector);\n    }\n    selectCnt = 0;\n} else if (unexpectedSelectorWakeup(selectCnt)) { // Unexpected wakeup (unusual case)\n    //既没有IO就绪事件，也没有异步任务，Reactor线程从Selector上被异常唤醒 触发JDK Epoll空轮训BUG\n    //重新构建Selector,selectCnt归零\n    selectCnt = 0;\n}\n\n\n在Reactor线程处理完IO就绪事件和异步任务后，会检查这次Reactor线程被唤醒有没有执行过异步任务和有没有IO就绪的Channel。\n\n * boolean ranTasks 这时候就派上了用场，这个ranTasks正是前边我们在讲runAllTasks方法时提到的返回值。用来表示是否执行过至少一次异步任务。\n * int strategy 正是JDK NIO Selector的select方法的返回值，用来表示IO就绪的Channel个数。\n\n如果ranTasks = false 并且 strategy = 0这代表Reactor线程本次既没有异步任务执行也没有IO就绪的Channel需要处理却被意外的唤醒。等于是空转了一圈啥也没干。\n\n这种情况下Netty就会认为可能已经触发了JDK NIO Epoll的空轮询BUG\n\n    int SELECTOR_AUTO_REBUILD_THRESHOLD = SystemPropertyUtil.getInt("io.netty.selectorAutoRebuildThreshold", 512);\n\n    private boolean unexpectedSelectorWakeup(int selectCnt) {\n          ..................省略...............\n\n        /**\n         * 走到这里的条件是 既没有IO就绪事件，也没有异步任务，Reactor线程从Selector上被异常唤醒\n         * 这种情况可能是已经触发了JDK Epoll的空轮询BUG，如果这种情况持续512次 则认为可能已经触发BUG，于是重建Selector\n         *\n         * */\n        if (SELECTOR_AUTO_REBUILD_THRESHOLD > 0 &&\n                selectCnt >= SELECTOR_AUTO_REBUILD_THRESHOLD) {\n            // The selector returned prematurely many times in a row.\n            // Rebuild the selector to work around the problem.\n            logger.warn("Selector.select() returned prematurely {} times in a row; rebuilding Selector {}.",\n                    selectCnt, selector);\n            rebuildSelector();\n            return true;\n        }\n        return false;\n    }\n\n\n * 如果Reactor这种意外唤醒的次数selectCnt超过了配置的次数SELECTOR_AUTO_REBUILD_THRESHOLD,那么Netty就会认定这种情况可能已经触发了JDK NIO Epoll空轮询BUG，则重建Selector(将之前注册的所有Channel重新注册到新的Selector上并关闭旧的Selector)，selectCnt计数归0。\n\n> SELECTOR_AUTO_REBUILD_THRESHOLD默认为512，可以通过系统变量-D io.netty.selectorAutoRebuildThreshold指定自定义数值。\n\n * 如果selectCnt小于SELECTOR_AUTO_REBUILD_THRESHOLD，则返回不做任何处理，selectCnt继续计数。\n\nNetty就这样通过计数Reactor被意外唤醒的次数，如果计数selectCnt达到了512次，则通过重建Selector 巧妙的绕开了JDK NIO Epoll空轮询BUG。\n\n> 我们在日常开发中也可以借鉴Netty这种处理问题的思路，比如在项目开发中，当我们发现我们无法保证彻底的解决一个问题时，或者为了解决这个问题导致我们的投入产出比不高时，我们就该考虑是不是应该换一种思路去绕过这个问题，从而达到同样的效果。*解决问题的最高境界就是不解决它，巧妙的绕过去~~~~~！！*\n\n----------------------------------------\n\n\n# 总结\n\n本文花了大量的篇幅介绍了Reactor整体的运行框架，并深入介绍了Reactor核心的工作模块的具体实现逻辑。\n\n通过本文的介绍我们知道了Reactor如何轮询注册在其上的所有Channel上感兴趣的IO事件，以及Reactor如何去处理IO就绪的事件，如何执行Netty框架中提交的异步任务和定时任务。\n\n最后介绍了 Netty 如何巧妙的绕过 JDK NIO Epoll 空轮询的 BUG ,达到解决问题的目的。\n\n提炼了新的解决问题的思路：解决问题的最高境界就是不解决它，巧妙的绕过去~~~~~！！\n\n好了，本文的内容就到这里了，我们下篇文章见---\n\n\n# 参考资料\n\nhttps://mp.weixin.qq.com/s/g69upk3juqsq6LbwmtitcQ',normalizedContent:'# 前言\n\n\n\n本文笔者来为大家介绍下netty的核心引擎reactor的运转架构，希望通过本文的介绍能够让大家对reactor是如何驱动着整个netty框架的运转有一个全面的认识。也为我们后续进一步介绍netty关于处理网络请求的整个生命周期的相关内容做一个前置知识的铺垫，方便大家后续理解。\n\n那么在开始本文正式的内容之前，笔者先来带着大家回顾下前边文章介绍的关于netty整个框架如何搭建的相关内容，没有看过笔者前边几篇文章的读者朋友也没关系，这些并不会影响到本文的阅读，只不过涉及到相关细节的部分，大家可以在回看下。\n\n\n# 前文回顾\n\n在一文中，我们介绍了netty服务端的核心引擎主从reactor线程组的创建过程以及相关核心组件里的重要属性。在这个过程中，我们还提到了netty对各种细节进行的优化，比如针对jdk nio 原生selector做的一些优化，展现了netty对性能极致的追求。最终我们创建出了如下结构的reactor。\n\n\n\n在上篇文章《详细图解netty reactor启动全流程》中，我们完整地介绍了netty服务端启动的整个流程，并介绍了在启动过程中涉及到的serverbootstrap相关的属性以及配置方式。用于接收连接的服务端nioserversocketchannel的创建和初始化过程以及其类的继承结构。其中重点介绍了nioserversocketchannel向reactor的注册过程以及reactor线程的启动时机和pipeline的初始化时机。最后介绍了nioserversocketchannel绑定端口地址的整个流程。在这个过程中我们了解了netty的这些核心组件是如何串联起来的。\n\n当netty启动完毕后，我们得到了如下的框架结构：\n\n\n\n主 reactor 线程组中管理的是 nioserversocketchannel 用于接收客户端连接，并在自己的 pipeline 中的 serverbootstrapacceptor 里初始化接收到的客户端连接，随后会将初始化好的客户端连接注册到从 reactor 线程组中\n\n从reactor线程组主要负责监听处理注册其上的所有客户端连接的 io 就绪事件\n\n其中一个 channel 只能分配给一个固定的 reactor。一个 reactor 负责处理多个channel上的io就绪事件，这样可以将服务端承载的全量客户端连接分摊到多个reactor中处理，同时也能保证channel上io处理的线程安全性。reactor与channel之间的对应关系如下图所示：\n\n\n\n以上内容就是对笔者前边几篇文章的相关内容回顾，大家能回忆起来更好，回忆不起来也没关系，一点也不影响大家理解本文的内容。如果对相关细节感兴趣的同学，可以在阅读完本文之后，在去回看下。\n\n我们言归正传，正式开始本文的内容，笔者接下来会为大家介绍这些核心组件是如何相互配合从而驱动着整个netty reactor框架运转的。\n\n----------------------------------------\n\n当netty reactor框架启动完毕后，接下来第一件事情也是最重要的事情就是如何来高效的接收客户端的连接。\n\n那么在探讨 netty 服务端如何接收连接之前，我们需要弄清楚 reactor线程 的运行机制，它是如何监听并处理 channel 上的io就绪事件的。\n\n本文相当于是后续我们介绍 reactor线程 监听处理accept事件，read事件，write事件 的前置篇，本文专注于讲述 reactor线程 的整个运行框架。理解了本文的内容，对理解后面 reactor线程 如何处理io事件会大有帮助。\n\n我们在netty框架的 创建阶段 和 启动阶段 无数次的提到了 reactor线程 ，那么在本文要介绍的 运行阶段 就该这个 reactor线程 来大显神威了。\n\n经过前边文章的介绍，我们了解到 netty 中的 reactor线程 主要干三件事情：\n\n * 轮询：轮询注册在 reactor 上的所有 channel 感兴趣的 io就绪事件\n * 处理 ：处理channel 上的 io就绪事件\n * 执行：执行 netty 中的异步任务\n\n正是这三个部分组成了 reactor 的运行框架，那么我们现在来看下这个运行框架具体是怎么运转的\n\n\n# reactor线程的整个运行框架\n\n大家还记不记得笔者在《透过 netty 看 io 模型》一文中提到的，io模型的演变是围绕着"如何用尽可能少的线程去管理尽可能多的连接"这一主题进行的\n\n笔记\n\nnetty 的 io模型 是通过 jdk nio selector 实现的 io多路复用模型，而 netty 的 io线程模型 为 `主从reactor线程模型\n\n根据《透过 netty 看 io 模型》一文中介绍的 io多路复用模型 我们很容易就能理解到 netty 会使用一个用户态的 reactor线程 去不断的通过 selector 在内核态去轮训 channel 上的 io就绪事件\n\n说白了 reactor线程 其实执行的就是一个 死循环，在 死循环 中不断的通过 selector 去轮训 io就绪事件，如果发生 io就绪事件 则从selector系统调用中返回并处理io就绪事件，如果没有发生io就绪事件则一直阻塞在selector系统调用上，直到满足selector唤醒条件\n\n以下三个条件中只要满足任意一个条件，reactor 线程就会被从 selector 上唤醒：\n\n * 当 selector 轮询到有 io 活跃事件发生时\n * 当 reactor 线程需要执行的 定时任务 到达任务执行时间 deadline 时\n * 当有 异步任务 提交给 reactor 时，reactor 线程需要从 selector 上被唤醒，这样才能及时的去执行 异步任务\n\n> 这里可以看出 netty 对reactor线程的压榨还是比较狠的，反正现在也没有io就绪事件需要去处理，不能让reactor线程在这里白白等着，要立即唤醒它，转去处理提交过来的异步任务以及定时任务。reactor线程堪称996典范一刻不停歇地运作着。\n\n\n\n在了解了 reactor线程 的大概运行框架后，我们接下来就到源码中去看下它的核心运转框架是如何实现出来的\n\n由于这块源码比较庞大繁杂，所以笔者先把它的运行框架提取出来，方便大家整体的理解整个运行过程的全貌\n\n\n\n上图所展示的就是 reactor 整个工作体系的全貌，主要分为如下几个重要的工作模块：\n\n 1. reactor 线程在 selector 上阻塞获取 io 就绪事件。在这个模块中首先会去检查当前是否有异步任务需要执行，如果有异步需要执行，那么不管当前有没有 io 就绪事件都不能阻塞在 selector 上，随后会去非阻塞的轮询一下 selector 上是否有 io 就绪事件，如果有，正好可以和异步任务一起执行。优先处理 io 就绪事件，在执行异步任务。\n 2. 如果当前没有异步任务需要执行，那么 reactor 线程会接着查看是否有定时任务需要执行，如果有则在 selector 上阻塞直到定时任务的到期时间 deadline，或者满足其他唤醒条件被唤醒。如果没有定时任务需要执行，reactor 线程则会在 selector 上一直阻塞直到满足唤醒条件。\n 3. 当 reactor 线程满足唤醒条件被唤醒后，首先会去判断当前是因为有 io 就绪事件被唤醒还是因为有异步任务需要执行被唤醒或者是两者都有。随后 reactor 线程就会去处理 io 就绪事件和执行异步任务\n 4. 最后 reactor 线程返回循环起点不断的重复上述三个步骤。\n\n以上就是 reactor 线程运行的整个核心逻辑，下面是笔者根据上述核心逻辑，将 reactor 的整体代码设计框架提取出来，大家可以结合上边的 reactor 工作流程图，从总体上先感受下整个源码实现框架，能够把 reactor 的核心处理步骤和代码中相应的处理模块对应起来即可，这里不需要读懂每一行代码，要以逻辑处理模块为单位理解。后面笔者会将这些一个一个的逻辑处理模块在单独拎出来为大家详细介绍。\n\n@override\nprotected void run() {\n    //记录轮询次数 用于解决 jdk epoll 的空轮训 bug\n    int selectcnt = 0;\n    for (;;) {\n        try {\n            //轮询结果\n            int strategy;\n            try {\n                //根据轮询策略获取轮询结果 这里的 hastasks() 主要检查的是普通队列和尾部队列中是否有异步任务等待执行\n                strategy = selectstrategy.calculatestrategy(selectnowsupplier, hastasks());\n                switch (strategy) {\n                    case selectstrategy.continue:\n                        continue;\n                    case selectstrategy.busy_wait:\n                        // nio不支持自旋（busy_wait）\n                    case selectstrategy.select:\n                        //核心逻辑是有任务需要执行，则 reactor 线程立马执行异步任务，如果没有异步任务执行，则进行轮询io事件\n                    default:\n                }\n            } catch (ioexception e) {\n                ................省略...............\n            }\n\n            //执行到这里说明满足了唤醒条件，reactor线程从selector上被唤醒开始处理io就绪事件和执行异步任务\n                /**\n                 * reactor线程需要保证及时的执行异步任务，只要有异步任务提交，就需要退出轮询。\n                 * 有io事件就优先处理io事件，然后处理异步任务\n                 * */\n\n            selectcnt++;\n            //主要用于从 io 就绪的 selectedkeys 集合中剔除已经失效的 selectkey\n            needstoselectagain = false;\n            //调整 reactor 线程执行 io事件和执行异步任务的cpu时间比例 默认50，表示执行io事件和异步任务的时间比例是一比一\n            final int ioratio = this.ioratio;\n           \t// 这里主要处理io就绪事件，以及执行异步任务,需要优先处理io就绪事件，然后根据ioratio设置的处理io事件cpu用时与异步任务cpu用时比例，,来决定执行多长时间的异步任务\n            //判断是否触发jdk epoll bug 触发空轮询\n            if (rantasks || strategy > 0) {\n                if (selectcnt > min_premature_selector_returns && logger.isdebugenabled()) {\n                    logger.debug("selector.select() returned prematurely {} times in a row for selector {}.",selectcnt - 1, selector);\n                }\n                selectcnt = 0;\n            } else if (unexpectedselectorwakeup(selectcnt)) { // unexpected wakeup (unusual case)\n                //既没有io就绪事件，也没有异步任务，reactor线程从selector上被异常唤醒 触发jdk epoll空轮训bug\n                //重新构建selector,selectcnt归零\n                selectcnt = 0;\n            }\n        } catch (cancelledkeyexception e) {\n            ................省略...............\n        } catch (error e) {\n            ................省略...............\n        } catch (throwable t) {\n            ................省略...............\n        } finally {\n            ................省略...............\n        }\n    }\n}\n\n\n从上面提取出来的reactor的源码实现框架中，我们可以看出reactor线程主要做了下面几个事情：\n\n 1. 通过jdk nio selector轮询注册在reactor上的所有channel感兴趣的io事件。对于nioserversocketchannel来说因为它主要负责接收客户端连接所以监听的是op_accept事件，对于客户端niosocketchannel来说因为它主要负责处理连接上的读写事件所以监听的是op_read和op_write事件。\n\n注意\n\n这里需要注意的是 netty 只会自动注册op_read事件，而 op_write 事件是在当socket写入缓冲区以满无法继续写入发送数据时由用户自己注册\n\n 2. 如果有异步任务需要执行，则立马停止轮询操作，转去执行异步任务。这里分为两种情况：\n\n> 这里第二种情况下只会执行64个异步任务，目的是为了防止过度执行异步任务，耽误了最重要的事情轮询io事件。\n\n * 既有io就绪事件发生，也有异步任务需要执行。则优先处理io就绪事件，然后根据ioratio设置的执行时间比例决定执行多长时间的异步任务。这里reactor线程需要控制异步任务的执行时间，因为reactor线程的核心是处理io就绪事件，不能因为异步任务的执行而耽误了最重要的事情。\n\n * 没有io就绪事件发生，但是有异步任务或者定时任务到期需要执行。则只执行异步任务，尽可能的去压榨reactor线程。没有io就绪事件发生也不能闲着。\n\n 3. 在最后netty会判断本次reactor线程的唤醒是否是由于触发了 jdk epoll 空轮询 bug导致的，如果触发了该bug，则重建selector。绕过 jdk bug，达到解决问题的目的。\n\n> 正常情况下reactor线程从selector中被唤醒有两种情况：\n> \n>  * 轮询到有io就绪事件发生\n>  * 有异步任务或者定时任务需要执行。而 jdk epoll 空轮询 bug会在上述两种情况都没有发生的时候，reactor线程 会意外的从selector中被唤醒，导致cpu空转\n\n> jdk epoll 空轮询 bug：https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6670302\n\n好了，reactor线程的总体运行结构框架我们现在已经了解了，下面我们来深入到这些核心处理模块中来各个击破它们\n\n\n# 1. reactor 线程轮询 io 就绪事件\n\n在《reactor在netty中的实现(创建篇)》一文中，笔者在讲述主从reactor线程组nioeventloopgroup的创建过程的时候，提到一个构造器参数selectstrategyfactory。\n\npublic nioeventloopgroup(\n    int nthreads, executor executor, final selectorprovider selectorprovider) {\n    this(nthreads, executor, selectorprovider, defaultselectstrategyfactory.instance);\n}\n\npublic nioeventloopgroup(int nthreads, executor executor, final selectorprovider selectorprovider,\n                         final selectstrategyfactory selectstrategyfactory) {\n    super(nthreads, executor, selectorprovider, selectstrategyfactory, rejectedexecutionhandlers.reject());\n}\n\n\nreactor线程最重要的一件事情就是轮询io就绪事件，selectstrategyfactory就是用于指定轮询策略的，默认实现为defaultselectstrategyfactory.instance。\n\n而在reactor线程开启轮询的一开始，就是用这个selectstrategy去计算一个轮询策略strategy，后续会根据这个strategy进行不同的逻辑处理。\n\n@override\nprotected void run() {\n    //记录轮询次数 用于解决jdk epoll的空轮训bug\n    int selectcnt = 0;\n    for (;;) {\n        try {\n            //轮询结果\n            int strategy;\n            try {\n                //根据轮询策略获取轮询结果 这里的hastasks()主要检查的是普通队列和尾部队列中是否有异步任务等待执行\n                strategy = selectstrategy.calculatestrategy(selectnowsupplier, hastasks());\n                switch (strategy) {\n                    case selectstrategy.continue:\n                        continue;\n                    case selectstrategy.busy_wait:\n                        // nio不支持自旋（busy_wait）\n                    case selectstrategy.select:\n                        //核心逻辑是有任务需要执行，则reactor线程立马执行异步任务，如果没有异步任务执行，则进行轮询io事件\n                    default:\n                }\n            } catch (ioexception e) {\n                ................省略...............\n            }\n\n            ................省略...............\n}\n\n\n下面我们来看这个轮询策略strategy具体的计算逻辑是什么样的？\n\n\n# 1.1 轮询策略\n\n\n\npublic interface selectstrategy {\n    /**\n     * indicates a blocking select should follow.\n     */\n    int select = -1;\n    /**\n     * indicates the io loop should be retried, no blocking select to follow directly.\n     */\n    int continue = -2;\n    /**\n     * indicates the io loop to poll for new events without blocking.\n     */\n    int busy_wait = -3;\n\n    int calculatestrategy(intsupplier selectsupplier, boolean hastasks) throws exception;\n}\n\n\n我们首先来看下 netty 中定义的这三种轮询策略：\n\n * selectstrategy.select：此时没有任何异步任务需要执行，reactor线程可以安心的阻塞在selector上等待io就绪事件的来临。\n * selectstrategy.continue：重新开启一轮io轮询。\n * selectstrategy.busy_wait： reactor线程进行自旋轮询，由于nio 不支持自旋操作，所以这里直接跳到selectstrategy.select 策略\n\n----------------------------------------\n\n下面我们来看下 轮询策略 的计算逻辑 calculatestrategy：\n\nfinal class defaultselectstrategy implements selectstrategy {\n    static final selectstrategy instance = new defaultselectstrategy();\n\n    private defaultselectstrategy() { }\n\n    @override\n    public int calculatestrategy(intsupplier selectsupplier, boolean hastasks) throws exception {\n        /**\n         * reactor线程要保证及时的执行异步任务\n         * 1：如果有异步任务等待执行，则马上执行selectnow()非阻塞轮询一次io就绪事件\n         * 2：没有异步任务，则跳到switch select分支\n         * */\n        return hastasks ? selectsupplier.get() : selectstrategy.select;\n    }\n}\n\n\n * 在 reactor线程 的轮询工作开始之前，需要首先判断下当前是否有异步任务需要执行。判断依据就是查看reactor中的异步任务队列taskqueue和用于统计信息任务用的尾部队列tailtask是否有异步任务。\n\n@override\nprotected boolean hastasks() {\n    return super.hastasks() || !tailtasks.isempty();\n}\n\nprotected boolean hastasks() {\n    assert ineventloop();\n    return !taskqueue.isempty();\n}\n\n\n * 如果 reactor 中有 异步任务 需要执行，那么reactor线程需要立即执行，不能阻塞在selector上。在返回前需要再顺带调用selectnow()非阻塞查看一下当前是否有io就绪事件发生。如果有，那么正好可以和异步任务一起被处理，如果没有，则及时地处理异步任务。\n\n笔记\n\n这里netty要表达的语义是：首先reactor线程需要优先保证io就绪事件的处理，然后在保证异步任务的及时执行。如果当前没有io就绪事件但是有异步任务需要执行时，reactor线程就要去及时执行异步任务而不是继续阻塞在selector上等待io就绪事件。\n\nprivate final intsupplier selectnowsupplier = new intsupplier() {\n    @override\n    public int get() throws exception {\n        return selectnow();\n    }\n};\n\nint selectnow() throws ioexception {\n    //非阻塞\n    return selector.selectnow();\n}\n\n\n * 如果当前reactor线程没有异步任务需要执行，那么calculatestrategy方法直接返回selectstrategy.select也就是selectstrategy接口中定义的常量-1。当calculatestrategy方法通过selectnow()返回非零数值时，表示此时有io就绪的channel，返回的数值表示有多少个io就绪的channel。\n\n@override\nprotected void run() {\n    //记录轮询次数 用于解决jdk epoll的空轮训bug\n    int selectcnt = 0;\n    for (;;) {\n        try {\n            //轮询结果\n            int strategy;\n            try {\n                //根据轮询策略获取轮询结果 这里的hastasks()主要检查的是普通队列和尾部队列中是否有异步任务等待执行\n                strategy = selectstrategy.calculatestrategy(selectnowsupplier, hastasks());\n                switch (strategy) {\n                    case selectstrategy.continue:\n                        continue;\n                    case selectstrategy.busy_wait:\n                        // nio不支持自旋（busy_wait）\n                    case selectstrategy.select:\n                        //核心逻辑是有任务需要执行，则reactor线程立马执行异步任务，如果没有异步任务执行，则进行轮询io事件\n                    default:\n                }\n            } catch (ioexception e) {\n                ................省略...............\n            }\n\n             ................处理io就绪事件以及执行异步任务...............\n}\n\n\n从默认的轮询策略我们可以看出 selectstrategy.calculatestrategy 只会返回三种情况：\n\n\n\n * 返回 -1： switch逻辑分支进入selectstrategy.select分支，表示此时reactor中没有异步任务需要执行，reactor线程可以安心的阻塞在selector上等待io就绪事件发生。\n * 返回 0： switch逻辑分支进入default分支，表示此时reactor中没有io就绪事件但是有异步任务需要执行，流程通过default分支直接进入了处理异步任务的逻辑部分。\n * 返回 > 0：switch逻辑分支进入default分支，表示此时reactor中既有io就绪事件发生也有异步任务需要执行，流程通过default分支直接进入了处理io就绪事件和执行异步任务逻辑部分。\n\n现在reactor的流程处理逻辑走向我们清楚了，那么接下来我们把重点放在selectstrategy.select分支中的轮询逻辑上。这块是reactor监听io就绪事件的核心。\n\n\n# 1.2 轮询逻辑\n\n\n\ncase selectstrategy.select:\n//当前没有异步任务执行，reactor线程可以放心的阻塞等待io就绪事件\n\n//从定时任务队列中取出即将快要执行的定时任务 deadline\nlong curdeadlinenanos = nextscheduledtaskdeadlinenanos();\nif (curdeadlinenanos == -1l) {\n    // -1代表当前定时任务队列中没有定时任务\n    curdeadlinenanos = none; // nothing on the calendar\n}\n\n//最早执行定时任务的 deadline 作为 select 的阻塞时间，意思是到了定时任务的执行时间\n//不管有无 io 就绪事件，必须唤醒 selector，从而使 reactor 线程执行定时任务\nnextwakeupnanos.set(curdeadlinenanos);\ntry {\n    if (!hastasks()) {\n        //再次检查普通任务队列中是否有异步任务\n        //没有的话开始select阻塞轮询io就绪事件\n        strategy = select(curdeadlinenanos);\n    }\n} finally {\n    // 执行到这里说明reactor已经从selector上被唤醒了\n    // 设置reactor的状态为苏醒状态awake\n    // lazyset优化不必要的volatile操作，不使用内存屏障，不保证写操作的可见性（单线程不需要保证）\n    nextwakeupnanos.lazyset(awake);\n}\n\n\n流程走到这里，说明现在reactor上没有任何事情可做，可以安心的阻塞在selector上等待io就绪事件到来。\n\n那么reactor线程到底应该在selector上阻塞多久呢？？\n\n在回答这个问题之前，我们在回顾下《reactor在netty中的实现(创建篇)》一文中在讲述reactor的创建时提到，reactor线程除了要轮询channel上的io就绪事件，以及处理io就绪事件外，还有一个任务就是负责执行netty框架中的异步任务。\n\n\n\n而netty框架中的 异步任务 分为三类：\n\n * 存放在普通任务队列taskqueue中的普通异步任务。\n * 存放在尾部队列tailtasks中的用于执行统计任务等收尾动作的尾部任务。\n * 还有一种就是这里即将提到的定时任务。存放在reactor中的定时任务队列scheduledtaskqueue中。\n\n从reactornioeventloop类中的继承结构我们也可以看出，reactor具备执行定时任务的能力。\n\n\n\n既然reactor需要执行定时任务，那么它就不能一直阻塞在selector上无限等待io就绪事件。\n\n那么我们回到本小节一开始提到的问题上，为了保证reactor能够及时地执行定时任务，reactor线程需要在即将要执行的的第一个定时任务deadline到达之前被唤醒。\n\n笔记\n\nredis 的文件事件与时间事件协同机制，以及nginx中的处理机制，和上述“根据定时任务到达时间执行阻塞”有异曲同工之妙\n\n所以在reactor线程开始轮询io就绪事件之前，我们需要首先计算出来reactor线程在selector上的阻塞超时时间。\n\n# 1.2.1 reactor的轮询超时时间\n\n首先我们需要从reactor的定时任务队列scheduledtaskqueue中取出即将快要执行的定时任务deadline。将这个deadline作为reactor线程在selector上轮询的超时时间。这样可以保证在定时任务即将要执行时，reactor 现在可以及时的从 selector 上被唤醒。\n\n    private static final long awake = -1l;\n    private static final long none = long.max_value;\n\n    // nextwakeupnanos is:\n    //    awake            when el is awake\n    //    none             when el is waiting with no wakeup scheduled\n    //    other value t    when el is waiting with wakeup scheduled at time t\n    private final atomiclong nextwakeupnanos = new atomiclong(awake);\n\n      long curdeadlinenanos = nextscheduledtaskdeadlinenanos();\n      if (curdeadlinenanos == -1l) {\n            // -1代表当前定时任务队列中没有定时任务\n            curdeadlinenanos = none; // nothing on the calendar\n      }\n\n      nextwakeupnanos.set(curdeadlinenanos);\npublic abstract class abstractscheduledeventexecutor extends abstracteventexecutor {\n\n    priorityqueue<scheduledfuturetask<?>> scheduledtaskqueue;\n\n    protected final long nextscheduledtaskdeadlinenanos() {\n        scheduledfuturetask<?> scheduledtask = peekscheduledtask();\n        return scheduledtask != null ? scheduledtask.deadlinenanos() : -1;\n    }\n\n    final scheduledfuturetask<?> peekscheduledtask() {\n        queue<scheduledfuturetask<?>> scheduledtaskqueue = this.scheduledtaskqueue;\n        return scheduledtaskqueue != null ? scheduledtaskqueue.peek() : null;\n    }\n\n}\n\n\nnextscheduledtaskdeadlinenanos方法会返回当前reactor定时任务队列中最近的一个定时任务deadline时间点，如果定时任务队列中没有定时任务，则返回-1。\n\nnioeventloop中nextwakeupnanos变量用来存放reactor从selector上被唤醒的时间点，设置为最近需要被执行定时任务的deadline，如果当前并没有定时任务需要执行，那么就设置为long.max_value一直阻塞，直到有io就绪事件到达或者有异步任务需要执行。\n\n# 1.2.2 reactor开始轮询io就绪事件\n\nif (!hastasks()) {\n    //再次检查普通任务队列中是否有异步任务， 没有的话  开始select阻塞轮询io就绪事件\n    strategy = select(curdeadlinenanos);\n}\n\n\n在 reactor线程 开始阻塞轮询io就绪事件之前还需要再次检查一下是否有异步任务需要执行。\n\n如果此时恰巧有 异步任务 提交，就需要停止io就绪事件的轮询，转去执行异步任务。如果没有异步任务，则正式开始轮询io就绪事件。\n\nprivate int select(long deadlinenanos) throws ioexception {\n    if (deadlinenanos == none) {\n        //无定时任务，无普通任务执行时，开始轮询io就绪事件，没有就一直阻塞 直到唤醒条件成立\n        return selector.select();\n    }\n\n    long timeoutmillis = deadlinetodelaynanos(deadlinenanos + 995000l) / 1000000l;\n\n    return timeoutmillis <= 0 ? selector.selectnow() : selector.select(timeoutmillis);\n}\n\n\n如果deadlinenanos == none，经过上小节的介绍，我们知道none表示当前reactor中并没有定时任务，所以可以安心的阻塞在selector上等待io就绪事件到来。\n\nselector.select()调用是一个阻塞调用，如果没有io就绪事件，reactor线程就会一直阻塞在这里直到io就绪事件到来。这里占时不考虑前边提到的jdk nio epoll的空轮询bug.\n\n读到这里那么问题来了，此时reactor线程正阻塞在selector.select()调用上等待io就绪事件的到来，如果此时正好有异步任务被提交到reactor中需要执行，并且此时无任何io就绪事件，而reactor线程由于没有io就绪事件到来，会继续在这里阻塞，那么如何去执行异步任务呢？\n\n----------------------------------------\n\n解铃还须系铃人，既然异步任务在被提交后希望立马得到执行，那么就在提交异步任务的时候去唤醒reactor线程。\n\n//addtaskwakesup = true 表示 当且仅当只有调用 addtask 方法时 才会唤醒reactor线程\n//addtaskwakesup = false 表示 并不是只有 addtask 方法才能唤醒 reactor 还有其他方法可以唤醒 reactor 默认设置 false\nprivate final boolean addtaskwakesup;\n\nprivate void execute(runnable task, boolean immediate) {\n    boolean ineventloop = ineventloop();\n    addtask(task);\n    if (!ineventloop) {\n        //如果当前线程不是reactor线程，则启动reactor线程\n        //这里可以看出reactor线程的启动是通过 向nioeventloop添加异步任务时启动的\n        startthread();\n        .....................省略...................\n    }\n\n    if (!addtaskwakesup && immediate) {\n        //io.netty.channel.nio.nioeventloop.wakeup\n        wakeup(ineventloop);\n    }\n}\n\n\n对于execute方法我想大家一定不会陌生，在上篇文章《详细图解netty reactor启动全流程》中我们在介绍reactor线程的启动时介绍过该方法。\n\n在启动过程中涉及到的重要操作 register操作，bind操作 都需要封装成 异步任务 通过该方法提交到 reactor 中执行。\n\n这里我们将重点放在 execute方法 后半段 wakeup 逻辑部分。\n\n我们先介绍下和wakeup逻辑相关的两个参数 boolean immediate 和 boolean addtaskwakesup\n\n * immediate：表示提交的task是否需要被立即执行。netty 中只要你提交的任务类型不是lazyrunnable类型的任务，都是需要立即执行的。immediate = true\n * addtaskwakesup： true 表示当且仅当只有调用addtask方法时才会唤醒reactor线程。调用别的方法并不会唤醒reactor线程。在初始化nioeventloop时会设置为false，表示并不是只有addtask方法才能唤醒reactor线程 还有其他方法可以唤醒reactor线程，比如这里的execute方法就会唤醒reactor线程。\n\n针对 execute 方法中的这个唤醒条件!addtaskwakesup && immediate，netty 这里要表达的语义是：当immediate参数为true的时候表示该异步任务需要立即执行，addtaskwakesup 默认设置为false 表示不仅只有addtask方法可以唤醒reactor，还有其他方法比如这里的execute方法也可以唤醒。但是当设置为true时，语义就变为只有addtask才可以唤醒reactor，即使execute方法里的immediate = true也不能唤醒reactor，因为执行的是execute方法而不是addtask方法。\n\nprivate static final long awake = -1l;\nprivate final atomiclong nextwakeupnanos = new atomiclong(awake);\n\nprotected void wakeup(boolean ineventloop) {\n    if (!ineventloop && nextwakeupnanos.getandset(awake) != awake) {\n        //将reactor线程从selector上唤醒\n        selector.wakeup();\n    }\n}\n\n\n当nextwakeupnanos = awake时表示当前reactor正处于苏醒状态，既然是苏醒状态也就没有必要去执行selector.wakeup()重复唤醒reactor了，同时也能省去这一次的系统调用开销。\n\n在《1.2小节 轮询逻辑》开始介绍的源码实现框架里 reactor 被唤醒之后执行代码会进入 finally{...} 语句块中，在那里会将nextwakeupnanos设置为awake。\n\ntry {\n    if (!hastasks()) {\n        strategy = select(curdeadlinenanos);\n    }\n} finally {\n    // 执行到这里说明reactor已经从selector上被唤醒了\n    // 设置reactor的状态为苏醒状态awake\n    // lazyset优化不必要的volatile操作，不使用内存屏障，不保证写操作的可见性（单线程不需要保证）\n    nextwakeupnanos.lazyset(awake);\n}\n\n\n笔记\n\n这里netty用了一个atomiclong类型的变量nextwakeupnanos，既能表示当前reactor线程的状态，又能表示reactor线程的阻塞超时时间。我们在日常开发中也可以学习下这种技巧。\n\n----------------------------------------\n\n我们继续回到 reactor线程 轮询 io就绪事件 的主线上\n\nprivate int select(long deadlinenanos) throws ioexception {\n    if (deadlinenanos == none) {\n        //无定时任务，无普通任务执行时，开始轮询io就绪事件，没有就一直阻塞 直到唤醒条件成立\n        return selector.select();\n    }\n\n    long timeoutmillis = deadlinetodelaynanos(deadlinenanos + 995000l) / 1000000l;\n\n    return timeoutmillis <= 0 ? selector.selectnow() : selector.select(timeoutmillis);\n}\n\n\n当deadlinenanos不为none，表示此时reactor有定时任务需要执行，reactor线程需要阻塞在selector上等待io就绪事件直到最近的一个定时任务执行时间点deadline到达。\n\n这里的deadlinenanos表示的就是reactor中最近的一个定时任务执行时间点deadline，单位是纳秒。指的是一个绝对时间。\n\n而我们需要计算的是reactor线程阻塞在selector的超时时间timeoutmillis，单位是毫秒，指的是一个相对时间。\n\n\n\n所以在reactor线程开始阻塞在selector上之前，我们需要将这个单位为纳秒的绝对时间deadlinenanos转化为单位为毫秒的相对时间timeoutmillis。\n\nprivate int select(long deadlinenanos) throws ioexception {\n    if (deadlinenanos == none) {\n        //无定时任务，无普通任务执行时，开始轮询io就绪事件，没有就一直阻塞 直到唤醒条件成立\n        return selector.select();\n    }\n\n    long timeoutmillis = deadlinetodelaynanos(deadlinenanos + 995000l) / 1000000l;\n\n    return timeoutmillis <= 0 ? selector.selectnow() : selector.select(timeoutmillis);\n}\n\n\n这里大家可能会好奇，通过deadlinetodelaynanos方法计算timeoutmillis的时候，为什么要给deadlinenanos在加上0.995毫秒呢？？\n\n大家想象一下这样的场景，当最近的一个定时任务的deadline即将在5微秒内到达，那么这时将纳秒转换成毫秒计算出的timeoutmillis会是0。\n\n而在netty中timeoutmillis = 0要表达的语义是：定时任务执行时间已经到达deadline时间点，需要被执行。\n\n而现实情况是定时任务还有5微秒才能够到达deadline，所以对于这种情况，需要在deadlinenanos在加上0.995毫秒凑成1毫秒不能让其为0。\n\n笔记\n\n所以从这里我们可以看出，reactor在有定时任务的情况下，至少要阻塞1毫秒。\n\npublic abstract class abstractscheduledeventexecutor extends abstracteventexecutor {\n\n    protected static long deadlinetodelaynanos(long deadlinenanos) {\n        return scheduledfuturetask.deadlinetodelaynanos(deadlinenanos);\n    }\n}\nfinal class scheduledfuturetask<v> extends promisetask<v> implements scheduledfuture<v>, priorityqueuenode {\n\n    static long deadlinetodelaynanos(long deadlinenanos) {\n        return deadlinenanos == 0l ? 0l : math.max(0l, deadlinenanos - nanotime());\n    }\n\n    //启动时间点\n    private static final long start_time = system.nanotime();\n\n    static long nanotime() {\n        return system.nanotime() - start_time;\n    }\n\n    static long deadlinenanos(long delay) {\n        //计算定时任务执行deadline  去除启动时间\n        long deadlinenanos = nanotime() + delay;\n        // guard against overflow\n        return deadlinenanos < 0 ? long.max_value : deadlinenanos;\n    }\n\n}\n\n\n这里需要注意一下，在创建定时任务时会通过deadlinenanos方法计算定时任务的执行deadline，deadline的计算逻辑是当前时间点+任务延时delay-系统启动时间。这里需要扣除系统启动的时间。\n\n所以这里在通过deadline计算延时delay（也就是timeout）的时候需要在加上系统启动的时间 : deadlinenanos - nanotime()\n\n当通过deadlinetodelaynanos计算出的timeoutmillis <= 0时，表示reactor目前有临近的定时任务需要执行，这时候就需要立马返回，不能阻塞在selector上影响定时任务的执行。当然在返回执行定时任务前，需要在顺手通过selector.selectnow()非阻塞轮询一下channel上是否有io就绪事件到达，防止耽误io事件的处理。真是操碎了心~~\n\n当timeoutmillis > 0时，reactor线程就可以安心的阻塞在selector上等待io事件的到来，直到timeoutmillis超时时间到达。\n\ntimeoutmillis <= 0 ? selector.selectnow() : selector.select(timeoutmillis)\n\n\n当注册在reactor上的channel中有io事件到来时，reactor线程就会从selector.select(timeoutmillis)调用中唤醒，立即去处理io就绪事件。\n\n这里假设一种极端情况，如果最近的一个定时任务的deadline是在未来很远的一个时间点，这样就会使timeoutmillis的时间非常非常久，那么reactor岂不是会一直阻塞在selector上造成 netty 无法工作？\n\n笔者觉得大家现在心里应该已经有了答案，我们在《1.2.2 reactor开始轮询io就绪事件》小节一开始介绍过，当reactor正在selector上阻塞时，如果此时用户线程向 reactor 提交了异步任务，reactor线程会通过execute方法被唤醒。\n\n----------------------------------------\n\n流程到这里，reactor中最重要也是最核心的逻辑：轮询channel上的 io就绪事件 的处理流程我们就讲解完了。\n\n当 reactor 轮询到有io活跃事件或者有异步任务需要执行时，就会从selector上被唤醒，下面就到了该介reactor 被唤醒之后是如何处理io就绪事件以及如何执行异步任务的时候了\n\nnetty毕竟是一个网络框架，所以它会优先去处理channel上的io事件，基于这个事实，所以netty不会容忍 异步任务 被无限制的执行从而影响io吞吐\n\nnetty通过ioratio变量来调配reactor线程在处理io事件和执行异步任务之间的cpu时间分配比例\n\n下面我们就来看下这个执行时间比例的分配逻辑是什么样的\n\n\n# 2. reactor 处理 io 与处理异步任务的时间比例分配\n\n无论什么时候，当有 io就绪事件 到来时，reactor都需要保证io事件被及时完整的处理完，而ioratio主要限制的是执行异步任务所需用时，防止 reactor线程 处理 异步任务 时间过长而导致i/o 事件得不到及时地处理\n\n\n\n//调整reactor线程执行io事件和执行异步任务的cpu时间比例 默认50，表示执行io事件和异步任务的时间比例是一比一\nfinal int ioratio = this.ioratio;\nboolean rantasks;\nif (ioratio == 100) { //先一股脑执行io事件，在一股脑执行异步任务（无时间限制）\n    try {\n        if (strategy > 0) {\n            //如果有io就绪事件 则处理io就绪事件\n            processselectedkeys();\n        }\n    } finally {\n        // ensure we always run tasks.\n        //处理所有异步任务\n        rantasks = runalltasks();\n    }\n} else if (strategy > 0) {//先执行io事件 用时iotime  执行异步任务只能用时iotime * (100 - ioratio) / ioratio\n    final long iostarttime = system.nanotime();\n    try {\n        processselectedkeys();\n    } finally {\n        // ensure we always run tasks.\n        final long iotime = system.nanotime() - iostarttime;\n        // 限定在超时时间内 处理有限的异步任务 防止reactor线程处理异步任务时间过长而导致 i/o 事件阻塞\n        rantasks = runalltasks(iotime * (100 - ioratio) / ioratio);\n    }\n} else { //没有io就绪事件处理，则只执行异步任务 最多执行64个 防止reactor线程处理异步任务时间过长而导致 i/o 事件阻塞\n    rantasks = runalltasks(0); // this will run the minimum number of tasks\n}\n\n\n * 当ioratio = 100时，表示无需考虑执行时间的限制，当有io就绪事件时（strategy > 0）reactor线程需要优先处理io就绪事件，处理完io事件后，执行所有的异步任务包括：普通任务，尾部任务，定时任务。无时间限制。\n\n> strategy的数值表示io就绪的channel个数。它是前边介绍的io.netty.channel.nio.nioeventloop#select方法的返回值。\n\n * 当ioratio设置的值不为100时，默认为50。需要先统计出执行io事件的用时iotime，根据iotime * (100 - ioratio) / ioratio计算出，后面执行异步任务的限制时间。也就是说reactor线程需要在这个限定的时间内，执行有限的异步任务，防止reactor线程由于处理异步任务时间过长而导致i/o 事件得不到及时地处理。\n\n> 默认情况下，执行io事件用时和执行异步任务用时比例设置的是一比一。ioratio设置的越高，则reactor线程执行异步任务的时间占比越小。\n\n要想得到reactor线程执行异步任务所需的时间限制，必须知道执行io事件的用时iotime然后在根据ioratio计算出执行异步任务的时间限制。\n\n那如果此时并没有io就绪事件需要reactor线程处理的话，这种情况下我们无法得到iotime，那怎么得到执行异步任务的限制时间呢？？\n\n在这种特殊情况下，netty只允许reactor线程最多执行64个异步任务，然后就结束执行。转去继续轮训io就绪事件。核心目的还是防止reactor线程由于处理异步任务时间过长而导致i/o 事件得不到及时地处理。\n\n> 默认情况下，当reactor有异步任务需要处理但是没有io就绪事件时，netty只会允许reactor线程执行最多64个异步任务。\n\n----------------------------------------\n\n现在我们对reactor处理io事件和异步任务的整体框架已经了解了，下面我们就来分别介绍下reactor线程在处理io事件和异步任务的具体逻辑是什么样的？\n\n\n# 3. reactor线程处理io就绪事件\n\n//该字段为持有selector对象selectedkeys的引用，当io事件就绪时，直接从这里获取\nprivate selectedselectionkeyset selectedkeys;\n\nprivate void processselectedkeys() {\n    //是否采用netty优化后的selectedkey集合类型 是由变量disable_key_set_optimization决定的 默认为false\n    if (selectedkeys != null) {\n        processselectedkeysoptimized();\n    } else {\n        processselectedkeysplain(selector.selectedkeys());\n    }\n}\n\n\n看到这段代码大家眼熟吗？？\n\n----------------------------------------\n\n不知大家还记不记得我们在《reactor在netty中的实现(创建篇)》 一文中介绍reactor nioeventloop类在创建selector的过程中提到，出于对jdk nio selector中selectedkeys 集合的插入和遍历操作性能的考虑netty将自己用数组实现的selectedselectionkeyset 集合替换掉了jdk nio selector中selectedkeys的hashset实现\n\npublic abstract class selectorimpl extends abstractselector {\n\n    // the set of keys with data ready for an operation\n    // //io就绪的selectionkey（里面包裹着channel）\n    protected set<selectionkey> selectedkeys;\n\n    // the set of keys registered with this selector\n    //注册在该selector上的所有selectionkey（里面包裹着channel）\n    protected hashset<selectionkey> keys;\n\n    ...............省略...................\n}\n\n\nnetty中通过优化开关disable_key_set_optimization控制是否对jdk nio selector进行优化。默认是需要优化。\n\n在优化开关开启的情况下，netty会将创建的selectedselectionkeyset 集合保存在nioeventloop的private selectedselectionkeyset selectedkeys字段中，方便reactor线程直接从这里获取io就绪的selectionkey。\n\n在优化开关关闭的情况下，netty会直接采用jdk nio selector的默认实现。此时nioeventloop的selectedkeys字段就会为null。\n\n> 忘记这段的同学可以在回顾下《reactor在netty中的实现(创建篇)》一文中关于reactor的创建过程。\n\n经过对前边内容的回顾，我们看到了在reactor处理io就绪事件的逻辑也分为两个部分，一个是经过netty优化的，一个是采用jdk 原生的。\n\n我们先来看采用jdk 原生的selector的处理方式，理解了这种方式，在看netty优化的方式会更加容易。\n\n\n# 3.1 processselectedkeysplain\n\n我们在《reactor在netty中的实现(创建篇)》一文中介绍jdk nio selector的工作过程时讲过，当注册在selector上的channel发生io就绪事件时，selector会将io就绪的selectionkey插入到set<selectionkey> selectedkeys集合中。\n\n这时reactor线程会从java.nio.channels.selector#select(long)调用中返回。随后调用java.nio.channels.selector#selectedkeys获取io就绪的selectionkey集合。\n\n所以reactor线程在调用processselectedkeysplain方法处理io就绪事件之前需要调用selector.selectedkeys()去获取所有io就绪的selectionkeys。\n\nprocessselectedkeysplain(selector.selectedkeys())\n    private void processselectedkeysplain(set<selectionkey> selectedkeys) {\n    if (selectedkeys.isempty()) {\n        return;\n    }\n\n    iterator<selectionkey> i = selectedkeys.iterator();\n    for (;;) {\n        final selectionkey k = i.next();\n        final object a = k.attachment();\n        //注意每次迭代末尾的keyiterator.remove()调用。selector不会自己从已选择键集中移除selectionkey实例。\n        //必须在处理完通道时自己移除。下次该通道变成就绪时，selector会再次将其放入已选择键集中。\n        i.remove();\n\n        if (a instanceof abstractniochannel) {\n            processselectedkey(k, (abstractniochannel) a);\n        } else {\n            @suppresswarnings("unchecked")\n            niotask<selectablechannel> task = (niotask<selectablechannel>) a;\n            processselectedkey(k, task);\n        }\n\n        if (!i.hasnext()) {\n            break;\n        }\n\n        //目的是再次进入for循环 移除失效的selectkey(socketchannel可能从selector上移除)\n        if (needstoselectagain) {\n            selectagain();\n            selectedkeys = selector.selectedkeys();\n\n            // create the iterator again to avoid concurrentmodificationexception\n            if (selectedkeys.isempty()) {\n                break;\n            } else {\n                i = selectedkeys.iterator();\n            }\n        }\n    }\n}\n\n\n# 3.1.1 获取 io 就绪的 channel\n\nset<selectionkey> selectedkeys 集合里面装的全部是io就绪的selectionkey，注意，此时set<selectionkey> selectedkeys的实现类型为hashset类型。因为我们这里首先介绍的是jdk nio 原生实现。\n\n通过获取hashset的迭代器，开始逐个处理io就绪的channel。\n\niterator<selectionkey> i = selectedkeys.iterator();\nfinal selectionkey k = i.next();\nfinal object a = k.attachment();\n\n\n大家还记得这个selectionkey中的attachment属性里存放的是什么吗？？\n\n在上篇文章《详细图解netty reactor启动全流程》中我们在讲nioserversocketchannel向main reactor注册的时候，通过this指针将自己作为selectionkey的attachment属性注册到selector中。这一步完成了netty自定义channel和jdk nio channel的绑定。\n\n\n\npublic abstract class abstractniochannel extends abstractchannel {\n\n    //channel注册到selector后获得的selectkey\n    volatile selectionkey selectionkey;\n\n    @override\n    protected void doregister() throws exception {\n        boolean selected = false;\n        for (;;) {\n            try {\n                selectionkey = javachannel().register(eventloop().unwrappedselector(), 0, this);\n                return;\n            } catch (cancelledkeyexception e) {\n                ...............省略....................\n            }\n        }\n    }\n\n}\n\n\n而我们也提到selectionkey就相当于是channel在selector中的一种表示，当channel上有io就绪事件时，selector会将channel对应的selectionkey返回给reactor线程，我们可以通过返回的这个selectionkey里的attachment属性获取到对应的netty自定义channel。\n\n> 对于客户端连接事件（op_accept）活跃时，这里的channel类型为nioserversocketchannel。对于客户端读写事件（read，write）活跃时，这里的channel类型为niosocketchannel。\n\n当我们通过k.attachment()获取到netty自定义的channel时，就需要把这个channel对应的selectionkey从selector的就绪集合set<selectionkey> selectedkeys中删除。因为selector自己不会主动删除已经处理完的selectionkey，需要调用者自己主动删除，这样当这个channel再次io就绪时，selector会再次将这个channel对应的selectionkey放入就绪集合set<selectionkey> selectedkeys中。\n\ni.remove();\n\n\n# 3.1.2 处理channel上的io事件\n\nif (a instanceof abstractniochannel) {\n    processselectedkey(k, (abstractniochannel) a);\n} else {\n    @suppresswarnings("unchecked")\n    niotask<selectablechannel> task = (niotask<selectablechannel>) a;\n    processselectedkey(k, task);\n}\n\n\n从这里我们可以看出 netty 向selectionkey中的attachment属性附加的对象分为两种：\n\n * 一种是我们熟悉的channel，无论是服务端使用的nioserversocketchannel还是客户端使用的niosocketchannel都属于abstractniochannel。channel上的io事件是由netty框架负责处理，也是本小节我们要重点介绍的\n * 另一种就是niotask，这种类型是netty提供给用户可以自定义一些当channel上发生io就绪事件时的自定义处理。\n\npublic interface niotask<c extends selectablechannel> {\n    /**\n     * invoked when the {@link selectablechannel} has been selected by the {@link selector}.\n     */\n    void channelready(c ch, selectionkey key) throws exception;\n\n    /**\n     * invoked when the {@link selectionkey} of the specified {@link selectablechannel} has been cancelled and thus\n     * this {@link niotask} will not be notified anymore.\n     *\n     * @param cause the cause of the unregistration. {@code null} if a user called {@link selectionkey#cancel()} or\n     *              the event loop has been shut down.\n     */\n    void channelunregistered(c ch, throwable cause) throws exception;\n}\n\n\n注意\n\nniotask和channel其实本质上是一样的都是负责处理channel上的io就绪事件，只不过一个是用户自定义处理，一个是netty框架处理。这里我们重点关注channel的io处理逻辑\n\n----------------------------------------\n\nprivate void processselectedkey(selectionkey k, abstractniochannel ch) {\n    //获取channel的底层操作类unsafe\n    final abstractniochannel.niounsafe unsafe = ch.unsafe();\n    if (!k.isvalid()) {\n        ......如果selectionkey已经失效则关闭对应的channel......\n    }\n\n    try {\n        //获取io就绪事件\n        int readyops = k.readyops();\n        //处理connect事件\n        if ((readyops & selectionkey.op_connect) != 0) {\n            int ops = k.interestops();\n            //移除对connect事件的监听，否则selector会一直通知\n            ops &= ~selectionkey.op_connect;\n            k.interestops(ops);\n            //触发channelactive事件处理connect事件\n            unsafe.finishconnect();\n        }\n\n        //处理write事件\n        if ((readyops & selectionkey.op_write) != 0) {\n            ch.unsafe().forceflush();\n        }\n\n        //处理read事件或者accept事件\n        if ((readyops & (selectionkey.op_read | selectionkey.op_accept)) != 0 || readyops == 0) {\n            unsafe.read();\n        }\n    } catch (cancelledkeyexception ignored) {\n        unsafe.close(unsafe.voidpromise());\n    }\n}\n\n\n * 首先我们需要获取io就绪channel底层的操作类unsafe，用于对具体io就绪事件的处理。\n\n> 这里可以看出，netty对io就绪事件的处理全部封装在unsafe类中。比如：对op_accept事件的具体处理逻辑是封装在nioserversocketchannel中的unsafe类中。对op_read或者op_write事件的处理是封装在niosocketchannel中的unsafe类中。\n\n * 从selectionkey中获取具体io就绪事件 readyops。\n\nselectonkey中关于io事件的集合有两个。一个是interestops,用于记录channel感兴趣的io事件，在channel向selector注册完毕后，通过pipeline中的headcontext节点的channelactive事件回调中添加。下面这段代码就是在channelactive事件回调中channel在向selector注册自己感兴趣的io事件。\n\npublic abstract class abstractniochannel extends abstractchannel {\n    @override\n    protected void dobeginread() throws exception {\n        // channel.read() or channelhandlercontext.read() was called\n        final selectionkey selectionkey = this.selectionkey;\n        if (!selectionkey.isvalid()) {\n            return;\n        }\n\n        readpending = true;\n\n        final int interestops = selectionkey.interestops();\n        /**\n         * 1：serversocketchannel 初始化时 readinterestop设置的是op_accept事件\n         * 2：socketchannel 初始化时 readinterestop设置的是op_read事件\n         * */\n        if ((interestops & readinterestop) == 0) {\n            //注册监听op_accept或者op_read事件\n            selectionkey.interestops(interestops | readinterestop);\n        }\n    }\n}\n\n\n另一个就是这里的readyops，用于记录在channel感兴趣的io事件中具体哪些io事件就绪了。\n\nnetty中将各种事件的集合用一个int型变量来保存。\n\n * 用&操作判断，某个事件是否在事件集合中：(readyops & selectionkey.op_connect) != 0，这里就是判断channel是否对connect事件感兴趣。\n * 用|操作向事件集合中添加事件：interestops | readinterestop\n * 从事件集合中删除某个事件，是通过先将要删除事件取反~，然后在和事件集合做&操作：ops &= ~selectionkey.op_connect\n\nnetty这种对空间的极致利用思想，很值得我们平时在日常开发中学习~~\n\n----------------------------------------\n\n现在我们已经知道哪些channel现在处于io就绪状态，并且知道了具体哪些类型的io事件已经就绪。\n\n下面就该针对channel上的不同io就绪事件做出相应的处理了\n\n# 3.1.2.1 处理 connect 事件\n\nnetty客户端向服务端发起连接，并向客户端的reactor注册connect事件，当连接建立成功后，客户端的niosocketchannel就会产生connect就绪事件，通过前面内容我们讲的reactor的运行框架，最终流程会走到这里。\n\nif ((readyops & selectionkey.op_connect) != 0) {\n    int ops = k.interestops();\n    ops &= ~selectionkey.op_connect;\n    k.interestops(ops);\n    //触发channelactive事件\n    unsafe.finishconnect();\n}\n\n\n如果io就绪的事件是connect事件，那么就调用对应客户端niosocketchannel中的unsafe操作类中的finishconnect方法处理connect事件。这时会在netty客户端niosocketchannel中的pipeline中传播channelactive事件。\n\n最后需要将op_connect事件从客户端niosocketchannel所关心的事件集合interestops中删除。否则selector会一直通知connect事件就绪\n\n# 3.1.2.2 处理write事件\n\n关于reactor线程处理netty中的write事件的流程，笔者后续会专门用一篇文章来为大家介绍。本文我们重点关注reactor线程的整体运行框架。\n\nif ((readyops & selectionkey.op_write) != 0) {\n    ch.unsafe().forceflush();\n}\n\n\n这里大家只需要记住，op_write事件的注册是由用户来完成的，当socket发送缓冲区已满无法继续写入数据时，用户会向reactor注册op_write事件，等到socket发送缓冲区变得可写时，reactor会收到op_write事件活跃通知，随后在这里调用客户端niosocketchannel中的forceflush方法将剩余数据发送出去。\n\n# 3.1.2.3 处理read事件或者accept事件\n\nif ((readyops & (selectionkey.op_read | selectionkey.op_accept)) != 0 || readyops == 0) {\n    unsafe.read();\n}\n\n\n这里可以看出netty中处理read事件和accept事件都是由对应channel中的unsafe操作类中的read方法处理。\n\n服务端nioserversocketchannel中的read方法处理的是accept事件，客户端niosocketchannel中的read方法处理的是read事件。\n\n> 这里大家只需记住各个io事件在对应channel中的处理入口，后续文章我们会详细分析这些入口函数。\n\n# 3.1.3 从 selector 中移除失效的 selectionkey\n\n//用于及时从selectedkeys中清除失效的selectkey 比如 socketchannel从selector上被用户移除\nprivate boolean needstoselectagain;\n\n//目的是再次进入for循环 移除失效的selectkey(socketchannel可能被用户从selector上移除)\nif (needstoselectagain) {\n    selectagain();\n    selectedkeys = selector.selectedkeys();\n\n    // create the iterator again to avoid concurrentmodificationexception\n    if (selectedkeys.isempty()) {\n        break;\n    } else {\n        i = selectedkeys.iterator();\n    }\n}\n\n\n在前边介绍reactor运行框架的时候，我们看到在每次reactor线程轮询结束，准备处理io就绪事件以及异步任务的时候，都会将needstoselectagain设置为false。\n\n那么这个needstoselectagain究竟是干嘛的？以及为什么我们需要去“select again”呢?\n\n首先我们来看下在什么情况下会将needstoselectagain这个变量设置为true，通过这个设置的过程，我们是否能够从中找到一些线索？\n\n我们知道channel可以将自己注册到selector上，那么当然也可以将自己从selector上取消移除。\n\n在上篇文章中我们也花了大量的篇幅讲解了这个注册的过程，现在我们来看下channel的取消注册。\n\npublic abstract class abstractniochannel extends abstractchannel {\n\n   //channel注册到selector后获得的selectkey\n    volatile selectionkey selectionkey;\n\n    @override\n    protected void doderegister() throws exception {\n        eventloop().cancel(selectionkey());\n    }\n\n    protected selectionkey selectionkey() {\n        assert selectionkey != null;\n        return selectionkey;\n    }\n}\n\n\nchannel取消注册的过程很简单，直接调用niochannel的doderegister方法，channel绑定的reactor会将其从selector中取消并停止监听channel上的io事件。\n\npublic final class nioeventloop extends singlethreadeventloop {\n\n    //记录selector上移除socketchannel的个数 达到256个 则需要将无效的selectkey从selectedkeys集合中清除掉\n    private int cancelledkeys;\n\n    private static final int cleanup_interval = 256;\n\n    /**\n     * 将socketchannel从selector中移除 取消监听io事件\n     * */\n    void cancel(selectionkey key) {\n        key.cancel();\n        cancelledkeys ++;\n        // 当从selector中移除的socketchannel数量达到256个，设置needstoselectagain为true\n        // 在io.netty.channel.nio.nioeventloop.processselectedkeysplain 中重新做一次轮询，将失效的selectkey移除，\n        // 以保证selectkeyset的有效性\n        if (cancelledkeys >= cleanup_interval) {\n            cancelledkeys = 0;\n            needstoselectagain = true;\n        }\n    }\n}\n\n\n * 调用jdk nio selectionkey的api cancel方法，将channel从selector中取消掉。selectionkey#cancel方法调用完毕后，此时调用selectionkey#isvalid将会返回false。selectionkey#cancel方法调用后，selector会将要取消的这个selectionkey加入到selector中的cancelledkeys集合中。\n\npublic abstract class abstractselector extends selector {\n\n    private final set<selectionkey> cancelledkeys = new hashset<selectionkey>();\n\n    void cancel(selectionkey k) {                      \n        synchronized (cancelledkeys) {\n            cancelledkeys.add(k);\n        }\n    }\n}\n\n\n * 当channel对应的selectionkey取消完毕后，channel取消计数器cancelledkeys会加1，当cancelledkeys = 256时，将needstoselectagain设置为true。\n * 随后在selector的**下一次轮询过程中，会将cancelledkeys集合中的selectionkey从selector中所有的keyset中移除**。这里的keyset包括selector用于存放就绪selectionkey的selectedkeys集合，以及用于存放所有注册的channel对应的selectionkey的keys集合。\n\npublic abstract class selectorimpl extends abstractselector {\n\n    protected set<selectionkey> selectedkeys = new hashset();\n    protected hashset<selectionkey> keys = new hashset();\n    \n     .....................省略...............\n}\n\n\n----------------------------------------\n\n我们看到reactor线程中对needstoselectagain的判断是在processselectedkeysplain方法处理io就绪的selectionkey的循环体中进行判断的。\n\n之所以这里特别提到needstoselectagain判断的位置，是要让大家注意到此时reactor正在处理本次轮询的io就绪事件。\n\n而前边也说了，当调用selectionkey#cancel方法后，需要等到下次轮询的过程中selector才会将这些取消的selectionkey从selector中的所有keyset集合中移除，当然这里也包括就绪集合selectedkeys。\n\n当在本次轮询期间，假如大量的channel从selector中取消，selector中的就绪集合selectedkeys中依然会保存这些channel对应selectionkey直到下次轮询。那么当然会影响本次轮询结果selectedkeys的有效性。\n\n所以为了保证selector中所有keyset的有效性，需要在channel取消个数达到256时，触发一次selectnow，目的是清除无效的selectionkey。\n\n    private void selectagain() {\n        needstoselectagain = false;\n        try {\n            selector.selectnow();\n        } catch (throwable t) {\n            logger.warn("failed to update selectionkeys.", t);\n        }\n    }\n\n\n----------------------------------------\n\n到这里，我们就对jdk 原生 selector的处理方式processselectedkeysplain方法就介绍完了，其实 对io就绪事件的处理逻辑都是一样的，在我们理解了processselectedkeysplain方法后，processselectedkeysoptimized方法对io就绪事件的处理，我们理解起来就非常轻松了。\n\n\n# 3.2 processselectedkeysoptimized\n\nnetty默认会采用优化过的selector对io就绪事件的处理。但是处理逻辑是大同小异的。下面我们主要介绍一下这两个方法的不同之处。\n\n    private void processselectedkeysoptimized() {\n        // 在openselector的时候将jdk中selector实现类中得selectedkeys和publicselectkeys字段类型\n        // 由原来的hashset类型替换为 netty优化后的数组实现的selectedselectionkeyset类型\n        for (int i = 0; i < selectedkeys.size; ++i) {\n            final selectionkey k = selectedkeys.keys[i];\n            // 对应迭代器中得remove   selector不会自己清除selectedkey\n            selectedkeys.keys[i] = null;\n\n            final object a = k.attachment();\n\n            if (a instanceof abstractniochannel) {\n                processselectedkey(k, (abstractniochannel) a);\n            } else {\n                @suppresswarnings("unchecked")\n                niotask<selectablechannel> task = (niotask<selectablechannel>) a;\n                processselectedkey(k, task);\n            }\n\n            if (needstoselectagain) {\n\n                selectedkeys.reset(i + 1);\n\n                selectagain();\n                i = -1;\n            }\n        }\n    }\n\n\n * jdk nio 原生 selector存放io就绪的selectionkey的集合为hashset类型的selectedkeys。而netty为了优化对selectedkeys 集合的遍历效率采用了自己实现的selectedselectionkeyset类型，从而用对数组的遍历代替用hashset的迭代器遍历。\n\n * selector会在每次轮询到io就绪事件时，将io就绪的channel对应的selectionkey插入到selectedkeys集合，但是selector只管向selectedkeys集合放入io就绪的selectionkey，当selectionkey被处理完毕后，selector是不会自己主动将其从selectedkeys集合中移除的，典型的管杀不管埋。所以需要netty自己在遍历到io就绪的 selectionkey后，将其删除。\n\n * * 在processselectedkeysplain中是直接将其从迭代器中删除。\n   * 在processselectedkeysoptimized中将其在数组中对应的位置置为null，方便垃圾回收。\n\n * 在最后清除无效的selectionkey时，在processselectedkeysplain中由于采用的是jdk nio 原生的selector，所以只需要执行selectagain就可以，selector会自动清除无效key。但是在processselectedkeysoptimized中由于是netty自己实现的优化类型，所以需要netty自己将selectedselectionkeyset数组中的selectionkey全部清除，最后在执行selectagain。\n\n----------------------------------------\n\n好了，到这里，我们就将reactor线程如何处理io就绪事件的整个过程讲述完了，下面我们就该到了介绍reactor线程如何处理netty框架中的异步任务了。\n\n\n# 4. reactor线程处理异步任务\n\nnetty关于处理异步任务的方法有两个：\n\n * 一个是无超时时间限制的runalltasks()方法。当ioratio设置为100时，reactor线程会先一股脑的处理io就绪事件，然后在一股脑的执行异步任务，并没有时间的限制。\n * 另一个是有超时时间限制的runalltasks(long timeoutnanos)方法。当ioratio != 100时，reactor线程执行异步任务会有时间限制，优先一股脑的处理完io就绪事件统计出执行io任务耗时iotime。根据公式iotime * (100 - ioratio) / ioratio)计算出reactor线程执行异步任务的超时时间。在超时时间限定范围内，执行有限的异步任务。\n\n\n\n下面我们来分别看下这两个执行异步任务的方法处理逻辑：\n\n\n# 4.1 runalltasks()\n\nprotected boolean runalltasks() {\n    assert ineventloop();\n    boolean fetchedall;\n    boolean ranatleastone = false;\n\n    do {\n        //将到达执行时间的定时任务转存到普通任务队列taskqueue中，统一由reactor线程从taskqueue中取出执行\n        fetchedall = fetchfromscheduledtaskqueue();\n        if (runalltasksfrom(taskqueue)) {\n            ranatleastone = true;\n        }\n    } while (!fetchedall); // keep on processing until we fetched all scheduled tasks.\n\n    if (ranatleastone) {\n        lastexecutiontime = scheduledfuturetask.nanotime();\n    }\n    //执行尾部队列任务\n    afterrunningalltasks();\n    return ranatleastone;\n}\n\n\nreactor线程执行异步任务的核心逻辑就是：\n\n * 先将到期的定时任务一股脑的从定时任务队列scheduledtaskqueue中取出并转存到普通任务队列taskqueue中。\n * 由reactor线程统一从普通任务队列taskqueue中取出任务执行。\n * 在reactor线程执行完定时任务和普通任务后，开始执行存储于尾部任务队列tailtasks中的尾部任务。\n\n下面我们来分别看下上述几个核心步骤的实现：\n\n# 4.1.1 fetchfromscheduledtaskqueue\n\n/**\n * 从定时任务队列中取出达到deadline执行时间的定时任务\n * 将定时任务 转存到 普通任务队列taskqueue中，统一由reactor线程从taskqueue中取出执行\n *\n * */\nprivate boolean fetchfromscheduledtaskqueue() {\n    if (scheduledtaskqueue == null || scheduledtaskqueue.isempty()) {\n        return true;\n    }\n    long nanotime = abstractscheduledeventexecutor.nanotime();\n    for (;;) {\n        //从定时任务队列中取出到达执行deadline的定时任务  deadline <= nanotime\n        runnable scheduledtask = pollscheduledtask(nanotime);\n        if (scheduledtask == null) {\n            return true;\n        }\n        if (!taskqueue.offer(scheduledtask)) {\n            // taskqueue没有空间容纳 则在将定时任务重新塞进定时任务队列中等待下次执行\n            scheduledtaskqueue.add((scheduledfuturetask<?>) scheduledtask);\n            return false;\n        }\n    }\n}\n\n\n 1. 获取当前要执行异步任务的时间点nanotime\n\nfinal class scheduledfuturetask<v> extends promisetask<v> implements scheduledfuture<v>, priorityqueuenode {\n    private static final long start_time = system.nanotime();\n\n    static long nanotime() {\n        return system.nanotime() - start_time;\n    }\n}\n\n\n 2. 从定时任务队列中找出deadline <= nanotime的异步任务。也就是说找出所有到期的定时任务。\n\n    protected final runnable pollscheduledtask(long nanotime) {\n        assert ineventloop();\n\n        //从定时队列中取出要执行的定时任务  deadline <= nanotime\n        scheduledfuturetask<?> scheduledtask = peekscheduledtask();\n        if (scheduledtask == null || scheduledtask.deadlinenanos() - nanotime > 0) {\n            return null;\n        }\n        //符合取出条件 则取出\n        scheduledtaskqueue.remove();\n        scheduledtask.setconsumed();\n        return scheduledtask;\n    }\n\n\n 3. 将到期的定时任务插入到普通任务队列taskqueue中，如果taskqueue已经没有空间容纳新的任务，则将定时任务重新塞进定时任务队列中等待下次拉取。\n\n            if (!taskqueue.offer(scheduledtask)) {\n                scheduledtaskqueue.add((scheduledfuturetask<?>) scheduledtask);\n                return false;\n            }\n\n\n 4. fetchfromscheduledtaskqueue方法的返回值为true时表示到期的定时任务已经全部拉取出来并转存到普通任务队列中。返回值为false时表示到期的定时任务只拉取出来一部分，因为这时普通任务队列已经满了，当执行完普通任务时，还需要在进行一次拉取。\n\n当到期的定时任务从定时任务队列中拉取完毕或者当普通任务队列已满时，这时就会停止拉取，开始执行普通任务队列中的异步任务。\n\n# 4.1.2 runalltasksfrom\n\nprotected final boolean runalltasksfrom(queue<runnable> taskqueue) {\n    runnable task = polltaskfrom(taskqueue);\n    if (task == null) {\n        return false;\n    }\n    for (;;) {\n        safeexecute(task);\n        task = polltaskfrom(taskqueue);\n        if (task == null) {\n            return true;\n        }\n    }\n}\n\n\n * 首先runalltasksfrom 方法的返回值表示是否执行了至少一个异步任务。后面会赋值给ranatleastone变量，这个返回值我们后续会用到。\n * 从普通任务队列中拉取异步任务。\n\nprotected static runnable polltaskfrom(queue<runnable> taskqueue) {\n    for (;;) {\n        runnable task = taskqueue.poll();\n        if (task != wakeup_task) {\n            return task;\n        }\n    }\n}\n\n\n * reactor线程执行异步任务。\n\nprotected static void safeexecute(runnable task) {\n    try {\n        task.run();\n    } catch (throwable t) {\n        logger.warn("a task raised an exception. task: {}", task, t);\n    }\n}\n\n\n# 4.1.3 afterrunningalltasks\n\nif (ranatleastone) {\n    lastexecutiontime = scheduledfuturetask.nanotime();\n}\n//执行尾部队列任务\nafterrunningalltasks();\nreturn ranatleastone;\n\n\n如果reactor线程执行了至少一个异步任务，那么设置lastexecutiontime，并将ranatleastone标识返回。这里的ranatleastone标识就是runalltasksfrom方法的返回值。\n\n最后执行收尾任务，也就是执行尾部任务队列中的尾部任务\n\n@override\nprotected void afterrunningalltasks() {\n    runalltasksfrom(tailtasks);\n}\n\n\n\n# 4.2 runalltasks(long timeoutnanos)\n\n\n\n这里在处理异步任务的核心逻辑还是和之前一样的，只不过就是多了对超时时间的控制。\n\nprotected boolean runalltasks(long timeoutnanos) {\n    fetchfromscheduledtaskqueue();\n    runnable task = polltask();\n    if (task == null) {\n        //普通队列中没有任务时  执行队尾队列的任务\n        afterrunningalltasks();\n        return false;\n    }\n\n    //异步任务执行超时deadline\n    final long deadline = timeoutnanos > 0 ? scheduledfuturetask.nanotime() + timeoutnanos : 0;\n    long runtasks = 0;\n    long lastexecutiontime;\n    for (;;) {\n        safeexecute(task);\n        runtasks ++;\n        //每运行64个异步任务 检查一下 是否达到 执行deadline\n        if ((runtasks & 0x3f) == 0) {\n            lastexecutiontime = scheduledfuturetask.nanotime();\n            if (lastexecutiontime >= deadline) {\n                //到达异步任务执行超时deadline，停止执行异步任务\n                break;\n            }\n        }\n\n        task = polltask();\n        if (task == null) {\n            lastexecutiontime = scheduledfuturetask.nanotime();\n            break;\n        }\n    }\n\n    afterrunningalltasks();\n    this.lastexecutiontime = lastexecutiontime;\n    return true;\n}\n\n\n * 首先还是通过fetchfromscheduledtaskqueue 方法从reactor中的定时任务队列中拉取到期的定时任务，转存到普通任务队列中。当普通任务队列已满或者到期定时任务全部拉取完毕时，停止拉取。\n * 将scheduledfuturetask.nanotime() + timeoutnanos作为reactor线程执行异步任务的超时时间点deadline。\n * 由于系统调用system.nanotime()需要一定的系统开销，所以每执行完64个异步任务的时候才会去检查一下执行时间是否到达了deadline。如果到达了执行截止时间deadline则退出停止执行异步任务。如果没有到达deadline则继续从普通任务队列中取出任务循环执行下去。\n\n> 从这个细节又可以看出netty对性能的考量还是相当讲究的\n\n----------------------------------------\n\n流程走到这里，我们就对reactor的整个运行框架以及如何轮询io就绪事件，如何处理io就绪事件，如何执行异步任务的具体实现逻辑就剖析完了。\n\n下面还有一个小小的尾巴，就是netty是如何解决文章开头提到的jdk nio epoll 的空轮询bug的，让我们一起来看下吧~~~\n\n\n# 5. 解决 jdk epoll 空轮询 bug\n\n前边提到，由于jdk nio epoll的空轮询bug存在，这样会导致reactor线程在没有任何事情可做的情况下被意外唤醒，导致cpu空转。\n\n其实netty也没有从根本上解决这个jdk bug，而是选择巧妙的绕过这个bug。\n\n下面我们来看下netty是如何做到的。\n\n\n\nif (rantasks || strategy > 0) {\n    if (selectcnt > min_premature_selector_returns && logger.isdebugenabled()) {\n        logger.debug("selector.select() returned prematurely {} times in a row for selector {}.",\n                     selectcnt - 1, selector);\n    }\n    selectcnt = 0;\n} else if (unexpectedselectorwakeup(selectcnt)) { // unexpected wakeup (unusual case)\n    //既没有io就绪事件，也没有异步任务，reactor线程从selector上被异常唤醒 触发jdk epoll空轮训bug\n    //重新构建selector,selectcnt归零\n    selectcnt = 0;\n}\n\n\n在reactor线程处理完io就绪事件和异步任务后，会检查这次reactor线程被唤醒有没有执行过异步任务和有没有io就绪的channel。\n\n * boolean rantasks 这时候就派上了用场，这个rantasks正是前边我们在讲runalltasks方法时提到的返回值。用来表示是否执行过至少一次异步任务。\n * int strategy 正是jdk nio selector的select方法的返回值，用来表示io就绪的channel个数。\n\n如果rantasks = false 并且 strategy = 0这代表reactor线程本次既没有异步任务执行也没有io就绪的channel需要处理却被意外的唤醒。等于是空转了一圈啥也没干。\n\n这种情况下netty就会认为可能已经触发了jdk nio epoll的空轮询bug\n\n    int selector_auto_rebuild_threshold = systempropertyutil.getint("io.netty.selectorautorebuildthreshold", 512);\n\n    private boolean unexpectedselectorwakeup(int selectcnt) {\n          ..................省略...............\n\n        /**\n         * 走到这里的条件是 既没有io就绪事件，也没有异步任务，reactor线程从selector上被异常唤醒\n         * 这种情况可能是已经触发了jdk epoll的空轮询bug，如果这种情况持续512次 则认为可能已经触发bug，于是重建selector\n         *\n         * */\n        if (selector_auto_rebuild_threshold > 0 &&\n                selectcnt >= selector_auto_rebuild_threshold) {\n            // the selector returned prematurely many times in a row.\n            // rebuild the selector to work around the problem.\n            logger.warn("selector.select() returned prematurely {} times in a row; rebuilding selector {}.",\n                    selectcnt, selector);\n            rebuildselector();\n            return true;\n        }\n        return false;\n    }\n\n\n * 如果reactor这种意外唤醒的次数selectcnt超过了配置的次数selector_auto_rebuild_threshold,那么netty就会认定这种情况可能已经触发了jdk nio epoll空轮询bug，则重建selector(将之前注册的所有channel重新注册到新的selector上并关闭旧的selector)，selectcnt计数归0。\n\n> selector_auto_rebuild_threshold默认为512，可以通过系统变量-d io.netty.selectorautorebuildthreshold指定自定义数值。\n\n * 如果selectcnt小于selector_auto_rebuild_threshold，则返回不做任何处理，selectcnt继续计数。\n\nnetty就这样通过计数reactor被意外唤醒的次数，如果计数selectcnt达到了512次，则通过重建selector 巧妙的绕开了jdk nio epoll空轮询bug。\n\n> 我们在日常开发中也可以借鉴netty这种处理问题的思路，比如在项目开发中，当我们发现我们无法保证彻底的解决一个问题时，或者为了解决这个问题导致我们的投入产出比不高时，我们就该考虑是不是应该换一种思路去绕过这个问题，从而达到同样的效果。*解决问题的最高境界就是不解决它，巧妙的绕过去~~~~~！！*\n\n----------------------------------------\n\n\n# 总结\n\n本文花了大量的篇幅介绍了reactor整体的运行框架，并深入介绍了reactor核心的工作模块的具体实现逻辑。\n\n通过本文的介绍我们知道了reactor如何轮询注册在其上的所有channel上感兴趣的io事件，以及reactor如何去处理io就绪的事件，如何执行netty框架中提交的异步任务和定时任务。\n\n最后介绍了 netty 如何巧妙的绕过 jdk nio epoll 空轮询的 bug ,达到解决问题的目的。\n\n提炼了新的解决问题的思路：解决问题的最高境界就是不解决它，巧妙的绕过去~~~~~！！\n\n好了，本文的内容就到这里了，我们下篇文章见---\n\n\n# 参考资料\n\nhttps://mp.weixin.qq.com/s/g69upk3juqsq6lbwmtitcq',charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"第四课：Netty 如何高效接收网络连接",frontmatter:{title:"第四课：Netty 如何高效接收网络连接",date:"2024-09-19T11:10:03.000Z",permalink:"/pages/be98dc/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/25.%E5%9B%9B%E3%80%81%E6%B7%B1%E5%85%A5%20Netty%20%E6%A0%B8%E5%BF%83/15.%E7%AC%AC%E5%9B%9B%E8%AF%BE%EF%BC%9ANetty%20%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E6%8E%A5%E6%94%B6%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5.html",relativePath:"Netty 系统设计/25.四、深入 Netty 核心/15.第四课：Netty 如何高效接收网络连接.md",key:"v-4be9d482",path:"/pages/be98dc/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:3,title:"前文回顾",slug:"前文回顾",normalizedTitle:"前文回顾",charIndex:600},{level:2,title:"1. Main Reactor 处理 OP_ACCEPT 事件",slug:"_1-main-reactor-处理-op-accept-事件",normalizedTitle:"1. main reactor 处理 op_accept 事件",charIndex:3665},{level:2,title:"2. 接收客户端连接核心流程框架总览",slug:"_2-接收客户端连接核心流程框架总览",normalizedTitle:"2. 接收客户端连接核心流程框架总览",charIndex:5376},{level:2,title:"3. RecvByteBufAllocator 简介",slug:"_3-recvbytebufallocator-简介",normalizedTitle:"3. recvbytebufallocator 简介",charIndex:12093},{level:3,title:"3.1 RecvByteBufAllocator.Handle的获取",slug:"_3-1-recvbytebufallocator-handle的获取",normalizedTitle:"3.1 recvbytebufallocator.handle的获取",charIndex:13323},{level:2,title:"4. 啊哈！！Bug ! !",slug:"_4-啊哈-bug",normalizedTitle:"4. 啊哈！！bug ! !",charIndex:17344},{level:3,title:"4.1 Bug 的修复",slug:"_4-1-bug-的修复",normalizedTitle:"4.1 bug 的修复",charIndex:21203},{level:2,title:"5. doReadMessages 接收客户端连接",slug:"_5-doreadmessages-接收客户端连接",normalizedTitle:"5. doreadmessages 接收客户端连接",charIndex:24029},{level:3,title:"5.1 创建客户端NioSocketChannel",slug:"_5-1-创建客户端niosocketchannel",normalizedTitle:"5.1 创建客户端niosocketchannel",charIndex:26233},{level:3,title:"5.3 对比NioSocketChannel与NioServerSocketChannel的不同",slug:"_5-3-对比niosocketchannel与nioserversocketchannel的不同",normalizedTitle:"5.3 对比niosocketchannel与nioserversocketchannel的不同",charIndex:27428},{level:4,title:"1：Channel的层次不同",slug:"_1-channel的层次不同",normalizedTitle:"1：channel的层次不同",charIndex:27480},{level:4,title:"2：向Reactor注册的IO事件不同",slug:"_2-向reactor注册的io事件不同",normalizedTitle:"2：向reactor注册的io事件不同",charIndex:28031},{level:4,title:"3: 功能属性不同造成继承结构的不同",slug:"_3-功能属性不同造成继承结构的不同",normalizedTitle:"3: 功能属性不同造成继承结构的不同",charIndex:28932},{level:2,title:"6. ChannelRead 事件的响应",slug:"_6-channelread-事件的响应",normalizedTitle:"6. channelread 事件的响应",charIndex:30605},{level:2,title:"7. 向SubReactorGroup中注册NioSocketChannel",slug:"_7-向subreactorgroup中注册niosocketchannel",normalizedTitle:"7. 向subreactorgroup中注册niosocketchannel",charIndex:36921},{level:3,title:"7.1 从Sub Reactor Group中选取一个Sub Reactor进行绑定",slug:"_7-1-从sub-reactor-group中选取一个sub-reactor进行绑定",normalizedTitle:"7.1 从sub reactor group中选取一个sub reactor进行绑定",charIndex:37504},{level:3,title:"7.2 向绑定的Sub Reactor上注册NioSocketChannel",slug:"_7-2-向绑定的sub-reactor上注册niosocketchannel",normalizedTitle:"7.2 向绑定的sub reactor上注册niosocketchannel",charIndex:37870},{level:3,title:"7.3 register0",slug:"_7-3-register0",normalizedTitle:"7.3 register0",charIndex:39637},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:45415},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:45922}],headersStr:"前言 前文回顾 1. Main Reactor 处理 OP_ACCEPT 事件 2. 接收客户端连接核心流程框架总览 3. RecvByteBufAllocator 简介 3.1 RecvByteBufAllocator.Handle的获取 4. 啊哈！！Bug ! ! 4.1 Bug 的修复 5. doReadMessages 接收客户端连接 5.1 创建客户端NioSocketChannel 5.3 对比NioSocketChannel与NioServerSocketChannel的不同 1：Channel的层次不同 2：向Reactor注册的IO事件不同 3: 功能属性不同造成继承结构的不同 6. ChannelRead 事件的响应 7. 向SubReactorGroup中注册NioSocketChannel 7.1 从Sub Reactor Group中选取一个Sub Reactor进行绑定 7.2 向绑定的Sub Reactor上注册NioSocketChannel 7.3 register0 总结 参考资料",content:"# 前言\n\n对于一个高性能网络通讯框架来说，最最重要也是最核心的工作就是如何高效的接收客户端连接，这就好比我们开了一个饭店，那么迎接客人就是饭店最重要的工作，我们要先把客人迎接进来，不能让客人一看人多就走掉，只要客人进来了，哪怕菜做的慢一点也没关系。\n\n本文笔者就来为大家介绍下netty这块最核心的内容，看看netty是如何高效的接收客户端连接的。\n\n下图为笔者在一个月黑风高天空显得那么深邃遥远的夜晚，闲来无事，于是捧起Netty关于如何接收连接这部分源码细细品读的时候，意外的发现了一个影响Netty接收连接吞吐的一个Bug。\n\n\n\n于是笔者就在Github提了一个?Issue#11708，阐述了下这个Bug产生的原因以及导致的结果并和Netty的作者一起讨论了下修复措施。如上图所示。\n\n> Issue#11708：https://github.com/netty/netty/issues/11708\n\n这里先不详细解释这个Issue，也不建议大家现在就打开这个Issue查看，笔者会在本文的介绍中随着源码深入的解读慢慢的为大家一层一层地拨开迷雾。\n\n之所以在文章的开头把这个拎出来，笔者是想让大家带着怀疑，审视，欣赏，崇敬，敬畏的态度来一起品读世界顶级程序员编写的代码。由衷的感谢他们在这一领域做出的贡献。\n\n好了，问题抛出来后，我们就带着这个疑问来开始本文的内容吧~~~\n\n\n\n\n# 前文回顾\n\n按照老规矩，再开始本文的内容之前，我们先来回顾下前边几篇文章的概要内容帮助大家梳理一个框架全貌出来。\n\n> 笔者这里再次想和读者朋友们强调的是本文可以独立观看，并不依赖前边系列文章的内容，只是大家如果对相关细节部分感兴趣的话，可以在阅读完本文之后在去回看相关文章。\n\n在前边的系列文章中，笔者为大家介绍了驱动Netty整个框架运转的核心引擎Reactor的创建，启动，运行的全流程。从现在开始Netty的整个核心框架就开始运转起来开始工作了，本文要介绍的主要内容就是Netty在启动之后要做的第一件事件：监听端口地址，高效接收客户端连接。\n\n在《聊聊Netty那些事儿之从内核角度看IO模型》一文中，我们是从整个网络框架的基石IO模型的角度整体阐述了下Netty的IO线程模型。\n\n而Netty中的Reactor正是IO线程在Netty中的模型定义。Reactor在Netty中是以Group的形式出现的，分为:\n\n * 主Reactor线程组也就是我们在启动代码中配置的EventLoopGroup bossGroup,main reactor group中的reactor主要负责监听客户端连接事件，高效的处理客户端连接。也是本文我们要介绍的重点。\n * 从Reactor线程组也就是我们在启动代码中配置的EventLoopGroup workerGroup，sub reactor group中的reactor主要负责处理客户端连接上的IO事件，以及异步任务的执行。\n\n最后我们得出Netty的整个IO模型如下：\n\n\n\n本文我们讨论的重点就是 MainReactorGroup 的核心工作上图中所示的步骤1，步骤2，步骤3。\n\n在从整体上介绍完Netty的IO模型之后，我们又在?《Reactor在Netty中的实现(创建篇)》中完整的介绍了Netty框架的骨架主从Reactor组的搭建过程，阐述了Reactor是如何被创建出来的，并介绍了它的核心组件如下图所示：\n\n\n\n * thread即为Reactor中的IO线程，主要负责监听IO事件，处理IO任务，执行异步任务。\n * selector则是JDK NIO对操作系统底层IO多路复用技术实现的封装。用于监听IO就绪事件。\n * taskQueue用于保存Reactor需要执行的异步任务，这些异步任务可以由用户在业务线程中向Reactor提交，也可以是Netty框架提交的一些自身核心的任务。\n * scheduledTaskQueue则是保存Reactor中执行的定时任务。代替了原有的时间轮来执行延时任务。\n * tailQueue保存了在Reactor需要执行的一些尾部收尾任务，在普通任务执行完后 Reactor线程会执行尾部任务，比如对Netty 的运行状态做一些统计数据，例如任务循环的耗时、占用物理内存的大小等等\n\n在骨架搭建完毕之后，我们随后又在在?《详细图解Netty Reactor启动全流程》》一文中介绍了本文的主角服务端NioServerSocketChannel的创建，初始化，绑定端口地址，向main reactor注册监听OP_ACCEPT事件的完整过程。\n\n\n\nmain reactor 如何处理 OP_ACCEPT 事件将会是本文的主要内容。\n\n自此Netty框架的main reactor group已经启动完毕，开始准备监听OP_accept事件，当客户端连接上来之后，OP_ACCEPT事件活跃，main reactor开始处理OP_ACCEPT事件接收客户端连接了。\n\n而netty中的IO事件分为：OP_ACCEPT事件，OP_READ事件，OP_WRITE事件和OP_CONNECT事件\n\nnetty对于IO事件的监听和处理统一封装在Reactor模型中，这四个IO事件的处理过程也是我们后续文章中要单独拿出来介绍的，本文我们聚焦OP_ACCEPT事件的处理。\n\n而为了让大家能够对IO事件的处理有一个完整性的认识，笔者写了?《一文聊透Netty核心引擎Reactor的运转架构》这篇文章，在文章中详细介绍了Reactor线程的整体运行框架。\n\n\n\nReactor 线程会在一个死循环中 996 不停的运转，在循环中会不断的轮询监听 Selector 上的IO事件，当 IO 事件活跃后，Reactor从Selector 上被唤醒转去执行 IO 就绪事件的处理，在这个过程中我们引出了上述四种IO事件的处理入口函数。\n\nprivate void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {\n    //获取Channel的底层操作类Unsafe\n    final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();\n    if (!k.isValid()) {\n        ......如果SelectionKey已经失效则关闭对应的Channel......\n    }\n\n    try {\n        //获取IO就绪事件\n        int readyOps = k.readyOps();\n        //处理Connect事件\n        if ((readyOps & SelectionKey.OP_CONNECT) != 0) {\n            int ops = k.interestOps();\n            //移除对Connect事件的监听，否则Selector会一直通知\n            ops &= SelectionKey.OP_CONNECT;\n            k.interestOps(ops);\n            //触发channelActive事件处理Connect事件\n            unsafe.finishConnect();\n        }\n\n        //处理Write事件\n        if ((readyOps & SelectionKey.OP_WRITE) != 0) {\n            ch.unsafe().forceFlush();\n        }\n\n        //处理Read事件或者Accept事件\n        if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {\n            unsafe.read();\n        }\n    } catch (CancelledKeyException ignored) {\n        unsafe.close(unsafe.voidPromise());\n    }\n}\n\n\n本文笔者将会为大家重点介绍OP_ACCEPT事件的处理入口函数unsafe.read()的整个源码实现。\n\n当客户端连接完成三次握手之后，main reactor 中的 selector 产生OP_ACCEPT事件活跃，main reactor 随即被唤醒，来到了OP_ACCEPT事件的处理入口函数开始接收客户端连接\n\n\n# 1. Main Reactor 处理 OP_ACCEPT 事件\n\n\n\n当Main Reactor轮询到NioServerSocketChannel上的OP_ACCEPT事件就绪时，Main Reactor线程就会从JDK Selector上的阻塞轮询APIselector.select(timeoutMillis)调用中返回。转而去处理NioServerSocketChannel上的OP_ACCEPT事件。\n\npublic final class NioEventLoop extends SingleThreadEventLoop {\n\n    private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {\n        final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();\n        ..............省略.................\n\n        try {\n            int readyOps = k.readyOps();\n\n            if ((readyOps & SelectionKey.OP_CONNECT) != 0) {\n               ..............处理OP_CONNECT事件.................\n            }\n\n\n            if ((readyOps & SelectionKey.OP_WRITE) != 0) {\n              ..............处理OP_WRITE事件.................\n            }\n\n\n            if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {\n                //本文重点处理OP_ACCEPT事件\n                unsafe.read();\n            }\n        } catch (CancelledKeyException ignored) {\n            unsafe.close(unsafe.voidPromise());\n        }\n    }\n\n}\n\n\n * 处理IO就绪事件的入口函数processSelectedKey中的参数AbstractNioChannel ch正是Netty服务端NioServerSocketChannel。因为此时的执行线程为main reactor线程，而main reactor上注册的正是netty服务端NioServerSocketChannel负责监听端口地址，接收客户端连接。\n * 通过ch.unsafe()获取到的NioUnsafe操作类正是NioServerSocketChannel中对底层JDK NIO ServerSocketChannel的Unsafe底层操作类。\n\n> Unsafe接口是Netty对Channel底层操作行为的封装，比如NioServerSocketChannel的底层Unsafe操作类干的事情就是绑定端口地址，处理OP_ACCEPT事件。\n\n这里我们看到，Netty将OP_ACCEPT事件处理的入口函数封装在NioServerSocketChannel里的底层操作类Unsafe的read方法中。\n\n\n\n而NioServerSocketChannel中的Unsafe操作类实现类型为NioMessageUnsafe定义在上图继承结构中的AbstractNioMessageChannel父类中\n\n下面我们到NioMessageUnsafe#read方法中来看下Netty对OP_ACCPET事件的具体处理过程：\n\n\n# 2. 接收客户端连接核心流程框架总览\n\n我们还是按照老规矩，先从整体上把整个 OP_ACCEPT 事件的逻辑处理框架提取出来，让大家先总体俯视下流程全貌，然后在针对每个核心点位进行各个击破\n\n\n\nmain reactor 线程是在一个do...while{...}循环 read loop中不断的调用 JDK NIO serverSocketChannel.accept()方法来接收完成三次握手的客户端连接 NioSocketChannel 的，并将接收到的客户端连接 NioSocketChannel 临时保存在List<Object> readBuf集合中，后续会服务端NioServerSocketChannel的pipeline中通过ChannelRead事件来传递，最终会在ServerBootstrapAcceptor这个ChannelHandler中被处理初始化，并将其注册到Sub Reator Group中\n\n这里的read loop循环会被限定只能读取16次，当main reactor从NioServerSocketChannel中读取客户端连接NioSocketChannel的次数达到16次之后，无论此时是否还有客户端连接都不能在继续读取了。\n\n因为我们在?《一文聊透Netty核心引擎Reactor的运转架构》一文中提到，netty 对 reactor 线程压榨的比较狠，要干的事情很多，除了要监听轮询IO就绪事件，处理 IO 就绪事件，还需要执行用户和 netty 框架本身提交的异步任务和定时任务。\n\n所以这里的main reactor线程不能在read loop中无限制的执行下去，因为还需要分配时间去执行异步任务，不能因为无限制的接收客户端连接而耽误了异步任务的执行。所以这里将 read loop 的循环次数限定为16次\n\n如果 main reactor 线程在read loop中读取客户端连接NioSocketChannel的次数已经满了16次，即使此时还有客户端连接未接收，那么main reactor线程也不会再去接收了，而是转去执行异步任务，当异步任务执行完毕后，还会在回来执行剩余接收连接的任务。\n\n\n\nmain reactor线程退出 read loop 循环的条件有两个：\n\n 1. 在限定的16次读取中，已经没有新的客户端连接要接收了。退出循环。\n 2. 从NioServerSocketChannel中读取客户端连接的次数达到了16次，无论此时是否还有客户端连接都需要退出循环。\n\n以上就是Netty在接收客户端连接时的整体核心逻辑，下面笔者将这部分逻辑的核心源码实现框架提取出来，方便大家根据上述核心逻辑与源码中的处理模块对应起来，还是那句话，这里只需要总体把握核心处理流程，不需要读懂每一行代码，笔者会在文章的后边分模块来各个击破它们。\n\npublic abstract class AbstractNioMessageChannel extends AbstractNioChannel {\n\n  private final class NioMessageUnsafe extends AbstractNioUnsafe {\n\n        //存放连接建立后，创建的客户端SocketChannel\n        private final List<Object> readBuf = new ArrayList<Object>();\n\n        @Override\n        public void read() {\n            //必须在Main Reactor线程中执行\n            assert eventLoop().inEventLoop();\n            //注意下面的config和pipeline都是服务端ServerSocketChannel中的\n            final ChannelConfig config = config();\n            final ChannelPipeline pipeline = pipeline();\n            //创建接收数据Buffer分配器（用于分配容量大小合适的byteBuffer用来容纳接收数据）\n            //在接收连接的场景中，这里的allocHandle只是用于控制read loop的循环读取创建连接的次数。\n            final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle();\n            allocHandle.reset(config);\n\n            boolean closed = false;\n            Throwable exception = null;\n            try {\n                try {\n                    do {\n                        //底层调用NioServerSocketChannel->doReadMessages 创建客户端SocketChannel\n                        int localRead = doReadMessages(readBuf);\n\n                        //已无新的连接可接收则退出read loop\n                        if (localRead == 0) {\n                            break;\n                        }\n                        if (localRead < 0) {\n                            closed = true;\n                            break;\n                        }\n                        //统计在当前事件循环中已经读取到得Message数量（创建连接的个数）\n                        allocHandle.incMessagesRead(localRead);\n                    } while (allocHandle.continueReading());//判断是否已经读满16次\n                } catch (Throwable t) {\n                    exception = t;\n                }\n\n                int size = readBuf.size();\n                for (int i = 0; i < size; i ++) {\n                    readPending = false;\n                    //在NioServerSocketChannel对应的pipeline中传播ChannelRead事件\n                    //初始化客户端SocketChannel，并将其绑定到Sub Reactor线程组中的一个Reactor上\n                    pipeline.fireChannelRead(readBuf.get(i));\n                }\n                //清除本次accept 创建的客户端SocketChannel集合\n                readBuf.clear();\n                allocHandle.readComplete();\n                //触发readComplete事件传播\n                pipeline.fireChannelReadComplete();\n                ....................省略............\n            } finally {\n                ....................省略............\n            }\n        }\n    }\n  }\n}\n\n\n这里首先要通过断言assert eventLoop().inEventLoop()确保处理接收客户端连接的线程必须为Main Reactor 线程。\n\n而main reactor中主要注册的是服务端 NioServerSocketChannel，主要负责处理OP_ACCEPT事件，所以当前main reactor线程是在NioServerSocketChannel中执行接收连接的工作。\n\n所以这里我们通过config()获取到的是NioServerSocketChannel的属性配置类NioServerSocketChannelConfig,它是在Reactor的启动阶段被创建出来的。\n\npublic NioServerSocketChannel(ServerSocketChannel channel) {\n    //父类AbstractNioChannel中保存JDK NIO原生ServerSocketChannel以及要监听的事件OP_ACCEPT\n    super(null, channel, SelectionKey.OP_ACCEPT);\n    //DefaultChannelConfig中设置用于Channel接收数据用的buffer->AdaptiveRecvByteBufAllocator\n    config = new NioServerSocketChannelConfig(this, javaChannel().socket());\n}\n\n\n同理这里通过pipeline()获取到的也是NioServerSocketChannel中的pipeline。它会在NioServerSocketChannel向main reactor注册成功之后被初始化。\n\n\n\n前边提到main reactor线程会被限定只能在read loop中向NioServerSocketChannel读取16次客户端连接，所以在开始read loop之前，我们需要创建一个能够保存记录读取次数的对象，在每次read loop循环之后，可以根据这个对象来判断是否结束read loop。\n\n这个对象就是这里的 RecvByteBufAllocator.Handle allocHandle专门用于统计read loop中接收客户端连接的次数，以及判断是否该结束read loop转去执行异步任务。\n\n当这一切准备就绪之后，main reactor 线程就开始在do{....}while(...)循环中接收客户端连接了。\n\n在 read loop中通过调用doReadMessages函数接收完成三次握手的客户端连接，底层会调用到JDK NIO ServerSocketChannel的accept方法，从内核全连接队列中取出客户端连接。\n\n返回值localRead表示接收到了多少客户端连接，客户端连接通过accept方法只会一个一个的接收，所以这里的localRead正常情况下都会返回1，当localRead <= 0时意味着已经没有新的客户端连接可以接收了，本次main reactor接收客户端的任务到这里就结束了，跳出read loop。开始新的一轮IO事件的监听处理。\n\npublic static SocketChannel accept(final ServerSocketChannel serverSocketChannel) throws IOException {\n    try {\n        return AccessController.doPrivileged(new PrivilegedExceptionAction<SocketChannel>() {\n            @Override\n            public SocketChannel run() throws IOException {\n                return serverSocketChannel.accept();\n            }\n        });\n    } catch (PrivilegedActionException e) {\n        throw (IOException) e.getCause();\n    }\n}\n\n\n随后会将接收到的客户端连接占时存放到List<Object> readBuf集合中。\n\nprivate final class NioMessageUnsafe extends AbstractNioUnsafe {\n\n    //存放连接建立后，创建的客户端SocketChannel\n    private final List<Object> readBuf = new ArrayList<Object>();\n}\n\n\n调用allocHandle.incMessagesRead统计本次事件循环中接收到的客户端连接个数，最后在read loop末尾通过allocHandle.continueReading判断是否达到了限定的16次。从而决定main reactor线程是继续接收客户端连接还是转去执行异步任务。\n\nmain reactor线程退出read loop的两个条件：\n\n 1. 在限定的16次读取中，已经没有新的客户端连接要接收了。退出循环。\n 2. 从NioServerSocketChannel中读取客户端连接的次数达到了16次，无论此时是否还有客户端连接都需要退出循环。\n\n当满足以上两个退出条件时，main reactor线程就会退出read loop，由于在read loop中接收到的客户端连接全部暂存在List<Object> readBuf集合中,随后开始遍历readBuf，在NioServerSocketChannel的pipeline中传播ChannelRead事件。\n\nint size = readBuf.size();\nfor (int i = 0; i < size; i ++) {\n    readPending = false;\n    //NioServerSocketChannel对应的pipeline中传播read事件\n    //io.netty.bootstrap.ServerBootstrap.ServerBootstrapAcceptor.channelRead\n    //初始化客户端SocketChannel，并将其绑定到Sub Reactor线程组中的一个Reactor上\n    pipeline.fireChannelRead(readBuf.get(i));\n}\n\n\n最终pipeline中的ChannelHandler(ServerBootstrapAcceptor)会响应ChannelRead事件，并在相应回调函数中初始化客户端NioSocketChannel，并将其注册到Sub Reactor Group中。此后客户端NioSocketChannel绑定到的sub reactor就开始监听处理客户端连接上的读写事件了。\n\nNetty整个接收客户端的逻辑过程如下图步骤1，2，3所示。\n\n\n\n以上内容就是笔者提取出来的整体流程框架，下面我们来将其中涉及到的重要核心模块拆开，一个一个详细解读下。\n\n\n# 3. RecvByteBufAllocator 简介\n\nReactor 在处理对应 Channel 上的 IO 数据时，都会采用一个ByteBuffer来接收Channel上的IO数据。而本小节要介绍的RecvByteBufAllocator 正是用来分配 ByteBuffer 的一个分配器。\n\n还记得这个RecvByteBufAllocator在哪里被创建的吗？？\n\n在?《聊聊Netty那些事儿之Reactor在Netty中的实现(创建篇)》一文中，在介绍NioServerSocketChannel的创建过程中提到，对应Channel的配置类NioServerSocketChannelConfig也会随着NioServerSocketChannel的创建而创建。\n\npublic NioServerSocketChannel(ServerSocketChannel channel) {\n    super(null, channel, SelectionKey.OP_ACCEPT);\n    config = new NioServerSocketChannelConfig(this, javaChannel().socket());\n}\n\n\n在创建NioServerSocketChannelConfig的过程中会创建RecvByteBufAllocator。\n\npublic DefaultChannelConfig(Channel channel) {\n    this(channel, new AdaptiveRecvByteBufAllocator());\n}\n\n\n这里我们看到 NioServerSocketChannel 中的RecvByteBufAllocator实际类型为AdaptiveRecvByteBufAllocator，顾名思义，这个类型的RecvByteBufAllocator可以根据Channel上每次到来的IO数据大小来自适应动态调整ByteBuffer的容量。\n\n对于服务端NioServerSocketChannel来说，它上边的IO数据就是客户端的连接，它的长度和类型都是固定的，所以在接收客户端连接的时候并不需要这样的一个ByteBuffer来接收，我们会将接收到的客户端连接存放在List<Object> readBuf集合中\n\n对于客户端NioSocketChannel来说，它上边的IO数据时客户端发送来的网络数据，长度是不定的，所以才会需要这样一个可以根据每次IO数据的大小来自适应动态调整容量的ByteBuffer来接收。\n\n那么看起来这个RecvByteBufAllocator和本文的主题不是很关联，因为在接收连接的过程中并不会怎么用到它，这个类笔者还会在后面的文章中详细介绍，之所以这里把它拎出来单独介绍是因为它和本文开头提到的Bug有关系，这个Bug就是由这个类引起的。\n\n\n# 3.1 RecvByteBufAllocator.Handle的获取\n\n在本文中，我们是通过NioServerSocketChannel中的unsafe底层操作类来获取RecvByteBufAllocator.Handle的\n\nfinal RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle();\nprotected abstract class AbstractUnsafe implements Unsafe {\n        @Override\n        public RecvByteBufAllocator.Handle recvBufAllocHandle() {\n            if (recvHandle == null) {\n                recvHandle = config().getRecvByteBufAllocator().newHandle();\n            }\n            return recvHandle;\n        }\n}\n\n\n我们看到最终会在NioServerSocketChannel的配置类NioServerSocketChannelConfig中获取到AdaptiveRecvByteBufAllocator\n\npublic class DefaultChannelConfig implements ChannelConfig {\n    //用于Channel接收数据用的buffer分配器  类型为AdaptiveRecvByteBufAllocator\n    private volatile RecvByteBufAllocator rcvBufAllocator;\n}\n\n\nAdaptiveRecvByteBufAllocator中会创建自适应动态调整容量的ByteBuffer分配器。\n\npublic class AdaptiveRecvByteBufAllocator extends DefaultMaxMessagesRecvByteBufAllocator {\n\n    @Override\n    public Handle newHandle() {\n        return new HandleImpl(minIndex, maxIndex, initial);\n    }\n    \n    private final class HandleImpl extends MaxMessageHandle {\n                  .................省略................\n    }\n}\n\n\n这里的newHandle方法返回的具体类型为MaxMessageHandle，这个MaxMessageHandle里边保存了每次从Channel中读取IO数据的容量指标，方便下次读取时分配合适大小的buffer。\n\n每次在使用allocHandle前需要调用allocHandle.reset(config);重置里边的统计指标。\n\n    public abstract class MaxMessageHandle implements ExtendedHandle {\n        private ChannelConfig config;\n        //每次事件轮询时，最多读取16次\n        private int maxMessagePerRead;\n        //本次事件轮询总共读取的message数,这里指的是接收连接的数量\n        private int totalMessages;\n        //本次事件轮询总共读取的字节数\n        private int totalBytesRead;\n\n       @Override\n        public void reset(ChannelConfig config) {\n            this.config = config;\n            //默认每次最多读取16次\n            maxMessagePerRead = maxMessagesPerRead();\n            totalMessages = totalBytesRead = 0;\n        }\n    }\n\n\n * maxMessagePerRead：用于控制每次read loop里最大可以循环读取的次数，默认为16次，可在启动配置类ServerBootstrap中通过ChannelOption.MAX_MESSAGES_PER_READ选项设置。\n\nServerBootstrap b = new ServerBootstrap();\nb.group(bossGroup, workerGroup)\n  .channel(NioServerSocketChannel.class)\n  .option(ChannelOption.MAX_MESSAGES_PER_READ, 自定义次数)\n\n\n * totalMessages：用于统计read loop中总共接收的连接个数，每次read loop循环后会调用allocHandle.incMessagesRead增加记录接收到的连接个数。\n\n        @Override\n        public final void incMessagesRead(int amt) {\n            totalMessages += amt;\n        }\n\n\n * totalBytesRead：用于统计在read loop中总共接收到客户端连接上的数据大小，这个字段主要用于sub reactor在接收客户端NioSocketChannel上的网络数据用的，本文我们介绍的是main reactor接收客户端连接，所以这里并不会用到这个字段。这个字段会在sub reactor每次读取完NioSocketChannel上的网络数据时增加记录。\n\n        @Override\n        public void lastBytesRead(int bytes) {\n            lastBytesRead = bytes;\n            if (bytes > 0) {\n                totalBytesRead += bytes;\n            }\n        }\n\n\nMaxMessageHandler中还有一个非常重要的方法就是在每次read loop末尾会调用allocHandle.continueReading()方法来判断读取连接次数是否已满16次，来决定main reactor线程是否退出循环。\n\n                  do {\n                        //底层调用NioServerSocketChannel->doReadMessages 创建客户端SocketChannel\n                        int localRead = doReadMessages(readBuf);\n                        if (localRead == 0) {\n                            break;\n                        }\n                        if (localRead < 0) {\n                            closed = true;\n                            break;\n                        }\n                        //统计在当前事件循环中已经读取到得Message数量（创建连接的个数）\n                        allocHandle.incMessagesRead(localRead);\n                    } while (allocHandle.continueReading());\n\n\nimage.png\n\n红框中圈出来的两个判断条件和本文主题无关，我们这里不需要关注，笔者会在后面的文章详细介绍。\n\n * totalMessages < maxMessagePerRead：在本文的接收客户端连接场景中，这个条件用于判断main reactor线程在read loop中的读取次数是否超过了16次。如果超过16次就会返回false，main reactor线程退出循环。\n * totalBytesRead > 0：用于判断当客户端NioSocketChannel上的OP_READ事件活跃时，sub reactor线程在read loop中是否读取到了网络数据。\n\n以上内容就是RecvByteBufAllocator.Handle在接收客户端连接场景下的作用，大家这里仔细看下这个allocHandle.continueReading()方法退出循环的判断条件，再结合整个do{....}while(...)接收连接循环体，感受下是否哪里有些不对劲？Bug即将出现~~~\n\nimage.png\n\n\n# 4. 啊哈！！Bug ! !\n\nimage.png\n\nnetty不论是在本文中处理接收客户端连接的场景还是在处理接收客户端连接上的网络数据场景都会在一个do{....}while(...)循环read loop中不断的处理。\n\n同时也都会利用在上一小节中介绍的RecvByteBufAllocator.Handle来记录每次read loop接收到的连接个数和从连接上读取到的网络数据大小。\n\n从而在read loop的末尾都会通过allocHandle.continueReading()方法判断是否应该退出read loop循环结束连接的接收流程或者是结束连接上数据的读取流程。\n\n无论是用于接收客户端连接的main reactor也好还是用于接收客户端连接上的网络数据的sub reactor也好，它们的运行框架都是一样的，只不过是具体分工不同。\n\n所以netty这里想用统一的RecvByteBufAllocator.Handle来处理以上两种场景。\n\n而RecvByteBufAllocator.Handle中的totalBytesRead字段主要记录sub reactor线程在处理客户端NioSocketChannel中OP_READ事件活跃时，总共在read loop中读取到的网络数据，而这里是main reactor线程在接收客户端连接所以这个字段并不会被设置。totalBytesRead字段的值在本文中永远会是0。\n\n所以无论同时有多少个客户端并发连接到服务端上，在接收连接的这个read loop中永远只会接受一个连接就会退出循环，因为allocHandle.continueReading()方法中的判断条件totalBytesRead > 0永远会返回false。\n\n                  do {\n                        //底层调用NioServerSocketChannel->doReadMessages 创建客户端SocketChannel\n                        int localRead = doReadMessages(readBuf);\n                        if (localRead == 0) {\n                            break;\n                        }\n                        if (localRead < 0) {\n                            closed = true;\n                            break;\n                        }\n                        //统计在当前事件循环中已经读取到得Message数量（创建连接的个数）\n                        allocHandle.incMessagesRead(localRead);\n                    } while (allocHandle.continueReading());\n\n\n而netty的本意是在这个read loop循环中尽可能多的去接收客户端的并发连接，同时又不影响main reactor线程执行异步任务。但是由于这个Bug，main reactor在这个循环中只执行一次就结束了。这也一定程度上就影响了netty的吞吐。\n\n让我们想象下这样的一个场景，当有16个客户端同时并发连接到了服务端，这时NioServerSocketChannel上的OP_ACCEPT事件活跃，main reactor从Selector上被唤醒，随后执行OP_ACCEPT事件的处理。\n\npublic final class NioEventLoop extends SingleThreadEventLoop {\n    @Override\n    protected void run() {\n        int selectCnt = 0;\n        for (;;) {\n            try { \n                int strategy;\n                try {\n                    strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks());\n                    switch (strategy) {\n                    case SelectStrategy.CONTINUE:                  \n                          ............省略.........\n                    case SelectStrategy.BUSY_WAIT:\n\n                          ............省略.........\n                    case SelectStrategy.SELECT:\n                            ............监听轮询IO事件.........\n                    default:\n                    }\n                } catch (IOException e) {\n                    ............省略.........\n                }\n\n                ............处理IO就绪事件.........\n                ............执行异步任务.........\n    }\n}\n\n\n但是由于这个Bug的存在，main reactor在接收客户端连接的这个read loop中只接收了一个客户端连接就匆匆返回了。\n\n      private final class NioMessageUnsafe extends AbstractNioUnsafe {\n                    do {\n                        int localRead = doReadMessages(readBuf);\n                        .........省略...........\n                    } while (allocHandle.continueReading());\n     }\n\n\n然后根据下图中这个Reactor的运行结构去执行异步任务，随后绕一大圈又会回到NioEventLoop#run方法中重新发起一轮OP_ACCEPT事件轮询。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)Reactor线程运行时结构.png\n\n由于现在还有15个客户端并发连接没有被接收，所以此时Main Reactor线程并不会在selector.select()上阻塞，最终绕一圈又会回到NioMessageUnsafe#read方法的do{.....}while()循环。在接收一个连接之后又退出循环。\n\n本来我们可以在一次read loop中把这16个并发的客户端连接全部接收完毕的，因为这个Bug，main reactor需要不断的发起OP_ACCEPT事件的轮询，绕了很大一个圈子。同时也增加了许多不必要的selector.select()系统调用开销\n\nissue讨论.png\n\n这时大家在看这个?Issue#11708中的讨论是不是就清晰很多了~~\n\n> Issue#11708：https://github.com/netty/netty/issues/11708\n\n\n# 4.1 Bug 的修复\n\n> 笔者在写这篇文章的时候，Netty最新版本是4.1.68.final，这个Bug在4.1.69.final中被修复。\n\nimage.png\n\n由于该Bug产生的原因正是因为服务端NioServerSocketChannel（用于监听端口地址和接收客户端连接）和 客户端NioSocketChannel（用于通信）中的Config配置类混用了同一个ByteBuffer分配器AdaptiveRecvByteBufAllocator而导致的。\n\n所以在新版本修复中专门为服务端ServerSocketChannel中的Config配置类引入了一个新的ByteBuffer分配器ServerChannelRecvByteBufAllocator，专门用于服务端ServerSocketChannel接收客户端连接的场景。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n在ServerChannelRecvByteBufAllocator的父类DefaultMaxMessagesRecvByteBufAllocator中引入了一个新的字段ignoreBytesRead，用于表示是否忽略网络字节的读取，在创建服务端Channel配置类NioServerSocketChannelConfig的时候，这个字段会被赋值为true。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n当main reactor线程在read loop循环中接收客户端连接的时候。\n\n      private final class NioMessageUnsafe extends AbstractNioUnsafe {\n\n                    do {\n                        int localRead = doReadMessages(readBuf);\n                        .........省略...........\n                    } while (allocHandle.continueReading());\n     }\n\n\n在read loop循环的末尾就会采用从ServerChannelRecvByteBufAllocator中创建的MaxMessageHandle#continueReading方法来判断读取连接次数是否超过了16次。由于这里的ignoreBytesRead == true这回我们就会忽略totalBytesRead == 0的情况，从而使得接收连接的read loop得以继续地执行下去。在一个read loop中一次性把16个连接全部接收完毕。\n\nimage.png\n\n以上就是对这个Bug产生的原因，以及发现的过程，最后修复的方案一个全面的介绍，因此笔者也出现在了netty 4.1.69.final版本发布公告里的thank-list中。哈哈，真是令人开心的一件事情~~~\n\nimage.png\n\n通过以上对netty接收客户端连接的全流程分析和对这个Bug来龙去脉以及修复方案的介绍，大家现在一定已经理解了整个接收连接的流程框架。\n\n接下来笔者就把这个流程中涉及到的一些核心模块在单独拎出来从细节入手，为大家各个击破~~~\n\n\n# 5. doReadMessages 接收客户端连接\n\npublic class NioServerSocketChannel extends AbstractNioMessageChannel\n                             implements io.netty.channel.socket.ServerSocketChannel {\n\n    @Override\n    protected int doReadMessages(List<Object> buf) throws Exception {\n        SocketChannel ch = SocketUtils.accept(javaChannel());\n\n        try {\n            if (ch != null) {\n                buf.add(new NioSocketChannel(this, ch));\n                return 1;\n            }\n        } catch (Throwable t) {\n            logger.warn(\"Failed to create a new channel from an accepted socket.\", t);\n\n            try {\n                ch.close();\n            } catch (Throwable t2) {\n                logger.warn(\"Failed to close a socket.\", t2);\n            }\n        }\n\n        return 0;\n    }\n\n}\n\n\n * 通过javaChannel()获取封装在Netty服务端NioServerSocketChannel中的JDK 原生 ServerSocketChannel。\n\n@Override\nprotected ServerSocketChannel javaChannel() {\n    return (ServerSocketChannel) super.javaChannel();\n}\n\n\n * 通过JDK NIO 原生的ServerSocketChannel的accept方法获取JDK NIO 原生客户端连接SocketChannel。\n\npublic static SocketChannel accept(final ServerSocketChannel serverSocketChannel) throws IOException {\n    try {\n        return AccessController.doPrivileged(new PrivilegedExceptionAction<SocketChannel>() {\n            @Override\n            public SocketChannel run() throws IOException {\n                return serverSocketChannel.accept();\n            }\n        });\n    } catch (PrivilegedActionException e) {\n        throw (IOException) e.getCause();\n    }\n}\n\n\n这一步就是我们在?《聊聊Netty那些事儿之从内核角度看IO模型》介绍到的调用监听Socket的accept方法，内核会基于监听Socket创建出来一个新的Socket专门用于与客户端之间的网络通信这个我们称之为客户端连接Socket。这里的ServerSocketChannel就类似于监听Socket。SocketChannel就类似于客户端连接Socket。\n\n由于我们在创建NioServerSocketChannel的时候，会将JDK NIO 原生的ServerSocketChannel设置为非阻塞，所以这里当ServerSocketChannel上有客户端连接时就会直接创建SocketChannel，如果此时并没有客户端连接时accept调用就会立刻返回null并不会阻塞。\n\nprotected AbstractNioChannel(Channel parent, SelectableChannel ch, int readInterestOp) {\n    super(parent);\n    this.ch = ch;\n    this.readInterestOp = readInterestOp;\n    try {\n        //设置Channel为非阻塞 配合IO多路复用模型\n        ch.configureBlocking(false);\n    } catch (IOException e) {\n        ..........省略.............\n    }\n}\n\n\n\n# 5.1 创建客户端NioSocketChannel\n\npublic class NioServerSocketChannel extends AbstractNioMessageChannel\n                             implements io.netty.channel.socket.ServerSocketChannel {\n\n    @Override\n    protected int doReadMessages(List<Object> buf) throws Exception {\n        SocketChannel ch = SocketUtils.accept(javaChannel());\n\n        try {\n            if (ch != null) {\n                buf.add(new NioSocketChannel(this, ch));\n                return 1;\n            }\n        } catch (Throwable t) {\n          .........省略.......\n        }\n\n        return 0;\n    }\n\n}\n\n\n这里会根据ServerSocketChannel的accept方法获取到JDK NIO 原生的SocketChannel（用于底层真正与客户端通信的Channel），来创建Netty中的NioSocketChannel。\n\npublic class NioSocketChannel extends AbstractNioByteChannel implements io.netty.channel.socket.SocketChannel {\n\n    public NioSocketChannel(Channel parent, SocketChannel socket) {\n        super(parent, socket);\n        config = new NioSocketChannelConfig(this, socket.socket());\n    }\n\n}\n\n\n创建客户端NioSocketChannel的过程其实和之前讲的创建服务端NioServerSocketChannel大体流程是一样的，我们这里只对客户端NioSocketChannel和服务端NioServerSocketChannel在创建过程中的不同之处做一个对比。\n\n> 具体细节部分大家可以在回看下?《详细图解Netty Reactor启动全流程》一文中关于NioServerSocketChannel的创建的详细细节。\n\n\n# 5.3 对比NioSocketChannel与NioServerSocketChannel的不同\n\n# 1：Channel的层次不同\n\n在我们介绍Reactor的创建文章中，我们提到Netty中的Channel是具有层次的。由于客户端NioSocketChannel是在main reactor接收连接时在服务端NioServerSocketChannel中被创建的，所以在创建客户端NioSocketChannel的时候会通过构造函数指定了parent属性为NioServerSocketChanel。并将JDK NIO 原生的SocketChannel封装进Netty的客户端NioSocketChannel中。\n\n而在Reactor启动过程中创建NioServerSocketChannel的时候parent属性指定是null。因为它就是顶层的Channel，负责创建客户端NioSocketChannel。\n\npublic NioServerSocketChannel(ServerSocketChannel channel) {\n    super(null, channel, SelectionKey.OP_ACCEPT);\n    config = new NioServerSocketChannelConfig(this, javaChannel().socket());\n}\n\n\n# 2：向Reactor注册的IO事件不同\n\n客户端 NioSocketChannel 向 Sub Reactor 注册的是 SelectionKey.OP_READ事件，而服务端 NioServerSocketChannel 向 Main Reactor 注册的是SelectionKey.OP_ACCEPT事件。\n\npublic abstract class AbstractNioByteChannel extends AbstractNioChannel {\n\n    protected AbstractNioByteChannel(Channel parent, SelectableChannel ch) {\n        super(parent, ch, SelectionKey.OP_READ);\n    }\n\n}\n\npublic class NioServerSocketChannel extends AbstractNioMessageChannel\n                             implements io.netty.channel.socket.ServerSocketChannel {\n\n   public NioServerSocketChannel(ServerSocketChannel channel) {\n        //父类AbstractNioChannel中保存JDK NIO原生ServerSocketChannel以及要监听的事件OP_ACCEPT\n        super(null, channel, SelectionKey.OP_ACCEPT);\n        //DefaultChannelConfig中设置用于Channel接收数据用的buffer->AdaptiveRecvByteBufAllocator\n        config = new NioServerSocketChannelConfig(this, javaChannel().socket());\n    }\n}\n\n\n# 3: 功能属性不同造成继承结构的不同\n\n\n\n客户端NioSocketChannel继承的是AbstractNioByteChannel，而服务端NioServerSocketChannel继承的是AbstractNioMessageChannel。它们继承的这两个抽象类一个前缀是Byte，一个前缀是Message有什么区别吗？？\n\n> 客户端NioSocketChannel主要处理的是服务端与客户端的通信，这里涉及到接收客户端发送来的数据，而Sub Reactor线程从NioSocketChannel中读取的正是网络通信数据单位为Byte。\n\n> 服务端NioServerSocketChannel主要负责处理OP_ACCEPT事件，创建用于通信的客户端NioSocketChannel。这时候客户端与服务端还没开始通信，所以Main Reactor线程从NioServerSocketChannel的读取对象为Message。这里的Message指的就是底层的SocketChannel客户端连接。\n\n----------------------------------------\n\n以上就是NioSocketChannel与NioServerSocketChannel创建过程中的不同之处，后面的过程就一样了。\n\n * 在AbstractNioChannel 类中封装JDK NIO 原生的SocketChannel，并将其底层的IO模型设置为非阻塞，保存需要监听的IO事件OP_READ。\n\nprotected AbstractNioChannel(Channel parent, SelectableChannel ch, int readInterestOp) {\n    super(parent);\n    this.ch = ch;\n    this.readInterestOp = readInterestOp;\n    try {\n        //设置Channel为非阻塞 配合IO多路复用模型\n        ch.configureBlocking(false);\n    } catch (IOException e) {\n\n    }\n}\n\n\n * 为客户端NioSocketChannel创建全局唯一的channelId，创建客户端NioSocketChannel的底层操作类NioByteUnsafe，创建pipeline。\n\nprotected AbstractChannel(Channel parent) {\n    this.parent = parent;\n    //channel全局唯一ID machineId+processId+sequence+timestamp+random\n    id = newId();\n    //unsafe用于底层socket的读写操作\n    unsafe = newUnsafe();\n    //为channel分配独立的pipeline用于IO事件编排\n    pipeline = newChannelPipeline();\n}\n\n\n * 在NioSocketChannelConfig的创建过程中，将NioSocketChannel的RecvByteBufAllocator类型设置为AdaptiveRecvByteBufAllocator。\n\npublic DefaultChannelConfig(Channel channel) {\n    this(channel, new AdaptiveRecvByteBufAllocator());\n}\n\n\n> 在Bug修复后的版本中服务端NioServerSocketChannel的RecvByteBufAllocator类型设置为ServerChannelRecvByteBufAllocator\n\n最终我们得到的客户端NioSocketChannel结构如下：\n\n\n\n\n# 6. ChannelRead 事件的响应\n\n\n\n在前边介绍接收连接的整体核心流程框架的时候，我们提到main reactor线程是在一个do{.....}while(...)循环read loop中不断的调用ServerSocketChannel#accept方法来接收客户端的连接。\n\n当满足退出read loop循环的条件有两个：\n\n 1. 在限定的16次读取中，已经没有新的客户端连接要接收了。退出循环。\n 2. 从NioServerSocketChannel中读取客户端连接的次数达到了16次，无论此时是否还有客户端连接都需要退出循环。\n\nmain reactor就会退出read loop循环，此时接收到的客户端连接NioSocketChannel暂存与List<Object> readBuf集合中。\n\nprivate final class NioMessageUnsafe extends AbstractNioUnsafe {\n\n    private final List<Object> readBuf = new ArrayList<Object>();\n\n    @Override\n    public void read() {\n        try {\n            try {\n                do {\n                    ........省略.........\n                        //底层调用NioServerSocketChannel->doReadMessages 创建客户端SocketChannel\n                        int localRead = doReadMessages(readBuf);\n                    ........省略.........\n                        allocHandle.incMessagesRead(localRead);\n                } while (allocHandle.continueReading());\n\n            } catch (Throwable t) {\n                exception = t;\n            }\n\n            int size = readBuf.size();\n            for (int i = 0; i < size; i ++) {\n                readPending = false;\n                pipeline.fireChannelRead(readBuf.get(i));\n            }\n\n            ........省略.........\n        } finally {\n            ........省略.........\n        }\n    }\n}\n\n\n随后main reactor线程会遍历List<Object> readBuf集合中的NioSocketChannel，并在NioServerSocketChannel的pipeline中传播ChannelRead事件。\n\n\n\n最终ChannelRead事件会传播到ServerBootstrapAcceptor中，这里正是Netty处理客户端连接的核心逻辑所在。\n\nServerBootstrapAcceptor主要的作用就是初始化客户端NioSocketChannel，并将客户端NioSocketChannel注册到Sub Reactor Group中，并监听OP_READ事件。\n\n在ServerBootstrapAcceptor 中会初始化客户端NioSocketChannel的这些属性。\n\n比如：从Reactor组EventLoopGroup childGroup，用于初始化NioSocketChannel中的pipeline用到的ChannelHandler childHandler，以及NioSocketChannel中的一些childOptions和childAttrs。\n\nprivate static class ServerBootstrapAcceptor extends ChannelInboundHandlerAdapter {\n\n        private final EventLoopGroup childGroup;\n        private final ChannelHandler childHandler;\n        private final Entry<ChannelOption<?>, Object>[] childOptions;\n        private final Entry<AttributeKey<?>, Object>[] childAttrs;\n\n        @Override\n        @SuppressWarnings(\"unchecked\")\n        public void channelRead(ChannelHandlerContext ctx, Object msg) {\n            final Channel child = (Channel) msg;\n\n            //向客户端NioSocketChannel的pipeline中\n            //添加在启动配置类ServerBootstrap中配置的ChannelHandler\n            child.pipeline().addLast(childHandler);\n\n            //利用配置的属性初始化客户端NioSocketChannel\n            setChannelOptions(child, childOptions, logger);\n            setAttributes(child, childAttrs);\n\n            try {\n                /**\n                 * 1：在Sub Reactor线程组中选择一个Reactor绑定\n                 * 2：将客户端SocketChannel注册到绑定的Reactor上\n                 * 3：SocketChannel注册到sub reactor中的selector上，并监听OP_READ事件\n                 * */\n                childGroup.register(child).addListener(new ChannelFutureListener() {\n                    @Override\n                    public void operationComplete(ChannelFuture future) throws Exception {\n                        if (!future.isSuccess()) {\n                            forceClose(child, future.cause());\n                        }\n                    }\n                });\n            } catch (Throwable t) {\n                forceClose(child, t);\n            }\n        }\n}\n\n\n正是在这里，netty会将我们在?《详细图解Netty Reactor启动全流程》的启动示例程序中在ServerBootstrap中配置的客户端NioSocketChannel的所有属性（child前缀配置）初始化到NioSocketChannel中。\n\npublic final class EchoServer {\n    static final int PORT = Integer.parseInt(System.getProperty(\"port\", \"8007\"));\n\n    public static void main(String[] args) throws Exception {\n        // Configure the server.\n        //创建主从Reactor线程组\n        EventLoopGroup bossGroup = new NioEventLoopGroup(1);\n        EventLoopGroup workerGroup = new NioEventLoopGroup();\n        final EchoServerHandler serverHandler = new EchoServerHandler();\n        try {\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)//配置主从Reactor\n             .channel(NioServerSocketChannel.class)//配置主Reactor中的channel类型\n             .option(ChannelOption.SO_BACKLOG, 100)//设置主Reactor中channel的option选项\n             .handler(new LoggingHandler(LogLevel.INFO))//设置主Reactor中Channel->pipline->handler\n             .childHandler(new ChannelInitializer<SocketChannel>() {//设置从Reactor中注册channel的pipeline\n                 @Override\n                 public void initChannel(SocketChannel ch) throws Exception {\n                     ChannelPipeline p = ch.pipeline();\n                     //p.addLast(new LoggingHandler(LogLevel.INFO));\n                     p.addLast(serverHandler);\n                 }\n             });\n\n            // Start the server. 绑定端口启动服务，开始监听accept事件\n            ChannelFuture f = b.bind(PORT).sync();\n            // Wait until the server socket is closed.\n            f.channel().closeFuture().sync();\n        } finally {\n            // Shut down all event loops to terminate all threads.\n            bossGroup.shutdownGracefully();\n            workerGroup.shutdownGracefully();\n        }\n    }\n}\n\n\n以上示例代码中通过ServerBootstrap配置的NioSocketChannel相关属性，会在Netty启动并开始初始化NioServerSocketChannel的时候将ServerBootstrapAcceptor的创建初始化工作封装成异步任务，然后在NioServerSocketChannel注册到Main Reactor中成功后执行。\n\npublic class ServerBootstrap extends AbstractBootstrap<ServerBootstrap, ServerChannel> {\n\n    @Override\n    void init(Channel channel) {\n        ................省略................\n\n        p.addLast(new ChannelInitializer<Channel>() {\n            @Override\n            public void initChannel(final Channel ch) {\n                final ChannelPipeline pipeline = ch.pipeline();\n                ................省略................\n                ch.eventLoop().execute(new Runnable() {\n                    @Override\n                    public void run() {\n                        pipeline.addLast(new ServerBootstrapAcceptor(\n                                ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs));\n                    }\n                });\n            }\n        });\n    }\n}\n\n\n在经过ServerBootstrapAccptor#chanelRead回调的处理之后，此时客户端NioSocketChannel中pipeline的结构为：\n\n客户端channel pipeline初始结构.png\n\n随后会将初始化好的客户端NioSocketChannel向Sub Reactor Group中注册，并监听OP_READ事件。\n\n如下图中的步骤3所示：\n\nnetty中的reactor.png\n\n\n# 7. 向SubReactorGroup中注册NioSocketChannel\n\nchildGroup.register(child).addListener(new ChannelFutureListener() {\n    @Override\n    public void operationComplete(ChannelFuture future) throws Exception {\n        if (!future.isSuccess()) {\n            forceClose(child, future.cause());\n        }\n    }\n});\n\n\n客户端NioSocketChannel向Sub Reactor Group注册的流程完全和服务端NioServerSocketChannel向Main Reactor Group注册流程一样。\n\n> 关于服务端NioServerSocketChannel的注册流程，笔者已经在?《详细图解Netty Reactor启动全流程》一文中做出了详细的介绍，对相关细节感兴趣的同学可以在回看下。\n\n这里笔者在带大家简要回顾下整个注册过程并着重区别对比客户端NioSocetChannel与服务端NioServerSocketChannel注册过程中不同的地方。\n\n\n# 7.1 从Sub Reactor Group中选取一个Sub Reactor进行绑定\n\npublic abstract class MultithreadEventLoopGroup extends MultithreadEventExecutorGroup implements EventLoopGroup {\n\n   @Override\n    public ChannelFuture register(Channel channel) {\n        return next().register(channel);\n    }\n\n    @Override\n    public EventExecutor next() {\n        return chooser.next();\n    }\n\n}\n\n\n\n# 7.2 向绑定的Sub Reactor上注册NioSocketChannel\n\npublic abstract class SingleThreadEventLoop extends SingleThreadEventExecutor implements EventLoop {\n\n    @Override\n    public ChannelFuture register(Channel channel) {\n        //注册channel到绑定的Reactor上\n        return register(new DefaultChannelPromise(channel, this));\n    }\n\n    @Override\n    public ChannelFuture register(final ChannelPromise promise) {\n        ObjectUtil.checkNotNull(promise, \"promise\");\n        //unsafe负责channel底层的各种操作\n        promise.channel().unsafe().register(this, promise);\n        return promise;\n    }\n\n}\n\n\n * 当时我们在介绍NioServerSocketChannel的注册过程时，这里的promise.channel()为NioServerSocketChannel。底层的unsafe操作类为NioMessageUnsafe。\n * 此时这里的promise.channel()为NioSocketChannel。底层的unsafe操作类为NioByteUnsafe。\n\n@Override\npublic final void register(EventLoop eventLoop, final ChannelPromise promise) {\n    ..............省略....................\n        //此时这里的eventLoop为Sub Reactor\n        AbstractChannel.this.eventLoop = eventLoop;\n\n    /**\n      * 执行channel注册的操作必须是Reactor线程来完成\n      *\n      * 1: 如果当前执行线程是Reactor线程，则直接执行register0进行注册\n      * 2：如果当前执行线程是外部线程，则需要将register0注册操作 封装程异步Task 由Reactor线程执行\n      * */\n    if (eventLoop.inEventLoop()) {\n        register0(promise);\n    } else {\n        try {\n            eventLoop.execute(new Runnable() {\n                @Override\n                public void run() {\n                    register0(promise);\n                }\n            });\n        } catch (Throwable t) {\n            ..............省略....................\n        }\n    }\n}\n\n\n注意此时传递进来的EventLoop eventLoop为Sub Reactor。\n\n但此时的执行线程为Main Reactor线程，并不是Sub Reactor线程（此时还未启动）。\n\n所以这里的eventLoop.inEventLoop()返回的是false。\n\nimage.png\n\n在else分支中向绑定的Sub Reactor提交注册NioSocketChannel的任务。\n\n> 当注册任务提交后，此时绑定的Sub Reactor线程启动。\n\n\n# 7.3 register0\n\n我们又来到了Channel注册的老地方register0方法。在?《详细图解Netty Reactor启动全流程》中我们花了大量的篇幅介绍了这个方法。这里我们只对比NioSocketChannel与NioServerSocketChannel不同的地方。\n\nprivate void register0(ChannelPromise promise) {\n    try {\n        ................省略..................\n            boolean firstRegistration = neverRegistered;\n        //执行真正的注册操作\n        doRegister();\n        //修改注册状态\n        neverRegistered = false;\n        registered = true;\n\n        pipeline.invokeHandlerAddedIfNeeded();\n\n        if (isActive()) {\n            if (firstRegistration) {\n                //触发channelActive事件\n                pipeline.fireChannelActive();\n            } else if (config().isAutoRead()) {\n                beginRead();\n            }\n        }\n    } catch (Throwable t) {\n        ................省略..................\n    }\n}\n\n\n这里 doRegister()方法将NioSocketChannel注册到Sub Reactor中的Selector上。\n\npublic abstract class AbstractNioChannel extends AbstractChannel {\n\n    @Override\n    protected void doRegister() throws Exception {\n        boolean selected = false;\n        for (;;) {\n            try {\n                selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this);\n                return;\n            } catch (CancelledKeyException e) {\n                ...............省略...............\n            }\n        }\n    }\n\n}\n\n\n这里是Netty客户端NioSocketChannel与JDK NIO 原生 SocketChannel关联的地方。此时注册的IO事件依然是0。目的也是只是为了获取NioSocketChannel在Selector中的SelectionKey。\n\n同时通过SelectableChannel#register方法将Netty自定义的NioSocketChannel（这里的this指针）附着在SelectionKey的attechment属性上，完成Netty自定义Channel与JDK NIO Channel的关系绑定。这样在每次对Selector进行IO就绪事件轮询时，Netty 都可以从 JDK NIO Selector返回的SelectionKey中获取到自定义的Channel对象（这里指的就是NioSocketChannel）。\n\nchannel与SelectionKey对应关系.png\n\n随后调用pipeline.invokeHandlerAddedIfNeeded()回调客户端NioSocketChannel上pipeline中的所有ChannelHandler的handlerAdded方法，此时pipeline的结构中只有一个ChannelInitializer。最终会在ChannelInitializer#handlerAdded回调方法中初始化客户端NioSocketChannel的pipeline。\n\n客户端channel pipeline初始结构.png\n\npublic abstract class ChannelInitializer<C extends Channel> extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void handlerAdded(ChannelHandlerContext ctx) throws Exception {\n        if (ctx.channel().isRegistered()) {\n            if (initChannel(ctx)) {\n                //初始化工作完成后，需要将自身从pipeline中移除\n                removeState(ctx);\n            }\n        }\n    }\n\n    protected abstract void initChannel(C ch) throws Exception;\n}\n\n\n> 关于对Channel中pipeline的详细初始化过程，对细节部分感兴趣的同学可以回看下?《详细图解Netty Reactor启动全流程》\n\n此时客户端NioSocketChannel中的pipeline中的结构就变为了我们自定义的样子，在示例代码中我们自定义的ChannelHandler为EchoServerHandler。\n\n客户端channel pipeline结构.png\n\n@Sharable\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(ChannelHandlerContext ctx, Object msg) {\n        ctx.write(msg);\n    }\n\n    @Override\n    public void channelReadComplete(ChannelHandlerContext ctx) {\n\n        ctx.flush();\n    }\n\n    @Override\n    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {\n        // Close the connection when an exception is raised.\n        cause.printStackTrace();\n        ctx.close();\n    }\n}\n\n\n当客户端NioSocketChannel中的pipeline初始化完毕后，netty就开始调用safeSetSuccess(promise)方法回调regFuture中注册的ChannelFutureListener，通知客户端NioSocketChannel已经成功注册到Sub Reactor上了。\n\nchildGroup.register(child).addListener(new ChannelFutureListener() {\n    @Override\n    public void operationComplete(ChannelFuture future) throws Exception {\n        if (!future.isSuccess()) {\n            forceClose(child, future.cause());\n        }\n    }\n});\n\n\n> 在服务端NioServerSocketChannel注册的时候我们会在listener中向Main Reactor提交bind绑定端口地址任务。但是在NioSocketChannel注册的时候，只会在listener中处理一下注册失败的情况。\n\n当Sub Reactor线程通知ChannelFutureListener注册成功之后，随后就会调用pipeline.fireChannelRegistered()在客户端NioSocketChannel的pipeline中传播ChannelRegistered事件。\n\n传播ChannelRegister事件.png\n\n这里笔者重点要强调下，在之前介绍NioServerSocketChannel注册的时候，我们提到因为此时NioServerSocketChannel并未绑定端口地址，所以这时的NioServerSocketChannel并未激活，这里的isActive()返回false。register0方法直接返回。\n\n> 服务端NioServerSocketChannel判断是否激活的标准为端口是否绑定成功。\n\npublic class NioServerSocketChannel extends AbstractNioMessageChannel\n                             implements io.netty.channel.socket.ServerSocketChannel {\n    @Override\n    public boolean isActive() {\n        return isOpen() && javaChannel().socket().isBound();\n    }\n}\n\n\n> 客户端NioSocketChannel判断是否激活的标准为是否处于Connected状态。那么显然这里肯定是处于connected状态的。\n\n@Override\npublic boolean isActive() {\n    SocketChannel ch = javaChannel();\n    return ch.isOpen() && ch.isConnected();\n}\n\n\nNioSocketChannel已经处于connected状态，这里并不需要绑定端口，所以这里的isActive()返回true。\n\nif (isActive()) {\n    /**\n     * 客户端SocketChannel注册成功后会走这里，在channelActive事件回调中注册OP_READ事件\n     * */\n    if (firstRegistration) {\n        //触发channelActive事件\n        pipeline.fireChannelActive();\n    } else if (config().isAutoRead()) {\n        .......省略..........\n    }\n\n}\n\n\n最后调用pipeline.fireChannelActive()在NioSocketChannel中的pipeline传播ChannelActive事件，最终在pipeline的头结点HeadContext中响应并注册OP_READ事件到Sub Reactor中的Selector上。\n\n传播ChannelActive事件.png\n\npublic abstract class AbstractNioChannel extends AbstractChannel { {\n\n    @Override\n    protected void doBeginRead() throws Exception {\n        ..............省略................\n\n        final int interestOps = selectionKey.interestOps();\n        /**\n         * 1：ServerSocketChannel 初始化时 readInterestOp设置的是OP_ACCEPT事件\n         * 2：SocketChannel 初始化时 readInterestOp设置的是OP_READ事件\n         * */\n        if ((interestOps & readInterestOp) == 0) {\n            //注册监听OP_ACCEPT或者OP_READ事件\n            selectionKey.interestOps(interestOps | readInterestOp);\n        }\n    }\n\n}\n\n\n> 注意这里的readInterestOp为客户端NioSocketChannel在初始化时设置的OP_READ事件。\n\n----------------------------------------\n\n到这里，Netty中的Main Reactor接收连接的整个流程，我们就介绍完了，此时Netty中主从Reactor组的结构就变为：\n\n主从Reactor组完整结构.png\n\n\n# 总结\n\n本文我们介绍了NioServerSocketChannel处理客户端连接事件的整个过程。\n\n * 接收连接的整个处理框架。\n * 影响Netty接收连接吞吐的Bug产生的原因，以及修复的方案。\n * 创建并初始化客户端NioSocketChannel。\n * 初始化NioSocketChannel中的pipeline。\n * 客户端NioSocketChannel向Sub Reactor注册的过程\n\n其中我们也对比了NioServerSocketChannel与NioSocketChannel在创建初始化以及后面向Reactor注册过程中的差异之处。\n\n当客户端NioSocketChannel接收完毕并向Sub Reactor注册成功后，那么接下来Sub Reactor就开始监听注册其上的所有客户端NioSocketChannel的OP_READ事件，并等待客户端向服务端发送网络数据。\n\n后面Reactor的主角就该变为Sub Reactor以及注册在其上的客户端NioSocketChannel了。\n\n下篇文章，我们将会讨论Netty是如何接收网络数据的~~~~ 我们下篇文章见~~\n\n\n# 参考资料\n\n抓到Netty一个Bug，顺带来透彻地聊一下Netty是如何高效接收网络连接的 (qq.com)",normalizedContent:"# 前言\n\n对于一个高性能网络通讯框架来说，最最重要也是最核心的工作就是如何高效的接收客户端连接，这就好比我们开了一个饭店，那么迎接客人就是饭店最重要的工作，我们要先把客人迎接进来，不能让客人一看人多就走掉，只要客人进来了，哪怕菜做的慢一点也没关系。\n\n本文笔者就来为大家介绍下netty这块最核心的内容，看看netty是如何高效的接收客户端连接的。\n\n下图为笔者在一个月黑风高天空显得那么深邃遥远的夜晚，闲来无事，于是捧起netty关于如何接收连接这部分源码细细品读的时候，意外的发现了一个影响netty接收连接吞吐的一个bug。\n\n\n\n于是笔者就在github提了一个?issue#11708，阐述了下这个bug产生的原因以及导致的结果并和netty的作者一起讨论了下修复措施。如上图所示。\n\n> issue#11708：https://github.com/netty/netty/issues/11708\n\n这里先不详细解释这个issue，也不建议大家现在就打开这个issue查看，笔者会在本文的介绍中随着源码深入的解读慢慢的为大家一层一层地拨开迷雾。\n\n之所以在文章的开头把这个拎出来，笔者是想让大家带着怀疑，审视，欣赏，崇敬，敬畏的态度来一起品读世界顶级程序员编写的代码。由衷的感谢他们在这一领域做出的贡献。\n\n好了，问题抛出来后，我们就带着这个疑问来开始本文的内容吧~~~\n\n\n\n\n# 前文回顾\n\n按照老规矩，再开始本文的内容之前，我们先来回顾下前边几篇文章的概要内容帮助大家梳理一个框架全貌出来。\n\n> 笔者这里再次想和读者朋友们强调的是本文可以独立观看，并不依赖前边系列文章的内容，只是大家如果对相关细节部分感兴趣的话，可以在阅读完本文之后在去回看相关文章。\n\n在前边的系列文章中，笔者为大家介绍了驱动netty整个框架运转的核心引擎reactor的创建，启动，运行的全流程。从现在开始netty的整个核心框架就开始运转起来开始工作了，本文要介绍的主要内容就是netty在启动之后要做的第一件事件：监听端口地址，高效接收客户端连接。\n\n在《聊聊netty那些事儿之从内核角度看io模型》一文中，我们是从整个网络框架的基石io模型的角度整体阐述了下netty的io线程模型。\n\n而netty中的reactor正是io线程在netty中的模型定义。reactor在netty中是以group的形式出现的，分为:\n\n * 主reactor线程组也就是我们在启动代码中配置的eventloopgroup bossgroup,main reactor group中的reactor主要负责监听客户端连接事件，高效的处理客户端连接。也是本文我们要介绍的重点。\n * 从reactor线程组也就是我们在启动代码中配置的eventloopgroup workergroup，sub reactor group中的reactor主要负责处理客户端连接上的io事件，以及异步任务的执行。\n\n最后我们得出netty的整个io模型如下：\n\n\n\n本文我们讨论的重点就是 mainreactorgroup 的核心工作上图中所示的步骤1，步骤2，步骤3。\n\n在从整体上介绍完netty的io模型之后，我们又在?《reactor在netty中的实现(创建篇)》中完整的介绍了netty框架的骨架主从reactor组的搭建过程，阐述了reactor是如何被创建出来的，并介绍了它的核心组件如下图所示：\n\n\n\n * thread即为reactor中的io线程，主要负责监听io事件，处理io任务，执行异步任务。\n * selector则是jdk nio对操作系统底层io多路复用技术实现的封装。用于监听io就绪事件。\n * taskqueue用于保存reactor需要执行的异步任务，这些异步任务可以由用户在业务线程中向reactor提交，也可以是netty框架提交的一些自身核心的任务。\n * scheduledtaskqueue则是保存reactor中执行的定时任务。代替了原有的时间轮来执行延时任务。\n * tailqueue保存了在reactor需要执行的一些尾部收尾任务，在普通任务执行完后 reactor线程会执行尾部任务，比如对netty 的运行状态做一些统计数据，例如任务循环的耗时、占用物理内存的大小等等\n\n在骨架搭建完毕之后，我们随后又在在?《详细图解netty reactor启动全流程》》一文中介绍了本文的主角服务端nioserversocketchannel的创建，初始化，绑定端口地址，向main reactor注册监听op_accept事件的完整过程。\n\n\n\nmain reactor 如何处理 op_accept 事件将会是本文的主要内容。\n\n自此netty框架的main reactor group已经启动完毕，开始准备监听op_accept事件，当客户端连接上来之后，op_accept事件活跃，main reactor开始处理op_accept事件接收客户端连接了。\n\n而netty中的io事件分为：op_accept事件，op_read事件，op_write事件和op_connect事件\n\nnetty对于io事件的监听和处理统一封装在reactor模型中，这四个io事件的处理过程也是我们后续文章中要单独拿出来介绍的，本文我们聚焦op_accept事件的处理。\n\n而为了让大家能够对io事件的处理有一个完整性的认识，笔者写了?《一文聊透netty核心引擎reactor的运转架构》这篇文章，在文章中详细介绍了reactor线程的整体运行框架。\n\n\n\nreactor 线程会在一个死循环中 996 不停的运转，在循环中会不断的轮询监听 selector 上的io事件，当 io 事件活跃后，reactor从selector 上被唤醒转去执行 io 就绪事件的处理，在这个过程中我们引出了上述四种io事件的处理入口函数。\n\nprivate void processselectedkey(selectionkey k, abstractniochannel ch) {\n    //获取channel的底层操作类unsafe\n    final abstractniochannel.niounsafe unsafe = ch.unsafe();\n    if (!k.isvalid()) {\n        ......如果selectionkey已经失效则关闭对应的channel......\n    }\n\n    try {\n        //获取io就绪事件\n        int readyops = k.readyops();\n        //处理connect事件\n        if ((readyops & selectionkey.op_connect) != 0) {\n            int ops = k.interestops();\n            //移除对connect事件的监听，否则selector会一直通知\n            ops &= selectionkey.op_connect;\n            k.interestops(ops);\n            //触发channelactive事件处理connect事件\n            unsafe.finishconnect();\n        }\n\n        //处理write事件\n        if ((readyops & selectionkey.op_write) != 0) {\n            ch.unsafe().forceflush();\n        }\n\n        //处理read事件或者accept事件\n        if ((readyops & (selectionkey.op_read | selectionkey.op_accept)) != 0 || readyops == 0) {\n            unsafe.read();\n        }\n    } catch (cancelledkeyexception ignored) {\n        unsafe.close(unsafe.voidpromise());\n    }\n}\n\n\n本文笔者将会为大家重点介绍op_accept事件的处理入口函数unsafe.read()的整个源码实现。\n\n当客户端连接完成三次握手之后，main reactor 中的 selector 产生op_accept事件活跃，main reactor 随即被唤醒，来到了op_accept事件的处理入口函数开始接收客户端连接\n\n\n# 1. main reactor 处理 op_accept 事件\n\n\n\n当main reactor轮询到nioserversocketchannel上的op_accept事件就绪时，main reactor线程就会从jdk selector上的阻塞轮询apiselector.select(timeoutmillis)调用中返回。转而去处理nioserversocketchannel上的op_accept事件。\n\npublic final class nioeventloop extends singlethreadeventloop {\n\n    private void processselectedkey(selectionkey k, abstractniochannel ch) {\n        final abstractniochannel.niounsafe unsafe = ch.unsafe();\n        ..............省略.................\n\n        try {\n            int readyops = k.readyops();\n\n            if ((readyops & selectionkey.op_connect) != 0) {\n               ..............处理op_connect事件.................\n            }\n\n\n            if ((readyops & selectionkey.op_write) != 0) {\n              ..............处理op_write事件.................\n            }\n\n\n            if ((readyops & (selectionkey.op_read | selectionkey.op_accept)) != 0 || readyops == 0) {\n                //本文重点处理op_accept事件\n                unsafe.read();\n            }\n        } catch (cancelledkeyexception ignored) {\n            unsafe.close(unsafe.voidpromise());\n        }\n    }\n\n}\n\n\n * 处理io就绪事件的入口函数processselectedkey中的参数abstractniochannel ch正是netty服务端nioserversocketchannel。因为此时的执行线程为main reactor线程，而main reactor上注册的正是netty服务端nioserversocketchannel负责监听端口地址，接收客户端连接。\n * 通过ch.unsafe()获取到的niounsafe操作类正是nioserversocketchannel中对底层jdk nio serversocketchannel的unsafe底层操作类。\n\n> unsafe接口是netty对channel底层操作行为的封装，比如nioserversocketchannel的底层unsafe操作类干的事情就是绑定端口地址，处理op_accept事件。\n\n这里我们看到，netty将op_accept事件处理的入口函数封装在nioserversocketchannel里的底层操作类unsafe的read方法中。\n\n\n\n而nioserversocketchannel中的unsafe操作类实现类型为niomessageunsafe定义在上图继承结构中的abstractniomessagechannel父类中\n\n下面我们到niomessageunsafe#read方法中来看下netty对op_accpet事件的具体处理过程：\n\n\n# 2. 接收客户端连接核心流程框架总览\n\n我们还是按照老规矩，先从整体上把整个 op_accept 事件的逻辑处理框架提取出来，让大家先总体俯视下流程全貌，然后在针对每个核心点位进行各个击破\n\n\n\nmain reactor 线程是在一个do...while{...}循环 read loop中不断的调用 jdk nio serversocketchannel.accept()方法来接收完成三次握手的客户端连接 niosocketchannel 的，并将接收到的客户端连接 niosocketchannel 临时保存在list<object> readbuf集合中，后续会服务端nioserversocketchannel的pipeline中通过channelread事件来传递，最终会在serverbootstrapacceptor这个channelhandler中被处理初始化，并将其注册到sub reator group中\n\n这里的read loop循环会被限定只能读取16次，当main reactor从nioserversocketchannel中读取客户端连接niosocketchannel的次数达到16次之后，无论此时是否还有客户端连接都不能在继续读取了。\n\n因为我们在?《一文聊透netty核心引擎reactor的运转架构》一文中提到，netty 对 reactor 线程压榨的比较狠，要干的事情很多，除了要监听轮询io就绪事件，处理 io 就绪事件，还需要执行用户和 netty 框架本身提交的异步任务和定时任务。\n\n所以这里的main reactor线程不能在read loop中无限制的执行下去，因为还需要分配时间去执行异步任务，不能因为无限制的接收客户端连接而耽误了异步任务的执行。所以这里将 read loop 的循环次数限定为16次\n\n如果 main reactor 线程在read loop中读取客户端连接niosocketchannel的次数已经满了16次，即使此时还有客户端连接未接收，那么main reactor线程也不会再去接收了，而是转去执行异步任务，当异步任务执行完毕后，还会在回来执行剩余接收连接的任务。\n\n\n\nmain reactor线程退出 read loop 循环的条件有两个：\n\n 1. 在限定的16次读取中，已经没有新的客户端连接要接收了。退出循环。\n 2. 从nioserversocketchannel中读取客户端连接的次数达到了16次，无论此时是否还有客户端连接都需要退出循环。\n\n以上就是netty在接收客户端连接时的整体核心逻辑，下面笔者将这部分逻辑的核心源码实现框架提取出来，方便大家根据上述核心逻辑与源码中的处理模块对应起来，还是那句话，这里只需要总体把握核心处理流程，不需要读懂每一行代码，笔者会在文章的后边分模块来各个击破它们。\n\npublic abstract class abstractniomessagechannel extends abstractniochannel {\n\n  private final class niomessageunsafe extends abstractniounsafe {\n\n        //存放连接建立后，创建的客户端socketchannel\n        private final list<object> readbuf = new arraylist<object>();\n\n        @override\n        public void read() {\n            //必须在main reactor线程中执行\n            assert eventloop().ineventloop();\n            //注意下面的config和pipeline都是服务端serversocketchannel中的\n            final channelconfig config = config();\n            final channelpipeline pipeline = pipeline();\n            //创建接收数据buffer分配器（用于分配容量大小合适的bytebuffer用来容纳接收数据）\n            //在接收连接的场景中，这里的allochandle只是用于控制read loop的循环读取创建连接的次数。\n            final recvbytebufallocator.handle allochandle = unsafe().recvbufallochandle();\n            allochandle.reset(config);\n\n            boolean closed = false;\n            throwable exception = null;\n            try {\n                try {\n                    do {\n                        //底层调用nioserversocketchannel->doreadmessages 创建客户端socketchannel\n                        int localread = doreadmessages(readbuf);\n\n                        //已无新的连接可接收则退出read loop\n                        if (localread == 0) {\n                            break;\n                        }\n                        if (localread < 0) {\n                            closed = true;\n                            break;\n                        }\n                        //统计在当前事件循环中已经读取到得message数量（创建连接的个数）\n                        allochandle.incmessagesread(localread);\n                    } while (allochandle.continuereading());//判断是否已经读满16次\n                } catch (throwable t) {\n                    exception = t;\n                }\n\n                int size = readbuf.size();\n                for (int i = 0; i < size; i ++) {\n                    readpending = false;\n                    //在nioserversocketchannel对应的pipeline中传播channelread事件\n                    //初始化客户端socketchannel，并将其绑定到sub reactor线程组中的一个reactor上\n                    pipeline.firechannelread(readbuf.get(i));\n                }\n                //清除本次accept 创建的客户端socketchannel集合\n                readbuf.clear();\n                allochandle.readcomplete();\n                //触发readcomplete事件传播\n                pipeline.firechannelreadcomplete();\n                ....................省略............\n            } finally {\n                ....................省略............\n            }\n        }\n    }\n  }\n}\n\n\n这里首先要通过断言assert eventloop().ineventloop()确保处理接收客户端连接的线程必须为main reactor 线程。\n\n而main reactor中主要注册的是服务端 nioserversocketchannel，主要负责处理op_accept事件，所以当前main reactor线程是在nioserversocketchannel中执行接收连接的工作。\n\n所以这里我们通过config()获取到的是nioserversocketchannel的属性配置类nioserversocketchannelconfig,它是在reactor的启动阶段被创建出来的。\n\npublic nioserversocketchannel(serversocketchannel channel) {\n    //父类abstractniochannel中保存jdk nio原生serversocketchannel以及要监听的事件op_accept\n    super(null, channel, selectionkey.op_accept);\n    //defaultchannelconfig中设置用于channel接收数据用的buffer->adaptiverecvbytebufallocator\n    config = new nioserversocketchannelconfig(this, javachannel().socket());\n}\n\n\n同理这里通过pipeline()获取到的也是nioserversocketchannel中的pipeline。它会在nioserversocketchannel向main reactor注册成功之后被初始化。\n\n\n\n前边提到main reactor线程会被限定只能在read loop中向nioserversocketchannel读取16次客户端连接，所以在开始read loop之前，我们需要创建一个能够保存记录读取次数的对象，在每次read loop循环之后，可以根据这个对象来判断是否结束read loop。\n\n这个对象就是这里的 recvbytebufallocator.handle allochandle专门用于统计read loop中接收客户端连接的次数，以及判断是否该结束read loop转去执行异步任务。\n\n当这一切准备就绪之后，main reactor 线程就开始在do{....}while(...)循环中接收客户端连接了。\n\n在 read loop中通过调用doreadmessages函数接收完成三次握手的客户端连接，底层会调用到jdk nio serversocketchannel的accept方法，从内核全连接队列中取出客户端连接。\n\n返回值localread表示接收到了多少客户端连接，客户端连接通过accept方法只会一个一个的接收，所以这里的localread正常情况下都会返回1，当localread <= 0时意味着已经没有新的客户端连接可以接收了，本次main reactor接收客户端的任务到这里就结束了，跳出read loop。开始新的一轮io事件的监听处理。\n\npublic static socketchannel accept(final serversocketchannel serversocketchannel) throws ioexception {\n    try {\n        return accesscontroller.doprivileged(new privilegedexceptionaction<socketchannel>() {\n            @override\n            public socketchannel run() throws ioexception {\n                return serversocketchannel.accept();\n            }\n        });\n    } catch (privilegedactionexception e) {\n        throw (ioexception) e.getcause();\n    }\n}\n\n\n随后会将接收到的客户端连接占时存放到list<object> readbuf集合中。\n\nprivate final class niomessageunsafe extends abstractniounsafe {\n\n    //存放连接建立后，创建的客户端socketchannel\n    private final list<object> readbuf = new arraylist<object>();\n}\n\n\n调用allochandle.incmessagesread统计本次事件循环中接收到的客户端连接个数，最后在read loop末尾通过allochandle.continuereading判断是否达到了限定的16次。从而决定main reactor线程是继续接收客户端连接还是转去执行异步任务。\n\nmain reactor线程退出read loop的两个条件：\n\n 1. 在限定的16次读取中，已经没有新的客户端连接要接收了。退出循环。\n 2. 从nioserversocketchannel中读取客户端连接的次数达到了16次，无论此时是否还有客户端连接都需要退出循环。\n\n当满足以上两个退出条件时，main reactor线程就会退出read loop，由于在read loop中接收到的客户端连接全部暂存在list<object> readbuf集合中,随后开始遍历readbuf，在nioserversocketchannel的pipeline中传播channelread事件。\n\nint size = readbuf.size();\nfor (int i = 0; i < size; i ++) {\n    readpending = false;\n    //nioserversocketchannel对应的pipeline中传播read事件\n    //io.netty.bootstrap.serverbootstrap.serverbootstrapacceptor.channelread\n    //初始化客户端socketchannel，并将其绑定到sub reactor线程组中的一个reactor上\n    pipeline.firechannelread(readbuf.get(i));\n}\n\n\n最终pipeline中的channelhandler(serverbootstrapacceptor)会响应channelread事件，并在相应回调函数中初始化客户端niosocketchannel，并将其注册到sub reactor group中。此后客户端niosocketchannel绑定到的sub reactor就开始监听处理客户端连接上的读写事件了。\n\nnetty整个接收客户端的逻辑过程如下图步骤1，2，3所示。\n\n\n\n以上内容就是笔者提取出来的整体流程框架，下面我们来将其中涉及到的重要核心模块拆开，一个一个详细解读下。\n\n\n# 3. recvbytebufallocator 简介\n\nreactor 在处理对应 channel 上的 io 数据时，都会采用一个bytebuffer来接收channel上的io数据。而本小节要介绍的recvbytebufallocator 正是用来分配 bytebuffer 的一个分配器。\n\n还记得这个recvbytebufallocator在哪里被创建的吗？？\n\n在?《聊聊netty那些事儿之reactor在netty中的实现(创建篇)》一文中，在介绍nioserversocketchannel的创建过程中提到，对应channel的配置类nioserversocketchannelconfig也会随着nioserversocketchannel的创建而创建。\n\npublic nioserversocketchannel(serversocketchannel channel) {\n    super(null, channel, selectionkey.op_accept);\n    config = new nioserversocketchannelconfig(this, javachannel().socket());\n}\n\n\n在创建nioserversocketchannelconfig的过程中会创建recvbytebufallocator。\n\npublic defaultchannelconfig(channel channel) {\n    this(channel, new adaptiverecvbytebufallocator());\n}\n\n\n这里我们看到 nioserversocketchannel 中的recvbytebufallocator实际类型为adaptiverecvbytebufallocator，顾名思义，这个类型的recvbytebufallocator可以根据channel上每次到来的io数据大小来自适应动态调整bytebuffer的容量。\n\n对于服务端nioserversocketchannel来说，它上边的io数据就是客户端的连接，它的长度和类型都是固定的，所以在接收客户端连接的时候并不需要这样的一个bytebuffer来接收，我们会将接收到的客户端连接存放在list<object> readbuf集合中\n\n对于客户端niosocketchannel来说，它上边的io数据时客户端发送来的网络数据，长度是不定的，所以才会需要这样一个可以根据每次io数据的大小来自适应动态调整容量的bytebuffer来接收。\n\n那么看起来这个recvbytebufallocator和本文的主题不是很关联，因为在接收连接的过程中并不会怎么用到它，这个类笔者还会在后面的文章中详细介绍，之所以这里把它拎出来单独介绍是因为它和本文开头提到的bug有关系，这个bug就是由这个类引起的。\n\n\n# 3.1 recvbytebufallocator.handle的获取\n\n在本文中，我们是通过nioserversocketchannel中的unsafe底层操作类来获取recvbytebufallocator.handle的\n\nfinal recvbytebufallocator.handle allochandle = unsafe().recvbufallochandle();\nprotected abstract class abstractunsafe implements unsafe {\n        @override\n        public recvbytebufallocator.handle recvbufallochandle() {\n            if (recvhandle == null) {\n                recvhandle = config().getrecvbytebufallocator().newhandle();\n            }\n            return recvhandle;\n        }\n}\n\n\n我们看到最终会在nioserversocketchannel的配置类nioserversocketchannelconfig中获取到adaptiverecvbytebufallocator\n\npublic class defaultchannelconfig implements channelconfig {\n    //用于channel接收数据用的buffer分配器  类型为adaptiverecvbytebufallocator\n    private volatile recvbytebufallocator rcvbufallocator;\n}\n\n\nadaptiverecvbytebufallocator中会创建自适应动态调整容量的bytebuffer分配器。\n\npublic class adaptiverecvbytebufallocator extends defaultmaxmessagesrecvbytebufallocator {\n\n    @override\n    public handle newhandle() {\n        return new handleimpl(minindex, maxindex, initial);\n    }\n    \n    private final class handleimpl extends maxmessagehandle {\n                  .................省略................\n    }\n}\n\n\n这里的newhandle方法返回的具体类型为maxmessagehandle，这个maxmessagehandle里边保存了每次从channel中读取io数据的容量指标，方便下次读取时分配合适大小的buffer。\n\n每次在使用allochandle前需要调用allochandle.reset(config);重置里边的统计指标。\n\n    public abstract class maxmessagehandle implements extendedhandle {\n        private channelconfig config;\n        //每次事件轮询时，最多读取16次\n        private int maxmessageperread;\n        //本次事件轮询总共读取的message数,这里指的是接收连接的数量\n        private int totalmessages;\n        //本次事件轮询总共读取的字节数\n        private int totalbytesread;\n\n       @override\n        public void reset(channelconfig config) {\n            this.config = config;\n            //默认每次最多读取16次\n            maxmessageperread = maxmessagesperread();\n            totalmessages = totalbytesread = 0;\n        }\n    }\n\n\n * maxmessageperread：用于控制每次read loop里最大可以循环读取的次数，默认为16次，可在启动配置类serverbootstrap中通过channeloption.max_messages_per_read选项设置。\n\nserverbootstrap b = new serverbootstrap();\nb.group(bossgroup, workergroup)\n  .channel(nioserversocketchannel.class)\n  .option(channeloption.max_messages_per_read, 自定义次数)\n\n\n * totalmessages：用于统计read loop中总共接收的连接个数，每次read loop循环后会调用allochandle.incmessagesread增加记录接收到的连接个数。\n\n        @override\n        public final void incmessagesread(int amt) {\n            totalmessages += amt;\n        }\n\n\n * totalbytesread：用于统计在read loop中总共接收到客户端连接上的数据大小，这个字段主要用于sub reactor在接收客户端niosocketchannel上的网络数据用的，本文我们介绍的是main reactor接收客户端连接，所以这里并不会用到这个字段。这个字段会在sub reactor每次读取完niosocketchannel上的网络数据时增加记录。\n\n        @override\n        public void lastbytesread(int bytes) {\n            lastbytesread = bytes;\n            if (bytes > 0) {\n                totalbytesread += bytes;\n            }\n        }\n\n\nmaxmessagehandler中还有一个非常重要的方法就是在每次read loop末尾会调用allochandle.continuereading()方法来判断读取连接次数是否已满16次，来决定main reactor线程是否退出循环。\n\n                  do {\n                        //底层调用nioserversocketchannel->doreadmessages 创建客户端socketchannel\n                        int localread = doreadmessages(readbuf);\n                        if (localread == 0) {\n                            break;\n                        }\n                        if (localread < 0) {\n                            closed = true;\n                            break;\n                        }\n                        //统计在当前事件循环中已经读取到得message数量（创建连接的个数）\n                        allochandle.incmessagesread(localread);\n                    } while (allochandle.continuereading());\n\n\nimage.png\n\n红框中圈出来的两个判断条件和本文主题无关，我们这里不需要关注，笔者会在后面的文章详细介绍。\n\n * totalmessages < maxmessageperread：在本文的接收客户端连接场景中，这个条件用于判断main reactor线程在read loop中的读取次数是否超过了16次。如果超过16次就会返回false，main reactor线程退出循环。\n * totalbytesread > 0：用于判断当客户端niosocketchannel上的op_read事件活跃时，sub reactor线程在read loop中是否读取到了网络数据。\n\n以上内容就是recvbytebufallocator.handle在接收客户端连接场景下的作用，大家这里仔细看下这个allochandle.continuereading()方法退出循环的判断条件，再结合整个do{....}while(...)接收连接循环体，感受下是否哪里有些不对劲？bug即将出现~~~\n\nimage.png\n\n\n# 4. 啊哈！！bug ! !\n\nimage.png\n\nnetty不论是在本文中处理接收客户端连接的场景还是在处理接收客户端连接上的网络数据场景都会在一个do{....}while(...)循环read loop中不断的处理。\n\n同时也都会利用在上一小节中介绍的recvbytebufallocator.handle来记录每次read loop接收到的连接个数和从连接上读取到的网络数据大小。\n\n从而在read loop的末尾都会通过allochandle.continuereading()方法判断是否应该退出read loop循环结束连接的接收流程或者是结束连接上数据的读取流程。\n\n无论是用于接收客户端连接的main reactor也好还是用于接收客户端连接上的网络数据的sub reactor也好，它们的运行框架都是一样的，只不过是具体分工不同。\n\n所以netty这里想用统一的recvbytebufallocator.handle来处理以上两种场景。\n\n而recvbytebufallocator.handle中的totalbytesread字段主要记录sub reactor线程在处理客户端niosocketchannel中op_read事件活跃时，总共在read loop中读取到的网络数据，而这里是main reactor线程在接收客户端连接所以这个字段并不会被设置。totalbytesread字段的值在本文中永远会是0。\n\n所以无论同时有多少个客户端并发连接到服务端上，在接收连接的这个read loop中永远只会接受一个连接就会退出循环，因为allochandle.continuereading()方法中的判断条件totalbytesread > 0永远会返回false。\n\n                  do {\n                        //底层调用nioserversocketchannel->doreadmessages 创建客户端socketchannel\n                        int localread = doreadmessages(readbuf);\n                        if (localread == 0) {\n                            break;\n                        }\n                        if (localread < 0) {\n                            closed = true;\n                            break;\n                        }\n                        //统计在当前事件循环中已经读取到得message数量（创建连接的个数）\n                        allochandle.incmessagesread(localread);\n                    } while (allochandle.continuereading());\n\n\n而netty的本意是在这个read loop循环中尽可能多的去接收客户端的并发连接，同时又不影响main reactor线程执行异步任务。但是由于这个bug，main reactor在这个循环中只执行一次就结束了。这也一定程度上就影响了netty的吞吐。\n\n让我们想象下这样的一个场景，当有16个客户端同时并发连接到了服务端，这时nioserversocketchannel上的op_accept事件活跃，main reactor从selector上被唤醒，随后执行op_accept事件的处理。\n\npublic final class nioeventloop extends singlethreadeventloop {\n    @override\n    protected void run() {\n        int selectcnt = 0;\n        for (;;) {\n            try { \n                int strategy;\n                try {\n                    strategy = selectstrategy.calculatestrategy(selectnowsupplier, hastasks());\n                    switch (strategy) {\n                    case selectstrategy.continue:                  \n                          ............省略.........\n                    case selectstrategy.busy_wait:\n\n                          ............省略.........\n                    case selectstrategy.select:\n                            ............监听轮询io事件.........\n                    default:\n                    }\n                } catch (ioexception e) {\n                    ............省略.........\n                }\n\n                ............处理io就绪事件.........\n                ............执行异步任务.........\n    }\n}\n\n\n但是由于这个bug的存在，main reactor在接收客户端连接的这个read loop中只接收了一个客户端连接就匆匆返回了。\n\n      private final class niomessageunsafe extends abstractniounsafe {\n                    do {\n                        int localread = doreadmessages(readbuf);\n                        .........省略...........\n                    } while (allochandle.continuereading());\n     }\n\n\n然后根据下图中这个reactor的运行结构去执行异步任务，随后绕一大圈又会回到nioeventloop#run方法中重新发起一轮op_accept事件轮询。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)reactor线程运行时结构.png\n\n由于现在还有15个客户端并发连接没有被接收，所以此时main reactor线程并不会在selector.select()上阻塞，最终绕一圈又会回到niomessageunsafe#read方法的do{.....}while()循环。在接收一个连接之后又退出循环。\n\n本来我们可以在一次read loop中把这16个并发的客户端连接全部接收完毕的，因为这个bug，main reactor需要不断的发起op_accept事件的轮询，绕了很大一个圈子。同时也增加了许多不必要的selector.select()系统调用开销\n\nissue讨论.png\n\n这时大家在看这个?issue#11708中的讨论是不是就清晰很多了~~\n\n> issue#11708：https://github.com/netty/netty/issues/11708\n\n\n# 4.1 bug 的修复\n\n> 笔者在写这篇文章的时候，netty最新版本是4.1.68.final，这个bug在4.1.69.final中被修复。\n\nimage.png\n\n由于该bug产生的原因正是因为服务端nioserversocketchannel（用于监听端口地址和接收客户端连接）和 客户端niosocketchannel（用于通信）中的config配置类混用了同一个bytebuffer分配器adaptiverecvbytebufallocator而导致的。\n\n所以在新版本修复中专门为服务端serversocketchannel中的config配置类引入了一个新的bytebuffer分配器serverchannelrecvbytebufallocator，专门用于服务端serversocketchannel接收客户端连接的场景。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n在serverchannelrecvbytebufallocator的父类defaultmaxmessagesrecvbytebufallocator中引入了一个新的字段ignorebytesread，用于表示是否忽略网络字节的读取，在创建服务端channel配置类nioserversocketchannelconfig的时候，这个字段会被赋值为true。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n当main reactor线程在read loop循环中接收客户端连接的时候。\n\n      private final class niomessageunsafe extends abstractniounsafe {\n\n                    do {\n                        int localread = doreadmessages(readbuf);\n                        .........省略...........\n                    } while (allochandle.continuereading());\n     }\n\n\n在read loop循环的末尾就会采用从serverchannelrecvbytebufallocator中创建的maxmessagehandle#continuereading方法来判断读取连接次数是否超过了16次。由于这里的ignorebytesread == true这回我们就会忽略totalbytesread == 0的情况，从而使得接收连接的read loop得以继续地执行下去。在一个read loop中一次性把16个连接全部接收完毕。\n\nimage.png\n\n以上就是对这个bug产生的原因，以及发现的过程，最后修复的方案一个全面的介绍，因此笔者也出现在了netty 4.1.69.final版本发布公告里的thank-list中。哈哈，真是令人开心的一件事情~~~\n\nimage.png\n\n通过以上对netty接收客户端连接的全流程分析和对这个bug来龙去脉以及修复方案的介绍，大家现在一定已经理解了整个接收连接的流程框架。\n\n接下来笔者就把这个流程中涉及到的一些核心模块在单独拎出来从细节入手，为大家各个击破~~~\n\n\n# 5. doreadmessages 接收客户端连接\n\npublic class nioserversocketchannel extends abstractniomessagechannel\n                             implements io.netty.channel.socket.serversocketchannel {\n\n    @override\n    protected int doreadmessages(list<object> buf) throws exception {\n        socketchannel ch = socketutils.accept(javachannel());\n\n        try {\n            if (ch != null) {\n                buf.add(new niosocketchannel(this, ch));\n                return 1;\n            }\n        } catch (throwable t) {\n            logger.warn(\"failed to create a new channel from an accepted socket.\", t);\n\n            try {\n                ch.close();\n            } catch (throwable t2) {\n                logger.warn(\"failed to close a socket.\", t2);\n            }\n        }\n\n        return 0;\n    }\n\n}\n\n\n * 通过javachannel()获取封装在netty服务端nioserversocketchannel中的jdk 原生 serversocketchannel。\n\n@override\nprotected serversocketchannel javachannel() {\n    return (serversocketchannel) super.javachannel();\n}\n\n\n * 通过jdk nio 原生的serversocketchannel的accept方法获取jdk nio 原生客户端连接socketchannel。\n\npublic static socketchannel accept(final serversocketchannel serversocketchannel) throws ioexception {\n    try {\n        return accesscontroller.doprivileged(new privilegedexceptionaction<socketchannel>() {\n            @override\n            public socketchannel run() throws ioexception {\n                return serversocketchannel.accept();\n            }\n        });\n    } catch (privilegedactionexception e) {\n        throw (ioexception) e.getcause();\n    }\n}\n\n\n这一步就是我们在?《聊聊netty那些事儿之从内核角度看io模型》介绍到的调用监听socket的accept方法，内核会基于监听socket创建出来一个新的socket专门用于与客户端之间的网络通信这个我们称之为客户端连接socket。这里的serversocketchannel就类似于监听socket。socketchannel就类似于客户端连接socket。\n\n由于我们在创建nioserversocketchannel的时候，会将jdk nio 原生的serversocketchannel设置为非阻塞，所以这里当serversocketchannel上有客户端连接时就会直接创建socketchannel，如果此时并没有客户端连接时accept调用就会立刻返回null并不会阻塞。\n\nprotected abstractniochannel(channel parent, selectablechannel ch, int readinterestop) {\n    super(parent);\n    this.ch = ch;\n    this.readinterestop = readinterestop;\n    try {\n        //设置channel为非阻塞 配合io多路复用模型\n        ch.configureblocking(false);\n    } catch (ioexception e) {\n        ..........省略.............\n    }\n}\n\n\n\n# 5.1 创建客户端niosocketchannel\n\npublic class nioserversocketchannel extends abstractniomessagechannel\n                             implements io.netty.channel.socket.serversocketchannel {\n\n    @override\n    protected int doreadmessages(list<object> buf) throws exception {\n        socketchannel ch = socketutils.accept(javachannel());\n\n        try {\n            if (ch != null) {\n                buf.add(new niosocketchannel(this, ch));\n                return 1;\n            }\n        } catch (throwable t) {\n          .........省略.......\n        }\n\n        return 0;\n    }\n\n}\n\n\n这里会根据serversocketchannel的accept方法获取到jdk nio 原生的socketchannel（用于底层真正与客户端通信的channel），来创建netty中的niosocketchannel。\n\npublic class niosocketchannel extends abstractniobytechannel implements io.netty.channel.socket.socketchannel {\n\n    public niosocketchannel(channel parent, socketchannel socket) {\n        super(parent, socket);\n        config = new niosocketchannelconfig(this, socket.socket());\n    }\n\n}\n\n\n创建客户端niosocketchannel的过程其实和之前讲的创建服务端nioserversocketchannel大体流程是一样的，我们这里只对客户端niosocketchannel和服务端nioserversocketchannel在创建过程中的不同之处做一个对比。\n\n> 具体细节部分大家可以在回看下?《详细图解netty reactor启动全流程》一文中关于nioserversocketchannel的创建的详细细节。\n\n\n# 5.3 对比niosocketchannel与nioserversocketchannel的不同\n\n# 1：channel的层次不同\n\n在我们介绍reactor的创建文章中，我们提到netty中的channel是具有层次的。由于客户端niosocketchannel是在main reactor接收连接时在服务端nioserversocketchannel中被创建的，所以在创建客户端niosocketchannel的时候会通过构造函数指定了parent属性为nioserversocketchanel。并将jdk nio 原生的socketchannel封装进netty的客户端niosocketchannel中。\n\n而在reactor启动过程中创建nioserversocketchannel的时候parent属性指定是null。因为它就是顶层的channel，负责创建客户端niosocketchannel。\n\npublic nioserversocketchannel(serversocketchannel channel) {\n    super(null, channel, selectionkey.op_accept);\n    config = new nioserversocketchannelconfig(this, javachannel().socket());\n}\n\n\n# 2：向reactor注册的io事件不同\n\n客户端 niosocketchannel 向 sub reactor 注册的是 selectionkey.op_read事件，而服务端 nioserversocketchannel 向 main reactor 注册的是selectionkey.op_accept事件。\n\npublic abstract class abstractniobytechannel extends abstractniochannel {\n\n    protected abstractniobytechannel(channel parent, selectablechannel ch) {\n        super(parent, ch, selectionkey.op_read);\n    }\n\n}\n\npublic class nioserversocketchannel extends abstractniomessagechannel\n                             implements io.netty.channel.socket.serversocketchannel {\n\n   public nioserversocketchannel(serversocketchannel channel) {\n        //父类abstractniochannel中保存jdk nio原生serversocketchannel以及要监听的事件op_accept\n        super(null, channel, selectionkey.op_accept);\n        //defaultchannelconfig中设置用于channel接收数据用的buffer->adaptiverecvbytebufallocator\n        config = new nioserversocketchannelconfig(this, javachannel().socket());\n    }\n}\n\n\n# 3: 功能属性不同造成继承结构的不同\n\n\n\n客户端niosocketchannel继承的是abstractniobytechannel，而服务端nioserversocketchannel继承的是abstractniomessagechannel。它们继承的这两个抽象类一个前缀是byte，一个前缀是message有什么区别吗？？\n\n> 客户端niosocketchannel主要处理的是服务端与客户端的通信，这里涉及到接收客户端发送来的数据，而sub reactor线程从niosocketchannel中读取的正是网络通信数据单位为byte。\n\n> 服务端nioserversocketchannel主要负责处理op_accept事件，创建用于通信的客户端niosocketchannel。这时候客户端与服务端还没开始通信，所以main reactor线程从nioserversocketchannel的读取对象为message。这里的message指的就是底层的socketchannel客户端连接。\n\n----------------------------------------\n\n以上就是niosocketchannel与nioserversocketchannel创建过程中的不同之处，后面的过程就一样了。\n\n * 在abstractniochannel 类中封装jdk nio 原生的socketchannel，并将其底层的io模型设置为非阻塞，保存需要监听的io事件op_read。\n\nprotected abstractniochannel(channel parent, selectablechannel ch, int readinterestop) {\n    super(parent);\n    this.ch = ch;\n    this.readinterestop = readinterestop;\n    try {\n        //设置channel为非阻塞 配合io多路复用模型\n        ch.configureblocking(false);\n    } catch (ioexception e) {\n\n    }\n}\n\n\n * 为客户端niosocketchannel创建全局唯一的channelid，创建客户端niosocketchannel的底层操作类niobyteunsafe，创建pipeline。\n\nprotected abstractchannel(channel parent) {\n    this.parent = parent;\n    //channel全局唯一id machineid+processid+sequence+timestamp+random\n    id = newid();\n    //unsafe用于底层socket的读写操作\n    unsafe = newunsafe();\n    //为channel分配独立的pipeline用于io事件编排\n    pipeline = newchannelpipeline();\n}\n\n\n * 在niosocketchannelconfig的创建过程中，将niosocketchannel的recvbytebufallocator类型设置为adaptiverecvbytebufallocator。\n\npublic defaultchannelconfig(channel channel) {\n    this(channel, new adaptiverecvbytebufallocator());\n}\n\n\n> 在bug修复后的版本中服务端nioserversocketchannel的recvbytebufallocator类型设置为serverchannelrecvbytebufallocator\n\n最终我们得到的客户端niosocketchannel结构如下：\n\n\n\n\n# 6. channelread 事件的响应\n\n\n\n在前边介绍接收连接的整体核心流程框架的时候，我们提到main reactor线程是在一个do{.....}while(...)循环read loop中不断的调用serversocketchannel#accept方法来接收客户端的连接。\n\n当满足退出read loop循环的条件有两个：\n\n 1. 在限定的16次读取中，已经没有新的客户端连接要接收了。退出循环。\n 2. 从nioserversocketchannel中读取客户端连接的次数达到了16次，无论此时是否还有客户端连接都需要退出循环。\n\nmain reactor就会退出read loop循环，此时接收到的客户端连接niosocketchannel暂存与list<object> readbuf集合中。\n\nprivate final class niomessageunsafe extends abstractniounsafe {\n\n    private final list<object> readbuf = new arraylist<object>();\n\n    @override\n    public void read() {\n        try {\n            try {\n                do {\n                    ........省略.........\n                        //底层调用nioserversocketchannel->doreadmessages 创建客户端socketchannel\n                        int localread = doreadmessages(readbuf);\n                    ........省略.........\n                        allochandle.incmessagesread(localread);\n                } while (allochandle.continuereading());\n\n            } catch (throwable t) {\n                exception = t;\n            }\n\n            int size = readbuf.size();\n            for (int i = 0; i < size; i ++) {\n                readpending = false;\n                pipeline.firechannelread(readbuf.get(i));\n            }\n\n            ........省略.........\n        } finally {\n            ........省略.........\n        }\n    }\n}\n\n\n随后main reactor线程会遍历list<object> readbuf集合中的niosocketchannel，并在nioserversocketchannel的pipeline中传播channelread事件。\n\n\n\n最终channelread事件会传播到serverbootstrapacceptor中，这里正是netty处理客户端连接的核心逻辑所在。\n\nserverbootstrapacceptor主要的作用就是初始化客户端niosocketchannel，并将客户端niosocketchannel注册到sub reactor group中，并监听op_read事件。\n\n在serverbootstrapacceptor 中会初始化客户端niosocketchannel的这些属性。\n\n比如：从reactor组eventloopgroup childgroup，用于初始化niosocketchannel中的pipeline用到的channelhandler childhandler，以及niosocketchannel中的一些childoptions和childattrs。\n\nprivate static class serverbootstrapacceptor extends channelinboundhandleradapter {\n\n        private final eventloopgroup childgroup;\n        private final channelhandler childhandler;\n        private final entry<channeloption<?>, object>[] childoptions;\n        private final entry<attributekey<?>, object>[] childattrs;\n\n        @override\n        @suppresswarnings(\"unchecked\")\n        public void channelread(channelhandlercontext ctx, object msg) {\n            final channel child = (channel) msg;\n\n            //向客户端niosocketchannel的pipeline中\n            //添加在启动配置类serverbootstrap中配置的channelhandler\n            child.pipeline().addlast(childhandler);\n\n            //利用配置的属性初始化客户端niosocketchannel\n            setchanneloptions(child, childoptions, logger);\n            setattributes(child, childattrs);\n\n            try {\n                /**\n                 * 1：在sub reactor线程组中选择一个reactor绑定\n                 * 2：将客户端socketchannel注册到绑定的reactor上\n                 * 3：socketchannel注册到sub reactor中的selector上，并监听op_read事件\n                 * */\n                childgroup.register(child).addlistener(new channelfuturelistener() {\n                    @override\n                    public void operationcomplete(channelfuture future) throws exception {\n                        if (!future.issuccess()) {\n                            forceclose(child, future.cause());\n                        }\n                    }\n                });\n            } catch (throwable t) {\n                forceclose(child, t);\n            }\n        }\n}\n\n\n正是在这里，netty会将我们在?《详细图解netty reactor启动全流程》的启动示例程序中在serverbootstrap中配置的客户端niosocketchannel的所有属性（child前缀配置）初始化到niosocketchannel中。\n\npublic final class echoserver {\n    static final int port = integer.parseint(system.getproperty(\"port\", \"8007\"));\n\n    public static void main(string[] args) throws exception {\n        // configure the server.\n        //创建主从reactor线程组\n        eventloopgroup bossgroup = new nioeventloopgroup(1);\n        eventloopgroup workergroup = new nioeventloopgroup();\n        final echoserverhandler serverhandler = new echoserverhandler();\n        try {\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)//配置主从reactor\n             .channel(nioserversocketchannel.class)//配置主reactor中的channel类型\n             .option(channeloption.so_backlog, 100)//设置主reactor中channel的option选项\n             .handler(new logginghandler(loglevel.info))//设置主reactor中channel->pipline->handler\n             .childhandler(new channelinitializer<socketchannel>() {//设置从reactor中注册channel的pipeline\n                 @override\n                 public void initchannel(socketchannel ch) throws exception {\n                     channelpipeline p = ch.pipeline();\n                     //p.addlast(new logginghandler(loglevel.info));\n                     p.addlast(serverhandler);\n                 }\n             });\n\n            // start the server. 绑定端口启动服务，开始监听accept事件\n            channelfuture f = b.bind(port).sync();\n            // wait until the server socket is closed.\n            f.channel().closefuture().sync();\n        } finally {\n            // shut down all event loops to terminate all threads.\n            bossgroup.shutdowngracefully();\n            workergroup.shutdowngracefully();\n        }\n    }\n}\n\n\n以上示例代码中通过serverbootstrap配置的niosocketchannel相关属性，会在netty启动并开始初始化nioserversocketchannel的时候将serverbootstrapacceptor的创建初始化工作封装成异步任务，然后在nioserversocketchannel注册到main reactor中成功后执行。\n\npublic class serverbootstrap extends abstractbootstrap<serverbootstrap, serverchannel> {\n\n    @override\n    void init(channel channel) {\n        ................省略................\n\n        p.addlast(new channelinitializer<channel>() {\n            @override\n            public void initchannel(final channel ch) {\n                final channelpipeline pipeline = ch.pipeline();\n                ................省略................\n                ch.eventloop().execute(new runnable() {\n                    @override\n                    public void run() {\n                        pipeline.addlast(new serverbootstrapacceptor(\n                                ch, currentchildgroup, currentchildhandler, currentchildoptions, currentchildattrs));\n                    }\n                });\n            }\n        });\n    }\n}\n\n\n在经过serverbootstrapaccptor#chanelread回调的处理之后，此时客户端niosocketchannel中pipeline的结构为：\n\n客户端channel pipeline初始结构.png\n\n随后会将初始化好的客户端niosocketchannel向sub reactor group中注册，并监听op_read事件。\n\n如下图中的步骤3所示：\n\nnetty中的reactor.png\n\n\n# 7. 向subreactorgroup中注册niosocketchannel\n\nchildgroup.register(child).addlistener(new channelfuturelistener() {\n    @override\n    public void operationcomplete(channelfuture future) throws exception {\n        if (!future.issuccess()) {\n            forceclose(child, future.cause());\n        }\n    }\n});\n\n\n客户端niosocketchannel向sub reactor group注册的流程完全和服务端nioserversocketchannel向main reactor group注册流程一样。\n\n> 关于服务端nioserversocketchannel的注册流程，笔者已经在?《详细图解netty reactor启动全流程》一文中做出了详细的介绍，对相关细节感兴趣的同学可以在回看下。\n\n这里笔者在带大家简要回顾下整个注册过程并着重区别对比客户端niosocetchannel与服务端nioserversocketchannel注册过程中不同的地方。\n\n\n# 7.1 从sub reactor group中选取一个sub reactor进行绑定\n\npublic abstract class multithreadeventloopgroup extends multithreadeventexecutorgroup implements eventloopgroup {\n\n   @override\n    public channelfuture register(channel channel) {\n        return next().register(channel);\n    }\n\n    @override\n    public eventexecutor next() {\n        return chooser.next();\n    }\n\n}\n\n\n\n# 7.2 向绑定的sub reactor上注册niosocketchannel\n\npublic abstract class singlethreadeventloop extends singlethreadeventexecutor implements eventloop {\n\n    @override\n    public channelfuture register(channel channel) {\n        //注册channel到绑定的reactor上\n        return register(new defaultchannelpromise(channel, this));\n    }\n\n    @override\n    public channelfuture register(final channelpromise promise) {\n        objectutil.checknotnull(promise, \"promise\");\n        //unsafe负责channel底层的各种操作\n        promise.channel().unsafe().register(this, promise);\n        return promise;\n    }\n\n}\n\n\n * 当时我们在介绍nioserversocketchannel的注册过程时，这里的promise.channel()为nioserversocketchannel。底层的unsafe操作类为niomessageunsafe。\n * 此时这里的promise.channel()为niosocketchannel。底层的unsafe操作类为niobyteunsafe。\n\n@override\npublic final void register(eventloop eventloop, final channelpromise promise) {\n    ..............省略....................\n        //此时这里的eventloop为sub reactor\n        abstractchannel.this.eventloop = eventloop;\n\n    /**\n      * 执行channel注册的操作必须是reactor线程来完成\n      *\n      * 1: 如果当前执行线程是reactor线程，则直接执行register0进行注册\n      * 2：如果当前执行线程是外部线程，则需要将register0注册操作 封装程异步task 由reactor线程执行\n      * */\n    if (eventloop.ineventloop()) {\n        register0(promise);\n    } else {\n        try {\n            eventloop.execute(new runnable() {\n                @override\n                public void run() {\n                    register0(promise);\n                }\n            });\n        } catch (throwable t) {\n            ..............省略....................\n        }\n    }\n}\n\n\n注意此时传递进来的eventloop eventloop为sub reactor。\n\n但此时的执行线程为main reactor线程，并不是sub reactor线程（此时还未启动）。\n\n所以这里的eventloop.ineventloop()返回的是false。\n\nimage.png\n\n在else分支中向绑定的sub reactor提交注册niosocketchannel的任务。\n\n> 当注册任务提交后，此时绑定的sub reactor线程启动。\n\n\n# 7.3 register0\n\n我们又来到了channel注册的老地方register0方法。在?《详细图解netty reactor启动全流程》中我们花了大量的篇幅介绍了这个方法。这里我们只对比niosocketchannel与nioserversocketchannel不同的地方。\n\nprivate void register0(channelpromise promise) {\n    try {\n        ................省略..................\n            boolean firstregistration = neverregistered;\n        //执行真正的注册操作\n        doregister();\n        //修改注册状态\n        neverregistered = false;\n        registered = true;\n\n        pipeline.invokehandleraddedifneeded();\n\n        if (isactive()) {\n            if (firstregistration) {\n                //触发channelactive事件\n                pipeline.firechannelactive();\n            } else if (config().isautoread()) {\n                beginread();\n            }\n        }\n    } catch (throwable t) {\n        ................省略..................\n    }\n}\n\n\n这里 doregister()方法将niosocketchannel注册到sub reactor中的selector上。\n\npublic abstract class abstractniochannel extends abstractchannel {\n\n    @override\n    protected void doregister() throws exception {\n        boolean selected = false;\n        for (;;) {\n            try {\n                selectionkey = javachannel().register(eventloop().unwrappedselector(), 0, this);\n                return;\n            } catch (cancelledkeyexception e) {\n                ...............省略...............\n            }\n        }\n    }\n\n}\n\n\n这里是netty客户端niosocketchannel与jdk nio 原生 socketchannel关联的地方。此时注册的io事件依然是0。目的也是只是为了获取niosocketchannel在selector中的selectionkey。\n\n同时通过selectablechannel#register方法将netty自定义的niosocketchannel（这里的this指针）附着在selectionkey的attechment属性上，完成netty自定义channel与jdk nio channel的关系绑定。这样在每次对selector进行io就绪事件轮询时，netty 都可以从 jdk nio selector返回的selectionkey中获取到自定义的channel对象（这里指的就是niosocketchannel）。\n\nchannel与selectionkey对应关系.png\n\n随后调用pipeline.invokehandleraddedifneeded()回调客户端niosocketchannel上pipeline中的所有channelhandler的handleradded方法，此时pipeline的结构中只有一个channelinitializer。最终会在channelinitializer#handleradded回调方法中初始化客户端niosocketchannel的pipeline。\n\n客户端channel pipeline初始结构.png\n\npublic abstract class channelinitializer<c extends channel> extends channelinboundhandleradapter {\n\n    @override\n    public void handleradded(channelhandlercontext ctx) throws exception {\n        if (ctx.channel().isregistered()) {\n            if (initchannel(ctx)) {\n                //初始化工作完成后，需要将自身从pipeline中移除\n                removestate(ctx);\n            }\n        }\n    }\n\n    protected abstract void initchannel(c ch) throws exception;\n}\n\n\n> 关于对channel中pipeline的详细初始化过程，对细节部分感兴趣的同学可以回看下?《详细图解netty reactor启动全流程》\n\n此时客户端niosocketchannel中的pipeline中的结构就变为了我们自定义的样子，在示例代码中我们自定义的channelhandler为echoserverhandler。\n\n客户端channel pipeline结构.png\n\n@sharable\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(channelhandlercontext ctx, object msg) {\n        ctx.write(msg);\n    }\n\n    @override\n    public void channelreadcomplete(channelhandlercontext ctx) {\n\n        ctx.flush();\n    }\n\n    @override\n    public void exceptioncaught(channelhandlercontext ctx, throwable cause) {\n        // close the connection when an exception is raised.\n        cause.printstacktrace();\n        ctx.close();\n    }\n}\n\n\n当客户端niosocketchannel中的pipeline初始化完毕后，netty就开始调用safesetsuccess(promise)方法回调regfuture中注册的channelfuturelistener，通知客户端niosocketchannel已经成功注册到sub reactor上了。\n\nchildgroup.register(child).addlistener(new channelfuturelistener() {\n    @override\n    public void operationcomplete(channelfuture future) throws exception {\n        if (!future.issuccess()) {\n            forceclose(child, future.cause());\n        }\n    }\n});\n\n\n> 在服务端nioserversocketchannel注册的时候我们会在listener中向main reactor提交bind绑定端口地址任务。但是在niosocketchannel注册的时候，只会在listener中处理一下注册失败的情况。\n\n当sub reactor线程通知channelfuturelistener注册成功之后，随后就会调用pipeline.firechannelregistered()在客户端niosocketchannel的pipeline中传播channelregistered事件。\n\n传播channelregister事件.png\n\n这里笔者重点要强调下，在之前介绍nioserversocketchannel注册的时候，我们提到因为此时nioserversocketchannel并未绑定端口地址，所以这时的nioserversocketchannel并未激活，这里的isactive()返回false。register0方法直接返回。\n\n> 服务端nioserversocketchannel判断是否激活的标准为端口是否绑定成功。\n\npublic class nioserversocketchannel extends abstractniomessagechannel\n                             implements io.netty.channel.socket.serversocketchannel {\n    @override\n    public boolean isactive() {\n        return isopen() && javachannel().socket().isbound();\n    }\n}\n\n\n> 客户端niosocketchannel判断是否激活的标准为是否处于connected状态。那么显然这里肯定是处于connected状态的。\n\n@override\npublic boolean isactive() {\n    socketchannel ch = javachannel();\n    return ch.isopen() && ch.isconnected();\n}\n\n\nniosocketchannel已经处于connected状态，这里并不需要绑定端口，所以这里的isactive()返回true。\n\nif (isactive()) {\n    /**\n     * 客户端socketchannel注册成功后会走这里，在channelactive事件回调中注册op_read事件\n     * */\n    if (firstregistration) {\n        //触发channelactive事件\n        pipeline.firechannelactive();\n    } else if (config().isautoread()) {\n        .......省略..........\n    }\n\n}\n\n\n最后调用pipeline.firechannelactive()在niosocketchannel中的pipeline传播channelactive事件，最终在pipeline的头结点headcontext中响应并注册op_read事件到sub reactor中的selector上。\n\n传播channelactive事件.png\n\npublic abstract class abstractniochannel extends abstractchannel { {\n\n    @override\n    protected void dobeginread() throws exception {\n        ..............省略................\n\n        final int interestops = selectionkey.interestops();\n        /**\n         * 1：serversocketchannel 初始化时 readinterestop设置的是op_accept事件\n         * 2：socketchannel 初始化时 readinterestop设置的是op_read事件\n         * */\n        if ((interestops & readinterestop) == 0) {\n            //注册监听op_accept或者op_read事件\n            selectionkey.interestops(interestops | readinterestop);\n        }\n    }\n\n}\n\n\n> 注意这里的readinterestop为客户端niosocketchannel在初始化时设置的op_read事件。\n\n----------------------------------------\n\n到这里，netty中的main reactor接收连接的整个流程，我们就介绍完了，此时netty中主从reactor组的结构就变为：\n\n主从reactor组完整结构.png\n\n\n# 总结\n\n本文我们介绍了nioserversocketchannel处理客户端连接事件的整个过程。\n\n * 接收连接的整个处理框架。\n * 影响netty接收连接吞吐的bug产生的原因，以及修复的方案。\n * 创建并初始化客户端niosocketchannel。\n * 初始化niosocketchannel中的pipeline。\n * 客户端niosocketchannel向sub reactor注册的过程\n\n其中我们也对比了nioserversocketchannel与niosocketchannel在创建初始化以及后面向reactor注册过程中的差异之处。\n\n当客户端niosocketchannel接收完毕并向sub reactor注册成功后，那么接下来sub reactor就开始监听注册其上的所有客户端niosocketchannel的op_read事件，并等待客户端向服务端发送网络数据。\n\n后面reactor的主角就该变为sub reactor以及注册在其上的客户端niosocketchannel了。\n\n下篇文章，我们将会讨论netty是如何接收网络数据的~~~~ 我们下篇文章见~~\n\n\n# 参考资料\n\n抓到netty一个bug，顺带来透彻地聊一下netty是如何高效接收网络连接的 (qq.com)",charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"第五课：Netty 如何高效接收网络数据",frontmatter:{title:"第五课：Netty 如何高效接收网络数据",date:"2024-09-19T11:11:04.000Z",permalink:"/pages/0938c1/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/25.%E5%9B%9B%E3%80%81%E6%B7%B1%E5%85%A5%20Netty%20%E6%A0%B8%E5%BF%83/18.%E7%AC%AC%E4%BA%94%E8%AF%BE%EF%BC%9ANetty%20%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E6%8E%A5%E6%94%B6%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE.html",relativePath:"Netty 系统设计/25.四、深入 Netty 核心/18.第五课：Netty 如何高效接收网络数据.md",key:"v-a7130124",path:"/pages/0938c1/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:3,title:"前文回顾",slug:"前文回顾",normalizedTitle:"前文回顾",charIndex:11},{level:2,title:"1. Sub Reactor处理OP_READ事件流程总览",slug:"_1-sub-reactor处理op-read事件流程总览",normalizedTitle:"1. sub reactor处理op_read事件流程总览",charIndex:1085},{level:2,title:"2. Netty接收网络数据流程总览",slug:"_2-netty接收网络数据流程总览",normalizedTitle:"2. netty接收网络数据流程总览",charIndex:3240},{level:3,title:"2.1 ChannelRead与ChannelReadComplete事件的区别",slug:"_2-1-channelread与channelreadcomplete事件的区别",normalizedTitle:"2.1 channelread与channelreadcomplete事件的区别",charIndex:7754},{level:2,title:"3. 源码核心框架总览",slug:"_3-源码核心框架总览",normalizedTitle:"3. 源码核心框架总览",charIndex:8934},{level:3,title:"3.1 分配DirectByteBuffer接收网络数据",slug:"_3-1-分配directbytebuffer接收网络数据",normalizedTitle:"3.1 分配directbytebuffer接收网络数据",charIndex:11835},{level:3,title:"3.2 从NioSocketChannel中读取数据",slug:"_3-2-从niosocketchannel中读取数据",normalizedTitle:"3.2 从niosocketchannel中读取数据",charIndex:19208},{level:2,title:"4. ByteBuffer动态自适应扩缩容机制",slug:"_4-bytebuffer动态自适应扩缩容机制",normalizedTitle:"4. bytebuffer动态自适应扩缩容机制",charIndex:19784},{level:3,title:"4.1 AdaptiveRecvByteBufAllocator的创建",slug:"_4-1-adaptiverecvbytebufallocator的创建",normalizedTitle:"4.1 adaptiverecvbytebufallocator的创建",charIndex:20235},{level:3,title:"4.2 AdaptiveRecvByteBufAllocator类的初始化",slug:"_4-2-adaptiverecvbytebufallocator类的初始化",normalizedTitle:"4.2 adaptiverecvbytebufallocator类的初始化",charIndex:21218},{level:3,title:"4.3 扩缩容逻辑",slug:"_4-3-扩缩容逻辑",normalizedTitle:"4.3 扩缩容逻辑",charIndex:23480},{level:4,title:"4.3.1 扩容",slug:"_4-3-1-扩容",normalizedTitle:"4.3.1 扩容",charIndex:23757},{level:4,title:"4.3.1  缩容",slug:"_4-3-1-缩容",normalizedTitle:"4.3.1  缩容",charIndex:null},{level:3,title:"4.4 扩缩容时机",slug:"_4-4-扩缩容时机",normalizedTitle:"4.4 扩缩容时机",charIndex:24100},{level:4,title:"4.4.1 缩容",slug:"_4-4-1-缩容",normalizedTitle:"4.4.1 缩容",charIndex:25886},{level:4,title:"4.4.2 扩容",slug:"_4-4-2-扩容",normalizedTitle:"4.4.2 扩容",charIndex:26462},{level:3,title:"4.5 AdaptiveRecvByteBufAllocator类的实例化",slug:"_4-5-adaptiverecvbytebufallocator类的实例化",normalizedTitle:"4.5 adaptiverecvbytebufallocator类的实例化",charIndex:26692},{level:4,title:"4.5.1 二分查找容量索引下标",slug:"_4-5-1-二分查找容量索引下标",normalizedTitle:"4.5.1 二分查找容量索引下标",charIndex:28524},{level:3,title:"4.6 RecvByteBufAllocator.Handle",slug:"_4-6-recvbytebufallocator-handle",normalizedTitle:"4.6 recvbytebufallocator.handle",charIndex:29332},{level:2,title:"5. 使用堆外内存为ByteBuffer分配内存",slug:"_5-使用堆外内存为bytebuffer分配内存",normalizedTitle:"5. 使用堆外内存为bytebuffer分配内存",charIndex:31289},{level:3,title:"5.1 类名前缀Pooled的来历",slug:"_5-1-类名前缀pooled的来历",normalizedTitle:"5.1 类名前缀pooled的来历",charIndex:31419},{level:3,title:"5.2 PooledByteBufAllocator的创建",slug:"_5-2-pooledbytebufallocator的创建",normalizedTitle:"5.2 pooledbytebufallocator的创建",charIndex:32722},{level:4,title:"创建时机",slug:"创建时机",normalizedTitle:"创建时机",charIndex:32755},{level:4,title:"创建过程",slug:"创建过程",normalizedTitle:"创建过程",charIndex:29452},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:34845},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:35702}],headersStr:"前言 前文回顾 1. Sub Reactor处理OP_READ事件流程总览 2. Netty接收网络数据流程总览 2.1 ChannelRead与ChannelReadComplete事件的区别 3. 源码核心框架总览 3.1 分配DirectByteBuffer接收网络数据 3.2 从NioSocketChannel中读取数据 4. ByteBuffer动态自适应扩缩容机制 4.1 AdaptiveRecvByteBufAllocator的创建 4.2 AdaptiveRecvByteBufAllocator类的初始化 4.3 扩缩容逻辑 4.3.1 扩容 4.3.1  缩容 4.4 扩缩容时机 4.4.1 缩容 4.4.2 扩容 4.5 AdaptiveRecvByteBufAllocator类的实例化 4.5.1 二分查找容量索引下标 4.6 RecvByteBufAllocator.Handle 5. 使用堆外内存为ByteBuffer分配内存 5.1 类名前缀Pooled的来历 5.2 PooledByteBufAllocator的创建 创建时机 创建过程 总结 参考资料",content:"# 前言\n\n\n\n\n# 前文回顾\n\n在前边的系列文章中，我们从内核如何收发网络数据开始以一个 C10K 的问题作为主线详细从内核角度阐述了网络IO模型的演变，最终在此基础上引出了Netty的网络IO模型如下图所示：\n\n\n\n> 详细内容可回看?《从内核角度看IO模型的演变》\n\n后续我们又围绕着Netty的主从Reactor网络IO线程模型，在?《Reactor模型在Netty中的实现》一文中详细阐述了Netty的主从Reactor模型的创建，以及介绍了Reactor模型的关键组件。搭建了Netty的核心骨架如下图所示：\n\n\n\n在核心骨架搭建完毕之后，我们随后又在?《详细图解Reactor启动全流程》一文中阐述了Reactor启动的全流程，一个非常重要的核心组件NioServerSocketChannel开始在这里初次亮相，承担着一个网络框架最重要的任务--高效接收网络连接。我们介绍了NioServerSocketChannel的创建，初始化，向Main Reactor注册并监听OP_ACCEPT事件的整个流程。在此基础上，Netty得以整装待发，枕戈待旦开始迎接海量的客户端连接。\n\n\n\n随后紧接着我们在?《Netty如何高效接收网络连接》一文中详细介绍了Netty高效接收客户端网络连接的全流程，在这里Netty的核心重要组件NioServerSocketChannel开始正是登场，在NioServerSocketChannel中我们创建了客户端连接NioSocketChannel，并详细介绍了NioSocketChannel的初始化过程，随后通过在NioServerSocketChannel的pipeline中触发ChannelRead事件，并最终在ServerBootstrapAcceptor中将客户端连接NioSocketChannel注册到Sub Reactor中开始监听客户端连接上的OP_READ事件，准备接收客户端发送的网络数据也就是本文的主题内容。\n\n\n\n自此Netty的核心组件全部就绪并启动完毕，开始起飞~~~\n\n\n\n之前文章中的主角是Netty中主Reactor组中的Main Reactor以及注册在Main Reactor上边的NioServerSocketChannel，那么从本文开始，我们文章中的主角就切换为Sub Reactor以及注册在SubReactor上的NioSocketChannel了。\n\n下面就让我们正式进入今天的主题，看一下Netty是如何处理OP_READ事件以及如何高效接收网络数据的。\n\n\n# 1. Sub Reactor处理OP_READ事件流程总览\n\n\n\n客户端发起系统IO调用向服务端发送数据之后，当网络数据到达服务端的网卡并经过内核协议栈的处理，最终数据到达Socket的接收缓冲区之后，Sub Reactor轮询到NioSocketChannel上的 OP_READ事件 就绪，随后 Sub Reactor 线程就会从 JDK Selector 上的阻塞轮询APIselector.select(timeoutMillis)调用中返回。转而去处理NioSocketChannel上的OP_READ事件。\n\n> 注意这里的Reactor为负责处理客户端连接的Sub Reactor。连接的类型为NioSocketChannel，处理的事件为OP_READ事件。\n\n在之前的文章中笔者已经多次强调过了，Reactor在处理Channel上的IO事件入口函数为NioEventLoop#processSelectedKey。\n\npublic final class NioEventLoop extends SingleThreadEventLoop {\n\n    private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {\n        final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();\n        ..............省略.................\n\n        try {\n            int readyOps = k.readyOps();\n\n            if ((readyOps & SelectionKey.OP_CONNECT) != 0) {\n               ..............处理OP_CONNECT事件.................\n            }\n\n\n            if ((readyOps & SelectionKey.OP_WRITE) != 0) {\n              ..............处理OP_WRITE事件.................\n            }\n\n\n            if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {\n                //本文重点处理OP_ACCEPT事件\n                unsafe.read();\n            }\n        } catch (CancelledKeyException ignored) {\n            unsafe.close(unsafe.voidPromise());\n        }\n    }\n\n}\n\n\n这里需要重点强调的是，当前的执行线程现在已经变成了 Sub Reactor，而 Sub Reactor 上注册的正是 netty 客户端 NioSocketChannel负责处理连接上的读写事件。\n\n所以这里入口函数的参数AbstractNioChannel ch则是IO就绪的客户端连接NioSocketChannel。\n\n开头通过ch.unsafe()获取到的NioUnsafe操作类正是NioSocketChannel中对底层JDK NIO SocketChannel的Unsafe底层操作类。实现类型为NioByteUnsafe定义在下图继承结构中的AbstractNioByteChannel父类中。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n下面我们到NioByteUnsafe#read方法中来看下Netty对OP_READ事件的具体处理过程：\n\n\n# 2. Netty接收网络数据流程总览\n\n我们直接按照老规矩，先从整体上把整个OP_READ事件的逻辑处理框架提取出来，让大家先总体俯视下流程全貌，然后在针对每个核心点位进行各个击破。\n\n\n\n> 流程中相关置灰的步骤为Netty处理连接关闭时的逻辑，和本文主旨无关，我们这里暂时忽略，等后续笔者介绍连接关闭时，会单独开一篇文章详细为大家介绍。\n\n从上面这张Netty接收网络数据总体流程图可以看出NioSocketChannel在接收网络数据的整个流程和我们在上篇文章?《Netty如何高效接收网络连接》中介绍的NioServerSocketChannel在接收客户端连接时的流程在总体框架上是一样的。\n\nNioSocketChannel在接收网络数据的过程处理中，也是通过在一个do{....}while(...)循环read loop中不断的循环读取连接NioSocketChannel上的数据。\n\n同样在NioSocketChannel读取连接数据的read loop中也是受最大读取次数的限制。默认配置最多只能读取16次，超过16次无论此时NioSocketChannel中是否还有数据可读都不能在进行读取了。\n\n这里read loop循环最大读取次数可在启动配置类ServerBootstrap中通过ChannelOption.MAX_MESSAGES_PER_READ选项设置，默认为16。\n\nServerBootstrap b = new ServerBootstrap();\nb.group(bossGroup, workerGroup)\n  .channel(NioServerSocketChannel.class)\n  .option(ChannelOption.MAX_MESSAGES_PER_READ, 自定义次数)\n\n\n**Netty这里为什么非得限制read loop的最大读取次数呢？**为什么不在 read loop 中一次性把数据读取完呢？\n\n这时候就是考验我们大局观的时候了，在前边的文章介绍中我们提到Netty的IO模型为主从Reactor线程组模型，在Sub Reactor Group中包含了多个Sub Reactor专门用于监听处理客户端连接上的IO事件。\n\n为了能够高效有序的处理全量客户端连接上的读写事件，Netty将服务端承载的全量客户端连接分摊到多个Sub Reactor中处理，同时也能保证Channel上IO处理的线程安全性。\n\n其中一个Channel只能分配给一个固定的Reactor。一个Reactor负责处理多个Channel上的IO就绪事件，Reactor与Channel之间的对应关系如下图所示：\n\n\n\n而一个 Sub Reactor上注册了多个NioSocketChannel，Netty 不可能在一个 NioSocketChannel 上无限制的处理下去，要将读取数据的机会均匀分摊给其他 NioSocketChannel，所以需要限定每个 NioSocketChannel 上的最大读取次数。\n\n此外，Sub Reactor除了需要监听处理所有注册在它上边的NioSocketChannel中的IO就绪事件之外，还需要腾出事件来处理有用户线程提交过来的异步任务。从这一点看，Netty也不会一直停留在NioSocketChannel的IO处理上。所以限制read loop的最大读取次数是非常必要的。\n\n> 关于Reactor的整体运转架构，对细节部分感兴趣的同学可以回看下笔者的?《一文聊透Netty核心引擎Reactor的运转架构》这篇文章。\n\n所以基于这个原因，我们需要在read loop循环中，每当通过doReadBytes方法从NioSocketChannel中读取到数据时（方法返回值会大于0，并记录在allocHandle.lastBytesRead中），都需要通过allocHandle.incMessagesRead(1)方法统计已经读取的次数。当达到16次时不管NioSocketChannel是否还有数据可读，都需要在read loop末尾退出循环。转去执行Sub Reactor上的异步任务。以及其他NioSocketChannel上的IO就绪事件。平均分配，雨露均沾！！\n\npublic abstract class MaxMessageHandle implements ExtendedHandle {\n\n        //read loop总共读取了多少次\n        private int totalMessages;\n\n       @Override\n        public final void incMessagesRead(int amt) {\n            totalMessages += amt;\n        }\n\n}\n\n\n本次 read loop 读取到的数据大小会记录在allocHandle.lastBytesRead中\n\npublic abstract class MaxMessageHandle implements ExtendedHandle {\n\n         //本次read loop读取到的字节数\n        private int lastBytesRead;\n        //整个read loop循环总共读取的字节数\n        private int totalBytesRead;\n\n        @Override\n        public void lastBytesRead(int bytes) {\n            lastBytesRead = bytes;\n            if (bytes > 0) {\n                totalBytesRead += bytes;\n            }\n        }\n}\n\n\n * lastBytesRead < 0：表示客户端主动发起了连接关闭流程，Netty开始连接关闭处理流程。这个和本文的主旨无关，我们先不用管。后面笔者会专门用一篇文章来详解关闭流程。\n * lastBytesRead = 0：表示当前NioSocketChannel上的数据已经全部读取完毕，没有数据可读了。本次OP_READ事件圆满处理完毕，可以开开心心的退出read loop。\n * 当lastBytesRead > 0：表示在本次read loop中从NioSocketChannel中读取到了数据，会在NioSocketChannel的pipeline中触发ChannelRead事件。进而在pipeline中负责IO处理的ChannelHandelr中响应，处理网络请求。\n\nfir\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(ChannelHandlerContext ctx, Object msg) {\n          .......处理网络请求，比如解码,反序列化等操作.......\n    }\n}\n\n\n最后会在read loop循环的末尾调用allocHandle.continueReading()判断是否结束本次read loop循环。这里的结束循环条件的判断会比我们在介绍NioServerSocketChannel接收连接时的判断条件复杂很多，笔者会将这个判断条件的详细解析放在文章后面细节部分为大家解读，这里大家只需要把握总体核心流程，不需要关注太多细节。\n\n总体上在NioSocketChannel中读取网络数据的read loop循环结束条件需要满足以下几点：\n\n * 当前NioSocketChannel中的数据已经全部读取完毕，则退出循环。\n * 本轮read loop如果没有读到任何数据，则退出循环。\n * read loop的读取次数达到16次，退出循环。\n\n当满足这里的read loop退出条件之后，Sub Reactor线程就会退出循环，随后会调用allocHandle.readComplete()方法根据本轮read loop总共读取到的字节数totalBytesRead来决定是否对用于接收下一轮OP_READ事件数据的ByteBuffer进行扩容或者缩容。\n\n最后在NioSocketChannel的pipeline中触发ChannelReadComplete事件，通知ChannelHandler本次OP_READ事件已经处理完毕。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)fireChannelReadComplete.png\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(ChannelHandlerContext ctx, Object msg) {\n       .......处理网络请求，比如解码,反序列化等操作.......\n    }\n\n    @Override\n    public void channelReadComplete(ChannelHandlerContext ctx) {\n        ......本次OP_READ事件处理完毕.......\n        ......决定是否向客户端响应处理结果......\n    }\n}\n\n\n\n# 2.1 ChannelRead与ChannelReadComplete事件的区别\n\n> 有些小伙伴可能对Netty中的一些传播事件触发的时机，或者事件之间的区别理解的不是很清楚，概念容易混淆。在后面的文章中笔者也会从源码的角度出发给大家说清楚Netty中定义的所有异步事件，以及这些事件之间的区别和联系和触发时机，传播机制。\n\n这里我们主要探讨本文主题中涉及到的两个事件：ChannelRead事件与ChannelReadComplete事件。\n\n从上述介绍的Netty接收网络数据流程总览中我们可以看出ChannelRead事件和ChannelReadComplete事件是不一样的，但是对于刚接触Netty的小伙伴来说从命名上乍一看感觉又差不多。\n\n下面我们来看这两个事件之间的差别：\n\nNetty服务端对于一次OP_READ事件的处理，会在一个do{}while()循环read loop中分多次从客户端NioSocketChannel中读取网络数据。每次读取我们分配的ByteBuffer容量大小，初始容量为2048。\n\n * ChanneRead事件：一次循环读取一次数据，就触发一次ChannelRead事件。本次最多读取在read loop循环开始分配的DirectByteBuffer容量大小。这个容量会动态调整，文章后续笔者会详细介绍。\n * ChannelReadComplete事件：当读取不到数据或者不满足continueReading的任意一个条件就会退出read loop，这时就会触发ChannelReadComplete事件。表示本次OP_READ事件处理完毕。\n\n> 这里需要特别注意下触发ChannelReadComplete事件并不代表NioSocketChannel中的数据已经读取完了，只能说明本次OP_READ事件处理完毕。因为有可能是客户端发送的数据太多，Netty读了16次还没读完，那就只能等到下次OP_READ事件到来的时候在进行读取了。\n\n----------------------------------------\n\n以上内容就是Netty在接收客户端发送网络数据的全部核心逻辑。目前为止我们还未涉及到这部分的主干核心源码，笔者想的是先给大家把核心逻辑讲解清楚之后，这样理解起来核心主干源码会更加清晰透彻。\n\n经过前边对网络数据接收的核心逻辑介绍，笔者在把这张流程图放出来，大家可以结合这张图在来回想下主干核心逻辑。\n\n\n\n下面笔者会结合这张流程图，给大家把这部分的核心主干源码框架展现出来，大家可以将我们介绍过的核心逻辑与主干源码做个一一对应，还是那句老话，我们要从主干框架层面把握整体处理流程，不需要读懂每一行代码，文章后续笔者会将这个过程中涉及到的核心点位给大家拆开来各个击破！！\n\n\n\n\n# 3. 源码核心框架总览\n\n@Override\npublic final void read() {\n    final ChannelConfig config = config();\n\n    ...............处理半关闭相关代码省略...............\n        //获取NioSocketChannel的pipeline\n        final ChannelPipeline pipeline = pipeline();\n    //PooledByteBufAllocator 具体用于实际分配ByteBuf的分配器\n    final ByteBufAllocator allocator = config.getAllocator();\n    //自适应ByteBuf分配器 AdaptiveRecvByteBufAllocator ,用于动态调节ByteBuf容量\n    //需要与具体的ByteBuf分配器配合使用 比如这里的PooledByteBufAllocator\n    final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle();\n    //allocHandler用于统计每次读取数据的大小，方便下次分配合适大小的ByteBuf\n    //重置清除上次的统计指标\n    allocHandle.reset(config);\n\n    ByteBuf byteBuf = null;\n    boolean close = false;\n    try {\n        do {\n            //利用PooledByteBufAllocator分配合适大小的byteBuf 初始大小为2048\n            byteBuf = allocHandle.allocate(allocator);\n            //记录本次读取了多少字节数\n            allocHandle.lastBytesRead(doReadBytes(byteBuf));\n            //如果本次没有读取到任何字节，则退出循环 进行下一轮事件轮询\n            if (allocHandle.lastBytesRead() <= 0) {\n                // nothing was read. release the buffer.\n                byteBuf.release();\n                byteBuf = null;\n                close = allocHandle.lastBytesRead() < 0;\n                if (close) {\n                    ......表示客户端发起连接关闭.....\n                }\n                break;\n            }\n\n            //read loop读取数据次数+1\n            allocHandle.incMessagesRead(1);\n            //客户端NioSocketChannel的pipeline中触发ChannelRead事件\n            pipeline.fireChannelRead(byteBuf);\n            //解除本次读取数据分配的ByteBuffer引用，方便下一轮read loop分配\n            byteBuf = null;\n        } while (allocHandle.continueReading());//判断是否应该继续read loop\n\n        //根据本次read loop总共读取的字节数，决定下次是否扩容或者缩容\n        allocHandle.readComplete();\n        //在NioSocketChannel的pipeline中触发ChannelReadComplete事件，表示一次read事件处理完毕\n        //但这并不表示 客户端发送来的数据已经全部读完，因为如果数据太多的话，这里只会读取16次，剩下的会等到下次read事件到来后在处理\n        pipeline.fireChannelReadComplete();\n\n        .........省略连接关闭流程处理.........\n    } catch (Throwable t) {\n        ...............省略...............\n    } finally {\n        ...............省略...............\n    }\n}\n}\n\n\n> 这里再次强调下当前执行线程为Sub Reactor线程，处理连接数据读取逻辑是在NioSocketChannel中。\n\n首先通过config()获取客户端NioSocketChannel的Channel配置类NioSocketChannelConfig。\n\n通过pipeline()获取NioSocketChannel的pipeline。我们在 ?《详细图解Netty Reactor启动全流程》一文中提到的Netty服务端模板所举的示例中，NioSocketChannelde pipeline中只有一个EchoChannelHandler。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)客户端channel pipeline结构.png\n\n\n# 3.1 分配DirectByteBuffer接收网络数据\n\nSub Reactor 在接收NioSocketChannel上的IO数据时，都会分配一个 ByteBuffer 用来存放接收到的IO数据。\n\n这里大家可能觉得比较奇怪，为什么在NioSocketChannel接收数据这里会有两个ByteBuffer分配器呢？一个是ByteBufAllocator，另一个是RecvByteBufAllocator。\n\nfinal ByteBufAllocator allocator = config.getAllocator();\nfinal RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle();\n\n\n这两个ByteBuffer又有什么区别和联系呢？\n\n在上篇文章?《抓到Netty一个Bug，顺带来透彻地聊一下Netty是如何高效接收网络连接》中，笔者为了阐述上篇文章中提到的Netty在接收网络连接时的 Bug 时，简单和大家介绍了下这个 RecvByteBufAllocator。\n\n在上篇文章提到的NioServerSocketChannelConfig中，这里的RecvByteBufAllocator类型为ServerChannelRecvByteBufAllocator。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n> 还记得这个ServerChannelRecvByteBufAllocator类型在4.1.69.final版本引入是为了解决笔者在上篇文章中提到的那个Bug吗？在4.1.69.final版本之前，NioServerSocketChannelConfig中的RecvByteBufAllocator类型为AdaptiveRecvByteBufAllocator。\n\n而在本文中NioSocketChannelConfig中的RecvByteBufAllocator类型为AdaptiveRecvByteBufAllocator。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n所以这里recvBufAllocHandle()获得到的RecvByteBufAllocator为AdaptiveRecvByteBufAllocator。顾名思义，这个类型的RecvByteBufAllocator可以根据NioSocketChannel上每次到来的IO数据大小来自适应动态调整ByteBuffer的容量。\n\n对于客户端NioSocketChannel来说，它里边包含的IO数据时客户端发送来的网络数据，长度是不定的，所以才会需要这样一个可以根据每次IO数据的大小来自适应动态调整容量的ByteBuffer来接收。\n\n如果我们把用于接收数据用的ByteBuffer看做一个桶的话，那么小数据用大桶装或者大数据用小桶装肯定是不合适的，所以我们需要根据接收数据的大小来动态调整桶的容量。而AdaptiveRecvByteBufAllocator的作用正是用来根据每次接收数据的容量大小来动态调整ByteBuffer的容量的。\n\n现在RecvByteBufAllocator笔者为大家解释清楚了，接下来我们继续看ByteBufAllocator。\n\n> 大家这里需要注意的是AdaptiveRecvByteBufAllocator并不会真正的去分配ByteBuffer，它只是负责动态调整分配ByteBuffer的大小。\n\n而真正具体执行内存分配动作的是这里的ByteBufAllocator类型为PooledByteBufAllocator。它会根据AdaptiveRecvByteBufAllocator动态调整出来的大小去真正的申请内存分配ByteBuffer。\n\n> PooledByteBufAllocator为Netty中的内存池，用来管理堆外内存DirectByteBuffer。\n\nAdaptiveRecvByteBufAllocator中的allocHandle在上篇文章中我们也介绍过了，它的实际类型为MaxMessageHandle。\n\npublic class AdaptiveRecvByteBufAllocator extends DefaultMaxMessagesRecvByteBufAllocator {\n\n    @Override\n    public Handle newHandle() {\n        return new HandleImpl(minIndex, maxIndex, initial);\n    }\n    \n    private final class HandleImpl extends MaxMessageHandle {\n                  .................省略................\n    }\n}\n\n\n在MaxMessageHandle中包含了用于动态调整ByteBuffer容量的统计指标。\n\n   public abstract class MaxMessageHandle implements ExtendedHandle {\n        private ChannelConfig config;\n\n        //用于控制每次read loop里最大可以循环读取的次数，默认为16次\n        //可在启动配置类ServerBootstrap中通过ChannelOption.MAX_MESSAGES_PER_READ选项设置。\n        private int maxMessagePerRead;\n\n        //用于统计read loop中总共接收的连接个数，NioSocketChannel中表示读取数据的次数\n        //每次read loop循环后会调用allocHandle.incMessagesRead增加记录接收到的连接个数\n        private int totalMessages;\n\n        //用于统计在read loop中总共接收到客户端连接上的数据大小\n        private int totalBytesRead;\n\n        //表示本次read loop 尝试读取多少字节，byteBuffer剩余可写的字节数\n        private int attemptedBytesRead;\n\n        //本次read loop读取到的字节数\n        private int lastBytesRead;\n        \n        //预计下一次分配buffer的容量，初始：2048\n        private int nextReceiveBufferSize;\n        ...........省略.............\n}\n\n\n在每轮read loop开始之前，都会调用allocHandle.reset(config)重置清空上一轮read loop的统计指标。\n\n@Override\npublic void reset(ChannelConfig config) {\n    this.config = config;\n    //默认每次最多读取16次\n    maxMessagePerRead = maxMessagesPerRead();\n    totalMessages = totalBytesRead = 0;\n}\n\n\n在每次开始从NioSocketChannel中读取数据之前，需要利用PooledByteBufAllocator在内存池中为ByteBuffer分配内存，默认初始化大小为2048，这个容量由guess()方法决定。\n\nbyteBuf = allocHandle.allocate(allocator);\n@Override\npublic ByteBuf allocate(ByteBufAllocator alloc) {\n    return alloc.ioBuffer(guess());\n}\n\n@Override\npublic int guess() {\n    //预计下一次分配buffer的容量，一开始为2048\n    return nextReceiveBufferSize;\n}\n\n\n在每次通过doReadBytes从NioSocketChannel中读取到数据后，都会调用allocHandle.lastBytesRead(doReadBytes(byteBuf))记录本次读取了多少字节数据，并统计本轮read loop目前总共读取了多少字节。\n\n@Override\npublic void lastBytesRead(int bytes) {\n    lastBytesRead = bytes;\n    if (bytes > 0) {\n        totalBytesRead += bytes;\n    }\n}\n\n\n每次循环从NioSocketChannel中读取数据之后，都会调用allocHandle.incMessagesRead(1)。统计当前已经读取了多少次。如果超过了最大读取限制此时16次，就需要退出read loop。去处理其他NioSocketChannel上的IO事件。\n\n@Override\npublic final void incMessagesRead(int amt) {\n    totalMessages += amt;\n}\n\n\n在每次read loop循环的末尾都需要通过调用allocHandle.continueReading()来判断是否继续read loop循环读取NioSocketChannel中的数据。\n\n@Override\npublic boolean continueReading() {\n    return continueReading(defaultMaybeMoreSupplier);\n}\n\nprivate final UncheckedBooleanSupplier defaultMaybeMoreSupplier = new UncheckedBooleanSupplier() {\n    @Override\n    public boolean get() {\n        //判断本次读取byteBuffer是否满载而归\n        return attemptedBytesRead == lastBytesRead;\n    }\n};\n\n@Override\npublic boolean continueReading(UncheckedBooleanSupplier maybeMoreDataSupplier) {\n    return config.isAutoRead() &&\n        (!respectMaybeMoreData || maybeMoreDataSupplier.get()) &&\n        totalMessages < maxMessagePerRead &&\n        totalBytesRead > 0;\n}\n\n\n * attemptedBytesRead :表示当前ByteBuffer预计尝试要写入的字节数。\n * lastBytesRead :表示本次read loop真实读取到了多少个字节。\n\ndefaultMaybeMoreSupplier用于判断经过本次read loop读取数据后，ByteBuffer是否满载而归。如果是满载而归的话（attemptedBytesRead == lastBytesRead），表明可能NioSocketChannel里还有数据。如果不是满载而归，表明NioSocketChannel里没有数据了已经。\n\n是否继续进行read loop需要同时满足以下几个条件：\n\n * totalMessages < maxMessagePerRead 当前读取次数是否已经超过16次，如果超过，就退出do(...)while()循环。进行下一轮OP_READ事件的轮询。因为每个Sub Reactor管理了多个NioSocketChannel，不能在一个NioSocketChannel上占用太多时间，要将机会均匀地分配给Sub Reactor所管理的所有NioSocketChannel。\n\n * totalBytesRead > 0 本次OP_READ事件处理是否读取到了数据，如果已经没有数据可读了，那么就直接退出read loop。\n\n * !respectMaybeMoreData || maybeMoreDataSupplier.get() 这个条件比较复杂，它其实就是通过respectMaybeMoreData字段来控制NioSocketChannel中可能还有数据可读的情况下该如何处理。\n\n * * maybeMoreDataSupplier.get()：true表示本次读取从NioSocketChannel中读取数据，ByteBuffer满载而归。说明可能NioSocketChannel中还有数据没读完。fasle表示ByteBuffer还没有装满，说明NioSocketChannel中已经没有数据可读了。\n   * respectMaybeMoreData = true表示要对可能还有更多数据进行处理的这种情况要respect认真对待,如果本次循环读取到的数据已经装满ByteBuffer，表示后面可能还有数据，那么就要进行读取。如果ByteBuffer还没装满表示已经没有数据可读了那么就退出循环。![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)\n   * respectMaybeMoreData = false表示对可能还有更多数据的这种情况不认真对待 not respect。不管本次循环读取数据ByteBuffer是否满载而归，都要继续进行读取，直到读取不到数据在退出循环，属于无脑读取。\n\n同时满足以上三个条件，那么read loop继续进行。继续从NioSocketChannel中读取数据，直到读取不到或者不满足三个条件中的任意一个为止。\n\n\n# 3.2 从NioSocketChannel中读取数据\n\npublic class NioSocketChannel extends AbstractNioByteChannel implements io.netty.channel.socket.SocketChannel {\n\n    @Override\n    protected int doReadBytes(ByteBuf byteBuf) throws Exception {\n        final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle();\n        allocHandle.attemptedBytesRead(byteBuf.writableBytes());    \n        return byteBuf.writeBytes(javaChannel(), allocHandle.attemptedBytesRead());\n    }\n}\n\n\n这里会直接调用底层JDK NIO的SocketChannel#read方法将数据读取到DirectByteBuffer中。读取数据大小为本次分配的DirectByteBuffer容量，初始为2048。\n\n\n# 4. ByteBuffer动态自适应扩缩容机制\n\n由于我们一开始并不知道客户端会发送多大的网络数据，所以这里先利用PooledByteBufAllocator分配一个初始容量为2048的DirectByteBuffer用于接收数据。\n\nbyteBuf = allocHandle.allocate(allocator);\n\n\n这就好比我们需要拿着一个桶去排队装水，但是第一次去装的时候，我们并不知道管理员会给我们分配多少水，桶拿大了也不合适拿小了也不合适，于是我们就先预估一个差不多容量大小的桶，如果分配的多了，我们下次就拿更大一点的桶，如果分配少了，下次我们就拿一个小点的桶。\n\n在这种场景下，我们需要ByteBuffer可以自动根据每次网络数据的大小来动态自适应调整自己的容量。\n\n而ByteBuffer动态自适应扩缩容机制依赖于AdaptiveRecvByteBufAllocator类的实现。让我们先回到AdaptiveRecvByteBufAllocator类的创建起点开始说起~~\n\n\n# 4.1 AdaptiveRecvByteBufAllocator的创建\n\n在前文?《Netty是如何高效接收网络连接》中我们提到，当Main Reactor监听到OP_ACCPET事件活跃后，会在NioServerSocketChannel中accept完成三次握手的客户端连接。并创建NioSocketChannel，伴随着NioSocketChannel的创建其对应的配置类NioSocketChannelConfig类也会随之创建。\n\npublic NioSocketChannel(Channel parent, SocketChannel socket) {\n    super(parent, socket);\n    config = new NioSocketChannelConfig(this, socket.socket());\n}\n\n\n最终会在NioSocketChannelConfig的父类DefaultChannelConfig的构造器中创建AdaptiveRecvByteBufAllocator。并保存在RecvByteBufAllocator rcvBufAllocator字段中。\n\npublic class DefaultChannelConfig implements ChannelConfig {\n\n    //用于Channel接收数据用的buffer分配器  AdaptiveRecvByteBufAllocator\n    private volatile RecvByteBufAllocator rcvBufAllocator;\n\n    public DefaultChannelConfig(Channel channel) {\n            this(channel, new AdaptiveRecvByteBufAllocator());\n    }\n\n}\n\n\n在new AdaptiveRecvByteBufAllocator()创建AdaptiveRecvByteBufAllocator类实例的时候会先触发AdaptiveRecvByteBufAllocator类的初始化。\n\n我们先来看下AdaptiveRecvByteBufAllocator类的初始化都做了些什么事情：\n\n\n# 4.2 AdaptiveRecvByteBufAllocator类的初始化\n\npublic class AdaptiveRecvByteBufAllocator extends DefaultMaxMessagesRecvByteBufAllocator {\n\n    //扩容步长\n    private static final int INDEX_INCREMENT = 4;\n    //缩容步长\n    private static final int INDEX_DECREMENT = 1;\n\n    //RecvBuf分配容量表（扩缩容索引表）按照表中记录的容量大小进行扩缩容\n    private static final int[] SIZE_TABLE;\n\n   static {\n        //初始化RecvBuf容量分配表\n        List<Integer> sizeTable = new ArrayList<Integer>();\n        //当分配容量小于512时，扩容单位为16递增\n        for (int i = 16; i < 512; i += 16) {\n            sizeTable.add(i);\n        }\n\n        //当分配容量大于512时，扩容单位为一倍\n        for (int i = 512; i > 0; i <<= 1) {\n            sizeTable.add(i);\n        }\n\n        //初始化RecbBuf扩缩容索引表\n        SIZE_TABLE = new int[sizeTable.size()];\n        for (int i = 0; i < SIZE_TABLE.length; i ++) {\n            SIZE_TABLE[i] = sizeTable.get(i);\n        }\n    }\n}\n\n\nAdaptiveRecvByteBufAllocator 主要的作用就是为接收数据的ByteBuffer进行扩容缩容，那么每次怎么扩容？扩容多少？怎么缩容？缩容多少呢？\n\n这四个问题将是本小节笔者要为大家解答的内容~~~\n\nNetty中定义了一个int型的数组SIZE_TABLE来存储每个扩容单位对应的容量大小。建立起扩缩容的容量索引表。每次扩容多少，缩容多少全部记录在这个容量索引表中。\n\n在AdaptiveRecvByteBufAllocatorl类初始化的时候会在static{}静态代码块中对扩缩容索引表SIZE_TABLE进行初始化。\n\n从源码中我们可以看出SIZE_TABLE的初始化分为两个部分：\n\n * 当索引容量小于512时，SIZE_TABLE中定义的容量索引是从16开始按16递增。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n * 当索引容量大于512时，SIZE_TABLE中定义的容量索引是按前一个索引容量的2倍递增。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n\n# 4.3 扩缩容逻辑\n\n现在扩缩容索引表SIZE_TABLE已经初始化完毕了，那么当我们需要对ByteBuffer进行扩容或者缩容的时候如何根据SIZE_TABLE决定扩容多少或者缩容多少呢？？\n\n这就用到了在AdaptiveRecvByteBufAllocator类中定义的扩容步长INDEX_INCREMENT = 4，缩容步长INDEX_DECREMENT = 1了。\n\n我们就以上面两副扩缩容容量索引表SIZE_TABLE中的容量索引展示截图为例，来介绍下扩缩容逻辑，假设我们当前ByteBuffer的容量索引为33，对应的容量为2048。\n\n# 4.3.1 扩容\n\n当对容量为2048的ByteBuffer进行扩容时，根据当前的容量索引index = 33 加上 扩容步长INDEX_INCREMENT = 4计算出扩容后的容量索引为37，那么扩缩容索引表SIZE_TABLE下标37对应的容量就是本次ByteBuffer扩容后的容量SIZE_TABLE[37] = 32768\n\n# 4.3.1 缩容\n\n同理对容量为2048的ByteBuffer进行缩容时，我们就需要用当前容量索引index = 33 减去 缩容步长INDEX_DECREMENT = 1计算出缩容后的容量索引32，那么扩缩容索引表SIZE_TABLE下标32对应的容量就是本次ByteBuffer缩容后的容量SIZE_TABLE[32] = 1024\n\n\n# 4.4 扩缩容时机\n\npublic abstract class AbstractNioByteChannel extends AbstractNioChannel {\n        @Override\n        public final void read() {\n            .........省略......\n            try {\n                do {\n                      .........省略......\n                } while (allocHandle.continueReading());\n\n                //根据本次read loop总共读取的字节数，决定下次是否扩容或者缩容\n                allocHandle.readComplete();\n\n                .........省略.........\n\n            } catch (Throwable t) {\n                ...............省略...............\n            } finally {\n               ...............省略...............\n            }\n        }\n}\n\n\n在每轮read loop结束之后，我们都会调用allocHandle.readComplete()来根据在allocHandle中统计的在本轮read loop中读取字节总大小，来决定在下一轮read loop中是否对DirectByteBuffer进行扩容或者缩容。\n\npublic abstract class MaxMessageHandle implements ExtendedHandle {\n\n       @Override\n       public void readComplete() {\n                //是否对recvbuf进行扩容缩容\n                record(totalBytesRead());\n       }\n\n       private void record(int actualReadBytes) {\n            if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT)]) {\n                if (decreaseNow) {\n                    index = max(index - INDEX_DECREMENT, minIndex);\n                    nextReceiveBufferSize = SIZE_TABLE[index];\n                    decreaseNow = false;\n                } else {\n                    decreaseNow = true;\n                }\n            } else if (actualReadBytes >= nextReceiveBufferSize) {\n                index = min(index + INDEX_INCREMENT, maxIndex);\n                nextReceiveBufferSize = SIZE_TABLE[index];\n                decreaseNow = false;\n            }\n        }        \n}\n\n\n我们以当前ByteBuffer容量为2048，容量索引index = 33为例，对allocHandle的扩容缩容规则进行说明。\n\n> 扩容步长INDEX_INCREMENT = 4，缩容步长INDEX_DECREMENT = 1。\n\nimage.png\n\n# 4.4.1 缩容\n\n * 如果本次OP_READ事件实际读取到的总字节数actualReadBytes在SIZE_TABLE[index - INDEX_DECREMENT]与SIZE_TABLE[index]之间的话，也就是如果本轮read loop结束之后总共读取的字节数在[1024,2048]之间。说明此时分配的ByteBuffer容量正好，不需要进行缩容也不需要进行扩容。比如本次actualReadBytes = 2000，正好处在1024与2048之间。说明2048的容量正好。\n * 如果actualReadBytes 小于等于 SIZE_TABLE[index - INDEX_DECREMENT]，也就是如果本轮read loop结束之后总共读取的字节数小于等于1024。表示本次读取到的字节数比当前ByteBuffer容量的下一级容量还要小，说明当前ByteBuffer的容量分配的有些大了，设置缩容标识decreaseNow = true。当下次OP_READ事件继续满足缩容条件的时候，开始真正的进行缩容。缩容后的容量为SIZE_TABLE[index - INDEX_DECREMENT]，但不能小于SIZE_TABLE[minIndex]。\n\n> 注意需要满足两次缩容条件才会进行缩容，且缩容步长为1，缩容比较谨慎\n\n# 4.4.2 扩容\n\n如果本次OP_READ事件处理总共读取的字节数actualReadBytes 大于等于 当前ByteBuffer容量(nextReceiveBufferSize)时，说明ByteBuffer分配的容量有点小了，需要进行扩容。扩容后的容量为SIZE_TABLE[index + INDEX_INCREMENT]，但不能超过SIZE_TABLE[maxIndex]。\n\n> 满足一次扩容条件就进行扩容，并且扩容步长为4， 扩容比较奔放\n\n\n# 4.5 AdaptiveRecvByteBufAllocator类的实例化\n\nAdaptiveRecvByteBufAllocator类的实例化主要是确定ByteBuffer的初始容量，以及最小容量和最大容量在扩缩容索引表SIZE_TABLE中的下标：minIndex和maxIndex。\n\nAdaptiveRecvByteBufAllocator定义了三个关于ByteBuffer容量的字段：\n\n * DEFAULT_MINIMUM ：表示ByteBuffer最小的容量，默认为64，也就是无论ByteBuffer在怎么缩容，容量也不会低于64。\n * DEFAULT_INITIAL：表示ByteBuffer的初始化容量。默认为2048。\n * DEFAULT_MAXIMUM ：表示ByteBuffer的最大容量，默认为65536，也就是无论ByteBuffer在怎么扩容，容量也不会超过65536。\n\npublic class AdaptiveRecvByteBufAllocator extends DefaultMaxMessagesRecvByteBufAllocator {\n\n    static final int DEFAULT_MINIMUM = 64;\n    static final int DEFAULT_INITIAL = 2048;\n    static final int DEFAULT_MAXIMUM = 65536;\n\n    public AdaptiveRecvByteBufAllocator() {\n        this(DEFAULT_MINIMUM, DEFAULT_INITIAL, DEFAULT_MAXIMUM);\n    }\n\n    public AdaptiveRecvByteBufAllocator(int minimum, int initial, int maximum) {\n       \n         .................省略异常检查逻辑.............\n\n        //计算minIndex maxIndex\n        //在SIZE_TABLE中二分查找最小 >= minimum的容量索引 ：3\n        int minIndex = getSizeTableIndex(minimum);\n        if (SIZE_TABLE[minIndex] < minimum) {\n            this.minIndex = minIndex + 1;\n        } else {\n            this.minIndex = minIndex;\n        }\n\n        //在SIZE_TABLE中二分查找最大 <= maximum的容量索引 ：38\n        int maxIndex = getSizeTableIndex(maximum);\n        if (SIZE_TABLE[maxIndex] > maximum) {\n            this.maxIndex = maxIndex - 1;\n        } else {\n            this.maxIndex = maxIndex;\n        }\n\n        this.initial = initial;\n    }\n}\n\n\n接下来的事情就是确定最小容量DEFAULT_MINIMUM 在SIZE_TABLE中的下标minIndex，以及最大容量DEFAULT_MAXIMUM 在SIZE_TABLE中的下标maxIndex。\n\n从AdaptiveRecvByteBufAllocator类初始化的过程中，我们可以看出SIZE_TABLE中存储的数据特征是一个有序的集合。\n\n我们可以通过二分查找在SIZE_TABLE中找出第一个容量大于等于DEFAULT_MINIMUM的容量索引minIndex。\n\n同理通过二分查找在SIZE_TABLE中找出最后一个容量小于等于DEFAULT_MAXIMUM的容量索引maxIndex。\n\n根据上一小节关于SIZE_TABLE中容量数据分布的截图，我们可以看出minIndex = 3，maxIndex = 38\n\n# 4.5.1 二分查找容量索引下标\n\n    private static int getSizeTableIndex(final int size) {\n        for (int low = 0, high = SIZE_TABLE.length - 1;;) {\n            if (high < low) {\n                return low;\n            }\n            if (high == low) {\n                return high;\n            }\n\n            int mid = low + high >>> 1;//无符号右移，高位始终补0\n            int a = SIZE_TABLE[mid];\n            int b = SIZE_TABLE[mid + 1];\n            if (size > b) {\n                low = mid + 1;\n            } else if (size < a) {\n                high = mid - 1;\n            } else if (size == a) {\n                return mid;\n            } else {\n                return mid + 1;\n            }\n        }\n    }\n\n\n> 经常刷LeetCode的小伙伴肯定一眼就看出这个是二分查找的模板了。\n\n它的目的就是根据给定容量，在扩缩容索引表SIZE_TABLE中，通过二分查找找到最贴近给定size的容量的索引下标（第一个大于等于 size的容量）\n\n\n# 4.6 RecvByteBufAllocator.Handle\n\n前边我们提到最终动态调整ByteBuffer容量的是由AdaptiveRecvByteBufAllocator中的Handler负责的，我们来看下这个allocHandle的创建过程。\n\nprotected abstract class AbstractUnsafe implements Unsafe {\n\n        private RecvByteBufAllocator.Handle recvHandle;\n\n        @Override\n        public RecvByteBufAllocator.Handle recvBufAllocHandle() {\n            if (recvHandle == null) {\n                recvHandle = config().getRecvByteBufAllocator().newHandle();\n            }\n            return recvHandle;\n        }\n\n}\n\n\n从allocHandle的获取过程我们看到最allocHandle的创建是由AdaptiveRecvByteBufAllocator#newHandle方法执行的。\n\npublic class AdaptiveRecvByteBufAllocator extends DefaultMaxMessagesRecvByteBufAllocator {\n\n    @Override\n    public Handle newHandle() {\n        return new HandleImpl(minIndex, maxIndex, initial);\n    }\n\n    private final class HandleImpl extends MaxMessageHandle {\n        //最小容量在扩缩容索引表中的index\n        private final int minIndex;\n        //最大容量在扩缩容索引表中的index\n        private final int maxIndex;\n        //当前容量在扩缩容索引表中的index 初始33 对应容量2048\n        private int index;\n        //预计下一次分配buffer的容量，初始：2048\n        private int nextReceiveBufferSize;\n        //是否缩容\n        private boolean decreaseNow;\n\n        HandleImpl(int minIndex, int maxIndex, int initial) {\n            this.minIndex = minIndex;\n            this.maxIndex = maxIndex;\n\n            //在扩缩容索引表中二分查找到最小大于等于initial 的容量\n            index = getSizeTableIndex(initial);\n            //2048\n            nextReceiveBufferSize = SIZE_TABLE[index];\n        }\n\n        .......................省略...................\n    }\n\n}\n\n\n这里我们看到Netty中用于动态调整ByteBuffer容量的allocHandle的实际类型为MaxMessageHandle。\n\n下面我们来介绍下HandleImpl中的核心字段，它们都和ByteBuffer的容量有关：\n\n * minIndex ：最小容量在扩缩容索引表SIZE_TABE中的index。默认是3。\n * maxIndex ：最大容量在扩缩容索引表SIZE_TABE中的index。默认是38。\n * index ：当前容量在扩缩容索引表SIZE_TABE中的index。初始是33。\n * nextReceiveBufferSize ：预计下一次分配buffer的容量，初始为2048。在每次申请内存分配ByteBuffer的时候，采用nextReceiveBufferSize的值指定容量。\n * decreaseNow ： 是否需要进行缩容。\n\n\n# 5. 使用堆外内存为ByteBuffer分配内存\n\nAdaptiveRecvByteBufAllocator类只是负责动态调整ByteBuffer的容量，而具体为ByteBuffer申请内存空间的是由PooledByteBufAllocator负责。\n\n\n# 5.1 类名前缀Pooled的来历\n\n在我们使用Java进行日常开发过程中，在为对象分配内存空间的时候我们都会选择在 JVM 堆中为对象分配内存，这样做对我们 Java 开发者特别的友好，我们只管使用就好而不必过多关心这块申请的内存如何回收，因为 JVM 堆完全受Java虚拟机控制管理，Java 虚拟机会帮助我们回收不再使用的内存。\n\n但是 JVM 在进行垃圾回收时候的 stop the world 会对我们应用程序的性能造成一定的影响\n\n除此之外我们在?《聊聊Netty那些事儿之从内核角度看IO模型》一文中介绍IO模型的时候提到，当数据达到网卡时，网卡会通过DMA的方式将数据拷贝到内核空间中，这是第一次拷贝。当用户线程在用户空间发起系统IO调用时，CPU会将内核空间的数据再次拷贝到用户空间。这是第二次拷贝。\n\n于此不同的是当我们在 JVM 中发起 IO 调用时，比如我们使用JVM堆内存读取 Socket接收缓冲区 中的数据时，会多一次内存拷贝，CPU在第二次拷贝中将数据从内核空间拷贝到用户空间时，此时的用户空间站在 JVM 角度是堆外内存，所以还需要将堆外内存中的数据拷贝到 堆内内存 中。这就是 第三次内存拷贝\n\n同理当我们在JVM中发起IO调用向Socket发送缓冲区写入数据时，JVM会将IO数据先拷贝到堆外内存，然后才能发起系统IO调用。\n\n那为什么操作系统不直接使用 JVM 的堆内内存进行IO操作呢？\n\n因为 JVM 的内存布局和操作系统分配的内存是不一样的，操作系统不可能按照 JVM 规范来读写数据，所以就需要 第三次拷贝 中间做个转换将堆外内存中的数据拷贝到JVM堆中。\n\n----------------------------------------\n\n所以基于上述内容，在使用JVM堆内内存时会产生以下两点性能影响：\n\n 1. JVM 在垃圾回收堆内内存时，会发生 stop the world 导致应用程序卡顿。\n 2. 在进行 IO 操作的时候，会多产生一次由堆外内存到堆内内存的拷贝。\n\n基于以上两点使用JVM堆内内存对性能造成的影响，于是对性能有卓越追求的Netty采用堆外内存也就是DirectBuffer来为ByteBuffer分配内存空间。\n\n采用堆外内存为 ByteBuffer 分配内存的好处就是：\n\n * 堆外内存直接受操作系统的管理，不会受JVM的管理，所以JVM垃圾回收对应用程序的性能影响就没有了。\n * 网络数据到达之后直接在堆外内存上接收，进程读取网络数据时直接在堆外内存中读取，所以就避免了第三次内存拷贝。\n\n所以 Netty 在进行 I/O 操作时都是使用的堆外内存，可以避免数据从 JVM 堆内存到堆外内存的拷贝。但是由于堆外内存不受JVM的管理，所以就需要额外关注对内存的使用和释放，稍有不慎就会造成内存泄露，于是Netty就引入了内存池对堆外内存进行统一管理。\n\nPooledByteBufAllocator类的这个前缀Pooled就是内存池的意思，这个类会使用Netty的内存池为ByteBuffer分配堆外内存。\n\n\n# 5.2 PooledByteBufAllocator的创建\n\n# 创建时机\n\n在服务端NioServerSocketChannel的配置类NioServerSocketChannelConfig以及客户端NioSocketChannel的配置类NioSocketChannelConfig实例化的时候会触发PooledByteBufAllocator的创建。\n\npublic class DefaultChannelConfig implements ChannelConfig {\n    //PooledByteBufAllocator\n    private volatile ByteBufAllocator allocator = ByteBufAllocator.DEFAULT;\n\n    ..........省略......\n}\n\n\n创建出来的PooledByteBufAllocator实例保存在DefaultChannelConfig类中的ByteBufAllocator allocator字段中。\n\n# 创建过程\n\npublic interface ByteBufAllocator {\n\n    ByteBufAllocator DEFAULT = ByteBufUtil.DEFAULT_ALLOCATOR;\n    \n    ..................省略............\n}\npublic final class ByteBufUtil {\n\n    static final ByteBufAllocator DEFAULT_ALLOCATOR;\n\n    static {\n        String allocType = SystemPropertyUtil.get(\n                \"io.netty.allocator.type\", PlatformDependent.isAndroid() ? \"unpooled\" : \"pooled\");\n        allocType = allocType.toLowerCase(Locale.US).trim();\n\n        ByteBufAllocator alloc;\n        if (\"unpooled\".equals(allocType)) {\n            alloc = UnpooledByteBufAllocator.DEFAULT;\n            logger.debug(\"-Dio.netty.allocator.type: {}\", allocType);\n        } else if (\"pooled\".equals(allocType)) {\n            alloc = PooledByteBufAllocator.DEFAULT;\n            logger.debug(\"-Dio.netty.allocator.type: {}\", allocType);\n        } else {\n            alloc = PooledByteBufAllocator.DEFAULT;\n            logger.debug(\"-Dio.netty.allocator.type: pooled (unknown: {})\", allocType);\n        }\n\n        DEFAULT_ALLOCATOR = alloc;\n        \n        ...................省略..................\n    }\n}\n\n\n从ByteBufUtil类的初始化过程我们可以看出，在为ByteBuffer分配内存的时候是否使用内存池在Netty中是可以配置的。\n\n * 通过系统变量-D io.netty.allocator.type 可以配置是否使用内存池为ByteBuffer分配内存。默认情况下是需要使用内存池的。但是在安卓系统中默认是不使用内存池的。\n * 通过PooledByteBufAllocator.DEFAULT获取内存池ByteBuffer分配器。\n\npublic static final PooledByteBufAllocator DEFAULT =\n    new PooledByteBufAllocator(PlatformDependent.directBufferPreferred());\n\n\n> 由于本文的主线是介绍Sub Reactor处理OP_READ事件的完整过程，所以这里只介绍主线相关的内容，这里只是简单介绍下在接收数据的时候为什么会用PooledByteBufAllocator来为ByteBuffer分配内存。而内存池的架构设计比较复杂，所以笔者后面会单独写一篇关于Netty内存管理的文章。\n\n----------------------------------------\n\n\n# 总结\n\n本文介绍了Sub Reactor线程在处理OP_READ事件的整个过程。并深入剖析了AdaptiveRecvByteBufAllocator类动态调整ByteBuffer容量的原理。\n\n同时也介绍了Netty为什么会使用堆外内存来为ByteBuffer分配内存，并由此引出了Netty的内存池分配器PooledByteBufAllocator 。\n\n在介绍AdaptiveRecvByteBufAllocator类和PooledByteBufAllocator一起组合实现动态地为ByteBuffer分配容量的时候，笔者不禁想起了多年前看过的《Effective Java》中第16条 复合优先于继承。\n\nNetty在这里也遵循了这条军规，首先两个类设计的都是单一的功能。\n\n * AdaptiveRecvByteBufAllocator 类只负责动态的调整ByteBuffer容量，并不管具体的内存分配。\n * PooledByteBufAllocator 类负责具体的内存分配，用内存池的方式。\n\n这样设计的就比较灵活，具体内存分配的工作交给具体的ByteBufAllocator,可以使用内存池的分配方式PooledByteBufAllocator，也可以不使用内存池的分配方式UnpooledByteBufAllocator。具体的内存可以采用JVM堆内内存（HeapBuffer），也可以使用堆外内存（DirectBuffer）。\n\n而AdaptiveRecvByteBufAllocator只需要关注调整它们的容量工作就可以了，而并不需要关注它们具体的内存分配方式。\n\n最后通过io.netty.channel.RecvByteBufAllocator.Handle#allocate方法灵活组合不同的内存分配方式。这也是装饰模式的一种应用。\n\nbyteBuf = allocHandle.allocate(allocator);\n\n\n好了，今天的内容就到这里，我们下篇文章见~~~~\n\n\n# 参考资料\n\nNetty如何高效接收网络数据？一文聊透ByteBuffer动态自适应扩缩容机制 (qq.com)",normalizedContent:"# 前言\n\n\n\n\n# 前文回顾\n\n在前边的系列文章中，我们从内核如何收发网络数据开始以一个 c10k 的问题作为主线详细从内核角度阐述了网络io模型的演变，最终在此基础上引出了netty的网络io模型如下图所示：\n\n\n\n> 详细内容可回看?《从内核角度看io模型的演变》\n\n后续我们又围绕着netty的主从reactor网络io线程模型，在?《reactor模型在netty中的实现》一文中详细阐述了netty的主从reactor模型的创建，以及介绍了reactor模型的关键组件。搭建了netty的核心骨架如下图所示：\n\n\n\n在核心骨架搭建完毕之后，我们随后又在?《详细图解reactor启动全流程》一文中阐述了reactor启动的全流程，一个非常重要的核心组件nioserversocketchannel开始在这里初次亮相，承担着一个网络框架最重要的任务--高效接收网络连接。我们介绍了nioserversocketchannel的创建，初始化，向main reactor注册并监听op_accept事件的整个流程。在此基础上，netty得以整装待发，枕戈待旦开始迎接海量的客户端连接。\n\n\n\n随后紧接着我们在?《netty如何高效接收网络连接》一文中详细介绍了netty高效接收客户端网络连接的全流程，在这里netty的核心重要组件nioserversocketchannel开始正是登场，在nioserversocketchannel中我们创建了客户端连接niosocketchannel，并详细介绍了niosocketchannel的初始化过程，随后通过在nioserversocketchannel的pipeline中触发channelread事件，并最终在serverbootstrapacceptor中将客户端连接niosocketchannel注册到sub reactor中开始监听客户端连接上的op_read事件，准备接收客户端发送的网络数据也就是本文的主题内容。\n\n\n\n自此netty的核心组件全部就绪并启动完毕，开始起飞~~~\n\n\n\n之前文章中的主角是netty中主reactor组中的main reactor以及注册在main reactor上边的nioserversocketchannel，那么从本文开始，我们文章中的主角就切换为sub reactor以及注册在subreactor上的niosocketchannel了。\n\n下面就让我们正式进入今天的主题，看一下netty是如何处理op_read事件以及如何高效接收网络数据的。\n\n\n# 1. sub reactor处理op_read事件流程总览\n\n\n\n客户端发起系统io调用向服务端发送数据之后，当网络数据到达服务端的网卡并经过内核协议栈的处理，最终数据到达socket的接收缓冲区之后，sub reactor轮询到niosocketchannel上的 op_read事件 就绪，随后 sub reactor 线程就会从 jdk selector 上的阻塞轮询apiselector.select(timeoutmillis)调用中返回。转而去处理niosocketchannel上的op_read事件。\n\n> 注意这里的reactor为负责处理客户端连接的sub reactor。连接的类型为niosocketchannel，处理的事件为op_read事件。\n\n在之前的文章中笔者已经多次强调过了，reactor在处理channel上的io事件入口函数为nioeventloop#processselectedkey。\n\npublic final class nioeventloop extends singlethreadeventloop {\n\n    private void processselectedkey(selectionkey k, abstractniochannel ch) {\n        final abstractniochannel.niounsafe unsafe = ch.unsafe();\n        ..............省略.................\n\n        try {\n            int readyops = k.readyops();\n\n            if ((readyops & selectionkey.op_connect) != 0) {\n               ..............处理op_connect事件.................\n            }\n\n\n            if ((readyops & selectionkey.op_write) != 0) {\n              ..............处理op_write事件.................\n            }\n\n\n            if ((readyops & (selectionkey.op_read | selectionkey.op_accept)) != 0 || readyops == 0) {\n                //本文重点处理op_accept事件\n                unsafe.read();\n            }\n        } catch (cancelledkeyexception ignored) {\n            unsafe.close(unsafe.voidpromise());\n        }\n    }\n\n}\n\n\n这里需要重点强调的是，当前的执行线程现在已经变成了 sub reactor，而 sub reactor 上注册的正是 netty 客户端 niosocketchannel负责处理连接上的读写事件。\n\n所以这里入口函数的参数abstractniochannel ch则是io就绪的客户端连接niosocketchannel。\n\n开头通过ch.unsafe()获取到的niounsafe操作类正是niosocketchannel中对底层jdk nio socketchannel的unsafe底层操作类。实现类型为niobyteunsafe定义在下图继承结构中的abstractniobytechannel父类中。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n下面我们到niobyteunsafe#read方法中来看下netty对op_read事件的具体处理过程：\n\n\n# 2. netty接收网络数据流程总览\n\n我们直接按照老规矩，先从整体上把整个op_read事件的逻辑处理框架提取出来，让大家先总体俯视下流程全貌，然后在针对每个核心点位进行各个击破。\n\n\n\n> 流程中相关置灰的步骤为netty处理连接关闭时的逻辑，和本文主旨无关，我们这里暂时忽略，等后续笔者介绍连接关闭时，会单独开一篇文章详细为大家介绍。\n\n从上面这张netty接收网络数据总体流程图可以看出niosocketchannel在接收网络数据的整个流程和我们在上篇文章?《netty如何高效接收网络连接》中介绍的nioserversocketchannel在接收客户端连接时的流程在总体框架上是一样的。\n\nniosocketchannel在接收网络数据的过程处理中，也是通过在一个do{....}while(...)循环read loop中不断的循环读取连接niosocketchannel上的数据。\n\n同样在niosocketchannel读取连接数据的read loop中也是受最大读取次数的限制。默认配置最多只能读取16次，超过16次无论此时niosocketchannel中是否还有数据可读都不能在进行读取了。\n\n这里read loop循环最大读取次数可在启动配置类serverbootstrap中通过channeloption.max_messages_per_read选项设置，默认为16。\n\nserverbootstrap b = new serverbootstrap();\nb.group(bossgroup, workergroup)\n  .channel(nioserversocketchannel.class)\n  .option(channeloption.max_messages_per_read, 自定义次数)\n\n\n**netty这里为什么非得限制read loop的最大读取次数呢？**为什么不在 read loop 中一次性把数据读取完呢？\n\n这时候就是考验我们大局观的时候了，在前边的文章介绍中我们提到netty的io模型为主从reactor线程组模型，在sub reactor group中包含了多个sub reactor专门用于监听处理客户端连接上的io事件。\n\n为了能够高效有序的处理全量客户端连接上的读写事件，netty将服务端承载的全量客户端连接分摊到多个sub reactor中处理，同时也能保证channel上io处理的线程安全性。\n\n其中一个channel只能分配给一个固定的reactor。一个reactor负责处理多个channel上的io就绪事件，reactor与channel之间的对应关系如下图所示：\n\n\n\n而一个 sub reactor上注册了多个niosocketchannel，netty 不可能在一个 niosocketchannel 上无限制的处理下去，要将读取数据的机会均匀分摊给其他 niosocketchannel，所以需要限定每个 niosocketchannel 上的最大读取次数。\n\n此外，sub reactor除了需要监听处理所有注册在它上边的niosocketchannel中的io就绪事件之外，还需要腾出事件来处理有用户线程提交过来的异步任务。从这一点看，netty也不会一直停留在niosocketchannel的io处理上。所以限制read loop的最大读取次数是非常必要的。\n\n> 关于reactor的整体运转架构，对细节部分感兴趣的同学可以回看下笔者的?《一文聊透netty核心引擎reactor的运转架构》这篇文章。\n\n所以基于这个原因，我们需要在read loop循环中，每当通过doreadbytes方法从niosocketchannel中读取到数据时（方法返回值会大于0，并记录在allochandle.lastbytesread中），都需要通过allochandle.incmessagesread(1)方法统计已经读取的次数。当达到16次时不管niosocketchannel是否还有数据可读，都需要在read loop末尾退出循环。转去执行sub reactor上的异步任务。以及其他niosocketchannel上的io就绪事件。平均分配，雨露均沾！！\n\npublic abstract class maxmessagehandle implements extendedhandle {\n\n        //read loop总共读取了多少次\n        private int totalmessages;\n\n       @override\n        public final void incmessagesread(int amt) {\n            totalmessages += amt;\n        }\n\n}\n\n\n本次 read loop 读取到的数据大小会记录在allochandle.lastbytesread中\n\npublic abstract class maxmessagehandle implements extendedhandle {\n\n         //本次read loop读取到的字节数\n        private int lastbytesread;\n        //整个read loop循环总共读取的字节数\n        private int totalbytesread;\n\n        @override\n        public void lastbytesread(int bytes) {\n            lastbytesread = bytes;\n            if (bytes > 0) {\n                totalbytesread += bytes;\n            }\n        }\n}\n\n\n * lastbytesread < 0：表示客户端主动发起了连接关闭流程，netty开始连接关闭处理流程。这个和本文的主旨无关，我们先不用管。后面笔者会专门用一篇文章来详解关闭流程。\n * lastbytesread = 0：表示当前niosocketchannel上的数据已经全部读取完毕，没有数据可读了。本次op_read事件圆满处理完毕，可以开开心心的退出read loop。\n * 当lastbytesread > 0：表示在本次read loop中从niosocketchannel中读取到了数据，会在niosocketchannel的pipeline中触发channelread事件。进而在pipeline中负责io处理的channelhandelr中响应，处理网络请求。\n\nfir\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(channelhandlercontext ctx, object msg) {\n          .......处理网络请求，比如解码,反序列化等操作.......\n    }\n}\n\n\n最后会在read loop循环的末尾调用allochandle.continuereading()判断是否结束本次read loop循环。这里的结束循环条件的判断会比我们在介绍nioserversocketchannel接收连接时的判断条件复杂很多，笔者会将这个判断条件的详细解析放在文章后面细节部分为大家解读，这里大家只需要把握总体核心流程，不需要关注太多细节。\n\n总体上在niosocketchannel中读取网络数据的read loop循环结束条件需要满足以下几点：\n\n * 当前niosocketchannel中的数据已经全部读取完毕，则退出循环。\n * 本轮read loop如果没有读到任何数据，则退出循环。\n * read loop的读取次数达到16次，退出循环。\n\n当满足这里的read loop退出条件之后，sub reactor线程就会退出循环，随后会调用allochandle.readcomplete()方法根据本轮read loop总共读取到的字节数totalbytesread来决定是否对用于接收下一轮op_read事件数据的bytebuffer进行扩容或者缩容。\n\n最后在niosocketchannel的pipeline中触发channelreadcomplete事件，通知channelhandler本次op_read事件已经处理完毕。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)firechannelreadcomplete.png\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(channelhandlercontext ctx, object msg) {\n       .......处理网络请求，比如解码,反序列化等操作.......\n    }\n\n    @override\n    public void channelreadcomplete(channelhandlercontext ctx) {\n        ......本次op_read事件处理完毕.......\n        ......决定是否向客户端响应处理结果......\n    }\n}\n\n\n\n# 2.1 channelread与channelreadcomplete事件的区别\n\n> 有些小伙伴可能对netty中的一些传播事件触发的时机，或者事件之间的区别理解的不是很清楚，概念容易混淆。在后面的文章中笔者也会从源码的角度出发给大家说清楚netty中定义的所有异步事件，以及这些事件之间的区别和联系和触发时机，传播机制。\n\n这里我们主要探讨本文主题中涉及到的两个事件：channelread事件与channelreadcomplete事件。\n\n从上述介绍的netty接收网络数据流程总览中我们可以看出channelread事件和channelreadcomplete事件是不一样的，但是对于刚接触netty的小伙伴来说从命名上乍一看感觉又差不多。\n\n下面我们来看这两个事件之间的差别：\n\nnetty服务端对于一次op_read事件的处理，会在一个do{}while()循环read loop中分多次从客户端niosocketchannel中读取网络数据。每次读取我们分配的bytebuffer容量大小，初始容量为2048。\n\n * channeread事件：一次循环读取一次数据，就触发一次channelread事件。本次最多读取在read loop循环开始分配的directbytebuffer容量大小。这个容量会动态调整，文章后续笔者会详细介绍。\n * channelreadcomplete事件：当读取不到数据或者不满足continuereading的任意一个条件就会退出read loop，这时就会触发channelreadcomplete事件。表示本次op_read事件处理完毕。\n\n> 这里需要特别注意下触发channelreadcomplete事件并不代表niosocketchannel中的数据已经读取完了，只能说明本次op_read事件处理完毕。因为有可能是客户端发送的数据太多，netty读了16次还没读完，那就只能等到下次op_read事件到来的时候在进行读取了。\n\n----------------------------------------\n\n以上内容就是netty在接收客户端发送网络数据的全部核心逻辑。目前为止我们还未涉及到这部分的主干核心源码，笔者想的是先给大家把核心逻辑讲解清楚之后，这样理解起来核心主干源码会更加清晰透彻。\n\n经过前边对网络数据接收的核心逻辑介绍，笔者在把这张流程图放出来，大家可以结合这张图在来回想下主干核心逻辑。\n\n\n\n下面笔者会结合这张流程图，给大家把这部分的核心主干源码框架展现出来，大家可以将我们介绍过的核心逻辑与主干源码做个一一对应，还是那句老话，我们要从主干框架层面把握整体处理流程，不需要读懂每一行代码，文章后续笔者会将这个过程中涉及到的核心点位给大家拆开来各个击破！！\n\n\n\n\n# 3. 源码核心框架总览\n\n@override\npublic final void read() {\n    final channelconfig config = config();\n\n    ...............处理半关闭相关代码省略...............\n        //获取niosocketchannel的pipeline\n        final channelpipeline pipeline = pipeline();\n    //pooledbytebufallocator 具体用于实际分配bytebuf的分配器\n    final bytebufallocator allocator = config.getallocator();\n    //自适应bytebuf分配器 adaptiverecvbytebufallocator ,用于动态调节bytebuf容量\n    //需要与具体的bytebuf分配器配合使用 比如这里的pooledbytebufallocator\n    final recvbytebufallocator.handle allochandle = recvbufallochandle();\n    //allochandler用于统计每次读取数据的大小，方便下次分配合适大小的bytebuf\n    //重置清除上次的统计指标\n    allochandle.reset(config);\n\n    bytebuf bytebuf = null;\n    boolean close = false;\n    try {\n        do {\n            //利用pooledbytebufallocator分配合适大小的bytebuf 初始大小为2048\n            bytebuf = allochandle.allocate(allocator);\n            //记录本次读取了多少字节数\n            allochandle.lastbytesread(doreadbytes(bytebuf));\n            //如果本次没有读取到任何字节，则退出循环 进行下一轮事件轮询\n            if (allochandle.lastbytesread() <= 0) {\n                // nothing was read. release the buffer.\n                bytebuf.release();\n                bytebuf = null;\n                close = allochandle.lastbytesread() < 0;\n                if (close) {\n                    ......表示客户端发起连接关闭.....\n                }\n                break;\n            }\n\n            //read loop读取数据次数+1\n            allochandle.incmessagesread(1);\n            //客户端niosocketchannel的pipeline中触发channelread事件\n            pipeline.firechannelread(bytebuf);\n            //解除本次读取数据分配的bytebuffer引用，方便下一轮read loop分配\n            bytebuf = null;\n        } while (allochandle.continuereading());//判断是否应该继续read loop\n\n        //根据本次read loop总共读取的字节数，决定下次是否扩容或者缩容\n        allochandle.readcomplete();\n        //在niosocketchannel的pipeline中触发channelreadcomplete事件，表示一次read事件处理完毕\n        //但这并不表示 客户端发送来的数据已经全部读完，因为如果数据太多的话，这里只会读取16次，剩下的会等到下次read事件到来后在处理\n        pipeline.firechannelreadcomplete();\n\n        .........省略连接关闭流程处理.........\n    } catch (throwable t) {\n        ...............省略...............\n    } finally {\n        ...............省略...............\n    }\n}\n}\n\n\n> 这里再次强调下当前执行线程为sub reactor线程，处理连接数据读取逻辑是在niosocketchannel中。\n\n首先通过config()获取客户端niosocketchannel的channel配置类niosocketchannelconfig。\n\n通过pipeline()获取niosocketchannel的pipeline。我们在 ?《详细图解netty reactor启动全流程》一文中提到的netty服务端模板所举的示例中，niosocketchannelde pipeline中只有一个echochannelhandler。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)客户端channel pipeline结构.png\n\n\n# 3.1 分配directbytebuffer接收网络数据\n\nsub reactor 在接收niosocketchannel上的io数据时，都会分配一个 bytebuffer 用来存放接收到的io数据。\n\n这里大家可能觉得比较奇怪，为什么在niosocketchannel接收数据这里会有两个bytebuffer分配器呢？一个是bytebufallocator，另一个是recvbytebufallocator。\n\nfinal bytebufallocator allocator = config.getallocator();\nfinal recvbytebufallocator.handle allochandle = recvbufallochandle();\n\n\n这两个bytebuffer又有什么区别和联系呢？\n\n在上篇文章?《抓到netty一个bug，顺带来透彻地聊一下netty是如何高效接收网络连接》中，笔者为了阐述上篇文章中提到的netty在接收网络连接时的 bug 时，简单和大家介绍了下这个 recvbytebufallocator。\n\n在上篇文章提到的nioserversocketchannelconfig中，这里的recvbytebufallocator类型为serverchannelrecvbytebufallocator。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n> 还记得这个serverchannelrecvbytebufallocator类型在4.1.69.final版本引入是为了解决笔者在上篇文章中提到的那个bug吗？在4.1.69.final版本之前，nioserversocketchannelconfig中的recvbytebufallocator类型为adaptiverecvbytebufallocator。\n\n而在本文中niosocketchannelconfig中的recvbytebufallocator类型为adaptiverecvbytebufallocator。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n所以这里recvbufallochandle()获得到的recvbytebufallocator为adaptiverecvbytebufallocator。顾名思义，这个类型的recvbytebufallocator可以根据niosocketchannel上每次到来的io数据大小来自适应动态调整bytebuffer的容量。\n\n对于客户端niosocketchannel来说，它里边包含的io数据时客户端发送来的网络数据，长度是不定的，所以才会需要这样一个可以根据每次io数据的大小来自适应动态调整容量的bytebuffer来接收。\n\n如果我们把用于接收数据用的bytebuffer看做一个桶的话，那么小数据用大桶装或者大数据用小桶装肯定是不合适的，所以我们需要根据接收数据的大小来动态调整桶的容量。而adaptiverecvbytebufallocator的作用正是用来根据每次接收数据的容量大小来动态调整bytebuffer的容量的。\n\n现在recvbytebufallocator笔者为大家解释清楚了，接下来我们继续看bytebufallocator。\n\n> 大家这里需要注意的是adaptiverecvbytebufallocator并不会真正的去分配bytebuffer，它只是负责动态调整分配bytebuffer的大小。\n\n而真正具体执行内存分配动作的是这里的bytebufallocator类型为pooledbytebufallocator。它会根据adaptiverecvbytebufallocator动态调整出来的大小去真正的申请内存分配bytebuffer。\n\n> pooledbytebufallocator为netty中的内存池，用来管理堆外内存directbytebuffer。\n\nadaptiverecvbytebufallocator中的allochandle在上篇文章中我们也介绍过了，它的实际类型为maxmessagehandle。\n\npublic class adaptiverecvbytebufallocator extends defaultmaxmessagesrecvbytebufallocator {\n\n    @override\n    public handle newhandle() {\n        return new handleimpl(minindex, maxindex, initial);\n    }\n    \n    private final class handleimpl extends maxmessagehandle {\n                  .................省略................\n    }\n}\n\n\n在maxmessagehandle中包含了用于动态调整bytebuffer容量的统计指标。\n\n   public abstract class maxmessagehandle implements extendedhandle {\n        private channelconfig config;\n\n        //用于控制每次read loop里最大可以循环读取的次数，默认为16次\n        //可在启动配置类serverbootstrap中通过channeloption.max_messages_per_read选项设置。\n        private int maxmessageperread;\n\n        //用于统计read loop中总共接收的连接个数，niosocketchannel中表示读取数据的次数\n        //每次read loop循环后会调用allochandle.incmessagesread增加记录接收到的连接个数\n        private int totalmessages;\n\n        //用于统计在read loop中总共接收到客户端连接上的数据大小\n        private int totalbytesread;\n\n        //表示本次read loop 尝试读取多少字节，bytebuffer剩余可写的字节数\n        private int attemptedbytesread;\n\n        //本次read loop读取到的字节数\n        private int lastbytesread;\n        \n        //预计下一次分配buffer的容量，初始：2048\n        private int nextreceivebuffersize;\n        ...........省略.............\n}\n\n\n在每轮read loop开始之前，都会调用allochandle.reset(config)重置清空上一轮read loop的统计指标。\n\n@override\npublic void reset(channelconfig config) {\n    this.config = config;\n    //默认每次最多读取16次\n    maxmessageperread = maxmessagesperread();\n    totalmessages = totalbytesread = 0;\n}\n\n\n在每次开始从niosocketchannel中读取数据之前，需要利用pooledbytebufallocator在内存池中为bytebuffer分配内存，默认初始化大小为2048，这个容量由guess()方法决定。\n\nbytebuf = allochandle.allocate(allocator);\n@override\npublic bytebuf allocate(bytebufallocator alloc) {\n    return alloc.iobuffer(guess());\n}\n\n@override\npublic int guess() {\n    //预计下一次分配buffer的容量，一开始为2048\n    return nextreceivebuffersize;\n}\n\n\n在每次通过doreadbytes从niosocketchannel中读取到数据后，都会调用allochandle.lastbytesread(doreadbytes(bytebuf))记录本次读取了多少字节数据，并统计本轮read loop目前总共读取了多少字节。\n\n@override\npublic void lastbytesread(int bytes) {\n    lastbytesread = bytes;\n    if (bytes > 0) {\n        totalbytesread += bytes;\n    }\n}\n\n\n每次循环从niosocketchannel中读取数据之后，都会调用allochandle.incmessagesread(1)。统计当前已经读取了多少次。如果超过了最大读取限制此时16次，就需要退出read loop。去处理其他niosocketchannel上的io事件。\n\n@override\npublic final void incmessagesread(int amt) {\n    totalmessages += amt;\n}\n\n\n在每次read loop循环的末尾都需要通过调用allochandle.continuereading()来判断是否继续read loop循环读取niosocketchannel中的数据。\n\n@override\npublic boolean continuereading() {\n    return continuereading(defaultmaybemoresupplier);\n}\n\nprivate final uncheckedbooleansupplier defaultmaybemoresupplier = new uncheckedbooleansupplier() {\n    @override\n    public boolean get() {\n        //判断本次读取bytebuffer是否满载而归\n        return attemptedbytesread == lastbytesread;\n    }\n};\n\n@override\npublic boolean continuereading(uncheckedbooleansupplier maybemoredatasupplier) {\n    return config.isautoread() &&\n        (!respectmaybemoredata || maybemoredatasupplier.get()) &&\n        totalmessages < maxmessageperread &&\n        totalbytesread > 0;\n}\n\n\n * attemptedbytesread :表示当前bytebuffer预计尝试要写入的字节数。\n * lastbytesread :表示本次read loop真实读取到了多少个字节。\n\ndefaultmaybemoresupplier用于判断经过本次read loop读取数据后，bytebuffer是否满载而归。如果是满载而归的话（attemptedbytesread == lastbytesread），表明可能niosocketchannel里还有数据。如果不是满载而归，表明niosocketchannel里没有数据了已经。\n\n是否继续进行read loop需要同时满足以下几个条件：\n\n * totalmessages < maxmessageperread 当前读取次数是否已经超过16次，如果超过，就退出do(...)while()循环。进行下一轮op_read事件的轮询。因为每个sub reactor管理了多个niosocketchannel，不能在一个niosocketchannel上占用太多时间，要将机会均匀地分配给sub reactor所管理的所有niosocketchannel。\n\n * totalbytesread > 0 本次op_read事件处理是否读取到了数据，如果已经没有数据可读了，那么就直接退出read loop。\n\n * !respectmaybemoredata || maybemoredatasupplier.get() 这个条件比较复杂，它其实就是通过respectmaybemoredata字段来控制niosocketchannel中可能还有数据可读的情况下该如何处理。\n\n * * maybemoredatasupplier.get()：true表示本次读取从niosocketchannel中读取数据，bytebuffer满载而归。说明可能niosocketchannel中还有数据没读完。fasle表示bytebuffer还没有装满，说明niosocketchannel中已经没有数据可读了。\n   * respectmaybemoredata = true表示要对可能还有更多数据进行处理的这种情况要respect认真对待,如果本次循环读取到的数据已经装满bytebuffer，表示后面可能还有数据，那么就要进行读取。如果bytebuffer还没装满表示已经没有数据可读了那么就退出循环。![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)\n   * respectmaybemoredata = false表示对可能还有更多数据的这种情况不认真对待 not respect。不管本次循环读取数据bytebuffer是否满载而归，都要继续进行读取，直到读取不到数据在退出循环，属于无脑读取。\n\n同时满足以上三个条件，那么read loop继续进行。继续从niosocketchannel中读取数据，直到读取不到或者不满足三个条件中的任意一个为止。\n\n\n# 3.2 从niosocketchannel中读取数据\n\npublic class niosocketchannel extends abstractniobytechannel implements io.netty.channel.socket.socketchannel {\n\n    @override\n    protected int doreadbytes(bytebuf bytebuf) throws exception {\n        final recvbytebufallocator.handle allochandle = unsafe().recvbufallochandle();\n        allochandle.attemptedbytesread(bytebuf.writablebytes());    \n        return bytebuf.writebytes(javachannel(), allochandle.attemptedbytesread());\n    }\n}\n\n\n这里会直接调用底层jdk nio的socketchannel#read方法将数据读取到directbytebuffer中。读取数据大小为本次分配的directbytebuffer容量，初始为2048。\n\n\n# 4. bytebuffer动态自适应扩缩容机制\n\n由于我们一开始并不知道客户端会发送多大的网络数据，所以这里先利用pooledbytebufallocator分配一个初始容量为2048的directbytebuffer用于接收数据。\n\nbytebuf = allochandle.allocate(allocator);\n\n\n这就好比我们需要拿着一个桶去排队装水，但是第一次去装的时候，我们并不知道管理员会给我们分配多少水，桶拿大了也不合适拿小了也不合适，于是我们就先预估一个差不多容量大小的桶，如果分配的多了，我们下次就拿更大一点的桶，如果分配少了，下次我们就拿一个小点的桶。\n\n在这种场景下，我们需要bytebuffer可以自动根据每次网络数据的大小来动态自适应调整自己的容量。\n\n而bytebuffer动态自适应扩缩容机制依赖于adaptiverecvbytebufallocator类的实现。让我们先回到adaptiverecvbytebufallocator类的创建起点开始说起~~\n\n\n# 4.1 adaptiverecvbytebufallocator的创建\n\n在前文?《netty是如何高效接收网络连接》中我们提到，当main reactor监听到op_accpet事件活跃后，会在nioserversocketchannel中accept完成三次握手的客户端连接。并创建niosocketchannel，伴随着niosocketchannel的创建其对应的配置类niosocketchannelconfig类也会随之创建。\n\npublic niosocketchannel(channel parent, socketchannel socket) {\n    super(parent, socket);\n    config = new niosocketchannelconfig(this, socket.socket());\n}\n\n\n最终会在niosocketchannelconfig的父类defaultchannelconfig的构造器中创建adaptiverecvbytebufallocator。并保存在recvbytebufallocator rcvbufallocator字段中。\n\npublic class defaultchannelconfig implements channelconfig {\n\n    //用于channel接收数据用的buffer分配器  adaptiverecvbytebufallocator\n    private volatile recvbytebufallocator rcvbufallocator;\n\n    public defaultchannelconfig(channel channel) {\n            this(channel, new adaptiverecvbytebufallocator());\n    }\n\n}\n\n\n在new adaptiverecvbytebufallocator()创建adaptiverecvbytebufallocator类实例的时候会先触发adaptiverecvbytebufallocator类的初始化。\n\n我们先来看下adaptiverecvbytebufallocator类的初始化都做了些什么事情：\n\n\n# 4.2 adaptiverecvbytebufallocator类的初始化\n\npublic class adaptiverecvbytebufallocator extends defaultmaxmessagesrecvbytebufallocator {\n\n    //扩容步长\n    private static final int index_increment = 4;\n    //缩容步长\n    private static final int index_decrement = 1;\n\n    //recvbuf分配容量表（扩缩容索引表）按照表中记录的容量大小进行扩缩容\n    private static final int[] size_table;\n\n   static {\n        //初始化recvbuf容量分配表\n        list<integer> sizetable = new arraylist<integer>();\n        //当分配容量小于512时，扩容单位为16递增\n        for (int i = 16; i < 512; i += 16) {\n            sizetable.add(i);\n        }\n\n        //当分配容量大于512时，扩容单位为一倍\n        for (int i = 512; i > 0; i <<= 1) {\n            sizetable.add(i);\n        }\n\n        //初始化recbbuf扩缩容索引表\n        size_table = new int[sizetable.size()];\n        for (int i = 0; i < size_table.length; i ++) {\n            size_table[i] = sizetable.get(i);\n        }\n    }\n}\n\n\nadaptiverecvbytebufallocator 主要的作用就是为接收数据的bytebuffer进行扩容缩容，那么每次怎么扩容？扩容多少？怎么缩容？缩容多少呢？\n\n这四个问题将是本小节笔者要为大家解答的内容~~~\n\nnetty中定义了一个int型的数组size_table来存储每个扩容单位对应的容量大小。建立起扩缩容的容量索引表。每次扩容多少，缩容多少全部记录在这个容量索引表中。\n\n在adaptiverecvbytebufallocatorl类初始化的时候会在static{}静态代码块中对扩缩容索引表size_table进行初始化。\n\n从源码中我们可以看出size_table的初始化分为两个部分：\n\n * 当索引容量小于512时，size_table中定义的容量索引是从16开始按16递增。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n * 当索引容量大于512时，size_table中定义的容量索引是按前一个索引容量的2倍递增。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n\n# 4.3 扩缩容逻辑\n\n现在扩缩容索引表size_table已经初始化完毕了，那么当我们需要对bytebuffer进行扩容或者缩容的时候如何根据size_table决定扩容多少或者缩容多少呢？？\n\n这就用到了在adaptiverecvbytebufallocator类中定义的扩容步长index_increment = 4，缩容步长index_decrement = 1了。\n\n我们就以上面两副扩缩容容量索引表size_table中的容量索引展示截图为例，来介绍下扩缩容逻辑，假设我们当前bytebuffer的容量索引为33，对应的容量为2048。\n\n# 4.3.1 扩容\n\n当对容量为2048的bytebuffer进行扩容时，根据当前的容量索引index = 33 加上 扩容步长index_increment = 4计算出扩容后的容量索引为37，那么扩缩容索引表size_table下标37对应的容量就是本次bytebuffer扩容后的容量size_table[37] = 32768\n\n# 4.3.1 缩容\n\n同理对容量为2048的bytebuffer进行缩容时，我们就需要用当前容量索引index = 33 减去 缩容步长index_decrement = 1计算出缩容后的容量索引32，那么扩缩容索引表size_table下标32对应的容量就是本次bytebuffer缩容后的容量size_table[32] = 1024\n\n\n# 4.4 扩缩容时机\n\npublic abstract class abstractniobytechannel extends abstractniochannel {\n        @override\n        public final void read() {\n            .........省略......\n            try {\n                do {\n                      .........省略......\n                } while (allochandle.continuereading());\n\n                //根据本次read loop总共读取的字节数，决定下次是否扩容或者缩容\n                allochandle.readcomplete();\n\n                .........省略.........\n\n            } catch (throwable t) {\n                ...............省略...............\n            } finally {\n               ...............省略...............\n            }\n        }\n}\n\n\n在每轮read loop结束之后，我们都会调用allochandle.readcomplete()来根据在allochandle中统计的在本轮read loop中读取字节总大小，来决定在下一轮read loop中是否对directbytebuffer进行扩容或者缩容。\n\npublic abstract class maxmessagehandle implements extendedhandle {\n\n       @override\n       public void readcomplete() {\n                //是否对recvbuf进行扩容缩容\n                record(totalbytesread());\n       }\n\n       private void record(int actualreadbytes) {\n            if (actualreadbytes <= size_table[max(0, index - index_decrement)]) {\n                if (decreasenow) {\n                    index = max(index - index_decrement, minindex);\n                    nextreceivebuffersize = size_table[index];\n                    decreasenow = false;\n                } else {\n                    decreasenow = true;\n                }\n            } else if (actualreadbytes >= nextreceivebuffersize) {\n                index = min(index + index_increment, maxindex);\n                nextreceivebuffersize = size_table[index];\n                decreasenow = false;\n            }\n        }        \n}\n\n\n我们以当前bytebuffer容量为2048，容量索引index = 33为例，对allochandle的扩容缩容规则进行说明。\n\n> 扩容步长index_increment = 4，缩容步长index_decrement = 1。\n\nimage.png\n\n# 4.4.1 缩容\n\n * 如果本次op_read事件实际读取到的总字节数actualreadbytes在size_table[index - index_decrement]与size_table[index]之间的话，也就是如果本轮read loop结束之后总共读取的字节数在[1024,2048]之间。说明此时分配的bytebuffer容量正好，不需要进行缩容也不需要进行扩容。比如本次actualreadbytes = 2000，正好处在1024与2048之间。说明2048的容量正好。\n * 如果actualreadbytes 小于等于 size_table[index - index_decrement]，也就是如果本轮read loop结束之后总共读取的字节数小于等于1024。表示本次读取到的字节数比当前bytebuffer容量的下一级容量还要小，说明当前bytebuffer的容量分配的有些大了，设置缩容标识decreasenow = true。当下次op_read事件继续满足缩容条件的时候，开始真正的进行缩容。缩容后的容量为size_table[index - index_decrement]，但不能小于size_table[minindex]。\n\n> 注意需要满足两次缩容条件才会进行缩容，且缩容步长为1，缩容比较谨慎\n\n# 4.4.2 扩容\n\n如果本次op_read事件处理总共读取的字节数actualreadbytes 大于等于 当前bytebuffer容量(nextreceivebuffersize)时，说明bytebuffer分配的容量有点小了，需要进行扩容。扩容后的容量为size_table[index + index_increment]，但不能超过size_table[maxindex]。\n\n> 满足一次扩容条件就进行扩容，并且扩容步长为4， 扩容比较奔放\n\n\n# 4.5 adaptiverecvbytebufallocator类的实例化\n\nadaptiverecvbytebufallocator类的实例化主要是确定bytebuffer的初始容量，以及最小容量和最大容量在扩缩容索引表size_table中的下标：minindex和maxindex。\n\nadaptiverecvbytebufallocator定义了三个关于bytebuffer容量的字段：\n\n * default_minimum ：表示bytebuffer最小的容量，默认为64，也就是无论bytebuffer在怎么缩容，容量也不会低于64。\n * default_initial：表示bytebuffer的初始化容量。默认为2048。\n * default_maximum ：表示bytebuffer的最大容量，默认为65536，也就是无论bytebuffer在怎么扩容，容量也不会超过65536。\n\npublic class adaptiverecvbytebufallocator extends defaultmaxmessagesrecvbytebufallocator {\n\n    static final int default_minimum = 64;\n    static final int default_initial = 2048;\n    static final int default_maximum = 65536;\n\n    public adaptiverecvbytebufallocator() {\n        this(default_minimum, default_initial, default_maximum);\n    }\n\n    public adaptiverecvbytebufallocator(int minimum, int initial, int maximum) {\n       \n         .................省略异常检查逻辑.............\n\n        //计算minindex maxindex\n        //在size_table中二分查找最小 >= minimum的容量索引 ：3\n        int minindex = getsizetableindex(minimum);\n        if (size_table[minindex] < minimum) {\n            this.minindex = minindex + 1;\n        } else {\n            this.minindex = minindex;\n        }\n\n        //在size_table中二分查找最大 <= maximum的容量索引 ：38\n        int maxindex = getsizetableindex(maximum);\n        if (size_table[maxindex] > maximum) {\n            this.maxindex = maxindex - 1;\n        } else {\n            this.maxindex = maxindex;\n        }\n\n        this.initial = initial;\n    }\n}\n\n\n接下来的事情就是确定最小容量default_minimum 在size_table中的下标minindex，以及最大容量default_maximum 在size_table中的下标maxindex。\n\n从adaptiverecvbytebufallocator类初始化的过程中，我们可以看出size_table中存储的数据特征是一个有序的集合。\n\n我们可以通过二分查找在size_table中找出第一个容量大于等于default_minimum的容量索引minindex。\n\n同理通过二分查找在size_table中找出最后一个容量小于等于default_maximum的容量索引maxindex。\n\n根据上一小节关于size_table中容量数据分布的截图，我们可以看出minindex = 3，maxindex = 38\n\n# 4.5.1 二分查找容量索引下标\n\n    private static int getsizetableindex(final int size) {\n        for (int low = 0, high = size_table.length - 1;;) {\n            if (high < low) {\n                return low;\n            }\n            if (high == low) {\n                return high;\n            }\n\n            int mid = low + high >>> 1;//无符号右移，高位始终补0\n            int a = size_table[mid];\n            int b = size_table[mid + 1];\n            if (size > b) {\n                low = mid + 1;\n            } else if (size < a) {\n                high = mid - 1;\n            } else if (size == a) {\n                return mid;\n            } else {\n                return mid + 1;\n            }\n        }\n    }\n\n\n> 经常刷leetcode的小伙伴肯定一眼就看出这个是二分查找的模板了。\n\n它的目的就是根据给定容量，在扩缩容索引表size_table中，通过二分查找找到最贴近给定size的容量的索引下标（第一个大于等于 size的容量）\n\n\n# 4.6 recvbytebufallocator.handle\n\n前边我们提到最终动态调整bytebuffer容量的是由adaptiverecvbytebufallocator中的handler负责的，我们来看下这个allochandle的创建过程。\n\nprotected abstract class abstractunsafe implements unsafe {\n\n        private recvbytebufallocator.handle recvhandle;\n\n        @override\n        public recvbytebufallocator.handle recvbufallochandle() {\n            if (recvhandle == null) {\n                recvhandle = config().getrecvbytebufallocator().newhandle();\n            }\n            return recvhandle;\n        }\n\n}\n\n\n从allochandle的获取过程我们看到最allochandle的创建是由adaptiverecvbytebufallocator#newhandle方法执行的。\n\npublic class adaptiverecvbytebufallocator extends defaultmaxmessagesrecvbytebufallocator {\n\n    @override\n    public handle newhandle() {\n        return new handleimpl(minindex, maxindex, initial);\n    }\n\n    private final class handleimpl extends maxmessagehandle {\n        //最小容量在扩缩容索引表中的index\n        private final int minindex;\n        //最大容量在扩缩容索引表中的index\n        private final int maxindex;\n        //当前容量在扩缩容索引表中的index 初始33 对应容量2048\n        private int index;\n        //预计下一次分配buffer的容量，初始：2048\n        private int nextreceivebuffersize;\n        //是否缩容\n        private boolean decreasenow;\n\n        handleimpl(int minindex, int maxindex, int initial) {\n            this.minindex = minindex;\n            this.maxindex = maxindex;\n\n            //在扩缩容索引表中二分查找到最小大于等于initial 的容量\n            index = getsizetableindex(initial);\n            //2048\n            nextreceivebuffersize = size_table[index];\n        }\n\n        .......................省略...................\n    }\n\n}\n\n\n这里我们看到netty中用于动态调整bytebuffer容量的allochandle的实际类型为maxmessagehandle。\n\n下面我们来介绍下handleimpl中的核心字段，它们都和bytebuffer的容量有关：\n\n * minindex ：最小容量在扩缩容索引表size_tabe中的index。默认是3。\n * maxindex ：最大容量在扩缩容索引表size_tabe中的index。默认是38。\n * index ：当前容量在扩缩容索引表size_tabe中的index。初始是33。\n * nextreceivebuffersize ：预计下一次分配buffer的容量，初始为2048。在每次申请内存分配bytebuffer的时候，采用nextreceivebuffersize的值指定容量。\n * decreasenow ： 是否需要进行缩容。\n\n\n# 5. 使用堆外内存为bytebuffer分配内存\n\nadaptiverecvbytebufallocator类只是负责动态调整bytebuffer的容量，而具体为bytebuffer申请内存空间的是由pooledbytebufallocator负责。\n\n\n# 5.1 类名前缀pooled的来历\n\n在我们使用java进行日常开发过程中，在为对象分配内存空间的时候我们都会选择在 jvm 堆中为对象分配内存，这样做对我们 java 开发者特别的友好，我们只管使用就好而不必过多关心这块申请的内存如何回收，因为 jvm 堆完全受java虚拟机控制管理，java 虚拟机会帮助我们回收不再使用的内存。\n\n但是 jvm 在进行垃圾回收时候的 stop the world 会对我们应用程序的性能造成一定的影响\n\n除此之外我们在?《聊聊netty那些事儿之从内核角度看io模型》一文中介绍io模型的时候提到，当数据达到网卡时，网卡会通过dma的方式将数据拷贝到内核空间中，这是第一次拷贝。当用户线程在用户空间发起系统io调用时，cpu会将内核空间的数据再次拷贝到用户空间。这是第二次拷贝。\n\n于此不同的是当我们在 jvm 中发起 io 调用时，比如我们使用jvm堆内存读取 socket接收缓冲区 中的数据时，会多一次内存拷贝，cpu在第二次拷贝中将数据从内核空间拷贝到用户空间时，此时的用户空间站在 jvm 角度是堆外内存，所以还需要将堆外内存中的数据拷贝到 堆内内存 中。这就是 第三次内存拷贝\n\n同理当我们在jvm中发起io调用向socket发送缓冲区写入数据时，jvm会将io数据先拷贝到堆外内存，然后才能发起系统io调用。\n\n那为什么操作系统不直接使用 jvm 的堆内内存进行io操作呢？\n\n因为 jvm 的内存布局和操作系统分配的内存是不一样的，操作系统不可能按照 jvm 规范来读写数据，所以就需要 第三次拷贝 中间做个转换将堆外内存中的数据拷贝到jvm堆中。\n\n----------------------------------------\n\n所以基于上述内容，在使用jvm堆内内存时会产生以下两点性能影响：\n\n 1. jvm 在垃圾回收堆内内存时，会发生 stop the world 导致应用程序卡顿。\n 2. 在进行 io 操作的时候，会多产生一次由堆外内存到堆内内存的拷贝。\n\n基于以上两点使用jvm堆内内存对性能造成的影响，于是对性能有卓越追求的netty采用堆外内存也就是directbuffer来为bytebuffer分配内存空间。\n\n采用堆外内存为 bytebuffer 分配内存的好处就是：\n\n * 堆外内存直接受操作系统的管理，不会受jvm的管理，所以jvm垃圾回收对应用程序的性能影响就没有了。\n * 网络数据到达之后直接在堆外内存上接收，进程读取网络数据时直接在堆外内存中读取，所以就避免了第三次内存拷贝。\n\n所以 netty 在进行 i/o 操作时都是使用的堆外内存，可以避免数据从 jvm 堆内存到堆外内存的拷贝。但是由于堆外内存不受jvm的管理，所以就需要额外关注对内存的使用和释放，稍有不慎就会造成内存泄露，于是netty就引入了内存池对堆外内存进行统一管理。\n\npooledbytebufallocator类的这个前缀pooled就是内存池的意思，这个类会使用netty的内存池为bytebuffer分配堆外内存。\n\n\n# 5.2 pooledbytebufallocator的创建\n\n# 创建时机\n\n在服务端nioserversocketchannel的配置类nioserversocketchannelconfig以及客户端niosocketchannel的配置类niosocketchannelconfig实例化的时候会触发pooledbytebufallocator的创建。\n\npublic class defaultchannelconfig implements channelconfig {\n    //pooledbytebufallocator\n    private volatile bytebufallocator allocator = bytebufallocator.default;\n\n    ..........省略......\n}\n\n\n创建出来的pooledbytebufallocator实例保存在defaultchannelconfig类中的bytebufallocator allocator字段中。\n\n# 创建过程\n\npublic interface bytebufallocator {\n\n    bytebufallocator default = bytebufutil.default_allocator;\n    \n    ..................省略............\n}\npublic final class bytebufutil {\n\n    static final bytebufallocator default_allocator;\n\n    static {\n        string alloctype = systempropertyutil.get(\n                \"io.netty.allocator.type\", platformdependent.isandroid() ? \"unpooled\" : \"pooled\");\n        alloctype = alloctype.tolowercase(locale.us).trim();\n\n        bytebufallocator alloc;\n        if (\"unpooled\".equals(alloctype)) {\n            alloc = unpooledbytebufallocator.default;\n            logger.debug(\"-dio.netty.allocator.type: {}\", alloctype);\n        } else if (\"pooled\".equals(alloctype)) {\n            alloc = pooledbytebufallocator.default;\n            logger.debug(\"-dio.netty.allocator.type: {}\", alloctype);\n        } else {\n            alloc = pooledbytebufallocator.default;\n            logger.debug(\"-dio.netty.allocator.type: pooled (unknown: {})\", alloctype);\n        }\n\n        default_allocator = alloc;\n        \n        ...................省略..................\n    }\n}\n\n\n从bytebufutil类的初始化过程我们可以看出，在为bytebuffer分配内存的时候是否使用内存池在netty中是可以配置的。\n\n * 通过系统变量-d io.netty.allocator.type 可以配置是否使用内存池为bytebuffer分配内存。默认情况下是需要使用内存池的。但是在安卓系统中默认是不使用内存池的。\n * 通过pooledbytebufallocator.default获取内存池bytebuffer分配器。\n\npublic static final pooledbytebufallocator default =\n    new pooledbytebufallocator(platformdependent.directbufferpreferred());\n\n\n> 由于本文的主线是介绍sub reactor处理op_read事件的完整过程，所以这里只介绍主线相关的内容，这里只是简单介绍下在接收数据的时候为什么会用pooledbytebufallocator来为bytebuffer分配内存。而内存池的架构设计比较复杂，所以笔者后面会单独写一篇关于netty内存管理的文章。\n\n----------------------------------------\n\n\n# 总结\n\n本文介绍了sub reactor线程在处理op_read事件的整个过程。并深入剖析了adaptiverecvbytebufallocator类动态调整bytebuffer容量的原理。\n\n同时也介绍了netty为什么会使用堆外内存来为bytebuffer分配内存，并由此引出了netty的内存池分配器pooledbytebufallocator 。\n\n在介绍adaptiverecvbytebufallocator类和pooledbytebufallocator一起组合实现动态地为bytebuffer分配容量的时候，笔者不禁想起了多年前看过的《effective java》中第16条 复合优先于继承。\n\nnetty在这里也遵循了这条军规，首先两个类设计的都是单一的功能。\n\n * adaptiverecvbytebufallocator 类只负责动态的调整bytebuffer容量，并不管具体的内存分配。\n * pooledbytebufallocator 类负责具体的内存分配，用内存池的方式。\n\n这样设计的就比较灵活，具体内存分配的工作交给具体的bytebufallocator,可以使用内存池的分配方式pooledbytebufallocator，也可以不使用内存池的分配方式unpooledbytebufallocator。具体的内存可以采用jvm堆内内存（heapbuffer），也可以使用堆外内存（directbuffer）。\n\n而adaptiverecvbytebufallocator只需要关注调整它们的容量工作就可以了，而并不需要关注它们具体的内存分配方式。\n\n最后通过io.netty.channel.recvbytebufallocator.handle#allocate方法灵活组合不同的内存分配方式。这也是装饰模式的一种应用。\n\nbytebuf = allochandle.allocate(allocator);\n\n\n好了，今天的内容就到这里，我们下篇文章见~~~~\n\n\n# 参考资料\n\nnetty如何高效接收网络数据？一文聊透bytebuffer动态自适应扩缩容机制 (qq.com)",charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"第七课：Netty 中IO事件的触发时机和传播流程",frontmatter:{title:"第七课：Netty 中IO事件的触发时机和传播流程",date:"2024-09-19T11:14:00.000Z",permalink:"/pages/a1b0fe/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/25.%E5%9B%9B%E3%80%81%E6%B7%B1%E5%85%A5%20Netty%20%E6%A0%B8%E5%BF%83/25.%E7%AC%AC%E4%B8%83%E8%AF%BE%EF%BC%9ANetty%20%E4%B8%ADIO%E4%BA%8B%E4%BB%B6%E7%9A%84%E8%A7%A6%E5%8F%91%E6%97%B6%E6%9C%BA%E5%92%8C%E4%BC%A0%E6%92%AD%E6%B5%81%E7%A8%8B.html",relativePath:"Netty 系统设计/25.四、深入 Netty 核心/25.第七课：Netty 中IO事件的触发时机和传播流程.md",key:"v-58ecef53",path:"/pages/a1b0fe/",headers:[{level:2,title:"1. 前文回顾",slug:"_1-前文回顾",normalizedTitle:"1. 前文回顾",charIndex:2},{level:2,title:"2. pipeline的创建",slug:"_2-pipeline的创建",normalizedTitle:"2. pipeline的创建",charIndex:2497},{level:3,title:"2.1 HeadContext",slug:"_2-1-headcontext",normalizedTitle:"2.1 headcontext",charIndex:5296},{level:3,title:"2.2 TailContext",slug:"_2-2-tailcontext",normalizedTitle:"2.2 tailcontext",charIndex:8594},{level:4,title:"2.2.1 TailContext 作为一个 ChannelHandlerContext 的作用",slug:"_2-2-1-tailcontext-作为一个-channelhandlercontext-的作用",normalizedTitle:"2.2.1 tailcontext 作为一个 channelhandlercontext 的作用",charIndex:9326},{level:4,title:"2.2.2 TailContext 作为一个 ChannelInboundHandler 的作用",slug:"_2-2-2-tailcontext-作为一个-channelinboundhandler-的作用",normalizedTitle:"2.2.2 tailcontext 作为一个 channelinboundhandler 的作用",charIndex:12676},{level:2,title:"3. pipeline中的事件分类",slug:"_3-pipeline中的事件分类",normalizedTitle:"3. pipeline中的事件分类",charIndex:15136},{level:3,title:"3.1 inbound类事件",slug:"_3-1-inbound类事件",normalizedTitle:"3.1 inbound类事件",charIndex:15665},{level:3,title:"3.1.1 ExceptionCaught 事件",slug:"_3-1-1-exceptioncaught-事件",normalizedTitle:"3.1.1 exceptioncaught 事件",charIndex:17810},{level:3,title:"3.1.2 ChannelRegistered 事件",slug:"_3-1-2-channelregistered-事件",normalizedTitle:"3.1.2 channelregistered 事件",charIndex:20648},{level:3,title:"3.1.3 ChannelActive 事件",slug:"_3-1-3-channelactive-事件",normalizedTitle:"3.1.3 channelactive 事件",charIndex:21748},{level:3,title:"3.1.4 ChannelRead 和 ChannelReadComplete 事件",slug:"_3-1-4-channelread-和-channelreadcomplete-事件",normalizedTitle:"3.1.4 channelread 和 channelreadcomplete 事件",charIndex:24806},{level:3,title:"3.1.5 ChannelWritabilityChanged 事件",slug:"_3-1-5-channelwritabilitychanged-事件",normalizedTitle:"3.1.5 channelwritabilitychanged 事件",charIndex:27013},{level:3,title:"3.1.6 UserEventTriggered 事件",slug:"_3-1-6-usereventtriggered-事件",normalizedTitle:"3.1.6 usereventtriggered 事件",charIndex:30289},{level:3,title:"3.1.7 ChannelInactive和ChannelUnregistered事件",slug:"_3-1-7-channelinactive和channelunregistered事件",normalizedTitle:"3.1.7 channelinactive和channelunregistered事件",charIndex:31460},{level:3,title:"3.2 Outbound 类事件",slug:"_3-2-outbound-类事件",normalizedTitle:"3.2 outbound 类事件",charIndex:32185},{level:3,title:"3.2.1 read 事件",slug:"_3-2-1-read-事件",normalizedTitle:"3.2.1 read 事件",charIndex:32972},{level:3,title:"3.2.2 write 和 flush 事件",slug:"_3-2-2-write-和-flush-事件",normalizedTitle:"3.2.2 write 和 flush 事件",charIndex:36713},{level:3,title:"3.2.3 close 事件",slug:"_3-2-3-close-事件",normalizedTitle:"3.2.3 close 事件",charIndex:37952},{level:3,title:"3.2.4 deRegister 事件",slug:"_3-2-4-deregister-事件",normalizedTitle:"3.2.4 deregister 事件",charIndex:38673},{level:3,title:"3.2.5 connect 事件",slug:"_3-2-5-connect-事件",normalizedTitle:"3.2.5 connect 事件",charIndex:39434},{level:3,title:"3.2.6 disConnect 事件",slug:"_3-2-6-disconnect-事件",normalizedTitle:"3.2.6 disconnect 事件",charIndex:40318},{level:2,title:"4. 向pipeline添加channelHandler",slug:"_4-向pipeline添加channelhandler",normalizedTitle:"4. 向pipeline添加channelhandler",charIndex:41119},{level:2,title:"5. ChanneHandlerContext 的创建",slug:"_5-channehandlercontext-的创建",normalizedTitle:"5. channehandlercontext 的创建",charIndex:53663},{level:3,title:"5.1 filterName",slug:"_5-1-filtername",normalizedTitle:"5.1 filtername",charIndex:54810},{level:3,title:"5.2 childExecutor",slug:"_5-2-childexecutor",normalizedTitle:"5.2 childexecutor",charIndex:57943},{level:3,title:"5.3 ChanneHandlerContext",slug:"_5-3-channehandlercontext",normalizedTitle:"5.3 channehandlercontext",charIndex:63774},{level:2,title:"6. 从 pipeline 删除 channelHandler",slug:"_6-从-pipeline-删除-channelhandler",normalizedTitle:"6. 从 pipeline 删除 channelhandler",charIndex:75761},{level:3,title:"6.1 getContextOrDie",slug:"_6-1-getcontextordie",normalizedTitle:"6.1 getcontextordie",charIndex:76668},{level:3,title:"6.2 remove",slug:"_6-2-remove",normalizedTitle:"6.2 remove",charIndex:77682},{level:2,title:"7. pipeline 的初始化",slug:"_7-pipeline-的初始化",normalizedTitle:"7. pipeline 的初始化",charIndex:81558},{level:3,title:"7.1 NioServerSocketChannel 中 pipeline 的初始化",slug:"_7-1-nioserversocketchannel-中-pipeline-的初始化",normalizedTitle:"7.1 nioserversocketchannel 中 pipeline 的初始化",charIndex:81809},{level:3,title:"7.2 NioSocketChannel 中 pipeline 的初始化",slug:"_7-2-niosocketchannel-中-pipeline-的初始化",normalizedTitle:"7.2 niosocketchannel 中 pipeline 的初始化",charIndex:90255},{level:2,title:"8. 事件传播",slug:"_8-事件传播",normalizedTitle:"8. 事件传播",charIndex:92942},{level:3,title:"8.1 Inbound事件的传播",slug:"_8-1-inbound事件的传播",normalizedTitle:"8.1 inbound事件的传播",charIndex:93141},{level:3,title:"8.2 findContextInbound",slug:"_8-2-findcontextinbound",normalizedTitle:"8.2 findcontextinbound",charIndex:98212},{level:3,title:"8.3 skipContext",slug:"_8-3-skipcontext",normalizedTitle:"8.3 skipcontext",charIndex:99768},{level:3,title:"8.4 Outbound事件的传播",slug:"_8-4-outbound事件的传播",normalizedTitle:"8.4 outbound事件的传播",charIndex:103294},{level:3,title:"8.5 ExceptionCaught事件的传播",slug:"_8-5-exceptioncaught事件的传播",normalizedTitle:"8.5 exceptioncaught事件的传播",charIndex:103385},{level:3,title:"8.6 ExceptionCaught 事件和 Inbound 类事件的区别",slug:"_8-6-exceptioncaught-事件和-inbound-类事件的区别",normalizedTitle:"8.6 exceptioncaught 事件和 inbound 类事件的区别",charIndex:107575},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:108218}],headersStr:"1. 前文回顾 2. pipeline的创建 2.1 HeadContext 2.2 TailContext 2.2.1 TailContext 作为一个 ChannelHandlerContext 的作用 2.2.2 TailContext 作为一个 ChannelInboundHandler 的作用 3. pipeline中的事件分类 3.1 inbound类事件 3.1.1 ExceptionCaught 事件 3.1.2 ChannelRegistered 事件 3.1.3 ChannelActive 事件 3.1.4 ChannelRead 和 ChannelReadComplete 事件 3.1.5 ChannelWritabilityChanged 事件 3.1.6 UserEventTriggered 事件 3.1.7 ChannelInactive和ChannelUnregistered事件 3.2 Outbound 类事件 3.2.1 read 事件 3.2.2 write 和 flush 事件 3.2.3 close 事件 3.2.4 deRegister 事件 3.2.5 connect 事件 3.2.6 disConnect 事件 4. 向pipeline添加channelHandler 5. ChanneHandlerContext 的创建 5.1 filterName 5.2 childExecutor 5.3 ChanneHandlerContext 6. 从 pipeline 删除 channelHandler 6.1 getContextOrDie 6.2 remove 7. pipeline 的初始化 7.1 NioServerSocketChannel 中 pipeline 的初始化 7.2 NioSocketChannel 中 pipeline 的初始化 8. 事件传播 8.1 Inbound事件的传播 8.2 findContextInbound 8.3 skipContext 8.4 Outbound事件的传播 8.5 ExceptionCaught事件的传播 8.6 ExceptionCaught 事件和 Inbound 类事件的区别 总结",content:"# 1. 前文回顾\n\n在前边的系列文章中，笔者为大家详细剖析了 Reactor 模型在 netty 中的创建，启动，运行，接收连接，接收数据，发送数据的完整流程，在详细剖析整个 Reactor 模型如何在 netty 中实现的过程里，我们或多或少的见到了 pipeline 的身影。\n\nReactor启动后的结构.png\n\n比如在 Reactor 启动的过程中首先需要创建 NioServerSocketChannel ，在创建的过程中会为 NioServerSocketChannel 创建分配一个 pipeline ，用于对 OP_ACCEPT 事件的编排。\n\n当 NioServerSocketChannel 向 main reactor 注册成功后，会在 pipeline 中触发 ChannelRegistered 事件的传播。\n\n当 NioServerSocketChannel 绑定端口成功后，会在 pipeline 中触发 ChannelActive 事件的传播。\n\n主从Reactor组完整结构.png\n\n又比如在 Reactor 接收连接的过程中，当客户端发起一个连接并完成三次握手之后，连接对应的 Socket 会存放在内核中的全连接队列中，随后 JDK Selector 会通知 main reactor 此时 NioServerSocketChannel 上有 OP_ACCEPT 事件活跃，最后 main reactor 开始执行 NioServerSocketChannel 的底层操作类 NioMessageUnsafe#read 方法在 NioServerSocketChannel 中的 pipeline 中传播 ChannelRead 事件。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)传播ChannelRead事件.png\n\n最终会在 NioServerSocketChannel 的 pipeline 中的 ServerBootstrapAcceptor 中响应 ChannelRead 事件并创建初始化 NioSocketChannel ，随后会为每一个新创建的 NioSocetChannel 创建分配一个独立的 pipeline ，用于各自 NioSocketChannel 上的 IO 事件的编排。并向 sub reactor 注册 NioSocketChannel ，随后在 NioSocketChannel 的 pipeline 中传播 ChannelRegistered 事件，最后传播 ChannelActive 事件。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)传播ChannelRegister事件.png\n\n还有在《Netty如何高效接收网络数据》一文中，我们也提过当 sub reactor 读取 NioSocketChannel 中来自客户端的请求数据时，会在 NioSocketChannel 的 pipeline 中传播 ChannelRead 事件，在一个完整的 read loop 读取完毕后会传播 ChannelReadComplete 事件。\n\n在《一文搞懂Netty发送数据全流程》一文中，我们讲到了在用户经过业务处理后，通过 write 方法和 flush 方法分别在 NioSocketChannel 的 pipeline 中传播 write 事件和 flush 事件的过程。\n\n笔者带大家又回顾了一下在前边系列文章中关于 pipeline 的使用场景，但是在这些系列文章中并未对 pipeline 相关的细节进行完整全面地描述，那么本文笔者将为大家详细的剖析下 pipeline 在 IO 事件的编排和传播场景下的完整实现原理。\n\n内容概要.png\n\n\n# 2. pipeline的创建\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)主从Reactor组完整结构.png\n\nNetty 会为每一个 Channel 分配一个独立的 pipeline ，pipeline 伴随着 channel 的创建而创建。\n\n前边介绍到 NioServerSocketChannel 是在 netty 服务端启动的过程中创建的。而 NioSocketChannel 的创建是在当 NioServerSocketChannel 上的 OP_ACCEPT 事件活跃时，由 main reactor 线程在 NioServerSocketChannel 中创建，并在 NioServerSocketChannel 的 pipeline 中对 OP_ACCEPT 事件进行编排时（图中的 ServerBootstrapAcceptor 中）初始化的。\n\n无论是创建 NioServerSocketChannel 里的 pipeline 还是创建 NioSocketChannel 里的 pipeline , 最终都会委托给它们的父类 AbstractChannel 。\n\nimage.png\n\npublic abstract class AbstractChannel extends DefaultAttributeMap implements Channel {\n\n    protected AbstractChannel(Channel parent) {\n        this.parent = parent;\n        //channel全局唯一ID machineId+processId+sequence+timestamp+random\n        id = newId();\n        //unsafe用于底层socket的相关操作\n        unsafe = newUnsafe();\n        //为channel分配独立的pipeline用于IO事件编排\n        pipeline = newChannelPipeline();\n    }\n\n    protected DefaultChannelPipeline newChannelPipeline() {\n        return new DefaultChannelPipeline(this);\n    }\n\n}\npublic class DefaultChannelPipeline implements ChannelPipeline {\n\n      ....................\n\n    //pipeline中的头结点\n    final AbstractChannelHandlerContext head;\n    //pipeline中的尾结点\n    final AbstractChannelHandlerContext tail;\n\n    //pipeline中持有对应channel的引用\n    private final Channel channel;\n\n       ....................\n\n    protected DefaultChannelPipeline(Channel channel) {\n        //pipeline中持有对应channel的引用\n        this.channel = ObjectUtil.checkNotNull(channel, \"channel\");\n        \n        ............省略.......\n\n        tail = new TailContext(this);\n        head = new HeadContext(this);\n\n        head.next = tail;\n        tail.prev = head;\n    }\n\n       ....................\n}\n\n\n在前边的系列文章中笔者多次提到过，pipeline 的结构是由 ChannelHandlerContext 类型的节点构成的双向链表。其中头结点为 HeadContext ，尾结点为 TailContext 。其初始结构如下：\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)pipeline的初始结构.png\n\n\n# 2.1 HeadContext\n\n    private static final String HEAD_NAME = generateName0(HeadContext.class);\n\n    final class HeadContext extends AbstractChannelHandlerContext\n            implements ChannelOutboundHandler, ChannelInboundHandler {\n       //headContext中持有对channel unsafe操作类的引用 用于执行channel底层操作\n        private final Unsafe unsafe;\n\n        HeadContext(DefaultChannelPipeline pipeline) {\n            super(pipeline, null, HEAD_NAME, HeadContext.class);\n            //持有channel unsafe操作类的引用，后续用于执行channel底层操作\n            unsafe = pipeline.channel().unsafe();\n            //设置channelHandler的状态为ADD_COMPLETE\n            setAddComplete();\n        }\n\n        @Override\n        public ChannelHandler handler() {\n            return this;\n        }\n\n        .......................\n    }\n\n\n我们知道双向链表结构的 pipeline 中的节点元素为 ChannelHandlerContext ，既然 HeadContext 作为 pipeline 的头结点，那么它一定是 ChannelHandlerContext 类型的，所以它需要继承实现 AbstractChannelHandlerContext ，相当于一个哨兵的作用，因为用户可以以任意顺序向 pipeline 中添加 ChannelHandler ，需要用 HeadContext 来固定指向第一个 ChannelHandlerContext 。\n\n> 在《一文搞懂Netty发送数据全流程》 一文中的《1. ChannelHandlerContext》小节中，笔者曾为大家详细介绍过 ChannelHandlerContext 在 pipeline 中的作用，忘记的同学可以在回看下。\n\n于此同时 HeadContext 又实现了 ChannelInboundHandler 和 ChannelOutboundHandler 接口，说明 HeadContext 即是一个 ChannelHandlerContext 又是一个 ChannelHandler ，它可以同时处理 Inbound 事件和 Outbound 事件。\n\n我们也注意到 HeadContext 中持有了对应 channel 的底层操作类 unsafe ，这也说明 IO 事件在 pipeline 中的传播最终会落在 HeadContext 中进行最后的 IO 处理。它是 Inbound 事件的处理起点，也是 Outbound 事件的处理终点。这里也可以看出 HeadContext 除了起到哨兵的作用，它还承担了对 channel 底层相关的操作。\n\n比如我们在《Reactor在Netty中的实现(启动篇)》中介绍的 NioServerSocketChannel 在向 main reactor 注册完成后会触发 ChannelRegistered 事件从 HeadContext 开始依次在 pipeline 中向后传播。\n\n      @Override\n        public void channelRegistered(ChannelHandlerContext ctx) {\n            //此时firstRegistration已经变为false,在pipeline.invokeHandlerAddedIfNeeded中已被调用过\n            invokeHandlerAddedIfNeeded();\n            ctx.fireChannelRegistered();\n        }\n\n\n以及 NioServerSocketChannel 在与端口绑定成功后会触发 ChannelActive 事件从 HeadContext 开始依次在 pipeline 中向后传播，并在 HeadContext 中通过 unsafe.beginRead() 注册 OP_ACCEPT 事件到 main reactor 中。\n\n     @Override\n        public void read(ChannelHandlerContext ctx) {\n            //触发注册OP_ACCEPT或者OP_READ事件\n            unsafe.beginRead();\n        }\n\n\n同理在 NioSocketChannel 在向 sub reactor 注册成功后。会先后触发 ChannelRegistered 事件和 ChannelActive 事件从 HeadContext 开始在 pipeline 中向后传播。并在 HeadContext 中通过 unsafe.beginRead() 注册 OP_READ 事件到 sub reactor 中。\n\n        @Override\n        public void channelActive(ChannelHandlerContext ctx) {\n            //pipeline中继续向后传播channelActive事件\n            ctx.fireChannelActive();\n            //如果是autoRead 则自动触发read事件传播\n            //在read回调函数中 触发OP_ACCEPT或者OP_READ事件注册\n            readIfIsAutoRead();\n        }\n\n\n在《一文搞懂Netty发送数据全流程》中介绍的 write 事件和 flush 事件最终会在 pipeline 中从后向前一直传播到 HeadContext ，并在 HeadContext 中相应事件回调函数中调用 unsafe 类操作底层 channel 发送数据。\n\n        @Override\n        public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) {\n            //到headContext这里 msg的类型必须是ByteBuffer，也就是说必须经过编码器将业务层写入的实体编码为ByteBuffer\n            unsafe.write(msg, promise);\n        }\n\n        @Override\n        public void flush(ChannelHandlerContext ctx) {\n            unsafe.flush();\n        }\n\n\n> 从本小节的内容介绍中，我们可以看出在 Netty 中对于 Channel 的相关底层操作调用均是在 HeadContext 中触发的。\n\n\n# 2.2 TailContext\n\n    private static final String TAIL_NAME = generateName0(TailContext.class);\n\n    final class TailContext extends AbstractChannelHandlerContext implements ChannelInboundHandler {\n\n        TailContext(DefaultChannelPipeline pipeline) {\n            super(pipeline, null, TAIL_NAME, TailContext.class);\n            //设置channelHandler的状态为ADD_COMPLETE\n            setAddComplete();\n        }\n\n        @Override\n        public ChannelHandler handler() {\n            return this;\n        }\n    \n        ......................\n}\n\n\n同样 TailContext 作为双向链表结构的 pipeline 中的尾结点，也需要继承实现 AbstractChannelHandlerContext 。但它同时又实现了 ChannelInboundHandler 。\n\n这说明 TailContext 除了是一个 ChannelHandlerContext 同时也是一个 ChannelInboundHandler 。\n\n# 2.2.1 TailContext 作为一个 ChannelHandlerContext 的作用\n\nTailContext 作为一个 ChannelHandlerContext 的作用是负责将 outbound 事件从 pipeline 的末尾一直向前传播直到 HeadContext 。当然前提是用户需要调用 channel 的相关 outbound 方法。\n\npublic interface Channel extends AttributeMap, ChannelOutboundInvoker, Comparable<Channel> {\n\n    ChannelFuture write(Object msg);\n\n    ChannelFuture write(Object msg, ChannelPromise promise);\n\n    ChannelOutboundInvoker flush();\n\n    ChannelFuture writeAndFlush(Object msg, ChannelPromise promise);\n\n    ChannelFuture writeAndFlush(Object msg);\n\n}\npublic abstract class AbstractChannel extends DefaultAttributeMap implements Channel {\n\n   @Override\n    public ChannelFuture write(Object msg) {\n        return pipeline.write(msg);\n    }\n\n    @Override\n    public Channel flush() {\n        pipeline.flush();\n        return this;\n    }\n\n    @Override\n    public ChannelFuture writeAndFlush(Object msg) {\n        return pipeline.writeAndFlush(msg);\n    }\n}\npublic class DefaultChannelPipeline implements ChannelPipeline {\n\n   @Override\n    public final ChannelFuture write(Object msg) {\n        return tail.write(msg);\n    }\n\n    @Override\n    public final ChannelPipeline flush() {\n        tail.flush();\n        return this;\n    }\n\n   @Override\n    public final ChannelFuture writeAndFlush(Object msg) {\n        return tail.writeAndFlush(msg);\n    }\n\n}\n\n\n这里我们可以看到，当我们在自定义 ChannelHandler 中调用 ctx.channel().write(msg) 时，会在 AbstractChannel 中触发 pipeline.write(msg) ，最终在 DefaultChannelPipeline 中调用 tail.write(msg) 。使得 write 事件可以从 pipeline 的末尾开始向前传播，其他 outbound 事件的传播也是一样的道理。\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n        ctx.channel().write(msg);\n    }\n\n}\n\n\n而我们自定义的 ChannelHandler 会被封装在一个 ChannelHandlerContext 中从而加入到 pipeline 中，而这个用于装载自定义 ChannelHandler 的 ChannelHandlerContext 与 TailContext 一样本质也都是 ChannelHandlerContext ，只不过在 pipeline 中的位置不同罢了。\n\n客户端channel pipeline结构.png\n\npublic interface ChannelHandlerContext extends AttributeMap, ChannelInboundInvoker, ChannelOutboundInvoker {\n\n    ChannelFuture write(Object msg);\n\n    ChannelFuture write(Object msg, ChannelPromise promise);\n\n    ChannelOutboundInvoker flush();\n\n    ChannelFuture writeAndFlush(Object msg, ChannelPromise promise);\n\n    ChannelFuture writeAndFlush(Object msg);\n\n}\n\n\n我们看到 ChannelHandlerContext 接口本身也会继承 ChannelInboundInvoker 和 ChannelOutboundInvoker 接口，所以说 ContextHandlerContext 也可以触发 inbound 事件和 outbound 事件，只不过表达的语义是在 pipeline 中从当前 ChannelHandler 开始向前或者向后传播 outbound 事件或者 inbound 事件。\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n        ctx.write(msg);\n    }\n\n}\n\n\n这里表示 write 事件从当前 EchoServerHandler 开始在 pipeline 中向前传播直到 HeadContext 。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)客户端channel pipeline结构.png\n\n# 2.2.2 TailContext 作为一个 ChannelInboundHandler 的作用\n\n最后 TailContext 作为一个 ChannelInboundHandler 的作用就是为 inbound 事件在 pipeline 中的传播做一个兜底的处理。\n\n这里提到的兜底处理是什么意思呢？\n\n比如我们前边介绍到的，在 NioSocketChannel 向 sub reactor 注册成功后之后触发的 ChannelRegistered 事件和 ChannelActive 事件。或者在 reactor 线程读取 NioSocketChannel 中的请求数据时所触发的 channelRead 事件和 ChannelReadComplete 事件。\n\n这些 inbound 事件都会首先从 HeadContext 开始在 pipeline 中一个一个的向后传递。\n\n极端的情况是如果 pipeline 中所有 ChannelInboundHandler 中相应的 inbound 事件回调方法均不对事件作出处理，并继续向后传播。如下示例代码所示：\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n        ctx.fireChannelRead(msg);\n    }\n\n    @Override\n    public void channelReadComplete(ChannelHandlerContext ctx) {\n        ctx.fireChannelReadComplete();\n    }\n\n    @Override\n    public void channelRegistered(ChannelHandlerContext ctx) throws Exception {\n        ctx.fireChannelRegistered();\n    }\n\n    @Override\n    public void channelActive(ChannelHandlerContext ctx) throws Exception {\n        ctx.fireChannelActive();\n    }\n}\n\n\n最终这些 inbound 事件在 pipeline 中得不到处理，最后会传播到 TailContext 中。\n\nfinal class TailContext extends AbstractChannelHandlerContext implements ChannelInboundHandler {\n\n        @Override\n        public void channelRead(ChannelHandlerContext ctx, Object msg) {\n            onUnhandledInboundMessage(ctx, msg);\n        }\n\n        @Override\n        public void channelReadComplete(ChannelHandlerContext ctx) {\n            onUnhandledInboundChannelReadComplete();\n        }\n\n        @Override\n        public void channelActive(ChannelHandlerContext ctx) {\n            onUnhandledInboundChannelActive();\n        }\n\n}\n\n\n而在 TailContext 中需要对这些得不到任何处理的 inbound 事件做出最终处理。比如丢弃该 msg，并释放所占用的 directByteBuffer，以免发生内存泄露。\n\n    protected void onUnhandledInboundMessage(ChannelHandlerContext ctx, Object msg) {\n        onUnhandledInboundMessage(msg);\n        if (logger.isDebugEnabled()) {\n            logger.debug(\"Discarded message pipeline : {}. Channel : {}.\",\n                         ctx.pipeline().names(), ctx.channel());\n        }\n    }\n\n    protected void onUnhandledInboundMessage(Object msg) {\n        try {\n            logger.debug(\n                    \"Discarded inbound message {} that reached at the tail of the pipeline. \" +\n                            \"Please check your pipeline configuration.\", msg);\n        } finally {\n            ReferenceCountUtil.release(msg);\n        }\n    }\n\n\n\n# 3. pipeline中的事件分类\n\n在前边的系列文章中，笔者多次介绍过，Netty 中的 IO 事件一共分为两大类：inbound 类事件和 outbound 类事件。其实如果严格来分的话应该分为三类。第三种事件类型为 exceptionCaught 异常事件类型。\n\n而 exceptionCaught 事件在事件传播角度上来说和 inbound 类事件一样，都是从 pipeline 的 HeadContext 开始一直向后传递或者从当前 ChannelHandler 开始一直向后传递直到 TailContext 。所以一般也会将 exceptionCaught 事件统一归为 inbound 类事件。\n\n而根据事件类型的分类，相应负责处理事件回调的 ChannelHandler 也会被分为两类：\n\n * ChannelInboundHandler ：主要负责响应处理 inbound 类事件回调和 exceptionCaught 事件回调。\n * ChannelOutboundHandler ：主要负责响应处理 outbound 类事件回调。\n\n那么我们常说的 inbound 类事件和 outbound 类事件具体都包含哪些事件呢？\n\n\n# 3.1 inbound类事件\n\nfinal class ChannelHandlerMask {\n\n    // inbound事件集合\n    static final int MASK_ONLY_INBOUND =  MASK_CHANNEL_REGISTERED |\n            MASK_CHANNEL_UNREGISTERED | MASK_CHANNEL_ACTIVE | MASK_CHANNEL_INACTIVE | MASK_CHANNEL_READ |\n            MASK_CHANNEL_READ_COMPLETE | MASK_USER_EVENT_TRIGGERED | MASK_CHANNEL_WRITABILITY_CHANGED;\n\n    private static final int MASK_ALL_INBOUND = MASK_EXCEPTION_CAUGHT | MASK_ONLY_INBOUND;\n\n    // inbound 类事件相关掩码\n    static final int MASK_EXCEPTION_CAUGHT = 1;\n    static final int MASK_CHANNEL_REGISTERED = 1 << 1;\n    static final int MASK_CHANNEL_UNREGISTERED = 1 << 2;\n    static final int MASK_CHANNEL_ACTIVE = 1 << 3;\n    static final int MASK_CHANNEL_INACTIVE = 1 << 4;\n    static final int MASK_CHANNEL_READ = 1 << 5;\n    static final int MASK_CHANNEL_READ_COMPLETE = 1 << 6;\n    static final int MASK_USER_EVENT_TRIGGERED = 1 << 7;\n    static final int MASK_CHANNEL_WRITABILITY_CHANGED = 1 << 8;\n\n}\n\n\nnetty 会将其支持的所有异步事件用掩码来表示，定义在 ChannelHandlerMask 类中， netty 框架通过这些事件掩码可以很方便的知道用户自定义的 ChannelHandler 是属于什么类型的（ChannelInboundHandler or ChannelOutboundHandler ）。\n\n除此之外，inbound 类事件如此之多，用户也并不是对所有的 inbound 类事件感兴趣，用户可以在自定义的 ChannelInboundHandler 中覆盖自己感兴趣的 inbound 事件回调，从而达到针对特定 inbound 事件的监听。\n\n这些用户感兴趣的 inbound 事件集合同样也会用掩码的形式保存在自定义 ChannelHandler 对应的 ChannelHandlerContext 中，这样当特定 inbound 事件在 pipeline 中开始传播的时候，netty 可以根据对应 ChannelHandlerContext 中保存的 inbound 事件集合掩码来判断，用户自定义的 ChannelHandler 是否对该 inbound 事件感兴趣，从而决定是否执行用户自定义 ChannelHandler 中的相应回调方法或者跳过对该 inbound 事件不感兴趣的 ChannelHandler 继续向后传播。\n\n> 从以上描述中，我们也可以窥探出，Netty 引入 ChannelHandlerContext 来封装 ChannelHandler 的原因，在代码设计上还是遵循单一职责的原则， ChannelHandler 是用户接触最频繁的一个 netty 组件，netty 希望用户能够把全部注意力放在最核心的 IO 处理上，用户只需要关心自己对哪些异步事件感兴趣并考虑相应的处理逻辑即可，而并不需要关心异步事件在 pipeline 中如何传递，如何选择具有执行条件的 ChannelHandler 去执行或者跳过。这些切面性质的逻辑，netty 将它们作为上下文信息全部封装在 ChannelHandlerContext 中由netty框架本身负责处理。\n\n> 以上这些内容，笔者还会在事件传播相关小节做详细的介绍，之所以这里引出，还是为了让大家感受下利用掩码进行集合操作的便利性，netty 中类似这样的设计还有很多，比如前边系列文章中多次提到过的，channel 再向 reactor 注册 IO 事件时，netty 也是将 channel 感兴趣的 IO 事件用掩码的形式存储于 SelectionKey 中的 int interestOps 中。\n\n接下来笔者就为大家介绍下这些 inbound 事件，并梳理出这些 inbound 事件的触发时机。方便大家根据各自业务需求灵活地进行监听。\n\n\n# 3.1.1 ExceptionCaught 事件\n\n在本小节介绍的这些 inbound 类事件在 pipeline 中传播的过程中，如果在相应事件回调函数执行的过程中发生异常，那么就会触发对应 ChannelHandler 中的 exceptionCaught 事件回调。\n\n    private void invokeExceptionCaught(final Throwable cause) {\n        if (invokeHandler()) {\n            try {\n                handler().exceptionCaught(this, cause);\n            } catch (Throwable error) {\n                if (logger.isDebugEnabled()) {\n                    logger.debug(\n                        \"An exception {}\" +\n                        \"was thrown by a user handler's exceptionCaught() \" +\n                        \"method while handling the following exception:\",\n                        ThrowableUtil.stackTraceToString(error), cause);\n                } else if (logger.isWarnEnabled()) {\n                    logger.warn(\n                        \"An exception '{}' [enable DEBUG level for full stacktrace] \" +\n                        \"was thrown by a user handler's exceptionCaught() \" +\n                        \"method while handling the following exception:\", error, cause);\n                }\n            }\n        } else {\n            fireExceptionCaught(cause);\n        }\n    }\n\n\n当然用户可以选择在 exceptionCaught 事件回调中是否执行 ctx.fireExceptionCaught(cause) 从而决定是否将 exceptionCaught 事件继续向后传播。\n\n    @Override\n    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {\n        ..........\n        ctx.fireExceptionCaught(cause);\n    }\n\n\n当 netty 内核处理连接的接收，以及数据的读取过程中如果发生异常，会在整个 pipeline 中触发 exceptionCaught 事件的传播。\n\n这里笔者为什么要单独强调在 inbound 事件传播的过程中发生异常，才会回调 exceptionCaught 呢 ?\n\n因为 inbound 事件一般都是由 netty 内核触发传播的，而 outbound 事件一般都是由用户选择触发的，比如用户在处理完业务逻辑触发的 write 事件或者 flush 事件。\n\n而在用户触发 outbound 事件后，一般都会得到一个 ChannelPromise 。用户可以向 ChannelPromise 添加各种 listener 。当 outbound 事件在传播的过程中发生异常时，netty 会通知用户持有的这个 ChannelPromise ，但不会触发 exceptionCaught 的回调。\n\n比如我们在《一文搞懂Netty发送数据全流程》一文中介绍到的在 write 事件传播的过程中就不会触发 exceptionCaught 事件回调。只是去通知用户的 ChannelPromise 。\n\n    private void invokeWrite0(Object msg, ChannelPromise promise) {\n        try {\n            //调用当前ChannelHandler中的write方法\n            ((ChannelOutboundHandler) handler()).write(this, msg, promise);\n        } catch (Throwable t) {\n            notifyOutboundHandlerException(t, promise);\n        }\n    }\n\n    private static void notifyOutboundHandlerException(Throwable cause, ChannelPromise promise) {\n        PromiseNotificationUtil.tryFailure(promise, cause, promise instanceof VoidChannelPromise ? null : logger);\n    }\n\n\n而 outbound 事件中只有 flush 事件的传播是个例外，当 flush 事件在 pipeline 传播的过程中发生异常时，会触发对应异常 ChannelHandler 的 exceptionCaught 事件回调。因为 flush 方法的签名中不会给用户返回 ChannelPromise 。\n\n    @Override\n    ChannelHandlerContext flush();\n    private void invokeFlush0() {\n        try {\n            ((ChannelOutboundHandler) handler()).flush(this);\n        } catch (Throwable t) {\n            invokeExceptionCaught(t);\n        }\n    }\n\n\n\n# 3.1.2 ChannelRegistered 事件\n\n当 main reactor 在启动的时候，NioServerSocketChannel 会被创建并初始化，随后就会向main reactor注册，当注册成功后就会在 NioServerSocketChannel 中的 pipeline 中传播 ChannelRegistered 事件。\n\n当 main reactor 接收客户端发起的连接后，NioSocketChannel 会被创建并初始化，随后会向 sub reactor 注册，当注册成功后会在 NioSocketChannel 中的 pipeline 传播 ChannelRegistered 事件。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)传播ChannelRegister事件.png\n\nprivate void register0(ChannelPromise promise) {\n\n        ................\n        //执行真正的注册操作\n        doRegister();\n\n        ...........\n\n        //触发channelRegister事件\n        pipeline.fireChannelRegistered();\n\n        .......\n}\n\n\n> 注意：此时对应的 channel 还没有注册 IO 事件到相应的 reactor 中。\n\n\n# 3.1.3 ChannelActive 事件\n\n当 NioServerSocketChannel 再向 main reactor 注册成功并触发 ChannelRegistered 事件传播之后，随后就会在 pipeline 中触发 bind 事件，而 bind 事件是一个 outbound 事件，会从 pipeline 中的尾结点 TailContext 一直向前传播最终在 HeadContext 中执行真正的绑定操作。\n\n     @Override\n        public void bind(\n                ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) {\n            //触发AbstractChannel->bind方法 执行JDK NIO SelectableChannel 执行底层绑定操作\n            unsafe.bind(localAddress, promise);\n        }\n       @Override\n        public final void bind(final SocketAddress localAddress, final ChannelPromise promise) {\n             ..............\n\n            doBind(localAddress);\n\n            ...............\n\n            //绑定成功后 channel激活 触发channelActive事件传播\n            if (!wasActive && isActive()) {\n                invokeLater(new Runnable() {\n                    @Override\n                    public void run() {\n                        //HeadContext->channelActive回调方法 执行注册OP_ACCEPT事件\n                        pipeline.fireChannelActive();\n                    }\n                });\n            }\n  \n            ...............\n        }\n\n\n当 netty 服务端 NioServerSocketChannel 绑定端口成功之后，才算是真正的 Active ，随后触发 ChannelActive 事件在 pipeline 中的传播。\n\n之前我们也提到过判断 NioServerSocketChannel 是否 Active 的标准就是 : 底层 JDK Nio ServerSocketChannel 是否 open 并且 ServerSocket 是否已经完成绑定。\n\n    @Override\n    public boolean isActive() {\n        return isOpen() && javaChannel().socket().isBound();\n    }\n\n\n而客户端 NioSocketChannel 中触发 ChannelActive 事件就会比较简单，当 NioSocketChannel 再向 sub reactor 注册成功并触发 ChannelRegistered 之后，紧接着就会触发 ChannelActive 事件在 pipeline 中传播。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)传播ChannelActive事件.png\n\nprivate void register0(ChannelPromise promise) {\n\n        ................\n        //执行真正的注册操作\n        doRegister();\n\n        ...........\n\n        //触发channelRegister事件\n        pipeline.fireChannelRegistered();\n\n        .......\n\n        if (isActive()) {\n\n                    if (firstRegistration) {\n                        //触发channelActive事件\n                        pipeline.fireChannelActive();\n                    } else if (config().isAutoRead()) {\n                        beginRead();\n                    }\n          }\n}\n\n\n而客户端 NioSocketChannel 是否 Active 的标识是：底层 JDK NIO SocketChannel 是否 open 并且底层 socket 是否连接。毫无疑问，这里的 socket 一定是 connected 。所以直接触发 ChannelActive 事件。\n\n    @Override\n    public boolean isActive() {\n        SocketChannel ch = javaChannel();\n        return ch.isOpen() && ch.isConnected();\n    }\n\n\n> 注意：此时 channel 才会到相应的 reactor 中去注册感兴趣的 IO 事件。当用户自定义的 ChannelHandler 接收到 ChannelActive 事件时，表明 IO 事件已经注册到 reactor 中了。\n\n\n# 3.1.4 ChannelRead 和 ChannelReadComplete 事件\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)接收客户端连接.png\n\n当客户端有新连接请求的时候，服务端的 NioServerSocketChannel 上的 OP_ACCEPT 事件会活跃，随后 main reactor 会在一个 read loop 中不断的调用 serverSocketChannel.accept() 接收新的连接直到全部接收完毕或者达到 read loop 最大次数 16 次。\n\n在 NioServerSocketChannel 中，每 accept 一个新的连接，就会在 pipeline 中触发 ChannelRead 事件。一个完整的 read loop 结束之后，会触发 ChannelReadComplete 事件。\n\n    private final class NioMessageUnsafe extends AbstractNioUnsafe {\n\n        @Override\n        public void read() {\n            ......................\n\n\n                try {\n                    do {\n                        //底层调用NioServerSocketChannel->doReadMessages 创建客户端SocketChannel\n                        int localRead = doReadMessages(readBuf);\n                        .................\n                    } while (allocHandle.continueReading());\n\n                } catch (Throwable t) {\n                    exception = t;\n                }\n\n                int size = readBuf.size();\n                for (int i = 0; i < size; i ++) {            \n                    pipeline.fireChannelRead(readBuf.get(i));\n                }\n\n                pipeline.fireChannelReadComplete();\n\n                     .................\n        }\n    }\n\n\n当客户端 NioSocketChannel 上有请求数据到来时，NioSocketChannel 上的 OP_READ 事件活跃，随后 sub reactor 也会在一个 read loop 中对 NioSocketChannel 中的请求数据进行读取直到读取完毕或者达到 read loop 的最大次数 16 次。\n\n在 read loop 的读取过程中，每读取一次就会在 pipeline 中触发 ChannelRead 事件。当一个完整的 read loop 结束之后，会在 pipeline 中触发 ChannelReadComplete 事件。\n\nNetty接收网络数据流程.png\n\n这里需要注意的是当 ChannelReadComplete 事件触发时，此时并不代表 NioSocketChannel 中的请求数据已经读取完毕，可能的情况是发送的请求数据太多，在一个 read loop 中读取不完达到了最大限制次数 16 次，还没全部读取完毕就退出了 read loop 。一旦退出 read loop 就会触发 ChannelReadComplete 事件。详细内容可以查看笔者的这篇文章《Netty如何高效接收网络数据》。\n\n\n# 3.1.5 ChannelWritabilityChanged 事件\n\n当我们处理完业务逻辑得到业务处理结果后，会调用 ctx.write(msg) 触发 write 事件在 pipeline 中的传播。\n\n    @Override\n    public void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n         ctx.write(msg);\n    }\n\n\n最终 netty 会将发送数据 msg 写入 NioSocketChannel 中的待发送缓冲队列 ChannelOutboundBuffer 中。并等待用户调用 flush 操作从 ChannelOutboundBuffer 中将待发送数据 msg ，写入到底层 Socket 的发送缓冲区中。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)ChannelOutboundBuffer中缓存待发送数据.png\n\n当对端的接收处理速度非常慢或者网络状况极度拥塞时，使得 TCP 滑动窗口不断的缩小，这就导致发送端的发送速度也变得越来越小，而此时用户还在不断的调用 ctx.write(msg) ，这就会导致 ChannelOutboundBuffer 会急剧增大，从而可能导致 OOM 。netty 引入了高低水位线来控制 ChannelOutboundBuffer 的内存占用。\n\npublic final class WriteBufferWaterMark {\n\n    private static final int DEFAULT_LOW_WATER_MARK = 32 * 1024;\n    private static final int DEFAULT_HIGH_WATER_MARK = 64 * 1024;\n}\n\n\n当 ChanneOutboundBuffer 中的内存占用量超过高水位线时，netty 就会将对应的 channel 置为不可写状态，并在 pipeline 中触发 ChannelWritabilityChanged 事件。\n\n    private void setUnwritable(boolean invokeLater) {\n        for (;;) {\n            final int oldValue = unwritable;\n            final int newValue = oldValue | 1;\n            if (UNWRITABLE_UPDATER.compareAndSet(this, oldValue, newValue)) {\n                if (oldValue == 0) {\n                    //触发fireChannelWritabilityChanged事件 表示当前channel变为不可写\n                    fireChannelWritabilityChanged(invokeLater);\n                }\n                break;\n            }\n        }\n    }\n\n\n当 ChannelOutboundBuffer 中的内存占用量低于低水位线时，netty 又会将对应的 NioSocketChannel 设置为可写状态，并再次触发 ChannelWritabilityChanged 事件。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)响应channelWritabilityChanged事件.png\n\n    private void setWritable(boolean invokeLater) {\n        for (;;) {\n            final int oldValue = unwritable;\n            final int newValue = oldValue & ~1;\n            if (UNWRITABLE_UPDATER.compareAndSet(this, oldValue, newValue)) {\n                if (oldValue != 0 && newValue == 0) {\n                    fireChannelWritabilityChanged(invokeLater);\n                }\n                break;\n            }\n        }\n    }\n\n\n用户可在自定义 ChannelHandler 中通过 ctx.channel().isWritable() 判断当前 channel 是否可写。\n\n    @Override\n    public void channelWritabilityChanged(ChannelHandlerContext ctx) throws Exception {\n\n        if (ctx.channel().isWritable()) {\n            ...........当前channel可写.........\n        } else {\n            ...........当前channel不可写.........\n        }\n    }\n\n\n\n# 3.1.6 UserEventTriggered 事件\n\nnetty 提供了一种事件扩展机制可以允许用户自定义异步事件，这样可以使得用户能够灵活的定义各种复杂场景的处理机制。\n\n下面我们来看下如何在 Netty 中自定义异步事件。\n\n 1. 定义异步事件。\n\npublic final class OurOwnDefinedEvent {\n \n    public static final OurOwnDefinedEvent INSTANCE = new OurOwnDefinedEvent();\n\n    private OurOwnDefinedEvent() { }\n}\n\n\n 1. 触发自定义事件的传播\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n            ......省略.......\n            //事件在pipeline中从当前ChannelHandlerContext开始向后传播\n            ctx.fireUserEventTriggered(OurOwnDefinedEvent.INSTANCE);\n            //事件从pipeline的头结点headContext开始向后传播\n            ctx.channel().pipeline().fireUserEventTriggered(OurOwnDefinedEvent.INSTANCE);\n\n    }\n}\n     \n\n\n 1. 自定义事件的响应和处理。\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception {\n\n        if (OurOwnDefinedEvent.INSTANCE == evt) {\n              .....自定义事件处理......\n        }\n    }\n\n}\n\n\n后续随着我们源码解读的深入，我们还会看到 Netty 自己本身也定义了许多 UserEvent 事件，我们后面还会在介绍，大家这里只是稍微了解一下相关的用法即可。\n\n\n# 3.1.7 ChannelInactive和ChannelUnregistered事件\n\n当 Channel 被关闭之后会在 pipeline 中先触发 ChannelInactive 事件的传播然后在触发 ChannelUnregistered 事件的传播。\n\n我们可以在 Inbound 类型的 ChannelHandler 中响应 ChannelInactive 和 ChannelUnregistered 事件。\n\n    @Override\n    public void channelInactive(ChannelHandlerContext ctx) throws Exception {\n        \n        ......响应inActive事件...\n        \n        //继续向后传播inActive事件\n        super.channelInactive(ctx);\n    }\n\n    @Override\n    public void channelUnregistered(ChannelHandlerContext ctx) throws Exception {\n        \n          ......响应Unregistered事件...\n\n        //继续向后传播Unregistered事件\n        super.channelUnregistered(ctx);\n    }\n\n\n> 这里和连接建立之后的事件触发顺序正好相反，连接建立之后是先触发 ChannelRegistered 事件然后在触发 ChannelActive 事件。\n\n\n# 3.2 Outbound 类事件\n\nfinal class ChannelHandlerMask {\n\n    // outbound 事件的集合\n    static final int MASK_ONLY_OUTBOUND =  MASK_BIND | MASK_CONNECT | MASK_DISCONNECT |\n            MASK_CLOSE | MASK_DEREGISTER | MASK_READ | MASK_WRITE | MASK_FLUSH;\n\n    private static final int MASK_ALL_OUTBOUND = MASK_EXCEPTION_CAUGHT | MASK_ONLY_OUTBOUND;\n    \n    // outbound 事件掩码\n    static final int MASK_BIND = 1 << 9;\n    static final int MASK_CONNECT = 1 << 10;\n    static final int MASK_DISCONNECT = 1 << 11;\n    static final int MASK_CLOSE = 1 << 12;\n    static final int MASK_DEREGISTER = 1 << 13;\n    static final int MASK_READ = 1 << 14;\n    static final int MASK_WRITE = 1 << 15;\n    static final int MASK_FLUSH = 1 << 16;\n}\n\n\n和 Inbound 类事件一样，Outbound 类事件也有对应的掩码表示。下面我们来看下 Outbound类事件的触发时机：\n\n\n# 3.2.1 read 事件\n\n大家这里需要注意区分 read 事件和 ChannelRead 事件的不同。\n\nChannelRead 事件前边我们已经介绍了，当 NioServerSocketChannel 接收到新连接时，会触发 ChannelRead 事件在其 pipeline 上传播。\n\n当 NioSocketChannel 上有请求数据时，在 read loop 中读取请求数据时会触发 ChannelRead 事件在其 pipeline 上传播。\n\n而 read 事件则和 ChannelRead 事件完全不同，read 事件特指使 Channel 具备感知 IO 事件的能力。NioServerSocketChannel 对应的 OP_ACCEPT 事件的感知能力，NioSocketChannel 对应的是 OP_READ 事件的感知能力。\n\nread 事件的触发是在当 channel 需要向其对应的 reactor 注册读类型事件时（比如 OP_ACCEPT 事件 和 OP_READ 事件）才会触发。read 事件的响应就是将 channel 感兴趣的 IO 事件注册到对应的 reactor 上。\n\n比如 NioServerSocketChannel 感兴趣的是 OP_ACCEPT 事件， NioSocketChannel 感兴趣的是 OP_READ 事件。\n\n在前边介绍 ChannelActive 事件时我们提到，当 channel 处于 active 状态后会在 pipeline 中传播 ChannelActive 事件。而在 HeadContext 中的 ChannelActive 事件回调中会触发 Read 事件的传播。\n\nfinal class HeadContext extends AbstractChannelHandlerContext\n            implements ChannelOutboundHandler, ChannelInboundHandler {\n\n        @Override\n        public void channelActive(ChannelHandlerContext ctx) {\n            ctx.fireChannelActive();  \n            readIfIsAutoRead();\n        }\n\n        private void readIfIsAutoRead() {\n            if (channel.config().isAutoRead()) {\n                //如果是autoRead 则触发read事件传播\n                channel.read();\n            }\n        }\n\n        @Override\n        public void read(ChannelHandlerContext ctx) {\n            //触发注册OP_ACCEPT或者OP_READ事件\n            unsafe.beginRead();\n        }\n }\n\n\n而在 HeadContext 中的 read 事件回调中会调用 Channel 的底层操作类 unsafe 的 beginRead 方法，在该方法中会向 reactor 注册 channel 感兴趣的 IO 事件。对于 NioServerSocketChannel 来说这里注册的就是 OP_ACCEPT 事件，对于 NioSocketChannel 来说这里注册的则是 OP_READ 事件。\n\n    @Override\n    protected void doBeginRead() throws Exception {\n        // Channel.read() or ChannelHandlerContext.read() was called\n        final SelectionKey selectionKey = this.selectionKey;\n        if (!selectionKey.isValid()) {\n            return;\n        }\n\n        readPending = true;\n\n        final int interestOps = selectionKey.interestOps();\n\n        if ((interestOps & readInterestOp) == 0) {\n            //注册监听OP_ACCEPT或者OP_READ事件\n            selectionKey.interestOps(interestOps | readInterestOp);\n        }\n    }\n\n\n细心的同学可能注意到了 channel 对应的配置类中包含了一个 autoRead 属性，那么这个 autoRead 到底是干什么的呢？\n\n其实这是 netty 为大家提供的一种背压机制，用来防止 OOM ，想象一下当对端发送数据非常多并且发送速度非常快，而服务端处理速度非常慢，一时间消费不过来。而对端又在不停的大量发送数据，服务端的 reactor 线程不得不在 read loop 中不停的读取，并且为读取到的数据分配 ByteBuffer 。而服务端业务线程又处理不过来，这就导致了大量来不及处理的数据占用了大量的内存空间，从而导致 OOM 。\n\n面对这种情况，我们可以通过 channelHandlerContext.channel().config().setAutoRead(false) 将 autoRead 属性设置为 false 。随后 netty 就会将 channel 中感兴趣的读类型事件从 reactor 中注销，从此 reactor 不会再对相应事件进行监听。这样 channel 就不会在读取数据了。\n\n> 这里 NioServerSocketChannel 对应的是 OP_ACCEPT 事件， NioSocketChannel 对应的是 OP_READ 事件。\n\n        protected final void removeReadOp() {\n            SelectionKey key = selectionKey();\n            if (!key.isValid()) {\n                return;\n            }\n            int interestOps = key.interestOps();\n            if ((interestOps & readInterestOp) != 0) {        \n                key.interestOps(interestOps & ~readInterestOp);\n            }\n        }\n\n\n而当服务端的处理速度恢复正常，我们又可以通过 channelHandlerContext.channel().config().setAutoRead(true) 将 autoRead 属性设置为 true 。这样 netty 会在 pipeline 中触发 read 事件，最终在 HeadContext 中的 read 事件回调方法中通过调用 unsafe#beginRead 方法将 channel 感兴趣的读类型事件重新注册到对应的 reactor 中。\n\n    @Override\n    public ChannelConfig setAutoRead(boolean autoRead) {\n        boolean oldAutoRead = AUTOREAD_UPDATER.getAndSet(this, autoRead ? 1 : 0) == 1;\n        if (autoRead && !oldAutoRead) {\n            //autoRead从false变为true\n            channel.read();\n        } else if (!autoRead && oldAutoRead) {\n            //autoRead从true变为false\n            autoReadCleared();\n        }\n        return this;\n    }\n\n\n> read 事件可以理解为使 channel 拥有读的能力，当有了读的能力后， channelRead 就可以读取具体的数据了。\n\n\n# 3.2.2 write 和 flush 事件\n\nwrite 事件和 flush 事件我们在《一文搞懂Netty发送数据全流程》一文中已经非常详尽的介绍过了，这里笔者在带大家简单回顾一下。\n\nwrite 事件和 flush 事件均由用户在处理完业务请求得到业务结果后在业务线程中主动触发。\n\n用户既可以通过 ChannelHandlerContext 触发也可以通过 Channel 来触发。\n\n不同之处在于如果通过 ChannelHandlerContext 触发，那么 write 事件或者 flush 事件就会在 pipeline 中从当前 ChannelHandler 开始一直向前传播直到 HeadContext 。\n\n    @Override\n    public void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n       ctx.write(msg);\n    }\n\n    @Override\n    public void channelReadComplete(ChannelHandlerContext ctx) {\n        ctx.flush();\n    }\n\n\n如果通过 Channel 触发，那么 write 事件和 flush 事件就会从 pipeline 的尾部节点 TailContext 开始一直向前传播直到 HeadContext 。\n\n    @Override\n    public void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n       ctx.channel().write(msg);\n    }\n\n    @Override\n    public void channelReadComplete(ChannelHandlerContext ctx) {\n        ctx.channel().flush();\n    }\n\n\n当然还有一个 writeAndFlush 方法，也会分为 ChannelHandlerContext 触发和 Channel 的触发。触发 writeAndFlush 后，write 事件首先会在 pipeline 中传播，最后 flush 事件在 pipeline 中传播。\n\nnetty 对 write 事件的处理最终会将发送数据写入 Channel 对应的写缓冲队列 ChannelOutboundBuffer 中。此时数据并没有发送出去而是在写缓冲队列中缓存，这也是 netty 实现异步写的核心设计。\n\n最终通过 flush 操作从 Channel 中的写缓冲队列 ChannelOutboundBuffer 中获取到待发送数据，并写入到 Socket 的发送缓冲区中。\n\n\n# 3.2.3 close 事件\n\n当用户在 ChannelHandler 中调用如下方法对 Channel 进行关闭时，会触发 Close 事件在 pipeline 中从后向前传播。\n\n//close事件从当前ChannelHandlerContext开始在pipeline中向前传播\nctx.close();\n//close事件从pipeline的尾结点tailContext开始向前传播\nctx.channel().close();\n\n\n我们可以在Outbound类型的ChannelHandler中响应close事件。\n\npublic class ExampleChannelHandler extends ChannelOutboundHandlerAdapter {\n\n    @Override\n    public void close(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception {\n        \n        .....客户端channel关闭之前的处理回调.....\n        \n        //继续向前传播close事件\n        super.close(ctx, promise);\n    }\n}\n\n\n最终 close 事件会在 pipeline 中一直向前传播直到头结点 HeadConnect 中，并在 HeadContext 中完成连接关闭的操作，当连接完成关闭之后，会在 pipeline中先后触发 ChannelInactive 事件和 ChannelUnregistered 事件。\n\n\n# 3.2.4 deRegister 事件\n\n用户可调用如下代码将当前 Channel 从 Reactor 中注销掉。\n\n//deregister事件从当前ChannelHandlerContext开始在pipeline中向前传播\nctx.deregister();\n//deregister事件从pipeline的尾结点tailContext开始向前传播\nctx.channel().deregister();\n\n\n我们可以在 Outbound 类型的 ChannelHandler 中响应 deregister 事件。\n\npublic class ExampleChannelHandler extends ChannelOutboundHandlerAdapter {\n\n    @Override\n    public void deregister(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception {\n\n\n        .....客户端channel取消注册之前的处理回调.....\n\n        //继续向前传播connect事件\n        super.deregister(ctx, promise);\n    }\n}\n\n\n最终 deRegister 事件会传播至 pipeline 中的头结点 HeadContext 中，并在 HeadContext 中完成底层 channel 取消注册的操作。当 Channel 从 Reactor 上注销之后，从此 Reactor 将不会在监听 Channel 上的 IO 事件，并触发 ChannelUnregistered 事件在 pipeline 中传播。\n\n\n# 3.2.5 connect 事件\n\n在 Netty 的客户端中我们可以利用 NioSocketChannel 的 connect 方法触发 connect 事件在 pipeline 中传播。\n\n//connect事件从当前ChannelHandlerContext开始在pipeline中向前传播\nctx.connect(remoteAddress);\n//connect事件从pipeline的尾结点tailContext开始向前传播\nctx.channel().connect(remoteAddress);\n\n\n我们可以在 Outbound 类型的 ChannelHandler 中响应 connect 事件。\n\npublic class ExampleChannelHandler extends ChannelOutboundHandlerAdapter {\n\n    @Override\n    public void connect(ChannelHandlerContext ctx, SocketAddress remoteAddress, SocketAddress localAddress,\n                        ChannelPromise promise) throws Exception {\n                 \n        \n        .....客户端channel连接成功之前的处理回调.....\n        \n        //继续向前传播connect事件\n        super.connect(ctx, remoteAddress, localAddress, promise);\n    }\n}\n\n\n最终 connect 事件会在 pipeline 中的头结点 headContext 中触发底层的连接建立请求。当客户端成功连接到服务端之后，会在客户端 NioSocketChannel 的 pipeline 中传播 channelActive 事件。\n\n\n# 3.2.6 disConnect 事件\n\n在 Netty 的客户端中我们也可以调用 NioSocketChannel 的 disconnect 方法在 pipeline 中触发 disconnect 事件，这会导致 NioSocketChannel 的关闭。\n\n//disconnect事件从当前ChannelHandlerContext开始在pipeline中向前传播\nctx.disconnect();\n//disconnect事件从pipeline的尾结点tailContext开始向前传播\nctx.channel().disconnect();\n\n\n我们可以在 Outbound 类型的 ChannelHandler 中响应 disconnect 事件。\n\npublic class ExampleChannelHandler extends ChannelOutboundHandlerAdapter {\n\n\n    @Override\n    public void disconnect(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception {\n        \n        .....客户端channel即将关闭前的处理回调.....\n        \n        //继续向前传播disconnect事件\n        super.disconnect(ctx, promise);\n    }\n}\n\n\n最终 disconnect 事件会传播到 HeadContext 中，并在 HeadContext 中完成底层的断开连接操作，当客户端断开连接成功关闭之后，会在 pipeline 中先后触发 ChannelInactive 事件和 ChannelUnregistered 事件。\n\n\n# 4. 向pipeline添加channelHandler\n\n在我们详细介绍了全部的 inbound 类事件和 outbound 类事件的掩码表示以及事件的触发和传播路径后，相信大家现在可以通过 ChannelInboundHandler 和 ChannelOutboundHandler 来根据具体的业务场景选择合适的 ChannelHandler 类型以及监听合适的事件来完成业务需求了。\n\n本小节就该介绍一下自定义的 ChannelHandler 是如何添加到 pipeline 中的，netty 在这个过程中帮我们作了哪些工作?\n\n           final EchoServerHandler serverHandler = new EchoServerHandler();\n\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)\n             .channel(NioServerSocketChannel.class)\n\n             .............\n\n             .childHandler(new ChannelInitializer<SocketChannel>() {\n                 @Override\n                 public void initChannel(SocketChannel ch) throws Exception {\n                     ChannelPipeline p = ch.pipeline();          \n                     p.addLast(serverHandler);\n\n                     ......可添加多个channelHandler......\n                 }\n             });\n\n\n以上是笔者简化的一个 netty 服务端配置 ServerBootstrap 启动类的一段示例代码。我们可以看到再向 channel 对应的 pipeline 中添加 ChannelHandler 是通过 ChannelPipeline#addLast 方法将指定 ChannelHandler 添加到 pipeline 的末尾处。\n\npublic interface ChannelPipeline\n        extends ChannelInboundInvoker, ChannelOutboundInvoker, Iterable<Entry<String, ChannelHandler>> {\n\n    //向pipeline的末尾处批量添加多个channelHandler\n    ChannelPipeline addLast(ChannelHandler... handlers);\n\n    //指定channelHandler的executor，由指定的executor执行channelHandler中的回调方法\n    ChannelPipeline addLast(EventExecutorGroup group, ChannelHandler... handlers);\n\n     //为channelHandler指定名称\n    ChannelPipeline addLast(String name, ChannelHandler handler);\n\n    //为channelHandler指定executor和name\n    ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler);\n}\npublic class DefaultChannelPipeline implements ChannelPipeline {\n\n    @Override\n    public final ChannelPipeline addLast(ChannelHandler... handlers) {\n        return addLast(null, handlers);\n    }\n\n    @Override\n    public final ChannelPipeline addLast(EventExecutorGroup executor, ChannelHandler... handlers) {\n        ObjectUtil.checkNotNull(handlers, \"handlers\");\n\n        for (ChannelHandler h: handlers) {\n            if (h == null) {\n                break;\n            }\n            addLast(executor, null, h);\n        }\n\n        return this;\n    }\n\n    @Override\n    public final ChannelPipeline addLast(String name, ChannelHandler handler) {\n        return addLast(null, name, handler);\n    }\n}\n\n\n最终 addLast 的这些重载方法都会调用到 DefaultChannelPipeline#addLast(EventExecutorGroup, String, ChannelHandler) 这个方法从而完成 ChannelHandler 的添加。\n\n    @Override\n    public final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) {\n        final AbstractChannelHandlerContext newCtx;\n        synchronized (this) {\n            //检查同一个channelHandler实例是否允许被重复添加\n            checkMultiplicity(handler);\n\n            //创建channelHandlerContext包裹channelHandler并封装执行传播事件相关的上下文信息\n            newCtx = newContext(group, filterName(name, handler), handler);\n\n            //将channelHandelrContext插入到pipeline中的末尾处。双向链表操作\n            //此时channelHandler的状态还是ADD_PENDING，只有当channelHandler的handlerAdded方法被回调后，状态才会为ADD_COMPLETE\n            addLast0(newCtx);\n\n            //如果当前channel还没有向reactor注册，则将handlerAdded方法的回调添加进pipeline的任务队列中\n            if (!registered) {\n                //这里主要是用来处理ChannelInitializer的情况\n                //设置channelHandler的状态为ADD_PENDING 即等待添加,当状态变为ADD_COMPLETE时 channelHandler中的handlerAdded会被回调\n                newCtx.setAddPending();\n                //向pipeline中添加PendingHandlerAddedTask任务，在任务中回调handlerAdded\n                //当channel注册到reactor后，pipeline中的pendingHandlerCallbackHead任务链表会被挨个执行\n                callHandlerCallbackLater(newCtx, true);\n                return this;\n            }\n\n            //如果当前channel已经向reactor注册成功，那么就直接回调channelHandler中的handlerAddded方法\n            EventExecutor executor = newCtx.executor();\n            if (!executor.inEventLoop()) {\n                //这里需要确保channelHandler中handlerAdded方法的回调是在channel指定的executor中\n                callHandlerAddedInEventLoop(newCtx, executor);\n                return this;\n            }\n        }\n        //回调channelHandler中的handlerAddded方法\n        callHandlerAdded0(newCtx);\n        return this;\n    }\n\n\n这个方法的逻辑还是比较复杂的，涉及到很多细节，为了清晰地为大家讲述，笔者这里还是采用总分总的结构，先描述该方法的总体逻辑，然后在针对核心细节要点展开细节分析。\n\n因为向 pipeline 中添加 channelHandler 的操作可能会在多个线程中进行，所以为了确保添加操作的线程安全性，这里采用一个 synchronized 语句块将整个添加逻辑包裹起来。\n\n 1. 通过 checkMultiplicity 检查被添加的 ChannelHandler 是否是共享的（标注 @Sharable 注解），如果不是共享的那么则不会允许该 ChannelHandler 的同一实例被添加进多个 pipeline 中。如果是共享的，则允许该 ChannelHandler 的同一个实例被多次添加进多个 pipeline 中。\n\n    private static void checkMultiplicity(ChannelHandler handler) {\n        if (handler instanceof ChannelHandlerAdapter) {\n            ChannelHandlerAdapter h = (ChannelHandlerAdapter) handler;\n            //只有标注@Sharable注解的channelHandler，才被允许同一个实例被添加进多个pipeline中\n            //注意：标注@Sharable之后，一个channelHandler的实例可以被添加到多个channel对应的pipeline中\n            //可能被多线程执行，需要确保线程安全\n            if (!h.isSharable() && h.added) {\n                throw new ChannelPipelineException(\n                        h.getClass().getName() +\n                        \" is not a @Sharable handler, so can't be added or removed multiple times.\");\n            }\n            h.added = true;\n        }\n    }\n\n\n> 这里大家需要注意的是，如果一个 ChannelHandler 被标注了 @Sharable 注解,这就意味着它的一个实例可以被多次添加进多个 pipeline 中（每个 channel 对应一个 pipeline 实例），而这多个不同的 pipeline 可能会被不同的 reactor 线程执行，所以在使用共享 ChannelHandler 的时候需要确保其线程安全性。\n\n比如下面的实例代码：\n\n@Sharable\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n            .............需要确保线程安全.......\n}\n  final EchoServerHandler serverHandler = new EchoServerHandler();\n\n  ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)\n               ..................\n            .childHandler(new ChannelInitializer<SocketChannel>() {\n                 @Override\n                 public void initChannel(SocketChannel ch) throws Exception {\n                     ChannelPipeline p = ch.pipeline();\n                     p.addLast(serverHandler);\n                 }\n             });\n\n\nEchoServerHandler 为我们自定义的 ChannelHandler ，它被 @Sharable 注解标注，全局只有一个实例，被添加进多个 Channel 的 pipeline 中。从而会被多个 reactor 线程执行到。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)共享channelHandler.png\n\n 1. 为 ChannelHandler 创建其 ChannelHandlerContext ，用于封装 ChannelHandler 的名称，状态信息，执行上下文信息，以及用于感知 ChannelHandler 在 pipeline 中的位置信息。newContext 方法涉及的细节较多，后面我们单独介绍。\n 2. 通过 addLast0 将新创建出来的 ChannelHandlerContext 插入到 pipeline 中末尾处。方法的逻辑很简单其实就是一个普通的双向链表插入操作。\n\n    private void addLast0(AbstractChannelHandlerContext newCtx) {\n        AbstractChannelHandlerContext prev = tail.prev;\n        newCtx.prev = prev;\n        newCtx.next = tail;\n        prev.next = newCtx;\n        tail.prev = newCtx;\n    }\n\n\n**但是这里大家需要注意的点是：**虽然此时 ChannelHandlerContext 被物理的插入到了 pipeline 中，但是此时 channelHandler 的状态依然为 INIT 状态，从逻辑上来说并未算是真正的插入到 pipeline 中，需要等到 ChannelHandler 的 handlerAdded 方法被回调时，状态才变为 ADD_COMPLETE ，而只有 ADD_COMPLETE 状态的 ChannelHandler 才能响应 pipeline 中传播的事件。\n\nchannelhandelr的状态.png\n\n在上篇文章《一文搞懂Netty发送数据全流程》中的《3.1.5 触发nextChannelHandler的write方法回调》小节中我们也提过，在每次 write 事件或者 flush 事件传播的时候，都需要通过 invokeHandler 方法来判断 channelHandler 的状态是否为 ADD_COMPLETE ，否则当前 channelHandler 则不能响应正在 pipeline 中传播的事件。必须要等到对应的 handlerAdded 方法被回调才可以，因为 handlerAdded 方法中可能包含一些 ChannelHandler 初始化的重要逻辑。\n\n    private boolean invokeHandler() {\n        // 这里是一个优化点，netty 用一个局部变量保存 handlerState\n        // 目的是减少 volatile 变量 handlerState 的读取次数\n        int handlerState = this.handlerState;\n        return handlerState == ADD_COMPLETE || (!ordered && handlerState == ADD_PENDING);\n    }\n\n    void invokeWrite(Object msg, ChannelPromise promise) {\n        if (invokeHandler()) {\n            invokeWrite0(msg, promise);\n        } else {\n            // 当前channelHandler虽然添加到pipeline中，但是并没有调用handlerAdded\n            // 所以不能调用当前channelHandler中的回调方法，只能继续向前传递write事件\n            write(msg, promise);\n        }\n    }\n\n    private void invokeFlush() {\n        if (invokeHandler()) {\n            invokeFlush0();\n        } else {\n            //如果该ChannelHandler虽然加入到pipeline中但handlerAdded方法并未被回调，则继续向前传递flush事件\n            flush();\n        }\n    }\n\n\n> 事实上不仅仅是 write 事件和 flush 事件在传播的时候需要判断 ChannelHandler 的状态，所有的 inbound 类事件和 outbound 类事件在传播的时候都需要通过 invokeHandler 方法来判断当前 ChannelHandler 的状态是否为 ADD_COMPLETE ，需要确保在 ChannelHandler 响应事件之前，它的 handlerAdded 方法被回调。\n\n 1. 如果向 pipeline 中添加 ChannelHandler 的时候， channel 还没来得及注册到 reactor中，那么需要将当前 ChannelHandler 的状态先设置为 ADD_PENDING ，并将回调该 ChannelHandler 的 handlerAdded 方法封装成 PendingHandlerAddedTask 任务添加进 pipeline 中的任务列表中，等到 channel 向 reactor 注册之后，reactor 线程会挨个执行 pipeline 中任务列表中的任务。\n\n> 这段逻辑主要用来处理 ChannelInitializer 的添加场景，因为目前只有 ChannelInitializer 这个特殊的 channelHandler 会在 channel 没有注册之前被添加进 pipeline 中\n\n         if (!registered) {\n                newCtx.setAddPending();\n                callHandlerCallbackLater(newCtx, true);\n                return this;\n         }\n\n\n向 pipeline 的任务列表 pendingHandlerCallbackHead 中添加 PendingHandlerAddedTask 任务：\n\npublic class DefaultChannelPipeline implements ChannelPipeline {\n\n    // pipeline中的任务列表\n    private PendingHandlerCallback pendingHandlerCallbackHead;\n\n    // 向任务列表尾部添加PendingHandlerAddedTask\n    private void callHandlerCallbackLater(AbstractChannelHandlerContext ctx, boolean added) {\n        assert !registered;\n\n        PendingHandlerCallback task = added ? new PendingHandlerAddedTask(ctx) : new PendingHandlerRemovedTask(ctx);\n        PendingHandlerCallback pending = pendingHandlerCallbackHead;\n        if (pending == null) {\n            pendingHandlerCallbackHead = task;\n        } else {\n            // Find the tail of the linked-list.\n            while (pending.next != null) {\n                pending = pending.next;\n            }\n            pending.next = task;\n        }\n    }\n}\n\n\nPendingHandlerAddedTask 任务负责回调 ChannelHandler 中的 handlerAdded 方法。\n\nprivate final class PendingHandlerAddedTask extends PendingHandlerCallback {\n        ...............\n\n        @Override\n        public void run() {\n            callHandlerAdded0(ctx);\n        }\n\n       ...............\n}\n    private void callHandlerAdded0(final AbstractChannelHandlerContext ctx) {\n       try {\n            ctx.callHandlerAdded();\n        } catch (Throwable t) {\n           ...............\n        }\n    }\n\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)pipeline任务列表.png\n\n 1. 除了 ChannelInitializer 这个特殊的 ChannelHandler 的添加是在 channel 向 reactor 注册之前外，剩下的这些用户自定义的 ChannelHandler 的添加，均是在 channel 向 reactor 注册之后被添加进 pipeline 的。这种场景下的处理就会变得比较简单，在 ChannelHandler 被插入到 pipeline 中之后，就会立即回调该 ChannelHandler 的 handlerAdded 方法。但是需要确保 handlerAdded 方法的回调在 channel 指定的 executor 中进行。\n\n            EventExecutor executor = newCtx.executor();\n            if (!executor.inEventLoop()) {\n                callHandlerAddedInEventLoop(newCtx, executor);\n                return this;\n            }  \n      \n            callHandlerAdded0(newCtx);\n\n\n如果当前执行线程并不是 ChannelHandler 指定的 executor ( !executor.inEventLoop() ),那么就需要确保 handlerAdded 方法的回调在 channel 指定的 executor 中进行。\n\n    private void callHandlerAddedInEventLoop(final AbstractChannelHandlerContext newCtx, EventExecutor executor) {\n        newCtx.setAddPending();\n        executor.execute(new Runnable() {\n            @Override\n            public void run() {\n                callHandlerAdded0(newCtx);\n            }\n        });\n    }\n\n\n这里需要注意的是需要在回调 handlerAdded 方法之前将 ChannelHandler 的状态提前设置为 ADD_COMPLETE 。 因为用户可能在 ChannelHandler 中的 handerAdded 回调中触发一些事件，而如果此时 ChannelHandler 的状态不是 ADD_COMPLETE 的话，就会停止对事件的响应，从而错过事件的处理。\n\n> 这种属于一种用户极端的使用情况。\n\n    final void callHandlerAdded() throws Exception {\n        if (setAddComplete()) {\n            handler().handlerAdded(this);\n        }\n    }\n\n\n\n# 5. ChanneHandlerContext 的创建\n\n在介绍完 ChannelHandler 向 pipeline 添加的整个逻辑过程后，本小节我们来看下如何为 ChannelHandler 创建对应的 ChannelHandlerContext ，以及 ChannelHandlerContext 中具体包含了哪些上下文信息。\n\npublic class DefaultChannelPipeline implements ChannelPipeline {\n    @Override\n    public final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) {\n        final AbstractChannelHandlerContext newCtx;\n        synchronized (this) {\n             \n            ................\n\n            //创建channelHandlerContext包裹channelHandler并封装执行传播相关的上下文信息\n            newCtx = newContext(group, filterName(name, handler), handler);\n\n             ................\n        }\n\n    }\n\n    private AbstractChannelHandlerContext newContext(EventExecutorGroup group, String name, ChannelHandler handler) {\n        return new DefaultChannelHandlerContext(this, childExecutor(group), name, handler);\n    }\n\n}\n\n\n在创建 ChannelHandlerContext 之前，需要做两个重要的前置操作：\n\n * 通过 filterName 方法为 ChannelHandlerContext 过滤出在 pipeline 中唯一的名称。\n * 如果用户为 ChannelHandler 指定了特殊的 EventExecutorGroup ，这里就需要通过 childExecutor 方法从指定的 EventExecutorGroup 中选出一个 EventExecutor 与 ChannelHandler 绑定。\n\n\n# 5.1 filterName\n\n    private String filterName(String name, ChannelHandler handler) {\n        if (name == null) {\n            // 如果没有指定name,则会为handler默认生成一个name，该方法可确保默认生成的name在pipeline中不会重复\n            return generateName(handler);\n        }\n\n        // 如果指定了name，需要确保name在pipeline中是唯一的\n        checkDuplicateName(name);\n        return name;\n    }\n\n\n如果用户再向 pipeline 添加 ChannelHandler 的时候，为其指定了具体的名称，那么这里需要确保用户指定的名称在 pipeline 中是唯一的。\n\n    private void checkDuplicateName(String name) {\n        if (context0(name) != null) {\n            throw new IllegalArgumentException(\"Duplicate handler name: \" + name);\n        }\n    }\n\n    /**\n     * 通过指定名称在pipeline中查找对应的channelHandler 没有返回null\n     * */\n    private AbstractChannelHandlerContext context0(String name) {\n        AbstractChannelHandlerContext context = head.next;\n        while (context != tail) {\n            if (context.name().equals(name)) {\n                return context;\n            }\n            context = context.next;\n        }\n        return null;\n    }\n\n\n如果用户没有为 ChannelHandler 指定名称，那么就需要为 ChannelHandler 在 pipeline 中默认生成一个唯一的名称。\n\n    // pipeline中channelHandler对应的name缓存\n    private static final FastThreadLocal<Map<Class<?>, String>> nameCaches =\n            new FastThreadLocal<Map<Class<?>, String>>() {\n        @Override\n        protected Map<Class<?>, String> initialValue() {\n            return new WeakHashMap<Class<?>, String>();\n        }\n    };\n\n    private String generateName(ChannelHandler handler) {\n        // 获取pipeline中channelHandler对应的name缓存\n        Map<Class<?>, String> cache = nameCaches.get();\n        Class<?> handlerType = handler.getClass();\n        String name = cache.get(handlerType);\n        if (name == null) {\n            // 当前handler还没对应的name缓存，则默认生成：simpleClassName + #0\n            name = generateName0(handlerType);\n            cache.put(handlerType, name);\n        }\n\n        if (context0(name) != null) {\n            // 不断重试名称后缀#n + 1 直到没有重复\n            String baseName = name.substring(0, name.length() - 1); \n            for (int i = 1;; i ++) {\n                String newName = baseName + i;\n                if (context0(newName) == null) {\n                    name = newName;\n                    break;\n                }\n            }\n        }\n        return name;\n    }\n\n    private static String generateName0(Class<?> handlerType) {\n        return StringUtil.simpleClassName(handlerType) + \"#0\";\n    }\n\n\npipeline 中使用了一个 FastThreadLocal 类型的 nameCaches 来缓存各种类型 ChannelHandler 的基础名称。后面会根据这个基础名称不断的重试生成一个没有冲突的正式名称。缓存 nameCaches 中的 key 表示特定的 ChannelHandler 类型，value 表示该特定类型的 ChannelHandler 的基础名称 simpleClassName + #0。\n\n自动为 ChannelHandler 生成默认名称的逻辑是：\n\n * 首先从缓存中 nameCaches 获取当前添加的 ChannelHandler 的基础名称 simpleClassName + #0。\n * 如果该基础名称 simpleClassName + #0 在 pipeline 中是唯一的，那么就将基础名称作为 ChannelHandler 的名称。\n * 如果缓存的基础名称在 pipeline 中不是唯一的，则不断的增加名称后缀 simpleClassName#1 ,simpleClassName#2 ...... simpleClassName#n 直到产生一个没有重复的名称。\n\n> 虽然用户不大可能将同一类型的 channelHandler 重复添加到 pipeline 中，但是 netty 为了防止这种反复添加同一类型 ChannelHandler 的行为导致的名称冲突，从而利用 nameCaches 来缓存同一类型 ChannelHandler 的基础名称 simpleClassName + #0，然后通过不断的重试递增名称后缀，来生成一个在pipeline中唯一的名称。\n\n\n# 5.2 childExecutor\n\n通过前边的介绍我们了解到，当我们向 pipeline 添加 ChannelHandler 的时候，netty 允许我们为 ChannelHandler 指定特定的 executor 去执行 ChannelHandler 中的各种事件回调方法。\n\n通常我们会为 ChannelHandler 指定一个EventExecutorGroup，在创建ChannelHandlerContext 的时候，会通过 childExecutor 方法从 EventExecutorGroup 中选取一个 EventExecutor 来与该 ChannelHandler 绑定。\n\n> EventExecutorGroup 是 netty 自定义的一个线程池模型，其中包含多个 EventExecutor ，而 EventExecutor 在 netty 中是一个线程的执行模型。相关的具体实现和用法笔者已经在《Reactor在Netty中的实现(创建篇)》一文中给出了详尽的介绍，忘记的同学可以在回顾下。\n\n在介绍 executor 的绑定逻辑之前，这里笔者需要先为大家介绍一个相关的重要参数：SINGLE_EVENTEXECUTOR_PER_GROUP ，默认为 true 。\n\nServerBootstrap b = new ServerBootstrap();\nb.group(bossGroup, workerGroup)\n.channel(NioServerSocketChannel.class)\n  .........\n.childOption(ChannelOption.SINGLE_EVENTEXECUTOR_PER_GROUP,true）\n\n\n我们知道在 netty 中，每一个 channel 都会对应一个独立的 pipeline ，如果我们开启了 SINGLE_EVENTEXECUTOR_PER_GROUP 参数，表示在一个 channel 对应的 pipeline 中，如果我们为多个 ChannelHandler 指定了同一个 EventExecutorGroup ，那么这多个 channelHandler 只能绑定到 EventExecutorGroup 中的同一个 EventExecutor 上。\n\n什么意思呢？？比如我们有下面一段初始化pipeline的代码：\n\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)\n                 ........................\n             .childHandler(new ChannelInitializer<SocketChannel>() {\n                 @Override\n                 public void initChannel(SocketChannel ch) throws Exception {\n                     ChannelPipeline pipeline = ch.pipeline();\n                     pipeline.addLast(eventExecutorGroup,channelHandler1)\n                     pipeline.addLast(eventExecutorGroup,channelHandler2)\n                     pipeline.addLast(eventExecutorGroup,channelHandler3)\n                 }\n             });\n\n\neventExecutorGroup 中包含 EventExecutor1，EventExecutor2 ， EventExecutor3 三个执行线程。\n\n假设此时第一个连接进来，在创建 channel1 后初始化 pipeline1 的时候，如果在开启 SINGLE_EVENTEXECUTOR_PER_GROUP 参数的情况下，那么在 channel1 对应的 pipeline1 中 channelHandler1，channelHandler2 ， channelHandler3 绑定的 EventExecutor 均为 EventExecutorGroup 中的 EventExecutor1 。\n\n第二个连接 channel2 对应的 pipeline2 中 channelHandler1 ， channelHandler2 ，channelHandler3 绑定的 EventExecutor 均为 EventExecutorGroup 中的 EventExecutor2 。\n\n第三个连接 channel3 对应的 pipeline3 中 channelHandler1 ， channelHandler2 ，channelHandler3 绑定的 EventExecutor 均为 EventExecutorGroup 中的 EventExecutor3 。\n\n以此类推........\n\nSINGLE_EVENTEXECUTOR_PER_GROUP绑定.png\n\n如果在关闭 SINGLE_EVENTEXECUTOR_PER_GROUP 参数的情况下, channel1 对应的 pipeline1 中 channelHandler1 会绑定到 EventExecutorGroup 中的 EventExecutor1 ，channelHandler2 会绑定到 EventExecutor2 ，channelHandler3 会绑定到 EventExecutor3 。\n\n同理其他 channel 对应的 pipeline 中的 channelHandler 绑定逻辑同 channel1 。它们均会绑定到 EventExecutorGroup 中的不同 EventExecutor 中。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)SINGLE_EVENTEXECUTOR_PER_GROUP关闭.png\n\n当我们了解了 SINGLE_EVENTEXECUTOR_PER_GROUP参数的作用之后，再来看下面这段绑定逻辑就很容易理解了。\n\n     // 在每个pipeline中都会保存EventExecutorGroup中绑定的线程\n    private Map<EventExecutorGroup, EventExecutor> childExecutors;\n\n    private EventExecutor childExecutor(EventExecutorGroup group) {\n        if (group == null) {\n            return null;\n        }\n\n        Boolean pinEventExecutor = channel.config().getOption(ChannelOption.SINGLE_EVENTEXECUTOR_PER_GROUP);\n        if (pinEventExecutor != null && !pinEventExecutor) {\n            //如果没有开启SINGLE_EVENTEXECUTOR_PER_GROUP，则按顺序从指定的EventExecutorGroup中为channelHandler分配EventExecutor\n            return group.next();\n        }\n\n        //获取pipeline绑定到EventExecutorGroup的线程（在一个pipeline中会为每个指定的EventExecutorGroup绑定一个固定的线程）\n        Map<EventExecutorGroup, EventExecutor> childExecutors = this.childExecutors;\n        if (childExecutors == null) {\n            childExecutors = this.childExecutors = new IdentityHashMap<EventExecutorGroup, EventExecutor>(4);\n        }\n\n        //获取该pipeline绑定在指定EventExecutorGroup中的线程\n        EventExecutor childExecutor = childExecutors.get(group);\n        if (childExecutor == null) {\n            childExecutor = group.next();\n            childExecutors.put(group, childExecutor);\n        }\n        return childExecutor;\n    }\n\n\n如果我们并未特殊指定 ChannelHandler 的 executor ，那么默认会是对应 channel 绑定的 reactor 线程负责执行该 ChannelHandler 。\n\n如果我们未开启 SINGLE_EVENTEXECUTOR_PER_GROUP，netty 就会从我们指定的 EventExecutorGroup 中按照 round-robin 的方式为 ChannelHandler 绑定其中一个 eventExecutor 。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)SINGLE_EVENTEXECUTOR_PER_GROUP关闭.png\n\n如果我们开启了 SINGLE_EVENTEXECUTOR_PER_GROUP，相同的 EventExecutorGroup 在同一个 pipeline 实例中的绑定关系是固定的。在 pipeline 中如果多个 channelHandler 指定了同一个 EventExecutorGroup ，那么这些 channelHandler 的 executor 均会绑定到一个固定的 eventExecutor 上。\n\nSINGLE_EVENTEXECUTOR_PER_GROUP绑定.png\n\n这种固定的绑定关系缓存于每个 pipeline 中的 Map<EventExecutorGroup, EventExecutor> childExecutors 字段中，key 是用户为 channelHandler 指定的 EventExecutorGroup ，value 为该 EventExecutorGroup 在 pipeline 实例中的绑定 eventExecutor 。\n\n接下来就是从 childExecutors 中获取指定 EventExecutorGroup 在该 pipeline 实例中的绑定 eventExecutor，如果绑定关系还未建立，则通过 round-robin 的方式从 EventExecutorGroup 中选取一个 eventExecutor 进行绑定，并在 childExecutor 中缓存绑定关系。\n\n如果绑定关系已经建立，则直接为 ChannelHandler 指定绑定好的 eventExecutor。\n\n\n# 5.3 ChanneHandlerContext\n\n在介绍完创建 ChannelHandlerContext 的两个前置操作后，我们回头来看下 ChannelHandlerContext 中包含了哪些具体的上下文信息。\n\nfinal class DefaultChannelHandlerContext extends AbstractChannelHandlerContext {\n\n    // ChannelHandlerContext包裹的channelHandler\n    private final ChannelHandler handler;\n\n    DefaultChannelHandlerContext(\n            DefaultChannelPipeline pipeline, EventExecutor executor, String name, ChannelHandler handler) {\n        super(pipeline, executor, name, handler.getClass());\n        //包裹的channelHandler\n        this.handler = handler;\n    }\n\n    @Override\n    public ChannelHandler handler() {\n        return handler;\n    }\n}\nabstract class AbstractChannelHandlerContext implements ChannelHandlerContext, ResourceLeakHint {\n    //对应channelHandler的名称\n    private final String name;\n\n    //ChannelHandlerContext中持有pipeline的引用\n    private final DefaultChannelPipeline pipeline;\n\n    // channelHandler对应的executor 默认为reactor\n    final EventExecutor executor;\n\n    //channelHandlerContext中保存channelHandler的执行条件掩码（是什么类型的ChannelHandler,对什么事件感兴趣）\n    private final int executionMask;\n\n    //false表示 当channelHandler的状态为ADD_PENDING的时候，也可以响应pipeline中的事件\n    //true表示只有在channelHandler的状态为ADD_COMPLETE的时候才能响应pipeline中的事件\n    private final boolean ordered;\n\n    //channelHandelr的状态，初始化为INIT\n    private volatile int handlerState = INIT;\n\n    AbstractChannelHandlerContext(DefaultChannelPipeline pipeline, EventExecutor executor,\n                                  String name, Class<? extends ChannelHandler> handlerClass) {\n        this.name = ObjectUtil.checkNotNull(name, \"name\");\n        this.pipeline = pipeline;\n        this.executor = executor;\n        //channelHandlerContext中保存channelHandler的执行条件掩码（是什么类型的ChannelHandler,对什么事件感兴趣）\n        this.executionMask = mask(handlerClass);\n        ordered = executor == null || executor instanceof OrderedEventExecutor;\n    }\n\n}\n\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)ChannelHandlerContext.png\n\n这里笔者重点介绍 orderd 属性和 executionMask 属性，其他的属性大家很容易理解。\n\n  ordered = executor == null || executor instanceof OrderedEventExecutor;\n\n\n当我们不指定 channelHandler 的 executor 时或者指定的 executor 类型为 OrderedEventExecutor 时，ordered = true。\n\n那么这个 ordered 属性对于 ChannelHandler 响应 pipeline 中的事件有什么影响呢？\n\n我们之前介绍过在 ChannelHandler 响应 pipeline 中的事件之前都会调用 invokeHandler() 方法来判断是否回调 ChannelHandler 的事件回调方法还是跳过。\n\n   private boolean invokeHandler() {\n        int handlerState = this.handlerState;\n        return handlerState == ADD_COMPLETE || (!ordered && handlerState == ADD_PENDING);\n    }\n\n\n * 当 ordered == false 时，channelHandler 的状态为 ADD_PENDING 的时候，也可以响应 pipeline 中的事件。\n * 当 ordered == true 时，只有在 channelHandler 的状态为 ADD_COMPLETE 的时候才能响应 pipeline 中的事件\n\n另一个重要的属性 executionMask 保存的是当前 ChannelHandler 的一些执行条件信息掩码，比如：\n\n * 当前 ChannelHandler 是什么类型的（ ChannelInboundHandler or ChannelOutboundHandler ?）。\n * 当前 ChannelHandler 对哪些事件感兴趣（覆盖了哪些事件回调方法?）\n\n    private static final FastThreadLocal<Map<Class<? extends ChannelHandler>, Integer>> MASKS =\n            new FastThreadLocal<Map<Class<? extends ChannelHandler>, Integer>>() {\n                @Override\n                protected Map<Class<? extends ChannelHandler>, Integer> initialValue() {\n                    return new WeakHashMap<Class<? extends ChannelHandler>, Integer>(32);\n                }\n            };\n\n    static int mask(Class<? extends ChannelHandler> clazz) {\n        // 因为每建立一个channel就会初始化一个pipeline，这里需要将ChannelHandler对应的mask缓存\n        Map<Class<? extends ChannelHandler>, Integer> cache = MASKS.get();\n        Integer mask = cache.get(clazz);\n        if (mask == null) {\n            // 计算ChannelHandler对应的mask（什么类型的ChannelHandler，对什么事件感兴趣）\n            mask = mask0(clazz);\n            cache.put(clazz, mask);\n        }\n        return mask;\n    }\n\n\n这里需要一个 FastThreadLocal 类型的 MASKS 字段来缓存 ChannelHandler 对应的执行掩码。因为 ChannelHandler 类一旦被定义出来它的执行掩码就固定了，而 netty 需要接收大量的连接，创建大量的 channel ，并为这些 channel 初始化对应的 pipeline ，需要频繁的记录 channelHandler 的执行掩码到 context 类中，所以这里需要将掩码缓存起来。\n\n    private static int mask0(Class<? extends ChannelHandler> handlerType) {\n        int mask = MASK_EXCEPTION_CAUGHT;\n        try {\n            if (ChannelInboundHandler.class.isAssignableFrom(handlerType)) {\n                //如果该ChannelHandler是Inbound类型的，则先将inbound事件全部设置进掩码中\n                mask |= MASK_ALL_INBOUND;\n\n                //最后在对不感兴趣的事件一一排除（handler中的事件回调方法如果标注了@Skip注解，则认为handler对该事件不感兴趣）\n                if (isSkippable(handlerType, \"channelRegistered\", ChannelHandlerContext.class)) {\n                    mask &= ~MASK_CHANNEL_REGISTERED;\n                }\n                if (isSkippable(handlerType, \"channelUnregistered\", ChannelHandlerContext.class)) {\n                    mask &= ~MASK_CHANNEL_UNREGISTERED;\n                }\n                if (isSkippable(handlerType, \"channelActive\", ChannelHandlerContext.class)) {\n                    mask &= ~MASK_CHANNEL_ACTIVE;\n                }\n                if (isSkippable(handlerType, \"channelInactive\", ChannelHandlerContext.class)) {\n                    mask &= ~MASK_CHANNEL_INACTIVE;\n                }\n                if (isSkippable(handlerType, \"channelRead\", ChannelHandlerContext.class, Object.class)) {\n                    mask &= ~MASK_CHANNEL_READ;\n                }\n                if (isSkippable(handlerType, \"channelReadComplete\", ChannelHandlerContext.class)) {\n                    mask &= ~MASK_CHANNEL_READ_COMPLETE;\n                }\n                if (isSkippable(handlerType, \"channelWritabilityChanged\", ChannelHandlerContext.class)) {\n                    mask &= ~MASK_CHANNEL_WRITABILITY_CHANGED;\n                }\n                if (isSkippable(handlerType, \"userEventTriggered\", ChannelHandlerContext.class, Object.class)) {\n                    mask &= ~MASK_USER_EVENT_TRIGGERED;\n                }\n            }\n\n            if (ChannelOutboundHandler.class.isAssignableFrom(handlerType)) {\n                //如果handler为Outbound类型的，则先将全部outbound事件设置进掩码中\n                mask |= MASK_ALL_OUTBOUND;\n\n                //最后对handler不感兴趣的事件从掩码中一一排除\n                if (isSkippable(handlerType, \"bind\", ChannelHandlerContext.class,\n                        SocketAddress.class, ChannelPromise.class)) {\n                    mask &= ~MASK_BIND;\n                }\n                if (isSkippable(handlerType, \"connect\", ChannelHandlerContext.class, SocketAddress.class,\n                        SocketAddress.class, ChannelPromise.class)) {\n                    mask &= ~MASK_CONNECT;\n                }\n                if (isSkippable(handlerType, \"disconnect\", ChannelHandlerContext.class, ChannelPromise.class)) {\n                    mask &= ~MASK_DISCONNECT;\n                }\n                if (isSkippable(handlerType, \"close\", ChannelHandlerContext.class, ChannelPromise.class)) {\n                    mask &= ~MASK_CLOSE;\n                }\n                if (isSkippable(handlerType, \"deregister\", ChannelHandlerContext.class, ChannelPromise.class)) {\n                    mask &= ~MASK_DEREGISTER;\n                }\n                if (isSkippable(handlerType, \"read\", ChannelHandlerContext.class)) {\n                    mask &= ~MASK_READ;\n                }\n                if (isSkippable(handlerType, \"write\", ChannelHandlerContext.class,\n                        Object.class, ChannelPromise.class)) {\n                    mask &= ~MASK_WRITE;\n                }\n                if (isSkippable(handlerType, \"flush\", ChannelHandlerContext.class)) {\n                    mask &= ~MASK_FLUSH;\n                }\n            }\n\n            if (isSkippable(handlerType, \"exceptionCaught\", ChannelHandlerContext.class, Throwable.class)) {\n                mask &= ~MASK_EXCEPTION_CAUGHT;\n            }\n        } catch (Exception e) {\n            // Should never reach here.\n            PlatformDependent.throwException(e);\n        }\n\n        //计算出的掩码需要缓存，因为每次向pipeline中添加该类型的handler的时候都需要获取掩码（创建一个channel 就需要为其初始化pipeline）\n        return mask;\n    }\n\n\n计算 ChannelHandler 的执行掩码 mask0 方法虽然比较长，但是逻辑却十分简单。在本文的第三小节《3. pipeline中的事件分类》中，笔者为大家详细介绍了各种事件类型的掩码表示，这里我来看下如何利用这些基本事件掩码来计算出 ChannelHandler 的执行掩码的。\n\n如果 ChannelHandler 是 ChannelInboundHandler 类型的，那么首先会将所有 Inbound 事件掩码设置进执行掩码 mask 中。\n\n最后挨个遍历所有 Inbound 事件，从掩码集合 mask 中排除该 ChannelHandler 不感兴趣的事件。这样一轮下来，就得到了 ChannelHandler 的执行掩码。\n\n> 从这个过程中我们可以看到，ChannelHandler 的执行掩码包含的是该 ChannelHandler 感兴趣的事件掩码集合。当事件在 pipeline 中传播的时候，在 ChannelHandlerContext 中可以利用这个执行掩码来判断，当前 ChannelHandler 是否符合响应该事件的资格。\n\n同理我们也可以计算出 ChannelOutboundHandler 类型的 ChannelHandler 对应的执行掩码。\n\n那么 netty 框架是如何判断出我们自定义的 ChannelHandler 对哪些事件感兴趣，对哪些事件不感兴趣的呢?\n\n这里我们以 ChannelInboundHandler 类型举例说明，在本文第三小节中，笔者对所有 Inbound 类型的事件作了一个全面的介绍，但是在实际开发中，我们可能并不需要监听所有的 Inbound 事件，可能只是需要监听其中的一到两个事件。\n\n对于我们不感兴趣的事件，我们只需要在其对应的回调方法上标注 @Skip 注解即可，netty 就会认为该 ChannelHandler 对标注 @Skip 注解的事件不感兴趣，当不感兴趣的事件在 pipeline 传播的时候，该 ChannelHandler 就不需要执行响应。\n\n    private static boolean isSkippable(\n            final Class<?> handlerType, final String methodName, final Class<?>... paramTypes) throws Exception {\n        return AccessController.doPrivileged(new PrivilegedExceptionAction<Boolean>() {\n            @Override\n            public Boolean run() throws Exception {\n                Method m;\n                try {\n                    // 首先查看类中是否覆盖实现了对应的事件回调方法\n                    m = handlerType.getMethod(methodName, paramTypes);\n                } catch (NoSuchMethodException e) {\n                    if (logger.isDebugEnabled()) {\n                        logger.debug(\n                            \"Class {} missing method {}, assume we can not skip execution\", handlerType, methodName, e);\n                    }\n                    return false;\n                }\n                return m != null && m.isAnnotationPresent(Skip.class);\n            }\n        });\n    }\n\n\n那我们在编写自定义 ChannelHandler 的时候是不是要在 ChannelInboundHandler 或者 ChannelOutboundHandler 接口提供的所有事件回调方法上，对我们不感兴趣的事件繁琐地一一标注 @Skip 注解呢？\n\n其实是不需要的，netty 为我们提供了 ChannelInboundHandlerAdapter 类和 ChannelOutboundHandlerAdapter 类，netty 事先已经在这些 Adapter 类中的事件回调方法上全部标注了 @Skip 注解，我们在自定义实现 ChannelHandler 的时候只需要继承这些 Adapter 类并覆盖我们感兴趣的事件回调方法即可。\n\npublic class ChannelInboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelInboundHandler {\n\n    @Skip\n    @Override\n    public void channelRegistered(ChannelHandlerContext ctx) throws Exception {\n        ctx.fireChannelRegistered();\n    }\n\n    @Skip\n    @Override\n    public void channelUnregistered(ChannelHandlerContext ctx) throws Exception {\n        ctx.fireChannelUnregistered();\n    }\n\n    @Skip\n    @Override\n    public void channelActive(ChannelHandlerContext ctx) throws Exception {\n        ctx.fireChannelActive();\n    }\n\n    @Skip\n    @Override\n    public void channelInactive(ChannelHandlerContext ctx) throws Exception {\n        ctx.fireChannelInactive();\n    }\n\n    @Skip\n    @Override\n    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {\n        ctx.fireChannelRead(msg);\n    }\n\n    @Skip\n    @Override\n    public void channelReadComplete(ChannelHandlerContext ctx) throws Exception {\n        ctx.fireChannelReadComplete();\n    }\n\n    @Skip\n    @Override\n    public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception {\n        ctx.fireUserEventTriggered(evt);\n    }\n\n    @Skip\n    @Override\n    public void channelWritabilityChanged(ChannelHandlerContext ctx) throws Exception {\n        ctx.fireChannelWritabilityChanged();\n    }\n\n    @Skip\n    @Override\n    @SuppressWarnings(\"deprecation\")\n    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause)\n            throws Exception {\n        ctx.fireExceptionCaught(cause);\n    }\n}\n\n\n\n# 6. 从 pipeline 删除 channelHandler\n\n从上个小节的内容中我们可以看到向 pipeline 中添加 ChannelHandler 的逻辑还是比较复杂的，涉及到的细节比较多。\n\n那么在了解了向 pipeline 中添加 ChannelHandler 的过程之后，从 pipeline 中删除 ChannelHandler 的逻辑就变得很好理解了。\n\npublic interface ChannelPipeline\n        extends ChannelInboundInvoker, ChannelOutboundInvoker, Iterable<Entry<String, ChannelHandler>> {\n\n    //从pipeline中删除指定的channelHandler\n    ChannelPipeline remove(ChannelHandler handler);\n    //从pipeline中删除指定名称的channelHandler\n    ChannelHandler remove(String name);\n    //从pipeline中删除特定类型的channelHandler\n    <T extends ChannelHandler> T remove(Class<T> handlerType);\n}\n\n\nnetty 提供了以上三种方式从 pipeline 中删除指定 ChannelHandler ，下面我们以第一种方式为例来介绍 ChannelHandler 的删除过程。\n\npublic class DefaultChannelPipeline implements ChannelPipeline {\n\n    @Override\n    public final ChannelPipeline remove(ChannelHandler handler) {\n        remove(getContextOrDie(handler));\n        return this;\n    }\n\n}\n\n\n\n# 6.1 getContextOrDie\n\n首先需要通过 getContextOrDie 方法在 pipeline 中查找到指定的 ChannelHandler 对应的 ChannelHandelrContext 。以便确认要删除的 ChannelHandler 确实是存在于 pipeline 中。\n\ncontext 方法是通过遍历 pipeline 中的双向链表来查找要删除的 ChannelHandlerContext 。\n\n    private AbstractChannelHandlerContext getContextOrDie(ChannelHandler handler) {\n        AbstractChannelHandlerContext ctx = (AbstractChannelHandlerContext) context(handler);\n        if (ctx == null) {\n            throw new NoSuchElementException(handler.getClass().getName());\n        } else {\n            return ctx;\n        }\n    }\n\n    @Override\n    public final ChannelHandlerContext context(ChannelHandler handler) {\n        ObjectUtil.checkNotNull(handler, \"handler\");\n        // 获取 pipeline 双向链表结构的头结点\n        AbstractChannelHandlerContext ctx = head.next;\n        for (;;) {\n\n            if (ctx == null) {\n                return null;\n            }\n\n            if (ctx.handler() == handler) {\n                return ctx;\n            }\n\n            ctx = ctx.next;\n        }\n    }\n\n\n\n# 6.2 remove\n\nremove 方法的整体代码结构和 addLast0 方法的代码结构一样，整体逻辑也是先从 pipeline 中的双向链表结构中将指定的 ChanneHandlerContext 删除，然后在处理被删除的 ChannelHandler 中 handlerRemoved 方法的回调。\n\n    private AbstractChannelHandlerContext remove(final AbstractChannelHandlerContext ctx) {\n        assert ctx != head && ctx != tail;\n\n        synchronized (this) {\n            //从pipeline的双向列表中删除指定channelHandler对应的context\n            atomicRemoveFromHandlerList(ctx);\n\n            if (!registered) {\n                //如果此时channel还未向reactor注册，则通过向pipeline中添加PendingHandlerRemovedTask任务\n                //在注册之后回调channelHandelr中的handlerRemoved方法\n                callHandlerCallbackLater(ctx, false);\n                return ctx;\n            }\n\n            //channelHandelr从pipeline中删除后，需要回调其handlerRemoved方法\n            //需要确保handlerRemoved方法在channelHandelr指定的executor中进行\n            EventExecutor executor = ctx.executor();\n            if (!executor.inEventLoop()) {\n                executor.execute(new Runnable() {\n                    @Override\n                    public void run() {\n                        callHandlerRemoved0(ctx);\n                    }\n                });\n                return ctx;\n            }\n        }\n        callHandlerRemoved0(ctx);\n        return ctx;\n    }\n\n\n 1. 从 pipeline 中删除指定 ChannelHandler 对应的 ChannelHandlerContext 。逻辑比较简单，就是普通双向链表的删除操作。\n\n    private synchronized void atomicRemoveFromHandlerList(AbstractChannelHandlerContext ctx) {\n        AbstractChannelHandlerContext prev = ctx.prev;\n        AbstractChannelHandlerContext next = ctx.next;\n        prev.next = next;\n        next.prev = prev;\n    }\n\n\n 1. 如果此时 channel 并未向对应的 reactor 进行注册，则需要向 pipeline 的任务列表中添加 PendingHandlerRemovedTask 任务，再该任务中会执行 ChannelHandler 的 handlerRemoved 回调，当 channel 向 reactor 注册成功后，reactor 会执行 pipeline 中任务列表中的任务，从而回调被删除 ChannelHandler 的 handlerRemoved 方法。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)pipeline任务.png\n\n    private final class PendingHandlerRemovedTask extends PendingHandlerCallback {\n\n        PendingHandlerRemovedTask(AbstractChannelHandlerContext ctx) {\n            super(ctx);\n        }\n\n        @Override\n        public void run() {\n            callHandlerRemoved0(ctx);\n        }\n    }\n\n\n在执行 ChannelHandler 中 handlerRemoved 回调的时候，需要对 ChannelHandler 的状态进行判断：只有当 handlerState 为 ADD_COMPLETE 的时候才能回调 handlerRemoved 方法。\n\n> 这里表达的语义是只有当 ChannelHanler 的 handlerAdded 方法被回调之后，那么在 ChannelHanler 被从 pipeline 中删除的时候它的 handlerRemoved 方法才可以被回调。\n\n在 ChannelHandler 的 handlerRemove 方法被回调之后，将 ChannelHandler 的状态设置为 REMOVE_COMPLETE 。\n\n    private void callHandlerRemoved0(final AbstractChannelHandlerContext ctx) {\n\n        try {\n            // 在这里回调 handlerRemoved 方法\n            ctx.callHandlerRemoved();\n        } catch (Throwable t) {\n            fireExceptionCaught(new ChannelPipelineException(\n                    ctx.handler().getClass().getName() + \".handlerRemoved() has thrown an exception.\", t));\n        }\n    }\n\n    final void callHandlerRemoved() throws Exception {\n        try {\n            if (handlerState == ADD_COMPLETE) {\n                handler().handlerRemoved(this);\n            }\n        } finally {\n            // Mark the handler as removed in any case.\n            setRemoved();\n        }\n    }\n\n   final void setRemoved() {\n        handlerState = REMOVE_COMPLETE;\n    }\n\n\n 1. 如果 channel 已经在 reactor 中注册成功，那么当 channelHandler 从 pipeline 中删除之后，需要立即回调其 handlerRemoved 方法。但是需要确保 handlerRemoved 方法在 channelHandler 指定的 executor 中进行。\n\n\n# 7. pipeline 的初始化\n\n其实关于 pipeline 初始化的相关内容我们在《详细图解 Netty Reactor 启动全流程》中已经简要介绍了 NioServerSocketChannel 中的 pipeline 的初始化时机以及过程。\n\n在《Netty 如何高效接收网络连接》中笔者也简要介绍了 NioSocketChannel 中 pipeline 的初始化时机以及过程。\n\n本小节笔者将结合这两种类型的 Channel 来完整全面的介绍 pipeline 的整个初始化过程。\n\n\n# 7.1 NioServerSocketChannel 中 pipeline 的初始化\n\n从前边提到的这两篇文章以及本文前边的相关内容我们知道，Netty 提供了一个特殊的 ChannelInboundHandler 叫做 ChannelInitializer ，用户可以利用这个特殊的 ChannelHandler 对 Channel 中的 pipeline 进行自定义的初始化逻辑。\n\n如果用户只希望在 pipeline 中添加一个固定的 ChannelHandler 可以通过如下代码直接添加。\n\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)//配置主从Reactor\n                  ...........\n             .handler(new LoggingHandler(LogLevel.INFO))\n\n\n如果希望添加多个 ChannelHandler ，则可以通过 ChannelInitializer 来自定义添加逻辑。\n\n> 由于使用 ChannelInitializer 初始化 NioServerSocketChannel 中 pipeline 的逻辑会稍微复杂一点，下面我们均以这个复杂的案例来讲述 pipeline 的初始化过程。\n\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)//配置主从Reactor\n                  ...........\n               .handler(new ChannelInitializer<NioServerSocketChannel>() {\n                 @Override\n                 protected void initChannel(NioServerSocketChannel ch) throws Exception {\n                              ....自定义pipeline初始化逻辑....\n                               ChannelPipeline p = ch.pipeline();\n                               p.addLast(channelHandler1);\n                               p.addLast(channelHandler2);\n                               p.addLast(channelHandler3);\n                                    ........\n                 }\n             })\n\n\n以上这些由用户自定义的用于初始化 pipeline 的 ChannelInitializer ，被保存至 ServerBootstrap 启动类中的 handler 字段中。用于后续的初始化调用\n\npublic abstract class AbstractBootstrap<B extends AbstractBootstrap<B, C>, C extends Channel> implements Cloneable\n   private volatile ChannelHandler handler;\n}\n\n\n在服务端启动的时候，会伴随着 NioServeSocketChannel 的创建以及初始化，在初始化 NioServerSokcetChannel 的时候会将一个新的 ChannelInitializer 添加进 pipeline 中，在新的 ChannelInitializer 中才会将用户自定义的 ChannelInitializer 添加进 pipeline 中，随后才执行初始化过程。\n\nNetty 这里之所以引入一个新的 ChannelInitializer 来初始化 NioServerSocketChannel 中的 pipeline 的原因是需要兼容前边介绍的这两种初始化 pipeline 的方式。\n\n * 一种是直接使用一个具体的 ChannelHandler 来初始化 pipeline。\n * 另一种是使用 ChannelInitializer 来自定义初始化 pipeline 逻辑。\n\n> 忘记 netty 启动过程的同学可以在回看下笔者的《详细图解 Netty Reactor 启动全流程》这篇文章。\n\n   @Override\n    void init(Channel channel) {\n       .........\n\n        p.addLast(new ChannelInitializer<Channel>() {\n            @Override\n            public void initChannel(final Channel ch) {\n                final ChannelPipeline pipeline = ch.pipeline();\n                //ServerBootstrap中用户指定的channelHandler\n                ChannelHandler handler = config.handler();\n                if (handler != null) {\n                    pipeline.addLast(handler);\n                }\n\n                .........\n            }\n        });\n   }\n\n\n注意此时 NioServerSocketChannel 并未开始向 Main Reactor 注册，根据本文第四小节《4. 向 pipeline 添加 channelHandler 》中的介绍，此时向 pipeline 中添加这个新的 ChannelInitializer 之后，netty 会向 pipeline 的任务列表中添加 PendingHandlerAddedTask 。当 NioServerSocketChannel 向 Main Reactor 注册成功之后，紧接着 Main Reactor 线程会调用这个 PendingHandlerAddedTask ，在任务中会执行这个新的 ChannelInitializer 的 handlerAdded 回调。在这个回调方法中会执行上边 initChannel 方法里的代码。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)server channel pipeline 注册前结构.png\n\n当 NioServerSocketChannel 在向 Main Reactor 注册成功之后，就挨个执行 pipeline 中的任务列表中的任务。\n\n       private void register0(ChannelPromise promise) {\n                     .........\n                boolean firstRegistration = neverRegistered;\n                //执行真正的注册操作\n                doRegister();\n                //修改注册状态\n                neverRegistered = false;\n                registered = true;\n                //调用pipeline中的任务链表，执行PendingHandlerAddedTask\n                pipeline.invokeHandlerAddedIfNeeded();\n                .........\n    final void invokeHandlerAddedIfNeeded() {\n        assert channel.eventLoop().inEventLoop();\n        if (firstRegistration) {\n            firstRegistration = false;\n            // 执行 pipeline 任务列表中的 PendingHandlerAddedTask 任务。\n            callHandlerAddedForAllHandlers();\n        }\n    }\n\n\n执行 pipeline 任务列表中的 PendingHandlerAddedTask 任务：\n\n    private void callHandlerAddedForAllHandlers() {\n        // pipeline 任务列表中的头结点\n        final PendingHandlerCallback pendingHandlerCallbackHead;\n        synchronized (this) {\n            assert !registered;\n            // This Channel itself was registered.\n            registered = true;\n            pendingHandlerCallbackHead = this.pendingHandlerCallbackHead;\n            // Null out so it can be GC'ed.\n            this.pendingHandlerCallbackHead = null;\n        }\n\n        PendingHandlerCallback task = pendingHandlerCallbackHead;\n        // 挨个执行任务列表中的任务\n        while (task != null) {\n            //触发 ChannelInitializer 的 handlerAdded 回调\n            task.execute();\n            task = task.next;\n        }\n    }\n\n\n最终在 PendingHandlerAddedTask 中执行 pipeline 中 ChannelInitializer 的 handlerAdded 回调。\n\n> 这个 ChannelInitializer 就是在初始化 NioServerSocketChannel 的 init 方法中向 pipeline 添加的 ChannelInitializer。\n\n@Sharable\npublic abstract class ChannelInitializer<C extends Channel> extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void handlerAdded(ChannelHandlerContext ctx) throws Exception {\n        if (ctx.channel().isRegistered()) {\n\n            if (initChannel(ctx)) {\n                //初始化工作完成后，需要将自身从pipeline中移除\n                removeState(ctx);\n            }\n        }\n    }\n\n}\n\n\n在 handelrAdded 回调中执行 ChannelInitializer 匿名类中 initChannel 方法，注意此时执行的 ChannelInitializer 类为在本小节开头 init 方法中由 Netty 框架添加的 ChannelInitializer ，并不是用户自定义的 ChannelInitializer 。\n\n    @Override\n    void init(Channel channel) {\n       .........\n\n        p.addLast(new ChannelInitializer<Channel>() {\n            @Override\n            public void initChannel(final Channel ch) {\n                final ChannelPipeline pipeline = ch.pipeline();\n                //ServerBootstrap中用户指定的ChannelInitializer\n                ChannelHandler handler = config.handler();\n                if (handler != null) {\n                    pipeline.addLast(handler);\n                }\n\n                .........\n            }\n        });\n   }\n\n\n执行完 ChannelInitializer 匿名类中 initChannel 方法后，需将 ChannelInitializer 从 pipeline 中删除。并回调 ChannelInitializer 的 handlerRemoved 方法。删除过程笔者已经在第六小节《6. 从 pipeline 删除 channelHandler》详细介绍过了。\n\n    private boolean initChannel(ChannelHandlerContext ctx) throws Exception {\n        if (initMap.add(ctx)) { // Guard against re-entrance.\n            try {\n                //执行ChannelInitializer匿名类中的initChannel方法\n                initChannel((C) ctx.channel());\n            } catch (Throwable cause) {\n                exceptionCaught(ctx, cause);\n            } finally {\n                ChannelPipeline pipeline = ctx.pipeline();\n                if (pipeline.context(this) != null) {\n                    //初始化完毕后，从pipeline中移除自身\n                    pipeline.remove(this);\n                }\n            }\n            return true;\n        }\n        return false;\n    }\n\n\n当执行完 initChannel 方法后此时 pipeline 的结构如下图所示：\n\nserverSocketChannel注册之后pipeline结构.png\n\n当用户的自定义 ChannelInitializer 被添加进 pipeline 之后，根据第四小节所讲的添加逻辑，此时 NioServerSocketChannel 已经向 main reactor 成功注册完毕，不再需要向 pipeine 的任务列表中添加 PendingHandlerAddedTask 任务，而是直接调用自定义 ChannelInitializer 中的 handlerAdded 回调，和上面的逻辑一样。不同的是这里最终回调至用户自定义的初始化逻辑实现 initChannel 方法中。执行完用户自定义的初始化逻辑之后，从 pipeline 删除用户自定义的 ChannelInitializer 。\n\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)//配置主从Reactor\n                  ...........\n               .handler(new ChannelInitializer<NioServerSocketChannel>() {\n                 @Override\n                 protected void initChannel(NioServerSocketChannel ch) throws Exception {\n                              ....自定义pipeline初始化逻辑....\n                               ChannelPipeline p = ch.pipeline();\n                               p.addLast(channelHandler1);\n                               p.addLast(channelHandler2);\n                               p.addLast(channelHandler3);\n                                    ........\n                 }\n             })\n\n\n随后 netty 会以异步任务的形式向 pipeline 的末尾添加 ServerBootstrapAcceptor ，至此 NioServerSocketChannel 中 pipeline 的初始化工作就全部完成了。\n\n\n# 7.2 NioSocketChannel 中 pipeline 的初始化\n\n在 7.1 小节中笔者举的这个 pipeline 初始化的例子相对来说比较复杂，当我们把这个复杂例子的初始化逻辑搞清楚之后，NioSocketChannel 中 pipeline 的初始化过程就变的很简单了。\n\n            ServerBootstrap b = new ServerBootstrap();\n            b.group(bossGroup, workerGroup)//配置主从Reactor\n                  ...........\n               .childHandler(new ChannelInitializer<SocketChannel>() {\n                 @Override\n                 protected void initChannel(SocketChannel ch) throws Exception {\n                              ....自定义pipeline初始化逻辑....\n                               ChannelPipeline p = ch.pipeline();\n                               p.addLast(channelHandler1);\n                               p.addLast(channelHandler2);\n                               p.addLast(channelHandler3);\n                                    ........\n                 }\n             })\npublic class ServerBootstrap extends AbstractBootstrap<ServerBootstrap, ServerChannel> {\n    //保存用户自定义ChannelInitializer\n    private volatile ChannelHandler childHandler;\n}\n\n\n在《Netty 如何高效接收网络连接》一文中我们介绍过，当客户端发起连接，完成三次握手之后，NioServerSocketChannel 上的 OP_ACCEPT 事件活跃，随后会在 NioServerSocketChannel 的 pipeline 中触发 channelRead 事件。并最终在 ServerBootstrapAcceptor 中初始化客户端 NioSocketChannel 。\n\n主从Reactor组完整结构.png\n\nprivate static class ServerBootstrapAcceptor extends ChannelInboundHandlerAdapter {\n\n       @Override\n        @SuppressWarnings(\"unchecked\")\n        public void channelRead(ChannelHandlerContext ctx, Object msg) {\n            final Channel child = (Channel) msg;\n            child.pipeline().addLast(childHandler);\n                    ...........\n       }\n}\n\n\n在这里会将用户自定义的 ChannelInitializer 添加进 NioSocketChannel 中的 pipeline 中，由于此时 NioSocketChannel 还没有向 sub reactor 开始注册。所以在向 pipeline 中添加 ChannelInitializer 的同时会伴随着 PendingHandlerAddedTask 被添加进 pipeline 的任务列表中。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)niosocketchannel中pipeline的初始化.png\n\n后面的流程大家应该很熟悉了，和我们在7.1小节中介绍的一模一样，当 NioSocketChannel 再向 sub reactor 注册成功之后，会执行 pipeline 中的任务列表中的 PendingHandlerAddedTask 任务，在 PendingHandlerAddedTask 任务中会回调用户自定义 ChannelInitializer 的 handelrAdded 方法，在该方法中执行 initChannel 方法，用户自定义的初始化逻辑就封装在这里面。在初始化完 pipeline 后，将 ChannelInitializer 从 pipeline 中删除，并回调其 handlerRemoved 方法。\n\n至此客户端 NioSocketChannel 中 pipeline 初始化工作就全部完成了。\n\n\n# 8. 事件传播\n\n在本文第三小节《3. pipeline中的事件分类》中我们介绍了 Netty 事件类型共分为三大类，分别是 Inbound类事件，Outbound类事件，ExceptionCaught事件。并详细介绍了这三类事件的掩码表示，和触发时机，以及事件传播的方向。\n\n本小节我们就来按照 Netty 中异步事件的分类从源码角度分析下事件是如何在 pipeline 中进行传播的。\n\n\n# 8.1 Inbound事件的传播\n\n在第三小节中我们介绍了所有的 Inbound 类事件，这些事件在 pipeline 中的传播逻辑和传播方向都是一样的，唯一的区别就是执行的回调方法不同。\n\n本小节我们就以 ChannelRead 事件的传播为例，来说明 Inbound 类事件是如何在 pipeline 中进行传播的。\n\n第三小节中我们提到过，在 NioSocketChannel 中，ChannelRead 事件的触发时机是在每一次 read loop 读取数据之后在 pipeline 中触发的。\n\n               do {\n                          ............               \n                    allocHandle.lastBytesRead(doReadBytes(byteBuf));\n\n                          ............\n       \n                    // 在客户端NioSocketChannel的pipeline中触发ChannelRead事件\n                    pipeline.fireChannelRead(byteBuf);\n  \n                } while (allocHandle.continueReading());\n\n\n从这里可以看到，任何 Inbound 类事件在 pipeline 中的传播起点都是从 HeadContext 头结点开始的。\n\npublic class DefaultChannelPipeline implements ChannelPipeline {\n\n    @Override\n    public final ChannelPipeline fireChannelRead(Object msg) {\n        AbstractChannelHandlerContext.invokeChannelRead(head, msg);\n        return this;\n    }\n    \n                    .........\n}\n\n\nChannelRead 事件从 HeadContext 开始在 pipeline 中传播，首先就会回调 HeadContext 中的 channelRead 方法。\n\n> 在执行 ChannelHandler 中的相应事件回调方法时，需要确保回调方法的执行在指定的 executor 中进行。\n\n    static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) {\n        final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, \"msg\"), next);\n        EventExecutor executor = next.executor();\n        //需要保证channelRead事件回调在channelHandler指定的executor中进行\n        if (executor.inEventLoop()) {\n            next.invokeChannelRead(m);\n        } else {\n            executor.execute(new Runnable() {\n                @Override\n                public void run() {\n                    next.invokeChannelRead(m);\n                }\n            });\n        }\n    }\n\n    private void invokeChannelRead(Object msg) {\n        if (invokeHandler()) {\n            try {\n                ((ChannelInboundHandler) handler()).channelRead(this, msg);\n            } catch (Throwable t) {\n                invokeExceptionCaught(t);\n            }\n        } else {\n            fireChannelRead(msg);\n        }\n    }\n\n\n在执行 HeadContext 的 channelRead 方法发生异常时，就会回调 HeadContext 的 exceptionCaught 方法。并在相应的事件回调方法中决定是否将事件继续在 pipeline 中传播。\n\n    final class HeadContext extends AbstractChannelHandlerContext\n            implements ChannelOutboundHandler, ChannelInboundHandler {\n\n        @Override\n        public void channelRead(ChannelHandlerContext ctx, Object msg) {\n            ctx.fireChannelRead(msg);\n        }\n\n       @Override\n        public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {\n            ctx.fireExceptionCaught(cause);\n        }\n    }\n\n\n在 HeadContext 中通过 ctx.fireChannelRead(msg) 继续将 ChannelRead 事件在 pipeline 中向后传播。\n\nabstract class AbstractChannelHandlerContext implements ChannelHandlerContext, ResourceLeakHint {\n\n    @Override\n    public ChannelHandlerContext fireChannelRead(final Object msg) {\n        invokeChannelRead(findContextInbound(MASK_CHANNEL_READ), msg);\n        return this;\n    }\n\n}\n\n\n这里的 findContextInbound 方法是整个 inbound 类事件在 pipeline 中传播的核心所在。\n\n因为我们现在需要继续将 ChannelRead 事件在 pipeline 中传播，所以我们目前的核心问题就是通过 findContextInbound 方法在 pipeline 中找到下一个对 ChannelRead 事件感兴趣的 ChannelInboundHandler 。然后执行该 ChannelInboundHandler 的 ChannelRead 事件回调。\n\n    static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) {\n        final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, \"msg\"), next);\n        EventExecutor executor = next.executor();\n        //需要保证channelRead事件回调在channelHandler指定的executor中进行\n        if (executor.inEventLoop()) {\n            next.invokeChannelRead(m);\n        } else {\n            executor.execute(new Runnable() {\n                @Override\n                public void run() {\n                    next.invokeChannelRead(m);\n                }\n            });\n        }\n    }\n\n\nChannelRead 事件就这样循环往复的一直在 pipeline 中传播，在传播的过程中只有对 ChannelRead 事件感兴趣的 ChannelInboundHandler 才可以响应。其他类型的 ChannelHandler 则直接跳过。\n\n如果 ChannelRead 事件在 pipeline 中传播的过程中，没有得到其他 ChannelInboundHandler 的有效处理，最终会被传播到 pipeline 的末尾 TailContext 中。而在本文第二小节中，我们也提到过 TailContext 对于 inbound 事件存在的意义就是做一个兜底的处理。比如：打印日志，释放 bytebuffer 。\n\n final class TailContext extends AbstractChannelHandlerContext implements ChannelInboundHandler {\n\n    @Override\n    public void channelRead(ChannelHandlerContext ctx, Object msg) {\n            onUnhandledInboundMessage(ctx, msg);\n    }\n\n    protected void onUnhandledInboundMessage(ChannelHandlerContext ctx, Object msg) {\n        onUnhandledInboundMessage(msg);\n        if (logger.isDebugEnabled()) {\n            logger.debug(\"Discarded message pipeline : {}. Channel : {}.\",\n                         ctx.pipeline().names(), ctx.channel());\n        }\n    }\n\n    protected void onUnhandledInboundMessage(Object msg) {\n        try {\n            logger.debug(\n                    \"Discarded inbound message {} that reached at the tail of the pipeline. \" +\n                            \"Please check your pipeline configuration.\", msg);\n        } finally {\n            // 释放DirectByteBuffer\n            ReferenceCountUtil.release(msg);\n        }\n    }\n\n}\n\n\n\n# 8.2 findContextInbound\n\n本小节要介绍的 findContextInbound 方法和我们在上篇文章《一文聊透 Netty 发送数据全流程》中介绍的 findContextOutbound 方法均是 netty 异步事件在 pipeline 中传播的核心所在。\n\n事件传播的核心问题就是需要高效的在 pipeline 中按照事件的传播方向，找到下一个具有响应事件资格的 ChannelHandler 。\n\n比如：这里我们在 pipeline 中传播的 ChannelRead 事件，我们就需要在 pipeline 中找到下一个对 ChannelRead 事件感兴趣的 ChannelInboundHandler ，并执行该 ChannelInboudnHandler 的 ChannelRead 事件回调，在 ChannelRead 事件回调中对事件进行业务处理，并决定是否通过 ctx.fireChannelRead(msg) 将 ChannelRead 事件继续向后传播。\n\n    private AbstractChannelHandlerContext findContextInbound(int mask) {\n        AbstractChannelHandlerContext ctx = this;\n        EventExecutor currentExecutor = executor();\n        do {\n            ctx = ctx.next;\n        } while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_INBOUND));\n\n        return ctx;\n    }\n\n\n参数 mask 表示我们正在传播的 ChannelRead 事件掩码 MASK_CHANNEL_READ 。\n\n    static final int MASK_EXCEPTION_CAUGHT = 1;\n    static final int MASK_CHANNEL_REGISTERED = 1 << 1;\n    static final int MASK_CHANNEL_UNREGISTERED = 1 << 2;\n    static final int MASK_CHANNEL_ACTIVE = 1 << 3;\n    static final int MASK_CHANNEL_INACTIVE = 1 << 4;\n    static final int MASK_CHANNEL_READ = 1 << 5;\n    static final int MASK_CHANNEL_READ_COMPLETE = 1 << 6;\n    static final int MASK_USER_EVENT_TRIGGERED = 1 << 7;\n    static final int MASK_CHANNEL_WRITABILITY_CHANGED = 1 << 8;\n\n\n通过 ctx = ctx.next 在 pipeline 中找到下一个 ChannelHandler ，并通过 skipContext 方法判断下一个 ChannelHandler 是否具有响应事件的资格。如果没有则跳过继续向后查找。\n\n比如：下一个 ChannelHandler 如果是一个 ChannelOutboundHandler，或者下一个 ChannelInboundHandler 对 ChannelRead 事件不感兴趣，那么就直接跳过。\n\n\n# 8.3 skipContext\n\n该方法主要用来判断下一个 ChannelHandler 是否具有 mask 代表的事件的响应资格。\n\n    private static boolean skipContext(\n            AbstractChannelHandlerContext ctx, EventExecutor currentExecutor, int mask, int onlyMask) {\n\n        return (ctx.executionMask & (onlyMask | mask)) == 0 ||\n                (ctx.executor() == currentExecutor && (ctx.executionMask & mask) == 0);\n    }\n\n\n * 参数 onlyMask 表示我们需要查找的 ChannelHandler 类型，比如这里我们正在传播 ChannelRead 事件，它是一个 inbound 类事件，那么必须只能由 ChannelInboundHandler 来响应处理，所以这里传入的 onlyMask 为 MASK_ONLY_INBOUND （ ChannelInboundHandler 的掩码表示）\n * ctx.executionMask 我们已经在《5.3 ChanneHandlerContext》小节中详细介绍过了，当 ChannelHandler 被添加进 pipeline 中时，需要计算出该 ChannelHandler 感兴趣的事件集合掩码来，保存在对应 ChannelHandlerContext 的 executionMask 字段中。\n * 首先会通过 ctx.executionMask & (onlyMask | mask)) == 0 来判断下一个 ChannelHandler 类型是否正确，比如我们正在传播 inbound 类事件，下一个却是一个 ChannelOutboundHandler ，那么肯定是要跳过的，继续向后查找。\n * 如果下一个 ChannelHandler 的类型正确，那么就会通过 (ctx.executionMask & mask) == 0 来判断该 ChannelHandler 是否对正在传播的 mask 事件感兴趣。如果该 ChannelHandler 中覆盖了 ChannelRead 回调则执行，如果没有覆盖对应的事件回调方法则跳过，继续向后查找，直到 TailContext 。\n\n以上就是 skipContext 方法的核心逻辑，这里表达的核心语义是：\n\n * 如果 pipeline 中传播的是 inbound 类事件，则必须由 ChannelInboundHandler 来响应，并且该 ChannelHandler 必须覆盖实现对应的 inbound 事件回调。\n * 如果 pipeline 中传播的是 outbound 类事件，则必须由 ChannelOutboundHandler 来响应，并且该 ChannelHandler 必须覆盖实现对应的 outbound 事件回调。\n\n这里大部分同学可能会对 ctx.executor() == currentExecutor这个条件感到很疑惑。加上这个条件，其实对我们这里的核心语义并没有多大影响。\n\n * 当 ctx.executor() == currentExecutor 也就是说前后两个 ChannelHandler 指定的 executor 相同时，我们核心语义保持不变。\n * 当 ctx.executor() != currentExecutor也就是前后两个 ChannelHandler 指定的 executor 不同时，语义变为：只要前后两个 ChannelHandler 指定的 executor 不同，不管下一个ChannelHandler有没有覆盖实现指定事件的回调方法，均不能跳过。 在这种情况下会执行到 ChannelHandler 的默认事件回调方法，继续在 pipeline 中传递事件。我们在《5.3 ChanneHandlerContext》小节提到过 ChannelInboundHandlerAdapter 和 ChannelOutboundHandlerAdapter 会分别对 inbound 类事件回调方法和 outbound 类事件回调方法进行默认的实现。\n\npublic class ChannelOutboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelOutboundHandler {\n\n    @Skip\n    @Override\n    public void bind(ChannelHandlerContext ctx, SocketAddress localAddress,\n            ChannelPromise promise) throws Exception {\n        ctx.bind(localAddress, promise);\n    }\n\n    @Skip\n    @Override\n    public void connect(ChannelHandlerContext ctx, SocketAddress remoteAddress,\n            SocketAddress localAddress, ChannelPromise promise) throws Exception {\n        ctx.connect(remoteAddress, localAddress, promise);\n    }\n\n    @Skip\n    @Override\n    public void disconnect(ChannelHandlerContext ctx, ChannelPromise promise)\n            throws Exception {\n        ctx.disconnect(promise);\n    }\n\n    @Skip\n    @Override\n    public void close(ChannelHandlerContext ctx, ChannelPromise promise)\n            throws Exception {\n        ctx.close(promise);\n    }\n\n    @Skip\n    @Override\n    public void deregister(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception {\n        ctx.deregister(promise);\n    }\n\n    @Skip\n    @Override\n    public void read(ChannelHandlerContext ctx) throws Exception {\n        ctx.read();\n    }\n\n    @Skip\n    @Override\n    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception {\n        ctx.write(msg, promise);\n    }\n\n    @Skip\n    @Override\n    public void flush(ChannelHandlerContext ctx) throws Exception {\n        ctx.flush();\n    }\n}\n\n\n而这里之所以需要加入 ctx.executor() == currentExecutor 条件的判断，是为了防止 HttpContentCompressor 在被指定不同的 executor 情况下无法正确的创建压缩内容，导致的一些异常。但这个不是本文的重点，大家只需要理解这里的核心语义就好，这种特殊情况的特殊处理了解一下就好。\n\n\n# 8.4 Outbound事件的传播\n\n关于 Outbound 类事件的传播，笔者在上篇文章《一文搞懂 Netty 发送数据全流程》中已经进行了详细的介绍，本小节就不在赘述。\n\n\n# 8.5 ExceptionCaught事件的传播\n\n在最后我们来介绍下异常事件在 pipeline 中的传播，ExceptionCaught 事件和 Inbound 类事件一样都是在 pipeline 中从前往后开始传播。\n\nExceptionCaught 事件的触发有两种情况：一种是 netty 框架内部产生的异常，这时 netty 会直接在 pipeline 中触发 ExceptionCaught 事件的传播。异常事件会在 pipeline 中从 HeadContext 开始一直向后传播直到 TailContext。\n\n比如 netty 在 read loop 中读取数据时发生异常：\n\n     try {\n               ...........\n\n               do {\n                          ............               \n                    allocHandle.lastBytesRead(doReadBytes(byteBuf));\n\n                          ............\n       \n                    //客户端NioSocketChannel的pipeline中触发ChannelRead事件\n                    pipeline.fireChannelRead(byteBuf);\n  \n                } while (allocHandle.continueReading());\n\n                         ...........\n        }  catch (Throwable t) {\n                handleReadException(pipeline, byteBuf, t, close, allocHandle);\n       } \n\n\n这时会 netty 会直接从 pipeline 中触发 ExceptionCaught 事件的传播。\n\n       private void handleReadException(ChannelPipeline pipeline, ByteBuf byteBuf, Throwable cause, boolean close,\n                RecvByteBufAllocator.Handle allocHandle) {\n             \n                    .............\n\n            pipeline.fireExceptionCaught(cause);\n\n                    .............\n\n        }\n\n\n和 Inbound 类事件一样，ExceptionCaught 事件会在 pipeline 中从 HeadContext 开始一直向后传播。\n\n    @Override\n    public final ChannelPipeline fireExceptionCaught(Throwable cause) {\n        AbstractChannelHandlerContext.invokeExceptionCaught(head, cause);\n        return this;\n    }\n\n\n第二种触发 ExceptionCaught 事件的情况是，当 Inbound 类事件或者 flush 事件在 pipeline 中传播的过程中，在某个 ChannelHandler 中的事件回调方法处理中发生异常，这时该 ChannelHandler 的 exceptionCaught 方法会被回调。用户可以在这里处理异常事件，并决定是否通过 ctx.fireExceptionCaught(cause) 继续向后传播异常事件。\n\n比如我们在 ChannelInboundHandler 中的 ChannelRead 回调中处理业务请求时发生异常，就会触发该 ChannelInboundHandler 的 exceptionCaught 方法。\n\n    private void invokeChannelRead(Object msg) {\n        if (invokeHandler()) {\n            try {\n                ((ChannelInboundHandler) handler()).channelRead(this, msg);\n            } catch (Throwable t) {\n                invokeExceptionCaught(t);\n            }\n        } else {\n            fireChannelRead(msg);\n        }\n    }\n    private void invokeExceptionCaught(final Throwable cause) {\n        if (invokeHandler()) {\n            try {\n                //触发channelHandler的exceptionCaught回调\n                handler().exceptionCaught(this, cause);\n            } catch (Throwable error) {\n                  ........\n        } else {\n                  ........\n        }\n    }\n\n\n再比如：当我们在 ChannelOutboundHandler 中的 flush 回调中处理业务结果发送的时候发生异常，也会触发该 ChannelOutboundHandler 的 exceptionCaught 方法。\n\n   private void invokeFlush0() {\n        try {\n            ((ChannelOutboundHandler) handler()).flush(this);\n        } catch (Throwable t) {\n            invokeExceptionCaught(t);\n        }\n    }\n\n\n我们可以在 ChannelHandler 的 exceptionCaught 回调中进行异常处理，并决定是否通过 ctx.fireExceptionCaught(cause) 继续向后传播异常事件。\n\n    @Override\n    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause)\n            throws Exception {\n\n        .........异常处理.......\n\n        ctx.fireExceptionCaught(cause);\n    }\n    @Override\n    public ChannelHandlerContext fireExceptionCaught(final Throwable cause) {\n        invokeExceptionCaught(findContextInbound(MASK_EXCEPTION_CAUGHT), cause);\n        return this;\n    }\n\n   static void invokeExceptionCaught(final AbstractChannelHandlerContext next, final Throwable cause) {\n        ObjectUtil.checkNotNull(cause, \"cause\");\n        EventExecutor executor = next.executor();\n        if (executor.inEventLoop()) {\n            next.invokeExceptionCaught(cause);\n        } else {\n            try {\n                executor.execute(new Runnable() {\n                    @Override\n                    public void run() {\n                        next.invokeExceptionCaught(cause);\n                    }\n                });\n            } catch (Throwable t) {\n                if (logger.isWarnEnabled()) {\n                    logger.warn(\"Failed to submit an exceptionCaught() event.\", t);\n                    logger.warn(\"The exceptionCaught() event that was failed to submit was:\", cause);\n                }\n            }\n        }\n    }\n\n\n\n# 8.6 ExceptionCaught 事件和 Inbound 类事件的区别\n\n虽然 ExceptionCaught 事件和 Inbound 类事件在传播方向都是在 pipeline 中从前向后传播。但是大家这里注意区分这两个事件的区别。\n\n在 Inbound 类事件传播过程中是会查找下一个具有事件响应资格的 ChannelInboundHandler 。遇到 ChannelOutboundHandler 会直接跳过。\n\n而 ExceptionCaught 事件无论是在哪种类型的 channelHandler 中触发的，都会从当前异常 ChannelHandler 开始一直向后传播，ChannelInboundHandler 可以响应该异常事件，ChannelOutboundHandler 也可以响应该异常事件。\n\n由于无论异常是在 ChannelInboundHandler 中产生的还是在 ChannelOutboundHandler 中产生的， exceptionCaught 事件都会在 pipeline 中是从前向后传播，并且不关心 ChannelHandler 的类型。所以我们一般将负责统一异常处理的 ChannelHandler 放在 pipeline 的最后，这样它对于 inbound 类异常和 outbound 类异常均可以捕获得到。\n\n异常事件的传播.png\n\n----------------------------------------\n\n\n# 总结\n\n本文涉及到的内容比较多，通过 netty 异步事件在 pipeline 中的编排和传播这条主线，我们相当于将之前的文章内容重新又回顾总结了一遍。\n\n本文中我们详细介绍了 pipeline 的组成结构，它主要是由 ChannelHandlerContext 类型节点组成的双向链表。ChannelHandlerContext 包含了 ChannelHandler 执行上下文的信息，从而可以使 ChannelHandler 只关注于 IO 事件的处理，遵循了单一原则和开闭原则。\n\n此外 pipeline 结构中还包含了一个任务链表，用来存放执行 ChannelHandler 中的 handlerAdded 回调和 handlerRemoved 回调。pipeline 还持有了所属 channel 的引用。\n\n我们还详细介绍了 Netty 中异步事件的分类：Inbound 类事件，Outbound 类事件，ExceptionCaught 事件。并详细介绍了每种分类下的所有事件的触发时机和在 pipeline 中的传播路径。\n\n最后介绍了 pipeline 的结构以及创建和初始化过程，以及对 pipeline 相关操作的源码实现。\n\n中间我们又穿插介绍了 ChannelHanderContext 的结构，介绍了 ChannelHandlerContext 具体封装了哪些关于 ChannelHandler 执行的上下文信息。\n\n本文的内容到这里就结束了，感谢大家的观看，我们下篇文章见~~~",normalizedContent:"# 1. 前文回顾\n\n在前边的系列文章中，笔者为大家详细剖析了 reactor 模型在 netty 中的创建，启动，运行，接收连接，接收数据，发送数据的完整流程，在详细剖析整个 reactor 模型如何在 netty 中实现的过程里，我们或多或少的见到了 pipeline 的身影。\n\nreactor启动后的结构.png\n\n比如在 reactor 启动的过程中首先需要创建 nioserversocketchannel ，在创建的过程中会为 nioserversocketchannel 创建分配一个 pipeline ，用于对 op_accept 事件的编排。\n\n当 nioserversocketchannel 向 main reactor 注册成功后，会在 pipeline 中触发 channelregistered 事件的传播。\n\n当 nioserversocketchannel 绑定端口成功后，会在 pipeline 中触发 channelactive 事件的传播。\n\n主从reactor组完整结构.png\n\n又比如在 reactor 接收连接的过程中，当客户端发起一个连接并完成三次握手之后，连接对应的 socket 会存放在内核中的全连接队列中，随后 jdk selector 会通知 main reactor 此时 nioserversocketchannel 上有 op_accept 事件活跃，最后 main reactor 开始执行 nioserversocketchannel 的底层操作类 niomessageunsafe#read 方法在 nioserversocketchannel 中的 pipeline 中传播 channelread 事件。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)传播channelread事件.png\n\n最终会在 nioserversocketchannel 的 pipeline 中的 serverbootstrapacceptor 中响应 channelread 事件并创建初始化 niosocketchannel ，随后会为每一个新创建的 niosocetchannel 创建分配一个独立的 pipeline ，用于各自 niosocketchannel 上的 io 事件的编排。并向 sub reactor 注册 niosocketchannel ，随后在 niosocketchannel 的 pipeline 中传播 channelregistered 事件，最后传播 channelactive 事件。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)传播channelregister事件.png\n\n还有在《netty如何高效接收网络数据》一文中，我们也提过当 sub reactor 读取 niosocketchannel 中来自客户端的请求数据时，会在 niosocketchannel 的 pipeline 中传播 channelread 事件，在一个完整的 read loop 读取完毕后会传播 channelreadcomplete 事件。\n\n在《一文搞懂netty发送数据全流程》一文中，我们讲到了在用户经过业务处理后，通过 write 方法和 flush 方法分别在 niosocketchannel 的 pipeline 中传播 write 事件和 flush 事件的过程。\n\n笔者带大家又回顾了一下在前边系列文章中关于 pipeline 的使用场景，但是在这些系列文章中并未对 pipeline 相关的细节进行完整全面地描述，那么本文笔者将为大家详细的剖析下 pipeline 在 io 事件的编排和传播场景下的完整实现原理。\n\n内容概要.png\n\n\n# 2. pipeline的创建\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)主从reactor组完整结构.png\n\nnetty 会为每一个 channel 分配一个独立的 pipeline ，pipeline 伴随着 channel 的创建而创建。\n\n前边介绍到 nioserversocketchannel 是在 netty 服务端启动的过程中创建的。而 niosocketchannel 的创建是在当 nioserversocketchannel 上的 op_accept 事件活跃时，由 main reactor 线程在 nioserversocketchannel 中创建，并在 nioserversocketchannel 的 pipeline 中对 op_accept 事件进行编排时（图中的 serverbootstrapacceptor 中）初始化的。\n\n无论是创建 nioserversocketchannel 里的 pipeline 还是创建 niosocketchannel 里的 pipeline , 最终都会委托给它们的父类 abstractchannel 。\n\nimage.png\n\npublic abstract class abstractchannel extends defaultattributemap implements channel {\n\n    protected abstractchannel(channel parent) {\n        this.parent = parent;\n        //channel全局唯一id machineid+processid+sequence+timestamp+random\n        id = newid();\n        //unsafe用于底层socket的相关操作\n        unsafe = newunsafe();\n        //为channel分配独立的pipeline用于io事件编排\n        pipeline = newchannelpipeline();\n    }\n\n    protected defaultchannelpipeline newchannelpipeline() {\n        return new defaultchannelpipeline(this);\n    }\n\n}\npublic class defaultchannelpipeline implements channelpipeline {\n\n      ....................\n\n    //pipeline中的头结点\n    final abstractchannelhandlercontext head;\n    //pipeline中的尾结点\n    final abstractchannelhandlercontext tail;\n\n    //pipeline中持有对应channel的引用\n    private final channel channel;\n\n       ....................\n\n    protected defaultchannelpipeline(channel channel) {\n        //pipeline中持有对应channel的引用\n        this.channel = objectutil.checknotnull(channel, \"channel\");\n        \n        ............省略.......\n\n        tail = new tailcontext(this);\n        head = new headcontext(this);\n\n        head.next = tail;\n        tail.prev = head;\n    }\n\n       ....................\n}\n\n\n在前边的系列文章中笔者多次提到过，pipeline 的结构是由 channelhandlercontext 类型的节点构成的双向链表。其中头结点为 headcontext ，尾结点为 tailcontext 。其初始结构如下：\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)pipeline的初始结构.png\n\n\n# 2.1 headcontext\n\n    private static final string head_name = generatename0(headcontext.class);\n\n    final class headcontext extends abstractchannelhandlercontext\n            implements channeloutboundhandler, channelinboundhandler {\n       //headcontext中持有对channel unsafe操作类的引用 用于执行channel底层操作\n        private final unsafe unsafe;\n\n        headcontext(defaultchannelpipeline pipeline) {\n            super(pipeline, null, head_name, headcontext.class);\n            //持有channel unsafe操作类的引用，后续用于执行channel底层操作\n            unsafe = pipeline.channel().unsafe();\n            //设置channelhandler的状态为add_complete\n            setaddcomplete();\n        }\n\n        @override\n        public channelhandler handler() {\n            return this;\n        }\n\n        .......................\n    }\n\n\n我们知道双向链表结构的 pipeline 中的节点元素为 channelhandlercontext ，既然 headcontext 作为 pipeline 的头结点，那么它一定是 channelhandlercontext 类型的，所以它需要继承实现 abstractchannelhandlercontext ，相当于一个哨兵的作用，因为用户可以以任意顺序向 pipeline 中添加 channelhandler ，需要用 headcontext 来固定指向第一个 channelhandlercontext 。\n\n> 在《一文搞懂netty发送数据全流程》 一文中的《1. channelhandlercontext》小节中，笔者曾为大家详细介绍过 channelhandlercontext 在 pipeline 中的作用，忘记的同学可以在回看下。\n\n于此同时 headcontext 又实现了 channelinboundhandler 和 channeloutboundhandler 接口，说明 headcontext 即是一个 channelhandlercontext 又是一个 channelhandler ，它可以同时处理 inbound 事件和 outbound 事件。\n\n我们也注意到 headcontext 中持有了对应 channel 的底层操作类 unsafe ，这也说明 io 事件在 pipeline 中的传播最终会落在 headcontext 中进行最后的 io 处理。它是 inbound 事件的处理起点，也是 outbound 事件的处理终点。这里也可以看出 headcontext 除了起到哨兵的作用，它还承担了对 channel 底层相关的操作。\n\n比如我们在《reactor在netty中的实现(启动篇)》中介绍的 nioserversocketchannel 在向 main reactor 注册完成后会触发 channelregistered 事件从 headcontext 开始依次在 pipeline 中向后传播。\n\n      @override\n        public void channelregistered(channelhandlercontext ctx) {\n            //此时firstregistration已经变为false,在pipeline.invokehandleraddedifneeded中已被调用过\n            invokehandleraddedifneeded();\n            ctx.firechannelregistered();\n        }\n\n\n以及 nioserversocketchannel 在与端口绑定成功后会触发 channelactive 事件从 headcontext 开始依次在 pipeline 中向后传播，并在 headcontext 中通过 unsafe.beginread() 注册 op_accept 事件到 main reactor 中。\n\n     @override\n        public void read(channelhandlercontext ctx) {\n            //触发注册op_accept或者op_read事件\n            unsafe.beginread();\n        }\n\n\n同理在 niosocketchannel 在向 sub reactor 注册成功后。会先后触发 channelregistered 事件和 channelactive 事件从 headcontext 开始在 pipeline 中向后传播。并在 headcontext 中通过 unsafe.beginread() 注册 op_read 事件到 sub reactor 中。\n\n        @override\n        public void channelactive(channelhandlercontext ctx) {\n            //pipeline中继续向后传播channelactive事件\n            ctx.firechannelactive();\n            //如果是autoread 则自动触发read事件传播\n            //在read回调函数中 触发op_accept或者op_read事件注册\n            readifisautoread();\n        }\n\n\n在《一文搞懂netty发送数据全流程》中介绍的 write 事件和 flush 事件最终会在 pipeline 中从后向前一直传播到 headcontext ，并在 headcontext 中相应事件回调函数中调用 unsafe 类操作底层 channel 发送数据。\n\n        @override\n        public void write(channelhandlercontext ctx, object msg, channelpromise promise) {\n            //到headcontext这里 msg的类型必须是bytebuffer，也就是说必须经过编码器将业务层写入的实体编码为bytebuffer\n            unsafe.write(msg, promise);\n        }\n\n        @override\n        public void flush(channelhandlercontext ctx) {\n            unsafe.flush();\n        }\n\n\n> 从本小节的内容介绍中，我们可以看出在 netty 中对于 channel 的相关底层操作调用均是在 headcontext 中触发的。\n\n\n# 2.2 tailcontext\n\n    private static final string tail_name = generatename0(tailcontext.class);\n\n    final class tailcontext extends abstractchannelhandlercontext implements channelinboundhandler {\n\n        tailcontext(defaultchannelpipeline pipeline) {\n            super(pipeline, null, tail_name, tailcontext.class);\n            //设置channelhandler的状态为add_complete\n            setaddcomplete();\n        }\n\n        @override\n        public channelhandler handler() {\n            return this;\n        }\n    \n        ......................\n}\n\n\n同样 tailcontext 作为双向链表结构的 pipeline 中的尾结点，也需要继承实现 abstractchannelhandlercontext 。但它同时又实现了 channelinboundhandler 。\n\n这说明 tailcontext 除了是一个 channelhandlercontext 同时也是一个 channelinboundhandler 。\n\n# 2.2.1 tailcontext 作为一个 channelhandlercontext 的作用\n\ntailcontext 作为一个 channelhandlercontext 的作用是负责将 outbound 事件从 pipeline 的末尾一直向前传播直到 headcontext 。当然前提是用户需要调用 channel 的相关 outbound 方法。\n\npublic interface channel extends attributemap, channeloutboundinvoker, comparable<channel> {\n\n    channelfuture write(object msg);\n\n    channelfuture write(object msg, channelpromise promise);\n\n    channeloutboundinvoker flush();\n\n    channelfuture writeandflush(object msg, channelpromise promise);\n\n    channelfuture writeandflush(object msg);\n\n}\npublic abstract class abstractchannel extends defaultattributemap implements channel {\n\n   @override\n    public channelfuture write(object msg) {\n        return pipeline.write(msg);\n    }\n\n    @override\n    public channel flush() {\n        pipeline.flush();\n        return this;\n    }\n\n    @override\n    public channelfuture writeandflush(object msg) {\n        return pipeline.writeandflush(msg);\n    }\n}\npublic class defaultchannelpipeline implements channelpipeline {\n\n   @override\n    public final channelfuture write(object msg) {\n        return tail.write(msg);\n    }\n\n    @override\n    public final channelpipeline flush() {\n        tail.flush();\n        return this;\n    }\n\n   @override\n    public final channelfuture writeandflush(object msg) {\n        return tail.writeandflush(msg);\n    }\n\n}\n\n\n这里我们可以看到，当我们在自定义 channelhandler 中调用 ctx.channel().write(msg) 时，会在 abstractchannel 中触发 pipeline.write(msg) ，最终在 defaultchannelpipeline 中调用 tail.write(msg) 。使得 write 事件可以从 pipeline 的末尾开始向前传播，其他 outbound 事件的传播也是一样的道理。\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(final channelhandlercontext ctx, final object msg) {\n        ctx.channel().write(msg);\n    }\n\n}\n\n\n而我们自定义的 channelhandler 会被封装在一个 channelhandlercontext 中从而加入到 pipeline 中，而这个用于装载自定义 channelhandler 的 channelhandlercontext 与 tailcontext 一样本质也都是 channelhandlercontext ，只不过在 pipeline 中的位置不同罢了。\n\n客户端channel pipeline结构.png\n\npublic interface channelhandlercontext extends attributemap, channelinboundinvoker, channeloutboundinvoker {\n\n    channelfuture write(object msg);\n\n    channelfuture write(object msg, channelpromise promise);\n\n    channeloutboundinvoker flush();\n\n    channelfuture writeandflush(object msg, channelpromise promise);\n\n    channelfuture writeandflush(object msg);\n\n}\n\n\n我们看到 channelhandlercontext 接口本身也会继承 channelinboundinvoker 和 channeloutboundinvoker 接口，所以说 contexthandlercontext 也可以触发 inbound 事件和 outbound 事件，只不过表达的语义是在 pipeline 中从当前 channelhandler 开始向前或者向后传播 outbound 事件或者 inbound 事件。\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(final channelhandlercontext ctx, final object msg) {\n        ctx.write(msg);\n    }\n\n}\n\n\n这里表示 write 事件从当前 echoserverhandler 开始在 pipeline 中向前传播直到 headcontext 。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)客户端channel pipeline结构.png\n\n# 2.2.2 tailcontext 作为一个 channelinboundhandler 的作用\n\n最后 tailcontext 作为一个 channelinboundhandler 的作用就是为 inbound 事件在 pipeline 中的传播做一个兜底的处理。\n\n这里提到的兜底处理是什么意思呢？\n\n比如我们前边介绍到的，在 niosocketchannel 向 sub reactor 注册成功后之后触发的 channelregistered 事件和 channelactive 事件。或者在 reactor 线程读取 niosocketchannel 中的请求数据时所触发的 channelread 事件和 channelreadcomplete 事件。\n\n这些 inbound 事件都会首先从 headcontext 开始在 pipeline 中一个一个的向后传递。\n\n极端的情况是如果 pipeline 中所有 channelinboundhandler 中相应的 inbound 事件回调方法均不对事件作出处理，并继续向后传播。如下示例代码所示：\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(final channelhandlercontext ctx, final object msg) {\n        ctx.firechannelread(msg);\n    }\n\n    @override\n    public void channelreadcomplete(channelhandlercontext ctx) {\n        ctx.firechannelreadcomplete();\n    }\n\n    @override\n    public void channelregistered(channelhandlercontext ctx) throws exception {\n        ctx.firechannelregistered();\n    }\n\n    @override\n    public void channelactive(channelhandlercontext ctx) throws exception {\n        ctx.firechannelactive();\n    }\n}\n\n\n最终这些 inbound 事件在 pipeline 中得不到处理，最后会传播到 tailcontext 中。\n\nfinal class tailcontext extends abstractchannelhandlercontext implements channelinboundhandler {\n\n        @override\n        public void channelread(channelhandlercontext ctx, object msg) {\n            onunhandledinboundmessage(ctx, msg);\n        }\n\n        @override\n        public void channelreadcomplete(channelhandlercontext ctx) {\n            onunhandledinboundchannelreadcomplete();\n        }\n\n        @override\n        public void channelactive(channelhandlercontext ctx) {\n            onunhandledinboundchannelactive();\n        }\n\n}\n\n\n而在 tailcontext 中需要对这些得不到任何处理的 inbound 事件做出最终处理。比如丢弃该 msg，并释放所占用的 directbytebuffer，以免发生内存泄露。\n\n    protected void onunhandledinboundmessage(channelhandlercontext ctx, object msg) {\n        onunhandledinboundmessage(msg);\n        if (logger.isdebugenabled()) {\n            logger.debug(\"discarded message pipeline : {}. channel : {}.\",\n                         ctx.pipeline().names(), ctx.channel());\n        }\n    }\n\n    protected void onunhandledinboundmessage(object msg) {\n        try {\n            logger.debug(\n                    \"discarded inbound message {} that reached at the tail of the pipeline. \" +\n                            \"please check your pipeline configuration.\", msg);\n        } finally {\n            referencecountutil.release(msg);\n        }\n    }\n\n\n\n# 3. pipeline中的事件分类\n\n在前边的系列文章中，笔者多次介绍过，netty 中的 io 事件一共分为两大类：inbound 类事件和 outbound 类事件。其实如果严格来分的话应该分为三类。第三种事件类型为 exceptioncaught 异常事件类型。\n\n而 exceptioncaught 事件在事件传播角度上来说和 inbound 类事件一样，都是从 pipeline 的 headcontext 开始一直向后传递或者从当前 channelhandler 开始一直向后传递直到 tailcontext 。所以一般也会将 exceptioncaught 事件统一归为 inbound 类事件。\n\n而根据事件类型的分类，相应负责处理事件回调的 channelhandler 也会被分为两类：\n\n * channelinboundhandler ：主要负责响应处理 inbound 类事件回调和 exceptioncaught 事件回调。\n * channeloutboundhandler ：主要负责响应处理 outbound 类事件回调。\n\n那么我们常说的 inbound 类事件和 outbound 类事件具体都包含哪些事件呢？\n\n\n# 3.1 inbound类事件\n\nfinal class channelhandlermask {\n\n    // inbound事件集合\n    static final int mask_only_inbound =  mask_channel_registered |\n            mask_channel_unregistered | mask_channel_active | mask_channel_inactive | mask_channel_read |\n            mask_channel_read_complete | mask_user_event_triggered | mask_channel_writability_changed;\n\n    private static final int mask_all_inbound = mask_exception_caught | mask_only_inbound;\n\n    // inbound 类事件相关掩码\n    static final int mask_exception_caught = 1;\n    static final int mask_channel_registered = 1 << 1;\n    static final int mask_channel_unregistered = 1 << 2;\n    static final int mask_channel_active = 1 << 3;\n    static final int mask_channel_inactive = 1 << 4;\n    static final int mask_channel_read = 1 << 5;\n    static final int mask_channel_read_complete = 1 << 6;\n    static final int mask_user_event_triggered = 1 << 7;\n    static final int mask_channel_writability_changed = 1 << 8;\n\n}\n\n\nnetty 会将其支持的所有异步事件用掩码来表示，定义在 channelhandlermask 类中， netty 框架通过这些事件掩码可以很方便的知道用户自定义的 channelhandler 是属于什么类型的（channelinboundhandler or channeloutboundhandler ）。\n\n除此之外，inbound 类事件如此之多，用户也并不是对所有的 inbound 类事件感兴趣，用户可以在自定义的 channelinboundhandler 中覆盖自己感兴趣的 inbound 事件回调，从而达到针对特定 inbound 事件的监听。\n\n这些用户感兴趣的 inbound 事件集合同样也会用掩码的形式保存在自定义 channelhandler 对应的 channelhandlercontext 中，这样当特定 inbound 事件在 pipeline 中开始传播的时候，netty 可以根据对应 channelhandlercontext 中保存的 inbound 事件集合掩码来判断，用户自定义的 channelhandler 是否对该 inbound 事件感兴趣，从而决定是否执行用户自定义 channelhandler 中的相应回调方法或者跳过对该 inbound 事件不感兴趣的 channelhandler 继续向后传播。\n\n> 从以上描述中，我们也可以窥探出，netty 引入 channelhandlercontext 来封装 channelhandler 的原因，在代码设计上还是遵循单一职责的原则， channelhandler 是用户接触最频繁的一个 netty 组件，netty 希望用户能够把全部注意力放在最核心的 io 处理上，用户只需要关心自己对哪些异步事件感兴趣并考虑相应的处理逻辑即可，而并不需要关心异步事件在 pipeline 中如何传递，如何选择具有执行条件的 channelhandler 去执行或者跳过。这些切面性质的逻辑，netty 将它们作为上下文信息全部封装在 channelhandlercontext 中由netty框架本身负责处理。\n\n> 以上这些内容，笔者还会在事件传播相关小节做详细的介绍，之所以这里引出，还是为了让大家感受下利用掩码进行集合操作的便利性，netty 中类似这样的设计还有很多，比如前边系列文章中多次提到过的，channel 再向 reactor 注册 io 事件时，netty 也是将 channel 感兴趣的 io 事件用掩码的形式存储于 selectionkey 中的 int interestops 中。\n\n接下来笔者就为大家介绍下这些 inbound 事件，并梳理出这些 inbound 事件的触发时机。方便大家根据各自业务需求灵活地进行监听。\n\n\n# 3.1.1 exceptioncaught 事件\n\n在本小节介绍的这些 inbound 类事件在 pipeline 中传播的过程中，如果在相应事件回调函数执行的过程中发生异常，那么就会触发对应 channelhandler 中的 exceptioncaught 事件回调。\n\n    private void invokeexceptioncaught(final throwable cause) {\n        if (invokehandler()) {\n            try {\n                handler().exceptioncaught(this, cause);\n            } catch (throwable error) {\n                if (logger.isdebugenabled()) {\n                    logger.debug(\n                        \"an exception {}\" +\n                        \"was thrown by a user handler's exceptioncaught() \" +\n                        \"method while handling the following exception:\",\n                        throwableutil.stacktracetostring(error), cause);\n                } else if (logger.iswarnenabled()) {\n                    logger.warn(\n                        \"an exception '{}' [enable debug level for full stacktrace] \" +\n                        \"was thrown by a user handler's exceptioncaught() \" +\n                        \"method while handling the following exception:\", error, cause);\n                }\n            }\n        } else {\n            fireexceptioncaught(cause);\n        }\n    }\n\n\n当然用户可以选择在 exceptioncaught 事件回调中是否执行 ctx.fireexceptioncaught(cause) 从而决定是否将 exceptioncaught 事件继续向后传播。\n\n    @override\n    public void exceptioncaught(channelhandlercontext ctx, throwable cause) {\n        ..........\n        ctx.fireexceptioncaught(cause);\n    }\n\n\n当 netty 内核处理连接的接收，以及数据的读取过程中如果发生异常，会在整个 pipeline 中触发 exceptioncaught 事件的传播。\n\n这里笔者为什么要单独强调在 inbound 事件传播的过程中发生异常，才会回调 exceptioncaught 呢 ?\n\n因为 inbound 事件一般都是由 netty 内核触发传播的，而 outbound 事件一般都是由用户选择触发的，比如用户在处理完业务逻辑触发的 write 事件或者 flush 事件。\n\n而在用户触发 outbound 事件后，一般都会得到一个 channelpromise 。用户可以向 channelpromise 添加各种 listener 。当 outbound 事件在传播的过程中发生异常时，netty 会通知用户持有的这个 channelpromise ，但不会触发 exceptioncaught 的回调。\n\n比如我们在《一文搞懂netty发送数据全流程》一文中介绍到的在 write 事件传播的过程中就不会触发 exceptioncaught 事件回调。只是去通知用户的 channelpromise 。\n\n    private void invokewrite0(object msg, channelpromise promise) {\n        try {\n            //调用当前channelhandler中的write方法\n            ((channeloutboundhandler) handler()).write(this, msg, promise);\n        } catch (throwable t) {\n            notifyoutboundhandlerexception(t, promise);\n        }\n    }\n\n    private static void notifyoutboundhandlerexception(throwable cause, channelpromise promise) {\n        promisenotificationutil.tryfailure(promise, cause, promise instanceof voidchannelpromise ? null : logger);\n    }\n\n\n而 outbound 事件中只有 flush 事件的传播是个例外，当 flush 事件在 pipeline 传播的过程中发生异常时，会触发对应异常 channelhandler 的 exceptioncaught 事件回调。因为 flush 方法的签名中不会给用户返回 channelpromise 。\n\n    @override\n    channelhandlercontext flush();\n    private void invokeflush0() {\n        try {\n            ((channeloutboundhandler) handler()).flush(this);\n        } catch (throwable t) {\n            invokeexceptioncaught(t);\n        }\n    }\n\n\n\n# 3.1.2 channelregistered 事件\n\n当 main reactor 在启动的时候，nioserversocketchannel 会被创建并初始化，随后就会向main reactor注册，当注册成功后就会在 nioserversocketchannel 中的 pipeline 中传播 channelregistered 事件。\n\n当 main reactor 接收客户端发起的连接后，niosocketchannel 会被创建并初始化，随后会向 sub reactor 注册，当注册成功后会在 niosocketchannel 中的 pipeline 传播 channelregistered 事件。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)传播channelregister事件.png\n\nprivate void register0(channelpromise promise) {\n\n        ................\n        //执行真正的注册操作\n        doregister();\n\n        ...........\n\n        //触发channelregister事件\n        pipeline.firechannelregistered();\n\n        .......\n}\n\n\n> 注意：此时对应的 channel 还没有注册 io 事件到相应的 reactor 中。\n\n\n# 3.1.3 channelactive 事件\n\n当 nioserversocketchannel 再向 main reactor 注册成功并触发 channelregistered 事件传播之后，随后就会在 pipeline 中触发 bind 事件，而 bind 事件是一个 outbound 事件，会从 pipeline 中的尾结点 tailcontext 一直向前传播最终在 headcontext 中执行真正的绑定操作。\n\n     @override\n        public void bind(\n                channelhandlercontext ctx, socketaddress localaddress, channelpromise promise) {\n            //触发abstractchannel->bind方法 执行jdk nio selectablechannel 执行底层绑定操作\n            unsafe.bind(localaddress, promise);\n        }\n       @override\n        public final void bind(final socketaddress localaddress, final channelpromise promise) {\n             ..............\n\n            dobind(localaddress);\n\n            ...............\n\n            //绑定成功后 channel激活 触发channelactive事件传播\n            if (!wasactive && isactive()) {\n                invokelater(new runnable() {\n                    @override\n                    public void run() {\n                        //headcontext->channelactive回调方法 执行注册op_accept事件\n                        pipeline.firechannelactive();\n                    }\n                });\n            }\n  \n            ...............\n        }\n\n\n当 netty 服务端 nioserversocketchannel 绑定端口成功之后，才算是真正的 active ，随后触发 channelactive 事件在 pipeline 中的传播。\n\n之前我们也提到过判断 nioserversocketchannel 是否 active 的标准就是 : 底层 jdk nio serversocketchannel 是否 open 并且 serversocket 是否已经完成绑定。\n\n    @override\n    public boolean isactive() {\n        return isopen() && javachannel().socket().isbound();\n    }\n\n\n而客户端 niosocketchannel 中触发 channelactive 事件就会比较简单，当 niosocketchannel 再向 sub reactor 注册成功并触发 channelregistered 之后，紧接着就会触发 channelactive 事件在 pipeline 中传播。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)传播channelactive事件.png\n\nprivate void register0(channelpromise promise) {\n\n        ................\n        //执行真正的注册操作\n        doregister();\n\n        ...........\n\n        //触发channelregister事件\n        pipeline.firechannelregistered();\n\n        .......\n\n        if (isactive()) {\n\n                    if (firstregistration) {\n                        //触发channelactive事件\n                        pipeline.firechannelactive();\n                    } else if (config().isautoread()) {\n                        beginread();\n                    }\n          }\n}\n\n\n而客户端 niosocketchannel 是否 active 的标识是：底层 jdk nio socketchannel 是否 open 并且底层 socket 是否连接。毫无疑问，这里的 socket 一定是 connected 。所以直接触发 channelactive 事件。\n\n    @override\n    public boolean isactive() {\n        socketchannel ch = javachannel();\n        return ch.isopen() && ch.isconnected();\n    }\n\n\n> 注意：此时 channel 才会到相应的 reactor 中去注册感兴趣的 io 事件。当用户自定义的 channelhandler 接收到 channelactive 事件时，表明 io 事件已经注册到 reactor 中了。\n\n\n# 3.1.4 channelread 和 channelreadcomplete 事件\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)接收客户端连接.png\n\n当客户端有新连接请求的时候，服务端的 nioserversocketchannel 上的 op_accept 事件会活跃，随后 main reactor 会在一个 read loop 中不断的调用 serversocketchannel.accept() 接收新的连接直到全部接收完毕或者达到 read loop 最大次数 16 次。\n\n在 nioserversocketchannel 中，每 accept 一个新的连接，就会在 pipeline 中触发 channelread 事件。一个完整的 read loop 结束之后，会触发 channelreadcomplete 事件。\n\n    private final class niomessageunsafe extends abstractniounsafe {\n\n        @override\n        public void read() {\n            ......................\n\n\n                try {\n                    do {\n                        //底层调用nioserversocketchannel->doreadmessages 创建客户端socketchannel\n                        int localread = doreadmessages(readbuf);\n                        .................\n                    } while (allochandle.continuereading());\n\n                } catch (throwable t) {\n                    exception = t;\n                }\n\n                int size = readbuf.size();\n                for (int i = 0; i < size; i ++) {            \n                    pipeline.firechannelread(readbuf.get(i));\n                }\n\n                pipeline.firechannelreadcomplete();\n\n                     .................\n        }\n    }\n\n\n当客户端 niosocketchannel 上有请求数据到来时，niosocketchannel 上的 op_read 事件活跃，随后 sub reactor 也会在一个 read loop 中对 niosocketchannel 中的请求数据进行读取直到读取完毕或者达到 read loop 的最大次数 16 次。\n\n在 read loop 的读取过程中，每读取一次就会在 pipeline 中触发 channelread 事件。当一个完整的 read loop 结束之后，会在 pipeline 中触发 channelreadcomplete 事件。\n\nnetty接收网络数据流程.png\n\n这里需要注意的是当 channelreadcomplete 事件触发时，此时并不代表 niosocketchannel 中的请求数据已经读取完毕，可能的情况是发送的请求数据太多，在一个 read loop 中读取不完达到了最大限制次数 16 次，还没全部读取完毕就退出了 read loop 。一旦退出 read loop 就会触发 channelreadcomplete 事件。详细内容可以查看笔者的这篇文章《netty如何高效接收网络数据》。\n\n\n# 3.1.5 channelwritabilitychanged 事件\n\n当我们处理完业务逻辑得到业务处理结果后，会调用 ctx.write(msg) 触发 write 事件在 pipeline 中的传播。\n\n    @override\n    public void channelread(final channelhandlercontext ctx, final object msg) {\n         ctx.write(msg);\n    }\n\n\n最终 netty 会将发送数据 msg 写入 niosocketchannel 中的待发送缓冲队列 channeloutboundbuffer 中。并等待用户调用 flush 操作从 channeloutboundbuffer 中将待发送数据 msg ，写入到底层 socket 的发送缓冲区中。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)channeloutboundbuffer中缓存待发送数据.png\n\n当对端的接收处理速度非常慢或者网络状况极度拥塞时，使得 tcp 滑动窗口不断的缩小，这就导致发送端的发送速度也变得越来越小，而此时用户还在不断的调用 ctx.write(msg) ，这就会导致 channeloutboundbuffer 会急剧增大，从而可能导致 oom 。netty 引入了高低水位线来控制 channeloutboundbuffer 的内存占用。\n\npublic final class writebufferwatermark {\n\n    private static final int default_low_water_mark = 32 * 1024;\n    private static final int default_high_water_mark = 64 * 1024;\n}\n\n\n当 channeoutboundbuffer 中的内存占用量超过高水位线时，netty 就会将对应的 channel 置为不可写状态，并在 pipeline 中触发 channelwritabilitychanged 事件。\n\n    private void setunwritable(boolean invokelater) {\n        for (;;) {\n            final int oldvalue = unwritable;\n            final int newvalue = oldvalue | 1;\n            if (unwritable_updater.compareandset(this, oldvalue, newvalue)) {\n                if (oldvalue == 0) {\n                    //触发firechannelwritabilitychanged事件 表示当前channel变为不可写\n                    firechannelwritabilitychanged(invokelater);\n                }\n                break;\n            }\n        }\n    }\n\n\n当 channeloutboundbuffer 中的内存占用量低于低水位线时，netty 又会将对应的 niosocketchannel 设置为可写状态，并再次触发 channelwritabilitychanged 事件。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)响应channelwritabilitychanged事件.png\n\n    private void setwritable(boolean invokelater) {\n        for (;;) {\n            final int oldvalue = unwritable;\n            final int newvalue = oldvalue & ~1;\n            if (unwritable_updater.compareandset(this, oldvalue, newvalue)) {\n                if (oldvalue != 0 && newvalue == 0) {\n                    firechannelwritabilitychanged(invokelater);\n                }\n                break;\n            }\n        }\n    }\n\n\n用户可在自定义 channelhandler 中通过 ctx.channel().iswritable() 判断当前 channel 是否可写。\n\n    @override\n    public void channelwritabilitychanged(channelhandlercontext ctx) throws exception {\n\n        if (ctx.channel().iswritable()) {\n            ...........当前channel可写.........\n        } else {\n            ...........当前channel不可写.........\n        }\n    }\n\n\n\n# 3.1.6 usereventtriggered 事件\n\nnetty 提供了一种事件扩展机制可以允许用户自定义异步事件，这样可以使得用户能够灵活的定义各种复杂场景的处理机制。\n\n下面我们来看下如何在 netty 中自定义异步事件。\n\n 1. 定义异步事件。\n\npublic final class ourowndefinedevent {\n \n    public static final ourowndefinedevent instance = new ourowndefinedevent();\n\n    private ourowndefinedevent() { }\n}\n\n\n 1. 触发自定义事件的传播\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(final channelhandlercontext ctx, final object msg) {\n            ......省略.......\n            //事件在pipeline中从当前channelhandlercontext开始向后传播\n            ctx.fireusereventtriggered(ourowndefinedevent.instance);\n            //事件从pipeline的头结点headcontext开始向后传播\n            ctx.channel().pipeline().fireusereventtriggered(ourowndefinedevent.instance);\n\n    }\n}\n     \n\n\n 1. 自定义事件的响应和处理。\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void usereventtriggered(channelhandlercontext ctx, object evt) throws exception {\n\n        if (ourowndefinedevent.instance == evt) {\n              .....自定义事件处理......\n        }\n    }\n\n}\n\n\n后续随着我们源码解读的深入，我们还会看到 netty 自己本身也定义了许多 userevent 事件，我们后面还会在介绍，大家这里只是稍微了解一下相关的用法即可。\n\n\n# 3.1.7 channelinactive和channelunregistered事件\n\n当 channel 被关闭之后会在 pipeline 中先触发 channelinactive 事件的传播然后在触发 channelunregistered 事件的传播。\n\n我们可以在 inbound 类型的 channelhandler 中响应 channelinactive 和 channelunregistered 事件。\n\n    @override\n    public void channelinactive(channelhandlercontext ctx) throws exception {\n        \n        ......响应inactive事件...\n        \n        //继续向后传播inactive事件\n        super.channelinactive(ctx);\n    }\n\n    @override\n    public void channelunregistered(channelhandlercontext ctx) throws exception {\n        \n          ......响应unregistered事件...\n\n        //继续向后传播unregistered事件\n        super.channelunregistered(ctx);\n    }\n\n\n> 这里和连接建立之后的事件触发顺序正好相反，连接建立之后是先触发 channelregistered 事件然后在触发 channelactive 事件。\n\n\n# 3.2 outbound 类事件\n\nfinal class channelhandlermask {\n\n    // outbound 事件的集合\n    static final int mask_only_outbound =  mask_bind | mask_connect | mask_disconnect |\n            mask_close | mask_deregister | mask_read | mask_write | mask_flush;\n\n    private static final int mask_all_outbound = mask_exception_caught | mask_only_outbound;\n    \n    // outbound 事件掩码\n    static final int mask_bind = 1 << 9;\n    static final int mask_connect = 1 << 10;\n    static final int mask_disconnect = 1 << 11;\n    static final int mask_close = 1 << 12;\n    static final int mask_deregister = 1 << 13;\n    static final int mask_read = 1 << 14;\n    static final int mask_write = 1 << 15;\n    static final int mask_flush = 1 << 16;\n}\n\n\n和 inbound 类事件一样，outbound 类事件也有对应的掩码表示。下面我们来看下 outbound类事件的触发时机：\n\n\n# 3.2.1 read 事件\n\n大家这里需要注意区分 read 事件和 channelread 事件的不同。\n\nchannelread 事件前边我们已经介绍了，当 nioserversocketchannel 接收到新连接时，会触发 channelread 事件在其 pipeline 上传播。\n\n当 niosocketchannel 上有请求数据时，在 read loop 中读取请求数据时会触发 channelread 事件在其 pipeline 上传播。\n\n而 read 事件则和 channelread 事件完全不同，read 事件特指使 channel 具备感知 io 事件的能力。nioserversocketchannel 对应的 op_accept 事件的感知能力，niosocketchannel 对应的是 op_read 事件的感知能力。\n\nread 事件的触发是在当 channel 需要向其对应的 reactor 注册读类型事件时（比如 op_accept 事件 和 op_read 事件）才会触发。read 事件的响应就是将 channel 感兴趣的 io 事件注册到对应的 reactor 上。\n\n比如 nioserversocketchannel 感兴趣的是 op_accept 事件， niosocketchannel 感兴趣的是 op_read 事件。\n\n在前边介绍 channelactive 事件时我们提到，当 channel 处于 active 状态后会在 pipeline 中传播 channelactive 事件。而在 headcontext 中的 channelactive 事件回调中会触发 read 事件的传播。\n\nfinal class headcontext extends abstractchannelhandlercontext\n            implements channeloutboundhandler, channelinboundhandler {\n\n        @override\n        public void channelactive(channelhandlercontext ctx) {\n            ctx.firechannelactive();  \n            readifisautoread();\n        }\n\n        private void readifisautoread() {\n            if (channel.config().isautoread()) {\n                //如果是autoread 则触发read事件传播\n                channel.read();\n            }\n        }\n\n        @override\n        public void read(channelhandlercontext ctx) {\n            //触发注册op_accept或者op_read事件\n            unsafe.beginread();\n        }\n }\n\n\n而在 headcontext 中的 read 事件回调中会调用 channel 的底层操作类 unsafe 的 beginread 方法，在该方法中会向 reactor 注册 channel 感兴趣的 io 事件。对于 nioserversocketchannel 来说这里注册的就是 op_accept 事件，对于 niosocketchannel 来说这里注册的则是 op_read 事件。\n\n    @override\n    protected void dobeginread() throws exception {\n        // channel.read() or channelhandlercontext.read() was called\n        final selectionkey selectionkey = this.selectionkey;\n        if (!selectionkey.isvalid()) {\n            return;\n        }\n\n        readpending = true;\n\n        final int interestops = selectionkey.interestops();\n\n        if ((interestops & readinterestop) == 0) {\n            //注册监听op_accept或者op_read事件\n            selectionkey.interestops(interestops | readinterestop);\n        }\n    }\n\n\n细心的同学可能注意到了 channel 对应的配置类中包含了一个 autoread 属性，那么这个 autoread 到底是干什么的呢？\n\n其实这是 netty 为大家提供的一种背压机制，用来防止 oom ，想象一下当对端发送数据非常多并且发送速度非常快，而服务端处理速度非常慢，一时间消费不过来。而对端又在不停的大量发送数据，服务端的 reactor 线程不得不在 read loop 中不停的读取，并且为读取到的数据分配 bytebuffer 。而服务端业务线程又处理不过来，这就导致了大量来不及处理的数据占用了大量的内存空间，从而导致 oom 。\n\n面对这种情况，我们可以通过 channelhandlercontext.channel().config().setautoread(false) 将 autoread 属性设置为 false 。随后 netty 就会将 channel 中感兴趣的读类型事件从 reactor 中注销，从此 reactor 不会再对相应事件进行监听。这样 channel 就不会在读取数据了。\n\n> 这里 nioserversocketchannel 对应的是 op_accept 事件， niosocketchannel 对应的是 op_read 事件。\n\n        protected final void removereadop() {\n            selectionkey key = selectionkey();\n            if (!key.isvalid()) {\n                return;\n            }\n            int interestops = key.interestops();\n            if ((interestops & readinterestop) != 0) {        \n                key.interestops(interestops & ~readinterestop);\n            }\n        }\n\n\n而当服务端的处理速度恢复正常，我们又可以通过 channelhandlercontext.channel().config().setautoread(true) 将 autoread 属性设置为 true 。这样 netty 会在 pipeline 中触发 read 事件，最终在 headcontext 中的 read 事件回调方法中通过调用 unsafe#beginread 方法将 channel 感兴趣的读类型事件重新注册到对应的 reactor 中。\n\n    @override\n    public channelconfig setautoread(boolean autoread) {\n        boolean oldautoread = autoread_updater.getandset(this, autoread ? 1 : 0) == 1;\n        if (autoread && !oldautoread) {\n            //autoread从false变为true\n            channel.read();\n        } else if (!autoread && oldautoread) {\n            //autoread从true变为false\n            autoreadcleared();\n        }\n        return this;\n    }\n\n\n> read 事件可以理解为使 channel 拥有读的能力，当有了读的能力后， channelread 就可以读取具体的数据了。\n\n\n# 3.2.2 write 和 flush 事件\n\nwrite 事件和 flush 事件我们在《一文搞懂netty发送数据全流程》一文中已经非常详尽的介绍过了，这里笔者在带大家简单回顾一下。\n\nwrite 事件和 flush 事件均由用户在处理完业务请求得到业务结果后在业务线程中主动触发。\n\n用户既可以通过 channelhandlercontext 触发也可以通过 channel 来触发。\n\n不同之处在于如果通过 channelhandlercontext 触发，那么 write 事件或者 flush 事件就会在 pipeline 中从当前 channelhandler 开始一直向前传播直到 headcontext 。\n\n    @override\n    public void channelread(final channelhandlercontext ctx, final object msg) {\n       ctx.write(msg);\n    }\n\n    @override\n    public void channelreadcomplete(channelhandlercontext ctx) {\n        ctx.flush();\n    }\n\n\n如果通过 channel 触发，那么 write 事件和 flush 事件就会从 pipeline 的尾部节点 tailcontext 开始一直向前传播直到 headcontext 。\n\n    @override\n    public void channelread(final channelhandlercontext ctx, final object msg) {\n       ctx.channel().write(msg);\n    }\n\n    @override\n    public void channelreadcomplete(channelhandlercontext ctx) {\n        ctx.channel().flush();\n    }\n\n\n当然还有一个 writeandflush 方法，也会分为 channelhandlercontext 触发和 channel 的触发。触发 writeandflush 后，write 事件首先会在 pipeline 中传播，最后 flush 事件在 pipeline 中传播。\n\nnetty 对 write 事件的处理最终会将发送数据写入 channel 对应的写缓冲队列 channeloutboundbuffer 中。此时数据并没有发送出去而是在写缓冲队列中缓存，这也是 netty 实现异步写的核心设计。\n\n最终通过 flush 操作从 channel 中的写缓冲队列 channeloutboundbuffer 中获取到待发送数据，并写入到 socket 的发送缓冲区中。\n\n\n# 3.2.3 close 事件\n\n当用户在 channelhandler 中调用如下方法对 channel 进行关闭时，会触发 close 事件在 pipeline 中从后向前传播。\n\n//close事件从当前channelhandlercontext开始在pipeline中向前传播\nctx.close();\n//close事件从pipeline的尾结点tailcontext开始向前传播\nctx.channel().close();\n\n\n我们可以在outbound类型的channelhandler中响应close事件。\n\npublic class examplechannelhandler extends channeloutboundhandleradapter {\n\n    @override\n    public void close(channelhandlercontext ctx, channelpromise promise) throws exception {\n        \n        .....客户端channel关闭之前的处理回调.....\n        \n        //继续向前传播close事件\n        super.close(ctx, promise);\n    }\n}\n\n\n最终 close 事件会在 pipeline 中一直向前传播直到头结点 headconnect 中，并在 headcontext 中完成连接关闭的操作，当连接完成关闭之后，会在 pipeline中先后触发 channelinactive 事件和 channelunregistered 事件。\n\n\n# 3.2.4 deregister 事件\n\n用户可调用如下代码将当前 channel 从 reactor 中注销掉。\n\n//deregister事件从当前channelhandlercontext开始在pipeline中向前传播\nctx.deregister();\n//deregister事件从pipeline的尾结点tailcontext开始向前传播\nctx.channel().deregister();\n\n\n我们可以在 outbound 类型的 channelhandler 中响应 deregister 事件。\n\npublic class examplechannelhandler extends channeloutboundhandleradapter {\n\n    @override\n    public void deregister(channelhandlercontext ctx, channelpromise promise) throws exception {\n\n\n        .....客户端channel取消注册之前的处理回调.....\n\n        //继续向前传播connect事件\n        super.deregister(ctx, promise);\n    }\n}\n\n\n最终 deregister 事件会传播至 pipeline 中的头结点 headcontext 中，并在 headcontext 中完成底层 channel 取消注册的操作。当 channel 从 reactor 上注销之后，从此 reactor 将不会在监听 channel 上的 io 事件，并触发 channelunregistered 事件在 pipeline 中传播。\n\n\n# 3.2.5 connect 事件\n\n在 netty 的客户端中我们可以利用 niosocketchannel 的 connect 方法触发 connect 事件在 pipeline 中传播。\n\n//connect事件从当前channelhandlercontext开始在pipeline中向前传播\nctx.connect(remoteaddress);\n//connect事件从pipeline的尾结点tailcontext开始向前传播\nctx.channel().connect(remoteaddress);\n\n\n我们可以在 outbound 类型的 channelhandler 中响应 connect 事件。\n\npublic class examplechannelhandler extends channeloutboundhandleradapter {\n\n    @override\n    public void connect(channelhandlercontext ctx, socketaddress remoteaddress, socketaddress localaddress,\n                        channelpromise promise) throws exception {\n                 \n        \n        .....客户端channel连接成功之前的处理回调.....\n        \n        //继续向前传播connect事件\n        super.connect(ctx, remoteaddress, localaddress, promise);\n    }\n}\n\n\n最终 connect 事件会在 pipeline 中的头结点 headcontext 中触发底层的连接建立请求。当客户端成功连接到服务端之后，会在客户端 niosocketchannel 的 pipeline 中传播 channelactive 事件。\n\n\n# 3.2.6 disconnect 事件\n\n在 netty 的客户端中我们也可以调用 niosocketchannel 的 disconnect 方法在 pipeline 中触发 disconnect 事件，这会导致 niosocketchannel 的关闭。\n\n//disconnect事件从当前channelhandlercontext开始在pipeline中向前传播\nctx.disconnect();\n//disconnect事件从pipeline的尾结点tailcontext开始向前传播\nctx.channel().disconnect();\n\n\n我们可以在 outbound 类型的 channelhandler 中响应 disconnect 事件。\n\npublic class examplechannelhandler extends channeloutboundhandleradapter {\n\n\n    @override\n    public void disconnect(channelhandlercontext ctx, channelpromise promise) throws exception {\n        \n        .....客户端channel即将关闭前的处理回调.....\n        \n        //继续向前传播disconnect事件\n        super.disconnect(ctx, promise);\n    }\n}\n\n\n最终 disconnect 事件会传播到 headcontext 中，并在 headcontext 中完成底层的断开连接操作，当客户端断开连接成功关闭之后，会在 pipeline 中先后触发 channelinactive 事件和 channelunregistered 事件。\n\n\n# 4. 向pipeline添加channelhandler\n\n在我们详细介绍了全部的 inbound 类事件和 outbound 类事件的掩码表示以及事件的触发和传播路径后，相信大家现在可以通过 channelinboundhandler 和 channeloutboundhandler 来根据具体的业务场景选择合适的 channelhandler 类型以及监听合适的事件来完成业务需求了。\n\n本小节就该介绍一下自定义的 channelhandler 是如何添加到 pipeline 中的，netty 在这个过程中帮我们作了哪些工作?\n\n           final echoserverhandler serverhandler = new echoserverhandler();\n\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)\n             .channel(nioserversocketchannel.class)\n\n             .............\n\n             .childhandler(new channelinitializer<socketchannel>() {\n                 @override\n                 public void initchannel(socketchannel ch) throws exception {\n                     channelpipeline p = ch.pipeline();          \n                     p.addlast(serverhandler);\n\n                     ......可添加多个channelhandler......\n                 }\n             });\n\n\n以上是笔者简化的一个 netty 服务端配置 serverbootstrap 启动类的一段示例代码。我们可以看到再向 channel 对应的 pipeline 中添加 channelhandler 是通过 channelpipeline#addlast 方法将指定 channelhandler 添加到 pipeline 的末尾处。\n\npublic interface channelpipeline\n        extends channelinboundinvoker, channeloutboundinvoker, iterable<entry<string, channelhandler>> {\n\n    //向pipeline的末尾处批量添加多个channelhandler\n    channelpipeline addlast(channelhandler... handlers);\n\n    //指定channelhandler的executor，由指定的executor执行channelhandler中的回调方法\n    channelpipeline addlast(eventexecutorgroup group, channelhandler... handlers);\n\n     //为channelhandler指定名称\n    channelpipeline addlast(string name, channelhandler handler);\n\n    //为channelhandler指定executor和name\n    channelpipeline addlast(eventexecutorgroup group, string name, channelhandler handler);\n}\npublic class defaultchannelpipeline implements channelpipeline {\n\n    @override\n    public final channelpipeline addlast(channelhandler... handlers) {\n        return addlast(null, handlers);\n    }\n\n    @override\n    public final channelpipeline addlast(eventexecutorgroup executor, channelhandler... handlers) {\n        objectutil.checknotnull(handlers, \"handlers\");\n\n        for (channelhandler h: handlers) {\n            if (h == null) {\n                break;\n            }\n            addlast(executor, null, h);\n        }\n\n        return this;\n    }\n\n    @override\n    public final channelpipeline addlast(string name, channelhandler handler) {\n        return addlast(null, name, handler);\n    }\n}\n\n\n最终 addlast 的这些重载方法都会调用到 defaultchannelpipeline#addlast(eventexecutorgroup, string, channelhandler) 这个方法从而完成 channelhandler 的添加。\n\n    @override\n    public final channelpipeline addlast(eventexecutorgroup group, string name, channelhandler handler) {\n        final abstractchannelhandlercontext newctx;\n        synchronized (this) {\n            //检查同一个channelhandler实例是否允许被重复添加\n            checkmultiplicity(handler);\n\n            //创建channelhandlercontext包裹channelhandler并封装执行传播事件相关的上下文信息\n            newctx = newcontext(group, filtername(name, handler), handler);\n\n            //将channelhandelrcontext插入到pipeline中的末尾处。双向链表操作\n            //此时channelhandler的状态还是add_pending，只有当channelhandler的handleradded方法被回调后，状态才会为add_complete\n            addlast0(newctx);\n\n            //如果当前channel还没有向reactor注册，则将handleradded方法的回调添加进pipeline的任务队列中\n            if (!registered) {\n                //这里主要是用来处理channelinitializer的情况\n                //设置channelhandler的状态为add_pending 即等待添加,当状态变为add_complete时 channelhandler中的handleradded会被回调\n                newctx.setaddpending();\n                //向pipeline中添加pendinghandleraddedtask任务，在任务中回调handleradded\n                //当channel注册到reactor后，pipeline中的pendinghandlercallbackhead任务链表会被挨个执行\n                callhandlercallbacklater(newctx, true);\n                return this;\n            }\n\n            //如果当前channel已经向reactor注册成功，那么就直接回调channelhandler中的handleraddded方法\n            eventexecutor executor = newctx.executor();\n            if (!executor.ineventloop()) {\n                //这里需要确保channelhandler中handleradded方法的回调是在channel指定的executor中\n                callhandleraddedineventloop(newctx, executor);\n                return this;\n            }\n        }\n        //回调channelhandler中的handleraddded方法\n        callhandleradded0(newctx);\n        return this;\n    }\n\n\n这个方法的逻辑还是比较复杂的，涉及到很多细节，为了清晰地为大家讲述，笔者这里还是采用总分总的结构，先描述该方法的总体逻辑，然后在针对核心细节要点展开细节分析。\n\n因为向 pipeline 中添加 channelhandler 的操作可能会在多个线程中进行，所以为了确保添加操作的线程安全性，这里采用一个 synchronized 语句块将整个添加逻辑包裹起来。\n\n 1. 通过 checkmultiplicity 检查被添加的 channelhandler 是否是共享的（标注 @sharable 注解），如果不是共享的那么则不会允许该 channelhandler 的同一实例被添加进多个 pipeline 中。如果是共享的，则允许该 channelhandler 的同一个实例被多次添加进多个 pipeline 中。\n\n    private static void checkmultiplicity(channelhandler handler) {\n        if (handler instanceof channelhandleradapter) {\n            channelhandleradapter h = (channelhandleradapter) handler;\n            //只有标注@sharable注解的channelhandler，才被允许同一个实例被添加进多个pipeline中\n            //注意：标注@sharable之后，一个channelhandler的实例可以被添加到多个channel对应的pipeline中\n            //可能被多线程执行，需要确保线程安全\n            if (!h.issharable() && h.added) {\n                throw new channelpipelineexception(\n                        h.getclass().getname() +\n                        \" is not a @sharable handler, so can't be added or removed multiple times.\");\n            }\n            h.added = true;\n        }\n    }\n\n\n> 这里大家需要注意的是，如果一个 channelhandler 被标注了 @sharable 注解,这就意味着它的一个实例可以被多次添加进多个 pipeline 中（每个 channel 对应一个 pipeline 实例），而这多个不同的 pipeline 可能会被不同的 reactor 线程执行，所以在使用共享 channelhandler 的时候需要确保其线程安全性。\n\n比如下面的实例代码：\n\n@sharable\npublic class echoserverhandler extends channelinboundhandleradapter {\n            .............需要确保线程安全.......\n}\n  final echoserverhandler serverhandler = new echoserverhandler();\n\n  serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)\n               ..................\n            .childhandler(new channelinitializer<socketchannel>() {\n                 @override\n                 public void initchannel(socketchannel ch) throws exception {\n                     channelpipeline p = ch.pipeline();\n                     p.addlast(serverhandler);\n                 }\n             });\n\n\nechoserverhandler 为我们自定义的 channelhandler ，它被 @sharable 注解标注，全局只有一个实例，被添加进多个 channel 的 pipeline 中。从而会被多个 reactor 线程执行到。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)共享channelhandler.png\n\n 1. 为 channelhandler 创建其 channelhandlercontext ，用于封装 channelhandler 的名称，状态信息，执行上下文信息，以及用于感知 channelhandler 在 pipeline 中的位置信息。newcontext 方法涉及的细节较多，后面我们单独介绍。\n 2. 通过 addlast0 将新创建出来的 channelhandlercontext 插入到 pipeline 中末尾处。方法的逻辑很简单其实就是一个普通的双向链表插入操作。\n\n    private void addlast0(abstractchannelhandlercontext newctx) {\n        abstractchannelhandlercontext prev = tail.prev;\n        newctx.prev = prev;\n        newctx.next = tail;\n        prev.next = newctx;\n        tail.prev = newctx;\n    }\n\n\n**但是这里大家需要注意的点是：**虽然此时 channelhandlercontext 被物理的插入到了 pipeline 中，但是此时 channelhandler 的状态依然为 init 状态，从逻辑上来说并未算是真正的插入到 pipeline 中，需要等到 channelhandler 的 handleradded 方法被回调时，状态才变为 add_complete ，而只有 add_complete 状态的 channelhandler 才能响应 pipeline 中传播的事件。\n\nchannelhandelr的状态.png\n\n在上篇文章《一文搞懂netty发送数据全流程》中的《3.1.5 触发nextchannelhandler的write方法回调》小节中我们也提过，在每次 write 事件或者 flush 事件传播的时候，都需要通过 invokehandler 方法来判断 channelhandler 的状态是否为 add_complete ，否则当前 channelhandler 则不能响应正在 pipeline 中传播的事件。必须要等到对应的 handleradded 方法被回调才可以，因为 handleradded 方法中可能包含一些 channelhandler 初始化的重要逻辑。\n\n    private boolean invokehandler() {\n        // 这里是一个优化点，netty 用一个局部变量保存 handlerstate\n        // 目的是减少 volatile 变量 handlerstate 的读取次数\n        int handlerstate = this.handlerstate;\n        return handlerstate == add_complete || (!ordered && handlerstate == add_pending);\n    }\n\n    void invokewrite(object msg, channelpromise promise) {\n        if (invokehandler()) {\n            invokewrite0(msg, promise);\n        } else {\n            // 当前channelhandler虽然添加到pipeline中，但是并没有调用handleradded\n            // 所以不能调用当前channelhandler中的回调方法，只能继续向前传递write事件\n            write(msg, promise);\n        }\n    }\n\n    private void invokeflush() {\n        if (invokehandler()) {\n            invokeflush0();\n        } else {\n            //如果该channelhandler虽然加入到pipeline中但handleradded方法并未被回调，则继续向前传递flush事件\n            flush();\n        }\n    }\n\n\n> 事实上不仅仅是 write 事件和 flush 事件在传播的时候需要判断 channelhandler 的状态，所有的 inbound 类事件和 outbound 类事件在传播的时候都需要通过 invokehandler 方法来判断当前 channelhandler 的状态是否为 add_complete ，需要确保在 channelhandler 响应事件之前，它的 handleradded 方法被回调。\n\n 1. 如果向 pipeline 中添加 channelhandler 的时候， channel 还没来得及注册到 reactor中，那么需要将当前 channelhandler 的状态先设置为 add_pending ，并将回调该 channelhandler 的 handleradded 方法封装成 pendinghandleraddedtask 任务添加进 pipeline 中的任务列表中，等到 channel 向 reactor 注册之后，reactor 线程会挨个执行 pipeline 中任务列表中的任务。\n\n> 这段逻辑主要用来处理 channelinitializer 的添加场景，因为目前只有 channelinitializer 这个特殊的 channelhandler 会在 channel 没有注册之前被添加进 pipeline 中\n\n         if (!registered) {\n                newctx.setaddpending();\n                callhandlercallbacklater(newctx, true);\n                return this;\n         }\n\n\n向 pipeline 的任务列表 pendinghandlercallbackhead 中添加 pendinghandleraddedtask 任务：\n\npublic class defaultchannelpipeline implements channelpipeline {\n\n    // pipeline中的任务列表\n    private pendinghandlercallback pendinghandlercallbackhead;\n\n    // 向任务列表尾部添加pendinghandleraddedtask\n    private void callhandlercallbacklater(abstractchannelhandlercontext ctx, boolean added) {\n        assert !registered;\n\n        pendinghandlercallback task = added ? new pendinghandleraddedtask(ctx) : new pendinghandlerremovedtask(ctx);\n        pendinghandlercallback pending = pendinghandlercallbackhead;\n        if (pending == null) {\n            pendinghandlercallbackhead = task;\n        } else {\n            // find the tail of the linked-list.\n            while (pending.next != null) {\n                pending = pending.next;\n            }\n            pending.next = task;\n        }\n    }\n}\n\n\npendinghandleraddedtask 任务负责回调 channelhandler 中的 handleradded 方法。\n\nprivate final class pendinghandleraddedtask extends pendinghandlercallback {\n        ...............\n\n        @override\n        public void run() {\n            callhandleradded0(ctx);\n        }\n\n       ...............\n}\n    private void callhandleradded0(final abstractchannelhandlercontext ctx) {\n       try {\n            ctx.callhandleradded();\n        } catch (throwable t) {\n           ...............\n        }\n    }\n\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)pipeline任务列表.png\n\n 1. 除了 channelinitializer 这个特殊的 channelhandler 的添加是在 channel 向 reactor 注册之前外，剩下的这些用户自定义的 channelhandler 的添加，均是在 channel 向 reactor 注册之后被添加进 pipeline 的。这种场景下的处理就会变得比较简单，在 channelhandler 被插入到 pipeline 中之后，就会立即回调该 channelhandler 的 handleradded 方法。但是需要确保 handleradded 方法的回调在 channel 指定的 executor 中进行。\n\n            eventexecutor executor = newctx.executor();\n            if (!executor.ineventloop()) {\n                callhandleraddedineventloop(newctx, executor);\n                return this;\n            }  \n      \n            callhandleradded0(newctx);\n\n\n如果当前执行线程并不是 channelhandler 指定的 executor ( !executor.ineventloop() ),那么就需要确保 handleradded 方法的回调在 channel 指定的 executor 中进行。\n\n    private void callhandleraddedineventloop(final abstractchannelhandlercontext newctx, eventexecutor executor) {\n        newctx.setaddpending();\n        executor.execute(new runnable() {\n            @override\n            public void run() {\n                callhandleradded0(newctx);\n            }\n        });\n    }\n\n\n这里需要注意的是需要在回调 handleradded 方法之前将 channelhandler 的状态提前设置为 add_complete 。 因为用户可能在 channelhandler 中的 handeradded 回调中触发一些事件，而如果此时 channelhandler 的状态不是 add_complete 的话，就会停止对事件的响应，从而错过事件的处理。\n\n> 这种属于一种用户极端的使用情况。\n\n    final void callhandleradded() throws exception {\n        if (setaddcomplete()) {\n            handler().handleradded(this);\n        }\n    }\n\n\n\n# 5. channehandlercontext 的创建\n\n在介绍完 channelhandler 向 pipeline 添加的整个逻辑过程后，本小节我们来看下如何为 channelhandler 创建对应的 channelhandlercontext ，以及 channelhandlercontext 中具体包含了哪些上下文信息。\n\npublic class defaultchannelpipeline implements channelpipeline {\n    @override\n    public final channelpipeline addlast(eventexecutorgroup group, string name, channelhandler handler) {\n        final abstractchannelhandlercontext newctx;\n        synchronized (this) {\n             \n            ................\n\n            //创建channelhandlercontext包裹channelhandler并封装执行传播相关的上下文信息\n            newctx = newcontext(group, filtername(name, handler), handler);\n\n             ................\n        }\n\n    }\n\n    private abstractchannelhandlercontext newcontext(eventexecutorgroup group, string name, channelhandler handler) {\n        return new defaultchannelhandlercontext(this, childexecutor(group), name, handler);\n    }\n\n}\n\n\n在创建 channelhandlercontext 之前，需要做两个重要的前置操作：\n\n * 通过 filtername 方法为 channelhandlercontext 过滤出在 pipeline 中唯一的名称。\n * 如果用户为 channelhandler 指定了特殊的 eventexecutorgroup ，这里就需要通过 childexecutor 方法从指定的 eventexecutorgroup 中选出一个 eventexecutor 与 channelhandler 绑定。\n\n\n# 5.1 filtername\n\n    private string filtername(string name, channelhandler handler) {\n        if (name == null) {\n            // 如果没有指定name,则会为handler默认生成一个name，该方法可确保默认生成的name在pipeline中不会重复\n            return generatename(handler);\n        }\n\n        // 如果指定了name，需要确保name在pipeline中是唯一的\n        checkduplicatename(name);\n        return name;\n    }\n\n\n如果用户再向 pipeline 添加 channelhandler 的时候，为其指定了具体的名称，那么这里需要确保用户指定的名称在 pipeline 中是唯一的。\n\n    private void checkduplicatename(string name) {\n        if (context0(name) != null) {\n            throw new illegalargumentexception(\"duplicate handler name: \" + name);\n        }\n    }\n\n    /**\n     * 通过指定名称在pipeline中查找对应的channelhandler 没有返回null\n     * */\n    private abstractchannelhandlercontext context0(string name) {\n        abstractchannelhandlercontext context = head.next;\n        while (context != tail) {\n            if (context.name().equals(name)) {\n                return context;\n            }\n            context = context.next;\n        }\n        return null;\n    }\n\n\n如果用户没有为 channelhandler 指定名称，那么就需要为 channelhandler 在 pipeline 中默认生成一个唯一的名称。\n\n    // pipeline中channelhandler对应的name缓存\n    private static final fastthreadlocal<map<class<?>, string>> namecaches =\n            new fastthreadlocal<map<class<?>, string>>() {\n        @override\n        protected map<class<?>, string> initialvalue() {\n            return new weakhashmap<class<?>, string>();\n        }\n    };\n\n    private string generatename(channelhandler handler) {\n        // 获取pipeline中channelhandler对应的name缓存\n        map<class<?>, string> cache = namecaches.get();\n        class<?> handlertype = handler.getclass();\n        string name = cache.get(handlertype);\n        if (name == null) {\n            // 当前handler还没对应的name缓存，则默认生成：simpleclassname + #0\n            name = generatename0(handlertype);\n            cache.put(handlertype, name);\n        }\n\n        if (context0(name) != null) {\n            // 不断重试名称后缀#n + 1 直到没有重复\n            string basename = name.substring(0, name.length() - 1); \n            for (int i = 1;; i ++) {\n                string newname = basename + i;\n                if (context0(newname) == null) {\n                    name = newname;\n                    break;\n                }\n            }\n        }\n        return name;\n    }\n\n    private static string generatename0(class<?> handlertype) {\n        return stringutil.simpleclassname(handlertype) + \"#0\";\n    }\n\n\npipeline 中使用了一个 fastthreadlocal 类型的 namecaches 来缓存各种类型 channelhandler 的基础名称。后面会根据这个基础名称不断的重试生成一个没有冲突的正式名称。缓存 namecaches 中的 key 表示特定的 channelhandler 类型，value 表示该特定类型的 channelhandler 的基础名称 simpleclassname + #0。\n\n自动为 channelhandler 生成默认名称的逻辑是：\n\n * 首先从缓存中 namecaches 获取当前添加的 channelhandler 的基础名称 simpleclassname + #0。\n * 如果该基础名称 simpleclassname + #0 在 pipeline 中是唯一的，那么就将基础名称作为 channelhandler 的名称。\n * 如果缓存的基础名称在 pipeline 中不是唯一的，则不断的增加名称后缀 simpleclassname#1 ,simpleclassname#2 ...... simpleclassname#n 直到产生一个没有重复的名称。\n\n> 虽然用户不大可能将同一类型的 channelhandler 重复添加到 pipeline 中，但是 netty 为了防止这种反复添加同一类型 channelhandler 的行为导致的名称冲突，从而利用 namecaches 来缓存同一类型 channelhandler 的基础名称 simpleclassname + #0，然后通过不断的重试递增名称后缀，来生成一个在pipeline中唯一的名称。\n\n\n# 5.2 childexecutor\n\n通过前边的介绍我们了解到，当我们向 pipeline 添加 channelhandler 的时候，netty 允许我们为 channelhandler 指定特定的 executor 去执行 channelhandler 中的各种事件回调方法。\n\n通常我们会为 channelhandler 指定一个eventexecutorgroup，在创建channelhandlercontext 的时候，会通过 childexecutor 方法从 eventexecutorgroup 中选取一个 eventexecutor 来与该 channelhandler 绑定。\n\n> eventexecutorgroup 是 netty 自定义的一个线程池模型，其中包含多个 eventexecutor ，而 eventexecutor 在 netty 中是一个线程的执行模型。相关的具体实现和用法笔者已经在《reactor在netty中的实现(创建篇)》一文中给出了详尽的介绍，忘记的同学可以在回顾下。\n\n在介绍 executor 的绑定逻辑之前，这里笔者需要先为大家介绍一个相关的重要参数：single_eventexecutor_per_group ，默认为 true 。\n\nserverbootstrap b = new serverbootstrap();\nb.group(bossgroup, workergroup)\n.channel(nioserversocketchannel.class)\n  .........\n.childoption(channeloption.single_eventexecutor_per_group,true）\n\n\n我们知道在 netty 中，每一个 channel 都会对应一个独立的 pipeline ，如果我们开启了 single_eventexecutor_per_group 参数，表示在一个 channel 对应的 pipeline 中，如果我们为多个 channelhandler 指定了同一个 eventexecutorgroup ，那么这多个 channelhandler 只能绑定到 eventexecutorgroup 中的同一个 eventexecutor 上。\n\n什么意思呢？？比如我们有下面一段初始化pipeline的代码：\n\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)\n                 ........................\n             .childhandler(new channelinitializer<socketchannel>() {\n                 @override\n                 public void initchannel(socketchannel ch) throws exception {\n                     channelpipeline pipeline = ch.pipeline();\n                     pipeline.addlast(eventexecutorgroup,channelhandler1)\n                     pipeline.addlast(eventexecutorgroup,channelhandler2)\n                     pipeline.addlast(eventexecutorgroup,channelhandler3)\n                 }\n             });\n\n\neventexecutorgroup 中包含 eventexecutor1，eventexecutor2 ， eventexecutor3 三个执行线程。\n\n假设此时第一个连接进来，在创建 channel1 后初始化 pipeline1 的时候，如果在开启 single_eventexecutor_per_group 参数的情况下，那么在 channel1 对应的 pipeline1 中 channelhandler1，channelhandler2 ， channelhandler3 绑定的 eventexecutor 均为 eventexecutorgroup 中的 eventexecutor1 。\n\n第二个连接 channel2 对应的 pipeline2 中 channelhandler1 ， channelhandler2 ，channelhandler3 绑定的 eventexecutor 均为 eventexecutorgroup 中的 eventexecutor2 。\n\n第三个连接 channel3 对应的 pipeline3 中 channelhandler1 ， channelhandler2 ，channelhandler3 绑定的 eventexecutor 均为 eventexecutorgroup 中的 eventexecutor3 。\n\n以此类推........\n\nsingle_eventexecutor_per_group绑定.png\n\n如果在关闭 single_eventexecutor_per_group 参数的情况下, channel1 对应的 pipeline1 中 channelhandler1 会绑定到 eventexecutorgroup 中的 eventexecutor1 ，channelhandler2 会绑定到 eventexecutor2 ，channelhandler3 会绑定到 eventexecutor3 。\n\n同理其他 channel 对应的 pipeline 中的 channelhandler 绑定逻辑同 channel1 。它们均会绑定到 eventexecutorgroup 中的不同 eventexecutor 中。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)single_eventexecutor_per_group关闭.png\n\n当我们了解了 single_eventexecutor_per_group参数的作用之后，再来看下面这段绑定逻辑就很容易理解了。\n\n     // 在每个pipeline中都会保存eventexecutorgroup中绑定的线程\n    private map<eventexecutorgroup, eventexecutor> childexecutors;\n\n    private eventexecutor childexecutor(eventexecutorgroup group) {\n        if (group == null) {\n            return null;\n        }\n\n        boolean pineventexecutor = channel.config().getoption(channeloption.single_eventexecutor_per_group);\n        if (pineventexecutor != null && !pineventexecutor) {\n            //如果没有开启single_eventexecutor_per_group，则按顺序从指定的eventexecutorgroup中为channelhandler分配eventexecutor\n            return group.next();\n        }\n\n        //获取pipeline绑定到eventexecutorgroup的线程（在一个pipeline中会为每个指定的eventexecutorgroup绑定一个固定的线程）\n        map<eventexecutorgroup, eventexecutor> childexecutors = this.childexecutors;\n        if (childexecutors == null) {\n            childexecutors = this.childexecutors = new identityhashmap<eventexecutorgroup, eventexecutor>(4);\n        }\n\n        //获取该pipeline绑定在指定eventexecutorgroup中的线程\n        eventexecutor childexecutor = childexecutors.get(group);\n        if (childexecutor == null) {\n            childexecutor = group.next();\n            childexecutors.put(group, childexecutor);\n        }\n        return childexecutor;\n    }\n\n\n如果我们并未特殊指定 channelhandler 的 executor ，那么默认会是对应 channel 绑定的 reactor 线程负责执行该 channelhandler 。\n\n如果我们未开启 single_eventexecutor_per_group，netty 就会从我们指定的 eventexecutorgroup 中按照 round-robin 的方式为 channelhandler 绑定其中一个 eventexecutor 。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)single_eventexecutor_per_group关闭.png\n\n如果我们开启了 single_eventexecutor_per_group，相同的 eventexecutorgroup 在同一个 pipeline 实例中的绑定关系是固定的。在 pipeline 中如果多个 channelhandler 指定了同一个 eventexecutorgroup ，那么这些 channelhandler 的 executor 均会绑定到一个固定的 eventexecutor 上。\n\nsingle_eventexecutor_per_group绑定.png\n\n这种固定的绑定关系缓存于每个 pipeline 中的 map<eventexecutorgroup, eventexecutor> childexecutors 字段中，key 是用户为 channelhandler 指定的 eventexecutorgroup ，value 为该 eventexecutorgroup 在 pipeline 实例中的绑定 eventexecutor 。\n\n接下来就是从 childexecutors 中获取指定 eventexecutorgroup 在该 pipeline 实例中的绑定 eventexecutor，如果绑定关系还未建立，则通过 round-robin 的方式从 eventexecutorgroup 中选取一个 eventexecutor 进行绑定，并在 childexecutor 中缓存绑定关系。\n\n如果绑定关系已经建立，则直接为 channelhandler 指定绑定好的 eventexecutor。\n\n\n# 5.3 channehandlercontext\n\n在介绍完创建 channelhandlercontext 的两个前置操作后，我们回头来看下 channelhandlercontext 中包含了哪些具体的上下文信息。\n\nfinal class defaultchannelhandlercontext extends abstractchannelhandlercontext {\n\n    // channelhandlercontext包裹的channelhandler\n    private final channelhandler handler;\n\n    defaultchannelhandlercontext(\n            defaultchannelpipeline pipeline, eventexecutor executor, string name, channelhandler handler) {\n        super(pipeline, executor, name, handler.getclass());\n        //包裹的channelhandler\n        this.handler = handler;\n    }\n\n    @override\n    public channelhandler handler() {\n        return handler;\n    }\n}\nabstract class abstractchannelhandlercontext implements channelhandlercontext, resourceleakhint {\n    //对应channelhandler的名称\n    private final string name;\n\n    //channelhandlercontext中持有pipeline的引用\n    private final defaultchannelpipeline pipeline;\n\n    // channelhandler对应的executor 默认为reactor\n    final eventexecutor executor;\n\n    //channelhandlercontext中保存channelhandler的执行条件掩码（是什么类型的channelhandler,对什么事件感兴趣）\n    private final int executionmask;\n\n    //false表示 当channelhandler的状态为add_pending的时候，也可以响应pipeline中的事件\n    //true表示只有在channelhandler的状态为add_complete的时候才能响应pipeline中的事件\n    private final boolean ordered;\n\n    //channelhandelr的状态，初始化为init\n    private volatile int handlerstate = init;\n\n    abstractchannelhandlercontext(defaultchannelpipeline pipeline, eventexecutor executor,\n                                  string name, class<? extends channelhandler> handlerclass) {\n        this.name = objectutil.checknotnull(name, \"name\");\n        this.pipeline = pipeline;\n        this.executor = executor;\n        //channelhandlercontext中保存channelhandler的执行条件掩码（是什么类型的channelhandler,对什么事件感兴趣）\n        this.executionmask = mask(handlerclass);\n        ordered = executor == null || executor instanceof orderedeventexecutor;\n    }\n\n}\n\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)channelhandlercontext.png\n\n这里笔者重点介绍 orderd 属性和 executionmask 属性，其他的属性大家很容易理解。\n\n  ordered = executor == null || executor instanceof orderedeventexecutor;\n\n\n当我们不指定 channelhandler 的 executor 时或者指定的 executor 类型为 orderedeventexecutor 时，ordered = true。\n\n那么这个 ordered 属性对于 channelhandler 响应 pipeline 中的事件有什么影响呢？\n\n我们之前介绍过在 channelhandler 响应 pipeline 中的事件之前都会调用 invokehandler() 方法来判断是否回调 channelhandler 的事件回调方法还是跳过。\n\n   private boolean invokehandler() {\n        int handlerstate = this.handlerstate;\n        return handlerstate == add_complete || (!ordered && handlerstate == add_pending);\n    }\n\n\n * 当 ordered == false 时，channelhandler 的状态为 add_pending 的时候，也可以响应 pipeline 中的事件。\n * 当 ordered == true 时，只有在 channelhandler 的状态为 add_complete 的时候才能响应 pipeline 中的事件\n\n另一个重要的属性 executionmask 保存的是当前 channelhandler 的一些执行条件信息掩码，比如：\n\n * 当前 channelhandler 是什么类型的（ channelinboundhandler or channeloutboundhandler ?）。\n * 当前 channelhandler 对哪些事件感兴趣（覆盖了哪些事件回调方法?）\n\n    private static final fastthreadlocal<map<class<? extends channelhandler>, integer>> masks =\n            new fastthreadlocal<map<class<? extends channelhandler>, integer>>() {\n                @override\n                protected map<class<? extends channelhandler>, integer> initialvalue() {\n                    return new weakhashmap<class<? extends channelhandler>, integer>(32);\n                }\n            };\n\n    static int mask(class<? extends channelhandler> clazz) {\n        // 因为每建立一个channel就会初始化一个pipeline，这里需要将channelhandler对应的mask缓存\n        map<class<? extends channelhandler>, integer> cache = masks.get();\n        integer mask = cache.get(clazz);\n        if (mask == null) {\n            // 计算channelhandler对应的mask（什么类型的channelhandler，对什么事件感兴趣）\n            mask = mask0(clazz);\n            cache.put(clazz, mask);\n        }\n        return mask;\n    }\n\n\n这里需要一个 fastthreadlocal 类型的 masks 字段来缓存 channelhandler 对应的执行掩码。因为 channelhandler 类一旦被定义出来它的执行掩码就固定了，而 netty 需要接收大量的连接，创建大量的 channel ，并为这些 channel 初始化对应的 pipeline ，需要频繁的记录 channelhandler 的执行掩码到 context 类中，所以这里需要将掩码缓存起来。\n\n    private static int mask0(class<? extends channelhandler> handlertype) {\n        int mask = mask_exception_caught;\n        try {\n            if (channelinboundhandler.class.isassignablefrom(handlertype)) {\n                //如果该channelhandler是inbound类型的，则先将inbound事件全部设置进掩码中\n                mask |= mask_all_inbound;\n\n                //最后在对不感兴趣的事件一一排除（handler中的事件回调方法如果标注了@skip注解，则认为handler对该事件不感兴趣）\n                if (isskippable(handlertype, \"channelregistered\", channelhandlercontext.class)) {\n                    mask &= ~mask_channel_registered;\n                }\n                if (isskippable(handlertype, \"channelunregistered\", channelhandlercontext.class)) {\n                    mask &= ~mask_channel_unregistered;\n                }\n                if (isskippable(handlertype, \"channelactive\", channelhandlercontext.class)) {\n                    mask &= ~mask_channel_active;\n                }\n                if (isskippable(handlertype, \"channelinactive\", channelhandlercontext.class)) {\n                    mask &= ~mask_channel_inactive;\n                }\n                if (isskippable(handlertype, \"channelread\", channelhandlercontext.class, object.class)) {\n                    mask &= ~mask_channel_read;\n                }\n                if (isskippable(handlertype, \"channelreadcomplete\", channelhandlercontext.class)) {\n                    mask &= ~mask_channel_read_complete;\n                }\n                if (isskippable(handlertype, \"channelwritabilitychanged\", channelhandlercontext.class)) {\n                    mask &= ~mask_channel_writability_changed;\n                }\n                if (isskippable(handlertype, \"usereventtriggered\", channelhandlercontext.class, object.class)) {\n                    mask &= ~mask_user_event_triggered;\n                }\n            }\n\n            if (channeloutboundhandler.class.isassignablefrom(handlertype)) {\n                //如果handler为outbound类型的，则先将全部outbound事件设置进掩码中\n                mask |= mask_all_outbound;\n\n                //最后对handler不感兴趣的事件从掩码中一一排除\n                if (isskippable(handlertype, \"bind\", channelhandlercontext.class,\n                        socketaddress.class, channelpromise.class)) {\n                    mask &= ~mask_bind;\n                }\n                if (isskippable(handlertype, \"connect\", channelhandlercontext.class, socketaddress.class,\n                        socketaddress.class, channelpromise.class)) {\n                    mask &= ~mask_connect;\n                }\n                if (isskippable(handlertype, \"disconnect\", channelhandlercontext.class, channelpromise.class)) {\n                    mask &= ~mask_disconnect;\n                }\n                if (isskippable(handlertype, \"close\", channelhandlercontext.class, channelpromise.class)) {\n                    mask &= ~mask_close;\n                }\n                if (isskippable(handlertype, \"deregister\", channelhandlercontext.class, channelpromise.class)) {\n                    mask &= ~mask_deregister;\n                }\n                if (isskippable(handlertype, \"read\", channelhandlercontext.class)) {\n                    mask &= ~mask_read;\n                }\n                if (isskippable(handlertype, \"write\", channelhandlercontext.class,\n                        object.class, channelpromise.class)) {\n                    mask &= ~mask_write;\n                }\n                if (isskippable(handlertype, \"flush\", channelhandlercontext.class)) {\n                    mask &= ~mask_flush;\n                }\n            }\n\n            if (isskippable(handlertype, \"exceptioncaught\", channelhandlercontext.class, throwable.class)) {\n                mask &= ~mask_exception_caught;\n            }\n        } catch (exception e) {\n            // should never reach here.\n            platformdependent.throwexception(e);\n        }\n\n        //计算出的掩码需要缓存，因为每次向pipeline中添加该类型的handler的时候都需要获取掩码（创建一个channel 就需要为其初始化pipeline）\n        return mask;\n    }\n\n\n计算 channelhandler 的执行掩码 mask0 方法虽然比较长，但是逻辑却十分简单。在本文的第三小节《3. pipeline中的事件分类》中，笔者为大家详细介绍了各种事件类型的掩码表示，这里我来看下如何利用这些基本事件掩码来计算出 channelhandler 的执行掩码的。\n\n如果 channelhandler 是 channelinboundhandler 类型的，那么首先会将所有 inbound 事件掩码设置进执行掩码 mask 中。\n\n最后挨个遍历所有 inbound 事件，从掩码集合 mask 中排除该 channelhandler 不感兴趣的事件。这样一轮下来，就得到了 channelhandler 的执行掩码。\n\n> 从这个过程中我们可以看到，channelhandler 的执行掩码包含的是该 channelhandler 感兴趣的事件掩码集合。当事件在 pipeline 中传播的时候，在 channelhandlercontext 中可以利用这个执行掩码来判断，当前 channelhandler 是否符合响应该事件的资格。\n\n同理我们也可以计算出 channeloutboundhandler 类型的 channelhandler 对应的执行掩码。\n\n那么 netty 框架是如何判断出我们自定义的 channelhandler 对哪些事件感兴趣，对哪些事件不感兴趣的呢?\n\n这里我们以 channelinboundhandler 类型举例说明，在本文第三小节中，笔者对所有 inbound 类型的事件作了一个全面的介绍，但是在实际开发中，我们可能并不需要监听所有的 inbound 事件，可能只是需要监听其中的一到两个事件。\n\n对于我们不感兴趣的事件，我们只需要在其对应的回调方法上标注 @skip 注解即可，netty 就会认为该 channelhandler 对标注 @skip 注解的事件不感兴趣，当不感兴趣的事件在 pipeline 传播的时候，该 channelhandler 就不需要执行响应。\n\n    private static boolean isskippable(\n            final class<?> handlertype, final string methodname, final class<?>... paramtypes) throws exception {\n        return accesscontroller.doprivileged(new privilegedexceptionaction<boolean>() {\n            @override\n            public boolean run() throws exception {\n                method m;\n                try {\n                    // 首先查看类中是否覆盖实现了对应的事件回调方法\n                    m = handlertype.getmethod(methodname, paramtypes);\n                } catch (nosuchmethodexception e) {\n                    if (logger.isdebugenabled()) {\n                        logger.debug(\n                            \"class {} missing method {}, assume we can not skip execution\", handlertype, methodname, e);\n                    }\n                    return false;\n                }\n                return m != null && m.isannotationpresent(skip.class);\n            }\n        });\n    }\n\n\n那我们在编写自定义 channelhandler 的时候是不是要在 channelinboundhandler 或者 channeloutboundhandler 接口提供的所有事件回调方法上，对我们不感兴趣的事件繁琐地一一标注 @skip 注解呢？\n\n其实是不需要的，netty 为我们提供了 channelinboundhandleradapter 类和 channeloutboundhandleradapter 类，netty 事先已经在这些 adapter 类中的事件回调方法上全部标注了 @skip 注解，我们在自定义实现 channelhandler 的时候只需要继承这些 adapter 类并覆盖我们感兴趣的事件回调方法即可。\n\npublic class channelinboundhandleradapter extends channelhandleradapter implements channelinboundhandler {\n\n    @skip\n    @override\n    public void channelregistered(channelhandlercontext ctx) throws exception {\n        ctx.firechannelregistered();\n    }\n\n    @skip\n    @override\n    public void channelunregistered(channelhandlercontext ctx) throws exception {\n        ctx.firechannelunregistered();\n    }\n\n    @skip\n    @override\n    public void channelactive(channelhandlercontext ctx) throws exception {\n        ctx.firechannelactive();\n    }\n\n    @skip\n    @override\n    public void channelinactive(channelhandlercontext ctx) throws exception {\n        ctx.firechannelinactive();\n    }\n\n    @skip\n    @override\n    public void channelread(channelhandlercontext ctx, object msg) throws exception {\n        ctx.firechannelread(msg);\n    }\n\n    @skip\n    @override\n    public void channelreadcomplete(channelhandlercontext ctx) throws exception {\n        ctx.firechannelreadcomplete();\n    }\n\n    @skip\n    @override\n    public void usereventtriggered(channelhandlercontext ctx, object evt) throws exception {\n        ctx.fireusereventtriggered(evt);\n    }\n\n    @skip\n    @override\n    public void channelwritabilitychanged(channelhandlercontext ctx) throws exception {\n        ctx.firechannelwritabilitychanged();\n    }\n\n    @skip\n    @override\n    @suppresswarnings(\"deprecation\")\n    public void exceptioncaught(channelhandlercontext ctx, throwable cause)\n            throws exception {\n        ctx.fireexceptioncaught(cause);\n    }\n}\n\n\n\n# 6. 从 pipeline 删除 channelhandler\n\n从上个小节的内容中我们可以看到向 pipeline 中添加 channelhandler 的逻辑还是比较复杂的，涉及到的细节比较多。\n\n那么在了解了向 pipeline 中添加 channelhandler 的过程之后，从 pipeline 中删除 channelhandler 的逻辑就变得很好理解了。\n\npublic interface channelpipeline\n        extends channelinboundinvoker, channeloutboundinvoker, iterable<entry<string, channelhandler>> {\n\n    //从pipeline中删除指定的channelhandler\n    channelpipeline remove(channelhandler handler);\n    //从pipeline中删除指定名称的channelhandler\n    channelhandler remove(string name);\n    //从pipeline中删除特定类型的channelhandler\n    <t extends channelhandler> t remove(class<t> handlertype);\n}\n\n\nnetty 提供了以上三种方式从 pipeline 中删除指定 channelhandler ，下面我们以第一种方式为例来介绍 channelhandler 的删除过程。\n\npublic class defaultchannelpipeline implements channelpipeline {\n\n    @override\n    public final channelpipeline remove(channelhandler handler) {\n        remove(getcontextordie(handler));\n        return this;\n    }\n\n}\n\n\n\n# 6.1 getcontextordie\n\n首先需要通过 getcontextordie 方法在 pipeline 中查找到指定的 channelhandler 对应的 channelhandelrcontext 。以便确认要删除的 channelhandler 确实是存在于 pipeline 中。\n\ncontext 方法是通过遍历 pipeline 中的双向链表来查找要删除的 channelhandlercontext 。\n\n    private abstractchannelhandlercontext getcontextordie(channelhandler handler) {\n        abstractchannelhandlercontext ctx = (abstractchannelhandlercontext) context(handler);\n        if (ctx == null) {\n            throw new nosuchelementexception(handler.getclass().getname());\n        } else {\n            return ctx;\n        }\n    }\n\n    @override\n    public final channelhandlercontext context(channelhandler handler) {\n        objectutil.checknotnull(handler, \"handler\");\n        // 获取 pipeline 双向链表结构的头结点\n        abstractchannelhandlercontext ctx = head.next;\n        for (;;) {\n\n            if (ctx == null) {\n                return null;\n            }\n\n            if (ctx.handler() == handler) {\n                return ctx;\n            }\n\n            ctx = ctx.next;\n        }\n    }\n\n\n\n# 6.2 remove\n\nremove 方法的整体代码结构和 addlast0 方法的代码结构一样，整体逻辑也是先从 pipeline 中的双向链表结构中将指定的 channehandlercontext 删除，然后在处理被删除的 channelhandler 中 handlerremoved 方法的回调。\n\n    private abstractchannelhandlercontext remove(final abstractchannelhandlercontext ctx) {\n        assert ctx != head && ctx != tail;\n\n        synchronized (this) {\n            //从pipeline的双向列表中删除指定channelhandler对应的context\n            atomicremovefromhandlerlist(ctx);\n\n            if (!registered) {\n                //如果此时channel还未向reactor注册，则通过向pipeline中添加pendinghandlerremovedtask任务\n                //在注册之后回调channelhandelr中的handlerremoved方法\n                callhandlercallbacklater(ctx, false);\n                return ctx;\n            }\n\n            //channelhandelr从pipeline中删除后，需要回调其handlerremoved方法\n            //需要确保handlerremoved方法在channelhandelr指定的executor中进行\n            eventexecutor executor = ctx.executor();\n            if (!executor.ineventloop()) {\n                executor.execute(new runnable() {\n                    @override\n                    public void run() {\n                        callhandlerremoved0(ctx);\n                    }\n                });\n                return ctx;\n            }\n        }\n        callhandlerremoved0(ctx);\n        return ctx;\n    }\n\n\n 1. 从 pipeline 中删除指定 channelhandler 对应的 channelhandlercontext 。逻辑比较简单，就是普通双向链表的删除操作。\n\n    private synchronized void atomicremovefromhandlerlist(abstractchannelhandlercontext ctx) {\n        abstractchannelhandlercontext prev = ctx.prev;\n        abstractchannelhandlercontext next = ctx.next;\n        prev.next = next;\n        next.prev = prev;\n    }\n\n\n 1. 如果此时 channel 并未向对应的 reactor 进行注册，则需要向 pipeline 的任务列表中添加 pendinghandlerremovedtask 任务，再该任务中会执行 channelhandler 的 handlerremoved 回调，当 channel 向 reactor 注册成功后，reactor 会执行 pipeline 中任务列表中的任务，从而回调被删除 channelhandler 的 handlerremoved 方法。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)pipeline任务.png\n\n    private final class pendinghandlerremovedtask extends pendinghandlercallback {\n\n        pendinghandlerremovedtask(abstractchannelhandlercontext ctx) {\n            super(ctx);\n        }\n\n        @override\n        public void run() {\n            callhandlerremoved0(ctx);\n        }\n    }\n\n\n在执行 channelhandler 中 handlerremoved 回调的时候，需要对 channelhandler 的状态进行判断：只有当 handlerstate 为 add_complete 的时候才能回调 handlerremoved 方法。\n\n> 这里表达的语义是只有当 channelhanler 的 handleradded 方法被回调之后，那么在 channelhanler 被从 pipeline 中删除的时候它的 handlerremoved 方法才可以被回调。\n\n在 channelhandler 的 handlerremove 方法被回调之后，将 channelhandler 的状态设置为 remove_complete 。\n\n    private void callhandlerremoved0(final abstractchannelhandlercontext ctx) {\n\n        try {\n            // 在这里回调 handlerremoved 方法\n            ctx.callhandlerremoved();\n        } catch (throwable t) {\n            fireexceptioncaught(new channelpipelineexception(\n                    ctx.handler().getclass().getname() + \".handlerremoved() has thrown an exception.\", t));\n        }\n    }\n\n    final void callhandlerremoved() throws exception {\n        try {\n            if (handlerstate == add_complete) {\n                handler().handlerremoved(this);\n            }\n        } finally {\n            // mark the handler as removed in any case.\n            setremoved();\n        }\n    }\n\n   final void setremoved() {\n        handlerstate = remove_complete;\n    }\n\n\n 1. 如果 channel 已经在 reactor 中注册成功，那么当 channelhandler 从 pipeline 中删除之后，需要立即回调其 handlerremoved 方法。但是需要确保 handlerremoved 方法在 channelhandler 指定的 executor 中进行。\n\n\n# 7. pipeline 的初始化\n\n其实关于 pipeline 初始化的相关内容我们在《详细图解 netty reactor 启动全流程》中已经简要介绍了 nioserversocketchannel 中的 pipeline 的初始化时机以及过程。\n\n在《netty 如何高效接收网络连接》中笔者也简要介绍了 niosocketchannel 中 pipeline 的初始化时机以及过程。\n\n本小节笔者将结合这两种类型的 channel 来完整全面的介绍 pipeline 的整个初始化过程。\n\n\n# 7.1 nioserversocketchannel 中 pipeline 的初始化\n\n从前边提到的这两篇文章以及本文前边的相关内容我们知道，netty 提供了一个特殊的 channelinboundhandler 叫做 channelinitializer ，用户可以利用这个特殊的 channelhandler 对 channel 中的 pipeline 进行自定义的初始化逻辑。\n\n如果用户只希望在 pipeline 中添加一个固定的 channelhandler 可以通过如下代码直接添加。\n\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)//配置主从reactor\n                  ...........\n             .handler(new logginghandler(loglevel.info))\n\n\n如果希望添加多个 channelhandler ，则可以通过 channelinitializer 来自定义添加逻辑。\n\n> 由于使用 channelinitializer 初始化 nioserversocketchannel 中 pipeline 的逻辑会稍微复杂一点，下面我们均以这个复杂的案例来讲述 pipeline 的初始化过程。\n\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)//配置主从reactor\n                  ...........\n               .handler(new channelinitializer<nioserversocketchannel>() {\n                 @override\n                 protected void initchannel(nioserversocketchannel ch) throws exception {\n                              ....自定义pipeline初始化逻辑....\n                               channelpipeline p = ch.pipeline();\n                               p.addlast(channelhandler1);\n                               p.addlast(channelhandler2);\n                               p.addlast(channelhandler3);\n                                    ........\n                 }\n             })\n\n\n以上这些由用户自定义的用于初始化 pipeline 的 channelinitializer ，被保存至 serverbootstrap 启动类中的 handler 字段中。用于后续的初始化调用\n\npublic abstract class abstractbootstrap<b extends abstractbootstrap<b, c>, c extends channel> implements cloneable\n   private volatile channelhandler handler;\n}\n\n\n在服务端启动的时候，会伴随着 nioservesocketchannel 的创建以及初始化，在初始化 nioserversokcetchannel 的时候会将一个新的 channelinitializer 添加进 pipeline 中，在新的 channelinitializer 中才会将用户自定义的 channelinitializer 添加进 pipeline 中，随后才执行初始化过程。\n\nnetty 这里之所以引入一个新的 channelinitializer 来初始化 nioserversocketchannel 中的 pipeline 的原因是需要兼容前边介绍的这两种初始化 pipeline 的方式。\n\n * 一种是直接使用一个具体的 channelhandler 来初始化 pipeline。\n * 另一种是使用 channelinitializer 来自定义初始化 pipeline 逻辑。\n\n> 忘记 netty 启动过程的同学可以在回看下笔者的《详细图解 netty reactor 启动全流程》这篇文章。\n\n   @override\n    void init(channel channel) {\n       .........\n\n        p.addlast(new channelinitializer<channel>() {\n            @override\n            public void initchannel(final channel ch) {\n                final channelpipeline pipeline = ch.pipeline();\n                //serverbootstrap中用户指定的channelhandler\n                channelhandler handler = config.handler();\n                if (handler != null) {\n                    pipeline.addlast(handler);\n                }\n\n                .........\n            }\n        });\n   }\n\n\n注意此时 nioserversocketchannel 并未开始向 main reactor 注册，根据本文第四小节《4. 向 pipeline 添加 channelhandler 》中的介绍，此时向 pipeline 中添加这个新的 channelinitializer 之后，netty 会向 pipeline 的任务列表中添加 pendinghandleraddedtask 。当 nioserversocketchannel 向 main reactor 注册成功之后，紧接着 main reactor 线程会调用这个 pendinghandleraddedtask ，在任务中会执行这个新的 channelinitializer 的 handleradded 回调。在这个回调方法中会执行上边 initchannel 方法里的代码。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)server channel pipeline 注册前结构.png\n\n当 nioserversocketchannel 在向 main reactor 注册成功之后，就挨个执行 pipeline 中的任务列表中的任务。\n\n       private void register0(channelpromise promise) {\n                     .........\n                boolean firstregistration = neverregistered;\n                //执行真正的注册操作\n                doregister();\n                //修改注册状态\n                neverregistered = false;\n                registered = true;\n                //调用pipeline中的任务链表，执行pendinghandleraddedtask\n                pipeline.invokehandleraddedifneeded();\n                .........\n    final void invokehandleraddedifneeded() {\n        assert channel.eventloop().ineventloop();\n        if (firstregistration) {\n            firstregistration = false;\n            // 执行 pipeline 任务列表中的 pendinghandleraddedtask 任务。\n            callhandleraddedforallhandlers();\n        }\n    }\n\n\n执行 pipeline 任务列表中的 pendinghandleraddedtask 任务：\n\n    private void callhandleraddedforallhandlers() {\n        // pipeline 任务列表中的头结点\n        final pendinghandlercallback pendinghandlercallbackhead;\n        synchronized (this) {\n            assert !registered;\n            // this channel itself was registered.\n            registered = true;\n            pendinghandlercallbackhead = this.pendinghandlercallbackhead;\n            // null out so it can be gc'ed.\n            this.pendinghandlercallbackhead = null;\n        }\n\n        pendinghandlercallback task = pendinghandlercallbackhead;\n        // 挨个执行任务列表中的任务\n        while (task != null) {\n            //触发 channelinitializer 的 handleradded 回调\n            task.execute();\n            task = task.next;\n        }\n    }\n\n\n最终在 pendinghandleraddedtask 中执行 pipeline 中 channelinitializer 的 handleradded 回调。\n\n> 这个 channelinitializer 就是在初始化 nioserversocketchannel 的 init 方法中向 pipeline 添加的 channelinitializer。\n\n@sharable\npublic abstract class channelinitializer<c extends channel> extends channelinboundhandleradapter {\n\n    @override\n    public void handleradded(channelhandlercontext ctx) throws exception {\n        if (ctx.channel().isregistered()) {\n\n            if (initchannel(ctx)) {\n                //初始化工作完成后，需要将自身从pipeline中移除\n                removestate(ctx);\n            }\n        }\n    }\n\n}\n\n\n在 handelradded 回调中执行 channelinitializer 匿名类中 initchannel 方法，注意此时执行的 channelinitializer 类为在本小节开头 init 方法中由 netty 框架添加的 channelinitializer ，并不是用户自定义的 channelinitializer 。\n\n    @override\n    void init(channel channel) {\n       .........\n\n        p.addlast(new channelinitializer<channel>() {\n            @override\n            public void initchannel(final channel ch) {\n                final channelpipeline pipeline = ch.pipeline();\n                //serverbootstrap中用户指定的channelinitializer\n                channelhandler handler = config.handler();\n                if (handler != null) {\n                    pipeline.addlast(handler);\n                }\n\n                .........\n            }\n        });\n   }\n\n\n执行完 channelinitializer 匿名类中 initchannel 方法后，需将 channelinitializer 从 pipeline 中删除。并回调 channelinitializer 的 handlerremoved 方法。删除过程笔者已经在第六小节《6. 从 pipeline 删除 channelhandler》详细介绍过了。\n\n    private boolean initchannel(channelhandlercontext ctx) throws exception {\n        if (initmap.add(ctx)) { // guard against re-entrance.\n            try {\n                //执行channelinitializer匿名类中的initchannel方法\n                initchannel((c) ctx.channel());\n            } catch (throwable cause) {\n                exceptioncaught(ctx, cause);\n            } finally {\n                channelpipeline pipeline = ctx.pipeline();\n                if (pipeline.context(this) != null) {\n                    //初始化完毕后，从pipeline中移除自身\n                    pipeline.remove(this);\n                }\n            }\n            return true;\n        }\n        return false;\n    }\n\n\n当执行完 initchannel 方法后此时 pipeline 的结构如下图所示：\n\nserversocketchannel注册之后pipeline结构.png\n\n当用户的自定义 channelinitializer 被添加进 pipeline 之后，根据第四小节所讲的添加逻辑，此时 nioserversocketchannel 已经向 main reactor 成功注册完毕，不再需要向 pipeine 的任务列表中添加 pendinghandleraddedtask 任务，而是直接调用自定义 channelinitializer 中的 handleradded 回调，和上面的逻辑一样。不同的是这里最终回调至用户自定义的初始化逻辑实现 initchannel 方法中。执行完用户自定义的初始化逻辑之后，从 pipeline 删除用户自定义的 channelinitializer 。\n\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)//配置主从reactor\n                  ...........\n               .handler(new channelinitializer<nioserversocketchannel>() {\n                 @override\n                 protected void initchannel(nioserversocketchannel ch) throws exception {\n                              ....自定义pipeline初始化逻辑....\n                               channelpipeline p = ch.pipeline();\n                               p.addlast(channelhandler1);\n                               p.addlast(channelhandler2);\n                               p.addlast(channelhandler3);\n                                    ........\n                 }\n             })\n\n\n随后 netty 会以异步任务的形式向 pipeline 的末尾添加 serverbootstrapacceptor ，至此 nioserversocketchannel 中 pipeline 的初始化工作就全部完成了。\n\n\n# 7.2 niosocketchannel 中 pipeline 的初始化\n\n在 7.1 小节中笔者举的这个 pipeline 初始化的例子相对来说比较复杂，当我们把这个复杂例子的初始化逻辑搞清楚之后，niosocketchannel 中 pipeline 的初始化过程就变的很简单了。\n\n            serverbootstrap b = new serverbootstrap();\n            b.group(bossgroup, workergroup)//配置主从reactor\n                  ...........\n               .childhandler(new channelinitializer<socketchannel>() {\n                 @override\n                 protected void initchannel(socketchannel ch) throws exception {\n                              ....自定义pipeline初始化逻辑....\n                               channelpipeline p = ch.pipeline();\n                               p.addlast(channelhandler1);\n                               p.addlast(channelhandler2);\n                               p.addlast(channelhandler3);\n                                    ........\n                 }\n             })\npublic class serverbootstrap extends abstractbootstrap<serverbootstrap, serverchannel> {\n    //保存用户自定义channelinitializer\n    private volatile channelhandler childhandler;\n}\n\n\n在《netty 如何高效接收网络连接》一文中我们介绍过，当客户端发起连接，完成三次握手之后，nioserversocketchannel 上的 op_accept 事件活跃，随后会在 nioserversocketchannel 的 pipeline 中触发 channelread 事件。并最终在 serverbootstrapacceptor 中初始化客户端 niosocketchannel 。\n\n主从reactor组完整结构.png\n\nprivate static class serverbootstrapacceptor extends channelinboundhandleradapter {\n\n       @override\n        @suppresswarnings(\"unchecked\")\n        public void channelread(channelhandlercontext ctx, object msg) {\n            final channel child = (channel) msg;\n            child.pipeline().addlast(childhandler);\n                    ...........\n       }\n}\n\n\n在这里会将用户自定义的 channelinitializer 添加进 niosocketchannel 中的 pipeline 中，由于此时 niosocketchannel 还没有向 sub reactor 开始注册。所以在向 pipeline 中添加 channelinitializer 的同时会伴随着 pendinghandleraddedtask 被添加进 pipeline 的任务列表中。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)niosocketchannel中pipeline的初始化.png\n\n后面的流程大家应该很熟悉了，和我们在7.1小节中介绍的一模一样，当 niosocketchannel 再向 sub reactor 注册成功之后，会执行 pipeline 中的任务列表中的 pendinghandleraddedtask 任务，在 pendinghandleraddedtask 任务中会回调用户自定义 channelinitializer 的 handelradded 方法，在该方法中执行 initchannel 方法，用户自定义的初始化逻辑就封装在这里面。在初始化完 pipeline 后，将 channelinitializer 从 pipeline 中删除，并回调其 handlerremoved 方法。\n\n至此客户端 niosocketchannel 中 pipeline 初始化工作就全部完成了。\n\n\n# 8. 事件传播\n\n在本文第三小节《3. pipeline中的事件分类》中我们介绍了 netty 事件类型共分为三大类，分别是 inbound类事件，outbound类事件，exceptioncaught事件。并详细介绍了这三类事件的掩码表示，和触发时机，以及事件传播的方向。\n\n本小节我们就来按照 netty 中异步事件的分类从源码角度分析下事件是如何在 pipeline 中进行传播的。\n\n\n# 8.1 inbound事件的传播\n\n在第三小节中我们介绍了所有的 inbound 类事件，这些事件在 pipeline 中的传播逻辑和传播方向都是一样的，唯一的区别就是执行的回调方法不同。\n\n本小节我们就以 channelread 事件的传播为例，来说明 inbound 类事件是如何在 pipeline 中进行传播的。\n\n第三小节中我们提到过，在 niosocketchannel 中，channelread 事件的触发时机是在每一次 read loop 读取数据之后在 pipeline 中触发的。\n\n               do {\n                          ............               \n                    allochandle.lastbytesread(doreadbytes(bytebuf));\n\n                          ............\n       \n                    // 在客户端niosocketchannel的pipeline中触发channelread事件\n                    pipeline.firechannelread(bytebuf);\n  \n                } while (allochandle.continuereading());\n\n\n从这里可以看到，任何 inbound 类事件在 pipeline 中的传播起点都是从 headcontext 头结点开始的。\n\npublic class defaultchannelpipeline implements channelpipeline {\n\n    @override\n    public final channelpipeline firechannelread(object msg) {\n        abstractchannelhandlercontext.invokechannelread(head, msg);\n        return this;\n    }\n    \n                    .........\n}\n\n\nchannelread 事件从 headcontext 开始在 pipeline 中传播，首先就会回调 headcontext 中的 channelread 方法。\n\n> 在执行 channelhandler 中的相应事件回调方法时，需要确保回调方法的执行在指定的 executor 中进行。\n\n    static void invokechannelread(final abstractchannelhandlercontext next, object msg) {\n        final object m = next.pipeline.touch(objectutil.checknotnull(msg, \"msg\"), next);\n        eventexecutor executor = next.executor();\n        //需要保证channelread事件回调在channelhandler指定的executor中进行\n        if (executor.ineventloop()) {\n            next.invokechannelread(m);\n        } else {\n            executor.execute(new runnable() {\n                @override\n                public void run() {\n                    next.invokechannelread(m);\n                }\n            });\n        }\n    }\n\n    private void invokechannelread(object msg) {\n        if (invokehandler()) {\n            try {\n                ((channelinboundhandler) handler()).channelread(this, msg);\n            } catch (throwable t) {\n                invokeexceptioncaught(t);\n            }\n        } else {\n            firechannelread(msg);\n        }\n    }\n\n\n在执行 headcontext 的 channelread 方法发生异常时，就会回调 headcontext 的 exceptioncaught 方法。并在相应的事件回调方法中决定是否将事件继续在 pipeline 中传播。\n\n    final class headcontext extends abstractchannelhandlercontext\n            implements channeloutboundhandler, channelinboundhandler {\n\n        @override\n        public void channelread(channelhandlercontext ctx, object msg) {\n            ctx.firechannelread(msg);\n        }\n\n       @override\n        public void exceptioncaught(channelhandlercontext ctx, throwable cause) {\n            ctx.fireexceptioncaught(cause);\n        }\n    }\n\n\n在 headcontext 中通过 ctx.firechannelread(msg) 继续将 channelread 事件在 pipeline 中向后传播。\n\nabstract class abstractchannelhandlercontext implements channelhandlercontext, resourceleakhint {\n\n    @override\n    public channelhandlercontext firechannelread(final object msg) {\n        invokechannelread(findcontextinbound(mask_channel_read), msg);\n        return this;\n    }\n\n}\n\n\n这里的 findcontextinbound 方法是整个 inbound 类事件在 pipeline 中传播的核心所在。\n\n因为我们现在需要继续将 channelread 事件在 pipeline 中传播，所以我们目前的核心问题就是通过 findcontextinbound 方法在 pipeline 中找到下一个对 channelread 事件感兴趣的 channelinboundhandler 。然后执行该 channelinboundhandler 的 channelread 事件回调。\n\n    static void invokechannelread(final abstractchannelhandlercontext next, object msg) {\n        final object m = next.pipeline.touch(objectutil.checknotnull(msg, \"msg\"), next);\n        eventexecutor executor = next.executor();\n        //需要保证channelread事件回调在channelhandler指定的executor中进行\n        if (executor.ineventloop()) {\n            next.invokechannelread(m);\n        } else {\n            executor.execute(new runnable() {\n                @override\n                public void run() {\n                    next.invokechannelread(m);\n                }\n            });\n        }\n    }\n\n\nchannelread 事件就这样循环往复的一直在 pipeline 中传播，在传播的过程中只有对 channelread 事件感兴趣的 channelinboundhandler 才可以响应。其他类型的 channelhandler 则直接跳过。\n\n如果 channelread 事件在 pipeline 中传播的过程中，没有得到其他 channelinboundhandler 的有效处理，最终会被传播到 pipeline 的末尾 tailcontext 中。而在本文第二小节中，我们也提到过 tailcontext 对于 inbound 事件存在的意义就是做一个兜底的处理。比如：打印日志，释放 bytebuffer 。\n\n final class tailcontext extends abstractchannelhandlercontext implements channelinboundhandler {\n\n    @override\n    public void channelread(channelhandlercontext ctx, object msg) {\n            onunhandledinboundmessage(ctx, msg);\n    }\n\n    protected void onunhandledinboundmessage(channelhandlercontext ctx, object msg) {\n        onunhandledinboundmessage(msg);\n        if (logger.isdebugenabled()) {\n            logger.debug(\"discarded message pipeline : {}. channel : {}.\",\n                         ctx.pipeline().names(), ctx.channel());\n        }\n    }\n\n    protected void onunhandledinboundmessage(object msg) {\n        try {\n            logger.debug(\n                    \"discarded inbound message {} that reached at the tail of the pipeline. \" +\n                            \"please check your pipeline configuration.\", msg);\n        } finally {\n            // 释放directbytebuffer\n            referencecountutil.release(msg);\n        }\n    }\n\n}\n\n\n\n# 8.2 findcontextinbound\n\n本小节要介绍的 findcontextinbound 方法和我们在上篇文章《一文聊透 netty 发送数据全流程》中介绍的 findcontextoutbound 方法均是 netty 异步事件在 pipeline 中传播的核心所在。\n\n事件传播的核心问题就是需要高效的在 pipeline 中按照事件的传播方向，找到下一个具有响应事件资格的 channelhandler 。\n\n比如：这里我们在 pipeline 中传播的 channelread 事件，我们就需要在 pipeline 中找到下一个对 channelread 事件感兴趣的 channelinboundhandler ，并执行该 channelinboudnhandler 的 channelread 事件回调，在 channelread 事件回调中对事件进行业务处理，并决定是否通过 ctx.firechannelread(msg) 将 channelread 事件继续向后传播。\n\n    private abstractchannelhandlercontext findcontextinbound(int mask) {\n        abstractchannelhandlercontext ctx = this;\n        eventexecutor currentexecutor = executor();\n        do {\n            ctx = ctx.next;\n        } while (skipcontext(ctx, currentexecutor, mask, mask_only_inbound));\n\n        return ctx;\n    }\n\n\n参数 mask 表示我们正在传播的 channelread 事件掩码 mask_channel_read 。\n\n    static final int mask_exception_caught = 1;\n    static final int mask_channel_registered = 1 << 1;\n    static final int mask_channel_unregistered = 1 << 2;\n    static final int mask_channel_active = 1 << 3;\n    static final int mask_channel_inactive = 1 << 4;\n    static final int mask_channel_read = 1 << 5;\n    static final int mask_channel_read_complete = 1 << 6;\n    static final int mask_user_event_triggered = 1 << 7;\n    static final int mask_channel_writability_changed = 1 << 8;\n\n\n通过 ctx = ctx.next 在 pipeline 中找到下一个 channelhandler ，并通过 skipcontext 方法判断下一个 channelhandler 是否具有响应事件的资格。如果没有则跳过继续向后查找。\n\n比如：下一个 channelhandler 如果是一个 channeloutboundhandler，或者下一个 channelinboundhandler 对 channelread 事件不感兴趣，那么就直接跳过。\n\n\n# 8.3 skipcontext\n\n该方法主要用来判断下一个 channelhandler 是否具有 mask 代表的事件的响应资格。\n\n    private static boolean skipcontext(\n            abstractchannelhandlercontext ctx, eventexecutor currentexecutor, int mask, int onlymask) {\n\n        return (ctx.executionmask & (onlymask | mask)) == 0 ||\n                (ctx.executor() == currentexecutor && (ctx.executionmask & mask) == 0);\n    }\n\n\n * 参数 onlymask 表示我们需要查找的 channelhandler 类型，比如这里我们正在传播 channelread 事件，它是一个 inbound 类事件，那么必须只能由 channelinboundhandler 来响应处理，所以这里传入的 onlymask 为 mask_only_inbound （ channelinboundhandler 的掩码表示）\n * ctx.executionmask 我们已经在《5.3 channehandlercontext》小节中详细介绍过了，当 channelhandler 被添加进 pipeline 中时，需要计算出该 channelhandler 感兴趣的事件集合掩码来，保存在对应 channelhandlercontext 的 executionmask 字段中。\n * 首先会通过 ctx.executionmask & (onlymask | mask)) == 0 来判断下一个 channelhandler 类型是否正确，比如我们正在传播 inbound 类事件，下一个却是一个 channeloutboundhandler ，那么肯定是要跳过的，继续向后查找。\n * 如果下一个 channelhandler 的类型正确，那么就会通过 (ctx.executionmask & mask) == 0 来判断该 channelhandler 是否对正在传播的 mask 事件感兴趣。如果该 channelhandler 中覆盖了 channelread 回调则执行，如果没有覆盖对应的事件回调方法则跳过，继续向后查找，直到 tailcontext 。\n\n以上就是 skipcontext 方法的核心逻辑，这里表达的核心语义是：\n\n * 如果 pipeline 中传播的是 inbound 类事件，则必须由 channelinboundhandler 来响应，并且该 channelhandler 必须覆盖实现对应的 inbound 事件回调。\n * 如果 pipeline 中传播的是 outbound 类事件，则必须由 channeloutboundhandler 来响应，并且该 channelhandler 必须覆盖实现对应的 outbound 事件回调。\n\n这里大部分同学可能会对 ctx.executor() == currentexecutor这个条件感到很疑惑。加上这个条件，其实对我们这里的核心语义并没有多大影响。\n\n * 当 ctx.executor() == currentexecutor 也就是说前后两个 channelhandler 指定的 executor 相同时，我们核心语义保持不变。\n * 当 ctx.executor() != currentexecutor也就是前后两个 channelhandler 指定的 executor 不同时，语义变为：只要前后两个 channelhandler 指定的 executor 不同，不管下一个channelhandler有没有覆盖实现指定事件的回调方法，均不能跳过。 在这种情况下会执行到 channelhandler 的默认事件回调方法，继续在 pipeline 中传递事件。我们在《5.3 channehandlercontext》小节提到过 channelinboundhandleradapter 和 channeloutboundhandleradapter 会分别对 inbound 类事件回调方法和 outbound 类事件回调方法进行默认的实现。\n\npublic class channeloutboundhandleradapter extends channelhandleradapter implements channeloutboundhandler {\n\n    @skip\n    @override\n    public void bind(channelhandlercontext ctx, socketaddress localaddress,\n            channelpromise promise) throws exception {\n        ctx.bind(localaddress, promise);\n    }\n\n    @skip\n    @override\n    public void connect(channelhandlercontext ctx, socketaddress remoteaddress,\n            socketaddress localaddress, channelpromise promise) throws exception {\n        ctx.connect(remoteaddress, localaddress, promise);\n    }\n\n    @skip\n    @override\n    public void disconnect(channelhandlercontext ctx, channelpromise promise)\n            throws exception {\n        ctx.disconnect(promise);\n    }\n\n    @skip\n    @override\n    public void close(channelhandlercontext ctx, channelpromise promise)\n            throws exception {\n        ctx.close(promise);\n    }\n\n    @skip\n    @override\n    public void deregister(channelhandlercontext ctx, channelpromise promise) throws exception {\n        ctx.deregister(promise);\n    }\n\n    @skip\n    @override\n    public void read(channelhandlercontext ctx) throws exception {\n        ctx.read();\n    }\n\n    @skip\n    @override\n    public void write(channelhandlercontext ctx, object msg, channelpromise promise) throws exception {\n        ctx.write(msg, promise);\n    }\n\n    @skip\n    @override\n    public void flush(channelhandlercontext ctx) throws exception {\n        ctx.flush();\n    }\n}\n\n\n而这里之所以需要加入 ctx.executor() == currentexecutor 条件的判断，是为了防止 httpcontentcompressor 在被指定不同的 executor 情况下无法正确的创建压缩内容，导致的一些异常。但这个不是本文的重点，大家只需要理解这里的核心语义就好，这种特殊情况的特殊处理了解一下就好。\n\n\n# 8.4 outbound事件的传播\n\n关于 outbound 类事件的传播，笔者在上篇文章《一文搞懂 netty 发送数据全流程》中已经进行了详细的介绍，本小节就不在赘述。\n\n\n# 8.5 exceptioncaught事件的传播\n\n在最后我们来介绍下异常事件在 pipeline 中的传播，exceptioncaught 事件和 inbound 类事件一样都是在 pipeline 中从前往后开始传播。\n\nexceptioncaught 事件的触发有两种情况：一种是 netty 框架内部产生的异常，这时 netty 会直接在 pipeline 中触发 exceptioncaught 事件的传播。异常事件会在 pipeline 中从 headcontext 开始一直向后传播直到 tailcontext。\n\n比如 netty 在 read loop 中读取数据时发生异常：\n\n     try {\n               ...........\n\n               do {\n                          ............               \n                    allochandle.lastbytesread(doreadbytes(bytebuf));\n\n                          ............\n       \n                    //客户端niosocketchannel的pipeline中触发channelread事件\n                    pipeline.firechannelread(bytebuf);\n  \n                } while (allochandle.continuereading());\n\n                         ...........\n        }  catch (throwable t) {\n                handlereadexception(pipeline, bytebuf, t, close, allochandle);\n       } \n\n\n这时会 netty 会直接从 pipeline 中触发 exceptioncaught 事件的传播。\n\n       private void handlereadexception(channelpipeline pipeline, bytebuf bytebuf, throwable cause, boolean close,\n                recvbytebufallocator.handle allochandle) {\n             \n                    .............\n\n            pipeline.fireexceptioncaught(cause);\n\n                    .............\n\n        }\n\n\n和 inbound 类事件一样，exceptioncaught 事件会在 pipeline 中从 headcontext 开始一直向后传播。\n\n    @override\n    public final channelpipeline fireexceptioncaught(throwable cause) {\n        abstractchannelhandlercontext.invokeexceptioncaught(head, cause);\n        return this;\n    }\n\n\n第二种触发 exceptioncaught 事件的情况是，当 inbound 类事件或者 flush 事件在 pipeline 中传播的过程中，在某个 channelhandler 中的事件回调方法处理中发生异常，这时该 channelhandler 的 exceptioncaught 方法会被回调。用户可以在这里处理异常事件，并决定是否通过 ctx.fireexceptioncaught(cause) 继续向后传播异常事件。\n\n比如我们在 channelinboundhandler 中的 channelread 回调中处理业务请求时发生异常，就会触发该 channelinboundhandler 的 exceptioncaught 方法。\n\n    private void invokechannelread(object msg) {\n        if (invokehandler()) {\n            try {\n                ((channelinboundhandler) handler()).channelread(this, msg);\n            } catch (throwable t) {\n                invokeexceptioncaught(t);\n            }\n        } else {\n            firechannelread(msg);\n        }\n    }\n    private void invokeexceptioncaught(final throwable cause) {\n        if (invokehandler()) {\n            try {\n                //触发channelhandler的exceptioncaught回调\n                handler().exceptioncaught(this, cause);\n            } catch (throwable error) {\n                  ........\n        } else {\n                  ........\n        }\n    }\n\n\n再比如：当我们在 channeloutboundhandler 中的 flush 回调中处理业务结果发送的时候发生异常，也会触发该 channeloutboundhandler 的 exceptioncaught 方法。\n\n   private void invokeflush0() {\n        try {\n            ((channeloutboundhandler) handler()).flush(this);\n        } catch (throwable t) {\n            invokeexceptioncaught(t);\n        }\n    }\n\n\n我们可以在 channelhandler 的 exceptioncaught 回调中进行异常处理，并决定是否通过 ctx.fireexceptioncaught(cause) 继续向后传播异常事件。\n\n    @override\n    public void exceptioncaught(channelhandlercontext ctx, throwable cause)\n            throws exception {\n\n        .........异常处理.......\n\n        ctx.fireexceptioncaught(cause);\n    }\n    @override\n    public channelhandlercontext fireexceptioncaught(final throwable cause) {\n        invokeexceptioncaught(findcontextinbound(mask_exception_caught), cause);\n        return this;\n    }\n\n   static void invokeexceptioncaught(final abstractchannelhandlercontext next, final throwable cause) {\n        objectutil.checknotnull(cause, \"cause\");\n        eventexecutor executor = next.executor();\n        if (executor.ineventloop()) {\n            next.invokeexceptioncaught(cause);\n        } else {\n            try {\n                executor.execute(new runnable() {\n                    @override\n                    public void run() {\n                        next.invokeexceptioncaught(cause);\n                    }\n                });\n            } catch (throwable t) {\n                if (logger.iswarnenabled()) {\n                    logger.warn(\"failed to submit an exceptioncaught() event.\", t);\n                    logger.warn(\"the exceptioncaught() event that was failed to submit was:\", cause);\n                }\n            }\n        }\n    }\n\n\n\n# 8.6 exceptioncaught 事件和 inbound 类事件的区别\n\n虽然 exceptioncaught 事件和 inbound 类事件在传播方向都是在 pipeline 中从前向后传播。但是大家这里注意区分这两个事件的区别。\n\n在 inbound 类事件传播过程中是会查找下一个具有事件响应资格的 channelinboundhandler 。遇到 channeloutboundhandler 会直接跳过。\n\n而 exceptioncaught 事件无论是在哪种类型的 channelhandler 中触发的，都会从当前异常 channelhandler 开始一直向后传播，channelinboundhandler 可以响应该异常事件，channeloutboundhandler 也可以响应该异常事件。\n\n由于无论异常是在 channelinboundhandler 中产生的还是在 channeloutboundhandler 中产生的， exceptioncaught 事件都会在 pipeline 中是从前向后传播，并且不关心 channelhandler 的类型。所以我们一般将负责统一异常处理的 channelhandler 放在 pipeline 的最后，这样它对于 inbound 类异常和 outbound 类异常均可以捕获得到。\n\n异常事件的传播.png\n\n----------------------------------------\n\n\n# 总结\n\n本文涉及到的内容比较多，通过 netty 异步事件在 pipeline 中的编排和传播这条主线，我们相当于将之前的文章内容重新又回顾总结了一遍。\n\n本文中我们详细介绍了 pipeline 的组成结构，它主要是由 channelhandlercontext 类型节点组成的双向链表。channelhandlercontext 包含了 channelhandler 执行上下文的信息，从而可以使 channelhandler 只关注于 io 事件的处理，遵循了单一原则和开闭原则。\n\n此外 pipeline 结构中还包含了一个任务链表，用来存放执行 channelhandler 中的 handleradded 回调和 handlerremoved 回调。pipeline 还持有了所属 channel 的引用。\n\n我们还详细介绍了 netty 中异步事件的分类：inbound 类事件，outbound 类事件，exceptioncaught 事件。并详细介绍了每种分类下的所有事件的触发时机和在 pipeline 中的传播路径。\n\n最后介绍了 pipeline 的结构以及创建和初始化过程，以及对 pipeline 相关操作的源码实现。\n\n中间我们又穿插介绍了 channelhandercontext 的结构，介绍了 channelhandlercontext 具体封装了哪些关于 channelhandler 执行的上下文信息。\n\n本文的内容到这里就结束了，感谢大家的观看，我们下篇文章见~~~",charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"第一课：ByteBuf 体系的设计与实现",frontmatter:{title:"第一课：ByteBuf 体系的设计与实现",date:"2024-09-19T11:20:49.000Z",permalink:"/pages/8be98a/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/30.%E4%BA%94%E3%80%81%E6%B7%B1%E5%85%A5%20Netty%20%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/05.%E7%AC%AC%E4%B8%80%E8%AF%BE%EF%BC%9AByteBuf%20%E4%BD%93%E7%B3%BB%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Netty 系统设计/30.五、深入 Netty 内存管理/05.第一课：ByteBuf 体系的设计与实现.md",key:"v-4a3c8296",path:"/pages/8be98a/",headers:[{level:2,title:"1. JDK 中的 ByteBuffer 设计有何不妥",slug:"_1-jdk-中的-bytebuffer-设计有何不妥",normalizedTitle:"1. jdk 中的 bytebuffer 设计有何不妥",charIndex:2278},{level:2,title:"2. Netty  对于 ByteBuf 的设计与实现",slug:"_2-netty-对于-bytebuf-的设计与实现",normalizedTitle:"2. netty  对于 bytebuf 的设计与实现",charIndex:null},{level:3,title:"2.1 ByteBuf 的基本结构",slug:"_2-1-bytebuf-的基本结构",normalizedTitle:"2.1 bytebuf 的基本结构",charIndex:6431},{level:3,title:"2.2 ByteBuf 的读取操作",slug:"_2-2-bytebuf-的读取操作",normalizedTitle:"2.2 bytebuf 的读取操作",charIndex:12092},{level:3,title:"2.3 discardReadBytes",slug:"_2-3-discardreadbytes",normalizedTitle:"2.3 discardreadbytes",charIndex:19425},{level:3,title:"2.4 ByteBuf 的写入操作",slug:"_2-4-bytebuf-的写入操作",normalizedTitle:"2.4 bytebuf 的写入操作",charIndex:25307},{level:3,title:"2.5 ByteBuf 的扩容机制",slug:"_2-5-bytebuf-的扩容机制",normalizedTitle:"2.5 bytebuf 的扩容机制",charIndex:31406},{level:4,title:"2.5.1 newCapacity 的计算逻辑",slug:"_2-5-1-newcapacity-的计算逻辑",normalizedTitle:"2.5.1 newcapacity 的计算逻辑",charIndex:33525},{level:4,title:"2.5.2 ByteBuf 的扩容逻辑",slug:"_2-5-2-bytebuf-的扩容逻辑",normalizedTitle:"2.5.2 bytebuf 的扩容逻辑",charIndex:37319},{level:4,title:"2.5.3 强制扩容",slug:"_2-5-3-强制扩容",normalizedTitle:"2.5.3 强制扩容",charIndex:39669},{level:4,title:"2.5.4 自适应动态扩容",slug:"_2-5-4-自适应动态扩容",normalizedTitle:"2.5.4 自适应动态扩容",charIndex:41684},{level:3,title:"2.6 ByteBuf 的引用计数设计",slug:"_2-6-bytebuf-的引用计数设计",normalizedTitle:"2.6 bytebuf 的引用计数设计",charIndex:46495},{level:4,title:"2.6.1 为什么要引入引用计数",slug:"_2-6-1-为什么要引入引用计数",normalizedTitle:"2.6.1 为什么要引入引用计数",charIndex:47823},{level:4,title:"2.6.2 引用计数的最初设计",slug:"_2-6-2-引用计数的最初设计",normalizedTitle:"2.6.2 引用计数的最初设计",charIndex:49215},{level:4,title:"2.6.3 引入指令级别上的优化",slug:"_2-6-3-引入指令级别上的优化",normalizedTitle:"2.6.3 引入指令级别上的优化",charIndex:51422},{level:4,title:"2.6.4 并发安全问题的引入",slug:"_2-6-4-并发安全问题的引入",normalizedTitle:"2.6.4 并发安全问题的引入",charIndex:53788},{level:4,title:"2.6.5 在性能与并发安全之间的权衡",slug:"_2-6-5-在性能与并发安全之间的权衡",normalizedTitle:"2.6.5 在性能与并发安全之间的权衡",charIndex:56543},{level:4,title:"2.6.6 奇偶设计的引入",slug:"_2-6-6-奇偶设计的引入",normalizedTitle:"2.6.6 奇偶设计的引入",charIndex:58439},{level:4,title:"2.6.7 尽量避免内存屏障的开销",slug:"_2-6-7-尽量避免内存屏障的开销",normalizedTitle:"2.6.7 尽量避免内存屏障的开销",charIndex:68835},{level:3,title:"2.7 ByteBuf 的视图设计",slug:"_2-7-bytebuf-的视图设计",normalizedTitle:"2.7 bytebuf 的视图设计",charIndex:72679},{level:3,title:"2.8 CompositeByteBuf 的零拷贝设计",slug:"_2-8-compositebytebuf-的零拷贝设计",normalizedTitle:"2.8 compositebytebuf 的零拷贝设计",charIndex:80813},{level:4,title:"2.8.1 CompositeByteBuf 的总体架构",slug:"_2-8-1-compositebytebuf-的总体架构",normalizedTitle:"2.8.1 compositebytebuf 的总体架构",charIndex:81788},{level:4,title:"2.8.2 CompositeByteBuf 的创建",slug:"_2-8-2-compositebytebuf-的创建",normalizedTitle:"2.8.2 compositebytebuf 的创建",charIndex:91348},{level:4,title:"2.8.3 shiftComps 为新的 ByteBuf 腾挪空间",slug:"_2-8-3-shiftcomps-为新的-bytebuf-腾挪空间",normalizedTitle:"2.8.3 shiftcomps 为新的 bytebuf 腾挪空间",charIndex:94193},{level:4,title:"2.8.4 Component 如何封装 ByteBuf",slug:"_2-8-4-component-如何封装-bytebuf",normalizedTitle:"2.8.4 component 如何封装 bytebuf",charIndex:99101},{level:4,title:"2.8.5 addComponents0",slug:"_2-8-5-addcomponents0",normalizedTitle:"2.8.5 addcomponents0",charIndex:105076},{level:4,title:"2.8.6 consolidateIfNeeded",slug:"_2-8-6-consolidateifneeded",normalizedTitle:"2.8.6 consolidateifneeded",charIndex:111916},{level:4,title:"2.8.7 CompositeByteBuf 的应用",slug:"_2-8-7-compositebytebuf-的应用",normalizedTitle:"2.8.7 compositebytebuf 的应用",charIndex:114041},{level:2,title:"3. Heap or Direct",slug:"_3-heap-or-direct",normalizedTitle:"3. heap or direct",charIndex:118466},{level:2,title:"4. Cleaner or NoCleaner",slug:"_4-cleaner-or-nocleaner",normalizedTitle:"4. cleaner or nocleaner",charIndex:121877},{level:2,title:"5. Unsafe or NoUnsafe",slug:"_5-unsafe-or-nounsafe",normalizedTitle:"5. unsafe or nounsafe",charIndex:130931},{level:2,title:"6. Pooled or Unpooled",slug:"_6-pooled-or-unpooled",normalizedTitle:"6. pooled or unpooled",charIndex:137555},{level:2,title:"7. Metric",slug:"_7-metric",normalizedTitle:"7. metric",charIndex:140974},{level:2,title:"8. ByteBufAllocator",slug:"_8-bytebufallocator",normalizedTitle:"8. bytebufallocator",charIndex:147794},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:72559}],headersStr:"1. JDK 中的 ByteBuffer 设计有何不妥 2. Netty  对于 ByteBuf 的设计与实现 2.1 ByteBuf 的基本结构 2.2 ByteBuf 的读取操作 2.3 discardReadBytes 2.4 ByteBuf 的写入操作 2.5 ByteBuf 的扩容机制 2.5.1 newCapacity 的计算逻辑 2.5.2 ByteBuf 的扩容逻辑 2.5.3 强制扩容 2.5.4 自适应动态扩容 2.6 ByteBuf 的引用计数设计 2.6.1 为什么要引入引用计数 2.6.2 引用计数的最初设计 2.6.3 引入指令级别上的优化 2.6.4 并发安全问题的引入 2.6.5 在性能与并发安全之间的权衡 2.6.6 奇偶设计的引入 2.6.7 尽量避免内存屏障的开销 2.7 ByteBuf 的视图设计 2.8 CompositeByteBuf 的零拷贝设计 2.8.1 CompositeByteBuf 的总体架构 2.8.2 CompositeByteBuf 的创建 2.8.3 shiftComps 为新的 ByteBuf 腾挪空间 2.8.4 Component 如何封装 ByteBuf 2.8.5 addComponents0 2.8.6 consolidateIfNeeded 2.8.7 CompositeByteBuf 的应用 3. Heap or Direct 4. Cleaner or NoCleaner 5. Unsafe or NoUnsafe 6. Pooled or Unpooled 7. Metric 8. ByteBufAllocator 总结",content:"时光芿苒，岁月如梭，好久没有给大家更新 Netty 相关的文章了，在断更 Netty 的这段日子里，笔者一直在持续更新 Linux 内存管理相关的文章 ，目前为止，算是将 Linux 内存管理子系统相关的主干源码较为完整的给大家呈现了出来，同时也结识了很多喜欢内核的读者，经常在后台留言讨论一些代码的设计细节，在这个过程中，我们相互分享，相互学习，浓浓的感受到了大家对技术那份纯粹的热爱，对于我自己来说，也是一种激励，学习，提高的机会。\n\n之前系列文章的视角一直是停留在内核态，笔者试图从 Linux 内核的角度来为大家揭秘内存管理的本质，那么从今天开始，我们把视角在往上挪一挪，从内核态转换到用户态，继续沿着内存管理这条主线，来看一看用户态的内存管理是如何进行的。\n\n接下来笔者计划用三篇文章的篇幅为大家剖析一下 Netty 的内存管理模块，本文是第一篇，主要是围绕 Netty 内存管理的外围介绍一下 ByteBuf 的总体设计。\n\n\n\n别看 ByteBuf 体系涉及到的类比较多，一眼望过去比较头大，但是我们按照不同的视角，将它们一一分类，整个体系脉络就变得很清晰了：\n\n * 从 JVM 内存区域布局的角度来看，Netty 的 ByteBuf 主要分为 HeapByteBuf（堆内） 和 DirectByteBuf（堆外）这两种类型。\n * 从内存管理的角度来看，Netty 的 ByteBuf 又分为 PooledByteBuf （池化）和 UnpooledByteBuf（非池化）两种子类型。一种是被内存池统一管理，另一种则和普通的 ByteBuf 一样，用的时候临时创建，不用的时候释放。\n * 从内存访问的角度来看，Netty 又将 ByteBuf 分为了 UnsafeByteBuf 和普通的 ByteBuf。UnsafeByteBuf 主要是依赖 Unsafe 类提供的底层 API 来直接对内存地址进行操作。而普通 ByteBuf 对内存的操作主要是依赖 NIO 中的 ByteBuffer。\n * 从内存回收的角度来看，ByteBuf 又分为了带 Cleaner 的 ByteBuf 以及不带 Cleaner 的 NoCleanerByteBuf，Cleaner 在 JDK 中是用来释放 NIO ByteBuffer 背后所引用的 Native Memory 的，内存的释放由 JVM 统一管理。而 NoCleanerByteBuf 背后的 Native Memory 则需要我们进行手动释放。\n * 从内存占用统计的角度来说，Netty 又近一步将 ByteBuf 分为了 InstrumentedByteBuf 和普通的 ByteBuf，其中 InstrumentedByteBuf 会带有内存占用相关 Metrics 的统计供我们进行监控，而普通的 ByteBuf 则不带有热任何 Metrics。\n * 从零拷贝的角度来看，Netty 又引入了 CompositeByteBuf，目的是为多个 ByteBuf 在聚合的时候提供一个统一的逻辑视图，将多个 ByteBuf 聚合成一个逻辑上的 CompositeByteBuf，而传统的聚合操作则是首先要分配一个大的 ByteBuf，然后将需要聚合的多个 ByteBuf 中的内容在拷贝到新的 ByteBuf 中。CompositeByteBuf 避免了分配大段内存以及内存拷贝的开销。注意这里的零拷贝指的是 Netty 在用户态层面自己实现的避免内存拷贝的设计，而不是 OS 层面上的零拷贝。\n * 另外 Netty 的 ByteBuf 支持引用计数以及自动地内存泄露探测，如果有内存泄露的情况，Netty 会将具体发生泄露的位置报告出来。\n * Netty 的 ByteBuf 支持扩容，而 NIO 的 ByteBuffer 则不支持扩容，\n\n在将 Netty 的 ByteBuf 设计体系梳理完整之后，我们就会发现，Netty 的 ByteBuf 其实是对 JDK ByteBuffer 的一种扩展和完善，所以下面笔者的行文思路是与 JDK ByteBuffer 对比着进行介绍 Netty 的 ByteBuf ，有了对比，我们才能更加深刻的体会到 Netty 设计的精妙。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)本文概要.png\n\n\n# 1. JDK 中的 ByteBuffer 设计有何不妥\n\n笔者曾在 《一步一图带你深入剖析 JDK NIO ByteBuffer 在不同字节序下的设计与实现》 一文中完整的介绍过 JDK ByteBuffer 的整个设计体系，下面我们来简短回忆一下 ByteBuffer 的几个核心要素。\n\npublic abstract class Buffer {\n    private int mark = -1;\n    private int position = 0;\n    private int limit;\n    private int capacity;\n}\n\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n * capacity 规定了整个 Buffer 的容量，具体可以容纳多少个元素。capacity 之前的元素均是 Buffer 可操作的空间，JDK 中的 ByteBuffer 是不可扩容的。\n * position 用于指向 Buffer 中下一个可操作性的元素，初始值为 0。对于 Buffer 的读写操作全部都共用这一个 position 指针，在 Buffer 的写模式下，position 指针用于指向下一个可写位置。在读模式下，position 指针指向下一个可读位置。\n * limit 用于限定 Buffer 可操作元素的上限，position 指针不能超过 limit。\n\n由于 JDK ByteBuffer 只设计了一个 position 指针，所以我们在读写 ByteBuffer 的时候需要不断的调整 position 的位置。比如，利用 flip() ，rewind()，compact()，clear() 等方法不断的进行读写模式的切换。\n\n一些具体的场景体现就是，当我们对一个 ByteBuffer 进行写入的时候，随着数据不断的向 ByteBuffer 写入，position 指针会不断的向后移动。在写入操作完成之后，如果我们想要从 ByteBuffer 读取刚刚写入的数据就麻烦了。\n\n由于 JDK 在对 ByteBuffer 的设计中读写操作都是混用一个 position 指针，所以在读取 ByteBuffer 之前，我们还需要通过 flip() 调整 position 的位置，进行读模式的切换。\n\nflip切换读模式.png\n\n    public final Buffer flip() {\n        limit = position;\n        position = 0;\n        mark = -1;\n        return this;\n    }\n\n\n当我们将 ByteBuffer 中的数据全部读取完之后，如果再次向 ByteBuffer 写入数据，那么还需要重新调整 position 的位置，通过 clear() 来进行写模式的切换。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)clear切换写模式.png\n\n    public final Buffer clear() {\n        position = 0;\n        limit = capacity;\n        mark = -1;\n        return this;\n    }\n\n\n如果我们只是部分读取了 ByteBuffer 中的数据而不是全部读取，那么在写入的时候，为了避免未被读取的部分被接下来的写入操作覆盖，我们则需要通过 compact() 方法来切换写模式。\n\ncomapct切换写模式.png\n\nclass HeapByteBuffer extends ByteBuffer {\n\n    //HeapBuffer中底层负责存储数据的数组\n    final byte[] hb; \n\n    public ByteBuffer compact() {\n        System.arraycopy(hb, ix(position()), hb, ix(0), remaining());\n        position(remaining());\n        limit(capacity());\n        discardMark();\n        return this;\n    }\n\n    public final int remaining() {\n        return limit - position;\n    }\n\n   final void discardMark() {                          \n        mark = -1;\n    }\n}\n\n\n从上面列举的这些读写 ByteBuffer 场景可以看出，当我们在操作 ByteBuffer 的时候，需要时刻保持头脑清醒，对 ByteBuffer 中哪些部分是可读的，哪些部分是可写的要有一个清醒的认识，稍不留神就会出错。在复杂的编解码逻辑中，如果使用 ByteBuffer 的话，就需要不断的进行读写模式的切换，切的切的人就傻了。\n\n除了对 ByteBuffer 的相关操作比较麻烦之外，JDK 对于 ByteBuffer 没有设计池化管理机制，而面对大量需要使用堆外内存的场景，我们就需要不断的创建 DirectBuffer，DirectBuffer 在使用完之后，回收又是个问题。\n\nJDK 自身对于 DirectBuffer 的回收是有延迟的，我们需要等到一次 FullGc ，这些 DirectBuffer 背后引用的 Native Memory 才能被 JVM 自动回收。所以为了及时回收这些 Native Memory ，我们又需要操心 DirectBuffer 的手动释放。\n\nJDK 的 ByteBuffer 不支持引用计数，没有引用计数的设计，我们就无从得知一个 DirectBuffer 被引用了多少次，又被释放了多少次，面对 DirectBuffer 引起的内存泄露问题，也就无法进行自动探测。\n\n另外 JDK 的 ByteBuffer 不支持动态按需自适应扩容，当一个 ByteBuffer 被创建出来之后，它的容量就固定了。但实际上，我们很难在一开始就能准确的评估出到底需要多大的 ByteBuffer。分配的容量大了，会造成浪费。分配的容量小了，我们又需要每次在写入的时候判断剩余容量是否足够，如果不足，又需要手动去申请一个更大的 ByteBuffer，然后在将原有 ByteBuffer 中的数据迁移到新的 ByteBuffer 中，想想都麻烦。\n\n还有就是当多个 JDK 的 ByteBuffer 在面对合并聚合的场景，总是要先创建一个更大的 ByteBuffer，然后将原有的多个 ByteBuffer 中的内容在拷贝到新的 ByteBuffer 中。这就涉及到了内存分配和拷贝的开销。\n\n那为什么不能利用原有的这些 ByteBuffer 所占用的内存空间，在此基础上只创建一个逻辑上的视图 ByteBuffer，将对视图 ByteBuffer 的逻辑操作全部转移到原有的内存空间上，这样一来不就可以省去重新分配内存以及内存拷贝的开销了么 ？\n\n下面我们就来一起看下，Netty 中的 ByteBuf 是如何解决并完善上述问题的~~~\n\n\n# 2. Netty 对于 ByteBuf 的设计与实现\n\n在之前介绍 JDK ByteBuffer 整体设计的时候，笔者是以 HeapByteBuffer 为例将 ByteBuffer 的整个设计体系串联起来的，那么本文笔者将会用 DirectByteBuf 为大家串联 Netty ByteBuf 的设计体系。\n\n\n# 2.1 ByteBuf 的基本结构\n\npublic abstract class AbstractByteBuf extends ByteBuf {\n    int readerIndex;\n    int writerIndex;\n    private int markedReaderIndex;\n    private int markedWriterIndex;\n    private int maxCapacity;\n}\n\npublic class UnpooledDirectByteBuf extends AbstractReferenceCountedByteBuf {\n    private int capacity;\n}\n\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n为了避免 JDK ByteBuffer 在读写模式下共用一个 position 指针所引起的繁琐操作，Netty 为 ByteBuf 引入了两个指针，readerIndex 用于指向 ByteBuf 中第一个可读字节位置，writerIndex 用于指向 ByteBuf 中第一个可写的字节位置。有了这两个独立的指针之后，我们在对 Netty ByteBuf 进行读写操作的时候，就不需要进行繁琐的读写模式切换了。与之对应的 markedReaderIndex，markedWriterIndex 用于支持 ByteBuf 相关的 mark 和 reset 操作，这一点和 JDK 中的设计保持一致。\n\n    @Override\n    public ByteBuf markReaderIndex() {\n        markedReaderIndex = readerIndex;\n        return this;\n    }\n\n    @Override\n    public ByteBuf resetReaderIndex() {\n        readerIndex(markedReaderIndex);\n        return this;\n    }\n\n    @Override\n    public ByteBuf markWriterIndex() {\n        markedWriterIndex = writerIndex;\n        return this;\n    }\n\n    @Override\n    public ByteBuf resetWriterIndex() {\n        writerIndex(markedWriterIndex);\n        return this;\n    }\n\n\n由于 JDK ByteBuffer 在设计上不支持扩容机制，所以 Netty 为 ByteBuf 额外引入了一个新的字段 maxCapacity，用于表示 ByteBuf 容量最多只能扩容至 maxCapacity。\n\n    @Override\n    public int calculateNewCapacity(int minNewCapacity, int maxCapacity) {\n        if (minNewCapacity > maxCapacity) {\n            throw new IllegalArgumentException(String.format(\n                    \"minNewCapacity: %d (expected: not greater than maxCapacity(%d)\",\n                    minNewCapacity, maxCapacity));\n        }\n    }\n\n\nNetty ByteBuf 的 capacity 与 JDK ByteBuffer 中的 capacity 含义保持一致，用于表示 ByteBuf 的初始容量大小，也就是下面在创建 UnpooledDirectByteBuf 的时候传入的 initialCapacity 参数。\n\npublic class UnpooledDirectByteBuf extends AbstractReferenceCountedByteBuf {\n      // Netty ByteBuf 底层依赖的 JDK ByteBuffer\n      ByteBuffer buffer;\n      // ByteBuf 初始的容量，也是真正的内存占用\n      private int capacity;\n\n      public UnpooledDirectByteBuf(ByteBufAllocator alloc, int initialCapacity, int maxCapacity) {\n        // 设置最大可扩容的容量\n        super(maxCapacity);\n        this.alloc = alloc;\n        // 按照 initialCapacity 指定的初始容量，创建 JDK ByteBuffer\n        setByteBuffer(allocateDirect(initialCapacity), false);\n    }\n\n    void setByteBuffer(ByteBuffer buffer, boolean tryFree) {\n        // UnpooledDirectByteBuf 底层会依赖一个 JDK 的 ByteBuffer\n        // 后续对 UnpooledDirectByteBuf 的操作， Netty 全部会代理到 JDK ByteBuffer 中\n        this.buffer = buffer;\n        // 初始指定的 ByteBuf 容量 initialCapacity\n        capacity = buffer.remaining();    \n    }\n}\n\n\n由此一来，Netty 中的 ByteBuf 就会被 readerIndex，writerIndex，capacity，maxCapacity 这四个指针分割成四个部分，上图中笔者以按照不同的颜色进行了区分。\n\n * 其中 [0 , capacity) 这部分是创建 ByteBuf 的时候分配的初始容量，这部分是真正占用内存的，而 [capacity , maxCapacity)这部分表示 ByteBuf 可扩容的容量，这部分还未分配内存。\n * [0 , readerIndex) 这部分字节是已经被读取过的字节，是可以被丢弃的范围。\n * [readerIndex , writerIndex) 这部分字节表示 ByteBuf 中可以被读取的字节。\n * [writerIndex , capacity) 这部分表示 ByteBuf 的剩余容量，也就是可以写入的字节范围。\n\n这四个指针他们之间的关系为 ：0 <= readerIndex <= writerIndex <= capacity <= maxCapacity。\n\n  private static void checkIndexBounds(final int readerIndex, final int writerIndex, final int capacity) {\n        if (readerIndex < 0 || readerIndex > writerIndex || writerIndex > capacity) {\n            throw new IndexOutOfBoundsException(String.format(\n                    \"readerIndex: %d, writerIndex: %d (expected: 0 <= readerIndex <= writerIndex <= capacity(%d))\",\n                    readerIndex, writerIndex, capacity));\n        }\n    }\n\n\n当我们对 ByteBuf 进行读取操作的时候，需要通过 isReadable 判断 ByteBuf 是否可读。以及通过 readableBytes 判断 ByteBuf 具体还有多少字节可读。当 readerIndex 等于 writerIndex 的时候，ByteBuf 就不可读了。 [0 , readerIndex) 这部分字节就可以被丢弃了。\n\n    @Override\n    public boolean isReadable() {\n        return writerIndex > readerIndex;\n    }\n\n    @Override\n    public int readableBytes() {\n        return writerIndex - readerIndex;\n    }\n\n\n当我们对 ByteBuf 进行写入操作的时候，需要通过 isWritable 判断 ByteBuf 是否可写。以及通过 writableBytes 判断 ByteBuf 具体还可以写多少字节。当 writerIndex 等于 capacity 的时候，ByteBuf 就不可写了。\n\n   @Override\n    public boolean isWritable() {\n        return capacity() > writerIndex;\n    }\n\n    @Override\n    public int writableBytes() {\n        return capacity() - writerIndex;\n    }\n\n\n当 ByteBuf 的容量已经被写满，变为不可写的时候，如果继续对 ByteBuf 进行写入，那么就需要扩容了，但扩容后的 capacity 最大不能超过 maxCapacity。\n\n    final void ensureWritable0(int minWritableBytes) {\n        // minWritableBytes 表示本次要写入的字节数\n        // 获取当前 writerIndex 的位置\n        final int writerIndex = writerIndex();\n        // 为满足本次的写入操作，预期的 ByteBuf 容量大小\n        final int targetCapacity = writerIndex + minWritableBytes;\n        // 如果 targetCapacity 在（capacity , maxCapacity] 之间，则进行扩容\n        if (targetCapacity >= 0 & targetCapacity <= capacity()) {\n            // targetCapacity 在 [0 , capacity] 之间，则无需扩容，本来就可以满足\n            return;\n        }\n        // 扩容后的 capacity 最大不能超过 maxCapacity\n        if (checkBounds && (targetCapacity < 0 || targetCapacity > maxCapacity)) {\n            throw new IndexOutOfBoundsException(String.format(\n                    \"writerIndex(%d) + minWritableBytes(%d) exceeds maxCapacity(%d): %s\",\n                    writerIndex, minWritableBytes, maxCapacity, this));\n        }\n\n        ..... 扩容 ByteBuf ......\n    }\n\n\n\n# 2.2 ByteBuf 的读取操作\n\n明白了 ByteBuf 基本结构之后，我们来看一下针对 ByteBuf 的读写等基本操作是如何进行的。Netty 支持以多种基本类型为粒度对 ByteBuf 进行读写，除此之外还支持 Unsigned 基本类型的转换以及大小端的转换。下面笔者以 Byte 和 Int 这两种基本类型为例对 ByteBuf 的读取操作进行说明。\n\nByteBuf 中的 get 方法只是单纯地从 ByteBuf 中读取数据，并不改变其 readerIndex 的位置，我们可以通过 getByte 从 ByteBuf 中的指定位置 index 读取一个 Byte 出来，也可以通过 getUnsignedByte 从 ByteBuf 读取一个 Byte 并转换成 UnsignedByte 。\n\npublic abstract class AbstractByteBuf extends ByteBuf {\n    @Override\n    public byte getByte(int index) {\n        // 检查 index 的边界，index 不能超过 capacity（index < capacity）\n        checkIndex(index);\n        return _getByte(index);\n    }\n\n    @Override\n    public short getUnsignedByte(int index) {\n        // 将获取到的 Byte 转换为 UnsignedByte\n        return (short) (getByte(index) & 0xFF);\n    }   \n\n    protected abstract byte _getByte(int index);\n}\n\n\n其底层依赖的是一个抽象方法 _getByte，由 AbstractByteBuf 具体的子类负责实现。比如，在 UnpooledDirectByteBuf 类的实现中，直接将 _getByte 操作代理给其底层依赖的 JDK DirectByteBuffer。\n\npublic class UnpooledDirectByteBuf  {\n    // 底层依赖 JDK 的 DirectByteBuffer\n    ByteBuffer buffer;\n\n    @Override\n    protected byte _getByte(int index) {\n        return buffer.get(index);\n    }\n}\n\n\n而在 UnpooledUnsafeDirectByteBuf 类的实现中，则是通过 sun.misc.Unsafe 直接从对应的内存地址中读取。\n\npublic class UnpooledUnsafeDirectByteBuf {\n    // 直接操作 OS 的内存地址\n    long memoryAddress;\n    @Override\n    protected byte _getByte(int index) {\n        // 底层依赖 PlatformDependent0，直接通过内存地址读取 byte\n        return UnsafeByteBufUtil.getByte(addr(index));\n    }\n\n    final long addr(int index) {\n        // 获取偏移 index 对应的内存地址\n        return memoryAddress + index;\n    }\n}\n\nfinal class PlatformDependent0 {\n  // sun.misc.Unsafe\n  static final Unsafe UNSAFE;\n  static byte getByte(long address) {\n        return UNSAFE.getByte(address);\n    }\n}\n\n\nNetty 另外还提供了批量读取 Bytes 的操作，比如我们可以通过 getBytes 方法将 ByteBuf 中的数据读取到一个字节数组 byte[] 中，也可以读取到另一个 ByteBuf 中。\n\n    @Override\n    public ByteBuf getBytes(int index, byte[] dst) {\n        getBytes(index, dst, 0, dst.length);\n        return this;\n    }\n\n    public abstract ByteBuf getBytes(int index, byte[] dst, int dstIndex, int length);\n\n    @Override\n    public ByteBuf getBytes(int index, ByteBuf dst, int length) {\n        getBytes(index, dst, dst.writerIndex(), length);\n        // 调整 dst 的  writerIndex\n        dst.writerIndex(dst.writerIndex() + length);\n        return this;\n    }\n    \n    // 注意这里的 getBytes 方法既不会改变原来 ByteBuf 的 readerIndex 和 writerIndex\n    // 也不会改变目的 ByteBuf 的 readerIndex 和 writerIndex\n    public abstract ByteBuf getBytes(int index, ByteBuf dst, int dstIndex, int length);\n\n\n通过 getBytes 方法将原来 ByteBuf 的数据读取到目的 ByteBuf 之后，原来 ByteBuf 的 readerIndex 不会发生变化，但是目的 ByteBuf 的 writerIndex 会重新调整。\n\n对于 UnpooledDirectByteBuf 类的具体实现来说自然是将 getBytes 的操作直接代理给其底层依赖的 JDK DirectByteBuffer。对于 UnpooledUnsafeDirectByteBuf 类的具体实现来说，则是通过 UNSAFE.copyMemory 直接根据内存地址进行拷贝。\n\n而 ByteBuf 中的 read 方法则不仅会从 ByteBuf 中读取数据，而且会改变其 readerIndex 的位置。比如，readByte 方法首先会通过前面介绍的 _getByte 从 ByteBuf 中读取一个字节，然后将 readerIndex 向后移动一位。\n\n   @Override\n    public byte readByte() {\n        checkReadableBytes0(1);\n        int i = readerIndex;\n        byte b = _getByte(i);\n        readerIndex = i + 1;\n        return b;\n    }\n\n\n同样 Netty 也提供了从 ByteBuf 中批量读取数据的方法 readBytes，我们可以将一个 ByteBuf 中的数据通过 readBytes 方法读取到另一个 ByteBuf 中。但是这里，Netty 将会改变原来 ByteBuf 的 readerIndex 以及目的 ByteBuf 的 writerIndex。\n\n   @Override\n    public ByteBuf readBytes(ByteBuf dst, int length) {\n        readBytes(dst, dst.writerIndex(), length);\n        // 改变 dst 的 writerIndex\n        dst.writerIndex(dst.writerIndex() + length);\n        return this;\n    }\n\n\n另外我们还可以明确指定 dstIndex，使得我们可以从目的 ByteBuf 中的某一个位置处开始拷贝原来 ByteBuf 中的数据，但这里只会改变原来 ByteBuf 的 readerIndex，并不会改变目的 ByteBuf 的 writerIndex。这也很好理解，因为我们在写入目的 ByteBuf 的时候已经明确指定了 writerIndex（dstIndex），自然在写入完成之后，writerIndex 的位置并不需要改变。\n\n    @Override\n    public ByteBuf readBytes(ByteBuf dst, int dstIndex, int length) {\n        checkReadableBytes(length);\n        getBytes(readerIndex, dst, dstIndex, length);\n        // 改变原来 ByteBuf 的 readerIndex\n        readerIndex += length;\n        return this;\n    }\n\n\n除此之外，Netty 还支持将 ByteBuf 中的数据读取到不同的目的地，比如，读取到 JDK ByteBuffer 中，读取到 FileChannel 中，读取到 OutputStream 中，以及读取到 GatheringByteChannel 中。\n\npublic abstract ByteBuf readBytes(ByteBuffer dst);\npublic abstract ByteBuf readBytes(OutputStream out, int length) throws IOException;\npublic abstract int readBytes(GatheringByteChannel out, int length) throws IOException;\npublic abstract int readBytes(FileChannel out, long position, int length) throws IOException;\n\n\nNetty 除了支持以 Byte 为粒度对 ByteBuf 进行读写之外，还同时支持以多种基本类型对 ByteBuf 进行读写，这里笔者以 Int 类型为例进行说明。\n\n我们可以通过 readInt() 从 ByteBuf 中读取一个 Int 类型的数据出来，随后 ByteBuf 的 readerIndex 向后移动 4 个位置。\n\n   @Override\n    public int readInt() {\n        checkReadableBytes0(4);\n        int v = _getInt(readerIndex);\n        readerIndex += 4;\n        return v;\n    }\n\n    protected abstract int _getInt(int index);\n\n\n同理，真正负责读取数据的方法 _getInt 方法需要由 AbstractByteBuf 具体的子类实现，但这里和 _getByte 不同的是，_getInt 需要考虑字节序的问题，由于网络协议采用的是大端字节序传输，所以 Netty 的 ByteBuf 默认也是大端字节序。\n\n在 UnpooledDirectByteBuf 的实现中，同样也是将 getInt 的操作直接代理给其底层依赖的 JDK DirectByteBuffer。\n\npublic class UnpooledDirectByteBuf  {\n    @Override\n    protected int _getInt(int index) {\n        // 代理给其底层依赖的 JDK DirectByteBuffer\n        return buffer.getInt(index);\n    }\n}\n\n\n在 UnpooledUnsafeDirectByteBuf 的实现中，由于是通过 sun.misc.Unsafe 直接对内存地址进行操作，所以需要考虑字节序转换的细节。Netty 的 ByteBuf 默认是大端字节序，所以这里直接依次将低地址的字节放到 Int 数据的高位就可以了。\n\npublic class UnpooledUnsafeDirectByteBuf {\n    @Override\n    protected int _getInt(int index) {\n        return UnsafeByteBufUtil.getInt(addr(index));\n    }\n}\n\nfinal class UnsafeByteBufUtil {\n    static int getInt(long address) {    \n        return PlatformDependent.getByte(address) << 24 |\n               (PlatformDependent.getByte(address + 1) & 0xff) << 16 |\n               (PlatformDependent.getByte(address + 2) & 0xff) <<  8 |\n               PlatformDependent.getByte(address + 3)  & 0xff;\n    }\n}\n\n\n同时 Netty 也支持以小端字节序来从 ByteBuf 中读取 Int 数据，这里就涉及到字节序的转换了。\n\n    @Override\n    public int readIntLE() {\n        checkReadableBytes0(4);\n        int v = _getIntLE(readerIndex);\n        readerIndex += 4;\n        return v;\n    }\n\n protected abstract int _getIntLE(int index);\n\n\n在 UnpooledDirectByteBuf 的实现中，首先通过其依赖的 JDK DirectByteBuffer 以大端序读取一个 Int 数据，然后通过 ByteBufUtil.swapInt 切换成小端序返回。\n\npublic class UnpooledDirectByteBuf  {\n    @Override\n    protected int _getIntLE(int index) {\n        // 切换字节序，从大端变小端\n        return ByteBufUtil.swapInt(buffer.getInt(index));\n    }\n}\n\n\n在 UnpooledUnsafeDirectByteBuf 的实现中，则是直接将低地址上的字节依次放到 Int 数据的低位上就可以了。\n\npublic class UnpooledUnsafeDirectByteBuf {\n    @Override\n    protected int _getIntLE(int index) {\n        return UnsafeByteBufUtil.getIntLE(addr(index));\n    }\n}\n\nfinal class UnsafeByteBufUtil {\n    static int getIntLE(long address) {\n        return PlatformDependent.getByte(address) & 0xff |\n               (PlatformDependent.getByte(address + 1) & 0xff) <<  8 |\n               (PlatformDependent.getByte(address + 2) & 0xff) << 16 |\n               PlatformDependent.getByte(address + 3) << 24;\n    }\n}\n\n\n另外 Netty 也支持从 ByteBuf 中读取基本类型的 Unsigned 类型。\n\n    @Override\n    public long readUnsignedInt() {\n        return readInt() & 0xFFFFFFFFL;\n    }\n\n    @Override\n    public long readUnsignedIntLE() {\n        return readIntLE() & 0xFFFFFFFFL;\n    }\n\n\n其他基本类型的相关读取操作实现的逻辑都是大同小异，笔者就不一一列举了。\n\n\n# 2.3 discardReadBytes\n\n随着 readBytes 方法的不断调用， ByteBuf 中的 readerIndex 也会不断的向后移动，Netty 对 readerIndex 的设计有两层语义：\n\n 1. 第一层的语义比较明显，就是用来表示当前 ByteBuf 的读取位置，当我们调用 readBytes 方法的时候就是从 readerIndex 开始读取数据，当 readerIndex 等于 writerIndex 的时候，ByteBuf 就不可读取了。\n 2. 第二层语义比较含蓄，它是用来表示当前 ByteBuf 可以被丢弃的字节数，因为 readerIndex 用来指示当前的读取位置，那么位于 readerIndex 之前的字节肯定是已经被读取完毕了，已经被读取的字节继续驻留在 ByteBuf 中就没有必要了，还不如把空间腾出来，还能在多写入些数据。\n\nimage.png\n\n所以一个 ByteBuf 真正的剩余可写容量的计算方式除了上小节中介绍的 writableBytes() 方法返回的字节数之外还需要在加上 readerIndex。\n\n    @Override\n    public int writableBytes() {\n        return capacity() - writerIndex;\n    }\n\n\n举个具体点的例子就是，当我们准备向一个 ByteBuf 写入 n 个字节时，如果 writableBytes() 小于 n，那么就表示当前 ByteBuf 的剩余容量不能满足本次写入的字节数。\n\n但是 readerIndex + writableBytes()大于等于 n ， 则表示如果我们将 ByteBuf 中已经读取的字节数丢弃的话，那么就可以满足本次写入的请求。\n\n在这种情况下，我们就可以使用 discardReadBytes() 方法将 readerIndex 之前的字节丢弃掉，这样一来，可写的字节容就可以满足本次写入要求了，那么如果丢弃呢 ？\n\n我们先来看 readerIndex < writerIndex 的情况，这种情况下表示 ByteBuf 中还有未读取的字节。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\nByteBuf 目前可读取的字节范围为：[readerIndex, writerIndex)，位于 readerIndex 之前的字节均可以被丢弃，接下来我们就需要将 [readerIndex, writerIndex) 这段范围的字节全部拷贝到 ByteBuf 最前面，直接覆盖 readerIndex 之前的字节。\n\n然后调整 readerIndex 和 writerIndex 的位置，因为 readerIndex 之前的字节现在已经全部被可读字节覆盖了，所以 readerIndex 重新调整为 0 ，writerIndex 向前移动 readerIndex 大小。这样一来，当前 ByteBuf 的可写容量就多出了 readerIndex 大小。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n另外一种情况是 readerIndex = writerIndex 的情况，这种情况下表示 ByteBuf 中已经没有可读字节了。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n既然 ByteBuf 中已经没有任何可读字节了，自然也就不需要将可读字节拷贝到 ByteBuf 的开头了，直接将 readerIndex 和 writerIndex 重新调整为 0 即可。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\npublic abstract class AbstractByteBuf extends ByteBuf {\n    @Override\n    public ByteBuf discardReadBytes() {\n        // readerIndex 为 0 表示没有可以丢弃的字节\n        if (readerIndex == 0) {\n            return this;\n        }\n\n        if (readerIndex != writerIndex) {\n            // 将 [readerIndex, writerIndex) 这段字节范围移动到 ByteBuf 的开头\n            // 也就是丢弃 readerIndex 之前的字节\n            setBytes(0, this, readerIndex, writerIndex - readerIndex);\n            // writerIndex 和 readerIndex 都向前移动 readerIndex 大小\n            writerIndex -= readerIndex;\n            // 重新调整 markedReaderIndex 和 markedWriterIndex 的位置\n            // 都对应向前移动 readerIndex 大小。\n            adjustMarkers(readerIndex);\n            readerIndex = 0;\n        } else {\n            // readerIndex = writerIndex 表示当前 ByteBuf 已经不可读了\n            // 将 readerIndex 之前的字节全部丢弃，ByteBuf 恢复到最初的状态\n            // 整个 ByteBuf 的容量都可以被写入\n            ensureAccessible();\n            adjustMarkers(readerIndex);\n            writerIndex = readerIndex = 0;\n        }\n        return this;\n    }\n}\n\n\n如果 ByteBuf 存在可以被丢弃的字节的时候（readerIndex > 0），只要我们调用 discardReadBytes() 就会无条件丢弃 readerIndex 之前的字节。\n\nNetty 还另外提供了 discardSomeReadBytes() 方法进行有条件丢弃字节，丢弃条件有如下两种：\n\n 1. 当 ByteBuf 已经不可读的时候，则无条件丢弃已读字节。\n 2. 当已读的字节数超过整个 ByteBuf 一半容量时才会丢弃已读字节。否则无条件丢弃的话，收益就不高了。\n\n    @Override\n    public ByteBuf discardSomeReadBytes() {\n        if (readerIndex > 0) {\n            // 当 ByteBuf 已经不可读了，则无条件丢弃已读字节\n            if (readerIndex == writerIndex) {\n                adjustMarkers(readerIndex);\n                writerIndex = readerIndex = 0;\n                return this;\n            }\n            // 当已读的字节数超过整个 ByteBuf 的一半容量时才会丢弃已读字节\n            if (readerIndex >= capacity() >>> 1) {\n                setBytes(0, this, readerIndex, writerIndex - readerIndex);\n                writerIndex -= readerIndex;\n                adjustMarkers(readerIndex);\n                readerIndex = 0;\n                return this;\n            }\n        }\n        return this;\n    }\n\n\nNetty 设计的这个丢弃字节的方法在解码的场景非常有用，由于 TCP 是一个面向流的网络协议，它只会根据滑动窗口的大小进行字节流的发送，所以我们在应用层接收到的数据可能是一个半包也可能是一个粘包，反正不会是一个完整的数据包。\n\n这就要求我们在解码的时候，首先要判断 ByteBuf 中的数据是否构成一个完成的数据包，如果构成一个数据包，才会去读取 ByteBuf 中的字节，然后解码，随后 readerIndex 向后移动。\n\n如果不够一个数据包，那就需要将 ByteBuf 累积缓存起来，一直等到一个完整的数据包到来。一种极端的情况是，即使我们已经解码很多次了，但是缓存的 ByteBuf 中仍然还有半包，由于不断的会有粘包过来，这就导致 ByteBuf 会越来越大。由于已经解码了很多次，所以 ByteBuf 中可以被丢弃的字节占据了很大的内存空间，如果半包情况持续存在，将会导致 OutOfMemory。\n\n所以 Netty 规定，如果已经解码了 16 次之后，ByteBuf 中仍然有半包的情况，那么就会调用这里的 discardSomeReadBytes() 将已经解码过的字节全部丢弃，节省不必要的内存开销。\n\n\n# 2.4 ByteBuf 的写入操作\n\nByteBuf 的写入操作与读取操作互为相反的操作，每一个读取方法 getBytes , readBytes , readInt 等都有一个对应的 setBytes , writeBytes , writeInt 等基础类型的写入操作。\n\n和 get 方法一样，set 相关的方法也只是单纯的向 ByteBuf 中写入数据，并不会改变其 writerIndex 的位置，我们可以通过 setByte 向 ByteBuf 中的某一个指定位置 index 写入数据 value。\n\n    @Override\n    public ByteBuf setByte(int index, int value) {\n        checkIndex(index);\n        _setByte(index, value);\n        return this;\n    }\n\n    protected abstract void _setByte(int index, int value);\n\n\n执行具体的写入操作同样也是一个抽象方法，其具体的实现由 AbstractByteBuf 具体的子类负责。对于 UnpooledDirectByteBuf 的实现来说，_setByte 操作直接会代理给其底层依赖的 JDK DirectByteBuffer。\n\npublic class UnpooledDirectByteBuf  {\n    // 底层依赖 JDK 的 DirectByteBuffer\n    ByteBuffer buffer;\n\n    @Override\n    protected void _setByte(int index, int value) {\n        buffer.put(index, (byte) value);\n    }\n}\n\n\n对于 UnpooledUnsafeDirectByteBuf 的实现来说，则是直接通过 sun.misc.Unsafe 向对应的内存地址（memoryAddress + index）写入 Byte。\n\npublic class UnpooledUnsafeDirectByteBuf {\n    // 直接操作 OS 的内存地址，不依赖 JDK 的 buffer\n    long memoryAddress;\n\n   @Override\n    protected void _setByte(int index, int value) {\n        // 底层依赖 PlatformDependent0，直接向内存地址写入 byte\n        UnsafeByteBufUtil.setByte(addr(index), value);\n    }\n\n    final long addr(int index) {\n        // 获取偏移 index 对应的内存地址\n        return memoryAddress + index;\n    }\n}\n\nfinal class PlatformDependent0 {\n  // sun.misc.Unsafe\n  static final Unsafe UNSAFE;\n  static void putByte(long address, byte value) {\n        UNSAFE.putByte(address, value);\n  }\n}\n\n\nNetty 另外也提供了向 ByteBuf 批量写入 Bytes 的操作，setBytes 方法用于向 ByteBuf 的指定位置 index 批量写入一个字节数组 byte[] 中的数据。\n\n   @Override\n    public ByteBuf setBytes(int index, byte[] src) {\n        setBytes(index, src, 0, src.length);\n        return this;\n    }\n\n    public abstract ByteBuf setBytes(int index, byte[] src, int srcIndex, int length);\n\n\n对于 UnpooledDirectByteBuf 的实现来说，同样也是将 setBytes 的操作直接代理给 JDK DirectByteBuffer，将字节数组 byte[] 中的字节直接写入 DirectByteBuffer 中。\n\n对于 UnpooledUnsafeDirectByteBuf 的实现来说，则是直接操作字节数组和 ByteBuf 的内存地址，通过 UNSAFE.copyMemory 将字节数组对应内存地址中的数据拷贝到 ByteBuf 相应的内存地址上。\n\n我们还可以通过 setBytes 方法将其他 ByteBuf 中的字节数据写入到 ByteBuf 中。\n\n    @Override\n    public ByteBuf setBytes(int index, ByteBuf src, int length) {\n        setBytes(index, src, src.readerIndex(), length);\n        // 调整 src 的  readerIndex\n        src.readerIndex(src.readerIndex() + length);\n        return this;\n    }\n\n    // 注意这里的 setBytes 方法既不会改变原来 ByteBuf 的 readerIndex 和 writerIndex\n    // 也不会改变目的 ByteBuf 的 readerIndex 和 writerIndex\n    public abstract ByteBuf setBytes(int index, ByteBuf src, int srcIndex, int length);\n\n\n这里需要注意的是被写入 ByteBuf 的 writerIndex 并不会改变，但是原来 ByteBuf 的 readerIndex 会重新调整。\n\nByteBuf 中的 write 方法底层依赖的是相关的 set 方法，不同的是 write 方法会改变 ByteBuf 中 writerIndex 的位置。比如，我们通过 writeByte 方法向 ByteBuf 中写入一个字节之后，writerIndex 就会向后移动一位。\n\n    @Override\n    public ByteBuf writeByte(int value) {\n        ensureWritable0(1);\n        _setByte(writerIndex++, value);\n        return this;\n    }\n\n\n我们也可以通过 writeBytes 向 ByteBuf 中批量写入数据，将一个字节数组中的数据或者另一个 ByteBuf 中的数据写入到 ByteBuf 中，但是这里，Netty 将会改变被写入 ByteBuf 的 writerIndex 以及数据来源 ByteBuf 的 readerIndex。\n\n    @Override\n    public ByteBuf writeBytes(ByteBuf src, int length) {\n        writeBytes(src, src.readerIndex(), length);\n        // 调整数据来源 ByteBuf 的 readerIndex\n        src.readerIndex(src.readerIndex() + length);\n        return this;\n    }\n\n\n如果我们明确指定了从数据来源 ByteBuf 中的哪一个位置（srcIndex）开始读取数据，那么数据来源 ByteBuf 中的 readerIndex 将不会被改变，只会改变被写入 ByteBuf 的 writerIndex。\n\n    @Override\n    public ByteBuf writeBytes(ByteBuf src, int srcIndex, int length) {\n        ensureWritable(length);\n        setBytes(writerIndex, src, srcIndex, length);\n        // 调整被写入 ByteBuf 的 writerIndex\n        writerIndex += length;\n        return this;\n    }\n\n\n除此之外，Netty 还支持从不同的数据来源向 ByteBuf 批量写入数据，比如，从 JDK ByteBuffer ，从 FileChannel ，从 InputStream ，以及从 ScatteringByteChannel 中。\n\npublic ByteBuf writeBytes(ByteBuffer src) \npublic int writeBytes(InputStream in, int length)\npublic int writeBytes(ScatteringByteChannel in, int length) throws IOException\npublic int writeBytes(FileChannel in, long position, int length) throws IOException\n\n\nNetty 除了支持以 Byte 为粒度向 ByteBuf 中写入数据之外，还同时支持以多种基本类型为粒度向写入 ByteBuf ，这里笔者以 Int 类型为例进行说明。\n\n我们可以通过 writeInt() 向 ByteBuf 写入一个 Int 类型的数据，随后 ByteBuf 的 writerIndex 向后移动 4 个位置。\n\n    @Override\n    public ByteBuf writeInt(int value) {\n        ensureWritable0(4);\n        _setInt(writerIndex, value);\n        writerIndex += 4;\n        return this;\n    }\n\n   protected abstract void _setInt(int index, int value);\n\n\n和写入 Byte 数据不同的是，这里需要考虑字节序，Netty ByteBuf 默认是大端字节序，和网络协议传输使用的字节序保持一致。这里我们需要将待写入数据 value 的高位依次放入到 ByteBuf 的低地址上。\n\npublic class UnpooledUnsafeDirectByteBuf {\n   @Override\n    protected void _setInt(int index, int value) {\n        // 以大端字节序写入 ByteBuf \n        UnsafeByteBufUtil.setInt(addr(index), value);\n    }\n}\n\nfinal class UnsafeByteBufUtil {\n   static void setInt(long address, int value) {\n            PlatformDependent.putByte(address, (byte) (value >>> 24));\n            PlatformDependent.putByte(address + 1, (byte) (value >>> 16));\n            PlatformDependent.putByte(address + 2, (byte) (value >>> 8));\n            PlatformDependent.putByte(address + 3, (byte) value);   \n    }\n}\n\n\n同时 Netty 也支持以小端字节序向 ByteBuf 写入数据。\n\n    @Override\n    public ByteBuf writeIntLE(int value) {\n        ensureWritable0(4);\n        _setIntLE(writerIndex, value);\n        writerIndex += 4;\n        return this;\n    }\n\n   protected abstract void _setIntLE(int index, int value);\n\n\n这里需要将待写入数据 value 的低位依次放到 ByteBuf 的低地址上。\n\npublic class UnpooledUnsafeDirectByteBuf {\n    @Override\n    protected void _setIntLE(int index, int value) {\n        // // 以小端字节序写入 ByteBuf \n        UnsafeByteBufUtil.setIntLE(addr(index), value);\n    }\n}\n\nfinal class UnsafeByteBufUtil {\n   static void setIntLE(long address, int value) {\n            PlatformDependent.putByte(address, (byte) value);\n            PlatformDependent.putByte(address + 1, (byte) (value >>> 8));\n            PlatformDependent.putByte(address + 2, (byte) (value >>> 16));\n            PlatformDependent.putByte(address + 3, (byte) (value >>> 24));\n    }\n}\n\n\n\n# 2.5 ByteBuf 的扩容机制\n\n在每次向 ByteBuf 写入数据的时候，Netty 都会调用 ensureWritable0 方法来判断当前 ByteBuf 剩余可写容量（capacity - writerIndex）是否能够满足本次需要写入的数据大小 minWritableBytes。如果剩余容量不足，那么就需要对 ByteBuf 进行扩容，但扩容后的容量不能超过 maxCapacity 的大小。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n    final void ensureWritable0(int minWritableBytes) {\n        final int writerIndex = writerIndex();\n        // 为满足本次的写入操作，预期的 ByteBuf 容量大小\n        final int targetCapacity = writerIndex + minWritableBytes;\n        // 剩余容量可以满足本次写入要求，直接返回，不需要扩容\n        if (targetCapacity >= 0 & targetCapacity <= capacity()) {\n            return;\n        }\n        // 扩容后的容量不能超过 maxCapacity\n        if (checkBounds && (targetCapacity < 0 || targetCapacity > maxCapacity)) {\n            ensureAccessible();\n            throw new IndexOutOfBoundsException(String.format(\n                    \"writerIndex(%d) + minWritableBytes(%d) exceeds maxCapacity(%d): %s\",\n                    writerIndex, minWritableBytes, maxCapacity, this));\n        }\n\n        // 如果 targetCapacity 在（capacity , maxCapacity] 之间，则进行扩容\n        // fastWritable 表示在不涉及到 memory reallocation or data-copy 的情况下，当前 ByteBuf 可以直接写入的容量\n        // 对于 UnpooledDirectBuffer 这里的 fastWritable = capacity - writerIndex\n        // PooledDirectBuffer 有另外的实现，这里先暂时不需要关注\n        final int fastWritable = maxFastWritableBytes();\n        // 计算扩容后的容量 newCapacity\n        // 对于 UnpooledDirectBuffer 来说这里直接通过 calculateNewCapacity 计算扩容后的容量。\n        int newCapacity = fastWritable >= minWritableBytes ? writerIndex + fastWritable\n                : alloc().calculateNewCapacity(targetCapacity, maxCapacity);\n\n        // 根据 new capacity 对 ByteBuf 进行扩容\n        capacity(newCapacity);\n    }\n\n\n# 2.5.1 newCapacity 的计算逻辑\n\nByteBuf 的初始默认 capacity 为 256 个字节，初始默认 maxCapacity 为 Integer.MAX_VALUE 也就是 2G 大小。\n\npublic abstract class AbstractByteBufAllocator implements ByteBufAllocator {\n    // ByteBuf 的初始默认 CAPACITY\n    static final int DEFAULT_INITIAL_CAPACITY = 256;\n    // ByteBuf 的初始默认 MAX_CAPACITY \n    static final int DEFAULT_MAX_CAPACITY = Integer.MAX_VALUE;\n\n    @Override\n    public ByteBuf directBuffer() {\n        return directBuffer(DEFAULT_INITIAL_CAPACITY, DEFAULT_MAX_CAPACITY);\n    }\n}\n\n\n为满足本次写入操作，对 ByteBuf 的最小容量要求为 minNewCapacity，它的值就是在 ensureWritable0 方法中计算出来的 targetCapacity, 计算方式为： minNewCapacity = writerIndex + minWritableBytes（本次将要写入的字节数）。\n\n在 ByteBuf 的扩容逻辑中，Netty 设置了一个重要的阈值 CALCULATE_THRESHOLD, 大小为 4M，它决定了 ByteBuf 扩容的尺度。\n\n    // 扩容的尺度\n    static final int CALCULATE_THRESHOLD = 1048576 * 4; // 4 MiB page\n\n\n如果 minNewCapacity 恰好等于 CALCULATE_THRESHOLD，那么扩容后的容量 newCapacity 就是 4M。\n\n如果 minNewCapacity 大于 CALCULATE_THRESHOLD，那么 newCapacity 就会按照 4M 的尺度进行扩容，具体的扩容逻辑如下：\n\n首先通过 minNewCapacity / threshold * threshold 计算出一个准备扩容之前的基准线，后面就会以此基准线为基础，按照 CALCULATE_THRESHOLD 的粒度进行扩容。\n\n该基准线的要求必须是 CALCULATE_THRESHOLD 的最小倍数，而且必须要小于等于 minNewCapacity。\n\n什么意思呢 ？ 假设 minNewCapacity 为 5M，那么它的扩容基准线就是 4M ， 这种情况下扩容之后的容量 newCapacity = 4M + CALCULATE_THRESHOLD = 8M 。\n\n如果计算出来的基准线超过了 maxCapacity - 4M , 那么 newCapacity 直接就扩容到 maxCapacity 。\n\n如果 minNewCapacity 小于 CALCULATE_THRESHOLD，那么 newCapacity 就会从 64 开始，一直循环 double , 也就是按照 64 的倍数进行扩容。直到 newCapacity 大于等于 minNewCapacity。\n\n        int newCapacity = 64;\n        while (newCapacity < minNewCapacity) {\n            newCapacity <<= 1;\n        }\n\n\n * 如果 minNewCapacity 在 [0 , 64] 这段范围内 ， 那么扩容后的 newCapacity 就是 64\n * 如果 minNewCapacity 在 [65 , 128] 这段范围内 ， 那么扩容后的 newCapacity 就是 128 。\n * 如果 minNewCapacity 在 [129 , 256] 这段范围内 ， 那么扩容后的 newCapacity 就是 256 。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\npublic abstract class AbstractByteBufAllocator implements ByteBufAllocator {\n\n    @Override\n    public int calculateNewCapacity(int minNewCapacity, int maxCapacity) {\n        // 满足本次写入操作的最小容量 minNewCapacity 不能超过 maxCapacity\n        if (minNewCapacity > maxCapacity) {\n            throw new IllegalArgumentException(String.format(\n                    \"minNewCapacity: %d (expected: not greater than maxCapacity(%d)\",\n                    minNewCapacity, maxCapacity));\n        }\n        // 用于决定扩容的尺度\n        final int threshold = CALCULATE_THRESHOLD; // 4 MiB page\n\n        if (minNewCapacity == threshold) {\n            return threshold;\n        }\n\n        // If over threshold, do not double but just increase by threshold.\n        if (minNewCapacity > threshold) {\n            // 计算扩容基准线。\n            // 要求必须是 CALCULATE_THRESHOLD 的最小倍数，而且必须要小于等于 minNewCapacity\n            int newCapacity = minNewCapacity / threshold * threshold;\n            if (newCapacity > maxCapacity - threshold) {\n                newCapacity = maxCapacity;\n            } else {\n                // 按照 threshold (4M)扩容\n                newCapacity += threshold;\n            }\n            return newCapacity;\n        }\n\n        // Not over threshold. Double up to 4 MiB, starting from 64.\n        // 按照 64 的倍数进行扩容。但 newCapacity 需要大于等于 minNewCapacity。\n        int newCapacity = 64;\n        while (newCapacity < minNewCapacity) {\n            newCapacity <<= 1;\n        }\n\n        return Math.min(newCapacity, maxCapacity);\n    }\n}\n\n\n# 2.5.2 ByteBuf 的扩容逻辑\n\npublic class UnpooledDirectByteBuf  {\n    // 底层依赖 JDK 的 DirectByteBuffer\n    ByteBuffer buffer;\n}\n\n\n对于 UnpooledDirectByteBuf 来说，其底层真正存储数据的地方其实是依赖 JDK 中的 DirectByteBuffer，扩容的逻辑很简单，就是首先根据上一小节计算出的 newCapacity 重新分配一个新的 JDK DirectByteBuffer ， 然后将原来 DirectByteBuffer 中的数据拷贝到新的 DirectByteBuffer 中，最后释放原来的 DirectByteBuffer，将新的 DirectByteBuffer 设置到 UnpooledDirectByteBuf 中。\n\npublic class UnpooledDirectByteBuf  {\n    void setByteBuffer(ByteBuffer buffer, boolean tryFree) {\n        if (tryFree) {\n            ByteBuffer oldBuffer = this.buffer;\n            // 释放原来的 buffer\n            freeDirect(oldBuffer);\n        }\n        // 重新设置新的 buffer\n        this.buffer = buffer;\n        capacity = buffer.remaining();\n    }\n}\n\n\n对于 UnpooledUnsafeDirectByteBuf 来说，由于它直接依赖的是 OS 内存地址，对 ByteBuf 的相关操作都是直接操作内存地址进行，所以 UnpooledUnsafeDirectByteBuf 的扩容逻辑除了要执行上面的内容之外，还需要将新 DirectByteBuffer 的内存地址设置到 memoryAddress 中。\n\npublic class UnpooledUnsafeDirectByteBuf extends UnpooledDirectByteBuf {\n    // ByteBuf 的内存地址\n    long memoryAddress;\n\n    @Override\n    final void setByteBuffer(ByteBuffer buffer, boolean tryFree) {\n        super.setByteBuffer(buffer, tryFree);\n        // 设置成新 buffer 的内存地址\n        memoryAddress = PlatformDependent.directBufferAddress(buffer);\n    }\n}\n\n\n下面是完整的扩容操作逻辑：\n\npublic class UnpooledDirectByteBuf  {\n    // 底层依赖 JDK 的 DirectByteBuffer\n    ByteBuffer buffer;\n\n    @Override\n    public ByteBuf capacity(int newCapacity) {\n        // newCapacity 不能超过 maxCapacity\n        checkNewCapacity(newCapacity);\n        int oldCapacity = capacity;\n        if (newCapacity == oldCapacity) {\n            return this;\n        }\n        // 计算扩容之后需要拷贝的字节数\n        int bytesToCopy;\n        if (newCapacity > oldCapacity) {\n            bytesToCopy = oldCapacity;\n        } else {\n            ........ 缩容 .......\n        }\n        ByteBuffer oldBuffer = buffer;\n        // 根据 newCapacity 分配一个新的 ByteBuffer（JDK）\n        ByteBuffer newBuffer = allocateDirect(newCapacity);\n        oldBuffer.position(0).limit(bytesToCopy);\n        newBuffer.position(0).limit(bytesToCopy);\n        // 将原来 oldBuffer 中的数据拷贝到 newBuffer 中\n        newBuffer.put(oldBuffer).clear();\n        // 释放 oldBuffer，设置 newBuffer\n        // 对于 UnpooledUnsafeDirectByteBuf 来说就是将 newBuffer 的地址设置到 memoryAddress 中\n        setByteBuffer(newBuffer, true);\n        return this;\n    }\n}\n\n\n# 2.5.3 强制扩容\n\n前面介绍的 ensureWritable 方法会检查本次写入的数据大小 minWritableBytes 是否超过 ByteBuf 的最大可写容量：maxCapacity - writerIndex。\n\npublic ByteBuf ensureWritable(int minWritableBytes) \n\n\n如果超过，则会抛出 IndexOutOfBoundsException 异常停止扩容，Netty 提供了另外一个带有 force 参数的扩容方法，用来决定在这种情况下是否强制进行扩容。\n\n public int ensureWritable(int minWritableBytes, boolean force) \n\n\n当 minWritableBytes 已经超过 ByteBuf 的最大可写容量得时候：\n\n * force = false ， 那么停止扩容，直接返回，不抛异常。\n * force = true , 则进行强制扩容，将 ByteBuf 扩容至 maxCapacity，但是如果当前容量已经达到了 maxCapacity，则停止扩容 。\n\n带 force 参数的 ensureWritable 并不会抛出异常，而是通过返回状态码来通知调用者 ByteBuf 的容量情况。\n\n 1. 返回 0 表示，ByteBuf 当前可写容量可以满足本次写入操作的需求，不需要扩容\n 2. 返回 1 表示，本次写入的数据大小已经超过了 ByteBuf 的最大可写容量，但 ByteBuf 的容量已经达到了 maxCapacity，无法进行扩容。\n 3. 返回 3 表示，本次写入的数据大小已经超过了 ByteBuf 的最大可写容量，这种情况下，强制将容量扩容至 maxCapacity。\n 4. 返回 2 表示，执行正常的扩容逻辑。\n\n返回值 0 和 2 均表示 ByteBuf 容量（扩容前或者扩容后）可以满足本次写入的数据大小，而返回值 1 和 3 表示 ByteBuf 容量（扩容前或者扩容后）都无法满足本次写入的数据大小。\n\n    @Override\n    public int ensureWritable(int minWritableBytes, boolean force) {\n        // 如果剩余容量可以满足本次写入操作，则不会扩容，直接返回\n        if (minWritableBytes <= writableBytes()) {\n            return 0;\n        }\n\n        final int maxCapacity = maxCapacity();\n        final int writerIndex = writerIndex();\n        // 如果本次写入的数据大小已经超过了 ByteBuf 的最大可写容量 maxCapacity - writerIndex\n        if (minWritableBytes > maxCapacity - writerIndex) {\n            // force = false ， 那么停止扩容，直接返回\n            // force = true, 直接扩容到 maxCapacity，如果当前 capacity 已经等于 maxCapacity 了则停止扩容\n            if (!force || capacity() == maxCapacity) {\n                return 1;\n            }\n            // 虽然扩容之后还是无法满足写入需求，但还是强制扩容至 maxCapacity\n            capacity(maxCapacity);\n            return 3;\n        }\n        // 下面就是普通的扩容逻辑\n        int fastWritable = maxFastWritableBytes();\n        int newCapacity = fastWritable >= minWritableBytes ? writerIndex + fastWritable\n                : alloc().calculateNewCapacity(writerIndex + minWritableBytes, maxCapacity);\n\n        // Adjust to the new capacity.\n        capacity(newCapacity);\n        return 2;\n    }\n\n\n# 2.5.4 自适应动态扩容\n\nNetty 在接收网络数据的过程中，其实一开始是很难确定出该用多大容量的 ByteBuf 去接收的，所以 Netty 在一开始会首先预估一个初始容量 DEFAULT_INITIAL (2048)。\n\npublic class AdaptiveRecvByteBufAllocator {\n    static final int DEFAULT_INITIAL = 2048;\n}\n\n\n用初始容量为 2048 大小的 ByteBuf 去读取 socket 中的数据，在每一次读取完 socket 之后，Netty 都会评估 ByteBuf 的容量大小是否合适。如果每一次都能把 ByteBuf 装满，那说明我们预估的容量太小了，socket 中还有更多的数据，那么就需要对 ByteBuf 进行扩容，下一次读取 socket 的时候就换一个容量更大的 ByteBuf。\n\n private final class HandleImpl extends MaxMessageHandle {\n        @Override\n        public void lastBytesRead(int bytes) {\n            // bytes 为本次从 socket 中真实读取的数据大小\n            // attemptedBytesRead 为 ByteBuf 可写的容量大小，初始为 2048\n            if (bytes == attemptedBytesRead()) {\n                // 如果本次读取 socket 中的数据将 ByteBuf 装满了\n                // 那么就对 ByteBuf 进行扩容，在下一次读取的时候用更大的 ByteBuf 去读\n                record(bytes);\n            }\n            // 记录本次从 socket 中读取的数据大小\n            super.lastBytesRead(bytes);\n        }\n }\n\n\nNetty 会在一个 read loop 中不停的读取 socket 中的数据直到数据被读取完毕或者读满 16 次，结束 read loop 停止读取。ByteBuf 越大那么 Netty 读取的次数就越少，ByteBuf 越小那么 Netty 读取的次数就越多，所以需要一种机制将 ByteBuf 的容量控制在一个合理的范围内。\n\nNetty 会统计每一轮 read loop 总共读取了多少数据 —— totalBytesRead。\n\n public abstract class MaxMessageHandle implements ExtendedHandle {\n        // 用于统计在一轮 read loop 中总共接收到客户端连接上的数据大小\n        private int totalBytesRead;\n }\n\n\n在每一轮的 read loop 结束之后，Netty 都会根据这个 totalBytesRead 来判断是否应该对 ByteBuf 进行扩容或者缩容，这样在下一轮 read loop 开始的时候，Netty 就可以用一个相对合理的容量去接收 socket 中的数据，尽量减少读取 socket 的次数。\n\nprivate final class HandleImpl extends MaxMessageHandle {\n        @Override\n        public void readComplete() {\n                // 是否对 ByteBuf 进行扩容或者缩容\n                record(totalBytesRead());\n        }\n}\n\n\n那么在什么情况下需要对 ByteBuf 扩容，每次扩容多少 ？ 什么情况下需要对 ByteBuf 进行缩容，每次缩容多少呢 ？\n\n这就用到了一个重要的容量索引结构 —— SIZE_TABLE，它里边定义索引了 ByteBuf 的每一种容量大小。相当于是扩缩容的容量索引表。每次扩容多少，缩容多少全部记录在这个容量索引表中。\n\npublic class AdaptiveRecvByteBufAllocator {\n    // 扩容步长\n    private static final int INDEX_INCREMENT = 4;\n    // 缩容步长\n    private static final int INDEX_DECREMENT = 1;\n\n    // ByteBuf分配容量表（扩缩容索引表）按照表中记录的容量大小进行扩缩容\n    private static final int[] SIZE_TABLE;\n}\n\n\n当索引容量小于 512 时，SIZE_TABLE 中定义的容量是从 16 开始按照 16 递增。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n当索引容量大于 512 时，SIZE_TABLE 中定义的容量是按前一个索引容量的 2 倍递增。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n那么当前 ByteBuf 的初始容量为 2048 ， 它在 SIZE_TABLE 中的 index 为 33 。当一轮 read loop 读取完毕之后，如果发现 totalBytesRead 在SIZE_TABLE[index - INDEX_DECREMENT] 与 SIZE_TABLE[index] 之间的话，也就是如果本轮 read loop 结束之后总共读取的字节数在 [1024 , 2048] 之间。说明此时分配的 ByteBuf 容量正好，不需要进行缩容也不需要进行扩容。比如本次 totalBytesRead = 2000，正好处在 1024 与 2048 之间。说明 2048 的容量正好。\n\n如果 totalBytesRead 小于等于 SIZE_TABLE[index - INDEX_DECREMENT]，也就是如果本轮 read loop 结束之后总共读取的字节数小于等于1024。表示本次读取到的字节数比当前 ByteBuf 容量的下一级容量还要小，说明当前 ByteBuf 的容量分配的有些大了，设置缩容标识decreaseNow = true。当下次 read loop 的时候如果继续满足缩容条件，那么就开始进行缩容。缩容后的容量为 SIZE_TABLE[index - INDEX_DECREMENT]，但不能小于SIZE_TABLE[minIndex]（16）。\n\n> 注意，这里需要满足两次缩容条件才会进行缩容，且缩容步长为 1 (INDEX_DECREMENT)，缩容比较谨慎。\n\n如果 totalBytesRead 大于等于当前 ByteBuf 容量—— nextReceiveBufferSize 时，说明 ByteBuf 的容量有点小了，需要进行扩容。扩容后的容量为 SIZE_TABLE[index + INDEX_INCREMENT]，但不能超过 SIZE_TABLE[maxIndex]（65535）。\n\n> 满足一次扩容条件就进行扩容，并且扩容步长为 4 (INDEX_INCREMENT)， 扩容比较奔放。\n\n        private void record(int actualReadBytes) {\n            if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT)]) {\n                // 缩容条件触发两次之后就进行缩容\n                if (decreaseNow) {\n                    index = max(index - INDEX_DECREMENT, minIndex);\n                    nextReceiveBufferSize = SIZE_TABLE[index];\n                    decreaseNow = false;\n                } else {\n                    decreaseNow = true;\n                }\n            } else if (actualReadBytes >= nextReceiveBufferSize) {\n                // 扩容条件满足一次之后就进行扩容\n                index = min(index + INDEX_INCREMENT, maxIndex);\n                nextReceiveBufferSize = SIZE_TABLE[index];\n                decreaseNow = false;\n            }\n        }\n\n\n\n# 2.6 ByteBuf 的引用计数设计\n\nNetty 为 ByteBuf 引入了引用计数的机制，在 ByteBuf 的整个设计体系中，所有的 ByteBuf 都会继承一个抽象类 AbstractReferenceCountedByteBuf ， 它是对接口 ReferenceCounted 的实现。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\npublic interface ReferenceCounted {\n     int refCnt();\n     ReferenceCounted retain();\n     ReferenceCounted retain(int increment);\n     boolean release();\n     boolean release(int decrement);\n}\n\n\n每个 ByteBuf 的内部都维护了一个叫做 refCnt 的引用计数，我们可以通过 refCnt() 方法来获取 ByteBuf 当前的引用计数 refCnt。当 ByteBuf 在其他上下文中被引用的时候，我们需要通过 retain() 方法将 ByteBuf 的引用计数加 1。另外我们也可以通过 retain(int increment) 方法来指定 refCnt 增加的大小（increment）。\n\n有对 ByteBuf 的引用那么就有对 ByteBuf 的释放，每当我们使用完 ByteBuf 的时候就需要手动调用 release() 方法将 ByteBuf 的引用计数减 1 。当引用计数 refCnt 变成 0 的时候，Netty 就会通过 deallocate 方法来释放 ByteBuf 所引用的内存资源。这时 release() 方法会返回 true , 如果 refCnt 还不为 0 ，那么就返回 false 。同样我们也可以通过 release(int decrement) 方法来指定 refCnt 减少多少（decrement）。\n\n# 2.6.1 为什么要引入引用计数\n\n”在其他上下文中引用 ByteBuf “ 是什么意思呢 ？ 比如我们在线程 1 中创建了一个 ByteBuf，然后将这个 ByteBuf 丢给线程 2 进行处理，线程 2 又可能丢给线程 3， 而每个线程都有自己的上下文处理逻辑，比如对 ByteBuf 的处理，释放等操作。这样就使得 ByteBuf 在事实上形成了在多个线程上下文中被共享的情况。\n\n面对这种情况我们就很难在一个单独的线程上下文中判断一个 ByteBuf 该不该被释放，比如线程 1 准备释放 ByteBuf 了，但是它可能正在被其他线程使用。所以这也是 Netty 为 ByteBuf 引入引用计数的重要原因，每当引用一次 ByteBuf 的时候就需要通过 retain() 方法将引用计数加 1， release() 释放的时候将引用计数减 1 ，当引用计数为 0 了，说明已经没有其他上下文引用 ByteBuf 了，这时 Netty 就可以释放它了。\n\n另外相比于 JDK DirectByteBuffer 需要依赖 GC 机制来释放其背后引用的 Native Memory , Netty 更倾向于手动及时释放 DirectByteBuf 。因为 JDK DirectByteBuffer 的释放需要等到 GC 发生，由于 DirectByteBuffer 的对象实例所占的 JVM 堆内存太小了，所以一时很难触发 GC , 这就导致被引用的 Native Memory 的释放有了一定的延迟，严重的情况会越积越多，导致 OOM 。而且也会导致进程中对 DirectByteBuffer 的申请操作有非常大的延迟。\n\n而 Netty 为了避免这些情况的出现，选择在每次使用完毕之后手动释放 Native Memory ，但是不依赖 JVM 的话，总会有内存泄露的情况，比如在使用完了 ByteBuf 却忘记调用 release() 方法来释放。\n\n所以为了检测内存泄露的发生，这也是 Netty 为 ByteBuf 引入了引用计数的另一个原因，当 ByteBuf 不再被引用的时候，也就是没有任何强引用或者软引用的时候，如果此时发生 GC , 那么这个 ByteBuf 实例（位于 JVM 堆中）就需要被回收了，这时 Netty 就会检查这个 ByteBuf 的引用计数是否为 0 ， 如果不为 0 ，说明我们忘记调用 release() 释放了，近而判断出这个 ByteBuf 发生了内存泄露。\n\n在探测到内存泄露发生之后，后续 Netty 就会通过 reportLeak() 将内存泄露的相关信息以 error 的日志级别输出到日志中。\n\n看到这里，大家可能不禁要问，不就是引入了一个小小的引用计数嘛，这有何难 ？ 值得这里大书特书吗 ？ 不就是在创建 ByteBuf 的时候将引用计数 refCnt 初始化为 1 ， 每次在其他上下文引用的时候将 refCnt 加 1， 每次释放的时候再将 refCnt 减 1 吗 ？减到 0 的时候就释放 Native Memory ，太简单了吧~~\n\n事实上 Netty 对引用计数的设计非常讲究，绝非如此简单，甚至有些复杂，其背后隐藏着大大的性能考究以及对复杂并发问题的全面考虑，在性能与线程安全问题之间的反复权衡。\n\n# 2.6.2 引用计数的最初设计\n\n所以为了理清关于引用计数的整个设计脉络，我们需要将版本回退到最初的起点 —— 4.1.16.Final 版本，来看一下原始的设计。\n\npublic abstract class AbstractReferenceCountedByteBuf extends AbstractByteBuf {\n    // 原子更新 refCnt 的 Updater\n    private static final AtomicIntegerFieldUpdater<AbstractReferenceCountedByteBuf> refCntUpdater =\n            AtomicIntegerFieldUpdater.newUpdater(AbstractReferenceCountedByteBuf.class, \"refCnt\");\n    // 引用计数，初始化为 1\n    private volatile int refCnt;\n\n    protected AbstractReferenceCountedByteBuf(int maxCapacity) {\n        super(maxCapacity);\n        // 引用计数初始化为 1\n        refCntUpdater.set(this, 1);\n    }\n\n    // 引用计数增加 increment\n    private ByteBuf retain0(int increment) {\n        for (;;) {\n            int refCnt = this.refCnt;\n            // 每次 retain 的时候对引用计数加 1\n            final int nextCnt = refCnt + increment;\n\n            // Ensure we not resurrect (which means the refCnt was 0) and also that we encountered an overflow.\n            if (nextCnt <= increment) {\n                // 如果 refCnt 已经为 0 或者发生溢出，则抛异常\n                throw new IllegalReferenceCountException(refCnt, increment);\n            }\n            // CAS 更新 refCnt\n            if (refCntUpdater.compareAndSet(this, refCnt, nextCnt)) {\n                break;\n            }\n        }\n        return this;\n    }\n\n    // 引用计数减少 decrement\n    private boolean release0(int decrement) {\n        for (;;) {\n            int refCnt = this.refCnt;\n            if (refCnt < decrement) {\n                // 引用的次数必须和释放的次数相等对应\n                throw new IllegalReferenceCountException(refCnt, -decrement);\n            }\n            // 每次 release 引用计数减 1 \n            // CAS 更新 refCnt\n            if (refCntUpdater.compareAndSet(this, refCnt, refCnt - decrement)) {\n                if (refCnt == decrement) {\n                    // 如果引用计数为 0 ，则释放 Native Memory，并返回 true\n                    deallocate();\n                    return true;\n                }\n                // 引用计数不为 0 ，返回 false\n                return false;\n            }\n        }\n    }\n}\n\n\n在 4.1.16.Final 之前的版本设计中，确实和我们当初想象的一样，非常简单，创建 ByteBuf 的时候将 refCnt 初始化为 1。 每次引用 retain 的时候将引用计数加 1 ，每次释放 release 的时候将引用计数减 1，在一个 for 循环中通过 CAS 替换。当引用计数为 0 的时候，通过 deallocate() 释放 Native Memory。\n\n# 2.6.3 引入指令级别上的优化\n\n4.1.16.Final 的设计简洁清晰，在我们看来完全没有任何问题，但 Netty 对性能的考究完全没有因此止步，由于在 x86 架构下 XADD 指令的性能要高于 CMPXCHG 指令， compareAndSet 方法底层是通过 CMPXCHG 指令实现的，而 getAndAdd 方法底层是 XADD 指令。\n\n所以在对性能极致的追求下，Netty 在 4.1.17.Final 版本中用 getAndAdd 方法来替换 compareAndSet 方法。\n\npublic abstract class AbstractReferenceCountedByteBuf extends AbstractByteBuf {\n\n    private volatile int refCnt;\n\n    protected AbstractReferenceCountedByteBuf(int maxCapacity) {\n        super(maxCapacity);\n        // 引用计数在初始的时候还是为 1 \n        refCntUpdater.set(this, 1);\n    }\n\n    private ByteBuf retain0(final int increment) {\n        // 相比于 compareAndSet 的实现，这里将 for 循环去掉\n        // 并且每次是先对 refCnt 增加计数 increment\n        int oldRef = refCntUpdater.getAndAdd(this, increment);\n        // 增加完 refCnt 计数之后才去判断异常情况\n        if (oldRef <= 0 || oldRef + increment < oldRef) {\n            // Ensure we don't resurrect (which means the refCnt was 0) and also that we encountered an overflow.\n            // 如果原来的 refCnt 已经为 0 或者 refCnt 溢出，则对 refCnt 进行回退，并抛出异常\n            refCntUpdater.getAndAdd(this, -increment);\n            throw new IllegalReferenceCountException(oldRef, increment);\n        }\n        return this;\n    }\n\n    private boolean release0(int decrement) {\n        // 先对 refCnt 减少计数 decrement\n        int oldRef = refCntUpdater.getAndAdd(this, -decrement);\n        // 如果 refCnt 已经为 0 则进行 Native Memory 的释放\n        if (oldRef == decrement) {\n            deallocate();\n            return true;\n        } else if (oldRef < decrement || oldRef - decrement > oldRef) {\n            // 如果释放次数大于 retain 次数 或者 refCnt 出现下溢\n            // 则对 refCnt 进行回退，并抛出异常\n            refCntUpdater.getAndAdd(this, decrement);\n            throw new IllegalReferenceCountException(oldRef, decrement);\n        }\n        return false;\n    }\n}\n\n\n在 4.1.16.Final 版本的实现中，Netty 是在一个 for 循环中，先对 retain 和 release 的异常情况进行校验，之后再通过 CAS 更新 refCnt。否则直接抛出 IllegalReferenceCountException。采用的是一种悲观更新引用计数的策略。\n\n而在 4.1.17.Final 版本的实现中 ， Netty 去掉了 for 循环，正好和 compareAndSet 的实现相反，而是先通过 getAndAdd 更新 refCnt，更新之后再来判断相关的异常情况，如果发现有异常，则进行回退，并抛出 IllegalReferenceCountException。采用的是一种乐观更新引用计数的策略。\n\n比如在 retain 增加引用计数的时候，先对 refCnt 增加计数 increment，然后判断原来的引用计数 oldRef 是否已经为 0 或者 refCnt 是否发生溢出，如果是，则需要对 refCnt 的值进行回退，并抛异常。\n\n在 release 减少引用计数的时候，先对 refCnt 减少计数 decrement，然后判断 release 的次数是否大于 retain 的次数防止 over-release ，以及 refCnt 是否发生下溢，如果是，则对 refCnt 的值进行回退，并抛异常。\n\n# 2.6.4 并发安全问题的引入\n\n在 4.1.17.Final 版本的设计中，我们对引用计数的 retain 以及 release 操作都要比 4.1.16.Final 版本的性能要高，虽然现在性能是高了，但是同时引入了新的并发问题。\n\n让我们先假设一个这样的场景，现在有一个 ByteBuf，它当前的 refCnt = 1 ，线程 1 对这个 ByteBuf 执行 release() 操作。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n在 4.1.17.Final 的实现中，Netty 会首先通过 getAndAdd 将 refCnt 更新为 0 ，然后接着调用 deallocate() 方法释放 Native Memory ，很简单也很清晰是吧，让我们再加点并发复杂度上去。\n\n现在我们在上图步骤一与步骤二之间插入一个线程 2 ， 线程 2 对这个 ByteBuf 并发执行 retain() 方法。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n在 4.1.17.Final 的实现中，线程 2 首先通过 getAndAdd 将 refCnt 从 0 更新为 1，紧接着线程 2 就会发现 refCnt 原来的值 oldRef 是等于 0 的，也就是说线程 2 在调用 retain() 的时候，ByteBuf 的引用计数已经为 0 了，并且线程 1 已经开始准备释放 Native Memory 了。\n\n所以线程 2 需要再次调用 getAndAdd 方法将 refCnt 的值进行回退，从 1 再次回退到 0 ，最后抛出 IllegalReferenceCountException。这样的结果显然是正确的，也是符合语义的。毕竟不能对一个引用计数为 0 的 ByteBuf 调用 retain() 。\n\n现在看来一切风平浪静，都是按照我们的设想有条不紊的进行，我们不妨再加点并发复杂度上去。在上图步骤 1.1 与步骤 1.2 之间在插入一个线程 3 ， 线程 3 对这个 ByteBuf 再次并发执行 retain() 方法。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n由于引用计数的更新（步骤 1.1）与引用计数的回退（步骤 1.2）这两个操作并不是一个原子操作，如果在这两个操作之间不巧插入了一个线程 3 ，线程 3 在并发执行 retain() 方法的时候，首先会通过 getAndAdd 将引用计数 refCnt 从 1 增加到 2 。\n\n> 注意，此时线程 2 还没来得及回退 refCnt ， 所以线程 3 此时看到的 refCnt 是 1 而不是 0 。\n\n由于此时线程 3 看到的 oldRef 是 1 ，所以线程 3 成功调用 retain() 方法将 ByteBuf 的引用计数增加到了 2 ，并且不会回退也不会抛出异常。在线程 3 看来此时的 ByteBuf 完完全全是一个正常可以被使用的 ByteBuf。\n\n紧接着线程 1 开始执行步骤 2 —— deallocate() 方法释放 Native Memory，此后线程 3 在访问这个 ByteBuf 的时候就有问题了，因为 Native Memory 已经被线程1 释放了。\n\n# 2.6.5 在性能与并发安全之间的权衡\n\n接下来 Netty 就需要在性能与并发安全之间进行权衡了，现在有两个选择，第一个选择是直接回滚到 4.1.16.Final 版本，放弃 XADD 指令带来的性能提升，之前的设计中采用的 CMPXCHG 指令虽然性能相对差一些，但是不会出现上述的并发安全问题。\n\n因为 Netty 是在一个 for 循环中采用悲观的策略来更新引用计数，先是判断异常情况，然后在通过 CAS 来更新 refCnt。即使多个线程看到了 refCnt 的中间状态也没关系，因为接下来进行的 CAS 也会跟着失败。\n\n比如上边例子中的线程 1 对 ByteBuf 进行 release 的时候，在线程 1 执行 CAS 将 refCnt 替换为 0 之前的这个间隙中，refCnt 是 1 ，如果在这个间隙中，线程 2 并发执行 retain 方法，此时线程 2 看到的 refCnt 确实为 1 ，它是一个中间状态，线程 2 执行 CAS 将 refCnt 替换为 2。\n\n此时线程 1 执行 CAS 就会失败，但会在下一轮 for 循环中将 refCnt 替换为 1，这是完全符合引用计数语义的。\n\n另外一种情况是线程 1 已经执行完 CAS 将 refCnt 替换为 0 ，这时候线程 2 去 retain ，由于 4.1.16.Final 版本中的设计是先检查异常后 CAS 替换，所以线程 2 首先会在 retain 方法中检查到 ByteBuf 的 refCnt 已经为 0 ，直接抛出 IllegalReferenceCountException，并不会执行 CAS 。这同样符合引用计数的语义，毕竟不能对一个引用计数已经为 0 的 ByteBuf 执行任何访问操作。\n\n第二个选择是既要保留 XADD 指令带来的性能提升，也要解决 4.1.17.Final 版本中引入的并发安全问题。毫无疑问，Netty 最终选择的是这种方案。\n\n在介绍 Netty 的精彩设计之前，我想我们还是应该在回顾下这个并发安全问题出现的根本原因是什么 ？\n\n在 4.1.17.Final 版本的设计中，Netty 首先是通过 getAndAdd 方法先对 refCnt 的值进行更新，如果出现异常情况，在进行回滚。而更新，回滚的这两个操作并不是原子的，之间的中间状态会被其他线程看到。\n\n比如，线程 2 看到了线程 1 的中间状态（refCnt = 0），于是将引用计数加到 1 , 在线程 2 进行回滚之前，这期间的中间状态（refCnt = 1，oldRef = 0）又被线程 3 看到了，于是线程 3 将引用计数增加到了 2 （refCnt = 2，oldRef = 1）。 此时线程 3 觉得这是一种正常的状态，但在线程 1 看来 refCnt 的值已经是 0 了，后续线程 1 就会释放 Native Memory ，这就出问题了。\n\n问题的根本原因其实是这里的 refCnt 不同的值均代表不同的语义，比如对于线程 1 来说，通过 release 将 refCnt 减到了 0 ，这里的语义是 ByteBuf 已经不在被引用了，可以释放 Native Memory 。\n\n随后线程 2 通过 retain 将 refCnt 加到了 1 ，这就把 ByteBuf 语义改变了，表示该 ByteBuf 在线程 2 中被引用了一次。最后线程 3 又通过 retain 将 refCnt 加到了 2 ，再一次改变了 ByteBuf 的语义。\n\n只要用到 XADD 指令来实现引用计数的更新，那么就不可避免的出现上述并发更新 refCnt 的情况，关键是 refCnt 的值每一次被其他线程并发修改之后，ByteBuf 的语义就变了。这才是 4.1.17.Final 版本中的关键问题所在。\n\n如果 Netty 想在同时享受 XADD 指令带来的性能提升之外，又要解决上述提到的并发安全问题，就要重新对引用计数进行设计。首先我们的要求是继续采用 XADD 指令来实现引用计数的更新，但这就会带来多线程并发修改所引起的 ByteBuf 语义改变。\n\n既然多线程并发修改无法避免，那么我们能不能重新设计一下引用计数，让 ByteBuf 语义无论多线程怎么修改，它的语义始终保持不变。也就是说只要线程 1 将 refCnt 减到了 0 ，那么无论线程 2 和线程 3 怎么并发修改 refCnt，怎么增加 refCnt 的值，refCnt 等于 0 的这个语义始终保持不变呢 ？\n\n# 2.6.6 奇偶设计的引入\n\n这里 Netty 有一个极奇巧妙精彩的设计，引用计数的设计不再是逻辑意义上的 0 , 1 , 2 , 3 .....，而是分为了两大类，要么是偶数，要么是奇数。\n\n * 偶数代表的语义是 ByteBuf 的 refCnt 不为 0 ，也就是说只要一个 ByteBuf 还在被引用，那么它的 refCnt 就是一个偶数，具体被引用多少次，可以通过 refCnt >>> 1 来获取。\n * 奇数代表的语义是 ByteBuf 的 refCnt 等于 0 ，只要一个 ByteBuf 已经没有任何地方引用它了，那么它的 refCnt 就是一个奇数，其背后引用的 Native Memory 随后就会被释放。\n\nByteBuf 在初始化的时候，refCnt 不在是 1 而是被初始化为 2 （偶数），每次 retain 的时候不在是对 refCnt 加 1 而是加 2 （偶数步长），每次 release 的时候不再是对 refCnt 减 1 而是减 2 （同样是偶数步长）。这样一来，只要一个 ByteBuf 的引用计数为偶数，那么多线程无论怎么并发调用 retain 方法，引用计数还是一个偶数，语义仍然保持不变。\n\n   public final int initialValue() {\n        return 2;\n    }\n\n\n当一个 ByteBuf 被 release 到没有任何引用计数的时候，Netty 不在将 refCnt 设置为 0 而是设置为 1 （奇数），对于一个值为奇数的 refCnt，无论多线程怎么并发调用 retain 方法和 release 方法，引用计数还是一个奇数，ByteBuf 引用计数为 0 的这层语义一直会保持不变。\n\n我们还是以上图中所展示的并发安全问题为例，在新的引用计数设计方案中，首先线程 1 对 ByteBuf 执行 release 方法，Netty 会将 refCnt 设置为 1 （奇数）。\n\n线程 2 并发调用 retain 方法，通过 getAndAdd 将 refCnt 从 1 加到了 3 ，refCnt 仍然是一个奇数，按照奇数所表示的语义 —— ByteBuf 引用计数已经是 0 了，那么线程 2 就会在 retain 方法中抛出 IllegalReferenceCountException。\n\n线程 3 并发调用 retain 方法，通过 getAndAdd 将 refCnt 从 3 加到了 5，看到了没 ，在新方案的设计中，无论多线程怎么并发执行 retain 方法，refCnt 的值一直都只会是一个奇数，随后线程 3 在 retain 方法中抛出 IllegalReferenceCountException。这完全符合引用计数的并发语义。\n\n这个新的引用计数设计方案是在 4.1.32.Final 版本引入进来的，仅仅通过一个奇偶设计，就非常巧妙的解决了 4.1.17.Final 版本中存在的并发安全问题。现在新方案的核心设计要素我们已经清楚了，那么接下来笔者将以 4.1.56.Final 版本来为大家继续介绍下新方案的实现细节。\n\nNetty 中的 ByteBuf 全部继承于 AbstractReferenceCountedByteBuf，在这个类中实现了所有对 ByteBuf 引用计数的操作，对于 ReferenceCounted 接口的实现就在这里。\n\npublic abstract class AbstractReferenceCountedByteBuf extends AbstractByteBuf {\n    // 获取 refCnt 字段在 ByteBuf 对象内存中的偏移\n    // 后续通过 Unsafe 对 refCnt 进行操作\n    private static final long REFCNT_FIELD_OFFSET =\n            ReferenceCountUpdater.getUnsafeOffset(AbstractReferenceCountedByteBuf.class, \"refCnt\");\n\n    // 获取 refCnt 字段 的 AtomicFieldUpdater\n    // 后续通过 AtomicFieldUpdater 来操作 refCnt 字段\n    private static final AtomicIntegerFieldUpdater<AbstractReferenceCountedByteBuf> AIF_UPDATER =\n            AtomicIntegerFieldUpdater.newUpdater(AbstractReferenceCountedByteBuf.class, \"refCnt\");\n\n    // 创建 ReferenceCountUpdater，对于引用计数的所有操作最终都会代理到这个类中\n    private static final ReferenceCountUpdater<AbstractReferenceCountedByteBuf> updater =\n            new ReferenceCountUpdater<AbstractReferenceCountedByteBuf>() {\n        @Override\n        protected AtomicIntegerFieldUpdater<AbstractReferenceCountedByteBuf> updater() {\n            // 通过 AtomicIntegerFieldUpdater 操作 refCnt 字段\n            return AIF_UPDATER;\n        }\n        @Override\n        protected long unsafeOffset() {\n            // 通过 Unsafe 操作 refCnt 字段\n            return REFCNT_FIELD_OFFSET;\n        }\n    };\n    // ByteBuf 中的引用计数，初始为 2 （偶数）\n    private volatile int refCnt = updater.initialValue();\n}\n\n\n其中定义了一个 refCnt 字段用于记录 ByteBuf 被引用的次数，由于采用了奇偶设计，在创建 ByteBuf 的时候，Netty 会将 refCnt 初始化为 2 （偶数），它的逻辑语义是该 ByteBuf 被引用一次。后续对 ByteBuf 执行 retain 就会对 refCnt 进行加 2 ，执行 release 就会对 refCnt 进行减 2 ，对于引用计数的单次操作都是以 2 为步长进行。\n\n由于在 Netty 中除了 AbstractReferenceCountedByteBuf 这个专门用于实现 ByteBuf 的引用计数功能之外，还有一个更加通用的引用计数抽象类 AbstractReferenceCounted，它用于实现所有系统资源类的引用计数功能（ByteBuf 只是其中的一种内存资源）。\n\n由于都是对引用计数的实现，所以在之前的版本中，这两个类中包含了很多重复的引用计数相关操作逻辑，所以 Netty 在 4.1.35.Final 版本中专门引入了一个 ReferenceCountUpdater 类，将所有引用计数的相关实现聚合在这里。\n\nReferenceCountUpdater 对于引用计数 refCnt 的操作有两种方式，一种是通过 AtomicFieldUpdater 来对 refCnt 进行操作，我们可以通过 updater() 获取到 refCnt 字段对应的 AtomicFieldUpdater。\n\n另一种则是通过 Unsafe 来对 refCnt 进行操作，我们可以通过 unsafeOffset() 来获取到 refCnt 字段在 ByteBuf 实例对象内存中的偏移。\n\n按理来说，我们采用一种方式就可以对 refCnt 进行访问或者更新了，那为什么 Netty 提供了两种方式呢 ？会显得有点多余吗 ？这个点大家可以先思考下为什么 ，后续在我们剖析到源码细节的时候笔者在为大家解答。\n\n好了，下面我们正式开始介绍新版引用计数设计方案的具体实现细节，第一个问题，在新的设计方案中，我们如何获取 ByteBuf 的逻辑引用计数 ？\n\npublic abstract class ReferenceCountUpdater<T extends ReferenceCounted> {\n    public final int initialValue() {\n        // ByteBuf 引用计数初始化为 2\n        return 2;\n    }\n\n    public final int refCnt(T instance) {\n        // 通过 updater 获取 refCnt\n        // 根据 refCnt 在  realRefCnt 中获取真实的引用计数\n        return realRefCnt(updater().get(instance));\n    }\n    // 获取 ByteBuf 的逻辑引用计数\n    private static int realRefCnt(int rawCnt) {\n        // 奇偶判断\n        return rawCnt != 2 && rawCnt != 4 && (rawCnt & 1) != 0 ? 0 : rawCnt >>> 1;\n    }\n}\n\n\n由于采用了奇偶引用计数的设计，所以我们在获取逻辑引用计数的时候需要判断当前 rawCnt（refCnt）是奇数还是偶数，它们分别代表了不同的语义。\n\n * 如果 rawCnt 是奇数，则表示当前 ByteBuf 已经没有任何地方引用了，逻辑引用计数返回 0.\n * 如果 rawCnt 是偶数，则表示当前 ByteBuf 还有地方在引用，逻辑引用计数则为 rawCnt >>> 1。\n\nrealRefCnt 函数其实就是简单的一个奇偶判断逻辑，但在它的实现中却体现出了 Netty 对性能的极致追求。比如，我们判断一个数是奇数还是偶数其实很简单，直接通过 rawCnt & 1 就可以判断，如果返回 0 表示 rawCnt 是一个偶数，如果返回 1 表示 rawCnt 是一个奇数。\n\n但是我们看到 Netty 在奇偶判断条件的前面又加上了 rawCnt != 2 && rawCnt != 4语句，这是干嘛的呢 ？\n\n其实 Netty 这里是为了尽量用性能更高的 == 运算来代替 & 运算，但又不可能用 == 运算来枚举出所有的偶数值（也没这必要），所以只用 == 运算来判断在实际场景中经常出现的引用计数，一般经常出现的引用计数值为 2 或者 4 ， 也就是说 ByteBuf 在大部分场景下只会被引用 1 次或者 2 次，对于这种高频出现的场景，Netty 用 == 运算来针对性优化，低频出现的场景就回退到 & 运算。\n\n> 大部分性能优化的套路都是相同的，我们通常不能一上来就奢求一个大而全的针对全局的优化方案，这是不可能的，也是十分低效的。往往最有效的，可以立竿见影的优化方案都是针对局部热点进行专门优化。\n\n对引用计数的设置也是一样，都需要考虑奇偶的转换，我们在 setRefCnt 方法中指定的参数 refCnt 表示逻辑上的引用计数 —— 0, 1 , 2 , 3 ....，但要设置到 ByteBuf 时，就需要对逻辑引用计数在乘以 2 ，让它始终是一个偶数。\n\n    public final void setRefCnt(T instance, int refCnt) {\n        updater().set(instance, refCnt > 0 ? refCnt << 1 : 1); // overflow OK here\n    }\n\n\n有了这些基础之后，我们下面就来看一下在新版本的 retain 方法设计中，Netty 是如何解决 4.1.17.Final 版本存在的并发安全问题。首先 Netty 对引用计数的奇偶设计对于用户来说是透明的。引用计数对于用户来说仍然是普通的自然数 —— 0, 1 , 2 , 3 .... 。\n\n所以每当用户调用 retain 方法试图增加 ByteBuf 的引用计数时，通常是指定逻辑增加步长 —— increment（用户视角），而在具体的实现角度，Netty 会增加两倍的 increment （rawIncrement）到 refCnt 字段中。\n\n    public final T retain(T instance) {\n        // 引用计数逻辑上是加 1 ，但实际上是加 2 （实现角度）\n        return retain0(instance, 1, 2);\n    }\n\n    public final T retain(T instance, int increment) {\n        // all changes to the raw count are 2x the \"real\" change - overflow is OK\n        // rawIncrement 始终是逻辑计数 increment 的两倍\n        int rawIncrement = checkPositive(increment, \"increment\") << 1;\n        // 将 rawIncrement 设置到 ByteBuf 的 refCnt 字段中\n        return retain0(instance, increment, rawIncrement);\n    }\n\n    // rawIncrement = increment << 1\n    // increment 表示引用计数的逻辑增长步长\n    // rawIncrement 表示引用计数的实际增长步长\n    private T retain0(T instance, final int increment, final int rawIncrement) {\n        // 先通过 XADD 指令将  refCnt 的值加起来\n        int oldRef = updater().getAndAdd(instance, rawIncrement);\n        // 如果 oldRef 是一个奇数，也就是 ByteBuf 已经没有引用了，抛出异常\n        if (oldRef != 2 && oldRef != 4 && (oldRef & 1) != 0) {\n            // 如果 oldRef 已经是一个奇数了，无论多线程在这里怎么并发 retain ，都是一个奇数，这里都会抛出异常\n            throw new IllegalReferenceCountException(0, increment);\n        }\n        // don't pass 0! \n        // refCnt 不可能为 0 ，只能是 1\n        if ((oldRef <= 0 && oldRef + rawIncrement >= 0)\n                || (oldRef >= 0 && oldRef + rawIncrement < oldRef)) {\n            // 如果 refCnt 字段已经溢出，则进行回退，并抛异常\n            updater().getAndAdd(instance, -rawIncrement);\n            throw new IllegalReferenceCountException(realRefCnt(oldRef), increment);\n        }\n        return instance;\n    }\n\n\n首先新版本的 retain0 方法仍然保留了 4.1.17.Final 版本引入的 XADD 指令带来的性能优势，大致的处理逻辑也是类似的，一上来先通过 getAndAdd 方法将 refCnt 增加 rawIncrement，对于 retain(T instance) 来说这里直接加 2 。\n\n然后判断原来的引用计数 oldRef 是否是一个奇数，如果是一个奇数，那么就表示 ByteBuf 已经没有任何引用了，逻辑引用计数早已经为 0 了，那么就抛出 IllegalReferenceCountException。\n\n在引用计数为奇数的情况下，无论多线程怎么对 refCnt 并发加 2 ，refCnt 始终是一个奇数，最终都会抛出异常。解决并发安全问题的要点就在这里，一定要保证 retain 方法的并发执行不能改变原来的语义。\n\n最后会判断一下 refCnt 字段是否发生溢出，如果溢出，则进行回退，并抛出异常。下面我们仍然以之前的并发场景为例，用一个具体的例子，来回味一下奇偶设计的精妙之处。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n现在线程 1 对一个 refCnt 为 2 的 ByteBuf 执行 release 方法，这时 ByteBuf 的逻辑引用计数就为 0 了，对于一个没有任何引用的 ByteBuf 来说，新版的设计中它的 refCnt 只能是一个奇数，不能为 0 ，所以这里 Netty 会将 refCnt 设置为 1 。然后在步骤 2 中调用 deallocate 方法释放 Native Memory。\n\n线程 2 在步骤 1 和步骤 2 之间插入进来对 ByteBuf 并发执行 retain 方法，这时线程 2 看到的 refCnt 是 1，然后通过 getAndAdd 将 refCnt 加到了 3 ，仍然是一个奇数，随后抛出 IllegalReferenceCountException 异常。\n\n线程 3 在步骤 1.1 和步骤 1.2 之间插入进来再次对 ByteBuf 并发执行 retain 方法，这时线程 3 看到的 refCnt 是 3，然后通过 getAndAdd 将 refCnt 加到了 5 ，还是一个奇数，随后抛出 IllegalReferenceCountException 异常。\n\n这样一来就保证了引用计数的并发语义 —— 只要一个 ByteBuf 没有任何引用的时候（refCnt = 1），其他线程无论怎么并发执行 retain 方法都会得到一个异常。\n\n但是引用计数并发语义的保证不能单单只靠 retain 方法，它还需要与 release 方法相互配合协作才可以，所以为了并发语义的保证 ， release 方法的设计就不能使用性能更高的 XADD 指令，而是要回退到 CMPXCHG 指令来实现。\n\n为什么这么说呢 ？因为新版引用计数的设计采用的是奇偶实现，refCnt 为偶数表示 ByteBuf 还有引用，refCnt 为奇数表示 ByteBuf 已经没有任何引用了，可以安全释放 Native Memory 。对于一个 refCnt 已经为奇数的 ByteBuf 来说，无论多线程怎么并发执行 retain 方法，得到的 refCnt 仍然是一个奇数，最终都会抛出 IllegalReferenceCountException，这就是引用计数的并发语义 。\n\n为了保证这一点，就需要在每次调用 retain ，release 方法的时候，以偶数步长来更新 refCnt，比如每一次调用 retain 方法就对 refCnt 加 2 ，每一次调用 release 方法就对 refCnt 减 2 。\n\n但总有一个时刻，refCnt 会被减到 0 的对吧，在新版的奇偶设计中，refCnt 是不允许为 0 的，因为一旦 refCnt 被减到了 0 ，多线程并发执行 retain 之后，就会将 refCnt 再次加成了偶数，这又会出现并发问题。\n\n而每一次调用 release 方法是对 refCnt 减 2 ，如果我们采用 XADD 指令实现 release 的话，回想一下 4.1.17.Final 版本中的设计，它首先进来是通过 getAndAdd 方法对 refCnt 减 2 ，这样一来，refCnt 就变成 0 了，就有并发安全问题了。所以我们需要通过 CMPXCHG 指令将 refCnt 更新为 1。\n\n这里有的同学可能要问了，那可不可以先进行一下 if 判断，如果 refCnt 减 2 之后变为 0 了，我们在通过 getAndAdd 方法将 refCnt 更新为 1 （减一个奇数），这样一来不也可以利用上 XADD 指令的性能优势吗 ？\n\n答案是不行的，因为 if 判断与 getAndAdd 更新这两个操作之间仍然不是原子的，多线程可以在这个间隙仍然有并发执行 retain 方法的可能，如下图所示：\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n在线程 1 执行 if 判断和 getAndAdd 更新这两个操作之间，线程 2 看到的 refCnt 其实 2 ，然后线程 2 会将 refCnt 加到 4 ，线程 3 紧接着会将 refCnt 增加到 6 ，在线程 2 和线程 3 看来这个 ByteBuf 完全是正常的，但是线程 1 马上就会释放 Native Memory 了。\n\n而且采用这种设计的话，一会通过 getAndAdd 对 refCnt 减一个奇数，一会通过 getAndAdd 对 refCnt 加一个偶数，这样就把原本的奇偶设计搞乱掉了。\n\n所以我们的设计目标是一定要保证在 ByteBuf 没有任何引用计数的时候，release 方法需要原子性的将 refCnt 更新为 1 。 因此必须采用 CMPXCHG 指令来实现而不能使用 XADD 指令。\n\n再者说， CMPXCHG 指令是可以原子性的判断当前是否有并发情况的，如果有并发情况出现，CAS 就会失败，我们可以继续重试。但 XADD 指令却无法原子性的判断是否有并发情况，因为它每次都是先更新，后判断并发，这就不是原子的了。这一点，在下面的源码实现中会体现的特别明显。\n\n# 2.6.7 尽量避免内存屏障的开销\n\n    public final boolean release(T instance) {\n        // 第一次尝试采用 unSafe nonVolatile 的方式读取 refCnf 的值\n        int rawCnt = nonVolatileRawCnt(instance);\n        // 如果逻辑引用计数被减到 0 了，那么就通过 tryFinalRelease0 使用 CAS 将 refCnf 更新为 1\n        // CAS 失败的话，则通过 retryRelease0 进行重试\n        // 如果逻辑引用计数不为 0 ，则通过 nonFinalRelease0 将 refCnf 减 2\n        return rawCnt == 2 ? tryFinalRelease0(instance, 2) || retryRelease0(instance, 1)\n                : nonFinalRelease0(instance, 1, rawCnt, toLiveRealRefCnt(rawCnt, 1));\n    }\n\n\n这里有一个小的细节再次体现出 Netty 对于性能的极致追求，refCnt 字段在 ByteBuf 中被 Netty 申明为一个 volatile 字段。\n\nprivate volatile int refCnt = updater.initialValue();\n\n\n我们对 refCnt 的普通读写都是要走内存屏障的，但 Netty 在 release 方法中首次读取 refCnt 的值是采用 nonVolatile 的方式，不走内存屏障，直接读取 cache line，避免了屏障开销。\n\n    private int nonVolatileRawCnt(T instance) {\n        // 获取 REFCNT_FIELD_OFFSET\n        final long offset = unsafeOffset();\n        // 通过 UnSafe 的方式来访问 refCnt ， 避免内存屏障的开销\n        return offset != -1 ? PlatformDependent.getInt(instance, offset) : updater().get(instance);\n    }\n\n\n那有的同学可能要问了，如果读取 refCnt 的时候不走内存屏障的话，读取到的 refCnt 不就可能是一个错误的值吗 ？\n\n事实上确实是这样的，但 Netty 不 care , 读到一个错误的值也无所谓，因为这里的引用计数采用了奇偶设计，我们在第一次读取引用计数的时候并不需要读取到一个精确的值，既然这样我们可以直接通过 UnSafe 来读取，还能剩下一笔内存屏障的开销。\n\n那为什么不需要一个精确的值呢 ？因为如果原来的 refCnt 是一个奇数，那无论多线程怎么并发 retain ，最终得到的还是一个奇数，我们这里只需要知道 refCnt 是一个奇数就可以直接抛 IllegalReferenceCountException 了。具体读到的是一个 3 还是一个 5 其实都无所谓。\n\n那如果原来的 refCnt 是一个偶数呢 ？其实也无所谓，我们可能读到一个正确的值也可能读到一个错误的值，如果恰好读到一个正确的值，那更好。如果读取到一个错误的值，也无所谓，因为我们后面是用 CAS 进行更新，这样的话 CAS 就会更新失败，我们只需要在一下轮 for 循环中更新正确就可以了。\n\n如果读取到的 refCnt 恰好是 2 ，那就意味着本次 release 之后，ByteBuf 的逻辑引用计数就为 0 了，Netty 会通过 CAS 将 refCnt 更新为 1 。\n\n   private boolean tryFinalRelease0(T instance, int expectRawCnt) {\n        return updater().compareAndSet(instance, expectRawCnt, 1); // any odd number will work\n    }\n\n\n如果 CAS 更新失败，则表示此时有多线程可能并发对 ByteBuf 执行 retain 方法，逻辑引用计数此时可能就不为 0 了，针对这种并发情况，Netty 会在 retryRelease0 方法中进行重试，将 refCnt 减 2 。\n\n    private boolean retryRelease0(T instance, int decrement) {\n        for (;;) {\n            // 采用 Volatile 的方式读取 refCnt\n            int rawCnt = updater().get(instance), \n            // 获取逻辑引用计数，如果 refCnt 已经变为奇数，则抛出异常\n            realCnt = toLiveRealRefCnt(rawCnt, decrement);\n            // 如果执行完本次 release , 逻辑引用计数为 0\n            if (decrement == realCnt) {\n                // CAS 将 refCnt 更新为 1\n                if (tryFinalRelease0(instance, rawCnt)) {\n                    return true;\n                }\n            } else if (decrement < realCnt) {\n                // 原来的逻辑引用计数 realCnt 大于 1（decrement）\n                // 则通过 CAS 将 refCnt 减 2\n                if (updater().compareAndSet(instance, rawCnt, rawCnt - (decrement << 1))) {\n                    return false;\n                }\n            } else {\n                // refCnt 字段如果发生溢出，则抛出异常\n                throw new IllegalReferenceCountException(realCnt, -decrement);\n            }\n            // CAS 失败之后调用 yield\n            // 减少无畏的竞争，否则所有线程在高并发情况下都在这里 CAS 失败\n            Thread.yield(); \n        }\n    }\n\n\n从 retryRelease0 方法的实现中我们可以看出，CAS 是可以原子性的探测到是否有并发情况出现的，如果有并发情况，这里的所有 CAS 都会失败，随后会在下一轮 for 循环中将正确的值更新到 refCnt 中。这一点 ，XADD 指令是做不到的。\n\n如果在进入 release 方法后，第一次读取的 refCnt 不是 2 ，那么就不能走上面的 tryFinalRelease0 逻辑，而是在 nonFinalRelease0 中通过 CAS 将 refCnt 的值减 2 。\n\n   private boolean nonFinalRelease0(T instance, int decrement, int rawCnt, int realCnt) {\n        if (decrement < realCnt\n                && updater().compareAndSet(instance, rawCnt, rawCnt - (decrement << 1))) {\n            // ByteBuf 的 rawCnt 减少 2 * decrement\n            return false;\n        }\n        // CAS  失败则一直重试，如果引用计数已经为 0 ，那么抛出异常，不能再次 release\n        return retryRelease0(instance, decrement);\n    }\n\n\n到这里，Netty 对引用计数的精彩设计，笔者就为大家完整的剖析完了，一共有四处非常精彩的优化设计，我们总结如下：\n\n 1. 使用性能更优的 XADD 指令来替换 CMPXCHG 指令。\n 2. 引用计数采用了奇偶设计，保证了并发语义。\n 3. 采用性能更优的 == 运算来替换 & 运算。\n 4. 能不走内存屏障就尽量不走内存屏障。\n\n\n# 2.7 ByteBuf 的视图设计\n\n和 JDK 的设计一样，Netty 中的 ByteBuf 也可以通过 slice() 方法以及 duplicate() 方法创建一个视图 ByteBuf 出来，原生 ByteBuf 和它的视图 ByteBuf 底层都是共用同一片内存区域，也就是说在视图 ByteBuf 上做的任何改动都会反应到原生 ByteBuf 上。同理，在原生 ByteBuf 上做的任何改动也会反应到它的视图 ByteBuf 上。我们可以将视图 ByteBuf 看做是原生 ByteBuf 的一份浅拷贝。\n\n原生 ByteBuf 和它的视图 ByteBuf 不同的是，它们都有各自独立的 readerIndex，writerIndex，capacity，maxCapacity。\n\nslice() 方法是在原生 ByteBuf 的 [readerIndex , writerIndex) 这段内存区域内创建一个视图 ByteBuf。也就是原生 ByteBuf 和视图 ByteBuf 共用 [readerIndex , writerIndex) 这段内存区域。视图 ByteBuf 的数据区域其实就是原生 ByteBuf 的可读字节区域。\n\n视图 ByteBuf 的 readerIndex = 0 ， writerIndex = capacity = maxCapacity = 原生 ByteBuf 的 readableBytes() 。\n\n  @Override\n    public int readableBytes() {\n        // 原生 ByteBuf\n        return writerIndex - readerIndex;\n    }\n\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n下面我们来看一下 slice()方法创建视图 ByteBuf 的逻辑实现：\n\npublic abstract class AbstractByteBuf extends ByteBuf {\n    @Override\n    public ByteBuf slice() {\n        return slice(readerIndex, readableBytes());\n    }\n\n    @Override\n    public ByteBuf slice(int index, int length) {\n        // 确保 ByteBuf 的引用计数不为 0 \n        ensureAccessible();\n        return new UnpooledSlicedByteBuf(this, index, length);\n    }\n}\n\n\nNetty 会将 slice 视图 ByteBuf 封装在 UnpooledSlicedByteBuf 类中，在这里会初始化 slice 视图 ByteBuf 的 readerIndex，writerIndex，capacity，maxCapacity。\n\nclass UnpooledSlicedByteBuf extends AbstractUnpooledSlicedByteBuf {\n    UnpooledSlicedByteBuf(AbstractByteBuf buffer, int index, int length) {\n        // index = readerIndex\n        // length = readableBytes()\n        super(buffer, index, length);\n    }\n\n    @Override\n    public int capacity() {\n        // 视图 ByteBuf 的 capacity 和 maxCapacity 相等\n        // 均为原生 ByteBuf 的 readableBytes() \n        return maxCapacity();\n    }\n}\n\n\n如上图所示，这里的 index 就是原生 ByteBuf 的 readerIndex = 4 ，index 用于表示视图 ByteBuf 的内存区域相对于原生 ByteBuf 的偏移，因为视图 ByteBuf 与原生 ByteBuf 共用的是同一片内存区域，针对视图 ByteBuf 的操作其实底层最终是转换为对原生 ByteBuf 的操作。\n\n但由于视图 ByteBuf 和原生 ByteBuf 各自都有独立的 readerIndex 和 writerIndex，比如上图中，视图 ByteBuf 中的 readerIndex = 0 其实指向的是原生 ByteBuf 中 readerIndex = 4 的位置。所以每次在我们对视图 ByteBuf 进行读写的时候都需要将视图 ByteBuf 的 readerIndex 加上一个偏移（index）转换成原生 ByteBuf 的 readerIndex，近而从原生 ByteBuf 中来读写数据。\n\n   @Override\n    protected byte _getByte(int index) {\n        // 底层其实是对原生 ByteBuf 的访问\n        return unwrap()._getByte(idx(index));\n    }\n\n    @Override\n    protected void _setByte(int index, int value) {\n        unwrap()._setByte(idx(index), value);\n    }\n\n   /**\n     * Returns the index with the needed adjustment.\n     */\n    final int idx(int index) {\n        // 转换为原生 ByteBuf 的 readerIndex 或者 writerIndex\n        return index + adjustment;\n    }\n\n\nidx(int index) 方法中的 adjustment 就是上面 UnpooledSlicedByteBuf 构造函数中的 index 偏移，初始化为原生 ByteBuf 的 readerIndex。\n\nlength 则初始化为原生 ByteBuf 的 readableBytes()，视图 ByteBuf 中的 writerIndex，capacity，maxCapacity 都是用 length 来初始化。\n\nabstract class AbstractUnpooledSlicedByteBuf extends AbstractDerivedByteBuf {\n    // 原生 ByteBuf\n    private final ByteBuf buffer;\n    // 视图 ByteBuf 相对于原生 ByteBuf的数据区域偏移\n    private final int adjustment;\n\n    AbstractUnpooledSlicedByteBuf(ByteBuf buffer, int index, int length) {\n        // 设置视图 ByteBuf 的 maxCapacity，readerIndex 为 0 \n        super(length);\n        // 原生 ByteBuf\n        this.buffer = buffer;\n        // 数据偏移为原生 ByteBuf 的 readerIndex\n        adjustment = index;\n        // 设置视图 ByteBuf 的 writerIndex\n        writerIndex(length);\n    }\n}\n\n\n但是通过 slice() 方法创建出来的视图 ByteBuf 并不会改变原生 ByteBuf 的引用计数，这会存在一个问题，就是由于视图 ByteBuf 和原生 ByteBuf 底层共用的是同一片内存区域，在原生 ByteBuf 或者视图 ByteBuf 各自的应用上下文中他们可能并不会意识到对方的存在。\n\n如果对原生 ByteBuf 调用 release 方法，恰好引用计数就为 0 了，接着就会释放原生 ByteBuf 的 Native Memory 。此时再对视图 ByteBuf 进行访问就有问题了，因为 Native Memory 已经被原生 ByteBuf 释放了。同样的道理，对视图 ByteBuf 调用 release 方法 ，也会对原生 ByteBuf 产生影响。\n\n为此 Netty 提供了一个 retainedSlice() 方法，在创建 slice 视图 ByteBuf 的同时对原生 ByteBuf 的引用计数加 1 ，两者共用同一个引用计数。\n\n    @Override\n    public ByteBuf retainedSlice() {\n        // 原生 ByteBuf 的引用计数加 1\n        return slice().retain();\n    }\n\n\n除了 slice() 之外，Netty 也提供了 duplicate() 方法来创建视图 ByteBuf 。\n\n    @Override\n    public ByteBuf duplicate() {\n        // 确保 ByteBuf 的引用计数不为 0 \n        ensureAccessible();\n        return new UnpooledDuplicatedByteBuf(this);\n    }\n\n\n但和 slice() 不同的是， duplicate() 是完全复刻了原生 ByteBuf，复刻出来的视图 ByteBuf 虽然与原生 ByteBuf 都有各自独立的 readerIndex，writerIndex，capacity，maxCapacity。但他们的值都是相同的。duplicate 视图 ByteBuf 也是和原生 ByteBuf 共用同一块 Native Memory 。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\npublic class DuplicatedByteBuf extends AbstractDerivedByteBuf {\n    // 原生 ByteBuf\n    private final ByteBuf buffer;\n\n    public DuplicatedByteBuf(ByteBuf buffer) {\n        this(buffer, buffer.readerIndex(), buffer.writerIndex());\n    }\n\n    DuplicatedByteBuf(ByteBuf buffer, int readerIndex, int writerIndex) {\n        // 初始化视图 ByteBuf 的 maxCapacity 与原生的相同\n        super(buffer.maxCapacity());\n        // 原生 ByteBuf\n        this.buffer = buffer;\n        // 视图 ByteBuf 的 readerIndex ， writerIndex 也与原生相同\n        setIndex(readerIndex, writerIndex);\n        markReaderIndex();\n        markWriterIndex();\n    }\n\n    @Override\n    public int capacity() {\n        // 视图 ByteBuf 的 capacity 也与原生相同\n        return unwrap().capacity();\n    }\n\n}\n\n\nNetty 同样也提供了对应的 retainedDuplicate() 方法，用于创建 duplicate 视图 ByteBuf 的同时增加原生 ByteBuf 的引用计数。视图 ByteBuf 与原生 ByteBuf 之间共用同一个引用计数。\n\n   @Override\n    public ByteBuf retainedDuplicate() {\n        return duplicate().retain();\n    }\n\n\n上面介绍的两种视图 ByteBuf 可以理解为是对原生 ByteBuf 的一层浅拷贝，Netty 也提供了 copy() 方法来实现对原生 ByteBuf 的深拷贝，copy 出来的 ByteBuf 是原生 ByteBuf 的一个副本，两者底层依赖的 Native Memory 是不同的，各自都有独立的 readerIndex，writerIndex，capacity，maxCapacity 。\n\npublic abstract class AbstractByteBuf extends ByteBuf {\n    @Override\n    public ByteBuf copy() {\n        // 从原生 ByteBuf 中的 readerIndex 开始，拷贝 readableBytes 个字节到新的 ByteBuf 中\n        return copy(readerIndex, readableBytes());\n    }\n}\n\n\ncopy() 方法是对原生 ByteBuf 的 [readerIndex , writerIndex)这段数据范围内容进行拷贝。copy 出来的 ByteBuf，它的 readerIndex = 0 ， writerIndex = capacity = 原生 ByteBuf 的 readableBytes()。maxCapacity 与原生 maxCapacity 相同。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\npublic class UnpooledDirectByteBuf  {\n  @Override\n    public ByteBuf copy(int index, int length) {\n        ensureAccessible();\n        ByteBuffer src;\n        try {\n            // 将原生 ByteBuf 中 [index , index + lengh) 这段范围的数据拷贝到新的 ByteBuf 中\n            src = (ByteBuffer) buffer.duplicate().clear().position(index).limit(index + length);\n        } catch (IllegalArgumentException ignored) {\n            throw new IndexOutOfBoundsException(\"Too many bytes to read - Need \" + (index + length));\n        }\n        // 首先新申请一段 native memory , 新的 ByteBuf 初始容量为 length (真实容量)，最大容量与原生 ByteBuf 的 maxCapacity 相等\n        // readerIndex = 0 , writerIndex = length\n        return alloc().directBuffer(length, maxCapacity()).writeBytes(src);\n    }\n}\n\n\n\n# 2.8 CompositeByteBuf 的零拷贝设计\n\n这里的零拷贝并不是我们经常提到的那种 OS 层面上的零拷贝，而是 Netty 在用户态层面自己实现的避免内存拷贝的设计。比如在传统意义上，如果我们想要将多个独立的 ByteBuf 聚合成一个 ByteBuf 的时候，我们首先需要向 OS 申请一段更大的内存，然后依次将多个 ByteBuf 中的内容拷贝到这段新申请的内存上，最后在释放这些 ByteBuf 的内存。\n\n这样一来就涉及到两个性能开销点，一个是我们需要向 OS 重新申请更大的内存，另一个是内存的拷贝。Netty 引入 CompositeByteBuf 的目的就是为了解决这两个问题。巧妙地利用原有 ByteBuf 所占的内存，在此基础之上，将它们组合成一个逻辑意义上的 CompositeByteBuf ，提供一个统一的逻辑视图。\n\nCompositeByteBuf 其实也是一种视图 ByteBuf ，这一点和上小节中我们介绍的 SlicedByteBuf ， DuplicatedByteBuf 一样，它们本身并不会占用 Native Memory，底层数据的存储全部依赖于原生的 ByteBuf。\n\n不同点在于，SlicedByteBuf，DuplicatedByteBuf 它们是在单一的原生 ByteBuf 基础之上创建出的视图 ByteBuf。而 CompositeByteBuf 是基于多个原生 ByteBuf 创建出的统一逻辑视图 ByteBuf。\n\nCompositeByteBuf 对于我们用户来说和其他的普通 ByteBuf 没有任何区别，有自己独立的 readerIndex，writerIndex，capacity，maxCapacity，前面几个小节中介绍的各种 ByteBuf 的设计要素，在 CompositeByteBuf 身上也都会体现。\n\n但从实现的角度来说，CompositeByteBuf 只是一个逻辑上的 ByteBuf，其本身并不会占用任何的 Native Memory ，对于 CompositeByteBuf 的任何操作，最终都需要转换到其内部具体的 ByteBuf 上。本小节我们就来深入到 CompositeByteBuf 的内部，来看一下 Netty 的巧妙设计。\n\n# 2.8.1 CompositeByteBuf 的总体架构\n\n从总体设计上来讲，CompositeByteBuf 包含如下五个重要属性，其中最为核心的就是 components 数组，那些需要被聚合的原生 ByteBuf 会被 Netty 封装在 Component 类中，并统一组织在 components 数组中。后续针对 CompositeByteBuf 的所有操作都需要和这个数组打交道。\n\npublic class CompositeByteBuf extends AbstractReferenceCountedByteBuf implements Iterable<ByteBuf> {\n    // 内部 ByteBuf 的分配器，用于后续扩容，copy , 合并等操作\n    private final ByteBufAllocator alloc;\n    // compositeDirectBuffer 还是 compositeHeapBuffer ?\n    private final boolean direct;\n    // 最大的 components 数组容量（16）\n    private final int maxNumComponents;\n    // 当前 CompositeByteBuf 中包含的 components 个数\n    private int componentCount;\n    // 存储 component 的数组\n    private Component[] components; // resized when needed\n}\n\n\nmaxNumComponents 表示 components 数组最大的容量，CompositeByteBuf 默认能够包含 Component 的最大个数为 16，如果超过这个数量的话，Netty 会将当前 CompositeByteBuf 中包含的所有 Components 重新合并成一个更大的 Component。\n\npublic abstract class AbstractByteBufAllocator implements ByteBufAllocator {\n    static final int DEFAULT_MAX_COMPONENTS = 16;\n}\n\n\ncomponentCount 表示当前 CompositeByteBuf 中包含的 Component 个数。每当我们通过 addComponent 方法向 CompositeByteBuf 添加一个新的 ByteBuf 时，Netty 都会用一个新的 Component 实例来包装这个 ByteBuf，然后存放在 components 数组中，最后 componentCount 的个数加 1 。\n\nCompositeByteBuf 与其底层聚合的真实 ByteBuf 架构设计关系，如下图所示：\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n而创建一个 CompositeByteBuf 的核心其实就是创建底层的 components 数组，后续添加到该 CompositeByteBuf 的所有原生 ByteBuf 都会被组织在这里。\n\n   private CompositeByteBuf(ByteBufAllocator alloc, boolean direct, int maxNumComponents, int initSize) {\n        // 设置 maxCapacity\n        super(AbstractByteBufAllocator.DEFAULT_MAX_CAPACITY);\n\n        this.alloc = ObjectUtil.checkNotNull(alloc, \"alloc\");\n        this.direct = direct;\n        this.maxNumComponents = maxNumComponents;\n        // 初始 Component 数组的容量为 maxNumComponents\n        components = newCompArray(initSize, maxNumComponents);\n    }\n\n\n这里的参数 initSize 表示的并不是 CompositeByteBuf 所包含的字节数，而是初始包装的原生 ByteBuf 个数，也就是初始 Component 的个数。components 数组的总体大小由参数 maxNumComponents 决定，但不能超过 16 。\n\n   private static Component[] newCompArray(int initComponents, int maxNumComponents) {\n        // MAX_COMPONENT\n        int capacityGuess = Math.min(AbstractByteBufAllocator.DEFAULT_MAX_COMPONENTS, maxNumComponents);\n        // 初始 Component 数组的容量为 maxNumComponents\n        return new Component[Math.max(initComponents, capacityGuess)];\n    }\n\n\n现在我们只是清楚了 CompositeByteBuf 的一个基本骨架，那么接下来 Netty 如何根据这个基本的骨架将多个原生 ByteBuf 组装成一个逻辑上的统一视图 ByteBuf 呢 ？\n\n也就是说我们依据 CompositeByteBuf 中的 readerIndex 以及 writerIndex 进行的读写操作逻辑如何转换到对应的底层原生 ByteBuf 之上呢 ？ 这个是整个设计的核心所在。\n\n下面笔者就带着大家从外到内，从易到难地一一拆解 CompositeByteBuf 中的那些核心设计要素。从 CompositeByteBuf 的最外层来看，其实我们并不陌生，对于用户来说它就是一个普通的 ByteBuf，拥有自己独立的 readerIndex ，writerIndex 。\n\nimage.png\n\n但 CompositeByteBuf 中那些逻辑上看起来连续的字节，背后其实存储在不同的原生 ByteBuf 中。不同 ByteBuf 的内存之间其实是不连续的。\n\nimage.png\n\n那么现在问题的关键就是我们如何判断 CompositeByteBuf 中的某一段逻辑数据背后对应的究竟是哪一个真实的 ByteBuf，如果我们能够通过 CompositeByteBuf 的相关 Index , 找到这个 Index 背后对应的 ByteBuf，近而可以找到 ByteBuf 的 Index ，这样是不是就可以将 CompositeByteBuf 的逻辑操作转换成对真实内存的读写操作了。\n\nCompositeByteBuf 到原生 ByteBuf 的转换关系，Netty 封装在 Component 类中，每一个被包装在 CompositeByteBuf 中的原生 ByteBuf 都对应一个 Component 实例。它们会按照顺序统一组织在 components 数组中。\n\n    private static final class Component {\n        // 原生 ByteBuf\n        final ByteBuf srcBuf; \n        // CompositeByteBuf 的 index 加上 srcAdjustment 就得到了srcBuf 的相关 index\n        int srcAdjustment; \n        // srcBuf 可能是一个被包装过的 ByteBuf，比如 SlicedByteBuf ， DuplicatedByteBuf\n        // 被 srcBuf 包装的最底层的 ByteBuf 就存放在 buf 字段中\n        final ByteBuf buf;      \n        // CompositeByteBuf 的 index 加上 adjustment 就得到了 buf 的相关 index      \n        int adjustment; \n \n        // 该 Component 在 CompositeByteBuf 视角中表示的数据范围 [offset , endOffset)\n        int offset; \n        int endOffset;        \n    }\n\n\n一个 Component 在 CompositeByteBuf 的视角中所能表示的数据逻辑范围是 [offset , endOffset)。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n比如上图中第一个绿色的 ByteBuf , 它里边存储的数据组成了 CompositeByteBuf 中 [0 , 4) 这段逻辑数据范围。第二个黄色的 ByteBuf，它里边存储的数据组成了 CompositeByteBuf 中 [4 , 8) 这段逻辑数据范围。第三个蓝色的 ByteBuf，它里边存储的数据组成了 CompositeByteBuf 中 [8 , 12) 这段逻辑数据范围。 上一个 Component 的 endOffset 恰好是下一个 Component 的 offset 。\n\n而这些真实存储数据的 ByteBuf 则存储在对应 Component 中的 srcBuf 字段中，当我们通过 CompositeByteBuf 的 readerIndex 或者 writerIndex 进行读写操作的时候，首先需要确定相关 index 所对应的 srcBuf，然后将 CompositeByteBuf 的 index 转换为 srcBuf 的 srcIndex，近而通过 srcIndex 对 srcBuf 进行读写。\n\n这个 index 的转换就是通过 srcAdjustment 来进行的，比如，当前 CompositeByteBuf 的 readerIndex 为 5 ，它对应的是第二个黄色的 ByteBuf。而 ByteBuf 的 readerIndex 却是 1 。\n\n所以第二个 Component 的 srcAdjustment 就是 -4 ， 这样我们读取 CompositeByteBuf 的时候，首先将它的 readerIndex 加上 srcAdjustment 就得到了 ByteBuf 的 readerIndex ，后面就是普通的 ByteBuf 读取操作了。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n在比如说，我们要对 CompositeByteBuf 进行写操作，当前的 writerIndex 为 10 ，对应的是第三个蓝色的 ByteBuf，它的 writerIndex 为 2 。\n\n所以第三个 Component 的 srcAdjustment 就是 -8 ，CompositeByteBuf 的 writerIndex 加上 srcAdjustment 就得到了 ByteBuf 的 writerIndex，后续就是普通的 ByteBuf 写入操作。\n\n       int srcIdx(int index) {\n            // CompositeByteBuf 相关的 index 转换成 srcBuf 的相关 index\n            return index + srcAdjustment;\n        }\n\n\n除了 srcBuf 之外，Component 实例中还有一个 buf 字段，这里大家可能会比较好奇，为什么设计了两个 ByteBuf 字段呢 ？Component 实例与 ByteBuf 不是一对一的关系吗 ？\n\nsrcBuf 是指我们通过 addComponent 方法添加到 CompositeByteBuf 中的原始 ByteBuf。而这个 srcBuf 可能是一个视图 ByteBuf，比如上一小节中介绍到的 SlicedByteBuf 和 DuplicatedByteBuf。srcBuf 还可能是一个被包装过的 ByteBuf，比如 WrappedByteBuf , SwappedByteBuf。\n\n假如 srcBuf 是一个 SlicedByteBuf 的话，我们需要将它的原生 ByteBuf 拆解出来并保存在 Component 实例的 buf 字段中。事实上 Component 中的 buf 才是真正存储数据的地方。\n\nabstract class AbstractUnpooledSlicedByteBuf {\n    // 原生 ByteBuf\n    private final ByteBuf buffer;\n}\n\n\n与 buf 对应的就是 adjustment ， 它用于将 CompositeByteBuf 的相关 index 转换成 buf 相关的 index ，假如我们在向一个 CompositeByteBuf 执行 read 操作，它的当前 readerIndex 是 5，而 buf 的 readerIndex 是 6 。\n\n所以在读取操作之前，我们需要将 CompositeByteBuf 的 readerIndex 加上 adjustment 得到 buf 的 readerIndex，近而将读取操作转移到 buf 中。其实就和上小节中介绍的视图 ByteBuf 是一模一样的，在读写之前都需要修正相关的 index 。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n   @Override\n    public byte getByte(int index) {\n        // 通过 CompositeByteBuf 的 index , 找到数据所属的 component\n        Component c = findComponent(index);\n        // 首先通过 idx 转换为 buf 相关的 index\n        // 将对 CompositeByteBuf 的读写操作转换为 buf 的读写操作\n        return c.buf.getByte(c.idx(index));\n    }\n\n    int idx(int index) {\n        // 将 CompositeByteBuf 的相关 index 转换为 buf 的相关 index\n        return index + adjustment;\n     }\n\n\n那么我们如何根据指定的 CompositeByteBuf 的 index 来查找其对应的底层数据究竟存储在哪个 Component 中呢 ？\n\n核心思想其实很简单，因为每个 Component 都会描述自己表示 CompositeByteBuf 中的哪一段数据范围 —— [offset , endOffset)。所有的 Components 都被有序的组织在 components 数组中。我们可以通过二分查找的方法来寻找这个 index 到底是落在了哪个 Component 表示的范围中。\n\n这个查找的过程是在 findComponent方法中实现的，Netty 会将最近一次访问到的 Component 缓存在 CompositeByteBuf 的 lastAccessed 字段中，每次进行查找的时候首先会判断 index 是否落在了 lastAccessed 所表示的数据范围内 —— [ la.offset , la.endOffset) 。\n\n如果 index 恰好被缓存的 Component（lastAccessed）所包含，那么就直接返回 lastAccessed 。\n\n    // 缓存最近一次查找到的 Component\n    private Component lastAccessed;\n\n    private Component findComponent(int offset) {\n        Component la = lastAccessed;\n        // 首先查找 offset 是否恰好落在 lastAccessed 的区间中\n        if (la != null && offset >= la.offset && offset < la.endOffset) {\n           return la;\n        }\n        // 在所有 Components 中进行二分查找\n        return findIt(offset);\n    }\n\n\n如果 index 不巧没有命中缓存，那么就在整个 components 数组中进行二分查找 ：\n\n    private Component findIt(int offset) {\n        for (int low = 0, high = componentCount; low <= high;) {\n            int mid = low + high >>> 1;\n            Component c = components[mid];\n            if (offset >= c.endOffset) {\n                low = mid + 1;\n            } else if (offset < c.offset) {\n                high = mid - 1;\n            } else {\n                lastAccessed = c;\n                return c;\n            }\n        }\n\n        throw new Error(\"should not reach here\");\n    }\n\n\n# 2.8.2 CompositeByteBuf 的创建\n\n好了，现在我们已经熟悉了 CompositeByteBuf 的总体架构，那么接下来我们就来看一下 Netty 是如何将多个 ByteBuf 逻辑聚合成一个 CompositeByteBuf 的。\n\npublic final class Unpooled {\n   public static ByteBuf wrappedBuffer(ByteBuf... buffers) {\n        return wrappedBuffer(buffers.length, buffers);\n    }\n}\n\n\nCompositeByteBuf 的初始 maxNumComponents 为 buffers 数组的长度，如果我们只是传入一个 ByteBuf 的话，那么就无需创建 CompositeByteBuf，而是直接返回该 ByteBuf 的 slice 视图。\n\n如果我们传入的是多个 ByteBuf 的话，则将这多个 ByteBuf 包装成 CompositeByteBuf 返回。\n\npublic final class Unpooled {\n    public static ByteBuf wrappedBuffer(int maxNumComponents, ByteBuf... buffers) {\n        switch (buffers.length) {\n        case 0:\n            break;\n        case 1:\n            ByteBuf buffer = buffers[0];\n            if (buffer.isReadable()) {\n                // 直接返回 buffer.slice() 视图\n                return wrappedBuffer(buffer.order(BIG_ENDIAN));\n            } else {\n                buffer.release();\n            }\n            break;\n        default:\n            for (int i = 0; i < buffers.length; i++) {\n                ByteBuf buf = buffers[i];\n                if (buf.isReadable()) {\n                    // 从第一个可读的 ByteBuf —— buffers[i] 开始创建 CompositeByteBuf\n                    return new CompositeByteBuf(ALLOC, false, maxNumComponents, buffers, i);\n                }\n                // buf 不可读则 release\n                buf.release();\n            }\n            break;\n        }\n        return EMPTY_BUFFER;\n    }\n}\n\n\n在进入 CompositeByteBuf 的创建流程之后，首先是创建出一个空的 CompositeByteBuf，也就是先把 CompositeByteBuf 的骨架搭建起来，这时它的 initSize 为 buffers.length - offset 。\n\n注意 initSize 表示的并不是 CompositeByteBuf 初始包含的字节个数，而是表示初始 Component 的个数。offset 则表示从 buffers 数组中的哪一个索引开始创建 CompositeByteBuf，就是上面 CompositeByteBuf 构造函数中最后一个参数 i 。\n\n随后通过 addComponents0 方法为 buffers 数组中的每一个 ByteBuf 创建初始化 Component 实例，并将他们有序的添加到 CompositeByteBuf 的 components 数组中。\n\n但这时 Component 实例的个数可能已经超过 maxNumComponents 限制的个数，那么接下来就会在 consolidateIfNeeded() 方法中将当前 CompositeByteBuf 中的所有 Components 合并成一个更大的 Component。CompositeByteBuf 中的 components 数组长度是不可以超过 maxNumComponents 限制的，如果超过就需要在这里合并。\n\n最后设置当前 CompositeByteBuf 的 readerIndex 和 writerIndex，在初始状态下 CompositeByteBuf 的 readerIndex 会被设置为 0 ，writerIndex 会被设置为最后一个 Component 的 endOffset 。\n\n    CompositeByteBuf(ByteBufAllocator alloc, boolean direct, int maxNumComponents,\n            ByteBuf[] buffers, int offset) {\n        // 先初始化一个空的 CompositeByteBuf\n        // initSize 为 buffers.length - offset\n        this(alloc, direct, maxNumComponents, buffers.length - offset);\n        // 为所有的 buffers 创建  Component 实例，并添加到 components 数组中\n        addComponents0(false, 0, buffers, offset);\n        // 如果当前 component 的个数已经超过了 maxNumComponents，则将所有 component 合并成一个\n        consolidateIfNeeded();\n        // 设置 CompositeByteBuf 的 readerIndex = 0\n        // writerIndex 为最后一个 component 的 endOffset\n        setIndex0(0, capacity());\n    }\n\n\n# 2.8.3 shiftComps 为新的 ByteBuf 腾挪空间\n\n在整个 CompositeByteBuf 的构造过程中，最核心也是最复杂的步骤其实就是 addComponents0 方法，将多个 ByteBuf 有序的添加到 CompositeByteBuf 的 components 数组中看似简单，其实还有很多种复杂的情况需要考虑。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n复杂之处在于这些 ByteBuf 需要插在 components 数组的哪个位置上 ？ 比较简单直观的情况是我们直接在 components 数组的末尾插入，也就是说要插入的位置索引 cIndex 等于 componentCount。这里分为两种情况：\n\n 1. cIndex = componentCount = 0 ，这种情况表示我们在向一个空的 CompositeByteBuf 插入 ByteBufs , 很简单，直接插入即可。\n 2. cIndex = componentCount > 0 ， 这种情况表示我们再向一个非空的 CompositeByteBuf 插入 ByteBufs，正如上图所示。同样也很简单，直接在 componentCount 的位置处插入即可。\n\n稍微复杂一点的情况是我们在 components 数组的中间位置进行插入而不是在末尾，也就是 cIndex < componentCount 的情况。如下如图所示，假设我们现在需要在 cIndex = 3的位置处插入两个 ByteBuf 进来，但现在 components[3] 以及 components[4] 的位置已经被占用了。所以我们需要将这两个位置上的原有 component 向后移动两个位置，将 components[3] 和 components[4] 的位置腾出来。\n\n// i = 3 , count = 2 , size = 5\nSystem.arraycopy(components, i, components, i + count, size - i);\n\n\nimage.png\n\n在复杂一点的情况就是 components 数组需要扩容，当一个 CompositeByteBuf 刚刚被初始化出来的时候，它的 components 数组长度等于 maxNumComponents。\n\n如果当前 components 数组中包含的 component 个数 —— componentCount 加上本次需要添加的 ByteBuf 个数 —— count 已经超过了 maxNumComponents 的时候，就需要对 components 数组进行扩容。\n\n        // 初始为 0，当前 CompositeByteBuf 中包含的 component 个数\n        final int size = componentCount, \n        // 本次 addComponents0 操作之后，新的 component 个数\n        newSize = size + count;\n       \n        // newSize 超过了 maxNumComponents 则对 components 数组进行扩容\n        if (newSize > components.length) {\n            ....... 扩容 ....\n\n            // 扩容后的新数组\n            components = newArr;\n        }\n\n\n扩容之后的 components 数组长度是在 newSize 与原来长度的 3 / 2之间取一个最大值。\n\nint newArrSize = Math.max(size + (size >> 1), newSize);\n\n\n如果我们原来恰好是希望在 components 数组的末尾插入，也就是 cIndex = componentCount 的情况，那么就需要通过 Arrays.copyOf 首先申请一段长度为 newArrSize 的数组，然后将原来的 components 数组中的内容原样拷贝过去。\n\nnewArr = Arrays.copyOf(components, newArrSize, Component[].class);\n\n\n这样新的 components 数组就有位置可以容纳本次需要加入的 ByteBuf 了。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n如果我们希望在原来 components 数组的中间插入，也就是 cIndex < componentCount 的情况，如下图所示：\n\nimage.png\n\n这种情况在扩容的时候就不能原样拷贝原 components 数组了，而是首先通过 System.arraycopy 将 [0 , cIndex) 这段范围的内容拷贝过去，在将 [cIndex , componentCount)这段范围的内容拷贝到新数组的 cIndex + count 位置处。\n\nimage.png\n\n这样一来，就在新 components 数组的 cIndex 索引处，空出了两个位置出来用来添加本次这两个 ByteBuf。最后更新 componentCount 的值。以上腾挪空间的逻辑封装在 shiftComps 方法中：\n\n    private void shiftComps(int i, int count) {\n        // 初始为 0，当前 CompositeByteBuf 中包含的 component 个数\n        final int size = componentCount, \n        // 本次 addComponents0 操作之后，新的 component 个数\n        newSize = size + count;\n       \n        // newSize 超过了 max components（16） 则对 components 数组进行扩容\n        if (newSize > components.length) {\n            // grow the array，扩容到原来的 3 / 2\n            int newArrSize = Math.max(size + (size >> 1), newSize);\n            Component[] newArr;\n            if (i == size) {\n                // 在 Component[] 数组的末尾进行插入\n                // 初始状态 i = size = 0\n                // size - 1 是 Component[] 数组的最后一个元素，指定的 i 恰好越界\n                // 原来 Component[] 数组中的内容全部拷贝到 newArr 中\n                newArr = Arrays.copyOf(components, newArrSize, Component[].class);\n            } else {\n                // 在 Component[] 数组的中间进行插入\n                newArr = new Component[newArrSize];\n                if (i > 0) {\n                    // [0 , i) 之间的内容拷贝到 newArr 中\n                    System.arraycopy(components, 0, newArr, 0, i);\n                }\n                if (i < size) {\n                    // 将剩下的 [i , size) 内容从 newArr 的 i + count 位置处开始拷贝。\n                    // 因为需要将原来的 [ i , i+count ） 这些位置让出来，添加本次新的 components，\n                    System.arraycopy(components, i, newArr, i + count, size - i);\n                }\n            }\n            // 扩容后的新数组\n            components = newArr;\n        } else if (i < size) {\n            // i < size 本次操作要覆盖原来的 [ i , i+count ） 之间的位置，所以这里需要将原来位置上的 component 向后移动\n            System.arraycopy(components, i, components, i + count, size - i);\n        }\n        // 更新 componentCount\n        componentCount = newSize;\n    }\n\n\n# 2.8.4 Component 如何封装 ByteBuf\n\n经过上一小节 shiftComps 方法的辗转腾挪之后，现在 CompositeByteBuf 中的 components 数组终于有位置可以容纳本次需要添加的 ByteBuf 了。接下来就需要为每一个 ByteBuf 创建初始化一个 Component 实例，最后将这些 Component 实例放到 components 数组对应的位置上。\n\n    private static final class Component {\n        // 原生 ByteBuf\n        final ByteBuf srcBuf; \n        // CompositeByteBuf 的 index 加上 srcAdjustment 就得到了srcBuf 的相关 index\n        int srcAdjustment; \n        // srcBuf 可能是一个被包装过的 ByteBuf，比如 SlicedByteBuf ， DuplicatedByteBuf\n        // 被 srcBuf 包装的最底层的 ByteBuf 就存放在 buf 字段中\n        final ByteBuf buf;      \n        // CompositeByteBuf 的 index 加上 adjustment 就得到了 buf 的相关 index      \n        int adjustment; \n \n        // 该 Component 在 CompositeByteBuf 视角中表示的数据范围 [offset , endOffset)\n        int offset; \n        int endOffset;        \n    }\n\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n我们首先需要初始化 Component 实例的 offset ， endOffset 属性，前面我们已经介绍了，一个 Component 在 CompositeByteBuf 的视角中所能表示的数据逻辑范围是 [offset , endOffset)。在 components 数组中，一般前一个 Component 的 endOffset 往往是后一个 Component 的 offset。\n\n如果我们期望从 components 数组的第一个位置处开始插入（cIndex = 0），那么第一个 Component 的 offset 自然是 0 。\n\n如果 cIndex > 0 , 那么我们就需要找到它上一个 Component —— components[cIndex - 1] ， 上一个 Component 的 endOffset 恰好就是当前 Component 的 offset。\n\n然后通过 newComponent 方法利用 ByteBuf 相关属性以及 offset 来初始化 Component 实例。随后将创建出来的 Component 实例放置在对应的位置上 —— components[cIndex] 。\n\n           // 获取当前正在插入 Component 的 offset\n           int nextOffset = cIndex > 0 ? components[cIndex - 1].endOffset : 0;\n            for (ci = cIndex; arrOffset < len; arrOffset++, ci++) {\n                // 待插入 ByteBuf\n                ByteBuf b = buffers[arrOffset];\n                if (b == null) {\n                    break;\n                }\n                // 将 ByteBuf 封装在 Component 中\n                Component c = newComponent(ensureAccessible(b), nextOffset);\n                components[ci] = c;\n                // 下一个 Component 的 Offset 是上一个 Component 的 endOffset\n                nextOffset = c.endOffset;\n            }\n\n\n假设现在有一个空的 CompositeByteBuf，我们需要将一个数据范围为 [1 , 4] , readerIndex = 1 的 srcBuf ， 插入到 CompositeByteBuf 的 components 数组中。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n但是如果该 srcBuf 是一个视图 ByteBuf 的话，比如：SlicedByteBuf ， DuplicatedByteBuf。或者是一个被包装过的 ByteBuf ，比如：WrappedByteBuf ， SwappedByteBuf。\n\n那么我们就需要对 srcBuf 不断的执行 unwrap(), 将其最底层的原生 ByteBuf 提取出来，如上图所示，原生 buf 的数据范围为 [4 , 7] , srcBuf 与 buf 之间相关 index 的偏移 adjustment 等于 3 , 原生 buf 的 readerIndex = 4 。\n\n最后我们会根据 srcBuf ， srcIndex（srcBuf 的 readerIndex），原生 buf ，unwrappedIndex（buf 的 readerIndex），offset ， len （srcBuf 中的可读字节数）来初始化 Component 实例。\n\n    private Component newComponent(final ByteBuf buf, final int offset) {\n        // srcBuf 的 readerIndex = 1\n        final int srcIndex = buf.readerIndex();\n        // srcBuf 中的可读字节数 = 4\n        final int len = buf.readableBytes();\n\n        // srcBuf 可能是一个被包装过的 ByteBuf，比如 SlicedByteBuf，DuplicatedByteBuf\n        // 获取 srcBuf 底层的原生 ByteBuf\n        ByteBuf unwrapped = buf;\n        // 原生 ByteBuf 的 readerIndex\n        int unwrappedIndex = srcIndex;\n        while (unwrapped instanceof WrappedByteBuf || unwrapped instanceof SwappedByteBuf) {\n            unwrapped = unwrapped.unwrap();\n        }\n\n        // unwrap if already sliced\n        if (unwrapped instanceof AbstractUnpooledSlicedByteBuf) {\n            // 获取视图 ByteBuf  相对于 原生 ByteBuf 的相关 index 偏移\n            // adjustment = 3\n            // unwrappedIndex = srcIndex + adjustment = 4\n            unwrappedIndex += ((AbstractUnpooledSlicedByteBuf) unwrapped).idx(0);\n            // 获取原生 ByteBuf\n            unwrapped = unwrapped.unwrap();\n        } else if (unwrapped instanceof PooledSlicedByteBuf) {\n            unwrappedIndex += ((PooledSlicedByteBuf) unwrapped).adjustment;\n            unwrapped = unwrapped.unwrap();\n        } else if (unwrapped instanceof DuplicatedByteBuf || unwrapped instanceof PooledDuplicatedByteBuf) {\n            unwrapped = unwrapped.unwrap();\n        }\n\n        return new Component(buf.order(ByteOrder.BIG_ENDIAN), srcIndex,\n                unwrapped.order(ByteOrder.BIG_ENDIAN), unwrappedIndex, offset, len, slice);\n    }\n\n\n由于当前的 CompositeByteBuf 还是空的，里面没有包含任何逻辑数据，当长度为 4 的 srcBuf 加入之后，CompositeByteBuf 就产生了 [0 , 3] 这段逻辑数据范围，所以 srcBuf 所属 Component 的 offset = 0 , endOffset = 4 ，srcAdjustment = 1 ，adjustment = 4。\n\nimage.png\n\n        Component(ByteBuf srcBuf, int srcOffset, ByteBuf buf, int bufOffset,\n                int offset, int len, ByteBuf slice) {\n            this.srcBuf = srcBuf;\n            // 用于将 CompositeByteBuf 的 index 转换为 srcBuf 的index\n            // 1 - 0 = 1\n            this.srcAdjustment = srcOffset - offset;\n            this.buf = buf;\n            // 用于将 CompositeByteBuf 的 index 转换为 buf 的index\n            // 4 - 0 = 4\n            this.adjustment = bufOffset - offset;\n            // CompositeByteBuf [offset , endOffset) 这段范围的字节存储在该 Component 中\n            //  0 \n            this.offset = offset;\n            // 下一个 Component 的 offset\n            // 4\n            this.endOffset = offset + len;\n        }\n\n\n当我们继续初始化下一个 Component 的时候，它的 Offset 其实就是这个 Component 的 endOffset 。后面的流程都是一样的了。\n\n# 2.8.5 addComponents0\n\n在我们清楚了以上背景知识之后，在看 addComponents0 方法的逻辑就很清晰了：\n\n    private CompositeByteBuf addComponents0(boolean increaseWriterIndex,\n            final int cIndex, ByteBuf[] buffers, int arrOffset) {\n        // buffers 数组长度\n        final int len = buffers.length, \n        // 本次批量添加的 ByteBuf 个数\n        count = len - arrOffset;\n        // ci 表示从 components 数组的哪个索引位置处开始添加\n        // 这里先给一个初始值，后续 shiftComps 完成之后还会重新设置\n        int ci = Integer.MAX_VALUE;\n        try {\n            // cIndex >= 0 && cIndex <= componentCount\n            checkComponentIndex(cIndex);\n            // 为新添加进来的 ByteBuf 腾挪位置，以及增加 componentCount 计数\n            shiftComps(cIndex, count); // will increase componentCount\n            // 获取当前正在插入 Component 的 offset\n            int nextOffset = cIndex > 0 ? components[cIndex - 1].endOffset : 0;\n            for (ci = cIndex; arrOffset < len; arrOffset++, ci++) {\n                ByteBuf b = buffers[arrOffset];\n                if (b == null) {\n                    break;\n                }\n                // 将 ByteBuf 封装在 Component 中\n                Component c = newComponent(ensureAccessible(b), nextOffset);\n                components[ci] = c;\n                // 下一个 Component 的 Offset 是上一个 Component 的 endOffset\n                nextOffset = c.endOffset;\n            }\n            return this;\n        } finally {\n            // ci is now the index following the last successfully added component\n            // ci = componentCount 说明是一直按照顺序向后追加 component\n            // ci < componentCount 表示在 components 数组的中间插入新的 component\n            if (ci < componentCount) {\n                // 如果上面 for 循环完整的走完，ci = cIndex + count\n                if (ci < cIndex + count) {\n                    // 上面 for 循环中有 break 的情况出现或者有异常发生\n                    // ci < componentCount ，在上面的 shiftComps 中将会涉及到 component 移动，因为要腾出位置\n                    // 如果发生异常，则将后面没有加入 components 数组的 component 位置删除掉\n                    // [ci, cIndex + count) 这段位置要删除，因为在 ci-1 处已经发生异常，重新调整 components 数组\n                    removeCompRange(ci, cIndex + count);\n                    for (; arrOffset < len; ++arrOffset) {\n                        ReferenceCountUtil.safeRelease(buffers[arrOffset]);\n                    }\n                }\n                // （在中间插入的情况下）需要调整 ci 到 size -1 之间的 component 的相关 Offset\n                updateComponentOffsets(ci); // only need to do this here for components after the added ones\n            }\n            if (increaseWriterIndex && ci > cIndex && ci <= componentCount) {\n                // 本次添加的最后一个 components[ci - 1]\n                // 本次添加的第一个 components[cIndex]\n                // 最后一个 endOffset 减去第一个的 offset 就是本次添加的字节个数\n                writerIndex += components[ci - 1].endOffset - components[cIndex].offset;\n            }\n        }\n    }\n\n\n这里我们重点介绍下 finally {} 代码块中的逻辑。首先 addComponents0 方法中的核心逻辑是先通过 shiftComps 方法为接下来新创建出来的 Component 腾挪位置，因为我们有可能是在原有 components 数组的中间位置插入。\n\n然后会在一个 for () 循环中不停的将新创建的 Component 放置到 components[ci] 位置上。\n\n当跳出 for 循环进入 finally 代码块的时候，ci 的值恰恰就是最后一个成功加入 components 数组的 Component 下一个位置，如下图所示，假设 components[0] ， components[1] ，components[2] 是我们刚刚在 for 循环中插入的新值，那么 for 循环结束之后，ci 的值就是 3 。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n如果 ci = componentCount 这恰恰说明我们一直是在 components 数组的末尾进行插入，这种情况下各个 Component 实例中的 [offset , endOffset) 都是连续的不需要做任何调整。\n\n但如果 ci < componentCount 这就说明了我们是在原来 components 数组的中间位置处开始插入，下图中的 components[3] ，components[4] 是插入位置，当插入完成之后 ci 的值为 5。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n这时候就需要重新调整 components[5]，components[6] 中的 [offset , endOffset) 范围，因为 shiftComps 方法只负责帮你腾挪位置，不负责重新调整 [offset , endOffset) 范围，当新的 Component 实例插入之后，原来彼此相邻的 Component 实例之间的 [offset , endOffset) 就不连续了，所以这里需要重新调整。\n\n比如下图中所展示的情况，原来的 components 数组包含五个 Component 实例，分别在 0 - 4 位置，它们之间原本的是连续的 [offset , endOffset)。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n现在我们要在位置 3 ，4 处插入两个新的 Component 实例，所以原来的 components[3] ，components[4] 需要移动到 components[5] ，components[6] 的位置上，但 shiftComps 只负责移动而不负责重新调整它们的 [offset , endOffset)。\n\n当新的 Component 实例插入之后，components[4]，components[5] ，components[6] 之间的 [offset , endOffset) 就不连续了。所以需要通过 updateComponentOffsets 方法重新调整。\n\n    private void updateComponentOffsets(int cIndex) {\n        int size = componentCount;\n        if (size <= cIndex) {\n            return;\n        }\n        // 重新调整 components[5] ，components[6] 之间的 [offset , endOffset)\n        int nextIndex = cIndex > 0 ? components[cIndex - 1].endOffset : 0;\n        for (; cIndex < size; cIndex++) {\n            Component c = components[cIndex];\n            // 重新调整 Component 的 offset ， endOffset\n            c.reposition(nextIndex);\n            nextIndex = c.endOffset;\n        }\n    }\n\n     void reposition(int newOffset) {\n            int move = newOffset - offset;\n            endOffset += move;\n            srcAdjustment -= move;\n            adjustment -= move;\n            offset = newOffset;\n      }\n        \n\n\n以上介绍的是正常情况下的逻辑，如果在执行 for 循环的过程中出现了 break 或者发生了异常，那么 ci 的值一定是小于 cIndex + count 的。什么意思呢 ？\n\n比如我们要向一个 components 数组 cIndex = 0 的位置插入 count = 5 个 Component 实例，但是在插入第四个 Component 的时候，也就是在 components[3] 的位置处出现了 break 或者异常的情况，那么就会退出 for 循环来到这里的 finally 代码块。\n\n此时的 ci 值为 3 ，cIndex + count 的值为 5，那么就说明出现了异常情况。\n\nimage.png\n\n值得我们注意的是，components[3] 以及 components[4] 这两个位置是之前通过 shiftComps 方法腾挪出来的，由于异常情况的发生，这两个位置将不会放置任何 Component 实例。\n\n这样一来 components 数组就出现了空洞，所以接下来我们还需要将 components[5] ， components[6] 位置上的 Component 实例重新移动回 components[3] 以及 components[4] 的位置上。\n\n由于异常情况，那些 ByteBuf 数组中没有被添加进 CompositeByteBuf 的 ByteBuf 需要执行 release 。\n\n# 2.8.6 consolidateIfNeeded\n\n到现在为止一个空的 CompositeByteBuf 就算被填充好了，但是这里有一个问题，就是 CompositeByteBuf 中所能包含的 Component 实例个数是受到 maxNumComponents 限制的。\n\n我们回顾一下整个 addComponents 的过程，好像还没有一个地方对 Component 的个数做出限制，甚至在 shiftComps 方法中还会对 components 数组进行扩容。\n\n那么这样一来，Component 的个数有很大可能会超过 maxNumComponents 的限制，如果当前 CompositeByteBuf 中包含的 component 个数已经超过了 maxNumComponents ，那么就需要在 consolidate0 方法中，将所有的 component 合并。\n\n    private void consolidateIfNeeded() {\n        int size = componentCount;\n        // 如果当前 component 的个数已经超过了 maxNumComponents，则将所有 component 合并成一个\n        if (size > maxNumComponents) {\n            consolidate0(0, size);\n        }\n    }\n\n\n在这里，Netty 会将当前 CompositeByteBuf 中包含的所有 Component 合并成一个更大的 Component。合并之后 ，CompositeByteBuf 中就只包含一个 Component 了。合并的核心逻辑如下：\n\n 1. 根据当前 CompositeByteBuf 的 capacity 重新申请一个更大的 ByteBuf ，该 ByteBuf 需要容纳下 CompositeByteBuf 所能表示的所有字节。\n 2. 将所有 Component 底层的 buf 中存储的内容全部转移到新的 ByteBuf 中，并释放原有 buf 的内存。\n 3. 删除 Component 数组中所有的 Component。\n 4. 根据新的 ByteBuf 创建一个新的 Component 实例，并放置在 components 数组的第一个位置上。\n\n    private void consolidate0(int cIndex, int numComponents) {\n        if (numComponents <= 1) {\n            return;\n        }\n        // 将 [cIndex , endCIndex) 之间的 Components 合并成一个\n        final int endCIndex = cIndex + numComponents;\n        final int startOffset = cIndex != 0 ? components[cIndex].offset : 0;\n        // 计算合并范围内 Components 的存储的字节总数\n        final int capacity = components[endCIndex - 1].endOffset - startOffset;\n        // 重新申请一个新的 ByteBuf\n        final ByteBuf consolidated = allocBuffer(capacity);\n        // 将合并范围内的 Components 中的数据全部转移到新的 ByteBuf 中\n        for (int i = cIndex; i < endCIndex; i ++) {\n            components[i].transferTo(consolidated);\n        }\n        lastAccessed = null;\n        // 数据转移完成之后，将合并之前的这些 components 删除\n        removeCompRange(cIndex + 1, endCIndex);\n        // 将合并之后的新 Component 存储在 cIndex 位置处\n        components[cIndex] = newComponent(consolidated, 0);\n        if (cIndex != 0 || numComponents != componentCount) {\n            // 如果 cIndex 不是从 0 开始的，那么就更新 newComponent 的相关 offset\n            updateComponentOffsets(cIndex);\n        }\n    }\n\n\n# 2.8.7 CompositeByteBuf 的应用\n\n当我们在传输层采用 TCP 协议进行数据传输的时候，经常会遇到半包或者粘包的问题，我们从 socket 中读取出来的 ByteBuf 很大可能还构不成一个完整的包，这样一来，我们就需要将每次从 socket 中读取出来的 ByteBuf 在用户态缓存累加起来。\n\n当累加起来的 ByteBuf 达到一个完整的数据包之后，我们在从这个被缓存的 ByteBuf 中读取字节，然后进行解码，最后将解码出来的对象沿着 pipeline 向后传递。\n\npublic abstract class ByteToMessageDecoder extends ChannelInboundHandlerAdapter {\n    // 缓存累加起来的 ByteBuf\n    ByteBuf cumulation;\n    // ByteBuf 的累加聚合器\n    private Cumulator cumulator = MERGE_CUMULATOR;\n    // 是否是第一次收包\n    private boolean first;\n\n    @Override\n    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {\n        if (msg instanceof ByteBuf) {\n            // 用于存储解码之后的对象\n            CodecOutputList out = CodecOutputList.newInstance();\n            try {\n                // 第一次收包\n                first = cumulation == null;\n                // 将新进来的 (ByteBuf) msg 与之前缓存的 cumulation 聚合累加起来\n                cumulation = cumulator.cumulate(ctx.alloc(),\n                        first ? Unpooled.EMPTY_BUFFER : cumulation, (ByteBuf) msg);\n                // 解码\n                callDecode(ctx, cumulation, out);\n            } catch (DecoderException e) {\n                throw e;\n            } catch (Exception e) {\n                throw new DecoderException(e);\n            } finally {\n                    ........ 省略 ........\n                    // 解码成功之后，就将解码出来的对象沿着 pipeline 向后传播\n                    fireChannelRead(ctx, out, size); \n            }\n        } else {\n            ctx.fireChannelRead(msg);\n        }\n    }\n}\n\n\nNetty 为此专门定义了一个 Cumulator 接口，用于将每次从 socket 中读取到的 ByteBuf 聚合累积起来。参数 alloc 是一个 ByteBuf 分配器，用于在聚合的过程中如果涉及到扩容，合并等操作可以用它来申请内存。\n\n参数 cumulation 就是之前缓存起来的 ByteBuf，当第一次收包的时候，这里的 cumulation 就是一个空的 ByteBuf —— Unpooled.EMPTY_BUFFER 。\n\n参数 in 则是本次刚刚从 socket 中读取出来的 ByteBuf，可能是一个半包，Cumulator 的作用就是将新读取出来的 ByteBuf （in），累加合并到之前缓存的 ByteBuf （cumulation）中。\n\n    public interface Cumulator {\n        ByteBuf cumulate(ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf in);\n    }\n\n\nNetty 提供了 Cumulator 接口的两个实现，一个是 MERGE_CUMULATOR ， 另一个是 COMPOSITE_CUMULATOR 。\n\npublic abstract class ByteToMessageDecoder extends ChannelInboundHandlerAdapter {\n\n    public static final Cumulator MERGE_CUMULATOR\n\n    public static final Cumulator COMPOSITE_CUMULATOR\n}\n\n\nMERGE_CUMULATOR 是 Netty 默认的 Cumulator ，也是传统意义上最为普遍的一种聚合 ByteBuf 的实现，它的核心思想是在聚合多个 ByteBuf 的时候，首先会申请一块更大的内存，然后将这些需要被聚合的 ByteBuf 中的内容全部拷贝到新的 ByteBuf 中。然后释放掉原来的 ByteBuf 。\n\n效果就是将多个 ByteBuf 重新聚合成一个更大的 ByteBuf ，但这种方式涉及到内存申请以及内存拷贝的开销，优势就是内存都是连续的，读取速度快。\n\n另外一种实现就是 COMPOSITE_CUMULATOR ，也是本小节的主题，它的核心思想是将多个 ByteBuf 聚合到一个 CompositeByteBuf 中，不需要额外申请内存，更不需要内存的拷贝。\n\n但由于 CompositeByteBuf 只是逻辑上的一个视图 ByteBuf，其底层依赖的内存还是原来的那些 ByteBuf，所以就导致了 CompositeByteBuf 中的内存不是连续的，在加上 CompositeByteBuf 的相关 index 设计的比较复杂，所以在读取速度方面可能会比 MERGE_CUMULATOR 更慢一点，所以我们需要根据自己的场景来权衡考虑，灵活选择。\n\n    public static final Cumulator COMPOSITE_CUMULATOR = new Cumulator() {\n        @Override\n        public ByteBuf cumulate(ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf in) {\n            if (!cumulation.isReadable()) {\n                // 之前缓存的已经解码完毕，这里将它释放，并从 in 开始重新累加。\n                cumulation.release();\n                return in;\n            }\n            CompositeByteBuf composite = null;\n            try {\n                // cumulation 是一个 CompositeByteBuf，说明 cumulation 之前是一个被聚合过的 ByteBuf\n                if (cumulation instanceof CompositeByteBuf && cumulation.refCnt() == 1) {\n                    composite = (CompositeByteBuf) cumulation;\n                    // 这里需要保证 CompositeByteBuf 的 writerIndex 与 capacity 相等\n                    // 因为我们需要每次在 CompositeByteBuf 的末尾聚合添加新的 ByteBuf\n                    if (composite.writerIndex() != composite.capacity()) {\n                        composite.capacity(composite.writerIndex());\n                    }\n                } else {\n                    // 如果 cumulation 不是 CompositeByteBuf，只是一个普通的 ByteBuf\n                    // 说明 cumulation 之前还没有被聚合过，这里是第一次聚合，所以需要先创建一个空的 CompositeByteBuf\n                    // 然后将 cumulation 添加到 CompositeByteBuf 中\n                    composite = alloc.compositeBuffer(Integer.MAX_VALUE).addFlattenedComponents(true, cumulation);\n                }\n                // 将本次新接收到的 ByteBuf（in）添加累积到 CompositeByteBuf 中\n                composite.addFlattenedComponents(true, in);\n                in = null;\n                return composite;\n            } finally {\n                 ........ 省略聚合失败的处理 ..........\n            }\n        }\n    };\n\n\n\n# 3. Heap or Direct\n\n在前面的几个小节中，我们讨论了很多 ByteBuf 的设计细节，接下来让我们跳出这些细节，重新站在全局的视角下来看一下 ByteBuf 的总体设计。\n\nimage.png\n\n在 ByteBuf 的整个设计体系中，Netty 从 ByteBuf 内存布局的角度上，将整个体系分为了 HeapByteBuf 和 DirectByteBuf 两个大类。Netty 提供了 PlatformDependent.directBufferPreferred()方法来指定在默认情况下，是否偏向于分配 Direct Memory。\n\npublic final class PlatformDependent {\n    // 是否偏向于分配 Direct Memory\n    private static final boolean DIRECT_BUFFER_PREFERRED;\n\n    public static boolean directBufferPreferred() {\n        return DIRECT_BUFFER_PREFERRED;\n    }\n}\n\n\n要想使得 DIRECT_BUFFER_PREFERRED 为 true ，必须同时满足以下两个条件：\n\n 1. -Dio.netty.noPreferDirect 参数必须指定为 false（默认）。\n 2. CLEANER 不为 NULL , 也就是需要 JDK 中包含有效的 CLEANER 机制。\n\n static {\n        DIRECT_BUFFER_PREFERRED = CLEANER != NOOP\n                                  && !SystemPropertyUtil.getBoolean(\"io.netty.noPreferDirect\", false);\n        if (logger.isDebugEnabled()) {\n            logger.debug(\"-Dio.netty.noPreferDirect: {}\", !DIRECT_BUFFER_PREFERRED);\n        }\n }\n\n\n如果是安卓平台，那么 CLEANER 直接就是 NOOP，不会做任何判断，默认情况下直接走 Heap Memory , 除非特殊指定要走 Direct Memory。\n\n        if (!isAndroid()) {\n            if (javaVersion() >= 9) {\n                // 检查 sun.misc.Unsafe 类中是否包含有效的 invokeCleaner 方法\n                CLEANER = CleanerJava9.isSupported() ? new CleanerJava9() : NOOP;\n            } else {\n                // 检查 java.nio.ByteBuffer 中是否包含了 cleaner 字段\n                CLEANER = CleanerJava6.isSupported() ? new CleanerJava6() : NOOP;\n            }\n        } else {\n            CLEANER = NOOP;\n        }\n\n\n如果是 JDK 9 以上的版本，Netty 会检查是否可以通过 sun.misc.Unsafe 的 invokeCleaner 方法正确执行 DirectBuffer 的 Cleaner，如果执行过程中发生异常，那么 CLEANER 就为 NOOP，Netty 在默认情况下就会走 Heap Memory。\n\npublic final class Unsafe {\n    public void invokeCleaner(java.nio.ByteBuffer directBuffer) {\n        if (!directBuffer.isDirect())\n            throw new IllegalArgumentException(\"buffer is non-direct\");\n\n        theInternalUnsafe.invokeCleaner(directBuffer);\n    }\n}\n\n\n如果是 JDK 9 以下的版本，Netty 就会通过反射的方式先去获取 DirectByteBuffer 的 cleaner 字段，如果 cleaner 为 null 或者在执行 clean 方法的过程中出现了异常，那么 CLEANER 就为 NOOP，Netty 在默认情况下就会走 Heap Memory。\n\nclass DirectByteBuffer extends MappedByteBuffer implements DirectBuffer\n{\n    private final Cleaner cleaner;\n\n    DirectByteBuffer(int cap) {                   // package-private\n\n        ...... 省略 .....   \n\n        base = UNSAFE.allocateMemory(size);\n        cleaner = Cleaner.create(this, new Deallocator(base, size, cap));\n    }\n}\n\n\n如果 PlatformDependent.directBufferPreferred() 方法返回 true ,那么 ByteBufAllocator 接下来在分配内存的时候，默认情况下就会分配 directBuffer。\n\npublic final class UnpooledByteBufAllocator  extends AbstractByteBufAllocator {\n    // ByteBuf 分配器\n    public static final UnpooledByteBufAllocator DEFAULT =\n            new UnpooledByteBufAllocator(PlatformDependent.directBufferPreferred());\n}\n\npublic abstract class AbstractByteBufAllocator implements ByteBufAllocator {\n    // 是否默认分配 directBuffer\n    private final boolean directByDefault;\n\n    protected AbstractByteBufAllocator(boolean preferDirect) {\n        directByDefault = preferDirect && PlatformDependent.hasUnsafe();\n    }\n\n    @Override\n    public ByteBuf buffer() {\n        if (directByDefault) {\n            return directBuffer();\n        }\n        return heapBuffer();\n    }\n}\n\n\n一般情况下，JDK 都会包含有效的 CLEANER 机制，所以我们完全可以仅是通过 -Dio.netty.noPreferDirect （默认 false）来控制 Netty 默认情况下走 Direct Memory。\n\n但如果是安卓平台，那么无论 -Dio.netty.noPreferDirect 如何设置，Netty 默认情况下都会走 Heap Memory 。\n\n\n# 4. Cleaner or NoCleaner\n\n站在内存回收的角度，Netty 将 ByteBuf 分为了带有 Cleaner 的 DirectByteBuf 和没有 Cleaner 的 DirectByteBuf 两个大类。在之前的文章《以 ZGC 为例，谈一谈 JVM 是如何实现 Reference 语义的》 中的第三小节，笔者详细的介绍过，JVM 如何利用 Cleaner 机制来回收 DirectByteBuffer 背后的 Native Memory 。\n\n而 Cleaner 回收 DirectByteBuffer 的 Native Memory 需要依赖 GC 的发生，当一个 DirectByteBuffer 没有任何强引用或者软引用的时候，如果此时发生 GC , Cleaner 才会去回收 Native Memory。如果很久都没发生 GC ,那么这些 DirectByteBuffer 所引用的 Native Memory 将一直不会释放。\n\n所以仅仅是依赖 Cleaner 来释放 Native Memory 是有一定延迟的，极端情况下，如果一直等不来 GC ,很有可能就会发生 OOM 。\n\n而 Netty 的 ByteBuf 设计相当于是对 NIO ByteBuffer 的一种完善扩展，其底层其实都会依赖一个 JDK 的 ByteBuffer。比如，前面介绍的 UnpooledDirectByteBuf ， UnpooledUnsafeDirectByteBuf 其底层依赖的就是 JDK DirectByteBuffer , 而这个 DirectByteBuffer 就是带有 Cleaner 的 ByteBuf 。\n\npublic class UnpooledDirectByteBuf extends AbstractReferenceCountedByteBuf {\n    // 底层依赖的 JDK DirectByteBuffer\n    ByteBuffer buffer;\n\n    public UnpooledDirectByteBuf(ByteBufAllocator alloc, int initialCapacity, int maxCapacity) {\n        // 创建 DirectByteBuffer\n        setByteBuffer(allocateDirect(initialCapacity), false);\n    }\n\n   protected ByteBuffer allocateDirect(int initialCapacity) {\n        return ByteBuffer.allocateDirect(initialCapacity);\n    }\npublic class UnpooledUnsafeDirectByteBuf extends UnpooledDirectByteBuf {\n    // 底层依赖的 JDK DirectByteBuffer 的内存地址\n    long memoryAddress;\n\n\n    public UnpooledUnsafeDirectByteBuf(ByteBufAllocator alloc, int initialCapacity, int maxCapacity) {\n         // 调用父类 UnpooledDirectByteBuf 构建函数创建底层依赖的 JDK DirectByteBuffer \n        super(alloc, initialCapacity, maxCapacity);\n    }\n\n    @Override\n    final void setByteBuffer(ByteBuffer buffer, boolean tryFree) {\n        super.setByteBuffer(buffer, tryFree);\n        // 获取 JDK DirectByteBuffer 的内存地址\n        memoryAddress = PlatformDependent.directBufferAddress(buffer);\n    }\n\n\n在 JDK NIO 中，凡是通过 ByteBuffer.allocateDirect 方法申请到 DirectByteBuffer 都是带有 Cleaer 的。\n\npublic abstract class ByteBuffer {\n  public static ByteBuffer allocateDirect(int capacity) {\n        return new DirectByteBuffer(capacity);\n    }\n}\n\nclass DirectByteBuffer extends MappedByteBuffer implements DirectBuffer\n{\n    private final Cleaner cleaner;\n\n    DirectByteBuffer(int cap) {                   // package-private\n\n        ...... 省略 .....   \n        // 通过该构造函数申请到的 Direct Memory 会受到 -XX:MaxDirectMemorySize 参数的限制\n        Bits.reserveMemory(size, cap);   \n        // 底层调用 malloc 申请内存\n        base = UNSAFE.allocateMemory(size);\n\n        ...... 省略 .....   \n        // 创建 Cleaner\n        cleaner = Cleaner.create(this, new Deallocator(base, size, cap));\n    }\n}\n\n\n而带有 Cleaner 的 DirectByteBuffer 背后所能引用的 Direct Memory 是受到 -XX:MaxDirectMemorySize JVM 参数限制的。由于 UnpooledDirectByteBuf 以及 UnpooledUnsafeDirectByteBuf 都带有 Cleaner，所以当他们在系统中没有任何强引用或者软引用的时候，如果发生 GC , Cleaner 就会释放他们的 Direct Memory 。\n\n由于 Cleaner 执行会依赖 GC , 而 GC 的发生往往不那么及时，会有一定的延时，所以 Netty 为了可以及时的释放 Direct Memory ，往往选择不依赖 JDK 的 Cleaner 机制，手动进行释放。所以就有了 NoCleaner 类型的 DirectByteBuf —— UnpooledUnsafeNoCleanerDirectByteBuf 。\n\nclass UnpooledUnsafeNoCleanerDirectByteBuf extends UnpooledUnsafeDirectByteBuf {\n\n    @Override\n    protected ByteBuffer allocateDirect(int initialCapacity) {\n        // 创建没有 Cleaner 的 JDK DirectByteBuffer \n        return PlatformDependent.allocateDirectNoCleaner(initialCapacity);\n    }\n\n    @Override\n    protected void freeDirect(ByteBuffer buffer) {\n        // 既然没有了 Cleaner ， 所以 Netty 要手动进行释放\n        PlatformDependent.freeDirectNoCleaner(buffer);\n    }\n}\n\n\nUnpooledUnsafeNoCleanerDirectByteBuf 的底层同样也会依赖一个 JDK DirectByteBuffer , 但和之前不同的是，这里的 DirectByteBuffer 是不带有 cleaner 的。\n\n我们通过 JNI 来调用 DirectByteBuffer(long addr, int cap) 构造函数创建出来的 JDK DirectByteBuffer 都是没有 cleaner 的。但通过这种方式创建出来的 DirectByteBuffer 背后引用的 Native Memory 是不会受到 -XX:MaxDirectMemorySize JVM 参数限制的。\n\nclass DirectByteBuffer {\n    // Invoked only by JNI: NewDirectByteBuffer(void*, long)\n    private DirectByteBuffer(long addr, int cap) {\n        super(-1, 0, cap, cap, null);\n        address = addr;\n        // cleaner 为 null\n        cleaner = null;\n    }\n}\n\n\n既然没有了 cleaner ， 所以 Netty 就无法依赖 GC 来释放 Direct Memory 了，这就要求 Netty 必须手动调用 freeDirect方法及时地释放 Direct Memory。\n\n> 事实上，无论 Netty 中的 DirectByteBuf 有没有 Cleaner， Netty 都会选择手动的进行释放，目的就是为了避免 GC 的延迟 ， 从而及时的释放 Direct Memory。\n\n那么 Netty 中的 DirectByteBuf 在什么情况下带有 Cleaner，又在什么情况下不带 Cleaner 呢 ？我们可以通过 PlatformDependent.useDirectBufferNoCleaner 方法的返回值进行判断：\n\npublic final class PlatformDependent {\n    // Netty 的 DirectByteBuf 是否带有 Cleaner\n    private static final boolean USE_DIRECT_BUFFER_NO_CLEANER;\n    public static boolean useDirectBufferNoCleaner() {\n        return USE_DIRECT_BUFFER_NO_CLEANER;\n    }\n}\n\n\n * USE_DIRECT_BUFFER_NO_CLEANER = TRUE 表示 Netty 创建出来的 DirectByteBuf 不带有 Cleaner 。 Direct Memory 的用量不会受到 JVM 参数 -XX:MaxDirectMemorySize 的限制。\n * USE_DIRECT_BUFFER_NO_CLEANER = FALSE 表示 Netty 创建出来的 DirectByteBuf 带有 Cleaner 。 Direct Memory 的用量会受到 JVM 参数 -XX:MaxDirectMemorySize 的限制。\n\n我们可以通过 -Dio.netty.maxDirectMemory 来设置 USE_DIRECT_BUFFER_NO_CLEANER 的值，除此之外，该参数还可以指定在 Netty 层面上可以使用的最大 DirectMemory 用量。\n\nio.netty.maxDirectMemory = 0 那么 USE_DIRECT_BUFFER_NO_CLEANER 就为 FALSE , 表示在 Netty 层面创建出来的 DirectByteBuf 都是带有 Cleaner 的，这种情况下 Netty 并不会限制 maxDirectMemory 的用量，因为限制了也没用，具体能用多少 maxDirectMemory，还是由 JVM 参数 -XX:MaxDirectMemorySize 决定的。\n\nio.netty.maxDirectMemory < 0 ，默认为 -1，也就是在默认情况下 USE_DIRECT_BUFFER_NO_CLEANER 为 TRUE , 创建出来的 DirectByteBuf 都是不带 Cleaner 的。由于在这种情况下 maxDirectMemory 的用量并不会受到 JVM 参数 -XX:MaxDirectMemorySize 的限制，所以在 Netty 层面上必须限制 maxDirectMemory 的用量，默认值就是 -XX:MaxDirectMemorySize 指定的值。\n\n这里需要特别注意的是，Netty 层面对于 maxDirectMemory 的容量限制和 JVM 层面对于 maxDirectMemory 的容量限制是单独分别计算的，互不影响。因此站在 JVM 进程的角度来说，总体 maxDirectMemory 的用量是 -XX:MaxDirectMemorySize 的两倍。\n\nio.netty.maxDirectMemory > 0 的情况和小于 0 的情况一样，唯一不同的是 Netty 层面的 maxDirectMemory 用量是专门由 -Dio.netty.maxDirectMemory 参数指定，仍然独立于 JVM 层面的 maxDirectMemory 限制之外单独计算。\n\n所以从这个层面来说，Netty 设计 NoCleaner 类型的 DirectByteBuf 的另外一个目的就是为了突破 JVM 对于 maxDirectMemory 用量的限制。\n\npublic final class PlatformDependent {\n    // Netty 层面  Direct Memory 的用量统计\n    // 为 NULL 表示在 Netty 层面不进行特殊限制，完全由 JVM 进行限制 Direct Memory 的用量\n    private static final AtomicLong DIRECT_MEMORY_COUNTER;\n    // Netty 层面 Direct Memory 的最大用量\n    private static final long DIRECT_MEMORY_LIMIT;\n    // JVM 指定的 -XX:MaxDirectMemorySize 最大堆外内存\n    private static final long MAX_DIRECT_MEMORY = maxDirectMemory0();\n\n    static {\n        long maxDirectMemory = SystemPropertyUtil.getLong(\"io.netty.maxDirectMemory\", -1);\n\n        if (maxDirectMemory == 0 || !hasUnsafe() || !PlatformDependent0.hasDirectBufferNoCleanerConstructor()) {\n            // maxDirectMemory = 0 表示后续创建的 DirectBuffer 是带有 Cleaner 的，Netty 自己不会强制限定 maxDirectMemory 的用量，完全交给 JDK 的 maxDirectMemory 来限制\n            // 因为 Netty 限制了也没用，其底层依然依赖的是 JDK  DirectBuffer（Cleaner），JDK 会限制 maxDirectMemory 的用量\n            // 在没有 Unsafe 的情况下，那么就必须使用 Cleaner，因为如果不使用 Cleaner 的话，又没有 Unsafe，我们就无法释放 Native Memory 了\n            // 如果 JDK 本身不包含创建 NoCleaner DirectBuffer 的构造函数 —— DirectByteBuffer(long, int)，那么自然只能使用 Cleaner\n            USE_DIRECT_BUFFER_NO_CLEANER = false;\n            // Netty 自身不会统计 Direct Memory 的用量，完全交给 JDK 来统计\n            DIRECT_MEMORY_COUNTER = null;\n        } else {\n            USE_DIRECT_BUFFER_NO_CLEANER = true;\n            if (maxDirectMemory < 0) {\n                // maxDirectMemory < 0 (默认 -1) 后续创建 NoCleaner DirectBuffer\n                // Netty 层面会单独限制 maxDirectMemory 用量，maxDirectMemory 的值与 -XX:MaxDirectMemorySize 的值相同\n                // 因为 JDK 不会统计和限制 NoCleaner DirectBuffer 的用量\n                // 注意，这里 Netty 的 maxDirectMemory 和 JDK 的 maxDirectMemory 是分别单独统计的\n                // 在 JVM 进程的角度来说，整体 maxDirectMemory 的用量是 -XX:MaxDirectMemorySize 的两倍（Netty用的和 JDK 用的之和）\n                maxDirectMemory = MAX_DIRECT_MEMORY;\n                if (maxDirectMemory <= 0) {\n                    DIRECT_MEMORY_COUNTER = null;\n                } else {\n                    // 统计 Netty DirectMemory 的用量\n                    DIRECT_MEMORY_COUNTER = new AtomicLong();\n                }\n            } else {\n                // maxDirectMemory > 0 后续创建 NoCleaner DirectBuffer,Netty 层面的 maxDirectMemory 就是 io.netty.maxDirectMemory 指定的值\n                DIRECT_MEMORY_COUNTER = new AtomicLong();\n            }\n        }\n        logger.debug(\"-Dio.netty.maxDirectMemory: {} bytes\", maxDirectMemory);\n        DIRECT_MEMORY_LIMIT = maxDirectMemory >= 1 ? maxDirectMemory : MAX_DIRECT_MEMORY;\n    }  \n}\n\n\n当 Netty 层面的 direct memory 用量超过了 -Dio.netty.maxDirectMemory 参数指定的值时，那么就会抛出 OutOfDirectMemoryError ，分配 DirectByteBuf 将会失败。\n\n    private static void incrementMemoryCounter(int capacity) {\n        if (DIRECT_MEMORY_COUNTER != null) {\n            long newUsedMemory = DIRECT_MEMORY_COUNTER.addAndGet(capacity);\n            if (newUsedMemory > DIRECT_MEMORY_LIMIT) {\n                DIRECT_MEMORY_COUNTER.addAndGet(-capacity);\n                throw new OutOfDirectMemoryError(\"failed to allocate \" + capacity\n                        + \" byte(s) of direct memory (used: \" + (newUsedMemory - capacity)\n                        + \", max: \" + DIRECT_MEMORY_LIMIT + ')');\n            }\n        }\n    }\n\n\n\n# 5. Unsafe or NoUnsafe\n\n站在内存访问方式的角度上来说 ， Netty 又会将 ByteBuf 分为了 Unsafe 和 NoUnsafe 两个大类，其中 NoUnsafe 的内存访问方式是依赖底层的 JDK ByteBuffer，对于 Netty ByteBuf 的任何操作最终都是会代理给底层 JDK 的 ByteBuffer。\n\npublic class UnpooledDirectByteBuf extends AbstractReferenceCountedByteBuf {\n    // 底层依赖的 JDK DirectByteBuffer\n    ByteBuffer buffer;\n\n   @Override\n    protected byte _getByte(int index) {\n        return buffer.get(index);\n    }\n\n    @Override\n    protected void _setByte(int index, int value) {\n        buffer.put(index, (byte) value);\n    }\n}\n\n\n而 Unsafe 的内存访问方式则是通过 sun.misc.Unsafe 类中提供的众多 low-level direct buffer access API 来对内存地址直接进行访问，由于是脱离 JVM 相关规范直接对内存地址进行访问，所以我们在调用 Unsafe 相关方法的时候需要考虑 JVM 以及 OS 的各种细节，一不小心就会踩坑出错，所以它是一种不安全的访问方式，但是足够灵活，高效。\n\npublic class UnpooledUnsafeDirectByteBuf extends UnpooledDirectByteBuf {\n    // 底层依赖的 JDK DirectByteBuffer 的内存地址\n    long memoryAddress;\n\n    @Override\n    protected byte _getByte(int index) {\n        return UnsafeByteBufUtil.getByte(addr(index));\n    }\n\n   final long addr(int index) {\n        // 直接通过内存地址进行访问\n        return memoryAddress + index;\n    }\n\n    @Override\n    protected void _setByte(int index, int value) {\n        UnsafeByteBufUtil.setByte(addr(index), value);\n    }\n\n}\n\n\nNetty 提供了 -Dio.netty.noUnsafe 参数来让我们决定是否采用 Unsafe 的内存访问方式，默认值是 false , 表示 Netty 默认开启 Unsafe 访问方式。\n\nfinal class PlatformDependent0 {\n    // 是否明确禁用 Unsafe，null 表示开启  Unsafe\n    private static final Throwable EXPLICIT_NO_UNSAFE_CAUSE = explicitNoUnsafeCause0();\n\n    private static Throwable explicitNoUnsafeCause0() {\n        final boolean noUnsafe = SystemPropertyUtil.getBoolean(\"io.netty.noUnsafe\", false);\n        logger.debug(\"-Dio.netty.noUnsafe: {}\", noUnsafe);\n\n        if (noUnsafe) {\n            logger.debug(\"sun.misc.Unsafe: unavailable (io.netty.noUnsafe)\");\n            return new UnsupportedOperationException(\"sun.misc.Unsafe: unavailable (io.netty.noUnsafe)\");\n        }\n\n        return null;\n    }\n}\n\n\n在确认开启了 Unsafe 方式之后，我们就需要近一步确认在当前 JRE 的 classpath 下是否存在 sun.misc.Unsafe 类，是否能通过反射的方式获取到 Unsafe 实例 —— theUnsafe 。\n\npublic final class Unsafe {\n    // Unsafe 实例\n    private static final Unsafe theUnsafe = new Unsafe();\n}\nfinal class PlatformDependent0 {\n    // 验证 Unsafe 是否可用，null 表示 Unsafe 是可用状态\n    private static final Throwable UNSAFE_UNAVAILABILITY_CAUSE;\n    static {\n           // 尝试通过反射的方式拿到 theUnsafe 实例\n           final Object maybeUnsafe = AccessController.doPrivileged(new PrivilegedAction<Object>() {\n                @Override\n                public Object run() {\n                    try {\n                        final Field unsafeField = Unsafe.class.getDeclaredField(\"theUnsafe\");\n                        Throwable cause = ReflectionUtil.trySetAccessible(unsafeField, false);\n                        if (cause != null) {\n                            return cause;\n                        }\n                        // the unsafe instance\n                        return unsafeField.get(null);\n                    } catch (NoSuchFieldException e) {\n                        return e;\n                    } catch (SecurityException e) {\n                        return e;\n                    } catch (IllegalAccessException e) {\n                        return e;\n                    } catch (NoClassDefFoundError e) {\n                        // Also catch NoClassDefFoundError in case someone uses for example OSGI and it made\n                        // Unsafe unloadable.\n                        return e;\n                    }\n                }\n            });\n    }\n}\n\n\n在获取到 Unsafe 实例之后，我们还需要检查 Unsafe 中是否包含所有 Netty 用到的 low-level direct buffer access API ，确保这些 API 可以正常有效的运行。比如，是否包含 copyMemory 方法。\n\npublic final class Unsafe {\n    @ForceInline\n    public void copyMemory(Object srcBase, long srcOffset,\n                           Object destBase, long destOffset,\n                           long bytes) {\n        theInternalUnsafe.copyMemory(srcBase, srcOffset, destBase, destOffset, bytes);\n    }\n}\n\n\n是否可以通过 Unsafe 访问到 NIO Buffer 的 address 字段，因为后续我们需要直接操作内存地址。\n\npublic abstract class Buffer {\n    // 内存地址\n    long address;\n}\n\n\n在整个过程中如果发生任何异常，则表示在当前 classpath 下，不存在 sun.misc.Unsafe 类或者是由于不同版本 JDK 的设计，Unsafe 中没有 Netty 所需要的一些必要的访存 API 。这样一来我们就无法使用 Unsafe，内存的访问方式就需要回退到 NoUnsafe。\n\n            if (maybeUnsafe instanceof Throwable) {\n                unsafe = null;\n                unsafeUnavailabilityCause = (Throwable) maybeUnsafe;\n                logger.debug(\"sun.misc.Unsafe.theUnsafe: unavailable\", (Throwable) maybeUnsafe);\n            } else {\n                unsafe = (Unsafe) maybeUnsafe;\n                logger.debug(\"sun.misc.Unsafe.theUnsafe: available\");\n            }\n            // 为 null 表示 Unsafe 可用\n            UNSAFE_UNAVAILABILITY_CAUSE = unsafeUnavailabilityCause;\n            UNSAFE = unsafe;\n\n\n如果在整个过程中没有发生任何异常，我们获取到了一个有效的 UNSAFE 实例，那么后续将正式开启 Unsafe 的内存访问方式。\n\nfinal class PlatformDependent0 {\n    static boolean hasUnsafe() {\n        return UNSAFE != null;\n    }\n}\n\n\n完整的 hasUnsafe() 判断逻辑如下：\n\n 1. 如果当前平台是安卓或者 .NET ，则不能开启 Unsafe，因为这些平台并不包含 sun.misc.Unsafe 类。\n 2. -Dio.netty.noUnsafe 参数需要设置为 false （默认开启）。\n\n3.. 当前 classpath 下是否包含有效的 sun.misc.Unsafe 类。\n\n 1. Unsafe 实例需要包含必要的访存 API 。\n\npublic final class PlatformDependent {\n    private static final Throwable UNSAFE_UNAVAILABILITY_CAUSE = unsafeUnavailabilityCause0();\n\n    public static boolean hasUnsafe() {\n        return UNSAFE_UNAVAILABILITY_CAUSE == null;\n    }\n    private static Throwable unsafeUnavailabilityCause0() {\n        if (isAndroid()) {\n            logger.debug(\"sun.misc.Unsafe: unavailable (Android)\");\n            return new UnsupportedOperationException(\"sun.misc.Unsafe: unavailable (Android)\");\n        }\n\n        if (isIkvmDotNet()) {\n            logger.debug(\"sun.misc.Unsafe: unavailable (IKVM.NET)\");\n            return new UnsupportedOperationException(\"sun.misc.Unsafe: unavailable (IKVM.NET)\");\n        }\n\n        Throwable cause = PlatformDependent0.getUnsafeUnavailabilityCause();\n        if (cause != null) {\n            return cause;\n        }\n\n        try {\n            boolean hasUnsafe = PlatformDependent0.hasUnsafe();\n            logger.debug(\"sun.misc.Unsafe: {}\", hasUnsafe ? \"available\" : \"unavailable\");\n            return hasUnsafe ? null : PlatformDependent0.getUnsafeUnavailabilityCause();\n        } catch (Throwable t) {\n            logger.trace(\"Could not determine if Unsafe is available\", t);\n            // Probably failed to initialize PlatformDependent0.\n            return new UnsupportedOperationException(\"Could not determine if Unsafe is available\", t);\n        }\n    }\n}\n\n\n如果 PlatformDependent.hasUnsafe() 方法返回 true , 那么后续 Netty 都会创建 Unsafe 类型的 ByteBuf。\n\n\n# 6. Pooled or Unpooled\n\n站在内存管理的角度上来讲，Netty 将 ByteBuf 分为了 池化（Pooled） 和 非池化（Unpooled）两个大类，其中 Unpooled 类型的 ByteBuf 是用到的时候才去临时创建，使用完的时候再去释放。\n\n而 Direct Memory 的申请和释放开销相较于 Heap Memory 会大很多，Netty 在面对高并发网络通信的场景下，Direct Memory 的申请和释放是一个非常频繁的操作，这种大量频繁地内存申请释放操作对程序的性能影响是巨大的，因此 Netty 引入了内存池将这些 Direct Memory 统一池化管理起来。\n\nNetty 提供了 -Dio.netty.allocator.type 参数来让我们决定是否采用内存池来管理 ByteBuf ， 默认值是 pooled , 也就是说 Netty 默认是采用池化的方式来管理 PooledByteBuf 。如果是安卓平台，那么默认是使用非池化的 ByteBuf （unpooled）。\n\n * 当参数 io.netty.allocator.type 的值为 pooled 时，Netty 的默认 ByteBufAllocator 是 PooledByteBufAllocator.DEFAULT 。\n * 当参数 io.netty.allocator.type 的值为 unpooled 时，Netty 的默认 ByteBufAllocator 是 UnpooledByteBufAllocator.DEFAULT 。\n\npublic final class ByteBufUtil {\n    // 默认 PooledByteBufAllocator，池化管理 ByteBuf\n    static final ByteBufAllocator DEFAULT_ALLOCATOR;\n\n    static {\n        // 默认为 pooled\n        String allocType = SystemPropertyUtil.get(\n                \"io.netty.allocator.type\", PlatformDependent.isAndroid() ? \"unpooled\" : \"pooled\");\n        allocType = allocType.toLowerCase(Locale.US).trim();\n\n        ByteBufAllocator alloc;\n        if (\"unpooled\".equals(allocType)) {\n            alloc = UnpooledByteBufAllocator.DEFAULT;\n            logger.debug(\"-Dio.netty.allocator.type: {}\", allocType);\n        } else if (\"pooled\".equals(allocType)) {\n            alloc = PooledByteBufAllocator.DEFAULT;\n            logger.debug(\"-Dio.netty.allocator.type: {}\", allocType);\n        } else {\n            alloc = PooledByteBufAllocator.DEFAULT;\n            logger.debug(\"-Dio.netty.allocator.type: pooled (unknown: {})\", allocType);\n        }\n\n        DEFAULT_ALLOCATOR = alloc;\n    }\n}\n\n\n后续 Netty 在创建 SocketChannel 的时候，在 SocketChannelConfig 中指定的 ByteBufAllocator 就是这里的 ByteBufUtil.DEFAULT_ALLOCATOR，默认情况下为 PooledByteBufAllocator。\n\npublic interface ByteBufAllocator {\n    ByteBufAllocator DEFAULT = ByteBufUtil.DEFAULT_ALLOCATOR;\n}\n\npublic class DefaultChannelConfig implements ChannelConfig {\n    // PooledByteBufAllocator\n    private volatile ByteBufAllocator allocator = ByteBufAllocator.DEFAULT;\n}\n\n\n当 Netty 读取 Socket 中的网络数据时，首先会从 DefaultChannelConfig 中将 ByteBufAllocator 获取到，然后利用 ByteBufAllocator 从内存池中获取一个 DirectByteBuf ，最后将 Socket 中的数据读取到 DirectByteBuf 中，随后沿着 pipeline 向后传播，进行 IO 处理。\n\nprotected class NioByteUnsafe extends AbstractNioUnsafe {\n        @Override\n        public final void read() {\n            // 获取 SocketChannelConfig\n            final ChannelConfig config = config();\n            // 获取 ByteBufAllocator ， 默认为 PooledByteBufAllocator\n            final ByteBufAllocator allocator = config.getAllocator();\n            // 从内存池中获取 byteBuf\n            byteBuf = allocHandle.allocate(allocator);\n            // 读取 socket 中的数据到 byteBuf\n            allocHandle.lastBytesRead(doReadBytes(byteBuf));\n            // 将 byteBuf 沿着 pipeline 向后传播\n            pipeline.fireChannelRead(byteBuf);\n\n            ....... 省略 .......\n        }\n}\n\n\n除此之外，Netty 还提供了 ChannelOption.ALLOCATOR 选项，让我们可以在配置 ServerBootstrap 的时候为 SocketChannel 灵活指定自定义的 ByteBufAllocator 。\n\n        EventLoopGroup bossGroup = new NioEventLoopGroup(1);\n        EventLoopGroup workerGroup = new NioEventLoopGroup();\n\n        ServerBootstrap b = new ServerBootstrap();\n        b.group(bossGroup, workerGroup)\n            // 灵活配置 ByteBufAllocator\n          .childOption(ChannelOption.ALLOCATOR, UnpooledByteBufAllocator.DEFAULT;);\n\n\n这里通过 ChannelOption 来配置 Socket 相关的属性是最高优先级的，它会覆盖掉一切默认配置。\n\n\n# 7. Metric\n\n在第四小节中，我们介绍了 Cleaner 和 NoCleaner 这两种 DirectByteBuf，其中 CleanerDirectByteBuf 的整体 Direct Memory 的用量是受到 JVM 参数 -XX:MaxDirectMemorySize 限制的，而 NoCleanerDirectByteBuf 的整体 Direct Memory 可以突破该参数的限制，JVM 并不会统计这块 Direct Memory 的用量。\n\nNetty 为了及时地释放这些 Direct Memory，通常默认选择 NoCleanerDirectByteBuf，这就要求 Netty 需要对这部分 Direct Memory 的用量进行自行统计限制。NoCleanerDirectByteBuf 的最大可用 Direct Memory 我们可以通过 -Dio.netty.maxDirectMemory 来指定，默认情况下等于 -XX:MaxDirectMemorySize 设置的值。\n\nPlatformDependent 类中的 DIRECT_MEMORY_COUNTER 字段用于统计在 Netty 层面上，所有 NoCleanerDirectByteBuf 占用的 Direct Memory 大小。注意这里并不会统计 CleanerDirectByteBuf 的 Direct Memory 占用，这部分统计由 JVM 负责。\n\npublic final class PlatformDependent { \n    // 用于统计 NoCleaner 的 DirectByteBuf 所引用的 Native Memory 大小\n    private static final AtomicLong DIRECT_MEMORY_COUNTER;\n\n    public static ByteBuffer allocateDirectNoCleaner(int capacity) {\n        // 增加 Native Memory 用量统计\n        incrementMemoryCounter(capacity);\n        try {\n            // 分配 Native Memory\n            // 初始化 NoCleaner 的 DirectByteBuffer\n            return PlatformDependent0.allocateDirectNoCleaner(capacity);\n        } catch (Throwable e) {\n            decrementMemoryCounter(capacity);\n            throwException(e);\n            return null;\n        }\n    \n\n    public static void freeDirectNoCleaner(ByteBuffer buffer) {\n        int capacity = buffer.capacity();\n        // 释放 Native Memory\n        PlatformDependent0.freeMemory(PlatformDependent0.directBufferAddress(buffer));\n        // 减少 Native Memory 用量统计\n        decrementMemoryCounter(capacity);\n    }  \n}\n\n\nPlatformDependent 类是 Netty 最底层的一个类，所有内存的分配，释放动作最终都是在该类中执行，因此 DIRECT_MEMORY_COUNTER 字段统计的是全局的 Direct Memory 大小（Netty 层面）。\n\n每一次的内存申请 —— allocateDirectNoCleaner ， 都会增加 DIRECT_MEMORY_COUNTER 计数，每一次的内存释放 —— freeDirectNoCleaner，都会减少 DIRECT_MEMORY_COUNTER 计数。\n\n我们可以通过 PlatformDependent.usedDirectMemory()方法来获取 Netty 当前所占用的 Direct Memory 大小。但如果我们特殊指定了需要使用 CleanerDirectByteBuf ， 比如，将 -Dio.netty.maxDirectMemory 参数设置为 0 , 那么这里将会返回 -1 。\n\n    private static void incrementMemoryCounter(int capacity) {\n        // 只统计 NoCleaner 的 DirectByteBuf 所引用的 Native Memory \n        if (DIRECT_MEMORY_COUNTER != null) {\n            long newUsedMemory = DIRECT_MEMORY_COUNTER.addAndGet(capacity);\n            if (newUsedMemory > DIRECT_MEMORY_LIMIT) {\n                DIRECT_MEMORY_COUNTER.addAndGet(-capacity);\n                throw new OutOfDirectMemoryError(\"failed to allocate \" + capacity\n                        + \" byte(s) of direct memory (used: \" + (newUsedMemory - capacity)\n                        + \", max: \" + DIRECT_MEMORY_LIMIT + ')');\n            }\n        }\n    }\n\n    private static void decrementMemoryCounter(int capacity) {\n        if (DIRECT_MEMORY_COUNTER != null) {\n            long usedMemory = DIRECT_MEMORY_COUNTER.addAndGet(-capacity);\n            assert usedMemory >= 0;\n        }\n    }\n\n    public static long usedDirectMemory() {\n        return DIRECT_MEMORY_COUNTER != null ? DIRECT_MEMORY_COUNTER.get() : -1;\n    }\n\n\n除了 PlatformDependent 这里的全局统计之外，Netty 还提供了以 ByteBufAllocator 为粒度的内存占用统计，统计的维度包括 Heap Memory 的占用和 Direct Memory 的占用。\n\npublic final class UnpooledByteBufAllocator extends AbstractByteBufAllocator implements ByteBufAllocatorMetricProvider {\n    // 从该 ByteBufAllocator 分配出去的内存统计\n    private final UnpooledByteBufAllocatorMetric metric = new UnpooledByteBufAllocatorMetric();\n\n    @Override\n    public ByteBufAllocatorMetric metric() {\n        return metric;\n    }\n    // 统计 Direct Memory 的占用\n    void incrementDirect(int amount) {\n        metric.directCounter.add(amount);\n    }\n\n    void decrementDirect(int amount) {\n        metric.directCounter.add(-amount);\n    }\n    // 统计 Heap Memory 的占用\n    void incrementHeap(int amount) {\n        metric.heapCounter.add(amount);\n    }\n\n    void decrementHeap(int amount) {\n        metric.heapCounter.add(-amount);\n    }\n\n}\n\n\nNetty 定义的每一个 ByteBufAllocator 中，都会有一个 ByteBufAllocatorMetric 类型的字段，该类定义两个计数字段：directCounter，heapCounter。 分别用于统计 Direct Memory 和 Heap Memory 的占用。\n\n    private static final class UnpooledByteBufAllocatorMetric implements ByteBufAllocatorMetric {\n        final LongCounter directCounter = PlatformDependent.newLongCounter();\n        final LongCounter heapCounter = PlatformDependent.newLongCounter();\n\n        @Override\n        public long usedHeapMemory() {\n            return heapCounter.value();\n        }\n\n        @Override\n        public long usedDirectMemory() {\n            return directCounter.value();\n        }\n\n        @Override\n        public String toString() {\n            return StringUtil.simpleClassName(this) +\n                    \"(usedHeapMemory: \" + usedHeapMemory() + \"; usedDirectMemory: \" + usedDirectMemory() + ')';\n        }\n    }\n\n\n因此从内存占用统计的角度上来说，Netty 又会将整个 ByteBuf 体系分为 Instrumented 和 NoInstrumented 两大类，带有 Instrumented 前缀的 ByteBuf ，无论你是 Heap or Direct ， Cleaner or NoCleaner，Unsafe or NoUnsafe 类型的 ByteBuf ，Netty 都会统计这部分内存占用。\n\n    private static final class InstrumentedUnpooledUnsafeNoCleanerDirectByteBuf\n            extends UnpooledUnsafeNoCleanerDirectByteBuf {\n        InstrumentedUnpooledUnsafeNoCleanerDirectByteBuf(\n                UnpooledByteBufAllocator alloc, int initialCapacity, int maxCapacity) {\n            // 构造普通的 UnpooledUnsafeNoCleanerDirectByteBuf\n            super(alloc, initialCapacity, maxCapacity);\n        }\n        \n        // 分配，释放 的时候更新 Direct Memory \n        @Override\n        protected ByteBuffer allocateDirect(int initialCapacity) {\n            ByteBuffer buffer = super.allocateDirect(initialCapacity);\n            ((UnpooledByteBufAllocator) alloc()).incrementDirect(buffer.capacity());\n            return buffer;\n        }\n\n        @Override\n        protected void freeDirect(ByteBuffer buffer) {\n            int capacity = buffer.capacity();\n            super.freeDirect(buffer);\n            ((UnpooledByteBufAllocator) alloc()).decrementDirect(capacity);\n        }\n    }\n    private static final class InstrumentedUnpooledUnsafeDirectByteBuf extends UnpooledUnsafeDirectByteBuf {\n        InstrumentedUnpooledUnsafeDirectByteBuf(\n                UnpooledByteBufAllocator alloc, int initialCapacity, int maxCapacity) {\n            // 构造普通的 UnpooledUnsafeDirectByteBuf\n            super(alloc, initialCapacity, maxCapacity);\n        }\n\n        // 分配，释放 的时候更新 Direct Memory \n        @Override\n        protected ByteBuffer allocateDirect(int initialCapacity) {\n            ByteBuffer buffer = super.allocateDirect(initialCapacity);\n            ((UnpooledByteBufAllocator) alloc()).incrementDirect(buffer.capacity());\n            return buffer;\n        }\n\n        @Override\n        protected void freeDirect(ByteBuffer buffer) {\n            int capacity = buffer.capacity();\n            super.freeDirect(buffer);\n            ((UnpooledByteBufAllocator) alloc()).decrementDirect(capacity);\n        }\n    }\n\n\n\n# 8. ByteBufAllocator\n\n在 Netty 中，ByteBuf 的创建必须通过 ByteBufAllocator 进行，不能直接显示地调用 ByteBuf 相关的构造函数自行创建。Netty 定义了两种类型的 ByteBufAllocator ：\n\n 1. PooledByteBufAllocator 负责池化 ByteBuf，这里正是 Netty 内存管理的核心，在下一篇文章中，笔者会详细的和大家介绍它。\n 2. UnpooledByteBufAllocator 负责分配非池化的 ByteBuf，创建 ByteBuf 的时候临时向 OS 申请 Native Memory ，使用完之后，需要及时的手动调用 release 将 Native Memory 释放给 OS 。\n\n-Dio.netty.allocator.type 参数可以让我们自行选择 ByteBufAllocator 的类型，默认值为 pooled, Netty 默认是采用池化的方式来管理 ByteBuf 。\n\npublic interface ByteBufAllocator {\n    // 默认为 PooledByteBufAllocator\n    ByteBufAllocator DEFAULT = ByteBufUtil.DEFAULT_ALLOCATOR;\n}\n\n\n除了以上两种官方定义的 ByteBufAllocator 之外，我们还可以根据自己实际业务场景来自行定制 ByteBufAllocator ， 然后通过第六小节中介绍的 ChannelOption.ALLOCATOR 选项，将 ByteBufAllocator 灵活指定为我们自行定制的实现。\n\n对于 UnpooledByteBuf 来说，Netty 还专门提供了一个工具类 Unpooled，这里定义实现了很多针对 ByteBuf 的实用操作，比如，allocate，wrapped，copied 等。这里笔者以 DirectByteBuf 的创建为例进行说明：\n\npublic final class Unpooled {\n\n    private static final ByteBufAllocator ALLOC = UnpooledByteBufAllocator.DEFAULT;\n\n    public static ByteBuf directBuffer() {\n        return ALLOC.directBuffer();\n    }\n}\n\n\nUnpooled 底层依赖了 UnpooledByteBufAllocator ， 所有对 ByteBuf 的创建动作最终都会代理给这个 Allocator 。在 DirectBuffer 的创建过程中，我们可以看到前面介绍的所有类型的 ByteBuf。\n\npublic final class UnpooledByteBufAllocator {\n    @Override\n    protected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {\n        final ByteBuf buf;\n        if (PlatformDependent.hasUnsafe()) {\n            buf = noCleaner ? new InstrumentedUnpooledUnsafeNoCleanerDirectByteBuf(this, initialCapacity, maxCapacity) :\n                    new InstrumentedUnpooledUnsafeDirectByteBuf(this, initialCapacity, maxCapacity);\n        } else {\n            buf = new InstrumentedUnpooledDirectByteBuf(this, initialCapacity, maxCapacity);\n        }\n        // 是否启动内存泄露探测，如果启动则额外用 LeakAwareByteBuf 进行包装返回\n        return disableLeakDetector ? buf : toLeakAwareBuffer(buf);\n    }\n}\n\n\n * 首先 Netty 创建出来的所有 ByteBuf 都是带有 Metric 统计的，具体的 ByteBuf 类型都会带有 Instrumented 前缀。\n * 如果当前 JRE 环境支持 Unsafe ， 那么后续就会通过 Unsafe 的方式来对 ByteBuf 进行相关操作（默认），具体的 ByteBuf 类型都会带有 Unsafe 前缀。\n * 如果我们明确指定了 NoCleaner 类型的 DirectByteBuf（默认），那么创建出来的 ByteBuf 类型就会带有 NoCleaner 前缀，由于没有 Cleaner ，这就要求我们使用完 ByteBuf 的时候必须及时地手动进行释放。\n * 如果我们开启了内存泄露探测，那么创建流程的最后，Netty 会用一个 LeakAwareByteBuf 去包装新创建出来的 ByteBuf，当这个 ByteBuf 被 GC 的时候，Netty 会通过相关引用计数来判断是否存在忘记 release 的情况，从而确定出是否发生内存泄露。\n\n\n# 总结\n\n本文笔者从八个角度为大家详细的剖析了 ByteBuf 的整体设计，这八个角度分别是：内存区域分布的角度，内存管理的角度，内存访问的角度，内存回收的角度，内存统计 Metric 的角度，零拷贝的角度，引用计数的角度，扩容的角度。\n\n到现在为止，我们只是扫清了 Netty 内存管理外围的一些障碍，那么下一篇文章，笔者将带大家深入到内存管理的核心，彻底让大家弄懂 Netty 的内存管理机制。好了，本文的内容就到这里，我们下篇文章见~~~",normalizedContent:"时光芿苒，岁月如梭，好久没有给大家更新 netty 相关的文章了，在断更 netty 的这段日子里，笔者一直在持续更新 linux 内存管理相关的文章 ，目前为止，算是将 linux 内存管理子系统相关的主干源码较为完整的给大家呈现了出来，同时也结识了很多喜欢内核的读者，经常在后台留言讨论一些代码的设计细节，在这个过程中，我们相互分享，相互学习，浓浓的感受到了大家对技术那份纯粹的热爱，对于我自己来说，也是一种激励，学习，提高的机会。\n\n之前系列文章的视角一直是停留在内核态，笔者试图从 linux 内核的角度来为大家揭秘内存管理的本质，那么从今天开始，我们把视角在往上挪一挪，从内核态转换到用户态，继续沿着内存管理这条主线，来看一看用户态的内存管理是如何进行的。\n\n接下来笔者计划用三篇文章的篇幅为大家剖析一下 netty 的内存管理模块，本文是第一篇，主要是围绕 netty 内存管理的外围介绍一下 bytebuf 的总体设计。\n\n\n\n别看 bytebuf 体系涉及到的类比较多，一眼望过去比较头大，但是我们按照不同的视角，将它们一一分类，整个体系脉络就变得很清晰了：\n\n * 从 jvm 内存区域布局的角度来看，netty 的 bytebuf 主要分为 heapbytebuf（堆内） 和 directbytebuf（堆外）这两种类型。\n * 从内存管理的角度来看，netty 的 bytebuf 又分为 pooledbytebuf （池化）和 unpooledbytebuf（非池化）两种子类型。一种是被内存池统一管理，另一种则和普通的 bytebuf 一样，用的时候临时创建，不用的时候释放。\n * 从内存访问的角度来看，netty 又将 bytebuf 分为了 unsafebytebuf 和普通的 bytebuf。unsafebytebuf 主要是依赖 unsafe 类提供的底层 api 来直接对内存地址进行操作。而普通 bytebuf 对内存的操作主要是依赖 nio 中的 bytebuffer。\n * 从内存回收的角度来看，bytebuf 又分为了带 cleaner 的 bytebuf 以及不带 cleaner 的 nocleanerbytebuf，cleaner 在 jdk 中是用来释放 nio bytebuffer 背后所引用的 native memory 的，内存的释放由 jvm 统一管理。而 nocleanerbytebuf 背后的 native memory 则需要我们进行手动释放。\n * 从内存占用统计的角度来说，netty 又近一步将 bytebuf 分为了 instrumentedbytebuf 和普通的 bytebuf，其中 instrumentedbytebuf 会带有内存占用相关 metrics 的统计供我们进行监控，而普通的 bytebuf 则不带有热任何 metrics。\n * 从零拷贝的角度来看，netty 又引入了 compositebytebuf，目的是为多个 bytebuf 在聚合的时候提供一个统一的逻辑视图，将多个 bytebuf 聚合成一个逻辑上的 compositebytebuf，而传统的聚合操作则是首先要分配一个大的 bytebuf，然后将需要聚合的多个 bytebuf 中的内容在拷贝到新的 bytebuf 中。compositebytebuf 避免了分配大段内存以及内存拷贝的开销。注意这里的零拷贝指的是 netty 在用户态层面自己实现的避免内存拷贝的设计，而不是 os 层面上的零拷贝。\n * 另外 netty 的 bytebuf 支持引用计数以及自动地内存泄露探测，如果有内存泄露的情况，netty 会将具体发生泄露的位置报告出来。\n * netty 的 bytebuf 支持扩容，而 nio 的 bytebuffer 则不支持扩容，\n\n在将 netty 的 bytebuf 设计体系梳理完整之后，我们就会发现，netty 的 bytebuf 其实是对 jdk bytebuffer 的一种扩展和完善，所以下面笔者的行文思路是与 jdk bytebuffer 对比着进行介绍 netty 的 bytebuf ，有了对比，我们才能更加深刻的体会到 netty 设计的精妙。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)本文概要.png\n\n\n# 1. jdk 中的 bytebuffer 设计有何不妥\n\n笔者曾在 《一步一图带你深入剖析 jdk nio bytebuffer 在不同字节序下的设计与实现》 一文中完整的介绍过 jdk bytebuffer 的整个设计体系，下面我们来简短回忆一下 bytebuffer 的几个核心要素。\n\npublic abstract class buffer {\n    private int mark = -1;\n    private int position = 0;\n    private int limit;\n    private int capacity;\n}\n\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n * capacity 规定了整个 buffer 的容量，具体可以容纳多少个元素。capacity 之前的元素均是 buffer 可操作的空间，jdk 中的 bytebuffer 是不可扩容的。\n * position 用于指向 buffer 中下一个可操作性的元素，初始值为 0。对于 buffer 的读写操作全部都共用这一个 position 指针，在 buffer 的写模式下，position 指针用于指向下一个可写位置。在读模式下，position 指针指向下一个可读位置。\n * limit 用于限定 buffer 可操作元素的上限，position 指针不能超过 limit。\n\n由于 jdk bytebuffer 只设计了一个 position 指针，所以我们在读写 bytebuffer 的时候需要不断的调整 position 的位置。比如，利用 flip() ，rewind()，compact()，clear() 等方法不断的进行读写模式的切换。\n\n一些具体的场景体现就是，当我们对一个 bytebuffer 进行写入的时候，随着数据不断的向 bytebuffer 写入，position 指针会不断的向后移动。在写入操作完成之后，如果我们想要从 bytebuffer 读取刚刚写入的数据就麻烦了。\n\n由于 jdk 在对 bytebuffer 的设计中读写操作都是混用一个 position 指针，所以在读取 bytebuffer 之前，我们还需要通过 flip() 调整 position 的位置，进行读模式的切换。\n\nflip切换读模式.png\n\n    public final buffer flip() {\n        limit = position;\n        position = 0;\n        mark = -1;\n        return this;\n    }\n\n\n当我们将 bytebuffer 中的数据全部读取完之后，如果再次向 bytebuffer 写入数据，那么还需要重新调整 position 的位置，通过 clear() 来进行写模式的切换。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)clear切换写模式.png\n\n    public final buffer clear() {\n        position = 0;\n        limit = capacity;\n        mark = -1;\n        return this;\n    }\n\n\n如果我们只是部分读取了 bytebuffer 中的数据而不是全部读取，那么在写入的时候，为了避免未被读取的部分被接下来的写入操作覆盖，我们则需要通过 compact() 方法来切换写模式。\n\ncomapct切换写模式.png\n\nclass heapbytebuffer extends bytebuffer {\n\n    //heapbuffer中底层负责存储数据的数组\n    final byte[] hb; \n\n    public bytebuffer compact() {\n        system.arraycopy(hb, ix(position()), hb, ix(0), remaining());\n        position(remaining());\n        limit(capacity());\n        discardmark();\n        return this;\n    }\n\n    public final int remaining() {\n        return limit - position;\n    }\n\n   final void discardmark() {                          \n        mark = -1;\n    }\n}\n\n\n从上面列举的这些读写 bytebuffer 场景可以看出，当我们在操作 bytebuffer 的时候，需要时刻保持头脑清醒，对 bytebuffer 中哪些部分是可读的，哪些部分是可写的要有一个清醒的认识，稍不留神就会出错。在复杂的编解码逻辑中，如果使用 bytebuffer 的话，就需要不断的进行读写模式的切换，切的切的人就傻了。\n\n除了对 bytebuffer 的相关操作比较麻烦之外，jdk 对于 bytebuffer 没有设计池化管理机制，而面对大量需要使用堆外内存的场景，我们就需要不断的创建 directbuffer，directbuffer 在使用完之后，回收又是个问题。\n\njdk 自身对于 directbuffer 的回收是有延迟的，我们需要等到一次 fullgc ，这些 directbuffer 背后引用的 native memory 才能被 jvm 自动回收。所以为了及时回收这些 native memory ，我们又需要操心 directbuffer 的手动释放。\n\njdk 的 bytebuffer 不支持引用计数，没有引用计数的设计，我们就无从得知一个 directbuffer 被引用了多少次，又被释放了多少次，面对 directbuffer 引起的内存泄露问题，也就无法进行自动探测。\n\n另外 jdk 的 bytebuffer 不支持动态按需自适应扩容，当一个 bytebuffer 被创建出来之后，它的容量就固定了。但实际上，我们很难在一开始就能准确的评估出到底需要多大的 bytebuffer。分配的容量大了，会造成浪费。分配的容量小了，我们又需要每次在写入的时候判断剩余容量是否足够，如果不足，又需要手动去申请一个更大的 bytebuffer，然后在将原有 bytebuffer 中的数据迁移到新的 bytebuffer 中，想想都麻烦。\n\n还有就是当多个 jdk 的 bytebuffer 在面对合并聚合的场景，总是要先创建一个更大的 bytebuffer，然后将原有的多个 bytebuffer 中的内容在拷贝到新的 bytebuffer 中。这就涉及到了内存分配和拷贝的开销。\n\n那为什么不能利用原有的这些 bytebuffer 所占用的内存空间，在此基础上只创建一个逻辑上的视图 bytebuffer，将对视图 bytebuffer 的逻辑操作全部转移到原有的内存空间上，这样一来不就可以省去重新分配内存以及内存拷贝的开销了么 ？\n\n下面我们就来一起看下，netty 中的 bytebuf 是如何解决并完善上述问题的~~~\n\n\n# 2. netty 对于 bytebuf 的设计与实现\n\n在之前介绍 jdk bytebuffer 整体设计的时候，笔者是以 heapbytebuffer 为例将 bytebuffer 的整个设计体系串联起来的，那么本文笔者将会用 directbytebuf 为大家串联 netty bytebuf 的设计体系。\n\n\n# 2.1 bytebuf 的基本结构\n\npublic abstract class abstractbytebuf extends bytebuf {\n    int readerindex;\n    int writerindex;\n    private int markedreaderindex;\n    private int markedwriterindex;\n    private int maxcapacity;\n}\n\npublic class unpooleddirectbytebuf extends abstractreferencecountedbytebuf {\n    private int capacity;\n}\n\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n为了避免 jdk bytebuffer 在读写模式下共用一个 position 指针所引起的繁琐操作，netty 为 bytebuf 引入了两个指针，readerindex 用于指向 bytebuf 中第一个可读字节位置，writerindex 用于指向 bytebuf 中第一个可写的字节位置。有了这两个独立的指针之后，我们在对 netty bytebuf 进行读写操作的时候，就不需要进行繁琐的读写模式切换了。与之对应的 markedreaderindex，markedwriterindex 用于支持 bytebuf 相关的 mark 和 reset 操作，这一点和 jdk 中的设计保持一致。\n\n    @override\n    public bytebuf markreaderindex() {\n        markedreaderindex = readerindex;\n        return this;\n    }\n\n    @override\n    public bytebuf resetreaderindex() {\n        readerindex(markedreaderindex);\n        return this;\n    }\n\n    @override\n    public bytebuf markwriterindex() {\n        markedwriterindex = writerindex;\n        return this;\n    }\n\n    @override\n    public bytebuf resetwriterindex() {\n        writerindex(markedwriterindex);\n        return this;\n    }\n\n\n由于 jdk bytebuffer 在设计上不支持扩容机制，所以 netty 为 bytebuf 额外引入了一个新的字段 maxcapacity，用于表示 bytebuf 容量最多只能扩容至 maxcapacity。\n\n    @override\n    public int calculatenewcapacity(int minnewcapacity, int maxcapacity) {\n        if (minnewcapacity > maxcapacity) {\n            throw new illegalargumentexception(string.format(\n                    \"minnewcapacity: %d (expected: not greater than maxcapacity(%d)\",\n                    minnewcapacity, maxcapacity));\n        }\n    }\n\n\nnetty bytebuf 的 capacity 与 jdk bytebuffer 中的 capacity 含义保持一致，用于表示 bytebuf 的初始容量大小，也就是下面在创建 unpooleddirectbytebuf 的时候传入的 initialcapacity 参数。\n\npublic class unpooleddirectbytebuf extends abstractreferencecountedbytebuf {\n      // netty bytebuf 底层依赖的 jdk bytebuffer\n      bytebuffer buffer;\n      // bytebuf 初始的容量，也是真正的内存占用\n      private int capacity;\n\n      public unpooleddirectbytebuf(bytebufallocator alloc, int initialcapacity, int maxcapacity) {\n        // 设置最大可扩容的容量\n        super(maxcapacity);\n        this.alloc = alloc;\n        // 按照 initialcapacity 指定的初始容量，创建 jdk bytebuffer\n        setbytebuffer(allocatedirect(initialcapacity), false);\n    }\n\n    void setbytebuffer(bytebuffer buffer, boolean tryfree) {\n        // unpooleddirectbytebuf 底层会依赖一个 jdk 的 bytebuffer\n        // 后续对 unpooleddirectbytebuf 的操作， netty 全部会代理到 jdk bytebuffer 中\n        this.buffer = buffer;\n        // 初始指定的 bytebuf 容量 initialcapacity\n        capacity = buffer.remaining();    \n    }\n}\n\n\n由此一来，netty 中的 bytebuf 就会被 readerindex，writerindex，capacity，maxcapacity 这四个指针分割成四个部分，上图中笔者以按照不同的颜色进行了区分。\n\n * 其中 [0 , capacity) 这部分是创建 bytebuf 的时候分配的初始容量，这部分是真正占用内存的，而 [capacity , maxcapacity)这部分表示 bytebuf 可扩容的容量，这部分还未分配内存。\n * [0 , readerindex) 这部分字节是已经被读取过的字节，是可以被丢弃的范围。\n * [readerindex , writerindex) 这部分字节表示 bytebuf 中可以被读取的字节。\n * [writerindex , capacity) 这部分表示 bytebuf 的剩余容量，也就是可以写入的字节范围。\n\n这四个指针他们之间的关系为 ：0 <= readerindex <= writerindex <= capacity <= maxcapacity。\n\n  private static void checkindexbounds(final int readerindex, final int writerindex, final int capacity) {\n        if (readerindex < 0 || readerindex > writerindex || writerindex > capacity) {\n            throw new indexoutofboundsexception(string.format(\n                    \"readerindex: %d, writerindex: %d (expected: 0 <= readerindex <= writerindex <= capacity(%d))\",\n                    readerindex, writerindex, capacity));\n        }\n    }\n\n\n当我们对 bytebuf 进行读取操作的时候，需要通过 isreadable 判断 bytebuf 是否可读。以及通过 readablebytes 判断 bytebuf 具体还有多少字节可读。当 readerindex 等于 writerindex 的时候，bytebuf 就不可读了。 [0 , readerindex) 这部分字节就可以被丢弃了。\n\n    @override\n    public boolean isreadable() {\n        return writerindex > readerindex;\n    }\n\n    @override\n    public int readablebytes() {\n        return writerindex - readerindex;\n    }\n\n\n当我们对 bytebuf 进行写入操作的时候，需要通过 iswritable 判断 bytebuf 是否可写。以及通过 writablebytes 判断 bytebuf 具体还可以写多少字节。当 writerindex 等于 capacity 的时候，bytebuf 就不可写了。\n\n   @override\n    public boolean iswritable() {\n        return capacity() > writerindex;\n    }\n\n    @override\n    public int writablebytes() {\n        return capacity() - writerindex;\n    }\n\n\n当 bytebuf 的容量已经被写满，变为不可写的时候，如果继续对 bytebuf 进行写入，那么就需要扩容了，但扩容后的 capacity 最大不能超过 maxcapacity。\n\n    final void ensurewritable0(int minwritablebytes) {\n        // minwritablebytes 表示本次要写入的字节数\n        // 获取当前 writerindex 的位置\n        final int writerindex = writerindex();\n        // 为满足本次的写入操作，预期的 bytebuf 容量大小\n        final int targetcapacity = writerindex + minwritablebytes;\n        // 如果 targetcapacity 在（capacity , maxcapacity] 之间，则进行扩容\n        if (targetcapacity >= 0 & targetcapacity <= capacity()) {\n            // targetcapacity 在 [0 , capacity] 之间，则无需扩容，本来就可以满足\n            return;\n        }\n        // 扩容后的 capacity 最大不能超过 maxcapacity\n        if (checkbounds && (targetcapacity < 0 || targetcapacity > maxcapacity)) {\n            throw new indexoutofboundsexception(string.format(\n                    \"writerindex(%d) + minwritablebytes(%d) exceeds maxcapacity(%d): %s\",\n                    writerindex, minwritablebytes, maxcapacity, this));\n        }\n\n        ..... 扩容 bytebuf ......\n    }\n\n\n\n# 2.2 bytebuf 的读取操作\n\n明白了 bytebuf 基本结构之后，我们来看一下针对 bytebuf 的读写等基本操作是如何进行的。netty 支持以多种基本类型为粒度对 bytebuf 进行读写，除此之外还支持 unsigned 基本类型的转换以及大小端的转换。下面笔者以 byte 和 int 这两种基本类型为例对 bytebuf 的读取操作进行说明。\n\nbytebuf 中的 get 方法只是单纯地从 bytebuf 中读取数据，并不改变其 readerindex 的位置，我们可以通过 getbyte 从 bytebuf 中的指定位置 index 读取一个 byte 出来，也可以通过 getunsignedbyte 从 bytebuf 读取一个 byte 并转换成 unsignedbyte 。\n\npublic abstract class abstractbytebuf extends bytebuf {\n    @override\n    public byte getbyte(int index) {\n        // 检查 index 的边界，index 不能超过 capacity（index < capacity）\n        checkindex(index);\n        return _getbyte(index);\n    }\n\n    @override\n    public short getunsignedbyte(int index) {\n        // 将获取到的 byte 转换为 unsignedbyte\n        return (short) (getbyte(index) & 0xff);\n    }   \n\n    protected abstract byte _getbyte(int index);\n}\n\n\n其底层依赖的是一个抽象方法 _getbyte，由 abstractbytebuf 具体的子类负责实现。比如，在 unpooleddirectbytebuf 类的实现中，直接将 _getbyte 操作代理给其底层依赖的 jdk directbytebuffer。\n\npublic class unpooleddirectbytebuf  {\n    // 底层依赖 jdk 的 directbytebuffer\n    bytebuffer buffer;\n\n    @override\n    protected byte _getbyte(int index) {\n        return buffer.get(index);\n    }\n}\n\n\n而在 unpooledunsafedirectbytebuf 类的实现中，则是通过 sun.misc.unsafe 直接从对应的内存地址中读取。\n\npublic class unpooledunsafedirectbytebuf {\n    // 直接操作 os 的内存地址\n    long memoryaddress;\n    @override\n    protected byte _getbyte(int index) {\n        // 底层依赖 platformdependent0，直接通过内存地址读取 byte\n        return unsafebytebufutil.getbyte(addr(index));\n    }\n\n    final long addr(int index) {\n        // 获取偏移 index 对应的内存地址\n        return memoryaddress + index;\n    }\n}\n\nfinal class platformdependent0 {\n  // sun.misc.unsafe\n  static final unsafe unsafe;\n  static byte getbyte(long address) {\n        return unsafe.getbyte(address);\n    }\n}\n\n\nnetty 另外还提供了批量读取 bytes 的操作，比如我们可以通过 getbytes 方法将 bytebuf 中的数据读取到一个字节数组 byte[] 中，也可以读取到另一个 bytebuf 中。\n\n    @override\n    public bytebuf getbytes(int index, byte[] dst) {\n        getbytes(index, dst, 0, dst.length);\n        return this;\n    }\n\n    public abstract bytebuf getbytes(int index, byte[] dst, int dstindex, int length);\n\n    @override\n    public bytebuf getbytes(int index, bytebuf dst, int length) {\n        getbytes(index, dst, dst.writerindex(), length);\n        // 调整 dst 的  writerindex\n        dst.writerindex(dst.writerindex() + length);\n        return this;\n    }\n    \n    // 注意这里的 getbytes 方法既不会改变原来 bytebuf 的 readerindex 和 writerindex\n    // 也不会改变目的 bytebuf 的 readerindex 和 writerindex\n    public abstract bytebuf getbytes(int index, bytebuf dst, int dstindex, int length);\n\n\n通过 getbytes 方法将原来 bytebuf 的数据读取到目的 bytebuf 之后，原来 bytebuf 的 readerindex 不会发生变化，但是目的 bytebuf 的 writerindex 会重新调整。\n\n对于 unpooleddirectbytebuf 类的具体实现来说自然是将 getbytes 的操作直接代理给其底层依赖的 jdk directbytebuffer。对于 unpooledunsafedirectbytebuf 类的具体实现来说，则是通过 unsafe.copymemory 直接根据内存地址进行拷贝。\n\n而 bytebuf 中的 read 方法则不仅会从 bytebuf 中读取数据，而且会改变其 readerindex 的位置。比如，readbyte 方法首先会通过前面介绍的 _getbyte 从 bytebuf 中读取一个字节，然后将 readerindex 向后移动一位。\n\n   @override\n    public byte readbyte() {\n        checkreadablebytes0(1);\n        int i = readerindex;\n        byte b = _getbyte(i);\n        readerindex = i + 1;\n        return b;\n    }\n\n\n同样 netty 也提供了从 bytebuf 中批量读取数据的方法 readbytes，我们可以将一个 bytebuf 中的数据通过 readbytes 方法读取到另一个 bytebuf 中。但是这里，netty 将会改变原来 bytebuf 的 readerindex 以及目的 bytebuf 的 writerindex。\n\n   @override\n    public bytebuf readbytes(bytebuf dst, int length) {\n        readbytes(dst, dst.writerindex(), length);\n        // 改变 dst 的 writerindex\n        dst.writerindex(dst.writerindex() + length);\n        return this;\n    }\n\n\n另外我们还可以明确指定 dstindex，使得我们可以从目的 bytebuf 中的某一个位置处开始拷贝原来 bytebuf 中的数据，但这里只会改变原来 bytebuf 的 readerindex，并不会改变目的 bytebuf 的 writerindex。这也很好理解，因为我们在写入目的 bytebuf 的时候已经明确指定了 writerindex（dstindex），自然在写入完成之后，writerindex 的位置并不需要改变。\n\n    @override\n    public bytebuf readbytes(bytebuf dst, int dstindex, int length) {\n        checkreadablebytes(length);\n        getbytes(readerindex, dst, dstindex, length);\n        // 改变原来 bytebuf 的 readerindex\n        readerindex += length;\n        return this;\n    }\n\n\n除此之外，netty 还支持将 bytebuf 中的数据读取到不同的目的地，比如，读取到 jdk bytebuffer 中，读取到 filechannel 中，读取到 outputstream 中，以及读取到 gatheringbytechannel 中。\n\npublic abstract bytebuf readbytes(bytebuffer dst);\npublic abstract bytebuf readbytes(outputstream out, int length) throws ioexception;\npublic abstract int readbytes(gatheringbytechannel out, int length) throws ioexception;\npublic abstract int readbytes(filechannel out, long position, int length) throws ioexception;\n\n\nnetty 除了支持以 byte 为粒度对 bytebuf 进行读写之外，还同时支持以多种基本类型对 bytebuf 进行读写，这里笔者以 int 类型为例进行说明。\n\n我们可以通过 readint() 从 bytebuf 中读取一个 int 类型的数据出来，随后 bytebuf 的 readerindex 向后移动 4 个位置。\n\n   @override\n    public int readint() {\n        checkreadablebytes0(4);\n        int v = _getint(readerindex);\n        readerindex += 4;\n        return v;\n    }\n\n    protected abstract int _getint(int index);\n\n\n同理，真正负责读取数据的方法 _getint 方法需要由 abstractbytebuf 具体的子类实现，但这里和 _getbyte 不同的是，_getint 需要考虑字节序的问题，由于网络协议采用的是大端字节序传输，所以 netty 的 bytebuf 默认也是大端字节序。\n\n在 unpooleddirectbytebuf 的实现中，同样也是将 getint 的操作直接代理给其底层依赖的 jdk directbytebuffer。\n\npublic class unpooleddirectbytebuf  {\n    @override\n    protected int _getint(int index) {\n        // 代理给其底层依赖的 jdk directbytebuffer\n        return buffer.getint(index);\n    }\n}\n\n\n在 unpooledunsafedirectbytebuf 的实现中，由于是通过 sun.misc.unsafe 直接对内存地址进行操作，所以需要考虑字节序转换的细节。netty 的 bytebuf 默认是大端字节序，所以这里直接依次将低地址的字节放到 int 数据的高位就可以了。\n\npublic class unpooledunsafedirectbytebuf {\n    @override\n    protected int _getint(int index) {\n        return unsafebytebufutil.getint(addr(index));\n    }\n}\n\nfinal class unsafebytebufutil {\n    static int getint(long address) {    \n        return platformdependent.getbyte(address) << 24 |\n               (platformdependent.getbyte(address + 1) & 0xff) << 16 |\n               (platformdependent.getbyte(address + 2) & 0xff) <<  8 |\n               platformdependent.getbyte(address + 3)  & 0xff;\n    }\n}\n\n\n同时 netty 也支持以小端字节序来从 bytebuf 中读取 int 数据，这里就涉及到字节序的转换了。\n\n    @override\n    public int readintle() {\n        checkreadablebytes0(4);\n        int v = _getintle(readerindex);\n        readerindex += 4;\n        return v;\n    }\n\n protected abstract int _getintle(int index);\n\n\n在 unpooleddirectbytebuf 的实现中，首先通过其依赖的 jdk directbytebuffer 以大端序读取一个 int 数据，然后通过 bytebufutil.swapint 切换成小端序返回。\n\npublic class unpooleddirectbytebuf  {\n    @override\n    protected int _getintle(int index) {\n        // 切换字节序，从大端变小端\n        return bytebufutil.swapint(buffer.getint(index));\n    }\n}\n\n\n在 unpooledunsafedirectbytebuf 的实现中，则是直接将低地址上的字节依次放到 int 数据的低位上就可以了。\n\npublic class unpooledunsafedirectbytebuf {\n    @override\n    protected int _getintle(int index) {\n        return unsafebytebufutil.getintle(addr(index));\n    }\n}\n\nfinal class unsafebytebufutil {\n    static int getintle(long address) {\n        return platformdependent.getbyte(address) & 0xff |\n               (platformdependent.getbyte(address + 1) & 0xff) <<  8 |\n               (platformdependent.getbyte(address + 2) & 0xff) << 16 |\n               platformdependent.getbyte(address + 3) << 24;\n    }\n}\n\n\n另外 netty 也支持从 bytebuf 中读取基本类型的 unsigned 类型。\n\n    @override\n    public long readunsignedint() {\n        return readint() & 0xffffffffl;\n    }\n\n    @override\n    public long readunsignedintle() {\n        return readintle() & 0xffffffffl;\n    }\n\n\n其他基本类型的相关读取操作实现的逻辑都是大同小异，笔者就不一一列举了。\n\n\n# 2.3 discardreadbytes\n\n随着 readbytes 方法的不断调用， bytebuf 中的 readerindex 也会不断的向后移动，netty 对 readerindex 的设计有两层语义：\n\n 1. 第一层的语义比较明显，就是用来表示当前 bytebuf 的读取位置，当我们调用 readbytes 方法的时候就是从 readerindex 开始读取数据，当 readerindex 等于 writerindex 的时候，bytebuf 就不可读取了。\n 2. 第二层语义比较含蓄，它是用来表示当前 bytebuf 可以被丢弃的字节数，因为 readerindex 用来指示当前的读取位置，那么位于 readerindex 之前的字节肯定是已经被读取完毕了，已经被读取的字节继续驻留在 bytebuf 中就没有必要了，还不如把空间腾出来，还能在多写入些数据。\n\nimage.png\n\n所以一个 bytebuf 真正的剩余可写容量的计算方式除了上小节中介绍的 writablebytes() 方法返回的字节数之外还需要在加上 readerindex。\n\n    @override\n    public int writablebytes() {\n        return capacity() - writerindex;\n    }\n\n\n举个具体点的例子就是，当我们准备向一个 bytebuf 写入 n 个字节时，如果 writablebytes() 小于 n，那么就表示当前 bytebuf 的剩余容量不能满足本次写入的字节数。\n\n但是 readerindex + writablebytes()大于等于 n ， 则表示如果我们将 bytebuf 中已经读取的字节数丢弃的话，那么就可以满足本次写入的请求。\n\n在这种情况下，我们就可以使用 discardreadbytes() 方法将 readerindex 之前的字节丢弃掉，这样一来，可写的字节容就可以满足本次写入要求了，那么如果丢弃呢 ？\n\n我们先来看 readerindex < writerindex 的情况，这种情况下表示 bytebuf 中还有未读取的字节。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\nbytebuf 目前可读取的字节范围为：[readerindex, writerindex)，位于 readerindex 之前的字节均可以被丢弃，接下来我们就需要将 [readerindex, writerindex) 这段范围的字节全部拷贝到 bytebuf 最前面，直接覆盖 readerindex 之前的字节。\n\n然后调整 readerindex 和 writerindex 的位置，因为 readerindex 之前的字节现在已经全部被可读字节覆盖了，所以 readerindex 重新调整为 0 ，writerindex 向前移动 readerindex 大小。这样一来，当前 bytebuf 的可写容量就多出了 readerindex 大小。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n另外一种情况是 readerindex = writerindex 的情况，这种情况下表示 bytebuf 中已经没有可读字节了。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n既然 bytebuf 中已经没有任何可读字节了，自然也就不需要将可读字节拷贝到 bytebuf 的开头了，直接将 readerindex 和 writerindex 重新调整为 0 即可。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\npublic abstract class abstractbytebuf extends bytebuf {\n    @override\n    public bytebuf discardreadbytes() {\n        // readerindex 为 0 表示没有可以丢弃的字节\n        if (readerindex == 0) {\n            return this;\n        }\n\n        if (readerindex != writerindex) {\n            // 将 [readerindex, writerindex) 这段字节范围移动到 bytebuf 的开头\n            // 也就是丢弃 readerindex 之前的字节\n            setbytes(0, this, readerindex, writerindex - readerindex);\n            // writerindex 和 readerindex 都向前移动 readerindex 大小\n            writerindex -= readerindex;\n            // 重新调整 markedreaderindex 和 markedwriterindex 的位置\n            // 都对应向前移动 readerindex 大小。\n            adjustmarkers(readerindex);\n            readerindex = 0;\n        } else {\n            // readerindex = writerindex 表示当前 bytebuf 已经不可读了\n            // 将 readerindex 之前的字节全部丢弃，bytebuf 恢复到最初的状态\n            // 整个 bytebuf 的容量都可以被写入\n            ensureaccessible();\n            adjustmarkers(readerindex);\n            writerindex = readerindex = 0;\n        }\n        return this;\n    }\n}\n\n\n如果 bytebuf 存在可以被丢弃的字节的时候（readerindex > 0），只要我们调用 discardreadbytes() 就会无条件丢弃 readerindex 之前的字节。\n\nnetty 还另外提供了 discardsomereadbytes() 方法进行有条件丢弃字节，丢弃条件有如下两种：\n\n 1. 当 bytebuf 已经不可读的时候，则无条件丢弃已读字节。\n 2. 当已读的字节数超过整个 bytebuf 一半容量时才会丢弃已读字节。否则无条件丢弃的话，收益就不高了。\n\n    @override\n    public bytebuf discardsomereadbytes() {\n        if (readerindex > 0) {\n            // 当 bytebuf 已经不可读了，则无条件丢弃已读字节\n            if (readerindex == writerindex) {\n                adjustmarkers(readerindex);\n                writerindex = readerindex = 0;\n                return this;\n            }\n            // 当已读的字节数超过整个 bytebuf 的一半容量时才会丢弃已读字节\n            if (readerindex >= capacity() >>> 1) {\n                setbytes(0, this, readerindex, writerindex - readerindex);\n                writerindex -= readerindex;\n                adjustmarkers(readerindex);\n                readerindex = 0;\n                return this;\n            }\n        }\n        return this;\n    }\n\n\nnetty 设计的这个丢弃字节的方法在解码的场景非常有用，由于 tcp 是一个面向流的网络协议，它只会根据滑动窗口的大小进行字节流的发送，所以我们在应用层接收到的数据可能是一个半包也可能是一个粘包，反正不会是一个完整的数据包。\n\n这就要求我们在解码的时候，首先要判断 bytebuf 中的数据是否构成一个完成的数据包，如果构成一个数据包，才会去读取 bytebuf 中的字节，然后解码，随后 readerindex 向后移动。\n\n如果不够一个数据包，那就需要将 bytebuf 累积缓存起来，一直等到一个完整的数据包到来。一种极端的情况是，即使我们已经解码很多次了，但是缓存的 bytebuf 中仍然还有半包，由于不断的会有粘包过来，这就导致 bytebuf 会越来越大。由于已经解码了很多次，所以 bytebuf 中可以被丢弃的字节占据了很大的内存空间，如果半包情况持续存在，将会导致 outofmemory。\n\n所以 netty 规定，如果已经解码了 16 次之后，bytebuf 中仍然有半包的情况，那么就会调用这里的 discardsomereadbytes() 将已经解码过的字节全部丢弃，节省不必要的内存开销。\n\n\n# 2.4 bytebuf 的写入操作\n\nbytebuf 的写入操作与读取操作互为相反的操作，每一个读取方法 getbytes , readbytes , readint 等都有一个对应的 setbytes , writebytes , writeint 等基础类型的写入操作。\n\n和 get 方法一样，set 相关的方法也只是单纯的向 bytebuf 中写入数据，并不会改变其 writerindex 的位置，我们可以通过 setbyte 向 bytebuf 中的某一个指定位置 index 写入数据 value。\n\n    @override\n    public bytebuf setbyte(int index, int value) {\n        checkindex(index);\n        _setbyte(index, value);\n        return this;\n    }\n\n    protected abstract void _setbyte(int index, int value);\n\n\n执行具体的写入操作同样也是一个抽象方法，其具体的实现由 abstractbytebuf 具体的子类负责。对于 unpooleddirectbytebuf 的实现来说，_setbyte 操作直接会代理给其底层依赖的 jdk directbytebuffer。\n\npublic class unpooleddirectbytebuf  {\n    // 底层依赖 jdk 的 directbytebuffer\n    bytebuffer buffer;\n\n    @override\n    protected void _setbyte(int index, int value) {\n        buffer.put(index, (byte) value);\n    }\n}\n\n\n对于 unpooledunsafedirectbytebuf 的实现来说，则是直接通过 sun.misc.unsafe 向对应的内存地址（memoryaddress + index）写入 byte。\n\npublic class unpooledunsafedirectbytebuf {\n    // 直接操作 os 的内存地址，不依赖 jdk 的 buffer\n    long memoryaddress;\n\n   @override\n    protected void _setbyte(int index, int value) {\n        // 底层依赖 platformdependent0，直接向内存地址写入 byte\n        unsafebytebufutil.setbyte(addr(index), value);\n    }\n\n    final long addr(int index) {\n        // 获取偏移 index 对应的内存地址\n        return memoryaddress + index;\n    }\n}\n\nfinal class platformdependent0 {\n  // sun.misc.unsafe\n  static final unsafe unsafe;\n  static void putbyte(long address, byte value) {\n        unsafe.putbyte(address, value);\n  }\n}\n\n\nnetty 另外也提供了向 bytebuf 批量写入 bytes 的操作，setbytes 方法用于向 bytebuf 的指定位置 index 批量写入一个字节数组 byte[] 中的数据。\n\n   @override\n    public bytebuf setbytes(int index, byte[] src) {\n        setbytes(index, src, 0, src.length);\n        return this;\n    }\n\n    public abstract bytebuf setbytes(int index, byte[] src, int srcindex, int length);\n\n\n对于 unpooleddirectbytebuf 的实现来说，同样也是将 setbytes 的操作直接代理给 jdk directbytebuffer，将字节数组 byte[] 中的字节直接写入 directbytebuffer 中。\n\n对于 unpooledunsafedirectbytebuf 的实现来说，则是直接操作字节数组和 bytebuf 的内存地址，通过 unsafe.copymemory 将字节数组对应内存地址中的数据拷贝到 bytebuf 相应的内存地址上。\n\n我们还可以通过 setbytes 方法将其他 bytebuf 中的字节数据写入到 bytebuf 中。\n\n    @override\n    public bytebuf setbytes(int index, bytebuf src, int length) {\n        setbytes(index, src, src.readerindex(), length);\n        // 调整 src 的  readerindex\n        src.readerindex(src.readerindex() + length);\n        return this;\n    }\n\n    // 注意这里的 setbytes 方法既不会改变原来 bytebuf 的 readerindex 和 writerindex\n    // 也不会改变目的 bytebuf 的 readerindex 和 writerindex\n    public abstract bytebuf setbytes(int index, bytebuf src, int srcindex, int length);\n\n\n这里需要注意的是被写入 bytebuf 的 writerindex 并不会改变，但是原来 bytebuf 的 readerindex 会重新调整。\n\nbytebuf 中的 write 方法底层依赖的是相关的 set 方法，不同的是 write 方法会改变 bytebuf 中 writerindex 的位置。比如，我们通过 writebyte 方法向 bytebuf 中写入一个字节之后，writerindex 就会向后移动一位。\n\n    @override\n    public bytebuf writebyte(int value) {\n        ensurewritable0(1);\n        _setbyte(writerindex++, value);\n        return this;\n    }\n\n\n我们也可以通过 writebytes 向 bytebuf 中批量写入数据，将一个字节数组中的数据或者另一个 bytebuf 中的数据写入到 bytebuf 中，但是这里，netty 将会改变被写入 bytebuf 的 writerindex 以及数据来源 bytebuf 的 readerindex。\n\n    @override\n    public bytebuf writebytes(bytebuf src, int length) {\n        writebytes(src, src.readerindex(), length);\n        // 调整数据来源 bytebuf 的 readerindex\n        src.readerindex(src.readerindex() + length);\n        return this;\n    }\n\n\n如果我们明确指定了从数据来源 bytebuf 中的哪一个位置（srcindex）开始读取数据，那么数据来源 bytebuf 中的 readerindex 将不会被改变，只会改变被写入 bytebuf 的 writerindex。\n\n    @override\n    public bytebuf writebytes(bytebuf src, int srcindex, int length) {\n        ensurewritable(length);\n        setbytes(writerindex, src, srcindex, length);\n        // 调整被写入 bytebuf 的 writerindex\n        writerindex += length;\n        return this;\n    }\n\n\n除此之外，netty 还支持从不同的数据来源向 bytebuf 批量写入数据，比如，从 jdk bytebuffer ，从 filechannel ，从 inputstream ，以及从 scatteringbytechannel 中。\n\npublic bytebuf writebytes(bytebuffer src) \npublic int writebytes(inputstream in, int length)\npublic int writebytes(scatteringbytechannel in, int length) throws ioexception\npublic int writebytes(filechannel in, long position, int length) throws ioexception\n\n\nnetty 除了支持以 byte 为粒度向 bytebuf 中写入数据之外，还同时支持以多种基本类型为粒度向写入 bytebuf ，这里笔者以 int 类型为例进行说明。\n\n我们可以通过 writeint() 向 bytebuf 写入一个 int 类型的数据，随后 bytebuf 的 writerindex 向后移动 4 个位置。\n\n    @override\n    public bytebuf writeint(int value) {\n        ensurewritable0(4);\n        _setint(writerindex, value);\n        writerindex += 4;\n        return this;\n    }\n\n   protected abstract void _setint(int index, int value);\n\n\n和写入 byte 数据不同的是，这里需要考虑字节序，netty bytebuf 默认是大端字节序，和网络协议传输使用的字节序保持一致。这里我们需要将待写入数据 value 的高位依次放入到 bytebuf 的低地址上。\n\npublic class unpooledunsafedirectbytebuf {\n   @override\n    protected void _setint(int index, int value) {\n        // 以大端字节序写入 bytebuf \n        unsafebytebufutil.setint(addr(index), value);\n    }\n}\n\nfinal class unsafebytebufutil {\n   static void setint(long address, int value) {\n            platformdependent.putbyte(address, (byte) (value >>> 24));\n            platformdependent.putbyte(address + 1, (byte) (value >>> 16));\n            platformdependent.putbyte(address + 2, (byte) (value >>> 8));\n            platformdependent.putbyte(address + 3, (byte) value);   \n    }\n}\n\n\n同时 netty 也支持以小端字节序向 bytebuf 写入数据。\n\n    @override\n    public bytebuf writeintle(int value) {\n        ensurewritable0(4);\n        _setintle(writerindex, value);\n        writerindex += 4;\n        return this;\n    }\n\n   protected abstract void _setintle(int index, int value);\n\n\n这里需要将待写入数据 value 的低位依次放到 bytebuf 的低地址上。\n\npublic class unpooledunsafedirectbytebuf {\n    @override\n    protected void _setintle(int index, int value) {\n        // // 以小端字节序写入 bytebuf \n        unsafebytebufutil.setintle(addr(index), value);\n    }\n}\n\nfinal class unsafebytebufutil {\n   static void setintle(long address, int value) {\n            platformdependent.putbyte(address, (byte) value);\n            platformdependent.putbyte(address + 1, (byte) (value >>> 8));\n            platformdependent.putbyte(address + 2, (byte) (value >>> 16));\n            platformdependent.putbyte(address + 3, (byte) (value >>> 24));\n    }\n}\n\n\n\n# 2.5 bytebuf 的扩容机制\n\n在每次向 bytebuf 写入数据的时候，netty 都会调用 ensurewritable0 方法来判断当前 bytebuf 剩余可写容量（capacity - writerindex）是否能够满足本次需要写入的数据大小 minwritablebytes。如果剩余容量不足，那么就需要对 bytebuf 进行扩容，但扩容后的容量不能超过 maxcapacity 的大小。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n    final void ensurewritable0(int minwritablebytes) {\n        final int writerindex = writerindex();\n        // 为满足本次的写入操作，预期的 bytebuf 容量大小\n        final int targetcapacity = writerindex + minwritablebytes;\n        // 剩余容量可以满足本次写入要求，直接返回，不需要扩容\n        if (targetcapacity >= 0 & targetcapacity <= capacity()) {\n            return;\n        }\n        // 扩容后的容量不能超过 maxcapacity\n        if (checkbounds && (targetcapacity < 0 || targetcapacity > maxcapacity)) {\n            ensureaccessible();\n            throw new indexoutofboundsexception(string.format(\n                    \"writerindex(%d) + minwritablebytes(%d) exceeds maxcapacity(%d): %s\",\n                    writerindex, minwritablebytes, maxcapacity, this));\n        }\n\n        // 如果 targetcapacity 在（capacity , maxcapacity] 之间，则进行扩容\n        // fastwritable 表示在不涉及到 memory reallocation or data-copy 的情况下，当前 bytebuf 可以直接写入的容量\n        // 对于 unpooleddirectbuffer 这里的 fastwritable = capacity - writerindex\n        // pooleddirectbuffer 有另外的实现，这里先暂时不需要关注\n        final int fastwritable = maxfastwritablebytes();\n        // 计算扩容后的容量 newcapacity\n        // 对于 unpooleddirectbuffer 来说这里直接通过 calculatenewcapacity 计算扩容后的容量。\n        int newcapacity = fastwritable >= minwritablebytes ? writerindex + fastwritable\n                : alloc().calculatenewcapacity(targetcapacity, maxcapacity);\n\n        // 根据 new capacity 对 bytebuf 进行扩容\n        capacity(newcapacity);\n    }\n\n\n# 2.5.1 newcapacity 的计算逻辑\n\nbytebuf 的初始默认 capacity 为 256 个字节，初始默认 maxcapacity 为 integer.max_value 也就是 2g 大小。\n\npublic abstract class abstractbytebufallocator implements bytebufallocator {\n    // bytebuf 的初始默认 capacity\n    static final int default_initial_capacity = 256;\n    // bytebuf 的初始默认 max_capacity \n    static final int default_max_capacity = integer.max_value;\n\n    @override\n    public bytebuf directbuffer() {\n        return directbuffer(default_initial_capacity, default_max_capacity);\n    }\n}\n\n\n为满足本次写入操作，对 bytebuf 的最小容量要求为 minnewcapacity，它的值就是在 ensurewritable0 方法中计算出来的 targetcapacity, 计算方式为： minnewcapacity = writerindex + minwritablebytes（本次将要写入的字节数）。\n\n在 bytebuf 的扩容逻辑中，netty 设置了一个重要的阈值 calculate_threshold, 大小为 4m，它决定了 bytebuf 扩容的尺度。\n\n    // 扩容的尺度\n    static final int calculate_threshold = 1048576 * 4; // 4 mib page\n\n\n如果 minnewcapacity 恰好等于 calculate_threshold，那么扩容后的容量 newcapacity 就是 4m。\n\n如果 minnewcapacity 大于 calculate_threshold，那么 newcapacity 就会按照 4m 的尺度进行扩容，具体的扩容逻辑如下：\n\n首先通过 minnewcapacity / threshold * threshold 计算出一个准备扩容之前的基准线，后面就会以此基准线为基础，按照 calculate_threshold 的粒度进行扩容。\n\n该基准线的要求必须是 calculate_threshold 的最小倍数，而且必须要小于等于 minnewcapacity。\n\n什么意思呢 ？ 假设 minnewcapacity 为 5m，那么它的扩容基准线就是 4m ， 这种情况下扩容之后的容量 newcapacity = 4m + calculate_threshold = 8m 。\n\n如果计算出来的基准线超过了 maxcapacity - 4m , 那么 newcapacity 直接就扩容到 maxcapacity 。\n\n如果 minnewcapacity 小于 calculate_threshold，那么 newcapacity 就会从 64 开始，一直循环 double , 也就是按照 64 的倍数进行扩容。直到 newcapacity 大于等于 minnewcapacity。\n\n        int newcapacity = 64;\n        while (newcapacity < minnewcapacity) {\n            newcapacity <<= 1;\n        }\n\n\n * 如果 minnewcapacity 在 [0 , 64] 这段范围内 ， 那么扩容后的 newcapacity 就是 64\n * 如果 minnewcapacity 在 [65 , 128] 这段范围内 ， 那么扩容后的 newcapacity 就是 128 。\n * 如果 minnewcapacity 在 [129 , 256] 这段范围内 ， 那么扩容后的 newcapacity 就是 256 。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\npublic abstract class abstractbytebufallocator implements bytebufallocator {\n\n    @override\n    public int calculatenewcapacity(int minnewcapacity, int maxcapacity) {\n        // 满足本次写入操作的最小容量 minnewcapacity 不能超过 maxcapacity\n        if (minnewcapacity > maxcapacity) {\n            throw new illegalargumentexception(string.format(\n                    \"minnewcapacity: %d (expected: not greater than maxcapacity(%d)\",\n                    minnewcapacity, maxcapacity));\n        }\n        // 用于决定扩容的尺度\n        final int threshold = calculate_threshold; // 4 mib page\n\n        if (minnewcapacity == threshold) {\n            return threshold;\n        }\n\n        // if over threshold, do not double but just increase by threshold.\n        if (minnewcapacity > threshold) {\n            // 计算扩容基准线。\n            // 要求必须是 calculate_threshold 的最小倍数，而且必须要小于等于 minnewcapacity\n            int newcapacity = minnewcapacity / threshold * threshold;\n            if (newcapacity > maxcapacity - threshold) {\n                newcapacity = maxcapacity;\n            } else {\n                // 按照 threshold (4m)扩容\n                newcapacity += threshold;\n            }\n            return newcapacity;\n        }\n\n        // not over threshold. double up to 4 mib, starting from 64.\n        // 按照 64 的倍数进行扩容。但 newcapacity 需要大于等于 minnewcapacity。\n        int newcapacity = 64;\n        while (newcapacity < minnewcapacity) {\n            newcapacity <<= 1;\n        }\n\n        return math.min(newcapacity, maxcapacity);\n    }\n}\n\n\n# 2.5.2 bytebuf 的扩容逻辑\n\npublic class unpooleddirectbytebuf  {\n    // 底层依赖 jdk 的 directbytebuffer\n    bytebuffer buffer;\n}\n\n\n对于 unpooleddirectbytebuf 来说，其底层真正存储数据的地方其实是依赖 jdk 中的 directbytebuffer，扩容的逻辑很简单，就是首先根据上一小节计算出的 newcapacity 重新分配一个新的 jdk directbytebuffer ， 然后将原来 directbytebuffer 中的数据拷贝到新的 directbytebuffer 中，最后释放原来的 directbytebuffer，将新的 directbytebuffer 设置到 unpooleddirectbytebuf 中。\n\npublic class unpooleddirectbytebuf  {\n    void setbytebuffer(bytebuffer buffer, boolean tryfree) {\n        if (tryfree) {\n            bytebuffer oldbuffer = this.buffer;\n            // 释放原来的 buffer\n            freedirect(oldbuffer);\n        }\n        // 重新设置新的 buffer\n        this.buffer = buffer;\n        capacity = buffer.remaining();\n    }\n}\n\n\n对于 unpooledunsafedirectbytebuf 来说，由于它直接依赖的是 os 内存地址，对 bytebuf 的相关操作都是直接操作内存地址进行，所以 unpooledunsafedirectbytebuf 的扩容逻辑除了要执行上面的内容之外，还需要将新 directbytebuffer 的内存地址设置到 memoryaddress 中。\n\npublic class unpooledunsafedirectbytebuf extends unpooleddirectbytebuf {\n    // bytebuf 的内存地址\n    long memoryaddress;\n\n    @override\n    final void setbytebuffer(bytebuffer buffer, boolean tryfree) {\n        super.setbytebuffer(buffer, tryfree);\n        // 设置成新 buffer 的内存地址\n        memoryaddress = platformdependent.directbufferaddress(buffer);\n    }\n}\n\n\n下面是完整的扩容操作逻辑：\n\npublic class unpooleddirectbytebuf  {\n    // 底层依赖 jdk 的 directbytebuffer\n    bytebuffer buffer;\n\n    @override\n    public bytebuf capacity(int newcapacity) {\n        // newcapacity 不能超过 maxcapacity\n        checknewcapacity(newcapacity);\n        int oldcapacity = capacity;\n        if (newcapacity == oldcapacity) {\n            return this;\n        }\n        // 计算扩容之后需要拷贝的字节数\n        int bytestocopy;\n        if (newcapacity > oldcapacity) {\n            bytestocopy = oldcapacity;\n        } else {\n            ........ 缩容 .......\n        }\n        bytebuffer oldbuffer = buffer;\n        // 根据 newcapacity 分配一个新的 bytebuffer（jdk）\n        bytebuffer newbuffer = allocatedirect(newcapacity);\n        oldbuffer.position(0).limit(bytestocopy);\n        newbuffer.position(0).limit(bytestocopy);\n        // 将原来 oldbuffer 中的数据拷贝到 newbuffer 中\n        newbuffer.put(oldbuffer).clear();\n        // 释放 oldbuffer，设置 newbuffer\n        // 对于 unpooledunsafedirectbytebuf 来说就是将 newbuffer 的地址设置到 memoryaddress 中\n        setbytebuffer(newbuffer, true);\n        return this;\n    }\n}\n\n\n# 2.5.3 强制扩容\n\n前面介绍的 ensurewritable 方法会检查本次写入的数据大小 minwritablebytes 是否超过 bytebuf 的最大可写容量：maxcapacity - writerindex。\n\npublic bytebuf ensurewritable(int minwritablebytes) \n\n\n如果超过，则会抛出 indexoutofboundsexception 异常停止扩容，netty 提供了另外一个带有 force 参数的扩容方法，用来决定在这种情况下是否强制进行扩容。\n\n public int ensurewritable(int minwritablebytes, boolean force) \n\n\n当 minwritablebytes 已经超过 bytebuf 的最大可写容量得时候：\n\n * force = false ， 那么停止扩容，直接返回，不抛异常。\n * force = true , 则进行强制扩容，将 bytebuf 扩容至 maxcapacity，但是如果当前容量已经达到了 maxcapacity，则停止扩容 。\n\n带 force 参数的 ensurewritable 并不会抛出异常，而是通过返回状态码来通知调用者 bytebuf 的容量情况。\n\n 1. 返回 0 表示，bytebuf 当前可写容量可以满足本次写入操作的需求，不需要扩容\n 2. 返回 1 表示，本次写入的数据大小已经超过了 bytebuf 的最大可写容量，但 bytebuf 的容量已经达到了 maxcapacity，无法进行扩容。\n 3. 返回 3 表示，本次写入的数据大小已经超过了 bytebuf 的最大可写容量，这种情况下，强制将容量扩容至 maxcapacity。\n 4. 返回 2 表示，执行正常的扩容逻辑。\n\n返回值 0 和 2 均表示 bytebuf 容量（扩容前或者扩容后）可以满足本次写入的数据大小，而返回值 1 和 3 表示 bytebuf 容量（扩容前或者扩容后）都无法满足本次写入的数据大小。\n\n    @override\n    public int ensurewritable(int minwritablebytes, boolean force) {\n        // 如果剩余容量可以满足本次写入操作，则不会扩容，直接返回\n        if (minwritablebytes <= writablebytes()) {\n            return 0;\n        }\n\n        final int maxcapacity = maxcapacity();\n        final int writerindex = writerindex();\n        // 如果本次写入的数据大小已经超过了 bytebuf 的最大可写容量 maxcapacity - writerindex\n        if (minwritablebytes > maxcapacity - writerindex) {\n            // force = false ， 那么停止扩容，直接返回\n            // force = true, 直接扩容到 maxcapacity，如果当前 capacity 已经等于 maxcapacity 了则停止扩容\n            if (!force || capacity() == maxcapacity) {\n                return 1;\n            }\n            // 虽然扩容之后还是无法满足写入需求，但还是强制扩容至 maxcapacity\n            capacity(maxcapacity);\n            return 3;\n        }\n        // 下面就是普通的扩容逻辑\n        int fastwritable = maxfastwritablebytes();\n        int newcapacity = fastwritable >= minwritablebytes ? writerindex + fastwritable\n                : alloc().calculatenewcapacity(writerindex + minwritablebytes, maxcapacity);\n\n        // adjust to the new capacity.\n        capacity(newcapacity);\n        return 2;\n    }\n\n\n# 2.5.4 自适应动态扩容\n\nnetty 在接收网络数据的过程中，其实一开始是很难确定出该用多大容量的 bytebuf 去接收的，所以 netty 在一开始会首先预估一个初始容量 default_initial (2048)。\n\npublic class adaptiverecvbytebufallocator {\n    static final int default_initial = 2048;\n}\n\n\n用初始容量为 2048 大小的 bytebuf 去读取 socket 中的数据，在每一次读取完 socket 之后，netty 都会评估 bytebuf 的容量大小是否合适。如果每一次都能把 bytebuf 装满，那说明我们预估的容量太小了，socket 中还有更多的数据，那么就需要对 bytebuf 进行扩容，下一次读取 socket 的时候就换一个容量更大的 bytebuf。\n\n private final class handleimpl extends maxmessagehandle {\n        @override\n        public void lastbytesread(int bytes) {\n            // bytes 为本次从 socket 中真实读取的数据大小\n            // attemptedbytesread 为 bytebuf 可写的容量大小，初始为 2048\n            if (bytes == attemptedbytesread()) {\n                // 如果本次读取 socket 中的数据将 bytebuf 装满了\n                // 那么就对 bytebuf 进行扩容，在下一次读取的时候用更大的 bytebuf 去读\n                record(bytes);\n            }\n            // 记录本次从 socket 中读取的数据大小\n            super.lastbytesread(bytes);\n        }\n }\n\n\nnetty 会在一个 read loop 中不停的读取 socket 中的数据直到数据被读取完毕或者读满 16 次，结束 read loop 停止读取。bytebuf 越大那么 netty 读取的次数就越少，bytebuf 越小那么 netty 读取的次数就越多，所以需要一种机制将 bytebuf 的容量控制在一个合理的范围内。\n\nnetty 会统计每一轮 read loop 总共读取了多少数据 —— totalbytesread。\n\n public abstract class maxmessagehandle implements extendedhandle {\n        // 用于统计在一轮 read loop 中总共接收到客户端连接上的数据大小\n        private int totalbytesread;\n }\n\n\n在每一轮的 read loop 结束之后，netty 都会根据这个 totalbytesread 来判断是否应该对 bytebuf 进行扩容或者缩容，这样在下一轮 read loop 开始的时候，netty 就可以用一个相对合理的容量去接收 socket 中的数据，尽量减少读取 socket 的次数。\n\nprivate final class handleimpl extends maxmessagehandle {\n        @override\n        public void readcomplete() {\n                // 是否对 bytebuf 进行扩容或者缩容\n                record(totalbytesread());\n        }\n}\n\n\n那么在什么情况下需要对 bytebuf 扩容，每次扩容多少 ？ 什么情况下需要对 bytebuf 进行缩容，每次缩容多少呢 ？\n\n这就用到了一个重要的容量索引结构 —— size_table，它里边定义索引了 bytebuf 的每一种容量大小。相当于是扩缩容的容量索引表。每次扩容多少，缩容多少全部记录在这个容量索引表中。\n\npublic class adaptiverecvbytebufallocator {\n    // 扩容步长\n    private static final int index_increment = 4;\n    // 缩容步长\n    private static final int index_decrement = 1;\n\n    // bytebuf分配容量表（扩缩容索引表）按照表中记录的容量大小进行扩缩容\n    private static final int[] size_table;\n}\n\n\n当索引容量小于 512 时，size_table 中定义的容量是从 16 开始按照 16 递增。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n当索引容量大于 512 时，size_table 中定义的容量是按前一个索引容量的 2 倍递增。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n那么当前 bytebuf 的初始容量为 2048 ， 它在 size_table 中的 index 为 33 。当一轮 read loop 读取完毕之后，如果发现 totalbytesread 在size_table[index - index_decrement] 与 size_table[index] 之间的话，也就是如果本轮 read loop 结束之后总共读取的字节数在 [1024 , 2048] 之间。说明此时分配的 bytebuf 容量正好，不需要进行缩容也不需要进行扩容。比如本次 totalbytesread = 2000，正好处在 1024 与 2048 之间。说明 2048 的容量正好。\n\n如果 totalbytesread 小于等于 size_table[index - index_decrement]，也就是如果本轮 read loop 结束之后总共读取的字节数小于等于1024。表示本次读取到的字节数比当前 bytebuf 容量的下一级容量还要小，说明当前 bytebuf 的容量分配的有些大了，设置缩容标识decreasenow = true。当下次 read loop 的时候如果继续满足缩容条件，那么就开始进行缩容。缩容后的容量为 size_table[index - index_decrement]，但不能小于size_table[minindex]（16）。\n\n> 注意，这里需要满足两次缩容条件才会进行缩容，且缩容步长为 1 (index_decrement)，缩容比较谨慎。\n\n如果 totalbytesread 大于等于当前 bytebuf 容量—— nextreceivebuffersize 时，说明 bytebuf 的容量有点小了，需要进行扩容。扩容后的容量为 size_table[index + index_increment]，但不能超过 size_table[maxindex]（65535）。\n\n> 满足一次扩容条件就进行扩容，并且扩容步长为 4 (index_increment)， 扩容比较奔放。\n\n        private void record(int actualreadbytes) {\n            if (actualreadbytes <= size_table[max(0, index - index_decrement)]) {\n                // 缩容条件触发两次之后就进行缩容\n                if (decreasenow) {\n                    index = max(index - index_decrement, minindex);\n                    nextreceivebuffersize = size_table[index];\n                    decreasenow = false;\n                } else {\n                    decreasenow = true;\n                }\n            } else if (actualreadbytes >= nextreceivebuffersize) {\n                // 扩容条件满足一次之后就进行扩容\n                index = min(index + index_increment, maxindex);\n                nextreceivebuffersize = size_table[index];\n                decreasenow = false;\n            }\n        }\n\n\n\n# 2.6 bytebuf 的引用计数设计\n\nnetty 为 bytebuf 引入了引用计数的机制，在 bytebuf 的整个设计体系中，所有的 bytebuf 都会继承一个抽象类 abstractreferencecountedbytebuf ， 它是对接口 referencecounted 的实现。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\npublic interface referencecounted {\n     int refcnt();\n     referencecounted retain();\n     referencecounted retain(int increment);\n     boolean release();\n     boolean release(int decrement);\n}\n\n\n每个 bytebuf 的内部都维护了一个叫做 refcnt 的引用计数，我们可以通过 refcnt() 方法来获取 bytebuf 当前的引用计数 refcnt。当 bytebuf 在其他上下文中被引用的时候，我们需要通过 retain() 方法将 bytebuf 的引用计数加 1。另外我们也可以通过 retain(int increment) 方法来指定 refcnt 增加的大小（increment）。\n\n有对 bytebuf 的引用那么就有对 bytebuf 的释放，每当我们使用完 bytebuf 的时候就需要手动调用 release() 方法将 bytebuf 的引用计数减 1 。当引用计数 refcnt 变成 0 的时候，netty 就会通过 deallocate 方法来释放 bytebuf 所引用的内存资源。这时 release() 方法会返回 true , 如果 refcnt 还不为 0 ，那么就返回 false 。同样我们也可以通过 release(int decrement) 方法来指定 refcnt 减少多少（decrement）。\n\n# 2.6.1 为什么要引入引用计数\n\n”在其他上下文中引用 bytebuf “ 是什么意思呢 ？ 比如我们在线程 1 中创建了一个 bytebuf，然后将这个 bytebuf 丢给线程 2 进行处理，线程 2 又可能丢给线程 3， 而每个线程都有自己的上下文处理逻辑，比如对 bytebuf 的处理，释放等操作。这样就使得 bytebuf 在事实上形成了在多个线程上下文中被共享的情况。\n\n面对这种情况我们就很难在一个单独的线程上下文中判断一个 bytebuf 该不该被释放，比如线程 1 准备释放 bytebuf 了，但是它可能正在被其他线程使用。所以这也是 netty 为 bytebuf 引入引用计数的重要原因，每当引用一次 bytebuf 的时候就需要通过 retain() 方法将引用计数加 1， release() 释放的时候将引用计数减 1 ，当引用计数为 0 了，说明已经没有其他上下文引用 bytebuf 了，这时 netty 就可以释放它了。\n\n另外相比于 jdk directbytebuffer 需要依赖 gc 机制来释放其背后引用的 native memory , netty 更倾向于手动及时释放 directbytebuf 。因为 jdk directbytebuffer 的释放需要等到 gc 发生，由于 directbytebuffer 的对象实例所占的 jvm 堆内存太小了，所以一时很难触发 gc , 这就导致被引用的 native memory 的释放有了一定的延迟，严重的情况会越积越多，导致 oom 。而且也会导致进程中对 directbytebuffer 的申请操作有非常大的延迟。\n\n而 netty 为了避免这些情况的出现，选择在每次使用完毕之后手动释放 native memory ，但是不依赖 jvm 的话，总会有内存泄露的情况，比如在使用完了 bytebuf 却忘记调用 release() 方法来释放。\n\n所以为了检测内存泄露的发生，这也是 netty 为 bytebuf 引入了引用计数的另一个原因，当 bytebuf 不再被引用的时候，也就是没有任何强引用或者软引用的时候，如果此时发生 gc , 那么这个 bytebuf 实例（位于 jvm 堆中）就需要被回收了，这时 netty 就会检查这个 bytebuf 的引用计数是否为 0 ， 如果不为 0 ，说明我们忘记调用 release() 释放了，近而判断出这个 bytebuf 发生了内存泄露。\n\n在探测到内存泄露发生之后，后续 netty 就会通过 reportleak() 将内存泄露的相关信息以 error 的日志级别输出到日志中。\n\n看到这里，大家可能不禁要问，不就是引入了一个小小的引用计数嘛，这有何难 ？ 值得这里大书特书吗 ？ 不就是在创建 bytebuf 的时候将引用计数 refcnt 初始化为 1 ， 每次在其他上下文引用的时候将 refcnt 加 1， 每次释放的时候再将 refcnt 减 1 吗 ？减到 0 的时候就释放 native memory ，太简单了吧~~\n\n事实上 netty 对引用计数的设计非常讲究，绝非如此简单，甚至有些复杂，其背后隐藏着大大的性能考究以及对复杂并发问题的全面考虑，在性能与线程安全问题之间的反复权衡。\n\n# 2.6.2 引用计数的最初设计\n\n所以为了理清关于引用计数的整个设计脉络，我们需要将版本回退到最初的起点 —— 4.1.16.final 版本，来看一下原始的设计。\n\npublic abstract class abstractreferencecountedbytebuf extends abstractbytebuf {\n    // 原子更新 refcnt 的 updater\n    private static final atomicintegerfieldupdater<abstractreferencecountedbytebuf> refcntupdater =\n            atomicintegerfieldupdater.newupdater(abstractreferencecountedbytebuf.class, \"refcnt\");\n    // 引用计数，初始化为 1\n    private volatile int refcnt;\n\n    protected abstractreferencecountedbytebuf(int maxcapacity) {\n        super(maxcapacity);\n        // 引用计数初始化为 1\n        refcntupdater.set(this, 1);\n    }\n\n    // 引用计数增加 increment\n    private bytebuf retain0(int increment) {\n        for (;;) {\n            int refcnt = this.refcnt;\n            // 每次 retain 的时候对引用计数加 1\n            final int nextcnt = refcnt + increment;\n\n            // ensure we not resurrect (which means the refcnt was 0) and also that we encountered an overflow.\n            if (nextcnt <= increment) {\n                // 如果 refcnt 已经为 0 或者发生溢出，则抛异常\n                throw new illegalreferencecountexception(refcnt, increment);\n            }\n            // cas 更新 refcnt\n            if (refcntupdater.compareandset(this, refcnt, nextcnt)) {\n                break;\n            }\n        }\n        return this;\n    }\n\n    // 引用计数减少 decrement\n    private boolean release0(int decrement) {\n        for (;;) {\n            int refcnt = this.refcnt;\n            if (refcnt < decrement) {\n                // 引用的次数必须和释放的次数相等对应\n                throw new illegalreferencecountexception(refcnt, -decrement);\n            }\n            // 每次 release 引用计数减 1 \n            // cas 更新 refcnt\n            if (refcntupdater.compareandset(this, refcnt, refcnt - decrement)) {\n                if (refcnt == decrement) {\n                    // 如果引用计数为 0 ，则释放 native memory，并返回 true\n                    deallocate();\n                    return true;\n                }\n                // 引用计数不为 0 ，返回 false\n                return false;\n            }\n        }\n    }\n}\n\n\n在 4.1.16.final 之前的版本设计中，确实和我们当初想象的一样，非常简单，创建 bytebuf 的时候将 refcnt 初始化为 1。 每次引用 retain 的时候将引用计数加 1 ，每次释放 release 的时候将引用计数减 1，在一个 for 循环中通过 cas 替换。当引用计数为 0 的时候，通过 deallocate() 释放 native memory。\n\n# 2.6.3 引入指令级别上的优化\n\n4.1.16.final 的设计简洁清晰，在我们看来完全没有任何问题，但 netty 对性能的考究完全没有因此止步，由于在 x86 架构下 xadd 指令的性能要高于 cmpxchg 指令， compareandset 方法底层是通过 cmpxchg 指令实现的，而 getandadd 方法底层是 xadd 指令。\n\n所以在对性能极致的追求下，netty 在 4.1.17.final 版本中用 getandadd 方法来替换 compareandset 方法。\n\npublic abstract class abstractreferencecountedbytebuf extends abstractbytebuf {\n\n    private volatile int refcnt;\n\n    protected abstractreferencecountedbytebuf(int maxcapacity) {\n        super(maxcapacity);\n        // 引用计数在初始的时候还是为 1 \n        refcntupdater.set(this, 1);\n    }\n\n    private bytebuf retain0(final int increment) {\n        // 相比于 compareandset 的实现，这里将 for 循环去掉\n        // 并且每次是先对 refcnt 增加计数 increment\n        int oldref = refcntupdater.getandadd(this, increment);\n        // 增加完 refcnt 计数之后才去判断异常情况\n        if (oldref <= 0 || oldref + increment < oldref) {\n            // ensure we don't resurrect (which means the refcnt was 0) and also that we encountered an overflow.\n            // 如果原来的 refcnt 已经为 0 或者 refcnt 溢出，则对 refcnt 进行回退，并抛出异常\n            refcntupdater.getandadd(this, -increment);\n            throw new illegalreferencecountexception(oldref, increment);\n        }\n        return this;\n    }\n\n    private boolean release0(int decrement) {\n        // 先对 refcnt 减少计数 decrement\n        int oldref = refcntupdater.getandadd(this, -decrement);\n        // 如果 refcnt 已经为 0 则进行 native memory 的释放\n        if (oldref == decrement) {\n            deallocate();\n            return true;\n        } else if (oldref < decrement || oldref - decrement > oldref) {\n            // 如果释放次数大于 retain 次数 或者 refcnt 出现下溢\n            // 则对 refcnt 进行回退，并抛出异常\n            refcntupdater.getandadd(this, decrement);\n            throw new illegalreferencecountexception(oldref, decrement);\n        }\n        return false;\n    }\n}\n\n\n在 4.1.16.final 版本的实现中，netty 是在一个 for 循环中，先对 retain 和 release 的异常情况进行校验，之后再通过 cas 更新 refcnt。否则直接抛出 illegalreferencecountexception。采用的是一种悲观更新引用计数的策略。\n\n而在 4.1.17.final 版本的实现中 ， netty 去掉了 for 循环，正好和 compareandset 的实现相反，而是先通过 getandadd 更新 refcnt，更新之后再来判断相关的异常情况，如果发现有异常，则进行回退，并抛出 illegalreferencecountexception。采用的是一种乐观更新引用计数的策略。\n\n比如在 retain 增加引用计数的时候，先对 refcnt 增加计数 increment，然后判断原来的引用计数 oldref 是否已经为 0 或者 refcnt 是否发生溢出，如果是，则需要对 refcnt 的值进行回退，并抛异常。\n\n在 release 减少引用计数的时候，先对 refcnt 减少计数 decrement，然后判断 release 的次数是否大于 retain 的次数防止 over-release ，以及 refcnt 是否发生下溢，如果是，则对 refcnt 的值进行回退，并抛异常。\n\n# 2.6.4 并发安全问题的引入\n\n在 4.1.17.final 版本的设计中，我们对引用计数的 retain 以及 release 操作都要比 4.1.16.final 版本的性能要高，虽然现在性能是高了，但是同时引入了新的并发问题。\n\n让我们先假设一个这样的场景，现在有一个 bytebuf，它当前的 refcnt = 1 ，线程 1 对这个 bytebuf 执行 release() 操作。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n在 4.1.17.final 的实现中，netty 会首先通过 getandadd 将 refcnt 更新为 0 ，然后接着调用 deallocate() 方法释放 native memory ，很简单也很清晰是吧，让我们再加点并发复杂度上去。\n\n现在我们在上图步骤一与步骤二之间插入一个线程 2 ， 线程 2 对这个 bytebuf 并发执行 retain() 方法。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n在 4.1.17.final 的实现中，线程 2 首先通过 getandadd 将 refcnt 从 0 更新为 1，紧接着线程 2 就会发现 refcnt 原来的值 oldref 是等于 0 的，也就是说线程 2 在调用 retain() 的时候，bytebuf 的引用计数已经为 0 了，并且线程 1 已经开始准备释放 native memory 了。\n\n所以线程 2 需要再次调用 getandadd 方法将 refcnt 的值进行回退，从 1 再次回退到 0 ，最后抛出 illegalreferencecountexception。这样的结果显然是正确的，也是符合语义的。毕竟不能对一个引用计数为 0 的 bytebuf 调用 retain() 。\n\n现在看来一切风平浪静，都是按照我们的设想有条不紊的进行，我们不妨再加点并发复杂度上去。在上图步骤 1.1 与步骤 1.2 之间在插入一个线程 3 ， 线程 3 对这个 bytebuf 再次并发执行 retain() 方法。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n由于引用计数的更新（步骤 1.1）与引用计数的回退（步骤 1.2）这两个操作并不是一个原子操作，如果在这两个操作之间不巧插入了一个线程 3 ，线程 3 在并发执行 retain() 方法的时候，首先会通过 getandadd 将引用计数 refcnt 从 1 增加到 2 。\n\n> 注意，此时线程 2 还没来得及回退 refcnt ， 所以线程 3 此时看到的 refcnt 是 1 而不是 0 。\n\n由于此时线程 3 看到的 oldref 是 1 ，所以线程 3 成功调用 retain() 方法将 bytebuf 的引用计数增加到了 2 ，并且不会回退也不会抛出异常。在线程 3 看来此时的 bytebuf 完完全全是一个正常可以被使用的 bytebuf。\n\n紧接着线程 1 开始执行步骤 2 —— deallocate() 方法释放 native memory，此后线程 3 在访问这个 bytebuf 的时候就有问题了，因为 native memory 已经被线程1 释放了。\n\n# 2.6.5 在性能与并发安全之间的权衡\n\n接下来 netty 就需要在性能与并发安全之间进行权衡了，现在有两个选择，第一个选择是直接回滚到 4.1.16.final 版本，放弃 xadd 指令带来的性能提升，之前的设计中采用的 cmpxchg 指令虽然性能相对差一些，但是不会出现上述的并发安全问题。\n\n因为 netty 是在一个 for 循环中采用悲观的策略来更新引用计数，先是判断异常情况，然后在通过 cas 来更新 refcnt。即使多个线程看到了 refcnt 的中间状态也没关系，因为接下来进行的 cas 也会跟着失败。\n\n比如上边例子中的线程 1 对 bytebuf 进行 release 的时候，在线程 1 执行 cas 将 refcnt 替换为 0 之前的这个间隙中，refcnt 是 1 ，如果在这个间隙中，线程 2 并发执行 retain 方法，此时线程 2 看到的 refcnt 确实为 1 ，它是一个中间状态，线程 2 执行 cas 将 refcnt 替换为 2。\n\n此时线程 1 执行 cas 就会失败，但会在下一轮 for 循环中将 refcnt 替换为 1，这是完全符合引用计数语义的。\n\n另外一种情况是线程 1 已经执行完 cas 将 refcnt 替换为 0 ，这时候线程 2 去 retain ，由于 4.1.16.final 版本中的设计是先检查异常后 cas 替换，所以线程 2 首先会在 retain 方法中检查到 bytebuf 的 refcnt 已经为 0 ，直接抛出 illegalreferencecountexception，并不会执行 cas 。这同样符合引用计数的语义，毕竟不能对一个引用计数已经为 0 的 bytebuf 执行任何访问操作。\n\n第二个选择是既要保留 xadd 指令带来的性能提升，也要解决 4.1.17.final 版本中引入的并发安全问题。毫无疑问，netty 最终选择的是这种方案。\n\n在介绍 netty 的精彩设计之前，我想我们还是应该在回顾下这个并发安全问题出现的根本原因是什么 ？\n\n在 4.1.17.final 版本的设计中，netty 首先是通过 getandadd 方法先对 refcnt 的值进行更新，如果出现异常情况，在进行回滚。而更新，回滚的这两个操作并不是原子的，之间的中间状态会被其他线程看到。\n\n比如，线程 2 看到了线程 1 的中间状态（refcnt = 0），于是将引用计数加到 1 , 在线程 2 进行回滚之前，这期间的中间状态（refcnt = 1，oldref = 0）又被线程 3 看到了，于是线程 3 将引用计数增加到了 2 （refcnt = 2，oldref = 1）。 此时线程 3 觉得这是一种正常的状态，但在线程 1 看来 refcnt 的值已经是 0 了，后续线程 1 就会释放 native memory ，这就出问题了。\n\n问题的根本原因其实是这里的 refcnt 不同的值均代表不同的语义，比如对于线程 1 来说，通过 release 将 refcnt 减到了 0 ，这里的语义是 bytebuf 已经不在被引用了，可以释放 native memory 。\n\n随后线程 2 通过 retain 将 refcnt 加到了 1 ，这就把 bytebuf 语义改变了，表示该 bytebuf 在线程 2 中被引用了一次。最后线程 3 又通过 retain 将 refcnt 加到了 2 ，再一次改变了 bytebuf 的语义。\n\n只要用到 xadd 指令来实现引用计数的更新，那么就不可避免的出现上述并发更新 refcnt 的情况，关键是 refcnt 的值每一次被其他线程并发修改之后，bytebuf 的语义就变了。这才是 4.1.17.final 版本中的关键问题所在。\n\n如果 netty 想在同时享受 xadd 指令带来的性能提升之外，又要解决上述提到的并发安全问题，就要重新对引用计数进行设计。首先我们的要求是继续采用 xadd 指令来实现引用计数的更新，但这就会带来多线程并发修改所引起的 bytebuf 语义改变。\n\n既然多线程并发修改无法避免，那么我们能不能重新设计一下引用计数，让 bytebuf 语义无论多线程怎么修改，它的语义始终保持不变。也就是说只要线程 1 将 refcnt 减到了 0 ，那么无论线程 2 和线程 3 怎么并发修改 refcnt，怎么增加 refcnt 的值，refcnt 等于 0 的这个语义始终保持不变呢 ？\n\n# 2.6.6 奇偶设计的引入\n\n这里 netty 有一个极奇巧妙精彩的设计，引用计数的设计不再是逻辑意义上的 0 , 1 , 2 , 3 .....，而是分为了两大类，要么是偶数，要么是奇数。\n\n * 偶数代表的语义是 bytebuf 的 refcnt 不为 0 ，也就是说只要一个 bytebuf 还在被引用，那么它的 refcnt 就是一个偶数，具体被引用多少次，可以通过 refcnt >>> 1 来获取。\n * 奇数代表的语义是 bytebuf 的 refcnt 等于 0 ，只要一个 bytebuf 已经没有任何地方引用它了，那么它的 refcnt 就是一个奇数，其背后引用的 native memory 随后就会被释放。\n\nbytebuf 在初始化的时候，refcnt 不在是 1 而是被初始化为 2 （偶数），每次 retain 的时候不在是对 refcnt 加 1 而是加 2 （偶数步长），每次 release 的时候不再是对 refcnt 减 1 而是减 2 （同样是偶数步长）。这样一来，只要一个 bytebuf 的引用计数为偶数，那么多线程无论怎么并发调用 retain 方法，引用计数还是一个偶数，语义仍然保持不变。\n\n   public final int initialvalue() {\n        return 2;\n    }\n\n\n当一个 bytebuf 被 release 到没有任何引用计数的时候，netty 不在将 refcnt 设置为 0 而是设置为 1 （奇数），对于一个值为奇数的 refcnt，无论多线程怎么并发调用 retain 方法和 release 方法，引用计数还是一个奇数，bytebuf 引用计数为 0 的这层语义一直会保持不变。\n\n我们还是以上图中所展示的并发安全问题为例，在新的引用计数设计方案中，首先线程 1 对 bytebuf 执行 release 方法，netty 会将 refcnt 设置为 1 （奇数）。\n\n线程 2 并发调用 retain 方法，通过 getandadd 将 refcnt 从 1 加到了 3 ，refcnt 仍然是一个奇数，按照奇数所表示的语义 —— bytebuf 引用计数已经是 0 了，那么线程 2 就会在 retain 方法中抛出 illegalreferencecountexception。\n\n线程 3 并发调用 retain 方法，通过 getandadd 将 refcnt 从 3 加到了 5，看到了没 ，在新方案的设计中，无论多线程怎么并发执行 retain 方法，refcnt 的值一直都只会是一个奇数，随后线程 3 在 retain 方法中抛出 illegalreferencecountexception。这完全符合引用计数的并发语义。\n\n这个新的引用计数设计方案是在 4.1.32.final 版本引入进来的，仅仅通过一个奇偶设计，就非常巧妙的解决了 4.1.17.final 版本中存在的并发安全问题。现在新方案的核心设计要素我们已经清楚了，那么接下来笔者将以 4.1.56.final 版本来为大家继续介绍下新方案的实现细节。\n\nnetty 中的 bytebuf 全部继承于 abstractreferencecountedbytebuf，在这个类中实现了所有对 bytebuf 引用计数的操作，对于 referencecounted 接口的实现就在这里。\n\npublic abstract class abstractreferencecountedbytebuf extends abstractbytebuf {\n    // 获取 refcnt 字段在 bytebuf 对象内存中的偏移\n    // 后续通过 unsafe 对 refcnt 进行操作\n    private static final long refcnt_field_offset =\n            referencecountupdater.getunsafeoffset(abstractreferencecountedbytebuf.class, \"refcnt\");\n\n    // 获取 refcnt 字段 的 atomicfieldupdater\n    // 后续通过 atomicfieldupdater 来操作 refcnt 字段\n    private static final atomicintegerfieldupdater<abstractreferencecountedbytebuf> aif_updater =\n            atomicintegerfieldupdater.newupdater(abstractreferencecountedbytebuf.class, \"refcnt\");\n\n    // 创建 referencecountupdater，对于引用计数的所有操作最终都会代理到这个类中\n    private static final referencecountupdater<abstractreferencecountedbytebuf> updater =\n            new referencecountupdater<abstractreferencecountedbytebuf>() {\n        @override\n        protected atomicintegerfieldupdater<abstractreferencecountedbytebuf> updater() {\n            // 通过 atomicintegerfieldupdater 操作 refcnt 字段\n            return aif_updater;\n        }\n        @override\n        protected long unsafeoffset() {\n            // 通过 unsafe 操作 refcnt 字段\n            return refcnt_field_offset;\n        }\n    };\n    // bytebuf 中的引用计数，初始为 2 （偶数）\n    private volatile int refcnt = updater.initialvalue();\n}\n\n\n其中定义了一个 refcnt 字段用于记录 bytebuf 被引用的次数，由于采用了奇偶设计，在创建 bytebuf 的时候，netty 会将 refcnt 初始化为 2 （偶数），它的逻辑语义是该 bytebuf 被引用一次。后续对 bytebuf 执行 retain 就会对 refcnt 进行加 2 ，执行 release 就会对 refcnt 进行减 2 ，对于引用计数的单次操作都是以 2 为步长进行。\n\n由于在 netty 中除了 abstractreferencecountedbytebuf 这个专门用于实现 bytebuf 的引用计数功能之外，还有一个更加通用的引用计数抽象类 abstractreferencecounted，它用于实现所有系统资源类的引用计数功能（bytebuf 只是其中的一种内存资源）。\n\n由于都是对引用计数的实现，所以在之前的版本中，这两个类中包含了很多重复的引用计数相关操作逻辑，所以 netty 在 4.1.35.final 版本中专门引入了一个 referencecountupdater 类，将所有引用计数的相关实现聚合在这里。\n\nreferencecountupdater 对于引用计数 refcnt 的操作有两种方式，一种是通过 atomicfieldupdater 来对 refcnt 进行操作，我们可以通过 updater() 获取到 refcnt 字段对应的 atomicfieldupdater。\n\n另一种则是通过 unsafe 来对 refcnt 进行操作，我们可以通过 unsafeoffset() 来获取到 refcnt 字段在 bytebuf 实例对象内存中的偏移。\n\n按理来说，我们采用一种方式就可以对 refcnt 进行访问或者更新了，那为什么 netty 提供了两种方式呢 ？会显得有点多余吗 ？这个点大家可以先思考下为什么 ，后续在我们剖析到源码细节的时候笔者在为大家解答。\n\n好了，下面我们正式开始介绍新版引用计数设计方案的具体实现细节，第一个问题，在新的设计方案中，我们如何获取 bytebuf 的逻辑引用计数 ？\n\npublic abstract class referencecountupdater<t extends referencecounted> {\n    public final int initialvalue() {\n        // bytebuf 引用计数初始化为 2\n        return 2;\n    }\n\n    public final int refcnt(t instance) {\n        // 通过 updater 获取 refcnt\n        // 根据 refcnt 在  realrefcnt 中获取真实的引用计数\n        return realrefcnt(updater().get(instance));\n    }\n    // 获取 bytebuf 的逻辑引用计数\n    private static int realrefcnt(int rawcnt) {\n        // 奇偶判断\n        return rawcnt != 2 && rawcnt != 4 && (rawcnt & 1) != 0 ? 0 : rawcnt >>> 1;\n    }\n}\n\n\n由于采用了奇偶引用计数的设计，所以我们在获取逻辑引用计数的时候需要判断当前 rawcnt（refcnt）是奇数还是偶数，它们分别代表了不同的语义。\n\n * 如果 rawcnt 是奇数，则表示当前 bytebuf 已经没有任何地方引用了，逻辑引用计数返回 0.\n * 如果 rawcnt 是偶数，则表示当前 bytebuf 还有地方在引用，逻辑引用计数则为 rawcnt >>> 1。\n\nrealrefcnt 函数其实就是简单的一个奇偶判断逻辑，但在它的实现中却体现出了 netty 对性能的极致追求。比如，我们判断一个数是奇数还是偶数其实很简单，直接通过 rawcnt & 1 就可以判断，如果返回 0 表示 rawcnt 是一个偶数，如果返回 1 表示 rawcnt 是一个奇数。\n\n但是我们看到 netty 在奇偶判断条件的前面又加上了 rawcnt != 2 && rawcnt != 4语句，这是干嘛的呢 ？\n\n其实 netty 这里是为了尽量用性能更高的 == 运算来代替 & 运算，但又不可能用 == 运算来枚举出所有的偶数值（也没这必要），所以只用 == 运算来判断在实际场景中经常出现的引用计数，一般经常出现的引用计数值为 2 或者 4 ， 也就是说 bytebuf 在大部分场景下只会被引用 1 次或者 2 次，对于这种高频出现的场景，netty 用 == 运算来针对性优化，低频出现的场景就回退到 & 运算。\n\n> 大部分性能优化的套路都是相同的，我们通常不能一上来就奢求一个大而全的针对全局的优化方案，这是不可能的，也是十分低效的。往往最有效的，可以立竿见影的优化方案都是针对局部热点进行专门优化。\n\n对引用计数的设置也是一样，都需要考虑奇偶的转换，我们在 setrefcnt 方法中指定的参数 refcnt 表示逻辑上的引用计数 —— 0, 1 , 2 , 3 ....，但要设置到 bytebuf 时，就需要对逻辑引用计数在乘以 2 ，让它始终是一个偶数。\n\n    public final void setrefcnt(t instance, int refcnt) {\n        updater().set(instance, refcnt > 0 ? refcnt << 1 : 1); // overflow ok here\n    }\n\n\n有了这些基础之后，我们下面就来看一下在新版本的 retain 方法设计中，netty 是如何解决 4.1.17.final 版本存在的并发安全问题。首先 netty 对引用计数的奇偶设计对于用户来说是透明的。引用计数对于用户来说仍然是普通的自然数 —— 0, 1 , 2 , 3 .... 。\n\n所以每当用户调用 retain 方法试图增加 bytebuf 的引用计数时，通常是指定逻辑增加步长 —— increment（用户视角），而在具体的实现角度，netty 会增加两倍的 increment （rawincrement）到 refcnt 字段中。\n\n    public final t retain(t instance) {\n        // 引用计数逻辑上是加 1 ，但实际上是加 2 （实现角度）\n        return retain0(instance, 1, 2);\n    }\n\n    public final t retain(t instance, int increment) {\n        // all changes to the raw count are 2x the \"real\" change - overflow is ok\n        // rawincrement 始终是逻辑计数 increment 的两倍\n        int rawincrement = checkpositive(increment, \"increment\") << 1;\n        // 将 rawincrement 设置到 bytebuf 的 refcnt 字段中\n        return retain0(instance, increment, rawincrement);\n    }\n\n    // rawincrement = increment << 1\n    // increment 表示引用计数的逻辑增长步长\n    // rawincrement 表示引用计数的实际增长步长\n    private t retain0(t instance, final int increment, final int rawincrement) {\n        // 先通过 xadd 指令将  refcnt 的值加起来\n        int oldref = updater().getandadd(instance, rawincrement);\n        // 如果 oldref 是一个奇数，也就是 bytebuf 已经没有引用了，抛出异常\n        if (oldref != 2 && oldref != 4 && (oldref & 1) != 0) {\n            // 如果 oldref 已经是一个奇数了，无论多线程在这里怎么并发 retain ，都是一个奇数，这里都会抛出异常\n            throw new illegalreferencecountexception(0, increment);\n        }\n        // don't pass 0! \n        // refcnt 不可能为 0 ，只能是 1\n        if ((oldref <= 0 && oldref + rawincrement >= 0)\n                || (oldref >= 0 && oldref + rawincrement < oldref)) {\n            // 如果 refcnt 字段已经溢出，则进行回退，并抛异常\n            updater().getandadd(instance, -rawincrement);\n            throw new illegalreferencecountexception(realrefcnt(oldref), increment);\n        }\n        return instance;\n    }\n\n\n首先新版本的 retain0 方法仍然保留了 4.1.17.final 版本引入的 xadd 指令带来的性能优势，大致的处理逻辑也是类似的，一上来先通过 getandadd 方法将 refcnt 增加 rawincrement，对于 retain(t instance) 来说这里直接加 2 。\n\n然后判断原来的引用计数 oldref 是否是一个奇数，如果是一个奇数，那么就表示 bytebuf 已经没有任何引用了，逻辑引用计数早已经为 0 了，那么就抛出 illegalreferencecountexception。\n\n在引用计数为奇数的情况下，无论多线程怎么对 refcnt 并发加 2 ，refcnt 始终是一个奇数，最终都会抛出异常。解决并发安全问题的要点就在这里，一定要保证 retain 方法的并发执行不能改变原来的语义。\n\n最后会判断一下 refcnt 字段是否发生溢出，如果溢出，则进行回退，并抛出异常。下面我们仍然以之前的并发场景为例，用一个具体的例子，来回味一下奇偶设计的精妙之处。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n现在线程 1 对一个 refcnt 为 2 的 bytebuf 执行 release 方法，这时 bytebuf 的逻辑引用计数就为 0 了，对于一个没有任何引用的 bytebuf 来说，新版的设计中它的 refcnt 只能是一个奇数，不能为 0 ，所以这里 netty 会将 refcnt 设置为 1 。然后在步骤 2 中调用 deallocate 方法释放 native memory。\n\n线程 2 在步骤 1 和步骤 2 之间插入进来对 bytebuf 并发执行 retain 方法，这时线程 2 看到的 refcnt 是 1，然后通过 getandadd 将 refcnt 加到了 3 ，仍然是一个奇数，随后抛出 illegalreferencecountexception 异常。\n\n线程 3 在步骤 1.1 和步骤 1.2 之间插入进来再次对 bytebuf 并发执行 retain 方法，这时线程 3 看到的 refcnt 是 3，然后通过 getandadd 将 refcnt 加到了 5 ，还是一个奇数，随后抛出 illegalreferencecountexception 异常。\n\n这样一来就保证了引用计数的并发语义 —— 只要一个 bytebuf 没有任何引用的时候（refcnt = 1），其他线程无论怎么并发执行 retain 方法都会得到一个异常。\n\n但是引用计数并发语义的保证不能单单只靠 retain 方法，它还需要与 release 方法相互配合协作才可以，所以为了并发语义的保证 ， release 方法的设计就不能使用性能更高的 xadd 指令，而是要回退到 cmpxchg 指令来实现。\n\n为什么这么说呢 ？因为新版引用计数的设计采用的是奇偶实现，refcnt 为偶数表示 bytebuf 还有引用，refcnt 为奇数表示 bytebuf 已经没有任何引用了，可以安全释放 native memory 。对于一个 refcnt 已经为奇数的 bytebuf 来说，无论多线程怎么并发执行 retain 方法，得到的 refcnt 仍然是一个奇数，最终都会抛出 illegalreferencecountexception，这就是引用计数的并发语义 。\n\n为了保证这一点，就需要在每次调用 retain ，release 方法的时候，以偶数步长来更新 refcnt，比如每一次调用 retain 方法就对 refcnt 加 2 ，每一次调用 release 方法就对 refcnt 减 2 。\n\n但总有一个时刻，refcnt 会被减到 0 的对吧，在新版的奇偶设计中，refcnt 是不允许为 0 的，因为一旦 refcnt 被减到了 0 ，多线程并发执行 retain 之后，就会将 refcnt 再次加成了偶数，这又会出现并发问题。\n\n而每一次调用 release 方法是对 refcnt 减 2 ，如果我们采用 xadd 指令实现 release 的话，回想一下 4.1.17.final 版本中的设计，它首先进来是通过 getandadd 方法对 refcnt 减 2 ，这样一来，refcnt 就变成 0 了，就有并发安全问题了。所以我们需要通过 cmpxchg 指令将 refcnt 更新为 1。\n\n这里有的同学可能要问了，那可不可以先进行一下 if 判断，如果 refcnt 减 2 之后变为 0 了，我们在通过 getandadd 方法将 refcnt 更新为 1 （减一个奇数），这样一来不也可以利用上 xadd 指令的性能优势吗 ？\n\n答案是不行的，因为 if 判断与 getandadd 更新这两个操作之间仍然不是原子的，多线程可以在这个间隙仍然有并发执行 retain 方法的可能，如下图所示：\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n在线程 1 执行 if 判断和 getandadd 更新这两个操作之间，线程 2 看到的 refcnt 其实 2 ，然后线程 2 会将 refcnt 加到 4 ，线程 3 紧接着会将 refcnt 增加到 6 ，在线程 2 和线程 3 看来这个 bytebuf 完全是正常的，但是线程 1 马上就会释放 native memory 了。\n\n而且采用这种设计的话，一会通过 getandadd 对 refcnt 减一个奇数，一会通过 getandadd 对 refcnt 加一个偶数，这样就把原本的奇偶设计搞乱掉了。\n\n所以我们的设计目标是一定要保证在 bytebuf 没有任何引用计数的时候，release 方法需要原子性的将 refcnt 更新为 1 。 因此必须采用 cmpxchg 指令来实现而不能使用 xadd 指令。\n\n再者说， cmpxchg 指令是可以原子性的判断当前是否有并发情况的，如果有并发情况出现，cas 就会失败，我们可以继续重试。但 xadd 指令却无法原子性的判断是否有并发情况，因为它每次都是先更新，后判断并发，这就不是原子的了。这一点，在下面的源码实现中会体现的特别明显。\n\n# 2.6.7 尽量避免内存屏障的开销\n\n    public final boolean release(t instance) {\n        // 第一次尝试采用 unsafe nonvolatile 的方式读取 refcnf 的值\n        int rawcnt = nonvolatilerawcnt(instance);\n        // 如果逻辑引用计数被减到 0 了，那么就通过 tryfinalrelease0 使用 cas 将 refcnf 更新为 1\n        // cas 失败的话，则通过 retryrelease0 进行重试\n        // 如果逻辑引用计数不为 0 ，则通过 nonfinalrelease0 将 refcnf 减 2\n        return rawcnt == 2 ? tryfinalrelease0(instance, 2) || retryrelease0(instance, 1)\n                : nonfinalrelease0(instance, 1, rawcnt, toliverealrefcnt(rawcnt, 1));\n    }\n\n\n这里有一个小的细节再次体现出 netty 对于性能的极致追求，refcnt 字段在 bytebuf 中被 netty 申明为一个 volatile 字段。\n\nprivate volatile int refcnt = updater.initialvalue();\n\n\n我们对 refcnt 的普通读写都是要走内存屏障的，但 netty 在 release 方法中首次读取 refcnt 的值是采用 nonvolatile 的方式，不走内存屏障，直接读取 cache line，避免了屏障开销。\n\n    private int nonvolatilerawcnt(t instance) {\n        // 获取 refcnt_field_offset\n        final long offset = unsafeoffset();\n        // 通过 unsafe 的方式来访问 refcnt ， 避免内存屏障的开销\n        return offset != -1 ? platformdependent.getint(instance, offset) : updater().get(instance);\n    }\n\n\n那有的同学可能要问了，如果读取 refcnt 的时候不走内存屏障的话，读取到的 refcnt 不就可能是一个错误的值吗 ？\n\n事实上确实是这样的，但 netty 不 care , 读到一个错误的值也无所谓，因为这里的引用计数采用了奇偶设计，我们在第一次读取引用计数的时候并不需要读取到一个精确的值，既然这样我们可以直接通过 unsafe 来读取，还能剩下一笔内存屏障的开销。\n\n那为什么不需要一个精确的值呢 ？因为如果原来的 refcnt 是一个奇数，那无论多线程怎么并发 retain ，最终得到的还是一个奇数，我们这里只需要知道 refcnt 是一个奇数就可以直接抛 illegalreferencecountexception 了。具体读到的是一个 3 还是一个 5 其实都无所谓。\n\n那如果原来的 refcnt 是一个偶数呢 ？其实也无所谓，我们可能读到一个正确的值也可能读到一个错误的值，如果恰好读到一个正确的值，那更好。如果读取到一个错误的值，也无所谓，因为我们后面是用 cas 进行更新，这样的话 cas 就会更新失败，我们只需要在一下轮 for 循环中更新正确就可以了。\n\n如果读取到的 refcnt 恰好是 2 ，那就意味着本次 release 之后，bytebuf 的逻辑引用计数就为 0 了，netty 会通过 cas 将 refcnt 更新为 1 。\n\n   private boolean tryfinalrelease0(t instance, int expectrawcnt) {\n        return updater().compareandset(instance, expectrawcnt, 1); // any odd number will work\n    }\n\n\n如果 cas 更新失败，则表示此时有多线程可能并发对 bytebuf 执行 retain 方法，逻辑引用计数此时可能就不为 0 了，针对这种并发情况，netty 会在 retryrelease0 方法中进行重试，将 refcnt 减 2 。\n\n    private boolean retryrelease0(t instance, int decrement) {\n        for (;;) {\n            // 采用 volatile 的方式读取 refcnt\n            int rawcnt = updater().get(instance), \n            // 获取逻辑引用计数，如果 refcnt 已经变为奇数，则抛出异常\n            realcnt = toliverealrefcnt(rawcnt, decrement);\n            // 如果执行完本次 release , 逻辑引用计数为 0\n            if (decrement == realcnt) {\n                // cas 将 refcnt 更新为 1\n                if (tryfinalrelease0(instance, rawcnt)) {\n                    return true;\n                }\n            } else if (decrement < realcnt) {\n                // 原来的逻辑引用计数 realcnt 大于 1（decrement）\n                // 则通过 cas 将 refcnt 减 2\n                if (updater().compareandset(instance, rawcnt, rawcnt - (decrement << 1))) {\n                    return false;\n                }\n            } else {\n                // refcnt 字段如果发生溢出，则抛出异常\n                throw new illegalreferencecountexception(realcnt, -decrement);\n            }\n            // cas 失败之后调用 yield\n            // 减少无畏的竞争，否则所有线程在高并发情况下都在这里 cas 失败\n            thread.yield(); \n        }\n    }\n\n\n从 retryrelease0 方法的实现中我们可以看出，cas 是可以原子性的探测到是否有并发情况出现的，如果有并发情况，这里的所有 cas 都会失败，随后会在下一轮 for 循环中将正确的值更新到 refcnt 中。这一点 ，xadd 指令是做不到的。\n\n如果在进入 release 方法后，第一次读取的 refcnt 不是 2 ，那么就不能走上面的 tryfinalrelease0 逻辑，而是在 nonfinalrelease0 中通过 cas 将 refcnt 的值减 2 。\n\n   private boolean nonfinalrelease0(t instance, int decrement, int rawcnt, int realcnt) {\n        if (decrement < realcnt\n                && updater().compareandset(instance, rawcnt, rawcnt - (decrement << 1))) {\n            // bytebuf 的 rawcnt 减少 2 * decrement\n            return false;\n        }\n        // cas  失败则一直重试，如果引用计数已经为 0 ，那么抛出异常，不能再次 release\n        return retryrelease0(instance, decrement);\n    }\n\n\n到这里，netty 对引用计数的精彩设计，笔者就为大家完整的剖析完了，一共有四处非常精彩的优化设计，我们总结如下：\n\n 1. 使用性能更优的 xadd 指令来替换 cmpxchg 指令。\n 2. 引用计数采用了奇偶设计，保证了并发语义。\n 3. 采用性能更优的 == 运算来替换 & 运算。\n 4. 能不走内存屏障就尽量不走内存屏障。\n\n\n# 2.7 bytebuf 的视图设计\n\n和 jdk 的设计一样，netty 中的 bytebuf 也可以通过 slice() 方法以及 duplicate() 方法创建一个视图 bytebuf 出来，原生 bytebuf 和它的视图 bytebuf 底层都是共用同一片内存区域，也就是说在视图 bytebuf 上做的任何改动都会反应到原生 bytebuf 上。同理，在原生 bytebuf 上做的任何改动也会反应到它的视图 bytebuf 上。我们可以将视图 bytebuf 看做是原生 bytebuf 的一份浅拷贝。\n\n原生 bytebuf 和它的视图 bytebuf 不同的是，它们都有各自独立的 readerindex，writerindex，capacity，maxcapacity。\n\nslice() 方法是在原生 bytebuf 的 [readerindex , writerindex) 这段内存区域内创建一个视图 bytebuf。也就是原生 bytebuf 和视图 bytebuf 共用 [readerindex , writerindex) 这段内存区域。视图 bytebuf 的数据区域其实就是原生 bytebuf 的可读字节区域。\n\n视图 bytebuf 的 readerindex = 0 ， writerindex = capacity = maxcapacity = 原生 bytebuf 的 readablebytes() 。\n\n  @override\n    public int readablebytes() {\n        // 原生 bytebuf\n        return writerindex - readerindex;\n    }\n\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n下面我们来看一下 slice()方法创建视图 bytebuf 的逻辑实现：\n\npublic abstract class abstractbytebuf extends bytebuf {\n    @override\n    public bytebuf slice() {\n        return slice(readerindex, readablebytes());\n    }\n\n    @override\n    public bytebuf slice(int index, int length) {\n        // 确保 bytebuf 的引用计数不为 0 \n        ensureaccessible();\n        return new unpooledslicedbytebuf(this, index, length);\n    }\n}\n\n\nnetty 会将 slice 视图 bytebuf 封装在 unpooledslicedbytebuf 类中，在这里会初始化 slice 视图 bytebuf 的 readerindex，writerindex，capacity，maxcapacity。\n\nclass unpooledslicedbytebuf extends abstractunpooledslicedbytebuf {\n    unpooledslicedbytebuf(abstractbytebuf buffer, int index, int length) {\n        // index = readerindex\n        // length = readablebytes()\n        super(buffer, index, length);\n    }\n\n    @override\n    public int capacity() {\n        // 视图 bytebuf 的 capacity 和 maxcapacity 相等\n        // 均为原生 bytebuf 的 readablebytes() \n        return maxcapacity();\n    }\n}\n\n\n如上图所示，这里的 index 就是原生 bytebuf 的 readerindex = 4 ，index 用于表示视图 bytebuf 的内存区域相对于原生 bytebuf 的偏移，因为视图 bytebuf 与原生 bytebuf 共用的是同一片内存区域，针对视图 bytebuf 的操作其实底层最终是转换为对原生 bytebuf 的操作。\n\n但由于视图 bytebuf 和原生 bytebuf 各自都有独立的 readerindex 和 writerindex，比如上图中，视图 bytebuf 中的 readerindex = 0 其实指向的是原生 bytebuf 中 readerindex = 4 的位置。所以每次在我们对视图 bytebuf 进行读写的时候都需要将视图 bytebuf 的 readerindex 加上一个偏移（index）转换成原生 bytebuf 的 readerindex，近而从原生 bytebuf 中来读写数据。\n\n   @override\n    protected byte _getbyte(int index) {\n        // 底层其实是对原生 bytebuf 的访问\n        return unwrap()._getbyte(idx(index));\n    }\n\n    @override\n    protected void _setbyte(int index, int value) {\n        unwrap()._setbyte(idx(index), value);\n    }\n\n   /**\n     * returns the index with the needed adjustment.\n     */\n    final int idx(int index) {\n        // 转换为原生 bytebuf 的 readerindex 或者 writerindex\n        return index + adjustment;\n    }\n\n\nidx(int index) 方法中的 adjustment 就是上面 unpooledslicedbytebuf 构造函数中的 index 偏移，初始化为原生 bytebuf 的 readerindex。\n\nlength 则初始化为原生 bytebuf 的 readablebytes()，视图 bytebuf 中的 writerindex，capacity，maxcapacity 都是用 length 来初始化。\n\nabstract class abstractunpooledslicedbytebuf extends abstractderivedbytebuf {\n    // 原生 bytebuf\n    private final bytebuf buffer;\n    // 视图 bytebuf 相对于原生 bytebuf的数据区域偏移\n    private final int adjustment;\n\n    abstractunpooledslicedbytebuf(bytebuf buffer, int index, int length) {\n        // 设置视图 bytebuf 的 maxcapacity，readerindex 为 0 \n        super(length);\n        // 原生 bytebuf\n        this.buffer = buffer;\n        // 数据偏移为原生 bytebuf 的 readerindex\n        adjustment = index;\n        // 设置视图 bytebuf 的 writerindex\n        writerindex(length);\n    }\n}\n\n\n但是通过 slice() 方法创建出来的视图 bytebuf 并不会改变原生 bytebuf 的引用计数，这会存在一个问题，就是由于视图 bytebuf 和原生 bytebuf 底层共用的是同一片内存区域，在原生 bytebuf 或者视图 bytebuf 各自的应用上下文中他们可能并不会意识到对方的存在。\n\n如果对原生 bytebuf 调用 release 方法，恰好引用计数就为 0 了，接着就会释放原生 bytebuf 的 native memory 。此时再对视图 bytebuf 进行访问就有问题了，因为 native memory 已经被原生 bytebuf 释放了。同样的道理，对视图 bytebuf 调用 release 方法 ，也会对原生 bytebuf 产生影响。\n\n为此 netty 提供了一个 retainedslice() 方法，在创建 slice 视图 bytebuf 的同时对原生 bytebuf 的引用计数加 1 ，两者共用同一个引用计数。\n\n    @override\n    public bytebuf retainedslice() {\n        // 原生 bytebuf 的引用计数加 1\n        return slice().retain();\n    }\n\n\n除了 slice() 之外，netty 也提供了 duplicate() 方法来创建视图 bytebuf 。\n\n    @override\n    public bytebuf duplicate() {\n        // 确保 bytebuf 的引用计数不为 0 \n        ensureaccessible();\n        return new unpooledduplicatedbytebuf(this);\n    }\n\n\n但和 slice() 不同的是， duplicate() 是完全复刻了原生 bytebuf，复刻出来的视图 bytebuf 虽然与原生 bytebuf 都有各自独立的 readerindex，writerindex，capacity，maxcapacity。但他们的值都是相同的。duplicate 视图 bytebuf 也是和原生 bytebuf 共用同一块 native memory 。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\npublic class duplicatedbytebuf extends abstractderivedbytebuf {\n    // 原生 bytebuf\n    private final bytebuf buffer;\n\n    public duplicatedbytebuf(bytebuf buffer) {\n        this(buffer, buffer.readerindex(), buffer.writerindex());\n    }\n\n    duplicatedbytebuf(bytebuf buffer, int readerindex, int writerindex) {\n        // 初始化视图 bytebuf 的 maxcapacity 与原生的相同\n        super(buffer.maxcapacity());\n        // 原生 bytebuf\n        this.buffer = buffer;\n        // 视图 bytebuf 的 readerindex ， writerindex 也与原生相同\n        setindex(readerindex, writerindex);\n        markreaderindex();\n        markwriterindex();\n    }\n\n    @override\n    public int capacity() {\n        // 视图 bytebuf 的 capacity 也与原生相同\n        return unwrap().capacity();\n    }\n\n}\n\n\nnetty 同样也提供了对应的 retainedduplicate() 方法，用于创建 duplicate 视图 bytebuf 的同时增加原生 bytebuf 的引用计数。视图 bytebuf 与原生 bytebuf 之间共用同一个引用计数。\n\n   @override\n    public bytebuf retainedduplicate() {\n        return duplicate().retain();\n    }\n\n\n上面介绍的两种视图 bytebuf 可以理解为是对原生 bytebuf 的一层浅拷贝，netty 也提供了 copy() 方法来实现对原生 bytebuf 的深拷贝，copy 出来的 bytebuf 是原生 bytebuf 的一个副本，两者底层依赖的 native memory 是不同的，各自都有独立的 readerindex，writerindex，capacity，maxcapacity 。\n\npublic abstract class abstractbytebuf extends bytebuf {\n    @override\n    public bytebuf copy() {\n        // 从原生 bytebuf 中的 readerindex 开始，拷贝 readablebytes 个字节到新的 bytebuf 中\n        return copy(readerindex, readablebytes());\n    }\n}\n\n\ncopy() 方法是对原生 bytebuf 的 [readerindex , writerindex)这段数据范围内容进行拷贝。copy 出来的 bytebuf，它的 readerindex = 0 ， writerindex = capacity = 原生 bytebuf 的 readablebytes()。maxcapacity 与原生 maxcapacity 相同。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\npublic class unpooleddirectbytebuf  {\n  @override\n    public bytebuf copy(int index, int length) {\n        ensureaccessible();\n        bytebuffer src;\n        try {\n            // 将原生 bytebuf 中 [index , index + lengh) 这段范围的数据拷贝到新的 bytebuf 中\n            src = (bytebuffer) buffer.duplicate().clear().position(index).limit(index + length);\n        } catch (illegalargumentexception ignored) {\n            throw new indexoutofboundsexception(\"too many bytes to read - need \" + (index + length));\n        }\n        // 首先新申请一段 native memory , 新的 bytebuf 初始容量为 length (真实容量)，最大容量与原生 bytebuf 的 maxcapacity 相等\n        // readerindex = 0 , writerindex = length\n        return alloc().directbuffer(length, maxcapacity()).writebytes(src);\n    }\n}\n\n\n\n# 2.8 compositebytebuf 的零拷贝设计\n\n这里的零拷贝并不是我们经常提到的那种 os 层面上的零拷贝，而是 netty 在用户态层面自己实现的避免内存拷贝的设计。比如在传统意义上，如果我们想要将多个独立的 bytebuf 聚合成一个 bytebuf 的时候，我们首先需要向 os 申请一段更大的内存，然后依次将多个 bytebuf 中的内容拷贝到这段新申请的内存上，最后在释放这些 bytebuf 的内存。\n\n这样一来就涉及到两个性能开销点，一个是我们需要向 os 重新申请更大的内存，另一个是内存的拷贝。netty 引入 compositebytebuf 的目的就是为了解决这两个问题。巧妙地利用原有 bytebuf 所占的内存，在此基础之上，将它们组合成一个逻辑意义上的 compositebytebuf ，提供一个统一的逻辑视图。\n\ncompositebytebuf 其实也是一种视图 bytebuf ，这一点和上小节中我们介绍的 slicedbytebuf ， duplicatedbytebuf 一样，它们本身并不会占用 native memory，底层数据的存储全部依赖于原生的 bytebuf。\n\n不同点在于，slicedbytebuf，duplicatedbytebuf 它们是在单一的原生 bytebuf 基础之上创建出的视图 bytebuf。而 compositebytebuf 是基于多个原生 bytebuf 创建出的统一逻辑视图 bytebuf。\n\ncompositebytebuf 对于我们用户来说和其他的普通 bytebuf 没有任何区别，有自己独立的 readerindex，writerindex，capacity，maxcapacity，前面几个小节中介绍的各种 bytebuf 的设计要素，在 compositebytebuf 身上也都会体现。\n\n但从实现的角度来说，compositebytebuf 只是一个逻辑上的 bytebuf，其本身并不会占用任何的 native memory ，对于 compositebytebuf 的任何操作，最终都需要转换到其内部具体的 bytebuf 上。本小节我们就来深入到 compositebytebuf 的内部，来看一下 netty 的巧妙设计。\n\n# 2.8.1 compositebytebuf 的总体架构\n\n从总体设计上来讲，compositebytebuf 包含如下五个重要属性，其中最为核心的就是 components 数组，那些需要被聚合的原生 bytebuf 会被 netty 封装在 component 类中，并统一组织在 components 数组中。后续针对 compositebytebuf 的所有操作都需要和这个数组打交道。\n\npublic class compositebytebuf extends abstractreferencecountedbytebuf implements iterable<bytebuf> {\n    // 内部 bytebuf 的分配器，用于后续扩容，copy , 合并等操作\n    private final bytebufallocator alloc;\n    // compositedirectbuffer 还是 compositeheapbuffer ?\n    private final boolean direct;\n    // 最大的 components 数组容量（16）\n    private final int maxnumcomponents;\n    // 当前 compositebytebuf 中包含的 components 个数\n    private int componentcount;\n    // 存储 component 的数组\n    private component[] components; // resized when needed\n}\n\n\nmaxnumcomponents 表示 components 数组最大的容量，compositebytebuf 默认能够包含 component 的最大个数为 16，如果超过这个数量的话，netty 会将当前 compositebytebuf 中包含的所有 components 重新合并成一个更大的 component。\n\npublic abstract class abstractbytebufallocator implements bytebufallocator {\n    static final int default_max_components = 16;\n}\n\n\ncomponentcount 表示当前 compositebytebuf 中包含的 component 个数。每当我们通过 addcomponent 方法向 compositebytebuf 添加一个新的 bytebuf 时，netty 都会用一个新的 component 实例来包装这个 bytebuf，然后存放在 components 数组中，最后 componentcount 的个数加 1 。\n\ncompositebytebuf 与其底层聚合的真实 bytebuf 架构设计关系，如下图所示：\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n而创建一个 compositebytebuf 的核心其实就是创建底层的 components 数组，后续添加到该 compositebytebuf 的所有原生 bytebuf 都会被组织在这里。\n\n   private compositebytebuf(bytebufallocator alloc, boolean direct, int maxnumcomponents, int initsize) {\n        // 设置 maxcapacity\n        super(abstractbytebufallocator.default_max_capacity);\n\n        this.alloc = objectutil.checknotnull(alloc, \"alloc\");\n        this.direct = direct;\n        this.maxnumcomponents = maxnumcomponents;\n        // 初始 component 数组的容量为 maxnumcomponents\n        components = newcomparray(initsize, maxnumcomponents);\n    }\n\n\n这里的参数 initsize 表示的并不是 compositebytebuf 所包含的字节数，而是初始包装的原生 bytebuf 个数，也就是初始 component 的个数。components 数组的总体大小由参数 maxnumcomponents 决定，但不能超过 16 。\n\n   private static component[] newcomparray(int initcomponents, int maxnumcomponents) {\n        // max_component\n        int capacityguess = math.min(abstractbytebufallocator.default_max_components, maxnumcomponents);\n        // 初始 component 数组的容量为 maxnumcomponents\n        return new component[math.max(initcomponents, capacityguess)];\n    }\n\n\n现在我们只是清楚了 compositebytebuf 的一个基本骨架，那么接下来 netty 如何根据这个基本的骨架将多个原生 bytebuf 组装成一个逻辑上的统一视图 bytebuf 呢 ？\n\n也就是说我们依据 compositebytebuf 中的 readerindex 以及 writerindex 进行的读写操作逻辑如何转换到对应的底层原生 bytebuf 之上呢 ？ 这个是整个设计的核心所在。\n\n下面笔者就带着大家从外到内，从易到难地一一拆解 compositebytebuf 中的那些核心设计要素。从 compositebytebuf 的最外层来看，其实我们并不陌生，对于用户来说它就是一个普通的 bytebuf，拥有自己独立的 readerindex ，writerindex 。\n\nimage.png\n\n但 compositebytebuf 中那些逻辑上看起来连续的字节，背后其实存储在不同的原生 bytebuf 中。不同 bytebuf 的内存之间其实是不连续的。\n\nimage.png\n\n那么现在问题的关键就是我们如何判断 compositebytebuf 中的某一段逻辑数据背后对应的究竟是哪一个真实的 bytebuf，如果我们能够通过 compositebytebuf 的相关 index , 找到这个 index 背后对应的 bytebuf，近而可以找到 bytebuf 的 index ，这样是不是就可以将 compositebytebuf 的逻辑操作转换成对真实内存的读写操作了。\n\ncompositebytebuf 到原生 bytebuf 的转换关系，netty 封装在 component 类中，每一个被包装在 compositebytebuf 中的原生 bytebuf 都对应一个 component 实例。它们会按照顺序统一组织在 components 数组中。\n\n    private static final class component {\n        // 原生 bytebuf\n        final bytebuf srcbuf; \n        // compositebytebuf 的 index 加上 srcadjustment 就得到了srcbuf 的相关 index\n        int srcadjustment; \n        // srcbuf 可能是一个被包装过的 bytebuf，比如 slicedbytebuf ， duplicatedbytebuf\n        // 被 srcbuf 包装的最底层的 bytebuf 就存放在 buf 字段中\n        final bytebuf buf;      \n        // compositebytebuf 的 index 加上 adjustment 就得到了 buf 的相关 index      \n        int adjustment; \n \n        // 该 component 在 compositebytebuf 视角中表示的数据范围 [offset , endoffset)\n        int offset; \n        int endoffset;        \n    }\n\n\n一个 component 在 compositebytebuf 的视角中所能表示的数据逻辑范围是 [offset , endoffset)。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n比如上图中第一个绿色的 bytebuf , 它里边存储的数据组成了 compositebytebuf 中 [0 , 4) 这段逻辑数据范围。第二个黄色的 bytebuf，它里边存储的数据组成了 compositebytebuf 中 [4 , 8) 这段逻辑数据范围。第三个蓝色的 bytebuf，它里边存储的数据组成了 compositebytebuf 中 [8 , 12) 这段逻辑数据范围。 上一个 component 的 endoffset 恰好是下一个 component 的 offset 。\n\n而这些真实存储数据的 bytebuf 则存储在对应 component 中的 srcbuf 字段中，当我们通过 compositebytebuf 的 readerindex 或者 writerindex 进行读写操作的时候，首先需要确定相关 index 所对应的 srcbuf，然后将 compositebytebuf 的 index 转换为 srcbuf 的 srcindex，近而通过 srcindex 对 srcbuf 进行读写。\n\n这个 index 的转换就是通过 srcadjustment 来进行的，比如，当前 compositebytebuf 的 readerindex 为 5 ，它对应的是第二个黄色的 bytebuf。而 bytebuf 的 readerindex 却是 1 。\n\n所以第二个 component 的 srcadjustment 就是 -4 ， 这样我们读取 compositebytebuf 的时候，首先将它的 readerindex 加上 srcadjustment 就得到了 bytebuf 的 readerindex ，后面就是普通的 bytebuf 读取操作了。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n在比如说，我们要对 compositebytebuf 进行写操作，当前的 writerindex 为 10 ，对应的是第三个蓝色的 bytebuf，它的 writerindex 为 2 。\n\n所以第三个 component 的 srcadjustment 就是 -8 ，compositebytebuf 的 writerindex 加上 srcadjustment 就得到了 bytebuf 的 writerindex，后续就是普通的 bytebuf 写入操作。\n\n       int srcidx(int index) {\n            // compositebytebuf 相关的 index 转换成 srcbuf 的相关 index\n            return index + srcadjustment;\n        }\n\n\n除了 srcbuf 之外，component 实例中还有一个 buf 字段，这里大家可能会比较好奇，为什么设计了两个 bytebuf 字段呢 ？component 实例与 bytebuf 不是一对一的关系吗 ？\n\nsrcbuf 是指我们通过 addcomponent 方法添加到 compositebytebuf 中的原始 bytebuf。而这个 srcbuf 可能是一个视图 bytebuf，比如上一小节中介绍到的 slicedbytebuf 和 duplicatedbytebuf。srcbuf 还可能是一个被包装过的 bytebuf，比如 wrappedbytebuf , swappedbytebuf。\n\n假如 srcbuf 是一个 slicedbytebuf 的话，我们需要将它的原生 bytebuf 拆解出来并保存在 component 实例的 buf 字段中。事实上 component 中的 buf 才是真正存储数据的地方。\n\nabstract class abstractunpooledslicedbytebuf {\n    // 原生 bytebuf\n    private final bytebuf buffer;\n}\n\n\n与 buf 对应的就是 adjustment ， 它用于将 compositebytebuf 的相关 index 转换成 buf 相关的 index ，假如我们在向一个 compositebytebuf 执行 read 操作，它的当前 readerindex 是 5，而 buf 的 readerindex 是 6 。\n\n所以在读取操作之前，我们需要将 compositebytebuf 的 readerindex 加上 adjustment 得到 buf 的 readerindex，近而将读取操作转移到 buf 中。其实就和上小节中介绍的视图 bytebuf 是一模一样的，在读写之前都需要修正相关的 index 。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n   @override\n    public byte getbyte(int index) {\n        // 通过 compositebytebuf 的 index , 找到数据所属的 component\n        component c = findcomponent(index);\n        // 首先通过 idx 转换为 buf 相关的 index\n        // 将对 compositebytebuf 的读写操作转换为 buf 的读写操作\n        return c.buf.getbyte(c.idx(index));\n    }\n\n    int idx(int index) {\n        // 将 compositebytebuf 的相关 index 转换为 buf 的相关 index\n        return index + adjustment;\n     }\n\n\n那么我们如何根据指定的 compositebytebuf 的 index 来查找其对应的底层数据究竟存储在哪个 component 中呢 ？\n\n核心思想其实很简单，因为每个 component 都会描述自己表示 compositebytebuf 中的哪一段数据范围 —— [offset , endoffset)。所有的 components 都被有序的组织在 components 数组中。我们可以通过二分查找的方法来寻找这个 index 到底是落在了哪个 component 表示的范围中。\n\n这个查找的过程是在 findcomponent方法中实现的，netty 会将最近一次访问到的 component 缓存在 compositebytebuf 的 lastaccessed 字段中，每次进行查找的时候首先会判断 index 是否落在了 lastaccessed 所表示的数据范围内 —— [ la.offset , la.endoffset) 。\n\n如果 index 恰好被缓存的 component（lastaccessed）所包含，那么就直接返回 lastaccessed 。\n\n    // 缓存最近一次查找到的 component\n    private component lastaccessed;\n\n    private component findcomponent(int offset) {\n        component la = lastaccessed;\n        // 首先查找 offset 是否恰好落在 lastaccessed 的区间中\n        if (la != null && offset >= la.offset && offset < la.endoffset) {\n           return la;\n        }\n        // 在所有 components 中进行二分查找\n        return findit(offset);\n    }\n\n\n如果 index 不巧没有命中缓存，那么就在整个 components 数组中进行二分查找 ：\n\n    private component findit(int offset) {\n        for (int low = 0, high = componentcount; low <= high;) {\n            int mid = low + high >>> 1;\n            component c = components[mid];\n            if (offset >= c.endoffset) {\n                low = mid + 1;\n            } else if (offset < c.offset) {\n                high = mid - 1;\n            } else {\n                lastaccessed = c;\n                return c;\n            }\n        }\n\n        throw new error(\"should not reach here\");\n    }\n\n\n# 2.8.2 compositebytebuf 的创建\n\n好了，现在我们已经熟悉了 compositebytebuf 的总体架构，那么接下来我们就来看一下 netty 是如何将多个 bytebuf 逻辑聚合成一个 compositebytebuf 的。\n\npublic final class unpooled {\n   public static bytebuf wrappedbuffer(bytebuf... buffers) {\n        return wrappedbuffer(buffers.length, buffers);\n    }\n}\n\n\ncompositebytebuf 的初始 maxnumcomponents 为 buffers 数组的长度，如果我们只是传入一个 bytebuf 的话，那么就无需创建 compositebytebuf，而是直接返回该 bytebuf 的 slice 视图。\n\n如果我们传入的是多个 bytebuf 的话，则将这多个 bytebuf 包装成 compositebytebuf 返回。\n\npublic final class unpooled {\n    public static bytebuf wrappedbuffer(int maxnumcomponents, bytebuf... buffers) {\n        switch (buffers.length) {\n        case 0:\n            break;\n        case 1:\n            bytebuf buffer = buffers[0];\n            if (buffer.isreadable()) {\n                // 直接返回 buffer.slice() 视图\n                return wrappedbuffer(buffer.order(big_endian));\n            } else {\n                buffer.release();\n            }\n            break;\n        default:\n            for (int i = 0; i < buffers.length; i++) {\n                bytebuf buf = buffers[i];\n                if (buf.isreadable()) {\n                    // 从第一个可读的 bytebuf —— buffers[i] 开始创建 compositebytebuf\n                    return new compositebytebuf(alloc, false, maxnumcomponents, buffers, i);\n                }\n                // buf 不可读则 release\n                buf.release();\n            }\n            break;\n        }\n        return empty_buffer;\n    }\n}\n\n\n在进入 compositebytebuf 的创建流程之后，首先是创建出一个空的 compositebytebuf，也就是先把 compositebytebuf 的骨架搭建起来，这时它的 initsize 为 buffers.length - offset 。\n\n注意 initsize 表示的并不是 compositebytebuf 初始包含的字节个数，而是表示初始 component 的个数。offset 则表示从 buffers 数组中的哪一个索引开始创建 compositebytebuf，就是上面 compositebytebuf 构造函数中最后一个参数 i 。\n\n随后通过 addcomponents0 方法为 buffers 数组中的每一个 bytebuf 创建初始化 component 实例，并将他们有序的添加到 compositebytebuf 的 components 数组中。\n\n但这时 component 实例的个数可能已经超过 maxnumcomponents 限制的个数，那么接下来就会在 consolidateifneeded() 方法中将当前 compositebytebuf 中的所有 components 合并成一个更大的 component。compositebytebuf 中的 components 数组长度是不可以超过 maxnumcomponents 限制的，如果超过就需要在这里合并。\n\n最后设置当前 compositebytebuf 的 readerindex 和 writerindex，在初始状态下 compositebytebuf 的 readerindex 会被设置为 0 ，writerindex 会被设置为最后一个 component 的 endoffset 。\n\n    compositebytebuf(bytebufallocator alloc, boolean direct, int maxnumcomponents,\n            bytebuf[] buffers, int offset) {\n        // 先初始化一个空的 compositebytebuf\n        // initsize 为 buffers.length - offset\n        this(alloc, direct, maxnumcomponents, buffers.length - offset);\n        // 为所有的 buffers 创建  component 实例，并添加到 components 数组中\n        addcomponents0(false, 0, buffers, offset);\n        // 如果当前 component 的个数已经超过了 maxnumcomponents，则将所有 component 合并成一个\n        consolidateifneeded();\n        // 设置 compositebytebuf 的 readerindex = 0\n        // writerindex 为最后一个 component 的 endoffset\n        setindex0(0, capacity());\n    }\n\n\n# 2.8.3 shiftcomps 为新的 bytebuf 腾挪空间\n\n在整个 compositebytebuf 的构造过程中，最核心也是最复杂的步骤其实就是 addcomponents0 方法，将多个 bytebuf 有序的添加到 compositebytebuf 的 components 数组中看似简单，其实还有很多种复杂的情况需要考虑。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n复杂之处在于这些 bytebuf 需要插在 components 数组的哪个位置上 ？ 比较简单直观的情况是我们直接在 components 数组的末尾插入，也就是说要插入的位置索引 cindex 等于 componentcount。这里分为两种情况：\n\n 1. cindex = componentcount = 0 ，这种情况表示我们在向一个空的 compositebytebuf 插入 bytebufs , 很简单，直接插入即可。\n 2. cindex = componentcount > 0 ， 这种情况表示我们再向一个非空的 compositebytebuf 插入 bytebufs，正如上图所示。同样也很简单，直接在 componentcount 的位置处插入即可。\n\n稍微复杂一点的情况是我们在 components 数组的中间位置进行插入而不是在末尾，也就是 cindex < componentcount 的情况。如下如图所示，假设我们现在需要在 cindex = 3的位置处插入两个 bytebuf 进来，但现在 components[3] 以及 components[4] 的位置已经被占用了。所以我们需要将这两个位置上的原有 component 向后移动两个位置，将 components[3] 和 components[4] 的位置腾出来。\n\n// i = 3 , count = 2 , size = 5\nsystem.arraycopy(components, i, components, i + count, size - i);\n\n\nimage.png\n\n在复杂一点的情况就是 components 数组需要扩容，当一个 compositebytebuf 刚刚被初始化出来的时候，它的 components 数组长度等于 maxnumcomponents。\n\n如果当前 components 数组中包含的 component 个数 —— componentcount 加上本次需要添加的 bytebuf 个数 —— count 已经超过了 maxnumcomponents 的时候，就需要对 components 数组进行扩容。\n\n        // 初始为 0，当前 compositebytebuf 中包含的 component 个数\n        final int size = componentcount, \n        // 本次 addcomponents0 操作之后，新的 component 个数\n        newsize = size + count;\n       \n        // newsize 超过了 maxnumcomponents 则对 components 数组进行扩容\n        if (newsize > components.length) {\n            ....... 扩容 ....\n\n            // 扩容后的新数组\n            components = newarr;\n        }\n\n\n扩容之后的 components 数组长度是在 newsize 与原来长度的 3 / 2之间取一个最大值。\n\nint newarrsize = math.max(size + (size >> 1), newsize);\n\n\n如果我们原来恰好是希望在 components 数组的末尾插入，也就是 cindex = componentcount 的情况，那么就需要通过 arrays.copyof 首先申请一段长度为 newarrsize 的数组，然后将原来的 components 数组中的内容原样拷贝过去。\n\nnewarr = arrays.copyof(components, newarrsize, component[].class);\n\n\n这样新的 components 数组就有位置可以容纳本次需要加入的 bytebuf 了。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n如果我们希望在原来 components 数组的中间插入，也就是 cindex < componentcount 的情况，如下图所示：\n\nimage.png\n\n这种情况在扩容的时候就不能原样拷贝原 components 数组了，而是首先通过 system.arraycopy 将 [0 , cindex) 这段范围的内容拷贝过去，在将 [cindex , componentcount)这段范围的内容拷贝到新数组的 cindex + count 位置处。\n\nimage.png\n\n这样一来，就在新 components 数组的 cindex 索引处，空出了两个位置出来用来添加本次这两个 bytebuf。最后更新 componentcount 的值。以上腾挪空间的逻辑封装在 shiftcomps 方法中：\n\n    private void shiftcomps(int i, int count) {\n        // 初始为 0，当前 compositebytebuf 中包含的 component 个数\n        final int size = componentcount, \n        // 本次 addcomponents0 操作之后，新的 component 个数\n        newsize = size + count;\n       \n        // newsize 超过了 max components（16） 则对 components 数组进行扩容\n        if (newsize > components.length) {\n            // grow the array，扩容到原来的 3 / 2\n            int newarrsize = math.max(size + (size >> 1), newsize);\n            component[] newarr;\n            if (i == size) {\n                // 在 component[] 数组的末尾进行插入\n                // 初始状态 i = size = 0\n                // size - 1 是 component[] 数组的最后一个元素，指定的 i 恰好越界\n                // 原来 component[] 数组中的内容全部拷贝到 newarr 中\n                newarr = arrays.copyof(components, newarrsize, component[].class);\n            } else {\n                // 在 component[] 数组的中间进行插入\n                newarr = new component[newarrsize];\n                if (i > 0) {\n                    // [0 , i) 之间的内容拷贝到 newarr 中\n                    system.arraycopy(components, 0, newarr, 0, i);\n                }\n                if (i < size) {\n                    // 将剩下的 [i , size) 内容从 newarr 的 i + count 位置处开始拷贝。\n                    // 因为需要将原来的 [ i , i+count ） 这些位置让出来，添加本次新的 components，\n                    system.arraycopy(components, i, newarr, i + count, size - i);\n                }\n            }\n            // 扩容后的新数组\n            components = newarr;\n        } else if (i < size) {\n            // i < size 本次操作要覆盖原来的 [ i , i+count ） 之间的位置，所以这里需要将原来位置上的 component 向后移动\n            system.arraycopy(components, i, components, i + count, size - i);\n        }\n        // 更新 componentcount\n        componentcount = newsize;\n    }\n\n\n# 2.8.4 component 如何封装 bytebuf\n\n经过上一小节 shiftcomps 方法的辗转腾挪之后，现在 compositebytebuf 中的 components 数组终于有位置可以容纳本次需要添加的 bytebuf 了。接下来就需要为每一个 bytebuf 创建初始化一个 component 实例，最后将这些 component 实例放到 components 数组对应的位置上。\n\n    private static final class component {\n        // 原生 bytebuf\n        final bytebuf srcbuf; \n        // compositebytebuf 的 index 加上 srcadjustment 就得到了srcbuf 的相关 index\n        int srcadjustment; \n        // srcbuf 可能是一个被包装过的 bytebuf，比如 slicedbytebuf ， duplicatedbytebuf\n        // 被 srcbuf 包装的最底层的 bytebuf 就存放在 buf 字段中\n        final bytebuf buf;      \n        // compositebytebuf 的 index 加上 adjustment 就得到了 buf 的相关 index      \n        int adjustment; \n \n        // 该 component 在 compositebytebuf 视角中表示的数据范围 [offset , endoffset)\n        int offset; \n        int endoffset;        \n    }\n\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n我们首先需要初始化 component 实例的 offset ， endoffset 属性，前面我们已经介绍了，一个 component 在 compositebytebuf 的视角中所能表示的数据逻辑范围是 [offset , endoffset)。在 components 数组中，一般前一个 component 的 endoffset 往往是后一个 component 的 offset。\n\n如果我们期望从 components 数组的第一个位置处开始插入（cindex = 0），那么第一个 component 的 offset 自然是 0 。\n\n如果 cindex > 0 , 那么我们就需要找到它上一个 component —— components[cindex - 1] ， 上一个 component 的 endoffset 恰好就是当前 component 的 offset。\n\n然后通过 newcomponent 方法利用 bytebuf 相关属性以及 offset 来初始化 component 实例。随后将创建出来的 component 实例放置在对应的位置上 —— components[cindex] 。\n\n           // 获取当前正在插入 component 的 offset\n           int nextoffset = cindex > 0 ? components[cindex - 1].endoffset : 0;\n            for (ci = cindex; arroffset < len; arroffset++, ci++) {\n                // 待插入 bytebuf\n                bytebuf b = buffers[arroffset];\n                if (b == null) {\n                    break;\n                }\n                // 将 bytebuf 封装在 component 中\n                component c = newcomponent(ensureaccessible(b), nextoffset);\n                components[ci] = c;\n                // 下一个 component 的 offset 是上一个 component 的 endoffset\n                nextoffset = c.endoffset;\n            }\n\n\n假设现在有一个空的 compositebytebuf，我们需要将一个数据范围为 [1 , 4] , readerindex = 1 的 srcbuf ， 插入到 compositebytebuf 的 components 数组中。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n但是如果该 srcbuf 是一个视图 bytebuf 的话，比如：slicedbytebuf ， duplicatedbytebuf。或者是一个被包装过的 bytebuf ，比如：wrappedbytebuf ， swappedbytebuf。\n\n那么我们就需要对 srcbuf 不断的执行 unwrap(), 将其最底层的原生 bytebuf 提取出来，如上图所示，原生 buf 的数据范围为 [4 , 7] , srcbuf 与 buf 之间相关 index 的偏移 adjustment 等于 3 , 原生 buf 的 readerindex = 4 。\n\n最后我们会根据 srcbuf ， srcindex（srcbuf 的 readerindex），原生 buf ，unwrappedindex（buf 的 readerindex），offset ， len （srcbuf 中的可读字节数）来初始化 component 实例。\n\n    private component newcomponent(final bytebuf buf, final int offset) {\n        // srcbuf 的 readerindex = 1\n        final int srcindex = buf.readerindex();\n        // srcbuf 中的可读字节数 = 4\n        final int len = buf.readablebytes();\n\n        // srcbuf 可能是一个被包装过的 bytebuf，比如 slicedbytebuf，duplicatedbytebuf\n        // 获取 srcbuf 底层的原生 bytebuf\n        bytebuf unwrapped = buf;\n        // 原生 bytebuf 的 readerindex\n        int unwrappedindex = srcindex;\n        while (unwrapped instanceof wrappedbytebuf || unwrapped instanceof swappedbytebuf) {\n            unwrapped = unwrapped.unwrap();\n        }\n\n        // unwrap if already sliced\n        if (unwrapped instanceof abstractunpooledslicedbytebuf) {\n            // 获取视图 bytebuf  相对于 原生 bytebuf 的相关 index 偏移\n            // adjustment = 3\n            // unwrappedindex = srcindex + adjustment = 4\n            unwrappedindex += ((abstractunpooledslicedbytebuf) unwrapped).idx(0);\n            // 获取原生 bytebuf\n            unwrapped = unwrapped.unwrap();\n        } else if (unwrapped instanceof pooledslicedbytebuf) {\n            unwrappedindex += ((pooledslicedbytebuf) unwrapped).adjustment;\n            unwrapped = unwrapped.unwrap();\n        } else if (unwrapped instanceof duplicatedbytebuf || unwrapped instanceof pooledduplicatedbytebuf) {\n            unwrapped = unwrapped.unwrap();\n        }\n\n        return new component(buf.order(byteorder.big_endian), srcindex,\n                unwrapped.order(byteorder.big_endian), unwrappedindex, offset, len, slice);\n    }\n\n\n由于当前的 compositebytebuf 还是空的，里面没有包含任何逻辑数据，当长度为 4 的 srcbuf 加入之后，compositebytebuf 就产生了 [0 , 3] 这段逻辑数据范围，所以 srcbuf 所属 component 的 offset = 0 , endoffset = 4 ，srcadjustment = 1 ，adjustment = 4。\n\nimage.png\n\n        component(bytebuf srcbuf, int srcoffset, bytebuf buf, int bufoffset,\n                int offset, int len, bytebuf slice) {\n            this.srcbuf = srcbuf;\n            // 用于将 compositebytebuf 的 index 转换为 srcbuf 的index\n            // 1 - 0 = 1\n            this.srcadjustment = srcoffset - offset;\n            this.buf = buf;\n            // 用于将 compositebytebuf 的 index 转换为 buf 的index\n            // 4 - 0 = 4\n            this.adjustment = bufoffset - offset;\n            // compositebytebuf [offset , endoffset) 这段范围的字节存储在该 component 中\n            //  0 \n            this.offset = offset;\n            // 下一个 component 的 offset\n            // 4\n            this.endoffset = offset + len;\n        }\n\n\n当我们继续初始化下一个 component 的时候，它的 offset 其实就是这个 component 的 endoffset 。后面的流程都是一样的了。\n\n# 2.8.5 addcomponents0\n\n在我们清楚了以上背景知识之后，在看 addcomponents0 方法的逻辑就很清晰了：\n\n    private compositebytebuf addcomponents0(boolean increasewriterindex,\n            final int cindex, bytebuf[] buffers, int arroffset) {\n        // buffers 数组长度\n        final int len = buffers.length, \n        // 本次批量添加的 bytebuf 个数\n        count = len - arroffset;\n        // ci 表示从 components 数组的哪个索引位置处开始添加\n        // 这里先给一个初始值，后续 shiftcomps 完成之后还会重新设置\n        int ci = integer.max_value;\n        try {\n            // cindex >= 0 && cindex <= componentcount\n            checkcomponentindex(cindex);\n            // 为新添加进来的 bytebuf 腾挪位置，以及增加 componentcount 计数\n            shiftcomps(cindex, count); // will increase componentcount\n            // 获取当前正在插入 component 的 offset\n            int nextoffset = cindex > 0 ? components[cindex - 1].endoffset : 0;\n            for (ci = cindex; arroffset < len; arroffset++, ci++) {\n                bytebuf b = buffers[arroffset];\n                if (b == null) {\n                    break;\n                }\n                // 将 bytebuf 封装在 component 中\n                component c = newcomponent(ensureaccessible(b), nextoffset);\n                components[ci] = c;\n                // 下一个 component 的 offset 是上一个 component 的 endoffset\n                nextoffset = c.endoffset;\n            }\n            return this;\n        } finally {\n            // ci is now the index following the last successfully added component\n            // ci = componentcount 说明是一直按照顺序向后追加 component\n            // ci < componentcount 表示在 components 数组的中间插入新的 component\n            if (ci < componentcount) {\n                // 如果上面 for 循环完整的走完，ci = cindex + count\n                if (ci < cindex + count) {\n                    // 上面 for 循环中有 break 的情况出现或者有异常发生\n                    // ci < componentcount ，在上面的 shiftcomps 中将会涉及到 component 移动，因为要腾出位置\n                    // 如果发生异常，则将后面没有加入 components 数组的 component 位置删除掉\n                    // [ci, cindex + count) 这段位置要删除，因为在 ci-1 处已经发生异常，重新调整 components 数组\n                    removecomprange(ci, cindex + count);\n                    for (; arroffset < len; ++arroffset) {\n                        referencecountutil.saferelease(buffers[arroffset]);\n                    }\n                }\n                // （在中间插入的情况下）需要调整 ci 到 size -1 之间的 component 的相关 offset\n                updatecomponentoffsets(ci); // only need to do this here for components after the added ones\n            }\n            if (increasewriterindex && ci > cindex && ci <= componentcount) {\n                // 本次添加的最后一个 components[ci - 1]\n                // 本次添加的第一个 components[cindex]\n                // 最后一个 endoffset 减去第一个的 offset 就是本次添加的字节个数\n                writerindex += components[ci - 1].endoffset - components[cindex].offset;\n            }\n        }\n    }\n\n\n这里我们重点介绍下 finally {} 代码块中的逻辑。首先 addcomponents0 方法中的核心逻辑是先通过 shiftcomps 方法为接下来新创建出来的 component 腾挪位置，因为我们有可能是在原有 components 数组的中间位置插入。\n\n然后会在一个 for () 循环中不停的将新创建的 component 放置到 components[ci] 位置上。\n\n当跳出 for 循环进入 finally 代码块的时候，ci 的值恰恰就是最后一个成功加入 components 数组的 component 下一个位置，如下图所示，假设 components[0] ， components[1] ，components[2] 是我们刚刚在 for 循环中插入的新值，那么 for 循环结束之后，ci 的值就是 3 。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n如果 ci = componentcount 这恰恰说明我们一直是在 components 数组的末尾进行插入，这种情况下各个 component 实例中的 [offset , endoffset) 都是连续的不需要做任何调整。\n\n但如果 ci < componentcount 这就说明了我们是在原来 components 数组的中间位置处开始插入，下图中的 components[3] ，components[4] 是插入位置，当插入完成之后 ci 的值为 5。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n这时候就需要重新调整 components[5]，components[6] 中的 [offset , endoffset) 范围，因为 shiftcomps 方法只负责帮你腾挪位置，不负责重新调整 [offset , endoffset) 范围，当新的 component 实例插入之后，原来彼此相邻的 component 实例之间的 [offset , endoffset) 就不连续了，所以这里需要重新调整。\n\n比如下图中所展示的情况，原来的 components 数组包含五个 component 实例，分别在 0 - 4 位置，它们之间原本的是连续的 [offset , endoffset)。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n现在我们要在位置 3 ，4 处插入两个新的 component 实例，所以原来的 components[3] ，components[4] 需要移动到 components[5] ，components[6] 的位置上，但 shiftcomps 只负责移动而不负责重新调整它们的 [offset , endoffset)。\n\n当新的 component 实例插入之后，components[4]，components[5] ，components[6] 之间的 [offset , endoffset) 就不连续了。所以需要通过 updatecomponentoffsets 方法重新调整。\n\n    private void updatecomponentoffsets(int cindex) {\n        int size = componentcount;\n        if (size <= cindex) {\n            return;\n        }\n        // 重新调整 components[5] ，components[6] 之间的 [offset , endoffset)\n        int nextindex = cindex > 0 ? components[cindex - 1].endoffset : 0;\n        for (; cindex < size; cindex++) {\n            component c = components[cindex];\n            // 重新调整 component 的 offset ， endoffset\n            c.reposition(nextindex);\n            nextindex = c.endoffset;\n        }\n    }\n\n     void reposition(int newoffset) {\n            int move = newoffset - offset;\n            endoffset += move;\n            srcadjustment -= move;\n            adjustment -= move;\n            offset = newoffset;\n      }\n        \n\n\n以上介绍的是正常情况下的逻辑，如果在执行 for 循环的过程中出现了 break 或者发生了异常，那么 ci 的值一定是小于 cindex + count 的。什么意思呢 ？\n\n比如我们要向一个 components 数组 cindex = 0 的位置插入 count = 5 个 component 实例，但是在插入第四个 component 的时候，也就是在 components[3] 的位置处出现了 break 或者异常的情况，那么就会退出 for 循环来到这里的 finally 代码块。\n\n此时的 ci 值为 3 ，cindex + count 的值为 5，那么就说明出现了异常情况。\n\nimage.png\n\n值得我们注意的是，components[3] 以及 components[4] 这两个位置是之前通过 shiftcomps 方法腾挪出来的，由于异常情况的发生，这两个位置将不会放置任何 component 实例。\n\n这样一来 components 数组就出现了空洞，所以接下来我们还需要将 components[5] ， components[6] 位置上的 component 实例重新移动回 components[3] 以及 components[4] 的位置上。\n\n由于异常情况，那些 bytebuf 数组中没有被添加进 compositebytebuf 的 bytebuf 需要执行 release 。\n\n# 2.8.6 consolidateifneeded\n\n到现在为止一个空的 compositebytebuf 就算被填充好了，但是这里有一个问题，就是 compositebytebuf 中所能包含的 component 实例个数是受到 maxnumcomponents 限制的。\n\n我们回顾一下整个 addcomponents 的过程，好像还没有一个地方对 component 的个数做出限制，甚至在 shiftcomps 方法中还会对 components 数组进行扩容。\n\n那么这样一来，component 的个数有很大可能会超过 maxnumcomponents 的限制，如果当前 compositebytebuf 中包含的 component 个数已经超过了 maxnumcomponents ，那么就需要在 consolidate0 方法中，将所有的 component 合并。\n\n    private void consolidateifneeded() {\n        int size = componentcount;\n        // 如果当前 component 的个数已经超过了 maxnumcomponents，则将所有 component 合并成一个\n        if (size > maxnumcomponents) {\n            consolidate0(0, size);\n        }\n    }\n\n\n在这里，netty 会将当前 compositebytebuf 中包含的所有 component 合并成一个更大的 component。合并之后 ，compositebytebuf 中就只包含一个 component 了。合并的核心逻辑如下：\n\n 1. 根据当前 compositebytebuf 的 capacity 重新申请一个更大的 bytebuf ，该 bytebuf 需要容纳下 compositebytebuf 所能表示的所有字节。\n 2. 将所有 component 底层的 buf 中存储的内容全部转移到新的 bytebuf 中，并释放原有 buf 的内存。\n 3. 删除 component 数组中所有的 component。\n 4. 根据新的 bytebuf 创建一个新的 component 实例，并放置在 components 数组的第一个位置上。\n\n    private void consolidate0(int cindex, int numcomponents) {\n        if (numcomponents <= 1) {\n            return;\n        }\n        // 将 [cindex , endcindex) 之间的 components 合并成一个\n        final int endcindex = cindex + numcomponents;\n        final int startoffset = cindex != 0 ? components[cindex].offset : 0;\n        // 计算合并范围内 components 的存储的字节总数\n        final int capacity = components[endcindex - 1].endoffset - startoffset;\n        // 重新申请一个新的 bytebuf\n        final bytebuf consolidated = allocbuffer(capacity);\n        // 将合并范围内的 components 中的数据全部转移到新的 bytebuf 中\n        for (int i = cindex; i < endcindex; i ++) {\n            components[i].transferto(consolidated);\n        }\n        lastaccessed = null;\n        // 数据转移完成之后，将合并之前的这些 components 删除\n        removecomprange(cindex + 1, endcindex);\n        // 将合并之后的新 component 存储在 cindex 位置处\n        components[cindex] = newcomponent(consolidated, 0);\n        if (cindex != 0 || numcomponents != componentcount) {\n            // 如果 cindex 不是从 0 开始的，那么就更新 newcomponent 的相关 offset\n            updatecomponentoffsets(cindex);\n        }\n    }\n\n\n# 2.8.7 compositebytebuf 的应用\n\n当我们在传输层采用 tcp 协议进行数据传输的时候，经常会遇到半包或者粘包的问题，我们从 socket 中读取出来的 bytebuf 很大可能还构不成一个完整的包，这样一来，我们就需要将每次从 socket 中读取出来的 bytebuf 在用户态缓存累加起来。\n\n当累加起来的 bytebuf 达到一个完整的数据包之后，我们在从这个被缓存的 bytebuf 中读取字节，然后进行解码，最后将解码出来的对象沿着 pipeline 向后传递。\n\npublic abstract class bytetomessagedecoder extends channelinboundhandleradapter {\n    // 缓存累加起来的 bytebuf\n    bytebuf cumulation;\n    // bytebuf 的累加聚合器\n    private cumulator cumulator = merge_cumulator;\n    // 是否是第一次收包\n    private boolean first;\n\n    @override\n    public void channelread(channelhandlercontext ctx, object msg) throws exception {\n        if (msg instanceof bytebuf) {\n            // 用于存储解码之后的对象\n            codecoutputlist out = codecoutputlist.newinstance();\n            try {\n                // 第一次收包\n                first = cumulation == null;\n                // 将新进来的 (bytebuf) msg 与之前缓存的 cumulation 聚合累加起来\n                cumulation = cumulator.cumulate(ctx.alloc(),\n                        first ? unpooled.empty_buffer : cumulation, (bytebuf) msg);\n                // 解码\n                calldecode(ctx, cumulation, out);\n            } catch (decoderexception e) {\n                throw e;\n            } catch (exception e) {\n                throw new decoderexception(e);\n            } finally {\n                    ........ 省略 ........\n                    // 解码成功之后，就将解码出来的对象沿着 pipeline 向后传播\n                    firechannelread(ctx, out, size); \n            }\n        } else {\n            ctx.firechannelread(msg);\n        }\n    }\n}\n\n\nnetty 为此专门定义了一个 cumulator 接口，用于将每次从 socket 中读取到的 bytebuf 聚合累积起来。参数 alloc 是一个 bytebuf 分配器，用于在聚合的过程中如果涉及到扩容，合并等操作可以用它来申请内存。\n\n参数 cumulation 就是之前缓存起来的 bytebuf，当第一次收包的时候，这里的 cumulation 就是一个空的 bytebuf —— unpooled.empty_buffer 。\n\n参数 in 则是本次刚刚从 socket 中读取出来的 bytebuf，可能是一个半包，cumulator 的作用就是将新读取出来的 bytebuf （in），累加合并到之前缓存的 bytebuf （cumulation）中。\n\n    public interface cumulator {\n        bytebuf cumulate(bytebufallocator alloc, bytebuf cumulation, bytebuf in);\n    }\n\n\nnetty 提供了 cumulator 接口的两个实现，一个是 merge_cumulator ， 另一个是 composite_cumulator 。\n\npublic abstract class bytetomessagedecoder extends channelinboundhandleradapter {\n\n    public static final cumulator merge_cumulator\n\n    public static final cumulator composite_cumulator\n}\n\n\nmerge_cumulator 是 netty 默认的 cumulator ，也是传统意义上最为普遍的一种聚合 bytebuf 的实现，它的核心思想是在聚合多个 bytebuf 的时候，首先会申请一块更大的内存，然后将这些需要被聚合的 bytebuf 中的内容全部拷贝到新的 bytebuf 中。然后释放掉原来的 bytebuf 。\n\n效果就是将多个 bytebuf 重新聚合成一个更大的 bytebuf ，但这种方式涉及到内存申请以及内存拷贝的开销，优势就是内存都是连续的，读取速度快。\n\n另外一种实现就是 composite_cumulator ，也是本小节的主题，它的核心思想是将多个 bytebuf 聚合到一个 compositebytebuf 中，不需要额外申请内存，更不需要内存的拷贝。\n\n但由于 compositebytebuf 只是逻辑上的一个视图 bytebuf，其底层依赖的内存还是原来的那些 bytebuf，所以就导致了 compositebytebuf 中的内存不是连续的，在加上 compositebytebuf 的相关 index 设计的比较复杂，所以在读取速度方面可能会比 merge_cumulator 更慢一点，所以我们需要根据自己的场景来权衡考虑，灵活选择。\n\n    public static final cumulator composite_cumulator = new cumulator() {\n        @override\n        public bytebuf cumulate(bytebufallocator alloc, bytebuf cumulation, bytebuf in) {\n            if (!cumulation.isreadable()) {\n                // 之前缓存的已经解码完毕，这里将它释放，并从 in 开始重新累加。\n                cumulation.release();\n                return in;\n            }\n            compositebytebuf composite = null;\n            try {\n                // cumulation 是一个 compositebytebuf，说明 cumulation 之前是一个被聚合过的 bytebuf\n                if (cumulation instanceof compositebytebuf && cumulation.refcnt() == 1) {\n                    composite = (compositebytebuf) cumulation;\n                    // 这里需要保证 compositebytebuf 的 writerindex 与 capacity 相等\n                    // 因为我们需要每次在 compositebytebuf 的末尾聚合添加新的 bytebuf\n                    if (composite.writerindex() != composite.capacity()) {\n                        composite.capacity(composite.writerindex());\n                    }\n                } else {\n                    // 如果 cumulation 不是 compositebytebuf，只是一个普通的 bytebuf\n                    // 说明 cumulation 之前还没有被聚合过，这里是第一次聚合，所以需要先创建一个空的 compositebytebuf\n                    // 然后将 cumulation 添加到 compositebytebuf 中\n                    composite = alloc.compositebuffer(integer.max_value).addflattenedcomponents(true, cumulation);\n                }\n                // 将本次新接收到的 bytebuf（in）添加累积到 compositebytebuf 中\n                composite.addflattenedcomponents(true, in);\n                in = null;\n                return composite;\n            } finally {\n                 ........ 省略聚合失败的处理 ..........\n            }\n        }\n    };\n\n\n\n# 3. heap or direct\n\n在前面的几个小节中，我们讨论了很多 bytebuf 的设计细节，接下来让我们跳出这些细节，重新站在全局的视角下来看一下 bytebuf 的总体设计。\n\nimage.png\n\n在 bytebuf 的整个设计体系中，netty 从 bytebuf 内存布局的角度上，将整个体系分为了 heapbytebuf 和 directbytebuf 两个大类。netty 提供了 platformdependent.directbufferpreferred()方法来指定在默认情况下，是否偏向于分配 direct memory。\n\npublic final class platformdependent {\n    // 是否偏向于分配 direct memory\n    private static final boolean direct_buffer_preferred;\n\n    public static boolean directbufferpreferred() {\n        return direct_buffer_preferred;\n    }\n}\n\n\n要想使得 direct_buffer_preferred 为 true ，必须同时满足以下两个条件：\n\n 1. -dio.netty.nopreferdirect 参数必须指定为 false（默认）。\n 2. cleaner 不为 null , 也就是需要 jdk 中包含有效的 cleaner 机制。\n\n static {\n        direct_buffer_preferred = cleaner != noop\n                                  && !systempropertyutil.getboolean(\"io.netty.nopreferdirect\", false);\n        if (logger.isdebugenabled()) {\n            logger.debug(\"-dio.netty.nopreferdirect: {}\", !direct_buffer_preferred);\n        }\n }\n\n\n如果是安卓平台，那么 cleaner 直接就是 noop，不会做任何判断，默认情况下直接走 heap memory , 除非特殊指定要走 direct memory。\n\n        if (!isandroid()) {\n            if (javaversion() >= 9) {\n                // 检查 sun.misc.unsafe 类中是否包含有效的 invokecleaner 方法\n                cleaner = cleanerjava9.issupported() ? new cleanerjava9() : noop;\n            } else {\n                // 检查 java.nio.bytebuffer 中是否包含了 cleaner 字段\n                cleaner = cleanerjava6.issupported() ? new cleanerjava6() : noop;\n            }\n        } else {\n            cleaner = noop;\n        }\n\n\n如果是 jdk 9 以上的版本，netty 会检查是否可以通过 sun.misc.unsafe 的 invokecleaner 方法正确执行 directbuffer 的 cleaner，如果执行过程中发生异常，那么 cleaner 就为 noop，netty 在默认情况下就会走 heap memory。\n\npublic final class unsafe {\n    public void invokecleaner(java.nio.bytebuffer directbuffer) {\n        if (!directbuffer.isdirect())\n            throw new illegalargumentexception(\"buffer is non-direct\");\n\n        theinternalunsafe.invokecleaner(directbuffer);\n    }\n}\n\n\n如果是 jdk 9 以下的版本，netty 就会通过反射的方式先去获取 directbytebuffer 的 cleaner 字段，如果 cleaner 为 null 或者在执行 clean 方法的过程中出现了异常，那么 cleaner 就为 noop，netty 在默认情况下就会走 heap memory。\n\nclass directbytebuffer extends mappedbytebuffer implements directbuffer\n{\n    private final cleaner cleaner;\n\n    directbytebuffer(int cap) {                   // package-private\n\n        ...... 省略 .....   \n\n        base = unsafe.allocatememory(size);\n        cleaner = cleaner.create(this, new deallocator(base, size, cap));\n    }\n}\n\n\n如果 platformdependent.directbufferpreferred() 方法返回 true ,那么 bytebufallocator 接下来在分配内存的时候，默认情况下就会分配 directbuffer。\n\npublic final class unpooledbytebufallocator  extends abstractbytebufallocator {\n    // bytebuf 分配器\n    public static final unpooledbytebufallocator default =\n            new unpooledbytebufallocator(platformdependent.directbufferpreferred());\n}\n\npublic abstract class abstractbytebufallocator implements bytebufallocator {\n    // 是否默认分配 directbuffer\n    private final boolean directbydefault;\n\n    protected abstractbytebufallocator(boolean preferdirect) {\n        directbydefault = preferdirect && platformdependent.hasunsafe();\n    }\n\n    @override\n    public bytebuf buffer() {\n        if (directbydefault) {\n            return directbuffer();\n        }\n        return heapbuffer();\n    }\n}\n\n\n一般情况下，jdk 都会包含有效的 cleaner 机制，所以我们完全可以仅是通过 -dio.netty.nopreferdirect （默认 false）来控制 netty 默认情况下走 direct memory。\n\n但如果是安卓平台，那么无论 -dio.netty.nopreferdirect 如何设置，netty 默认情况下都会走 heap memory 。\n\n\n# 4. cleaner or nocleaner\n\n站在内存回收的角度，netty 将 bytebuf 分为了带有 cleaner 的 directbytebuf 和没有 cleaner 的 directbytebuf 两个大类。在之前的文章《以 zgc 为例，谈一谈 jvm 是如何实现 reference 语义的》 中的第三小节，笔者详细的介绍过，jvm 如何利用 cleaner 机制来回收 directbytebuffer 背后的 native memory 。\n\n而 cleaner 回收 directbytebuffer 的 native memory 需要依赖 gc 的发生，当一个 directbytebuffer 没有任何强引用或者软引用的时候，如果此时发生 gc , cleaner 才会去回收 native memory。如果很久都没发生 gc ,那么这些 directbytebuffer 所引用的 native memory 将一直不会释放。\n\n所以仅仅是依赖 cleaner 来释放 native memory 是有一定延迟的，极端情况下，如果一直等不来 gc ,很有可能就会发生 oom 。\n\n而 netty 的 bytebuf 设计相当于是对 nio bytebuffer 的一种完善扩展，其底层其实都会依赖一个 jdk 的 bytebuffer。比如，前面介绍的 unpooleddirectbytebuf ， unpooledunsafedirectbytebuf 其底层依赖的就是 jdk directbytebuffer , 而这个 directbytebuffer 就是带有 cleaner 的 bytebuf 。\n\npublic class unpooleddirectbytebuf extends abstractreferencecountedbytebuf {\n    // 底层依赖的 jdk directbytebuffer\n    bytebuffer buffer;\n\n    public unpooleddirectbytebuf(bytebufallocator alloc, int initialcapacity, int maxcapacity) {\n        // 创建 directbytebuffer\n        setbytebuffer(allocatedirect(initialcapacity), false);\n    }\n\n   protected bytebuffer allocatedirect(int initialcapacity) {\n        return bytebuffer.allocatedirect(initialcapacity);\n    }\npublic class unpooledunsafedirectbytebuf extends unpooleddirectbytebuf {\n    // 底层依赖的 jdk directbytebuffer 的内存地址\n    long memoryaddress;\n\n\n    public unpooledunsafedirectbytebuf(bytebufallocator alloc, int initialcapacity, int maxcapacity) {\n         // 调用父类 unpooleddirectbytebuf 构建函数创建底层依赖的 jdk directbytebuffer \n        super(alloc, initialcapacity, maxcapacity);\n    }\n\n    @override\n    final void setbytebuffer(bytebuffer buffer, boolean tryfree) {\n        super.setbytebuffer(buffer, tryfree);\n        // 获取 jdk directbytebuffer 的内存地址\n        memoryaddress = platformdependent.directbufferaddress(buffer);\n    }\n\n\n在 jdk nio 中，凡是通过 bytebuffer.allocatedirect 方法申请到 directbytebuffer 都是带有 cleaer 的。\n\npublic abstract class bytebuffer {\n  public static bytebuffer allocatedirect(int capacity) {\n        return new directbytebuffer(capacity);\n    }\n}\n\nclass directbytebuffer extends mappedbytebuffer implements directbuffer\n{\n    private final cleaner cleaner;\n\n    directbytebuffer(int cap) {                   // package-private\n\n        ...... 省略 .....   \n        // 通过该构造函数申请到的 direct memory 会受到 -xx:maxdirectmemorysize 参数的限制\n        bits.reservememory(size, cap);   \n        // 底层调用 malloc 申请内存\n        base = unsafe.allocatememory(size);\n\n        ...... 省略 .....   \n        // 创建 cleaner\n        cleaner = cleaner.create(this, new deallocator(base, size, cap));\n    }\n}\n\n\n而带有 cleaner 的 directbytebuffer 背后所能引用的 direct memory 是受到 -xx:maxdirectmemorysize jvm 参数限制的。由于 unpooleddirectbytebuf 以及 unpooledunsafedirectbytebuf 都带有 cleaner，所以当他们在系统中没有任何强引用或者软引用的时候，如果发生 gc , cleaner 就会释放他们的 direct memory 。\n\n由于 cleaner 执行会依赖 gc , 而 gc 的发生往往不那么及时，会有一定的延时，所以 netty 为了可以及时的释放 direct memory ，往往选择不依赖 jdk 的 cleaner 机制，手动进行释放。所以就有了 nocleaner 类型的 directbytebuf —— unpooledunsafenocleanerdirectbytebuf 。\n\nclass unpooledunsafenocleanerdirectbytebuf extends unpooledunsafedirectbytebuf {\n\n    @override\n    protected bytebuffer allocatedirect(int initialcapacity) {\n        // 创建没有 cleaner 的 jdk directbytebuffer \n        return platformdependent.allocatedirectnocleaner(initialcapacity);\n    }\n\n    @override\n    protected void freedirect(bytebuffer buffer) {\n        // 既然没有了 cleaner ， 所以 netty 要手动进行释放\n        platformdependent.freedirectnocleaner(buffer);\n    }\n}\n\n\nunpooledunsafenocleanerdirectbytebuf 的底层同样也会依赖一个 jdk directbytebuffer , 但和之前不同的是，这里的 directbytebuffer 是不带有 cleaner 的。\n\n我们通过 jni 来调用 directbytebuffer(long addr, int cap) 构造函数创建出来的 jdk directbytebuffer 都是没有 cleaner 的。但通过这种方式创建出来的 directbytebuffer 背后引用的 native memory 是不会受到 -xx:maxdirectmemorysize jvm 参数限制的。\n\nclass directbytebuffer {\n    // invoked only by jni: newdirectbytebuffer(void*, long)\n    private directbytebuffer(long addr, int cap) {\n        super(-1, 0, cap, cap, null);\n        address = addr;\n        // cleaner 为 null\n        cleaner = null;\n    }\n}\n\n\n既然没有了 cleaner ， 所以 netty 就无法依赖 gc 来释放 direct memory 了，这就要求 netty 必须手动调用 freedirect方法及时地释放 direct memory。\n\n> 事实上，无论 netty 中的 directbytebuf 有没有 cleaner， netty 都会选择手动的进行释放，目的就是为了避免 gc 的延迟 ， 从而及时的释放 direct memory。\n\n那么 netty 中的 directbytebuf 在什么情况下带有 cleaner，又在什么情况下不带 cleaner 呢 ？我们可以通过 platformdependent.usedirectbuffernocleaner 方法的返回值进行判断：\n\npublic final class platformdependent {\n    // netty 的 directbytebuf 是否带有 cleaner\n    private static final boolean use_direct_buffer_no_cleaner;\n    public static boolean usedirectbuffernocleaner() {\n        return use_direct_buffer_no_cleaner;\n    }\n}\n\n\n * use_direct_buffer_no_cleaner = true 表示 netty 创建出来的 directbytebuf 不带有 cleaner 。 direct memory 的用量不会受到 jvm 参数 -xx:maxdirectmemorysize 的限制。\n * use_direct_buffer_no_cleaner = false 表示 netty 创建出来的 directbytebuf 带有 cleaner 。 direct memory 的用量会受到 jvm 参数 -xx:maxdirectmemorysize 的限制。\n\n我们可以通过 -dio.netty.maxdirectmemory 来设置 use_direct_buffer_no_cleaner 的值，除此之外，该参数还可以指定在 netty 层面上可以使用的最大 directmemory 用量。\n\nio.netty.maxdirectmemory = 0 那么 use_direct_buffer_no_cleaner 就为 false , 表示在 netty 层面创建出来的 directbytebuf 都是带有 cleaner 的，这种情况下 netty 并不会限制 maxdirectmemory 的用量，因为限制了也没用，具体能用多少 maxdirectmemory，还是由 jvm 参数 -xx:maxdirectmemorysize 决定的。\n\nio.netty.maxdirectmemory < 0 ，默认为 -1，也就是在默认情况下 use_direct_buffer_no_cleaner 为 true , 创建出来的 directbytebuf 都是不带 cleaner 的。由于在这种情况下 maxdirectmemory 的用量并不会受到 jvm 参数 -xx:maxdirectmemorysize 的限制，所以在 netty 层面上必须限制 maxdirectmemory 的用量，默认值就是 -xx:maxdirectmemorysize 指定的值。\n\n这里需要特别注意的是，netty 层面对于 maxdirectmemory 的容量限制和 jvm 层面对于 maxdirectmemory 的容量限制是单独分别计算的，互不影响。因此站在 jvm 进程的角度来说，总体 maxdirectmemory 的用量是 -xx:maxdirectmemorysize 的两倍。\n\nio.netty.maxdirectmemory > 0 的情况和小于 0 的情况一样，唯一不同的是 netty 层面的 maxdirectmemory 用量是专门由 -dio.netty.maxdirectmemory 参数指定，仍然独立于 jvm 层面的 maxdirectmemory 限制之外单独计算。\n\n所以从这个层面来说，netty 设计 nocleaner 类型的 directbytebuf 的另外一个目的就是为了突破 jvm 对于 maxdirectmemory 用量的限制。\n\npublic final class platformdependent {\n    // netty 层面  direct memory 的用量统计\n    // 为 null 表示在 netty 层面不进行特殊限制，完全由 jvm 进行限制 direct memory 的用量\n    private static final atomiclong direct_memory_counter;\n    // netty 层面 direct memory 的最大用量\n    private static final long direct_memory_limit;\n    // jvm 指定的 -xx:maxdirectmemorysize 最大堆外内存\n    private static final long max_direct_memory = maxdirectmemory0();\n\n    static {\n        long maxdirectmemory = systempropertyutil.getlong(\"io.netty.maxdirectmemory\", -1);\n\n        if (maxdirectmemory == 0 || !hasunsafe() || !platformdependent0.hasdirectbuffernocleanerconstructor()) {\n            // maxdirectmemory = 0 表示后续创建的 directbuffer 是带有 cleaner 的，netty 自己不会强制限定 maxdirectmemory 的用量，完全交给 jdk 的 maxdirectmemory 来限制\n            // 因为 netty 限制了也没用，其底层依然依赖的是 jdk  directbuffer（cleaner），jdk 会限制 maxdirectmemory 的用量\n            // 在没有 unsafe 的情况下，那么就必须使用 cleaner，因为如果不使用 cleaner 的话，又没有 unsafe，我们就无法释放 native memory 了\n            // 如果 jdk 本身不包含创建 nocleaner directbuffer 的构造函数 —— directbytebuffer(long, int)，那么自然只能使用 cleaner\n            use_direct_buffer_no_cleaner = false;\n            // netty 自身不会统计 direct memory 的用量，完全交给 jdk 来统计\n            direct_memory_counter = null;\n        } else {\n            use_direct_buffer_no_cleaner = true;\n            if (maxdirectmemory < 0) {\n                // maxdirectmemory < 0 (默认 -1) 后续创建 nocleaner directbuffer\n                // netty 层面会单独限制 maxdirectmemory 用量，maxdirectmemory 的值与 -xx:maxdirectmemorysize 的值相同\n                // 因为 jdk 不会统计和限制 nocleaner directbuffer 的用量\n                // 注意，这里 netty 的 maxdirectmemory 和 jdk 的 maxdirectmemory 是分别单独统计的\n                // 在 jvm 进程的角度来说，整体 maxdirectmemory 的用量是 -xx:maxdirectmemorysize 的两倍（netty用的和 jdk 用的之和）\n                maxdirectmemory = max_direct_memory;\n                if (maxdirectmemory <= 0) {\n                    direct_memory_counter = null;\n                } else {\n                    // 统计 netty directmemory 的用量\n                    direct_memory_counter = new atomiclong();\n                }\n            } else {\n                // maxdirectmemory > 0 后续创建 nocleaner directbuffer,netty 层面的 maxdirectmemory 就是 io.netty.maxdirectmemory 指定的值\n                direct_memory_counter = new atomiclong();\n            }\n        }\n        logger.debug(\"-dio.netty.maxdirectmemory: {} bytes\", maxdirectmemory);\n        direct_memory_limit = maxdirectmemory >= 1 ? maxdirectmemory : max_direct_memory;\n    }  \n}\n\n\n当 netty 层面的 direct memory 用量超过了 -dio.netty.maxdirectmemory 参数指定的值时，那么就会抛出 outofdirectmemoryerror ，分配 directbytebuf 将会失败。\n\n    private static void incrementmemorycounter(int capacity) {\n        if (direct_memory_counter != null) {\n            long newusedmemory = direct_memory_counter.addandget(capacity);\n            if (newusedmemory > direct_memory_limit) {\n                direct_memory_counter.addandget(-capacity);\n                throw new outofdirectmemoryerror(\"failed to allocate \" + capacity\n                        + \" byte(s) of direct memory (used: \" + (newusedmemory - capacity)\n                        + \", max: \" + direct_memory_limit + ')');\n            }\n        }\n    }\n\n\n\n# 5. unsafe or nounsafe\n\n站在内存访问方式的角度上来说 ， netty 又会将 bytebuf 分为了 unsafe 和 nounsafe 两个大类，其中 nounsafe 的内存访问方式是依赖底层的 jdk bytebuffer，对于 netty bytebuf 的任何操作最终都是会代理给底层 jdk 的 bytebuffer。\n\npublic class unpooleddirectbytebuf extends abstractreferencecountedbytebuf {\n    // 底层依赖的 jdk directbytebuffer\n    bytebuffer buffer;\n\n   @override\n    protected byte _getbyte(int index) {\n        return buffer.get(index);\n    }\n\n    @override\n    protected void _setbyte(int index, int value) {\n        buffer.put(index, (byte) value);\n    }\n}\n\n\n而 unsafe 的内存访问方式则是通过 sun.misc.unsafe 类中提供的众多 low-level direct buffer access api 来对内存地址直接进行访问，由于是脱离 jvm 相关规范直接对内存地址进行访问，所以我们在调用 unsafe 相关方法的时候需要考虑 jvm 以及 os 的各种细节，一不小心就会踩坑出错，所以它是一种不安全的访问方式，但是足够灵活，高效。\n\npublic class unpooledunsafedirectbytebuf extends unpooleddirectbytebuf {\n    // 底层依赖的 jdk directbytebuffer 的内存地址\n    long memoryaddress;\n\n    @override\n    protected byte _getbyte(int index) {\n        return unsafebytebufutil.getbyte(addr(index));\n    }\n\n   final long addr(int index) {\n        // 直接通过内存地址进行访问\n        return memoryaddress + index;\n    }\n\n    @override\n    protected void _setbyte(int index, int value) {\n        unsafebytebufutil.setbyte(addr(index), value);\n    }\n\n}\n\n\nnetty 提供了 -dio.netty.nounsafe 参数来让我们决定是否采用 unsafe 的内存访问方式，默认值是 false , 表示 netty 默认开启 unsafe 访问方式。\n\nfinal class platformdependent0 {\n    // 是否明确禁用 unsafe，null 表示开启  unsafe\n    private static final throwable explicit_no_unsafe_cause = explicitnounsafecause0();\n\n    private static throwable explicitnounsafecause0() {\n        final boolean nounsafe = systempropertyutil.getboolean(\"io.netty.nounsafe\", false);\n        logger.debug(\"-dio.netty.nounsafe: {}\", nounsafe);\n\n        if (nounsafe) {\n            logger.debug(\"sun.misc.unsafe: unavailable (io.netty.nounsafe)\");\n            return new unsupportedoperationexception(\"sun.misc.unsafe: unavailable (io.netty.nounsafe)\");\n        }\n\n        return null;\n    }\n}\n\n\n在确认开启了 unsafe 方式之后，我们就需要近一步确认在当前 jre 的 classpath 下是否存在 sun.misc.unsafe 类，是否能通过反射的方式获取到 unsafe 实例 —— theunsafe 。\n\npublic final class unsafe {\n    // unsafe 实例\n    private static final unsafe theunsafe = new unsafe();\n}\nfinal class platformdependent0 {\n    // 验证 unsafe 是否可用，null 表示 unsafe 是可用状态\n    private static final throwable unsafe_unavailability_cause;\n    static {\n           // 尝试通过反射的方式拿到 theunsafe 实例\n           final object maybeunsafe = accesscontroller.doprivileged(new privilegedaction<object>() {\n                @override\n                public object run() {\n                    try {\n                        final field unsafefield = unsafe.class.getdeclaredfield(\"theunsafe\");\n                        throwable cause = reflectionutil.trysetaccessible(unsafefield, false);\n                        if (cause != null) {\n                            return cause;\n                        }\n                        // the unsafe instance\n                        return unsafefield.get(null);\n                    } catch (nosuchfieldexception e) {\n                        return e;\n                    } catch (securityexception e) {\n                        return e;\n                    } catch (illegalaccessexception e) {\n                        return e;\n                    } catch (noclassdeffounderror e) {\n                        // also catch noclassdeffounderror in case someone uses for example osgi and it made\n                        // unsafe unloadable.\n                        return e;\n                    }\n                }\n            });\n    }\n}\n\n\n在获取到 unsafe 实例之后，我们还需要检查 unsafe 中是否包含所有 netty 用到的 low-level direct buffer access api ，确保这些 api 可以正常有效的运行。比如，是否包含 copymemory 方法。\n\npublic final class unsafe {\n    @forceinline\n    public void copymemory(object srcbase, long srcoffset,\n                           object destbase, long destoffset,\n                           long bytes) {\n        theinternalunsafe.copymemory(srcbase, srcoffset, destbase, destoffset, bytes);\n    }\n}\n\n\n是否可以通过 unsafe 访问到 nio buffer 的 address 字段，因为后续我们需要直接操作内存地址。\n\npublic abstract class buffer {\n    // 内存地址\n    long address;\n}\n\n\n在整个过程中如果发生任何异常，则表示在当前 classpath 下，不存在 sun.misc.unsafe 类或者是由于不同版本 jdk 的设计，unsafe 中没有 netty 所需要的一些必要的访存 api 。这样一来我们就无法使用 unsafe，内存的访问方式就需要回退到 nounsafe。\n\n            if (maybeunsafe instanceof throwable) {\n                unsafe = null;\n                unsafeunavailabilitycause = (throwable) maybeunsafe;\n                logger.debug(\"sun.misc.unsafe.theunsafe: unavailable\", (throwable) maybeunsafe);\n            } else {\n                unsafe = (unsafe) maybeunsafe;\n                logger.debug(\"sun.misc.unsafe.theunsafe: available\");\n            }\n            // 为 null 表示 unsafe 可用\n            unsafe_unavailability_cause = unsafeunavailabilitycause;\n            unsafe = unsafe;\n\n\n如果在整个过程中没有发生任何异常，我们获取到了一个有效的 unsafe 实例，那么后续将正式开启 unsafe 的内存访问方式。\n\nfinal class platformdependent0 {\n    static boolean hasunsafe() {\n        return unsafe != null;\n    }\n}\n\n\n完整的 hasunsafe() 判断逻辑如下：\n\n 1. 如果当前平台是安卓或者 .net ，则不能开启 unsafe，因为这些平台并不包含 sun.misc.unsafe 类。\n 2. -dio.netty.nounsafe 参数需要设置为 false （默认开启）。\n\n3.. 当前 classpath 下是否包含有效的 sun.misc.unsafe 类。\n\n 1. unsafe 实例需要包含必要的访存 api 。\n\npublic final class platformdependent {\n    private static final throwable unsafe_unavailability_cause = unsafeunavailabilitycause0();\n\n    public static boolean hasunsafe() {\n        return unsafe_unavailability_cause == null;\n    }\n    private static throwable unsafeunavailabilitycause0() {\n        if (isandroid()) {\n            logger.debug(\"sun.misc.unsafe: unavailable (android)\");\n            return new unsupportedoperationexception(\"sun.misc.unsafe: unavailable (android)\");\n        }\n\n        if (isikvmdotnet()) {\n            logger.debug(\"sun.misc.unsafe: unavailable (ikvm.net)\");\n            return new unsupportedoperationexception(\"sun.misc.unsafe: unavailable (ikvm.net)\");\n        }\n\n        throwable cause = platformdependent0.getunsafeunavailabilitycause();\n        if (cause != null) {\n            return cause;\n        }\n\n        try {\n            boolean hasunsafe = platformdependent0.hasunsafe();\n            logger.debug(\"sun.misc.unsafe: {}\", hasunsafe ? \"available\" : \"unavailable\");\n            return hasunsafe ? null : platformdependent0.getunsafeunavailabilitycause();\n        } catch (throwable t) {\n            logger.trace(\"could not determine if unsafe is available\", t);\n            // probably failed to initialize platformdependent0.\n            return new unsupportedoperationexception(\"could not determine if unsafe is available\", t);\n        }\n    }\n}\n\n\n如果 platformdependent.hasunsafe() 方法返回 true , 那么后续 netty 都会创建 unsafe 类型的 bytebuf。\n\n\n# 6. pooled or unpooled\n\n站在内存管理的角度上来讲，netty 将 bytebuf 分为了 池化（pooled） 和 非池化（unpooled）两个大类，其中 unpooled 类型的 bytebuf 是用到的时候才去临时创建，使用完的时候再去释放。\n\n而 direct memory 的申请和释放开销相较于 heap memory 会大很多，netty 在面对高并发网络通信的场景下，direct memory 的申请和释放是一个非常频繁的操作，这种大量频繁地内存申请释放操作对程序的性能影响是巨大的，因此 netty 引入了内存池将这些 direct memory 统一池化管理起来。\n\nnetty 提供了 -dio.netty.allocator.type 参数来让我们决定是否采用内存池来管理 bytebuf ， 默认值是 pooled , 也就是说 netty 默认是采用池化的方式来管理 pooledbytebuf 。如果是安卓平台，那么默认是使用非池化的 bytebuf （unpooled）。\n\n * 当参数 io.netty.allocator.type 的值为 pooled 时，netty 的默认 bytebufallocator 是 pooledbytebufallocator.default 。\n * 当参数 io.netty.allocator.type 的值为 unpooled 时，netty 的默认 bytebufallocator 是 unpooledbytebufallocator.default 。\n\npublic final class bytebufutil {\n    // 默认 pooledbytebufallocator，池化管理 bytebuf\n    static final bytebufallocator default_allocator;\n\n    static {\n        // 默认为 pooled\n        string alloctype = systempropertyutil.get(\n                \"io.netty.allocator.type\", platformdependent.isandroid() ? \"unpooled\" : \"pooled\");\n        alloctype = alloctype.tolowercase(locale.us).trim();\n\n        bytebufallocator alloc;\n        if (\"unpooled\".equals(alloctype)) {\n            alloc = unpooledbytebufallocator.default;\n            logger.debug(\"-dio.netty.allocator.type: {}\", alloctype);\n        } else if (\"pooled\".equals(alloctype)) {\n            alloc = pooledbytebufallocator.default;\n            logger.debug(\"-dio.netty.allocator.type: {}\", alloctype);\n        } else {\n            alloc = pooledbytebufallocator.default;\n            logger.debug(\"-dio.netty.allocator.type: pooled (unknown: {})\", alloctype);\n        }\n\n        default_allocator = alloc;\n    }\n}\n\n\n后续 netty 在创建 socketchannel 的时候，在 socketchannelconfig 中指定的 bytebufallocator 就是这里的 bytebufutil.default_allocator，默认情况下为 pooledbytebufallocator。\n\npublic interface bytebufallocator {\n    bytebufallocator default = bytebufutil.default_allocator;\n}\n\npublic class defaultchannelconfig implements channelconfig {\n    // pooledbytebufallocator\n    private volatile bytebufallocator allocator = bytebufallocator.default;\n}\n\n\n当 netty 读取 socket 中的网络数据时，首先会从 defaultchannelconfig 中将 bytebufallocator 获取到，然后利用 bytebufallocator 从内存池中获取一个 directbytebuf ，最后将 socket 中的数据读取到 directbytebuf 中，随后沿着 pipeline 向后传播，进行 io 处理。\n\nprotected class niobyteunsafe extends abstractniounsafe {\n        @override\n        public final void read() {\n            // 获取 socketchannelconfig\n            final channelconfig config = config();\n            // 获取 bytebufallocator ， 默认为 pooledbytebufallocator\n            final bytebufallocator allocator = config.getallocator();\n            // 从内存池中获取 bytebuf\n            bytebuf = allochandle.allocate(allocator);\n            // 读取 socket 中的数据到 bytebuf\n            allochandle.lastbytesread(doreadbytes(bytebuf));\n            // 将 bytebuf 沿着 pipeline 向后传播\n            pipeline.firechannelread(bytebuf);\n\n            ....... 省略 .......\n        }\n}\n\n\n除此之外，netty 还提供了 channeloption.allocator 选项，让我们可以在配置 serverbootstrap 的时候为 socketchannel 灵活指定自定义的 bytebufallocator 。\n\n        eventloopgroup bossgroup = new nioeventloopgroup(1);\n        eventloopgroup workergroup = new nioeventloopgroup();\n\n        serverbootstrap b = new serverbootstrap();\n        b.group(bossgroup, workergroup)\n            // 灵活配置 bytebufallocator\n          .childoption(channeloption.allocator, unpooledbytebufallocator.default;);\n\n\n这里通过 channeloption 来配置 socket 相关的属性是最高优先级的，它会覆盖掉一切默认配置。\n\n\n# 7. metric\n\n在第四小节中，我们介绍了 cleaner 和 nocleaner 这两种 directbytebuf，其中 cleanerdirectbytebuf 的整体 direct memory 的用量是受到 jvm 参数 -xx:maxdirectmemorysize 限制的，而 nocleanerdirectbytebuf 的整体 direct memory 可以突破该参数的限制，jvm 并不会统计这块 direct memory 的用量。\n\nnetty 为了及时地释放这些 direct memory，通常默认选择 nocleanerdirectbytebuf，这就要求 netty 需要对这部分 direct memory 的用量进行自行统计限制。nocleanerdirectbytebuf 的最大可用 direct memory 我们可以通过 -dio.netty.maxdirectmemory 来指定，默认情况下等于 -xx:maxdirectmemorysize 设置的值。\n\nplatformdependent 类中的 direct_memory_counter 字段用于统计在 netty 层面上，所有 nocleanerdirectbytebuf 占用的 direct memory 大小。注意这里并不会统计 cleanerdirectbytebuf 的 direct memory 占用，这部分统计由 jvm 负责。\n\npublic final class platformdependent { \n    // 用于统计 nocleaner 的 directbytebuf 所引用的 native memory 大小\n    private static final atomiclong direct_memory_counter;\n\n    public static bytebuffer allocatedirectnocleaner(int capacity) {\n        // 增加 native memory 用量统计\n        incrementmemorycounter(capacity);\n        try {\n            // 分配 native memory\n            // 初始化 nocleaner 的 directbytebuffer\n            return platformdependent0.allocatedirectnocleaner(capacity);\n        } catch (throwable e) {\n            decrementmemorycounter(capacity);\n            throwexception(e);\n            return null;\n        }\n    \n\n    public static void freedirectnocleaner(bytebuffer buffer) {\n        int capacity = buffer.capacity();\n        // 释放 native memory\n        platformdependent0.freememory(platformdependent0.directbufferaddress(buffer));\n        // 减少 native memory 用量统计\n        decrementmemorycounter(capacity);\n    }  \n}\n\n\nplatformdependent 类是 netty 最底层的一个类，所有内存的分配，释放动作最终都是在该类中执行，因此 direct_memory_counter 字段统计的是全局的 direct memory 大小（netty 层面）。\n\n每一次的内存申请 —— allocatedirectnocleaner ， 都会增加 direct_memory_counter 计数，每一次的内存释放 —— freedirectnocleaner，都会减少 direct_memory_counter 计数。\n\n我们可以通过 platformdependent.useddirectmemory()方法来获取 netty 当前所占用的 direct memory 大小。但如果我们特殊指定了需要使用 cleanerdirectbytebuf ， 比如，将 -dio.netty.maxdirectmemory 参数设置为 0 , 那么这里将会返回 -1 。\n\n    private static void incrementmemorycounter(int capacity) {\n        // 只统计 nocleaner 的 directbytebuf 所引用的 native memory \n        if (direct_memory_counter != null) {\n            long newusedmemory = direct_memory_counter.addandget(capacity);\n            if (newusedmemory > direct_memory_limit) {\n                direct_memory_counter.addandget(-capacity);\n                throw new outofdirectmemoryerror(\"failed to allocate \" + capacity\n                        + \" byte(s) of direct memory (used: \" + (newusedmemory - capacity)\n                        + \", max: \" + direct_memory_limit + ')');\n            }\n        }\n    }\n\n    private static void decrementmemorycounter(int capacity) {\n        if (direct_memory_counter != null) {\n            long usedmemory = direct_memory_counter.addandget(-capacity);\n            assert usedmemory >= 0;\n        }\n    }\n\n    public static long useddirectmemory() {\n        return direct_memory_counter != null ? direct_memory_counter.get() : -1;\n    }\n\n\n除了 platformdependent 这里的全局统计之外，netty 还提供了以 bytebufallocator 为粒度的内存占用统计，统计的维度包括 heap memory 的占用和 direct memory 的占用。\n\npublic final class unpooledbytebufallocator extends abstractbytebufallocator implements bytebufallocatormetricprovider {\n    // 从该 bytebufallocator 分配出去的内存统计\n    private final unpooledbytebufallocatormetric metric = new unpooledbytebufallocatormetric();\n\n    @override\n    public bytebufallocatormetric metric() {\n        return metric;\n    }\n    // 统计 direct memory 的占用\n    void incrementdirect(int amount) {\n        metric.directcounter.add(amount);\n    }\n\n    void decrementdirect(int amount) {\n        metric.directcounter.add(-amount);\n    }\n    // 统计 heap memory 的占用\n    void incrementheap(int amount) {\n        metric.heapcounter.add(amount);\n    }\n\n    void decrementheap(int amount) {\n        metric.heapcounter.add(-amount);\n    }\n\n}\n\n\nnetty 定义的每一个 bytebufallocator 中，都会有一个 bytebufallocatormetric 类型的字段，该类定义两个计数字段：directcounter，heapcounter。 分别用于统计 direct memory 和 heap memory 的占用。\n\n    private static final class unpooledbytebufallocatormetric implements bytebufallocatormetric {\n        final longcounter directcounter = platformdependent.newlongcounter();\n        final longcounter heapcounter = platformdependent.newlongcounter();\n\n        @override\n        public long usedheapmemory() {\n            return heapcounter.value();\n        }\n\n        @override\n        public long useddirectmemory() {\n            return directcounter.value();\n        }\n\n        @override\n        public string tostring() {\n            return stringutil.simpleclassname(this) +\n                    \"(usedheapmemory: \" + usedheapmemory() + \"; useddirectmemory: \" + useddirectmemory() + ')';\n        }\n    }\n\n\n因此从内存占用统计的角度上来说，netty 又会将整个 bytebuf 体系分为 instrumented 和 noinstrumented 两大类，带有 instrumented 前缀的 bytebuf ，无论你是 heap or direct ， cleaner or nocleaner，unsafe or nounsafe 类型的 bytebuf ，netty 都会统计这部分内存占用。\n\n    private static final class instrumentedunpooledunsafenocleanerdirectbytebuf\n            extends unpooledunsafenocleanerdirectbytebuf {\n        instrumentedunpooledunsafenocleanerdirectbytebuf(\n                unpooledbytebufallocator alloc, int initialcapacity, int maxcapacity) {\n            // 构造普通的 unpooledunsafenocleanerdirectbytebuf\n            super(alloc, initialcapacity, maxcapacity);\n        }\n        \n        // 分配，释放 的时候更新 direct memory \n        @override\n        protected bytebuffer allocatedirect(int initialcapacity) {\n            bytebuffer buffer = super.allocatedirect(initialcapacity);\n            ((unpooledbytebufallocator) alloc()).incrementdirect(buffer.capacity());\n            return buffer;\n        }\n\n        @override\n        protected void freedirect(bytebuffer buffer) {\n            int capacity = buffer.capacity();\n            super.freedirect(buffer);\n            ((unpooledbytebufallocator) alloc()).decrementdirect(capacity);\n        }\n    }\n    private static final class instrumentedunpooledunsafedirectbytebuf extends unpooledunsafedirectbytebuf {\n        instrumentedunpooledunsafedirectbytebuf(\n                unpooledbytebufallocator alloc, int initialcapacity, int maxcapacity) {\n            // 构造普通的 unpooledunsafedirectbytebuf\n            super(alloc, initialcapacity, maxcapacity);\n        }\n\n        // 分配，释放 的时候更新 direct memory \n        @override\n        protected bytebuffer allocatedirect(int initialcapacity) {\n            bytebuffer buffer = super.allocatedirect(initialcapacity);\n            ((unpooledbytebufallocator) alloc()).incrementdirect(buffer.capacity());\n            return buffer;\n        }\n\n        @override\n        protected void freedirect(bytebuffer buffer) {\n            int capacity = buffer.capacity();\n            super.freedirect(buffer);\n            ((unpooledbytebufallocator) alloc()).decrementdirect(capacity);\n        }\n    }\n\n\n\n# 8. bytebufallocator\n\n在 netty 中，bytebuf 的创建必须通过 bytebufallocator 进行，不能直接显示地调用 bytebuf 相关的构造函数自行创建。netty 定义了两种类型的 bytebufallocator ：\n\n 1. pooledbytebufallocator 负责池化 bytebuf，这里正是 netty 内存管理的核心，在下一篇文章中，笔者会详细的和大家介绍它。\n 2. unpooledbytebufallocator 负责分配非池化的 bytebuf，创建 bytebuf 的时候临时向 os 申请 native memory ，使用完之后，需要及时的手动调用 release 将 native memory 释放给 os 。\n\n-dio.netty.allocator.type 参数可以让我们自行选择 bytebufallocator 的类型，默认值为 pooled, netty 默认是采用池化的方式来管理 bytebuf 。\n\npublic interface bytebufallocator {\n    // 默认为 pooledbytebufallocator\n    bytebufallocator default = bytebufutil.default_allocator;\n}\n\n\n除了以上两种官方定义的 bytebufallocator 之外，我们还可以根据自己实际业务场景来自行定制 bytebufallocator ， 然后通过第六小节中介绍的 channeloption.allocator 选项，将 bytebufallocator 灵活指定为我们自行定制的实现。\n\n对于 unpooledbytebuf 来说，netty 还专门提供了一个工具类 unpooled，这里定义实现了很多针对 bytebuf 的实用操作，比如，allocate，wrapped，copied 等。这里笔者以 directbytebuf 的创建为例进行说明：\n\npublic final class unpooled {\n\n    private static final bytebufallocator alloc = unpooledbytebufallocator.default;\n\n    public static bytebuf directbuffer() {\n        return alloc.directbuffer();\n    }\n}\n\n\nunpooled 底层依赖了 unpooledbytebufallocator ， 所有对 bytebuf 的创建动作最终都会代理给这个 allocator 。在 directbuffer 的创建过程中，我们可以看到前面介绍的所有类型的 bytebuf。\n\npublic final class unpooledbytebufallocator {\n    @override\n    protected bytebuf newdirectbuffer(int initialcapacity, int maxcapacity) {\n        final bytebuf buf;\n        if (platformdependent.hasunsafe()) {\n            buf = nocleaner ? new instrumentedunpooledunsafenocleanerdirectbytebuf(this, initialcapacity, maxcapacity) :\n                    new instrumentedunpooledunsafedirectbytebuf(this, initialcapacity, maxcapacity);\n        } else {\n            buf = new instrumentedunpooleddirectbytebuf(this, initialcapacity, maxcapacity);\n        }\n        // 是否启动内存泄露探测，如果启动则额外用 leakawarebytebuf 进行包装返回\n        return disableleakdetector ? buf : toleakawarebuffer(buf);\n    }\n}\n\n\n * 首先 netty 创建出来的所有 bytebuf 都是带有 metric 统计的，具体的 bytebuf 类型都会带有 instrumented 前缀。\n * 如果当前 jre 环境支持 unsafe ， 那么后续就会通过 unsafe 的方式来对 bytebuf 进行相关操作（默认），具体的 bytebuf 类型都会带有 unsafe 前缀。\n * 如果我们明确指定了 nocleaner 类型的 directbytebuf（默认），那么创建出来的 bytebuf 类型就会带有 nocleaner 前缀，由于没有 cleaner ，这就要求我们使用完 bytebuf 的时候必须及时地手动进行释放。\n * 如果我们开启了内存泄露探测，那么创建流程的最后，netty 会用一个 leakawarebytebuf 去包装新创建出来的 bytebuf，当这个 bytebuf 被 gc 的时候，netty 会通过相关引用计数来判断是否存在忘记 release 的情况，从而确定出是否发生内存泄露。\n\n\n# 总结\n\n本文笔者从八个角度为大家详细的剖析了 bytebuf 的整体设计，这八个角度分别是：内存区域分布的角度，内存管理的角度，内存访问的角度，内存回收的角度，内存统计 metric 的角度，零拷贝的角度，引用计数的角度，扩容的角度。\n\n到现在为止，我们只是扫清了 netty 内存管理外围的一些障碍，那么下一篇文章，笔者将带大家深入到内存管理的核心，彻底让大家弄懂 netty 的内存管理机制。好了，本文的内容就到这里，我们下篇文章见~~~",charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"指南",frontmatter:{title:"指南",date:"2024-09-15T17:31:05.000Z",permalink:"/pages/252196/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80/01.%E6%8C%87%E5%8D%97.html",relativePath:"Redis 系统设计/01.一、前言/01.指南.md",key:"v-429c1b42",path:"/pages/252196/",headers:[{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:2},{level:2,title:"阅读方法",slug:"阅读方法",normalizedTitle:"阅读方法",charIndex:157},{level:3,title:"伪码蓝图",slug:"伪码蓝图",normalizedTitle:"伪码蓝图",charIndex:166},{level:3,title:"基础",slug:"基础",normalizedTitle:"基础",charIndex:137},{level:3,title:"主线",slug:"主线",normalizedTitle:"主线",charIndex:232},{level:3,title:"支线",slug:"支线",normalizedTitle:"支线",charIndex:777},{level:2,title:"学习资料推荐",slug:"学习资料推荐",normalizedTitle:"学习资料推荐",charIndex:1200}],headersStr:"前置知识 阅读方法 伪码蓝图 基础 主线 支线 学习资料推荐",content:"# 前置知识\n\n * 常用数据结构：数组、链表、哈希表、跳表\n * 网络协议：TCP 协议\n * 网络 IO 模型：IO 多路复用、非阻塞 IO、Reactor 网络模型\n * 操作系统：写时复制（Copy On Write）、常见系统调用、磁盘 IO 机制\n * C 语言基础：循环、分支、结构体、指针\n\n\n# 阅读方法\n\n\n# 伪码蓝图\n\n特别重要！务必反复观看\n\n\n# 基础\n\n这些基础模块就相当于一座大厦的地基，地基打好了，才能做到高楼耸立。\n\n\n\n\n# 主线\n\n掌握了数据结构模块之后，这时我们的重点就需要放在「核心主线」上来了。\n\n那么在读 Redis 源码时，什么才是它的核心主线呢？\n\n这里我分享一个非常好用的技巧，就是根据「Redis 究竟是怎么处理客户端发来的命令的？」为主线来梳理。\n\n举个例子，当我们在执行 SET testkey testval EX 60 这样一条命令时，就需要搞清楚 Redis 是怎么执行这条命令的\n\n也就是要明确，Redis 从收到客户端请求，到把数据存到 Redis 中、设置过期时间，最后把响应结果返回给客户端，整个过程的每一个环节，到底是如何处理的。\n\n有了这条主线，我们就有了非常明确的目标，而且沿着这条主线去读代码，我们还可以很清晰地把多个模块「串联」起来。比如从前面的例子中，我们会看到一条命令的执行，主要包含了这样几个阶段\n\n * Redis Server 初始化：加载配置、监听端口、注册连接建立事件、启动事件循环\n * 接收、解析客户端请求：初始化 client、注册读事件、读客户端\n * 处理具体的命令：找到对应的命令函数、执行命令\n * 返回响应给客户端：写客户端缓冲区、注册写事件、写客户端 socket\n\n\n\n沿着这条主线去读代码，我们就可以掌握一条命令的执行全过程\n\n\n# 支线\n\n不过，在攻打主线的过程中，我们肯定还会遇到各种「支线」逻辑，比如数据过期、替换淘汰、持久化、主从复制等。\n\n其实，在阅读主线逻辑的时候，我们并不需要去重点关注这些支线，而当整个主线逻辑「清晰」起来之后，我们再去读这些支线模块，就会容易很多了。\n\n这时，我们就可以从这些支线中，选取下一个「目标」，带着这个目标去阅读，比如说：\n\n * 过期策略是怎么实现的？（expire.c、lazyfree.c）\n * 淘汰策略是如何实现的？（evict.c）\n * 持久化 RDB、AOF 是怎么做的？（rdb.c、aof.c）\n * 主从复制是怎么做的？（replication.c）\n * 哨兵如何完成故障自动切换？（sentinel.c）\n * 分片逻辑如何实现？（cluster.c）\n * …\n\n有了新的支线目标后，我们依旧可以采用前面提到的「先整体后细节」的思路阅读相关模块，这样下来，整个项目的每个模块，就可以被「逐一击破」了\n\n\n# 学习资料推荐\n\n * 极客时间：Redis源码剖析与实战\n * Redis设计与实现\n * https://book-redis-design.netlify.app/\n * Github：redis 源码\n * Redis Basics & Notes - Yves Wiki (imzye.com)\n * 栏目：服务端技术 - 铁蕾的个人博客 (zhangtielei.com)\n * Category: Redis Source Code Analysis | Johnson Lin (linjiangxiong.com)\n * Redis源码解析 (youzan.com)\n * JasonLai256/the-little-redis-book (github.com)\n * huangzworks/redis-3.0-annotated: 带有详细注释的 Redis 3.0 代码（annotated Redis 3.0 source code）。 (github.com)\n * 通用业务场景_云数据库 Redis 版(Redis)-阿里云帮助中心 (aliyun.com)\n * 得分最高的 'redis' 问题 - Stack Overflow --- Highest scored 'redis' questions - Stack Overflow",normalizedContent:"# 前置知识\n\n * 常用数据结构：数组、链表、哈希表、跳表\n * 网络协议：tcp 协议\n * 网络 io 模型：io 多路复用、非阻塞 io、reactor 网络模型\n * 操作系统：写时复制（copy on write）、常见系统调用、磁盘 io 机制\n * c 语言基础：循环、分支、结构体、指针\n\n\n# 阅读方法\n\n\n# 伪码蓝图\n\n特别重要！务必反复观看\n\n\n# 基础\n\n这些基础模块就相当于一座大厦的地基，地基打好了，才能做到高楼耸立。\n\n\n\n\n# 主线\n\n掌握了数据结构模块之后，这时我们的重点就需要放在「核心主线」上来了。\n\n那么在读 redis 源码时，什么才是它的核心主线呢？\n\n这里我分享一个非常好用的技巧，就是根据「redis 究竟是怎么处理客户端发来的命令的？」为主线来梳理。\n\n举个例子，当我们在执行 set testkey testval ex 60 这样一条命令时，就需要搞清楚 redis 是怎么执行这条命令的\n\n也就是要明确，redis 从收到客户端请求，到把数据存到 redis 中、设置过期时间，最后把响应结果返回给客户端，整个过程的每一个环节，到底是如何处理的。\n\n有了这条主线，我们就有了非常明确的目标，而且沿着这条主线去读代码，我们还可以很清晰地把多个模块「串联」起来。比如从前面的例子中，我们会看到一条命令的执行，主要包含了这样几个阶段\n\n * redis server 初始化：加载配置、监听端口、注册连接建立事件、启动事件循环\n * 接收、解析客户端请求：初始化 client、注册读事件、读客户端\n * 处理具体的命令：找到对应的命令函数、执行命令\n * 返回响应给客户端：写客户端缓冲区、注册写事件、写客户端 socket\n\n\n\n沿着这条主线去读代码，我们就可以掌握一条命令的执行全过程\n\n\n# 支线\n\n不过，在攻打主线的过程中，我们肯定还会遇到各种「支线」逻辑，比如数据过期、替换淘汰、持久化、主从复制等。\n\n其实，在阅读主线逻辑的时候，我们并不需要去重点关注这些支线，而当整个主线逻辑「清晰」起来之后，我们再去读这些支线模块，就会容易很多了。\n\n这时，我们就可以从这些支线中，选取下一个「目标」，带着这个目标去阅读，比如说：\n\n * 过期策略是怎么实现的？（expire.c、lazyfree.c）\n * 淘汰策略是如何实现的？（evict.c）\n * 持久化 rdb、aof 是怎么做的？（rdb.c、aof.c）\n * 主从复制是怎么做的？（replication.c）\n * 哨兵如何完成故障自动切换？（sentinel.c）\n * 分片逻辑如何实现？（cluster.c）\n * …\n\n有了新的支线目标后，我们依旧可以采用前面提到的「先整体后细节」的思路阅读相关模块，这样下来，整个项目的每个模块，就可以被「逐一击破」了\n\n\n# 学习资料推荐\n\n * 极客时间：redis源码剖析与实战\n * redis设计与实现\n * https://book-redis-design.netlify.app/\n * github：redis 源码\n * redis basics & notes - yves wiki (imzye.com)\n * 栏目：服务端技术 - 铁蕾的个人博客 (zhangtielei.com)\n * category: redis source code analysis | johnson lin (linjiangxiong.com)\n * redis源码解析 (youzan.com)\n * jasonlai256/the-little-redis-book (github.com)\n * huangzworks/redis-3.0-annotated: 带有详细注释的 redis 3.0 代码（annotated redis 3.0 source code）。 (github.com)\n * 通用业务场景_云数据库 redis 版(redis)-阿里云帮助中心 (aliyun.com)\n * 得分最高的 'redis' 问题 - stack overflow --- highest scored 'redis' questions - stack overflow",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Redis 伪码蓝图【必看】",frontmatter:{title:"Redis 伪码蓝图【必看】",date:"2024-09-16T01:33:42.000Z",permalink:"/pages/69fbd7/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80/05.Redis%20%E4%BC%AA%E7%A0%81%E8%93%9D%E5%9B%BE%E3%80%90%E5%BF%85%E7%9C%8B%E3%80%91.html",relativePath:"Redis 系统设计/01.一、前言/05.Redis 伪码蓝图【必看】.md",key:"v-d01770aa",path:"/pages/69fbd7/",headersStr:null,content:'请点击下方窗口右下角进行全屏预览\n\n# Redis 启动流程\nfunction startRedis():\n    # 1. Redis Server 初始化\n    loadConfig()                         # 加载 redis.conf 配置文件\n    createServerSocket()                 # 创建 TCP 监听端口，准备接受客户端连接\n    initEventLoop()                      # 初始化事件循环（epoll/kqueue）\n\n    # 初始化支线功能\n    initPersistence()                    # 初始化 RDB 和 AOF 持久化\n    initReplication()                    # 初始化主从复制机制\n    initSentinel()                       # 如果是哨兵模式，初始化哨兵机制\n    initCluster()                        # 如果是集群模式，初始化集群路由和分片\n\n    # 启动主事件循环\n    eventLoop()\n\n# 事件循环，负责处理所有事件\nfunction eventLoop():\n    while server is running:\n        processTimers()                  # 处理定时任务（如过期 key、AOF 持久化等）\n        acceptClientConnections()        # 接受新的客户端连接\n        processClientRequests()          # 处理客户端命令请求\n        handleReplication()              # 处理主从复制数据同步\n        handleSentinel()                 # 处理哨兵监控和故障切换\n        handleCluster()                  # 集群模式下，处理分片路由和数据迁移\n\n# 处理定时任务，如过期 key、持久化\nfunction processTimers():\n    expireKeysIfNeeded()                 # 检查并删除过期的键\n    handleAOFAndRDBPersistence()         # 根据策略触发 AOF 和 RDB 持久化\n    evictKeysIfNeeded()                  # 如果内存超出上限，触发淘汰策略\n\n# 处理新客户端连接\nfunction acceptClientConnections():\n    when new client connects:\n        client = createClient()          # 创建客户端对象\n        registerReadEvent(client)        # 注册读事件，监听客户端请求\n\n# 处理客户端命令请求\nfunction processClientRequests():\n    for each client in server.clients:\n        if client has data to read:\n            readDataFromClient(client)   # 从 socket 中读取请求数据\n            if data is valid:\n                command = parseCommand(client)  # 解析 Redis 命令\n                executeCommand(client, command) # 执行命令\n\n# 执行 Redis 命令\nfunction executeCommand(client, command):\n    if command is valid:\n        cmdFunction = lookupCommand(command)  # 查找命令函数\n        result = cmdFunction(client)          # 执行命令\n        addToClientOutputBuffer(client, result) # 将结果写入输出缓冲区\n        replicateCommandToSlaves(command)     # 同步命令到从服务器（主从复制）\n\n        # 在命令执行过程中，可能触发支线操作\n        checkKeyExpiration(client, command)  # 检查 key 是否过期，进行惰性删除\n        handlePersistence(command)           # 根据配置，触发 AOF 或 RDB 持久化\n        evictKeysIfNeeded()                  # 当内存不足时，触发淘汰策略\n\n    else:\n        sendError(client, "Invalid command") # 返回错误消息\n\n# 返回响应给客户端\nfunction addToClientOutputBuffer(client, result):\n    client.outputBuffer.append(result)       # 将命令执行结果添加到输出缓冲区\n    registerWriteEvent(client)               # 注册写事件，准备发送给客户端\n\n# 发送响应\nfunction sendResponseToClient(client):\n    writeDataToSocket(client, client.outputBuffer)  # 将数据发送给客户端\n    if write success:\n        clearClientOutputBuffer(client)      # 清空缓冲区\n    else:\n        handleSocketError(client)            # 处理 socket 错误\n\n# 支线：检查 key 过期，过期策略\nfunction checkKeyExpiration(client, command):\n    key = getKeyFromCommand(command)\n    if isKeyExpired(key):\n        deleteKey(client.db, key)            # 惰性删除过期的 key\n\nfunction expireKeysIfNeeded():\n    for each db in server.databases:\n        for each key in db.keys:\n            if isKeyExpired(key):\n                deleteKey(db, key)           # 主动删除过期的 key\n\n# 支线：持久化，处理 RDB 和 AOF 逻辑\nfunction handlePersistence(command):\n    if AOF is enabled:\n        writeCommandToAOF(command)           # 将命令写入 AOF 文件\n    if RDB snapshot is needed:\n        saveRDBSnapshot()                    # 触发 RDB 快照生成\n\nfunction handleAOFAndRDBPersistence():\n    if timeToSaveRDB():\n        saveRDB()                            # 持久化 RDB 快照\n    if timeToRewriteAOF():\n        rewriteAOF()                         # 触发 AOF 重写以压缩日志\n\n# 支线：淘汰策略，LRU、LFU\nfunction evictKeysIfNeeded():\n    while memoryUsageExceedsLimit():\n        candidates = sampleKeys()            # 随机采样 key\n        keyToEvict = findLRUOrLFUKey(candidates) # 根据 LRU/LFU 策略选择淘汰 key\n        deleteKey(db, keyToEvict)            # 删除 key 以释放内存\n\n# 支线：主从复制，主服务器将命令同步给从服务器\nfunction replicateCommandToSlaves(command):\n    if server.isMaster():\n        for each slave in server.slaves:\n            sendCommandToSlave(slave, command)  # 复制命令到从服务器\n\nfunction handleReplication():\n    if server.isMaster():\n        sendReplicationDataToSlaves()          # 主服务器向从服务器同步数据\n    if server.isSlave():\n        syncWithMaster()                       # 从服务器与主服务器同步数据\n\n# 支线：哨兵监控与故障转移\nfunction handleSentinel():\n    if server.isSentinel():\n        monitorMasterStatus()                  # 哨兵模式，监控主服务器状态\n        if masterIsDown():\n            performFailover()                  # 主服务器故障时，触发故障转移\n\nfunction performFailover():\n    newMaster = selectNewMaster()              # 选举新的主服务器\n    promoteSlaveToMaster(newMaster)            # 提升从服务器为主服务器\n    notifyOtherSlavesAndClients(newMaster)     # 通知其他从服务器和客户端\n\n# 支线：集群模式下的分片与数据路由\nfunction handleCluster():\n    if server.isCluster():\n        for each command in client.commands:\n            keyHashSlot = computeKeyHashSlot(command.key) # 计算 key 的哈希槽\n            if hashSlotIsLocal(keyHashSlot):\n                executeCommandLocally(command)   # 如果哈希槽属于本节点，执行命令\n            else:\n                forwardCommandToCorrectNode(command) # 将命令转发到正确的节点\n\nfunction migrateKeysDuringRebalance():\n    if clusterNeedsRebalancing():\n        for each slot in migratingSlots:\n            migrateKeys(slot, targetNode)      # 将哈希槽的数据迁移到目标节点\n\n',normalizedContent:'请点击下方窗口右下角进行全屏预览\n\n# redis 启动流程\nfunction startredis():\n    # 1. redis server 初始化\n    loadconfig()                         # 加载 redis.conf 配置文件\n    createserversocket()                 # 创建 tcp 监听端口，准备接受客户端连接\n    initeventloop()                      # 初始化事件循环（epoll/kqueue）\n\n    # 初始化支线功能\n    initpersistence()                    # 初始化 rdb 和 aof 持久化\n    initreplication()                    # 初始化主从复制机制\n    initsentinel()                       # 如果是哨兵模式，初始化哨兵机制\n    initcluster()                        # 如果是集群模式，初始化集群路由和分片\n\n    # 启动主事件循环\n    eventloop()\n\n# 事件循环，负责处理所有事件\nfunction eventloop():\n    while server is running:\n        processtimers()                  # 处理定时任务（如过期 key、aof 持久化等）\n        acceptclientconnections()        # 接受新的客户端连接\n        processclientrequests()          # 处理客户端命令请求\n        handlereplication()              # 处理主从复制数据同步\n        handlesentinel()                 # 处理哨兵监控和故障切换\n        handlecluster()                  # 集群模式下，处理分片路由和数据迁移\n\n# 处理定时任务，如过期 key、持久化\nfunction processtimers():\n    expirekeysifneeded()                 # 检查并删除过期的键\n    handleaofandrdbpersistence()         # 根据策略触发 aof 和 rdb 持久化\n    evictkeysifneeded()                  # 如果内存超出上限，触发淘汰策略\n\n# 处理新客户端连接\nfunction acceptclientconnections():\n    when new client connects:\n        client = createclient()          # 创建客户端对象\n        registerreadevent(client)        # 注册读事件，监听客户端请求\n\n# 处理客户端命令请求\nfunction processclientrequests():\n    for each client in server.clients:\n        if client has data to read:\n            readdatafromclient(client)   # 从 socket 中读取请求数据\n            if data is valid:\n                command = parsecommand(client)  # 解析 redis 命令\n                executecommand(client, command) # 执行命令\n\n# 执行 redis 命令\nfunction executecommand(client, command):\n    if command is valid:\n        cmdfunction = lookupcommand(command)  # 查找命令函数\n        result = cmdfunction(client)          # 执行命令\n        addtoclientoutputbuffer(client, result) # 将结果写入输出缓冲区\n        replicatecommandtoslaves(command)     # 同步命令到从服务器（主从复制）\n\n        # 在命令执行过程中，可能触发支线操作\n        checkkeyexpiration(client, command)  # 检查 key 是否过期，进行惰性删除\n        handlepersistence(command)           # 根据配置，触发 aof 或 rdb 持久化\n        evictkeysifneeded()                  # 当内存不足时，触发淘汰策略\n\n    else:\n        senderror(client, "invalid command") # 返回错误消息\n\n# 返回响应给客户端\nfunction addtoclientoutputbuffer(client, result):\n    client.outputbuffer.append(result)       # 将命令执行结果添加到输出缓冲区\n    registerwriteevent(client)               # 注册写事件，准备发送给客户端\n\n# 发送响应\nfunction sendresponsetoclient(client):\n    writedatatosocket(client, client.outputbuffer)  # 将数据发送给客户端\n    if write success:\n        clearclientoutputbuffer(client)      # 清空缓冲区\n    else:\n        handlesocketerror(client)            # 处理 socket 错误\n\n# 支线：检查 key 过期，过期策略\nfunction checkkeyexpiration(client, command):\n    key = getkeyfromcommand(command)\n    if iskeyexpired(key):\n        deletekey(client.db, key)            # 惰性删除过期的 key\n\nfunction expirekeysifneeded():\n    for each db in server.databases:\n        for each key in db.keys:\n            if iskeyexpired(key):\n                deletekey(db, key)           # 主动删除过期的 key\n\n# 支线：持久化，处理 rdb 和 aof 逻辑\nfunction handlepersistence(command):\n    if aof is enabled:\n        writecommandtoaof(command)           # 将命令写入 aof 文件\n    if rdb snapshot is needed:\n        saverdbsnapshot()                    # 触发 rdb 快照生成\n\nfunction handleaofandrdbpersistence():\n    if timetosaverdb():\n        saverdb()                            # 持久化 rdb 快照\n    if timetorewriteaof():\n        rewriteaof()                         # 触发 aof 重写以压缩日志\n\n# 支线：淘汰策略，lru、lfu\nfunction evictkeysifneeded():\n    while memoryusageexceedslimit():\n        candidates = samplekeys()            # 随机采样 key\n        keytoevict = findlruorlfukey(candidates) # 根据 lru/lfu 策略选择淘汰 key\n        deletekey(db, keytoevict)            # 删除 key 以释放内存\n\n# 支线：主从复制，主服务器将命令同步给从服务器\nfunction replicatecommandtoslaves(command):\n    if server.ismaster():\n        for each slave in server.slaves:\n            sendcommandtoslave(slave, command)  # 复制命令到从服务器\n\nfunction handlereplication():\n    if server.ismaster():\n        sendreplicationdatatoslaves()          # 主服务器向从服务器同步数据\n    if server.isslave():\n        syncwithmaster()                       # 从服务器与主服务器同步数据\n\n# 支线：哨兵监控与故障转移\nfunction handlesentinel():\n    if server.issentinel():\n        monitormasterstatus()                  # 哨兵模式，监控主服务器状态\n        if masterisdown():\n            performfailover()                  # 主服务器故障时，触发故障转移\n\nfunction performfailover():\n    newmaster = selectnewmaster()              # 选举新的主服务器\n    promoteslavetomaster(newmaster)            # 提升从服务器为主服务器\n    notifyotherslavesandclients(newmaster)     # 通知其他从服务器和客户端\n\n# 支线：集群模式下的分片与数据路由\nfunction handlecluster():\n    if server.iscluster():\n        for each command in client.commands:\n            keyhashslot = computekeyhashslot(command.key) # 计算 key 的哈希槽\n            if hashslotislocal(keyhashslot):\n                executecommandlocally(command)   # 如果哈希槽属于本节点，执行命令\n            else:\n                forwardcommandtocorrectnode(command) # 将命令转发到正确的节点\n\nfunction migratekeysduringrebalance():\n    if clusterneedsrebalancing():\n        for each slot in migratingslots:\n            migratekeys(slot, targetnode)      # 将哈希槽的数据迁移到目标节点\n\n',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"String 设计与实现",frontmatter:{title:"String 设计与实现",date:"2024-09-16T02:46:18.000Z",permalink:"/pages/bdae41/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/02.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/01.String%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Redis 系统设计/02.二、基础知识/01.String 设计与实现.md",key:"v-4f326cac",path:"/pages/bdae41/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:233},{level:2,title:"SDS 的定义",slug:"sds-的定义",normalizedTitle:"sds 的定义",charIndex:537},{level:2,title:"SDS  与 C 字符串的区别",slug:"sds-与-c-字符串的区别",normalizedTitle:"sds  与 c 字符串的区别",charIndex:null},{level:3,title:"常数复杂度获取字符串长度",slug:"常数复杂度获取字符串长度",normalizedTitle:"常数复杂度获取字符串长度",charIndex:1300},{level:3,title:"杜绝缓冲区溢出",slug:"杜绝缓冲区溢出",normalizedTitle:"杜绝缓冲区溢出",charIndex:1511},{level:3,title:"减少修改字符串时带来的内存重分配次数",slug:"减少修改字符串时带来的内存重分配次数",normalizedTitle:"减少修改字符串时带来的内存重分配次数",charIndex:1825},{level:4,title:"空间预分配",slug:"空间预分配",normalizedTitle:"空间预分配",charIndex:98},{level:4,title:"惰性空间释放",slug:"惰性空间释放",normalizedTitle:"惰性空间释放",charIndex:2338},{level:3,title:"二进制安全",slug:"二进制安全",normalizedTitle:"二进制安全",charIndex:138},{level:3,title:"兼容部分 C 字符串函数",slug:"兼容部分-c-字符串函数",normalizedTitle:"兼容部分 c 字符串函数",charIndex:3427},{level:2,title:"SDS 的内存友好设计",slug:"sds-的内存友好设计",normalizedTitle:"sds 的内存友好设计",charIndex:3447},{level:4,title:"redisObject 结构体与位域定义方法",slug:"redisobject-结构体与位域定义方法",normalizedTitle:"redisobject 结构体与位域定义方法",charIndex:3943},{level:4,title:"嵌入式字符串",slug:"嵌入式字符串",normalizedTitle:"嵌入式字符串",charIndex:201},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:8863},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:8964}],headersStr:"前言 SDS 的定义 SDS  与 C 字符串的区别 常数复杂度获取字符串长度 杜绝缓冲区溢出 减少修改字符串时带来的内存重分配次数 空间预分配 惰性空间释放 二进制安全 兼容部分 C 字符串函数 SDS 的内存友好设计 redisObject 结构体与位域定义方法 嵌入式字符串 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. 为什么C语言的字符串在性能和安全性上不足以满足Redis需求？\n 2. SDS如何将获取字符串长度的操作从O(N)优化为O(1)？\n 3. 高并发下，SDS的“空间预分配”如何提升频繁字符串操作的性能？\n 4. Redis为什么需要SDS的二进制安全？传统字符串处理方法有什么局限？\n 5. 为什么SDS缩短时不立即释放多余内存？何时需要手动释放？\n 6. SDS的嵌入式字符串如何减少内存碎片？适用于多大长度的字符串？\n\n\n# 前言\n\n一个优雅的字符串设计，需要尽量满足以下三个要求：\n\n * 能支持丰富的 API 操作，比如字符串追加、拷贝、比较、获取长度等\n * 能保存任意的二进制数据，比如图片等\n * 能尽可能地节省内存开销\n\nRedis 设计了简单动态字符串（Simple Dynamic String，SDS）的结构，用来表示字符串。\n\n相比于 C 语言中的字符串实现，SDS 更适合 Redis 的特性，我会在本文一一叙述\n\nSDS 使用场景\n\n * 数据库中的字符串值\n * AOF 模块中的 AOF 缓冲区\n * 客户端状态中的输入缓冲区\n * 等等等等\n\nRedis 很多用到了字符串的地方都是使用 SDS\n\n\n# SDS 的定义\n\nstruct sdshdr{\n\t//记录buf数组中已使用的字节数 等于SDS所保存的字符串的长度\n\tint len;\n\t// 记录buf数组中未使用的字节数\n\tint free;\n\t//字节数组 用于保存字符串\n\tchar buf[];\n}\n\n\n\n\n笔记\n\nSDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作，都是由SDS函数自动完成的，所以这个空字符对于 SDS 的使用者来说是完全透明的。遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。\n\n\n# SDS 与 C 字符串的区别\n\n\n\nC 语言字符串使用长度为 N+1 的字符串数组来存放长度为 N 的字符串，并且字符数组的最后一个元素总是空字符串 \\0\n\n这并不能满足 Redis 对字符串在效率、安全性以及功能方面的要求\n\n接下来说明 SDS 比 C 字符串更适用于 Redis 的原因\n\n特性                  C 字符串            SDS (简单动态字符串)\n获取字符串长度的复杂度         O(N)             O(1)\nAPI 安全性             不安全，可能导致缓冲区溢出    安全，不会导致缓冲区溢出\n修改字符串长度时的内存重分配次数    修改N次，执行N次内存重分配   修改N次，最多执行N次内存重分配\n数据类型支持              只能保存文本数据         可以保存文本或二进制数据\n<string.h>库函数使用情况   可使用所有函数          可使用部分函数\n\n\n# 常数复杂度获取字符串长度\n\n * C 字符串并不记录自身的长度信息，所以为了获取一个C字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N)\n\n * SDS 中有 len 属性来存储字符串长度，使得 STRLEN 命令的复杂度仅为 O(1)\n\nlen 属性的设置和更新是由 SDS 的 API 在执行的时候自动完成的，我们无需手动设置\n\n\n\n\n# 杜绝缓冲区溢出\n\nchar *strcat(char *dest，const char *src);\n\n\n * C 字符串不记录自身长度，所以当调用 strcat 函数时，系统假定用户为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立，就会产生缓冲区溢出。需要用户对字符串进行扩容。\n\n * SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：SDS API需要对SDS进行修改的时候，API会先检查SDS的空间是否满足修改所需的要求，如果不满足的话，API会自动将SDS的空间进行扩容到指定的大小，然后再次执行实际的修改操作，所以SDS既不需要手动扩容，也不会缓冲区溢出\n\n\n# 减少修改字符串时带来的内存重分配次数\n\n因为 C 字符串的长度和底层数组的长度之间存在关联性，所以每次改变字符串长度的时候，程序总要对保存这个C字符串的数组进行一次内存重分配操作\n\n> 内存重分配：\n> \n>  * 如果程序执行的是增长字符串的操作，比如拼接操作append，那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小，如果忘了这一步就会产生缓冲区溢出\n>  * 如果程序执行的是缩短字符串的操作，比如截断操作trim，那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间，如果忘了这一步就会产生内存泄漏\n\n注意\n\n一般情况下，如果修改字符串长度的情况不常见，那么一般是可以接受内存重分配的。但是Redis作为数据库，为了极致化性能体验，就会尽量避免内存重分配\n\n所以SDS 通过使用 **free 属性 **记录未使用字节，从而解除了字符串长度和底层数组长度之间的关联：\n\n * 在SDS中，buf 数组的长度不一定就是字符数量加一\n * 并且数组里面可以包含未使用的字节\n\n那 SDS 是如何利用 free 属性去避免内存重分配的？\n\nSDS实现了「空间预分配」和「惰性空间释放」两种优化策略\n\n# 空间预分配\n\n「空间预分配」用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间\n\n额外分配的未使用空间数量有以下规则\n\n * 当对 SDS 进行修改之后，SDS 的 len 属性 小于 1 MB，那么程序分配和 len属性同样大小的未使用空间，这是 SDS len属性的值将和 free属性的值相同\n * 当对 SDS 进行修改之后，SDS 的 len 属性 小于等于 1 MB，那么程序会分配 1MB的使用空间\n\n通过「空间预分配」策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数\n\n# 惰性空间释放\n\n「惰性空间释放」用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用 举个例子，SDS 的 API：sdstrim函数接受一个 SDS 和一个 C 字符串作为参数，移除 SDS 中所有在C字符串中出现过的字符\n\n\n\n注意，执行 sdstrim 之后的 SDS 并没有释放多出来的 8 字节空间，而是将这 8 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场\n\n所以也就避免了缩短字符串时所需的内存重分配操作，并未将来可能有的操作提供了优化\n\n同时，SDS 也提供了相应的 API，让我们可以在有需要的时候释放 SDS 的未使用空间，不用担心惰性空间释放策略会造成内存浪费\n\n思考\n\n什么时候 Redis 会去主动释放 SDS 中的未使用空间？\n\n\n# 二进制安全\n\nC 字符串中的字符必须符合某种编码，比如ASCII，并且除了字符串的末尾之外，字符串里面不能包含空字符 \\0，否则最先被程序读入的空字符将被误认为是字符串结尾，这些限制使得C字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据\n\n为了确保 Redis 可以适用于各种不同的使用场景（文本数据或者二进制数据），SDS 的 API 都是二进制安全的，所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 buf 数组里的数据程序，同时不会对其中的数据做任何限制、过滤、或者假设，数据在写入时是什么样的，它被读取时就是什么样\n\n\n\n\n# 兼容部分 C 字符串函数\n\n略\n\n\n# SDS 的内存友好设计\n\n在 Redis 3.X 之后，SDS 设计了不同类型的结构头，包括 sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。这些不同类型的结构头可以适配不同大小的字符串，从而避免了内存浪费\n\nstruct __attribute__ ((__packed__)) sdshdr8 {\n    uint8_t len;  // 已使用的字符串长度（8位，最大255字节）\n    uint8_t alloc; // 已分配的空间（8位，最大255字节）\n    unsigned char flags; // 标志，用于标识使用的是哪种sdshdr类型\n    char buf[];   // 字符串内容\n};\n\n\n而且，Redis 在保存较小字符串时，其实还使用了嵌入式字符串的设计方法。这种方法避免了给字符串分配额外的空间，而是可以让字符串直接保存在 Redis 的基本数据对象结构体中。\n\n所以这也就是说，要想理解嵌入式字符串的设计与实现，我们就需要先来了解下，Redis 使用的基本数据对象结构体 redisObject 是什么样的\n\n# redisObject 结构体与位域定义方法\n\nredisObject 结构体是在 server.h 文件中定义的，主要功能是用来保存「键值对中的值」\n\n这个结构一共定义了「4 个元数据」和 「一个指针」\n\n * type：redisObject 的数据类型，是应用程序在 Redis 中保存的数据类型，包括 String、List、Hash 等。\n * encoding：redisObject 的编码类型，是 Redis 内部实现各种数据类型所用的数据结构。\n * lru：redisObject 的 LRU 时间。\n * refcount：redisObject 的引用计数。\n * ptr：指向值的指针。\n\n下面的代码展示了 redisObject 结构体的定义：\n\ntypedef struct redisObject {\n    unsigned type:4; //redisObject的数据类型，4个bits\n    unsigned encoding:4; //redisObject的编码类型，4个bits\n    unsigned lru:LRU_BITS;  //redisObject的LRU时间，LRU_BITS为24个bits\n    int refcount; //redisObject的引用计数，4个字节\n    void *ptr; //指向值的指针，8个字节\n} robj;\n\n\n从代码中我们可以看到，在 type、encoding 和 lru 三个变量后面都有一个冒号，并紧跟着一个数值，表示该元数据占用的比特数。其中，type 和 encoding 分别占 4bits。而 lru 占用的比特数，是由 server.h 中的宏定义 LRU_BITS 决定的，它的默认值是 24bits，如下所示：\n\n#define LRU_BITS 24\n\n\n而这里我想让你学习掌握的，就是这种变量后使用冒号和数值的定义方法。这实际上是 C 语言中的位域定义方法，可以用来有效地节省内存开销。\n\n这种方法比较适用的场景是，当一个变量占用不了一个数据类型的所有 bits 时，就可以使用位域定义方法，把一个数据类型中的 bits，划分成多个位域，每个位域占一定的 bit 数。这样一来，一个数据类型的所有 bits 就可以定义多个变量了，从而也就有效节省了内存开销。\n\n此外，你可能还会发现，对于 type、encoding 和 lru 三个变量来说，它们的数据类型都是 unsigned。已知一个 unsigned 类型是 4 字节，但这三个变量，是分别占用了一个 unsigned 类型 4 字节中的 4bits、4bits 和 24bits。因此，相较于三个变量，每个变量用一个 4 字节的 unsigned 类型定义来说，使用位域定义方法可以让三个变量只用 4 字节，最后就能节省 8 字节的开销。\n\n所以，当你在设计开发内存敏感型的软件时，就可以把这种位域定义方法使用起来。\n\n好，了解了 redisObject 结构体和它使用的位域定义方法以后，我们再来看嵌入式字符串是如何实现的。\n\n# 嵌入式字符串\n\n前面我说过，SDS 在保存比较小的字符串时，会使用嵌入式字符串的设计方法，将字符串直接保存在 redisObject 结构体中。然后在 redisObject 结构体中，存在一个指向值的指针 ptr，而一般来说，这个 ptr 指针会指向值的数据结构。\n\n这里我们就以创建一个 String 类型的值为例，Redis 会调用 createStringObject 函数，来创建相应的 redisObject，而这个 redisObject 中的 ptr 指针，就会指向 SDS 数据结构，如下图所示。\n\n\n\n在 Redis 源码中，createStringObject 函数会根据要创建的字符串的长度，决定具体调用哪个函数来完成创建。\n\n那么针对这个 createStringObject 函数来说，它的参数是字符串 ptr 和字符串长度 len。当 len 的长度大于 OBJ_ENCODING_EMBSTR_SIZE_LIMIT 这个宏定义时，createStringObject 函数会调用 createRawStringObject 函数，否则就调用 createEmbeddedStringObject 函数。\n\n而在我们分析的 Redis 5.0.8 源码版本中，这个 OBJ_ENCODING_EMBSTR_SIZE_LIMIT 默认定义为 44 字节。\n\n这部分代码如下所示：\n\n#define OBJ_ENCODING_EMBSTR_SIZE_LIMIT 44\nrobj *createStringObject(const char *ptr, size_t len) {\n    //创建嵌入式字符串，字符串长度小于等于44字节\n    if (len <= OBJ_ENCODING_EMBSTR_SIZE_LIMIT)\n        return createEmbeddedStringObject(ptr,len);\n    //创建普通字符串，字符串长度大于44字节\n    else\n        return createRawStringObject(ptr,len);\n}\n\n\n现在，我们就来分析一下 createStringObject 函数的源码实现，以此了解大于 44 字节的普通字符串和小于等于 44 字节的嵌入式字符串分别是如何创建的。\n\n首先，对于 createRawStringObject 函数 来说，它在创建 String 类型的值的时候，会调用 createObject 函数。\n\n> 补充：createObject 函数主要是用来创建 Redis 的数据对象的。因为 Redis 的数据对象有很多类型，比如 String、List、Hash 等，所以在 createObject 函数的两个参数中，有一个就是用来表示所要创建的数据对象类型，而另一个是指向数据对象的指针。\n\n然后，createRawStringObject 函数在调用 createObject 函数时，会传递 OBJ_STRING 类型，表示要创建 String 类型的对象，以及传递指向 SDS 结构的指针，如以下代码所示。这里需要注意的是，指向 SDS 结构的指针是由 sdsnewlen 函数返回的，而 sdsnewlen 函数正是用来创建 SDS 结构的。\n\nrobj *createRawStringObject(const char *ptr, size_t len) {\n    return createObject(OBJ_STRING, sdsnewlen(ptr,len));\n}\n\n\n最后，我们再来进一步看下 createObject 函数。这个函数会把参数中传入的、指向 SDS 结构体的指针直接赋值给 redisObject 中的 ptr，这部分的代码如下所示：\n\nrobj *createObject(int type, void *ptr) {\n    //给redisObject结构体分配空间\n    robj *o = zmalloc(sizeof(*o));\n    //设置redisObject的类型\n    o->type = type;\n    //设置redisObject的编码类型，此处是OBJ_ENCODING_RAW，表示常规的SDS\n    o->encoding = OBJ_ENCODING_RAW;\n    //直接将传入的指针赋值给redisObject中的指针。\n    o->ptr = ptr;\n    o->refcount = 1;\n    …\n    return o;\n}\n\n\n为了方便理解普通字符串创建方法，我画了一张图，你可以看下。\n\n\n\n这也就是说，在创建普通字符串时，Redis 需要分别给 redisObject 和 SDS 分别分配一次内存，这样就既带来了内存分配开销，同时也会导致内存碎片。因此，当字符串小于等于 44 字节时，Redis 就使用了嵌入式字符串的创建方法，以此减少内存分配和内存碎片。\n\n而这个创建方法，就是由我们前面提到的 createEmbeddedStringObject 函数来完成的，该函数会使用一块连续的内存空间，来同时保存 redisObject 和 SDS 结构。这样一来，内存分配只有一次，而且也避免了内存碎片。\n\ncreateEmbeddedStringObject 函数的原型定义如下，它的参数就是从 createStringObject 函数参数中获得的字符串指针 ptr，以及字符串长度 len\n\nrobj *createEmbeddedStringObject(const char *ptr, size_t len)\n\n\n那么下面，我们就来具体看看，createEmbeddedStringObject 函数是如何把 redisObject 和 SDS 放置在一起的。\n\n首先，createEmbeddedStringObject 函数会分配一块连续的内存空间，这块内存空间的大小等于 redisObject 结构体的大小、SDS 结构头 sdshdr8 的大小和字符串大小的总和，并且再加上 1 字节。注意，这里最后的 1 字节是 SDS 中加在字符串最后的结束字符“\\0”。\n\n这块连续内存空间的分配情况如以下代码所示：\n\nrobj *o = zmalloc(sizeof(robj)+sizeof(struct sdshdr8)+len+1);\n\n\n你也可以参考下图，其中展示了这块内存空间的布局。\n\n\n\n好，那么 createEmbeddedStringObject 函数在分配了内存空间之后，就会 创建 SDS 结构的指针 sh，并把 sh 指向这块连续空间中 SDS 结构头所在的位置，下面的代码显示了这步操作。其中，o 是 redisObject 结构体的变量，o+1 表示将内存地址从变量 o 开始移动一段距离，而移动的距离等于 redisObject 这个结构体的大小。\n\nstruct sdshdr8 *sh = (void*)(o+1);\n\n\n经过这步操作后，sh 指向的位置就如下图所示：\n\n\n\n紧接着，createEmbeddedStringObject 函数会 把 redisObject 中的指针 ptr，指向 SDS 结构中的字符数组。\n\n如以下代码所示，其中 sh 是刚才介绍的指向 SDS 结构的指针，属于 sdshdr8 类型。而 sh+1 表示把内存地址从 sh 起始地址开始移动一定的大小，移动的距离等于 sdshdr8 结构体的大小。\n\no->ptr = sh+1;\n\n\n这步操作完成后，redisObject 结构体中的指针 ptr 的指向位置就如下图所示，它会指向 SDS 结构头的末尾，同时也是字符数组的起始位置：\n\n\n\n最后，createEmbeddedStringObject 函数会把参数中传入的指针 ptr 指向的字符串，拷贝到 SDS 结构体中的字符数组，并在数组最后添加结束字符。这部分代码如下所示：\n\nmemcpy(sh->buf,ptr,len);\nsh->buf[len] = '\\0';\n\n\n下面这张图，也展示了 createEmbeddedStringObject 创建嵌入式字符串的过程，你可以再整体来看看。\n\n\n\n总之，你可以记住，Redis 会通过设计实现一块连续的内存空间，把 redisObject 结构体和 SDS 结构体紧凑地放置在一起。这样一来，对于不超过 44 字节的字符串来说，就可以避免内存碎片和两次内存分配的开销了。\n\n\n# 总结\n\n 1. Redis 使用 SDS 的原因\n 2. SDS 和 C字符串 的 5 个区别\n 3. SDS 的内存友好设计\n 4. SDS 在不同的 len 属性下对应了底层数组的不同行为\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 为什么c语言的字符串在性能和安全性上不足以满足redis需求？\n 2. sds如何将获取字符串长度的操作从o(n)优化为o(1)？\n 3. 高并发下，sds的“空间预分配”如何提升频繁字符串操作的性能？\n 4. redis为什么需要sds的二进制安全？传统字符串处理方法有什么局限？\n 5. 为什么sds缩短时不立即释放多余内存？何时需要手动释放？\n 6. sds的嵌入式字符串如何减少内存碎片？适用于多大长度的字符串？\n\n\n# 前言\n\n一个优雅的字符串设计，需要尽量满足以下三个要求：\n\n * 能支持丰富的 api 操作，比如字符串追加、拷贝、比较、获取长度等\n * 能保存任意的二进制数据，比如图片等\n * 能尽可能地节省内存开销\n\nredis 设计了简单动态字符串（simple dynamic string，sds）的结构，用来表示字符串。\n\n相比于 c 语言中的字符串实现，sds 更适合 redis 的特性，我会在本文一一叙述\n\nsds 使用场景\n\n * 数据库中的字符串值\n * aof 模块中的 aof 缓冲区\n * 客户端状态中的输入缓冲区\n * 等等等等\n\nredis 很多用到了字符串的地方都是使用 sds\n\n\n# sds 的定义\n\nstruct sdshdr{\n\t//记录buf数组中已使用的字节数 等于sds所保存的字符串的长度\n\tint len;\n\t// 记录buf数组中未使用的字节数\n\tint free;\n\t//字节数组 用于保存字符串\n\tchar buf[];\n}\n\n\n\n\n笔记\n\nsds 遵循 c 字符串以空字符结尾的惯例，保存空字符的 1 字节空间不计算在 sds 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作，都是由sds函数自动完成的，所以这个空字符对于 sds 的使用者来说是完全透明的。遵循空字符结尾这一惯例的好处是，sds 可以直接重用一部分 c 字符串函数库里面的函数。\n\n\n# sds 与 c 字符串的区别\n\n\n\nc 语言字符串使用长度为 n+1 的字符串数组来存放长度为 n 的字符串，并且字符数组的最后一个元素总是空字符串 \\0\n\n这并不能满足 redis 对字符串在效率、安全性以及功能方面的要求\n\n接下来说明 sds 比 c 字符串更适用于 redis 的原因\n\n特性                  c 字符串            sds (简单动态字符串)\n获取字符串长度的复杂度         o(n)             o(1)\napi 安全性             不安全，可能导致缓冲区溢出    安全，不会导致缓冲区溢出\n修改字符串长度时的内存重分配次数    修改n次，执行n次内存重分配   修改n次，最多执行n次内存重分配\n数据类型支持              只能保存文本数据         可以保存文本或二进制数据\n<string.h>库函数使用情况   可使用所有函数          可使用部分函数\n\n\n# 常数复杂度获取字符串长度\n\n * c 字符串并不记录自身的长度信息，所以为了获取一个c字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 o(n)\n\n * sds 中有 len 属性来存储字符串长度，使得 strlen 命令的复杂度仅为 o(1)\n\nlen 属性的设置和更新是由 sds 的 api 在执行的时候自动完成的，我们无需手动设置\n\n\n\n\n# 杜绝缓冲区溢出\n\nchar *strcat(char *dest，const char *src);\n\n\n * c 字符串不记录自身长度，所以当调用 strcat 函数时，系统假定用户为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立，就会产生缓冲区溢出。需要用户对字符串进行扩容。\n\n * sds 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：sds api需要对sds进行修改的时候，api会先检查sds的空间是否满足修改所需的要求，如果不满足的话，api会自动将sds的空间进行扩容到指定的大小，然后再次执行实际的修改操作，所以sds既不需要手动扩容，也不会缓冲区溢出\n\n\n# 减少修改字符串时带来的内存重分配次数\n\n因为 c 字符串的长度和底层数组的长度之间存在关联性，所以每次改变字符串长度的时候，程序总要对保存这个c字符串的数组进行一次内存重分配操作\n\n> 内存重分配：\n> \n>  * 如果程序执行的是增长字符串的操作，比如拼接操作append，那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小，如果忘了这一步就会产生缓冲区溢出\n>  * 如果程序执行的是缩短字符串的操作，比如截断操作trim，那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间，如果忘了这一步就会产生内存泄漏\n\n注意\n\n一般情况下，如果修改字符串长度的情况不常见，那么一般是可以接受内存重分配的。但是redis作为数据库，为了极致化性能体验，就会尽量避免内存重分配\n\n所以sds 通过使用 **free 属性 **记录未使用字节，从而解除了字符串长度和底层数组长度之间的关联：\n\n * 在sds中，buf 数组的长度不一定就是字符数量加一\n * 并且数组里面可以包含未使用的字节\n\n那 sds 是如何利用 free 属性去避免内存重分配的？\n\nsds实现了「空间预分配」和「惰性空间释放」两种优化策略\n\n# 空间预分配\n\n「空间预分配」用于优化 sds 的字符串增长操作：当 sds 的 api 对一个 sds 进行修改，并且需要对 sds 进行空间扩展的时候，程序不仅会为 sds 分配修改所必须要的空间，还会为 sds 分配额外的未使用空间\n\n额外分配的未使用空间数量有以下规则\n\n * 当对 sds 进行修改之后，sds 的 len 属性 小于 1 mb，那么程序分配和 len属性同样大小的未使用空间，这是 sds len属性的值将和 free属性的值相同\n * 当对 sds 进行修改之后，sds 的 len 属性 小于等于 1 mb，那么程序会分配 1mb的使用空间\n\n通过「空间预分配」策略，redis 可以减少连续执行字符串增长操作所需的内存重分配次数\n\n# 惰性空间释放\n\n「惰性空间释放」用于优化 sds 的字符串缩短操作：当 sds 的 api 需要缩短 sds 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用 举个例子，sds 的 api：sdstrim函数接受一个 sds 和一个 c 字符串作为参数，移除 sds 中所有在c字符串中出现过的字符\n\n\n\n注意，执行 sdstrim 之后的 sds 并没有释放多出来的 8 字节空间，而是将这 8 字节空间作为未使用空间保留在了 sds 里面，如果将来要对 sds 进行增长操作的话，这些未使用空间就可能会派上用场\n\n所以也就避免了缩短字符串时所需的内存重分配操作，并未将来可能有的操作提供了优化\n\n同时，sds 也提供了相应的 api，让我们可以在有需要的时候释放 sds 的未使用空间，不用担心惰性空间释放策略会造成内存浪费\n\n思考\n\n什么时候 redis 会去主动释放 sds 中的未使用空间？\n\n\n# 二进制安全\n\nc 字符串中的字符必须符合某种编码，比如ascii，并且除了字符串的末尾之外，字符串里面不能包含空字符 \\0，否则最先被程序读入的空字符将被误认为是字符串结尾，这些限制使得c字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据\n\n为了确保 redis 可以适用于各种不同的使用场景（文本数据或者二进制数据），sds 的 api 都是二进制安全的，所有 sds api 都会以处理二进制的方式来处理 sds 存放在 buf 数组里的数据程序，同时不会对其中的数据做任何限制、过滤、或者假设，数据在写入时是什么样的，它被读取时就是什么样\n\n\n\n\n# 兼容部分 c 字符串函数\n\n略\n\n\n# sds 的内存友好设计\n\n在 redis 3.x 之后，sds 设计了不同类型的结构头，包括 sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。这些不同类型的结构头可以适配不同大小的字符串，从而避免了内存浪费\n\nstruct __attribute__ ((__packed__)) sdshdr8 {\n    uint8_t len;  // 已使用的字符串长度（8位，最大255字节）\n    uint8_t alloc; // 已分配的空间（8位，最大255字节）\n    unsigned char flags; // 标志，用于标识使用的是哪种sdshdr类型\n    char buf[];   // 字符串内容\n};\n\n\n而且，redis 在保存较小字符串时，其实还使用了嵌入式字符串的设计方法。这种方法避免了给字符串分配额外的空间，而是可以让字符串直接保存在 redis 的基本数据对象结构体中。\n\n所以这也就是说，要想理解嵌入式字符串的设计与实现，我们就需要先来了解下，redis 使用的基本数据对象结构体 redisobject 是什么样的\n\n# redisobject 结构体与位域定义方法\n\nredisobject 结构体是在 server.h 文件中定义的，主要功能是用来保存「键值对中的值」\n\n这个结构一共定义了「4 个元数据」和 「一个指针」\n\n * type：redisobject 的数据类型，是应用程序在 redis 中保存的数据类型，包括 string、list、hash 等。\n * encoding：redisobject 的编码类型，是 redis 内部实现各种数据类型所用的数据结构。\n * lru：redisobject 的 lru 时间。\n * refcount：redisobject 的引用计数。\n * ptr：指向值的指针。\n\n下面的代码展示了 redisobject 结构体的定义：\n\ntypedef struct redisobject {\n    unsigned type:4; //redisobject的数据类型，4个bits\n    unsigned encoding:4; //redisobject的编码类型，4个bits\n    unsigned lru:lru_bits;  //redisobject的lru时间，lru_bits为24个bits\n    int refcount; //redisobject的引用计数，4个字节\n    void *ptr; //指向值的指针，8个字节\n} robj;\n\n\n从代码中我们可以看到，在 type、encoding 和 lru 三个变量后面都有一个冒号，并紧跟着一个数值，表示该元数据占用的比特数。其中，type 和 encoding 分别占 4bits。而 lru 占用的比特数，是由 server.h 中的宏定义 lru_bits 决定的，它的默认值是 24bits，如下所示：\n\n#define lru_bits 24\n\n\n而这里我想让你学习掌握的，就是这种变量后使用冒号和数值的定义方法。这实际上是 c 语言中的位域定义方法，可以用来有效地节省内存开销。\n\n这种方法比较适用的场景是，当一个变量占用不了一个数据类型的所有 bits 时，就可以使用位域定义方法，把一个数据类型中的 bits，划分成多个位域，每个位域占一定的 bit 数。这样一来，一个数据类型的所有 bits 就可以定义多个变量了，从而也就有效节省了内存开销。\n\n此外，你可能还会发现，对于 type、encoding 和 lru 三个变量来说，它们的数据类型都是 unsigned。已知一个 unsigned 类型是 4 字节，但这三个变量，是分别占用了一个 unsigned 类型 4 字节中的 4bits、4bits 和 24bits。因此，相较于三个变量，每个变量用一个 4 字节的 unsigned 类型定义来说，使用位域定义方法可以让三个变量只用 4 字节，最后就能节省 8 字节的开销。\n\n所以，当你在设计开发内存敏感型的软件时，就可以把这种位域定义方法使用起来。\n\n好，了解了 redisobject 结构体和它使用的位域定义方法以后，我们再来看嵌入式字符串是如何实现的。\n\n# 嵌入式字符串\n\n前面我说过，sds 在保存比较小的字符串时，会使用嵌入式字符串的设计方法，将字符串直接保存在 redisobject 结构体中。然后在 redisobject 结构体中，存在一个指向值的指针 ptr，而一般来说，这个 ptr 指针会指向值的数据结构。\n\n这里我们就以创建一个 string 类型的值为例，redis 会调用 createstringobject 函数，来创建相应的 redisobject，而这个 redisobject 中的 ptr 指针，就会指向 sds 数据结构，如下图所示。\n\n\n\n在 redis 源码中，createstringobject 函数会根据要创建的字符串的长度，决定具体调用哪个函数来完成创建。\n\n那么针对这个 createstringobject 函数来说，它的参数是字符串 ptr 和字符串长度 len。当 len 的长度大于 obj_encoding_embstr_size_limit 这个宏定义时，createstringobject 函数会调用 createrawstringobject 函数，否则就调用 createembeddedstringobject 函数。\n\n而在我们分析的 redis 5.0.8 源码版本中，这个 obj_encoding_embstr_size_limit 默认定义为 44 字节。\n\n这部分代码如下所示：\n\n#define obj_encoding_embstr_size_limit 44\nrobj *createstringobject(const char *ptr, size_t len) {\n    //创建嵌入式字符串，字符串长度小于等于44字节\n    if (len <= obj_encoding_embstr_size_limit)\n        return createembeddedstringobject(ptr,len);\n    //创建普通字符串，字符串长度大于44字节\n    else\n        return createrawstringobject(ptr,len);\n}\n\n\n现在，我们就来分析一下 createstringobject 函数的源码实现，以此了解大于 44 字节的普通字符串和小于等于 44 字节的嵌入式字符串分别是如何创建的。\n\n首先，对于 createrawstringobject 函数 来说，它在创建 string 类型的值的时候，会调用 createobject 函数。\n\n> 补充：createobject 函数主要是用来创建 redis 的数据对象的。因为 redis 的数据对象有很多类型，比如 string、list、hash 等，所以在 createobject 函数的两个参数中，有一个就是用来表示所要创建的数据对象类型，而另一个是指向数据对象的指针。\n\n然后，createrawstringobject 函数在调用 createobject 函数时，会传递 obj_string 类型，表示要创建 string 类型的对象，以及传递指向 sds 结构的指针，如以下代码所示。这里需要注意的是，指向 sds 结构的指针是由 sdsnewlen 函数返回的，而 sdsnewlen 函数正是用来创建 sds 结构的。\n\nrobj *createrawstringobject(const char *ptr, size_t len) {\n    return createobject(obj_string, sdsnewlen(ptr,len));\n}\n\n\n最后，我们再来进一步看下 createobject 函数。这个函数会把参数中传入的、指向 sds 结构体的指针直接赋值给 redisobject 中的 ptr，这部分的代码如下所示：\n\nrobj *createobject(int type, void *ptr) {\n    //给redisobject结构体分配空间\n    robj *o = zmalloc(sizeof(*o));\n    //设置redisobject的类型\n    o->type = type;\n    //设置redisobject的编码类型，此处是obj_encoding_raw，表示常规的sds\n    o->encoding = obj_encoding_raw;\n    //直接将传入的指针赋值给redisobject中的指针。\n    o->ptr = ptr;\n    o->refcount = 1;\n    …\n    return o;\n}\n\n\n为了方便理解普通字符串创建方法，我画了一张图，你可以看下。\n\n\n\n这也就是说，在创建普通字符串时，redis 需要分别给 redisobject 和 sds 分别分配一次内存，这样就既带来了内存分配开销，同时也会导致内存碎片。因此，当字符串小于等于 44 字节时，redis 就使用了嵌入式字符串的创建方法，以此减少内存分配和内存碎片。\n\n而这个创建方法，就是由我们前面提到的 createembeddedstringobject 函数来完成的，该函数会使用一块连续的内存空间，来同时保存 redisobject 和 sds 结构。这样一来，内存分配只有一次，而且也避免了内存碎片。\n\ncreateembeddedstringobject 函数的原型定义如下，它的参数就是从 createstringobject 函数参数中获得的字符串指针 ptr，以及字符串长度 len\n\nrobj *createembeddedstringobject(const char *ptr, size_t len)\n\n\n那么下面，我们就来具体看看，createembeddedstringobject 函数是如何把 redisobject 和 sds 放置在一起的。\n\n首先，createembeddedstringobject 函数会分配一块连续的内存空间，这块内存空间的大小等于 redisobject 结构体的大小、sds 结构头 sdshdr8 的大小和字符串大小的总和，并且再加上 1 字节。注意，这里最后的 1 字节是 sds 中加在字符串最后的结束字符“\\0”。\n\n这块连续内存空间的分配情况如以下代码所示：\n\nrobj *o = zmalloc(sizeof(robj)+sizeof(struct sdshdr8)+len+1);\n\n\n你也可以参考下图，其中展示了这块内存空间的布局。\n\n\n\n好，那么 createembeddedstringobject 函数在分配了内存空间之后，就会 创建 sds 结构的指针 sh，并把 sh 指向这块连续空间中 sds 结构头所在的位置，下面的代码显示了这步操作。其中，o 是 redisobject 结构体的变量，o+1 表示将内存地址从变量 o 开始移动一段距离，而移动的距离等于 redisobject 这个结构体的大小。\n\nstruct sdshdr8 *sh = (void*)(o+1);\n\n\n经过这步操作后，sh 指向的位置就如下图所示：\n\n\n\n紧接着，createembeddedstringobject 函数会 把 redisobject 中的指针 ptr，指向 sds 结构中的字符数组。\n\n如以下代码所示，其中 sh 是刚才介绍的指向 sds 结构的指针，属于 sdshdr8 类型。而 sh+1 表示把内存地址从 sh 起始地址开始移动一定的大小，移动的距离等于 sdshdr8 结构体的大小。\n\no->ptr = sh+1;\n\n\n这步操作完成后，redisobject 结构体中的指针 ptr 的指向位置就如下图所示，它会指向 sds 结构头的末尾，同时也是字符数组的起始位置：\n\n\n\n最后，createembeddedstringobject 函数会把参数中传入的指针 ptr 指向的字符串，拷贝到 sds 结构体中的字符数组，并在数组最后添加结束字符。这部分代码如下所示：\n\nmemcpy(sh->buf,ptr,len);\nsh->buf[len] = '\\0';\n\n\n下面这张图，也展示了 createembeddedstringobject 创建嵌入式字符串的过程，你可以再整体来看看。\n\n\n\n总之，你可以记住，redis 会通过设计实现一块连续的内存空间，把 redisobject 结构体和 sds 结构体紧凑地放置在一起。这样一来，对于不超过 44 字节的字符串来说，就可以避免内存碎片和两次内存分配的开销了。\n\n\n# 总结\n\n 1. redis 使用 sds 的原因\n 2. sds 和 c字符串 的 5 个区别\n 3. sds 的内存友好设计\n 4. sds 在不同的 len 属性下对应了底层数组的不同行为\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"List 设计与实现",frontmatter:{title:"List 设计与实现",date:"2024-09-16T02:46:18.000Z",permalink:"/pages/bd1e41/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/02.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02.List%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Redis 系统设计/02.二、基础知识/02.List 设计与实现.md",key:"v-19b74794",path:"/pages/bd1e41/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:321},{level:2,title:"ziplist 设计与实现",slug:"ziplist-设计与实现",normalizedTitle:"ziplist 设计与实现",charIndex:789},{level:3,title:"ziplist 和 整数集合 的设计",slug:"ziplist-和-整数集合-的设计",normalizedTitle:"ziplist 和 整数集合 的设计",charIndex:807},{level:3,title:"扩展：节省内存的数据访问",slug:"扩展-节省内存的数据访问",normalizedTitle:"扩展：节省内存的数据访问",charIndex:4474},{level:3,title:"ziplist 的不足",slug:"ziplist-的不足",normalizedTitle:"ziplist 的不足",charIndex:5420},{level:4,title:"查找复杂度高",slug:"查找复杂度高",normalizedTitle:"查找复杂度高",charIndex:5697},{level:4,title:"连锁更新风险",slug:"连锁更新风险",normalizedTitle:"连锁更新风险",charIndex:5710},{level:2,title:"quicklist 设计与实现",slug:"quicklist-设计与实现",normalizedTitle:"quicklist 设计与实现",charIndex:10003},{level:2,title:"listpack 设计与实现",slug:"listpack-设计与实现",normalizedTitle:"listpack 设计与实现",charIndex:12592},{level:3,title:"listpack 列表项编码方法",slug:"listpack-列表项编码方法",normalizedTitle:"listpack 列表项编码方法",charIndex:13863},{level:3,title:"listpack 避免连锁更新的实现方式",slug:"listpack-避免连锁更新的实现方式",normalizedTitle:"listpack 避免连锁更新的实现方式",charIndex:15319},{level:2,title:"对比",slug:"对比",normalizedTitle:"对比",charIndex:18086},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:18631},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:19503}],headersStr:"前言 ziplist 设计与实现 ziplist 和 整数集合 的设计 扩展：节省内存的数据访问 ziplist 的不足 查找复杂度高 连锁更新风险 quicklist 设计与实现 listpack 设计与实现 listpack 列表项编码方法 listpack 避免连锁更新的实现方式 对比 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. 为什么 Redis 设计了三种类似但不同的底层数据结构 ziplist、quicklist、listpack？\n 2. ziplist 为什么会面临查找效率降低和内存连锁更新的问题？\n 3. quicklist 是如何解决 ziplist 的性能瓶颈的？\n 4. listpack 是如何避免 ziplist 的连锁更新问题的？\n 5. 如何在实际开发中选择适合的 Redis List 底层结构，以平衡内存使用和性能？\n 6. ziplist 在 Redis 设计中有哪些优点被保留了下来，即使它存在性能不足？\n 7. 当数据插入量大时，quicklist 如何避免内存频繁重新分配而保持高效？\n\n\n# 前言\n\nRedis 中的 List 的底层有三种数据结构\n\n * ziplist\n * quicklist\n * listpack\n\nziplist 的最大特点，就是它被设计成一种 内存紧凑型 的数据结构，占用一块连续的内存空间，以达到节省内存的目的\n\n但是，在计算机系统中，任何一个设计都是有利有弊的。对于 ziplist 来说，这个道理同样成立。\n\n虽然 ziplist 节省了内存开销，可它也存在两个设计代价\n\n * 「不能保存过多的元素」，否则访问性能会降低\n * 「不能保存过大的元素」，否则容易导致内存重新分配，甚至可能引发连锁更新的问题。所谓的连锁更新，简单来说，就是 ziplist 中的每一项都要被重新分配内存空间，造成 ziplist 的性能降低\n\n因此，针对 ziplist 在设计上的不足，Redis 在演进的过程中，新增设计了两种数据结构\n\n * quicklist\n * istpack\n\n它们设计目标，是 尽可能地保持 ziplist 节省内存的优势，同时避免 ziplist 潜在的性能下降问题\n\n\n# ziplist 设计与实现\n\n\n# ziplist 和 整数集合 的设计\n\n首先你要知道，List、Hash 和 Sorted Set 这三种数据类型，都可以使用压缩列表（ziplist）来保存数据。压缩列表的函数定义和实现代码分别在 ziplist.h 和 ziplist.c 中\n\n不过，我们在 ziplist.h 文件中其实根本看不到压缩列表的结构体定义。这是因为压缩列表本身就是一块连续的内存空间，它通过使用不同的编码来保存数据\n\n这里为了方便理解压缩列表的设计与实现，我们先来看看它的创建函数 ziplistNew，如下所示：\n\nunsigned char *ziplistNew(void) {\n    //初始分配的大小\n    unsigned int bytes = ZIPLIST_HEADER_SIZE+ZIPLIST_END_SIZE;\n    unsigned char *zl = zmalloc(bytes);\n    …\n   //将列表尾设置为ZIP_END\n    zl[bytes-1] = ZIP_END;\n    return zl;\n}\n\n\n实际上，ziplistNew 函数的逻辑很简单，就是创建一块连续的内存空间，大小为 ZIPLIST_HEADER_SIZE 和 ZIPLIST_END_SIZE 的总和，然后再把该连续空间的最后一个字节赋值为 ZIP_END，表示列表结束。\n\n这三个宏分别表示 ziplist 的列表头大小、列表尾大小和列表尾字节内容\n\n//ziplist的列表头大小，包括2个32 bits整数和1个16bits整数，分别表示压缩列表的总字节数，列表最后一个元素的离列表头的偏移，以及列表中的元素个数\n#define ZIPLIST_HEADER_SIZE     (sizeof(uint32_t)*2+sizeof(uint16_t))\n//ziplist的列表尾大小，包括1个8 bits整数，表示列表结束。\n#define ZIPLIST_END_SIZE        (sizeof(uint8_t))\n//ziplist的列表尾字节内容\n#define ZIP_END 255\n\n\n那么，在创建一个新的 ziplist 后，该列表的内存布局就如下图所示。注意，此时列表中还没有实际的数据。\n\n\n\n然后，当我们往 ziplist 中插入数据时，ziplist 就会根据数据是字符串还是整数，以及它们的大小进行不同的编码。这种根据数据大小进行相应编码的设计思想，正是 Redis 为了节省内存而采用的\n\nziplist 列表项包括三部分内容，分别是前一项的长度（prevlen）、当前项长度信息的编码结果（encoding），以及当前项的实际数据（data）。下面的图展示了列表项的结构（图中除列表项之外的内容分别是 ziplist 内存空间的起始和尾部）。\n\n\n\n实际上，所谓的编码技术，就是指 用不同数量的字节来表示保存的信息。在 ziplist 中，编码技术主要应用在列表项中的 prevlen 和 encoding 这两个元数据上。而当前项的实际数据 data，则正常用整数或是字符串来表示。\n\n所以这里，我们就先来看下 prevlen 的编码设计。ziplist 中会包含多个列表项，每个列表项都是紧挨着彼此存放的，如下图所示。\n\n\n\n而为了方便查找，每个列表项中都会记录前一项的长度。因为每个列表项的长度不一样，所以如果使用相同的字节大小来记录 prevlen，就会造成内存空间浪费。\n\n我给你举个例子，假设我们统一使用 4 字节记录 prevlen，如果前一个列表项只是一个字符串“redis”，长度为 5 个字节，那么我们用 1 个字节（8 bits）就能表示 256 字节长度（2 的 8 次方等于 256）的字符串了。此时，prevlen 用 4 字节记录，其中就有 3 字节是浪费掉了。\n\n好，我们再回过头来看，ziplist 在对 prevlen 编码时，会先调用 zipStorePrevEntryLength 函数，用于判断前一个列表项是否小于 254 字节。如果是的话，那么 prevlen 就使用 1 字节表示；否则，zipStorePrevEntryLength 函数就调用 zipStorePrevEntryLengthLarge 函数进一步编码。这部分代码如下所示：\n\n//判断prevlen的长度是否小于ZIP_BIG_PREVLEN，ZIP_BIG_PREVLEN等于254\nif (len < ZIP_BIG_PREVLEN) {\n   //如果小于254字节，那么返回prevlen为1字节\n   p[0] = len;\n   return 1;\n} else {\n   //否则，调用zipStorePrevEntryLengthLarge进行编码\n   return zipStorePrevEntryLengthLarge(p,len);\n}\n\n\n也就是说，zipStorePrevEntryLengthLarge 函数会先将 prevlen 的第 1 字节设置为 254，然后使用内存拷贝函数 memcpy，将前一个列表项的长度值拷贝至 prevlen 的第 2 至第 5 字节。最后，zipStorePrevEntryLengthLarge 函数返回 prevlen 的大小，为 5 字节。\n\nif (p != NULL) {\n    //将prevlen的第1字节设置为ZIP_BIG_PREVLEN，即254\n    p[0] = ZIP_BIG_PREVLEN;\n  //将前一个列表项的长度值拷贝至prevlen的第2至第5字节，其中sizeof(len)的值为4\n    memcpy(p+1,&len,sizeof(len));\n    …\n}\n//返回prevlen的大小，为5字节\nreturn 1+sizeof(len);\n\n\n好，在了解了 prevlen 使用 1 字节和 5 字节两种编码方式后，我们再来学习下 encoding 的编码方法。\n\n我们知道，一个列表项的实际数据，既可以是整数也可以是字符串。整数可以是 16、32、64 等字节长度，同时字符串的长度也可以大小不一。\n\n所以，ziplist 在 zipStoreEntryEncoding 函数中，针对整数和字符串，就分别使用了不同字节长度的编码结果。下面的代码展示了 zipStoreEntryEncoding 函数的部分代码，你可以看到当数据是不同长度字符串或是整数时，编码结果的长度 len 大小不同。\n\n//默认编码结果是1字节\nunsigned char len = 1;\n//如果是字符串数据\nif (ZIP_IS_STR(encoding)) {\n    //字符串长度小于等于63字节（16进制为0x3f）\n    if (rawlen <= 0x3f) {\n        //默认编码结果是1字节\n        …\n    }\n    //字符串长度小于等于16383字节（16进制为0x3fff）\n    else if (rawlen <= 0x3fff) {\n        //编码结果是2字节\n        len += 1;\n        …\n    }\n    //字符串长度大于16383字节\n\n    else {\n        //编码结果是5字节\n        len += 4;\n        …\n    }\n} else {\n    /* 如果数据是整数，编码结果是1字节*/\n    if (!p) return len;\n    ...\n}\n\n\n简而言之，针对不同长度的数据，使用不同大小的元数据信息（prevlen 和 encoding），这种方法可以有效地节省内存开销\n\n除了 ziplist 之外，Redis 还设计了一个内存友好的数据结构，这就是整数集合（intset），它是作为底层结构来实现 Set 数据类型的。\n\n和 SDS 嵌入式字符串、ziplist 类似，整数集合也是一块连续的内存空间，这一点我们从整数集合的定义中就可以看到。intset.h 和 intset.c 分别包括了整数集合的定义和实现\n\n下面的代码展示了 intset 的结构定义。我们可以看到，整数集合结构体中记录数据的部分，就是一个 int8_t 类型的整数数组 contents。从内存使用的角度来看，整数数组就是一块连续内存空间，所以这样就避免了内存碎片，并提升了内存使用效率\n\ntypedef struct intset {\n    uint32_t encoding;\n    uint32_t length;\n    int8_t contents[];\n} intset;\n\n\n\n# 扩展：节省内存的数据访问\n\n我们知道，在 Redis 实例运行时，有些数据是会被经常访问的，比如常见的整数，Redis 协议中常见的回复信息，包括操作成功（“OK”字符串）、操作失败（ERR），以及常见的报错信息。\n\n所以，为了避免在内存中反复创建这些经常被访问的数据，Redis 就采用了共享对象的设计思想。这个设计思想很简单，就是把这些常用数据创建为共享对象，当上层应用需要访问它们时，直接读取就行。\n\n现在我们就来做个假设。有 1000 个客户端，都要保存“3”这个整数。如果 Redis 为每个客户端，都创建了一个值为 3 的 redisObject，那么内存中就会有大量的冗余。而使用了共享对象方法后，Redis 在内存中只用保存一个 3 的 redisObject 就行，这样就有效节省了内存空间。\n\n以下代码展示的是 server.c 文件中，创建共享对象的函数 createSharedObjects，你可以看下。\n\nvoid createSharedObjects(void) {\n   …\n   //常见回复信息\n   shared.ok = createObject(OBJ_STRING,sdsnew("+OK\\r\\n"));\n   shared.err = createObject(OBJ_STRING,sdsnew("-ERR\\r\\n"));\n   …\n   //常见报错信息\n shared.nokeyerr = createObject(OBJ_STRING,sdsnew("-ERR no such key\\r\\n"));\n shared.syntaxerr = createObject(OBJ_STRING,sdsnew("-ERR syntax error\\r\\n"));\n   //0到9999的整数\n   for (j = 0; j < OBJ_SHARED_INTEGERS; j++) {\n        shared.integers[j] =\n          makeObjectShared(createObject(OBJ_STRING,(void*)(long)j));\n        …\n    }\n   …\n}\n\n\n\n# ziplist 的不足\n\n你已经知道，一个 ziplist 数据结构在内存中的布局，就是一块连续的内存空间。这块空间的起始部分是大小固定的 10 字节元数据，其中记录了 ziplist 的总字节数、最后一个元素的偏移量以及列表元素的数量，而这 10 字节后面的内存空间则保存了实际的列表数据。在 ziplist 的最后部分，是一个 1 字节的标识（固定为 255），用来表示 ziplist 的结束，如下图所示：\n\n\n\n不过，虽然 ziplist 通过紧凑的内存布局来保存数据，节省了内存空间，但是 ziplist 也面临着随之而来的两个不足：\n\n * 查找复杂度高\n * 潜在的连锁更新风险\n\n那么下面，我们就分别来了解下这两个问题\n\n# 查找复杂度高\n\n因为 ziplist 头尾元数据的大小是固定的，所以可以很快找到 首部元素和尾部元素，但问题是\n\n * 当要查找中间元素时，ziplist 就得从列表头或列表尾遍历才行\n * 更糟糕的是，如果 ziplist 里面保存的是字符串，ziplist 在查找某个元素时，还需要逐一判断元素的每个字符，这样又进一步增加了复杂度\n * ziplist 在插入元素时，如果内存空间不够了，ziplist 还需要重新分配一块连续的内存空间，而这还会进一步引发连锁更新的问题\n\n也正因为如此，我们在使用 ziplist 保存 Hash 或 Sorted Set 数据时，都会在 redis.conf 文件中，通过 hash-max-ziplist-entries 和 zset-max-ziplist-entries 两个参数，来控制保存在 ziplist 中的元素个数\n\n# 连锁更新风险\n\n我们知道，因为 ziplist 必须使用一块连续的内存空间来保存数据，所以当新插入一个元素时，ziplist 就需要计算其所需的空间大小，并申请相应的内存空间。这一系列操作，我们可以从 ziplist 的元素插入函数 __ziplistInsert 中看到。\n\n__ziplistInsert 函数首先会计算获得当前 ziplist 的长度，这个步骤通过 ZIPLIST_BYTES 宏定义就可以完成，如下所示。同时，该函数还声明了 reqlen 变量，用于记录插入元素后所需的新增空间大小。\n\n//获取当前ziplist长度curlen；声明reqlen变量，用来记录新插入元素所需的长度\nsize_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), reqlen;\n\n\n然后，__ziplistInsert 函数会判断当前要插入的位置是否是列表末尾。如果不是末尾，那么就需要获取位于当前插入位置的元素的 prevlen 和 prevlensize。这部分代码如下所示：\n\n//如果插入的位置不是ziplist末尾，则获取前一项长度\nif (p[0] != ZIP_END) {\n    ZIP_DECODE_PREVLEN(p, prevlensize, prevlen);\n} else {\n    …\n}\n\n\n实际上，在 ziplist 中，每一个元素都会记录其前一项的长度，也就是 prevlen。然后，为了节省内存开销，ziplist 会使用不同的空间记录 prevlen，这个 prevlen 空间大小就是 prevlensize。\n\n举个简单的例子，当在一个元素 A 前插入一个新的元素 B 时，A 的 prevlen 和 prevlensize 都要根据 B 的长度进行相应的变化。\n\n那么现在，我们假设 A 的 prevlen 原本只占用 1 字节（也就是 prevlensize 等于 1），而能记录的前一项长度最大为 253 字节。此时，如果 B 的长度超过了 253 字节，A 的 prevlen 就需要使用 5 个字节来记录（prevlen 具体的编码方式，你可以复习回顾下第 4 讲），这样就需要申请额外的 4 字节空间了。不过，如果元素 B 的插入位置是列表末尾，那么插入元素 B 时，我们就不用考虑后面元素的 prevlen 了\n\n\n\n因此，为了保证 ziplist 有足够的内存空间，来保存插入元素以及插入位置元素的 prevlen 信息，__ziplistInsert 函数在获得插入位置元素的 prevlen 和 prevlensize 后，紧接着就会计算插入元素的长度。\n\n现在我们已知，一个 ziplist 元素包括了 prevlen、encoding 和实际数据 data 三个部分。所以，在计算插入元素的所需空间时，__ziplistInsert 函数也会分别计算这三个部分的长度。这个计算过程一共可以分成四步来完成。\n\n * 第一步，计算实际插入元素的长度。\n\n首先你要知道，这个计算过程和插入元素是整数还是字符串有关。__ziplistInsert 函数会先调用 zipTryEncoding 函数，这个函数会判断插入元素是否为整数。如果是整数，就按照不同的整数大小，计算 encoding 和实际数据 data 各自所需的空间；如果是字符串，那么就先把字符串长度记录为所需的新增空间大小。这一过程的代码如下所示：\n\n  if (zipTryEncoding(s,slen,&value,&encoding)) {\n          reqlen = zipIntSize(encoding);\n      } else {\n          reqlen = slen;\n      }\n\n\n * 第二步，调用 zipStorePrevEntryLength 函数，将插入位置元素的 prevlen 也计算到所需空间中。\n\n这是因为在插入元素后，__ziplistInsert 函数可能要为插入位置的元素分配新增空间。这部分代码如下所示：\n\nreqlen += zipStorePrevEntryLength(NULL,prevlen);\n\n\n * 第三步，调用 zipStoreEntryEncoding 函数，根据字符串的长度，计算相应 encoding 的大小。\n\n在刚才的第一步中，**ziplistInsert 函数对于字符串数据，只是记录了字符串本身的长度，所以在第三步中，**ziplistInsert 函数还会调用 zipStoreEntryEncoding 函数，根据字符串的长度来计算相应的 encoding 大小，如下所示：\n\nreqlen += zipStoreEntryEncoding(NULL,encoding,slen);\n\n\n好了，到这里，__ziplistInsert 函数就已经在 reqlen 变量中，记录了插入元素的 prevlen 长度、encoding 大小，以及实际数据 data 的长度。这样一来，插入元素的整体长度就有了，这也是插入位置元素的 prevlen 所要记录的大小。\n\n * 第四步，调用 zipPrevLenByteDiff 函数，判断插入位置元素的 prevlen 和实际所需的 prevlen 大小。\n\n最后，__ziplistInsert 函数会调用 zipPrevLenByteDiff 函数，用来判断插入位置元素的 prevlen 和实际所需的 prevlen，这两者间的大小差别。这部分代码如下所示，prevlen 的大小差别是使用 nextdiff 来记录的：\n\nnextdiff = (p[0] != ZIP_END) ? zipPrevLenByteDiff(p,reqlen) : 0;\n\n\n那么在这里，如果 nextdiff 大于 0，就表明插入位置元素的空间不够，需要新增 nextdiff 大小的空间，以便能保存新的 prevlen。然后，__ziplistInsert 函数在新增空间时，就会调用 ziplistResize 函数，来重新分配 ziplist 所需的空间。\n\nziplistResize 函数接收的参数分别是待重新分配的 ziplist 和重新分配的空间大小。而 __ziplistInsert 函数传入的重新分配大小的参数，是三个长度之和。\n\n那么是哪三个长度之和呢？\n\n这三个长度分别是 ziplist 现有大小（curlen）、待插入元素自身所需的新增空间（reqlen），以及插入位置元素 prevlen 所需的新增空间（nextdiff）。下面的代码显示了 ziplistResize 函数的调用和参数传递逻辑：\n\nzl = ziplistResize(zl,curlen+reqlen+nextdiff);\n\n\n进一步，那么 ziplistResize 函数在获得三个长度总和之后，具体是如何扩容呢？\n\n我们可以进一步看下 ziplistResize 函数的实现，这个函数会调用 zrealloc 函数，来完成空间的重新分配，而重新分配的空间大小就是由传入参数 len 决定的。这样，我们就了解到了 ziplistResize 函数涉及到内存分配操作，因此如果我们往 ziplist 频繁插入过多数据的话，就可能引起多次内存分配，从而会对 Redis 性能造成影响。\n\n下面的代码显示了 ziplistResize 函数的部分实现，你可以看下。\n\nunsigned char *ziplistResize(unsigned char *zl, unsigned int len) {\n    //对zl进行重新内存空间分配，重新分配的大小是len\n    zl = zrealloc(zl,len);\n    …\n    zl[len-1] = ZIP_END;\n    return zl;\n}\n\n\n好了，到这里，我们就了解了 ziplist 在新插入元素时，会计算其所需的新增空间，并进行重新分配。而当新插入的元素较大时，就会引起插入位置的元素 prevlensize 增加，进而就会导致插入位置的元素所占空间也增加。\n\n而如此一来，这种空间新增就会引起连锁更新的问题。\n\n实际上，所谓的连锁更新，就是指当一个元素插入后，会引起当前位置元素新增 prevlensize 的空间。而当前位置元素的空间增加后，又会进一步引起该元素的后续元素，其 prevlensize 所需空间的增加。\n\n这样，一旦插入位置后续的所有元素，都会因为前序元素的 prevlenszie 增加，而引起自身空间也要增加，这种每个元素的空间都需要增加的现象，就是连锁更新。我画了下面这张图，你可以看下。\n\n\n\n连锁更新一旦发生，就会导致 ziplist 占用的内存空间要多次重新分配，这就会直接影响到 ziplist 的访问性能。\n\n所以说，虽然 ziplist 紧凑型的内存布局能节省内存开销，但是如果保存的元素数量增加了，或是元素变大了，ziplist 就会面临性能问题。那么，有没有什么方法可以避免 ziplist 的问题呢\n\n这就是接下来我要给你介绍的 quicklist 和 listpack，这两种数据结构的设计思想了\n\n\n# quicklist 设计与实现\n\n我们先来学习下 quicklist 的实现思路。\n\nquicklist 的设计，其实是结合了链表和 ziplist 各自的优势。简单来说，一个 quicklist 就是一个链表，而链表中的每个元素又是一个 ziplist\n\n我们来看下 quicklist 的数据结构，这是在quicklist.h文件中定义的，而 quicklist 的具体实现是在quicklist.c文件中。\n\n首先，quicklist 元素的定义，也就是 quicklistNode。因为 quicklist 是一个链表，所以每个 quicklistNode 中，都包含了分别指向它前序和后序节点的指针prev 和 next**。同时，每个 quicklistNode 又是一个 ziplist，所以，在 quicklistNode 的结构体中，还有指向 ziplist 的**指针 zl。\n\n此外，quicklistNode 结构体中还定义了一些属性，比如 ziplist 的字节大小、包含的元素个数、编码格式、存储方式等。下面的代码显示了 quicklistNode 的结构体定义，你可以看下。\n\ntypedef struct quicklistNode {\n    struct quicklistNode *prev;     //前一个quicklistNode\n    struct quicklistNode *next;     //后一个quicklistNode\n    unsigned char *zl;              //quicklistNode指向的ziplist\n    unsigned int sz;                //ziplist的字节大小\n    unsigned int count : 16;        //ziplist中的元素个数\n    unsigned int encoding : 2;   //编码格式，原生字节数组或压缩存储\n    unsigned int container : 2;  //存储方式\n    unsigned int recompress : 1; //数据是否被压缩\n    unsigned int attempted_compress : 1; //数据能否被压缩\n    unsigned int extra : 10; //预留的bit位\n} quicklistNode;\n\n\n了解了 quicklistNode 的定义，我们再来看下 quicklist 的结构体定义。\n\nquicklist 作为一个链表结构，在它的数据结构中，是定义了整个 quicklist 的头、尾指针，这样一来，我们就可以通过 quicklist 的数据结构，来快速定位到 quicklist 的链表头和链表尾。\n\n此外，quicklist 中还定义了 quicklistNode 的个数、所有 ziplist 的总元素个数等属性。quicklist 的结构定义如下所示：\n\ntypedef struct quicklist {\n    quicklistNode *head;      //quicklist的链表头\n    quicklistNode *tail;      //quicklist的链表尾\n    unsigned long count;     //所有ziplist中的总元素个数\n    unsigned long len;       //quicklistNodes的个数\n    ...\n} quicklist;\n\n\n然后，从 quicklistNode 和 quicklist 的结构体定义中，我们就能画出下面这张 quicklist 的示意图。\n\n\n\n而也正因为 quicklist 采用了链表结构，所以当插入一个新的元素时，quicklist 首先就会检查插入位置的 ziplist 是否能容纳该元素，这是通过 _quicklistNodeAllowInsert 函数来完成判断的。\n\n_quicklistNodeAllowInsert 函数会计算新插入元素后的大小（new_sz），这个大小等于 quicklistNode 的当前大小（node->sz）、插入元素的大小（sz），以及插入元素后 ziplist 的 prevlen 占用大小。\n\n在计算完大小之后，_quicklistNodeAllowInsert 函数会依次判断新插入的数据大小（sz）是否满足要求，即单个 ziplist 是否不超过 8KB，或是单个 ziplist 里的元素个数是否满足要求。\n\n只要这里面的一个条件能满足，quicklist 就可以在当前的 quicklistNode 中插入新元素，否则 quicklist 就会新建一个 quicklistNode，以此来保存新插入的元素。\n\n下面代码显示了是否允许在当前 quicklistNode 插入数据的判断逻辑，你可以看下。\n\nunsigned int new_sz = node->sz + sz + ziplist_overhead;\nif (likely(_quicklistNodeSizeMeetsOptimizationRequirement(new_sz, fill)))\n    return 1;\nelse if (!sizeMeetsSafetyLimit(new_sz))\n    return 0;\nelse if ((int)node->count < fill)\n    return 1;\nelse\n    return 0;\n\n\n这样一来，quicklist 通过控制每个 quicklistNode 中，ziplist 的大小或是元素个数，就有效减少了在 ziplist 中新增或修改元素后，发生连锁更新的情况，从而提供了更好的访问性能。\n\n而 Redis 除了设计了 quicklist 结构来应对 ziplist 的问题以外，还在 5.0 版本中新增了 listpack 数据结构，用来彻底避免连锁更新。下面我们就继续来学习下它的设计实现思路。\n\n\n# listpack 设计与实现\n\nlistpack 也叫紧凑列表，它的特点就是用一块连续的内存空间来紧凑地保存数据，同时为了节省内存空间，listpack 列表项使用了多种编码方式，来表示不同长度的数据，这些数据包括整数和字符串。\n\n和 listpack 相关的实现文件是listpack.c，头文件包括listpack.h和listpack_malloc.h。我们先来看下 listpack 的创建函数 lpNew，因为从这个函数的代码逻辑中，我们可以了解到 listpack 的整体结构。\n\nlpNew 函数创建了一个空的 listpack，一开始分配的大小是 LP_HDR_SIZE 再加 1 个字节。LP_HDR_SIZE 宏定义是在 listpack.c 中，它默认是 6 个字节，其中 4 个字节是记录 listpack 的总字节数，2 个字节是记录 listpack 的元素数量。\n\n此外，listpack 的最后一个字节是用来标识 listpack 的结束，其默认值是宏定义 LP_EOF。和 ziplist 列表项的结束标记一样，LP_EOF 的值也是 255。\n\nunsigned char *lpNew(void) {\n    //分配LP_HRD_SIZE+1\n    unsigned char *lp = lp_malloc(LP_HDR_SIZE+1);\n    if (lp == NULL) return NULL;\n    //设置listpack的大小\n    lpSetTotalBytes(lp,LP_HDR_SIZE+1);\n    //设置listpack的元素个数，初始值为0\n    lpSetNumElements(lp,0);\n    //设置listpack的结尾标识为LP_EOF，值为255\n    lp[LP_HDR_SIZE] = LP_EOF;\n    return lp;\n}\n\n\n你可以看看下面这张图，展示的就是大小为 LP_HDR_SIZE 的 listpack 头和值为 255 的 listpack 尾。当有新元素插入时，该元素会被插在 listpack 头和尾之间。\n\n\n\n好了，了解了 listpack 的整体结构后，我们再来看下 listpack 列表项的设计。\n\n和 ziplist 列表项类似，listpack 列表项也包含了元数据信息和数据本身。不过，为了避免 ziplist 引起的连锁更新问题，listpack 中的每个列表项不再像 ziplist 列表项那样，保存其前一个列表项的长度，它只会包含三个方面内容，分别是当前元素的编码类型（entry-encoding）、元素数据 (entry-data)，以及编码类型和元素数据这两部分的长度 (entry-len)，如下图所示。\n\n\n\n这里，关于 listpack 列表项的设计，你需要重点掌握两方面的要点，分别是列表项元素的编码类型，以及列表项避免连锁更新的方法。下面我就带你具体了解下。\n\n\n# listpack 列表项编码方法\n\n我们先来看下 listpack 元素的编码类型。如果你看了 listpack.c 文件，你会发现该文件中有大量类似 LP_ENCODINGXX_BIT_INT 和 LP_ENCODINGXX_BIT_STR 的宏定义，如下所示：\n\n#define LP_ENCODING_7BIT_UINT 0\n#define LP_ENCODING_6BIT_STR 0x80\n#define LP_ENCODING_13BIT_INT 0xC0\n...\n#define LP_ENCODING_64BIT_INT 0xF4\n#define LP_ENCODING_32BIT_STR 0xF0\n\n\n这些宏定义其实就对应了 listpack 的元素编码类型。具体来说，listpack 元素会对不同长度的整数和字符串进行编码，这里我们分别来看下。\n\n首先，对于整数编码来说，当 listpack 元素的编码类型为 LP_ENCODING_7BIT_UINT 时，表示元素的实际数据是一个 7 bit 的无符号整数。又因为 LP_ENCODING_7BIT_UINT 本身的宏定义值为 0，所以编码类型的值也相应为 0，占 1 个 bit。\n\n此时，编码类型和元素实际数据共用 1 个字节，这个字节的最高位为 0，表示编码类型，后续的 7 位用来存储 7 bit 的无符号整数，如下图所示：\n\n\n\n而当编码类型为 LP_ENCODING_13BIT_INT 时，这表示元素的实际数据是 13 bit 的整数。同时，因为 LP_ENCODING_13BIT_INT 的宏定义值为 0xC0，转换为二进制值是 1100 0000，所以，这个二进制值中的后 5 位和后续的 1 个字节，共 13 位，会用来保存 13bit 的整数。而该二进制值中的前 3 位 110，则用来表示当前的编码类型。我画了下面这张图，你可以看下。\n\n\n\n好，在了解了 LP_ENCODING_7BIT_UINT 和 LP_ENCODING_13BIT_INT 这两种编码类型后，剩下的 LP_ENCODING_16BIT_INT、LP_ENCODING_24BIT_INT、LP_ENCODING_32BIT_INT 和 LP_ENCODING_64BIT_INT，你应该也就能知道它们的编码方式了。\n\n这四种类型是分别用 2 字节（16 bit）、3 字节（24 bit）、4 字节（32 bit）和 8 字节（64 bit）来保存整数数据。同时，它们的编码类型本身占 1 字节，编码类型值分别是它们的宏定义值。\n\n然后，对于字符串编码来说，一共有三种类型，分别是 LP_ENCODING_6BIT_STR、LP_ENCODING_12BIT_STR 和 LP_ENCODING_32BIT_STR。从刚才的介绍中，你可以看到，整数编码类型名称中 BIT 前面的数字，表示的是整数的长度。因此类似的，字符串编码类型名称中 BIT 前的数字，表示的就是字符串的长度。\n\n比如，当编码类型为 LP_ENCODING_6BIT_STR 时，编码类型占 1 字节。该类型的宏定义值是 0x80，对应的二进制值是 1000 0000，这其中的前 2 位是用来标识编码类型本身，而后 6 位保存的是字符串长度。然后，列表项中的数据部分保存了实际的字符串。\n\n下面的图展示了三种字符串编码类型和数据的布局，你可以看下。\n\n\n\n\n# listpack 避免连锁更新的实现方式\n\n最后，我们再来了解下 listpack 列表项是如何避免连锁更新的。\n\n在 listpack 中，因为每个列表项只记录自己的长度，而不会像 ziplist 中的列表项那样，会记录前一项的长度。所以，当我们在 listpack 中新增或修改元素时，实际上只会涉及每个列表项自己的操作，而不会影响后续列表项的长度变化，这就避免了连锁更新。\n\n不过，你可能会有疑问：如果 listpack 列表项只记录当前项的长度，那么 listpack 支持从左向右正向查询列表，或是从右向左反向查询列表吗？\n\n其实，listpack 是能支持正、反向查询列表的。\n\n当应用程序从左向右正向查询 listpack 时，我们可以先调用 lpFirst 函数。该函数的参数是指向 listpack 头的指针，它在执行时，会让指针向右偏移 LP_HDR_SIZE 大小，也就是跳过 listpack 头。你可以看下 lpFirst 函数的代码，如下所示：\n\nunsigned char *lpFirst(unsigned char *lp) {\n    lp += LP_HDR_SIZE; //跳过listpack头部6个字节\n    if (lp[0] == LP_EOF) return NULL;  //如果已经是listpack的末尾结束字节，则返回NULL\n    return lp;\n}\n\n\n然后，再调用 lpNext 函数，该函数的参数包括了指向 listpack 某个列表项的指针。lpNext 函数会进一步调用 lpSkip 函数，并传入当前列表项的指针，如下所示：\n\nunsigned char *lpNext(unsigned char *lp, unsigned char *p) {\n    ...\n    p = lpSkip(p);  //调用lpSkip函数，偏移指针指向下一个列表项\n    if (p[0] == LP_EOF) return NULL;\n    return p;\n}\n\n\n最后，lpSkip 函数会先后调用 lpCurrentEncodedSize 和 lpEncodeBacklen 这两个函数。\n\nlpCurrentEncodedSize 函数是根据当前列表项第 1 个字节的取值，来计算当前项的编码类型，并根据编码类型，计算当前项编码类型和实际数据的总长度。然后，lpEncodeBacklen 函数会根据编码类型和实际数据的长度之和，进一步计算列表项最后一部分 entry-len 本身的长度。\n\n这样一来，lpSkip 函数就知道当前项的编码类型、实际数据和 entry-len 的总长度了，也就可以将当前项指针向右偏移相应的长度，从而实现查到下一个列表项的目的。\n\n下面代码展示了 lpEncodeBacklen 函数的基本计算逻辑，你可以看下。\n\nunsigned long lpEncodeBacklen(unsigned char *buf, uint64_t l) {\n    //编码类型和实际数据的总长度小于等于127，entry-len长度为1字节\n    if (l <= 127) {\n        ...\n        return 1;\n    } else if (l < 16383) { //编码类型和实际数据的总长度大于127但小于16383，entry-len长度为2字节\n       ...\n        return 2;\n    } else if (l < 2097151) {//编码类型和实际数据的总长度大于16383但小于2097151，entry-len长度为3字节\n       ...\n        return 3;\n    } else if (l < 268435455) { //编码类型和实际数据的总长度大于2097151但小于268435455，entry-len长度为4字节\n        ...\n        return 4;\n    } else { //否则，entry-len长度为5字节\n       ...\n        return 5;\n    }\n}\n\n\n我也画了一张图，展示了从左向右遍历 listpack 的基本过程，你可以再回顾下。\n\n\n\n好，了解了从左向右正向查询 listpack，我们再来看下从右向左反向查询 listpack。\n\n首先，我们根据 listpack 头中记录的 listpack 总长度，就可以直接定位到 listapck 的尾部结束标记。然后，我们可以调用 lpPrev 函数，该函数的参数包括指向某个列表项的指针，并返回指向当前列表项前一项的指针。\n\nlpPrev 函数中的关键一步就是调用 lpDecodeBacklen 函数。lpDecodeBacklen 函数会从右向左，逐个字节地读取当前列表项的 entry-len。\n\n那么，lpDecodeBacklen 函数如何判断 entry-len 是否结束了呢？\n\n这就依赖于 entry-len 的编码方式了。entry-len 每个字节的最高位，是用来表示当前字节是否为 entry-len 的最后一个字节，这里存在两种情况，分别是：\n\n * 最高位为 1，表示 entry-len 还没有结束，当前字节的左边字节仍然表示 entry-len 的内容；\n * 最高位为 0，表示当前字节已经是 entry-len 最后一个字节了。\n\n而 entry-len 每个字节的低 7 位，则记录了实际的长度信息。这里你需要注意的是，entry-len 每个字节的低 7 位采用了大端模式存储，也就是说，entry-len 的低位字节保存在内存高地址上。\n\n我画了下面这张图，展示了 entry-len 这种特别的编码方式，你可以看下。\n\n\n\n实际上，正是因为有了 entry-len 的特别编码方式，lpDecodeBacklen 函数就可以从当前列表项起始位置的指针开始，向左逐个字节解析，得到前一项的 entry-len 值。这也是 lpDecodeBacklen 函数的返回值。而从刚才的介绍中，我们知道 entry-len 记录了编码类型和实际数据的长度之和。\n\n因此，lpPrev 函数会再调用 lpEncodeBacklen 函数，来计算得到 entry-len 本身长度，这样一来，我们就可以得到前一项的总长度，而 lpPrev 函数也就可以将指针指向前一项的起始位置了。所以按照这个方法，listpack 就实现了从右向左的查询功能。\n\n\n# 对比\n\n特性       ZIPLIST                QUICKLIST   LISTPACK\n设计复杂度    较为复杂，包含前一个节点长度字段       复杂          更加简化，没有前一个节点长度字段\n内存占用     存在冗余字段，内存利用率较低         高           更加紧凑，内存占用低\n操作复杂度    插入、删除操作需要更新前向节点长度，较慢   中           操作简单高效，无需处理前向节点长度，但是也要移动\n内存移动问题   频繁插入删除可能导致大范围内存移动      中           仍存在内存移动问题，但操作更加简单\n适用场景     小 hash，小 zset          list        小 hash，小 zset，小 stream\n\n\n\n笔记\n\necho 认为，ziplist 和 listpack 适用于元素较少时的存储，一旦元素变多就需采用 quicklist 这种类似于 linkedlist 的结构来进行存储，但是 quicklist 的 node 有两种选择，分别是 ziplist 和 listpack，在最新的版本中貌似都是采用 listpack 来实现的\n\n\n# 总结\n\n本文从 ziplist 的设计不足出发，到学习 quicklist 和 listpack 的设计思想\n\n你要知道，ziplist 的不足主要在于一旦 ziplist 中元素个数多了，它的查找效率就会降低。而且如果在 ziplist 里新增或修改数据，ziplist 占用的内存空间还需要重新分配；更糟糕的是，ziplist 新增某个元素或修改某个元素时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起连锁更新问题，导致每个元素的空间都要重新分配，这就会导致 ziplist 的访问性能下降。\n\n所以，为了应对 ziplist 的问题，Redis 先是在 3.0 版本中设计实现了 quicklist。quicklist 结构在 ziplist 基础上，使用链表将 ziplist 串联起来，链表的每个元素就是一个 ziplist。这种设计减少了数据插入时内存空间的重新分配，以及内存数据的拷贝。同时，quicklist 限制了每个节点上 ziplist 的大小，一旦一个 ziplist 过大，就会采用新增 quicklist 节点的方法。\n\n不过，又因为 quicklist 使用 quicklistNode 结构指向每个 ziplist，无疑增加了内存开销。为了减少内存开销，并进一步避免 ziplist 连锁更新问题，Redis 在 5.0 版本中，就设计实现了 listpack 结构。listpack 结构沿用了 ziplist 紧凑型的内存布局，把每个元素都紧挨着放置\n\nlistpack 中每个列表项不再包含前一项的长度了，因此当某个列表项中的数据发生变化，导致列表项长度变化时，其他列表项的长度是不会受影响的，因而这就避免了 ziplist 面临的连锁更新问题。\n\n总而言之，Redis 在内存紧凑型列表的设计与实现上，从 ziplist 到 quicklist，再到 listpack，你可以看到 Redis 在内存空间开销和访问性能之间的设计取舍，这一系列的设计变化，是非常值得你学习的\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 为什么 redis 设计了三种类似但不同的底层数据结构 ziplist、quicklist、listpack？\n 2. ziplist 为什么会面临查找效率降低和内存连锁更新的问题？\n 3. quicklist 是如何解决 ziplist 的性能瓶颈的？\n 4. listpack 是如何避免 ziplist 的连锁更新问题的？\n 5. 如何在实际开发中选择适合的 redis list 底层结构，以平衡内存使用和性能？\n 6. ziplist 在 redis 设计中有哪些优点被保留了下来，即使它存在性能不足？\n 7. 当数据插入量大时，quicklist 如何避免内存频繁重新分配而保持高效？\n\n\n# 前言\n\nredis 中的 list 的底层有三种数据结构\n\n * ziplist\n * quicklist\n * listpack\n\nziplist 的最大特点，就是它被设计成一种 内存紧凑型 的数据结构，占用一块连续的内存空间，以达到节省内存的目的\n\n但是，在计算机系统中，任何一个设计都是有利有弊的。对于 ziplist 来说，这个道理同样成立。\n\n虽然 ziplist 节省了内存开销，可它也存在两个设计代价\n\n * 「不能保存过多的元素」，否则访问性能会降低\n * 「不能保存过大的元素」，否则容易导致内存重新分配，甚至可能引发连锁更新的问题。所谓的连锁更新，简单来说，就是 ziplist 中的每一项都要被重新分配内存空间，造成 ziplist 的性能降低\n\n因此，针对 ziplist 在设计上的不足，redis 在演进的过程中，新增设计了两种数据结构\n\n * quicklist\n * istpack\n\n它们设计目标，是 尽可能地保持 ziplist 节省内存的优势，同时避免 ziplist 潜在的性能下降问题\n\n\n# ziplist 设计与实现\n\n\n# ziplist 和 整数集合 的设计\n\n首先你要知道，list、hash 和 sorted set 这三种数据类型，都可以使用压缩列表（ziplist）来保存数据。压缩列表的函数定义和实现代码分别在 ziplist.h 和 ziplist.c 中\n\n不过，我们在 ziplist.h 文件中其实根本看不到压缩列表的结构体定义。这是因为压缩列表本身就是一块连续的内存空间，它通过使用不同的编码来保存数据\n\n这里为了方便理解压缩列表的设计与实现，我们先来看看它的创建函数 ziplistnew，如下所示：\n\nunsigned char *ziplistnew(void) {\n    //初始分配的大小\n    unsigned int bytes = ziplist_header_size+ziplist_end_size;\n    unsigned char *zl = zmalloc(bytes);\n    …\n   //将列表尾设置为zip_end\n    zl[bytes-1] = zip_end;\n    return zl;\n}\n\n\n实际上，ziplistnew 函数的逻辑很简单，就是创建一块连续的内存空间，大小为 ziplist_header_size 和 ziplist_end_size 的总和，然后再把该连续空间的最后一个字节赋值为 zip_end，表示列表结束。\n\n这三个宏分别表示 ziplist 的列表头大小、列表尾大小和列表尾字节内容\n\n//ziplist的列表头大小，包括2个32 bits整数和1个16bits整数，分别表示压缩列表的总字节数，列表最后一个元素的离列表头的偏移，以及列表中的元素个数\n#define ziplist_header_size     (sizeof(uint32_t)*2+sizeof(uint16_t))\n//ziplist的列表尾大小，包括1个8 bits整数，表示列表结束。\n#define ziplist_end_size        (sizeof(uint8_t))\n//ziplist的列表尾字节内容\n#define zip_end 255\n\n\n那么，在创建一个新的 ziplist 后，该列表的内存布局就如下图所示。注意，此时列表中还没有实际的数据。\n\n\n\n然后，当我们往 ziplist 中插入数据时，ziplist 就会根据数据是字符串还是整数，以及它们的大小进行不同的编码。这种根据数据大小进行相应编码的设计思想，正是 redis 为了节省内存而采用的\n\nziplist 列表项包括三部分内容，分别是前一项的长度（prevlen）、当前项长度信息的编码结果（encoding），以及当前项的实际数据（data）。下面的图展示了列表项的结构（图中除列表项之外的内容分别是 ziplist 内存空间的起始和尾部）。\n\n\n\n实际上，所谓的编码技术，就是指 用不同数量的字节来表示保存的信息。在 ziplist 中，编码技术主要应用在列表项中的 prevlen 和 encoding 这两个元数据上。而当前项的实际数据 data，则正常用整数或是字符串来表示。\n\n所以这里，我们就先来看下 prevlen 的编码设计。ziplist 中会包含多个列表项，每个列表项都是紧挨着彼此存放的，如下图所示。\n\n\n\n而为了方便查找，每个列表项中都会记录前一项的长度。因为每个列表项的长度不一样，所以如果使用相同的字节大小来记录 prevlen，就会造成内存空间浪费。\n\n我给你举个例子，假设我们统一使用 4 字节记录 prevlen，如果前一个列表项只是一个字符串“redis”，长度为 5 个字节，那么我们用 1 个字节（8 bits）就能表示 256 字节长度（2 的 8 次方等于 256）的字符串了。此时，prevlen 用 4 字节记录，其中就有 3 字节是浪费掉了。\n\n好，我们再回过头来看，ziplist 在对 prevlen 编码时，会先调用 zipstorepreventrylength 函数，用于判断前一个列表项是否小于 254 字节。如果是的话，那么 prevlen 就使用 1 字节表示；否则，zipstorepreventrylength 函数就调用 zipstorepreventrylengthlarge 函数进一步编码。这部分代码如下所示：\n\n//判断prevlen的长度是否小于zip_big_prevlen，zip_big_prevlen等于254\nif (len < zip_big_prevlen) {\n   //如果小于254字节，那么返回prevlen为1字节\n   p[0] = len;\n   return 1;\n} else {\n   //否则，调用zipstorepreventrylengthlarge进行编码\n   return zipstorepreventrylengthlarge(p,len);\n}\n\n\n也就是说，zipstorepreventrylengthlarge 函数会先将 prevlen 的第 1 字节设置为 254，然后使用内存拷贝函数 memcpy，将前一个列表项的长度值拷贝至 prevlen 的第 2 至第 5 字节。最后，zipstorepreventrylengthlarge 函数返回 prevlen 的大小，为 5 字节。\n\nif (p != null) {\n    //将prevlen的第1字节设置为zip_big_prevlen，即254\n    p[0] = zip_big_prevlen;\n  //将前一个列表项的长度值拷贝至prevlen的第2至第5字节，其中sizeof(len)的值为4\n    memcpy(p+1,&len,sizeof(len));\n    …\n}\n//返回prevlen的大小，为5字节\nreturn 1+sizeof(len);\n\n\n好，在了解了 prevlen 使用 1 字节和 5 字节两种编码方式后，我们再来学习下 encoding 的编码方法。\n\n我们知道，一个列表项的实际数据，既可以是整数也可以是字符串。整数可以是 16、32、64 等字节长度，同时字符串的长度也可以大小不一。\n\n所以，ziplist 在 zipstoreentryencoding 函数中，针对整数和字符串，就分别使用了不同字节长度的编码结果。下面的代码展示了 zipstoreentryencoding 函数的部分代码，你可以看到当数据是不同长度字符串或是整数时，编码结果的长度 len 大小不同。\n\n//默认编码结果是1字节\nunsigned char len = 1;\n//如果是字符串数据\nif (zip_is_str(encoding)) {\n    //字符串长度小于等于63字节（16进制为0x3f）\n    if (rawlen <= 0x3f) {\n        //默认编码结果是1字节\n        …\n    }\n    //字符串长度小于等于16383字节（16进制为0x3fff）\n    else if (rawlen <= 0x3fff) {\n        //编码结果是2字节\n        len += 1;\n        …\n    }\n    //字符串长度大于16383字节\n\n    else {\n        //编码结果是5字节\n        len += 4;\n        …\n    }\n} else {\n    /* 如果数据是整数，编码结果是1字节*/\n    if (!p) return len;\n    ...\n}\n\n\n简而言之，针对不同长度的数据，使用不同大小的元数据信息（prevlen 和 encoding），这种方法可以有效地节省内存开销\n\n除了 ziplist 之外，redis 还设计了一个内存友好的数据结构，这就是整数集合（intset），它是作为底层结构来实现 set 数据类型的。\n\n和 sds 嵌入式字符串、ziplist 类似，整数集合也是一块连续的内存空间，这一点我们从整数集合的定义中就可以看到。intset.h 和 intset.c 分别包括了整数集合的定义和实现\n\n下面的代码展示了 intset 的结构定义。我们可以看到，整数集合结构体中记录数据的部分，就是一个 int8_t 类型的整数数组 contents。从内存使用的角度来看，整数数组就是一块连续内存空间，所以这样就避免了内存碎片，并提升了内存使用效率\n\ntypedef struct intset {\n    uint32_t encoding;\n    uint32_t length;\n    int8_t contents[];\n} intset;\n\n\n\n# 扩展：节省内存的数据访问\n\n我们知道，在 redis 实例运行时，有些数据是会被经常访问的，比如常见的整数，redis 协议中常见的回复信息，包括操作成功（“ok”字符串）、操作失败（err），以及常见的报错信息。\n\n所以，为了避免在内存中反复创建这些经常被访问的数据，redis 就采用了共享对象的设计思想。这个设计思想很简单，就是把这些常用数据创建为共享对象，当上层应用需要访问它们时，直接读取就行。\n\n现在我们就来做个假设。有 1000 个客户端，都要保存“3”这个整数。如果 redis 为每个客户端，都创建了一个值为 3 的 redisobject，那么内存中就会有大量的冗余。而使用了共享对象方法后，redis 在内存中只用保存一个 3 的 redisobject 就行，这样就有效节省了内存空间。\n\n以下代码展示的是 server.c 文件中，创建共享对象的函数 createsharedobjects，你可以看下。\n\nvoid createsharedobjects(void) {\n   …\n   //常见回复信息\n   shared.ok = createobject(obj_string,sdsnew("+ok\\r\\n"));\n   shared.err = createobject(obj_string,sdsnew("-err\\r\\n"));\n   …\n   //常见报错信息\n shared.nokeyerr = createobject(obj_string,sdsnew("-err no such key\\r\\n"));\n shared.syntaxerr = createobject(obj_string,sdsnew("-err syntax error\\r\\n"));\n   //0到9999的整数\n   for (j = 0; j < obj_shared_integers; j++) {\n        shared.integers[j] =\n          makeobjectshared(createobject(obj_string,(void*)(long)j));\n        …\n    }\n   …\n}\n\n\n\n# ziplist 的不足\n\n你已经知道，一个 ziplist 数据结构在内存中的布局，就是一块连续的内存空间。这块空间的起始部分是大小固定的 10 字节元数据，其中记录了 ziplist 的总字节数、最后一个元素的偏移量以及列表元素的数量，而这 10 字节后面的内存空间则保存了实际的列表数据。在 ziplist 的最后部分，是一个 1 字节的标识（固定为 255），用来表示 ziplist 的结束，如下图所示：\n\n\n\n不过，虽然 ziplist 通过紧凑的内存布局来保存数据，节省了内存空间，但是 ziplist 也面临着随之而来的两个不足：\n\n * 查找复杂度高\n * 潜在的连锁更新风险\n\n那么下面，我们就分别来了解下这两个问题\n\n# 查找复杂度高\n\n因为 ziplist 头尾元数据的大小是固定的，所以可以很快找到 首部元素和尾部元素，但问题是\n\n * 当要查找中间元素时，ziplist 就得从列表头或列表尾遍历才行\n * 更糟糕的是，如果 ziplist 里面保存的是字符串，ziplist 在查找某个元素时，还需要逐一判断元素的每个字符，这样又进一步增加了复杂度\n * ziplist 在插入元素时，如果内存空间不够了，ziplist 还需要重新分配一块连续的内存空间，而这还会进一步引发连锁更新的问题\n\n也正因为如此，我们在使用 ziplist 保存 hash 或 sorted set 数据时，都会在 redis.conf 文件中，通过 hash-max-ziplist-entries 和 zset-max-ziplist-entries 两个参数，来控制保存在 ziplist 中的元素个数\n\n# 连锁更新风险\n\n我们知道，因为 ziplist 必须使用一块连续的内存空间来保存数据，所以当新插入一个元素时，ziplist 就需要计算其所需的空间大小，并申请相应的内存空间。这一系列操作，我们可以从 ziplist 的元素插入函数 __ziplistinsert 中看到。\n\n__ziplistinsert 函数首先会计算获得当前 ziplist 的长度，这个步骤通过 ziplist_bytes 宏定义就可以完成，如下所示。同时，该函数还声明了 reqlen 变量，用于记录插入元素后所需的新增空间大小。\n\n//获取当前ziplist长度curlen；声明reqlen变量，用来记录新插入元素所需的长度\nsize_t curlen = intrev32ifbe(ziplist_bytes(zl)), reqlen;\n\n\n然后，__ziplistinsert 函数会判断当前要插入的位置是否是列表末尾。如果不是末尾，那么就需要获取位于当前插入位置的元素的 prevlen 和 prevlensize。这部分代码如下所示：\n\n//如果插入的位置不是ziplist末尾，则获取前一项长度\nif (p[0] != zip_end) {\n    zip_decode_prevlen(p, prevlensize, prevlen);\n} else {\n    …\n}\n\n\n实际上，在 ziplist 中，每一个元素都会记录其前一项的长度，也就是 prevlen。然后，为了节省内存开销，ziplist 会使用不同的空间记录 prevlen，这个 prevlen 空间大小就是 prevlensize。\n\n举个简单的例子，当在一个元素 a 前插入一个新的元素 b 时，a 的 prevlen 和 prevlensize 都要根据 b 的长度进行相应的变化。\n\n那么现在，我们假设 a 的 prevlen 原本只占用 1 字节（也就是 prevlensize 等于 1），而能记录的前一项长度最大为 253 字节。此时，如果 b 的长度超过了 253 字节，a 的 prevlen 就需要使用 5 个字节来记录（prevlen 具体的编码方式，你可以复习回顾下第 4 讲），这样就需要申请额外的 4 字节空间了。不过，如果元素 b 的插入位置是列表末尾，那么插入元素 b 时，我们就不用考虑后面元素的 prevlen 了\n\n\n\n因此，为了保证 ziplist 有足够的内存空间，来保存插入元素以及插入位置元素的 prevlen 信息，__ziplistinsert 函数在获得插入位置元素的 prevlen 和 prevlensize 后，紧接着就会计算插入元素的长度。\n\n现在我们已知，一个 ziplist 元素包括了 prevlen、encoding 和实际数据 data 三个部分。所以，在计算插入元素的所需空间时，__ziplistinsert 函数也会分别计算这三个部分的长度。这个计算过程一共可以分成四步来完成。\n\n * 第一步，计算实际插入元素的长度。\n\n首先你要知道，这个计算过程和插入元素是整数还是字符串有关。__ziplistinsert 函数会先调用 ziptryencoding 函数，这个函数会判断插入元素是否为整数。如果是整数，就按照不同的整数大小，计算 encoding 和实际数据 data 各自所需的空间；如果是字符串，那么就先把字符串长度记录为所需的新增空间大小。这一过程的代码如下所示：\n\n  if (ziptryencoding(s,slen,&value,&encoding)) {\n          reqlen = zipintsize(encoding);\n      } else {\n          reqlen = slen;\n      }\n\n\n * 第二步，调用 zipstorepreventrylength 函数，将插入位置元素的 prevlen 也计算到所需空间中。\n\n这是因为在插入元素后，__ziplistinsert 函数可能要为插入位置的元素分配新增空间。这部分代码如下所示：\n\nreqlen += zipstorepreventrylength(null,prevlen);\n\n\n * 第三步，调用 zipstoreentryencoding 函数，根据字符串的长度，计算相应 encoding 的大小。\n\n在刚才的第一步中，**ziplistinsert 函数对于字符串数据，只是记录了字符串本身的长度，所以在第三步中，**ziplistinsert 函数还会调用 zipstoreentryencoding 函数，根据字符串的长度来计算相应的 encoding 大小，如下所示：\n\nreqlen += zipstoreentryencoding(null,encoding,slen);\n\n\n好了，到这里，__ziplistinsert 函数就已经在 reqlen 变量中，记录了插入元素的 prevlen 长度、encoding 大小，以及实际数据 data 的长度。这样一来，插入元素的整体长度就有了，这也是插入位置元素的 prevlen 所要记录的大小。\n\n * 第四步，调用 zipprevlenbytediff 函数，判断插入位置元素的 prevlen 和实际所需的 prevlen 大小。\n\n最后，__ziplistinsert 函数会调用 zipprevlenbytediff 函数，用来判断插入位置元素的 prevlen 和实际所需的 prevlen，这两者间的大小差别。这部分代码如下所示，prevlen 的大小差别是使用 nextdiff 来记录的：\n\nnextdiff = (p[0] != zip_end) ? zipprevlenbytediff(p,reqlen) : 0;\n\n\n那么在这里，如果 nextdiff 大于 0，就表明插入位置元素的空间不够，需要新增 nextdiff 大小的空间，以便能保存新的 prevlen。然后，__ziplistinsert 函数在新增空间时，就会调用 ziplistresize 函数，来重新分配 ziplist 所需的空间。\n\nziplistresize 函数接收的参数分别是待重新分配的 ziplist 和重新分配的空间大小。而 __ziplistinsert 函数传入的重新分配大小的参数，是三个长度之和。\n\n那么是哪三个长度之和呢？\n\n这三个长度分别是 ziplist 现有大小（curlen）、待插入元素自身所需的新增空间（reqlen），以及插入位置元素 prevlen 所需的新增空间（nextdiff）。下面的代码显示了 ziplistresize 函数的调用和参数传递逻辑：\n\nzl = ziplistresize(zl,curlen+reqlen+nextdiff);\n\n\n进一步，那么 ziplistresize 函数在获得三个长度总和之后，具体是如何扩容呢？\n\n我们可以进一步看下 ziplistresize 函数的实现，这个函数会调用 zrealloc 函数，来完成空间的重新分配，而重新分配的空间大小就是由传入参数 len 决定的。这样，我们就了解到了 ziplistresize 函数涉及到内存分配操作，因此如果我们往 ziplist 频繁插入过多数据的话，就可能引起多次内存分配，从而会对 redis 性能造成影响。\n\n下面的代码显示了 ziplistresize 函数的部分实现，你可以看下。\n\nunsigned char *ziplistresize(unsigned char *zl, unsigned int len) {\n    //对zl进行重新内存空间分配，重新分配的大小是len\n    zl = zrealloc(zl,len);\n    …\n    zl[len-1] = zip_end;\n    return zl;\n}\n\n\n好了，到这里，我们就了解了 ziplist 在新插入元素时，会计算其所需的新增空间，并进行重新分配。而当新插入的元素较大时，就会引起插入位置的元素 prevlensize 增加，进而就会导致插入位置的元素所占空间也增加。\n\n而如此一来，这种空间新增就会引起连锁更新的问题。\n\n实际上，所谓的连锁更新，就是指当一个元素插入后，会引起当前位置元素新增 prevlensize 的空间。而当前位置元素的空间增加后，又会进一步引起该元素的后续元素，其 prevlensize 所需空间的增加。\n\n这样，一旦插入位置后续的所有元素，都会因为前序元素的 prevlenszie 增加，而引起自身空间也要增加，这种每个元素的空间都需要增加的现象，就是连锁更新。我画了下面这张图，你可以看下。\n\n\n\n连锁更新一旦发生，就会导致 ziplist 占用的内存空间要多次重新分配，这就会直接影响到 ziplist 的访问性能。\n\n所以说，虽然 ziplist 紧凑型的内存布局能节省内存开销，但是如果保存的元素数量增加了，或是元素变大了，ziplist 就会面临性能问题。那么，有没有什么方法可以避免 ziplist 的问题呢\n\n这就是接下来我要给你介绍的 quicklist 和 listpack，这两种数据结构的设计思想了\n\n\n# quicklist 设计与实现\n\n我们先来学习下 quicklist 的实现思路。\n\nquicklist 的设计，其实是结合了链表和 ziplist 各自的优势。简单来说，一个 quicklist 就是一个链表，而链表中的每个元素又是一个 ziplist\n\n我们来看下 quicklist 的数据结构，这是在quicklist.h文件中定义的，而 quicklist 的具体实现是在quicklist.c文件中。\n\n首先，quicklist 元素的定义，也就是 quicklistnode。因为 quicklist 是一个链表，所以每个 quicklistnode 中，都包含了分别指向它前序和后序节点的指针prev 和 next**。同时，每个 quicklistnode 又是一个 ziplist，所以，在 quicklistnode 的结构体中，还有指向 ziplist 的**指针 zl。\n\n此外，quicklistnode 结构体中还定义了一些属性，比如 ziplist 的字节大小、包含的元素个数、编码格式、存储方式等。下面的代码显示了 quicklistnode 的结构体定义，你可以看下。\n\ntypedef struct quicklistnode {\n    struct quicklistnode *prev;     //前一个quicklistnode\n    struct quicklistnode *next;     //后一个quicklistnode\n    unsigned char *zl;              //quicklistnode指向的ziplist\n    unsigned int sz;                //ziplist的字节大小\n    unsigned int count : 16;        //ziplist中的元素个数\n    unsigned int encoding : 2;   //编码格式，原生字节数组或压缩存储\n    unsigned int container : 2;  //存储方式\n    unsigned int recompress : 1; //数据是否被压缩\n    unsigned int attempted_compress : 1; //数据能否被压缩\n    unsigned int extra : 10; //预留的bit位\n} quicklistnode;\n\n\n了解了 quicklistnode 的定义，我们再来看下 quicklist 的结构体定义。\n\nquicklist 作为一个链表结构，在它的数据结构中，是定义了整个 quicklist 的头、尾指针，这样一来，我们就可以通过 quicklist 的数据结构，来快速定位到 quicklist 的链表头和链表尾。\n\n此外，quicklist 中还定义了 quicklistnode 的个数、所有 ziplist 的总元素个数等属性。quicklist 的结构定义如下所示：\n\ntypedef struct quicklist {\n    quicklistnode *head;      //quicklist的链表头\n    quicklistnode *tail;      //quicklist的链表尾\n    unsigned long count;     //所有ziplist中的总元素个数\n    unsigned long len;       //quicklistnodes的个数\n    ...\n} quicklist;\n\n\n然后，从 quicklistnode 和 quicklist 的结构体定义中，我们就能画出下面这张 quicklist 的示意图。\n\n\n\n而也正因为 quicklist 采用了链表结构，所以当插入一个新的元素时，quicklist 首先就会检查插入位置的 ziplist 是否能容纳该元素，这是通过 _quicklistnodeallowinsert 函数来完成判断的。\n\n_quicklistnodeallowinsert 函数会计算新插入元素后的大小（new_sz），这个大小等于 quicklistnode 的当前大小（node->sz）、插入元素的大小（sz），以及插入元素后 ziplist 的 prevlen 占用大小。\n\n在计算完大小之后，_quicklistnodeallowinsert 函数会依次判断新插入的数据大小（sz）是否满足要求，即单个 ziplist 是否不超过 8kb，或是单个 ziplist 里的元素个数是否满足要求。\n\n只要这里面的一个条件能满足，quicklist 就可以在当前的 quicklistnode 中插入新元素，否则 quicklist 就会新建一个 quicklistnode，以此来保存新插入的元素。\n\n下面代码显示了是否允许在当前 quicklistnode 插入数据的判断逻辑，你可以看下。\n\nunsigned int new_sz = node->sz + sz + ziplist_overhead;\nif (likely(_quicklistnodesizemeetsoptimizationrequirement(new_sz, fill)))\n    return 1;\nelse if (!sizemeetssafetylimit(new_sz))\n    return 0;\nelse if ((int)node->count < fill)\n    return 1;\nelse\n    return 0;\n\n\n这样一来，quicklist 通过控制每个 quicklistnode 中，ziplist 的大小或是元素个数，就有效减少了在 ziplist 中新增或修改元素后，发生连锁更新的情况，从而提供了更好的访问性能。\n\n而 redis 除了设计了 quicklist 结构来应对 ziplist 的问题以外，还在 5.0 版本中新增了 listpack 数据结构，用来彻底避免连锁更新。下面我们就继续来学习下它的设计实现思路。\n\n\n# listpack 设计与实现\n\nlistpack 也叫紧凑列表，它的特点就是用一块连续的内存空间来紧凑地保存数据，同时为了节省内存空间，listpack 列表项使用了多种编码方式，来表示不同长度的数据，这些数据包括整数和字符串。\n\n和 listpack 相关的实现文件是listpack.c，头文件包括listpack.h和listpack_malloc.h。我们先来看下 listpack 的创建函数 lpnew，因为从这个函数的代码逻辑中，我们可以了解到 listpack 的整体结构。\n\nlpnew 函数创建了一个空的 listpack，一开始分配的大小是 lp_hdr_size 再加 1 个字节。lp_hdr_size 宏定义是在 listpack.c 中，它默认是 6 个字节，其中 4 个字节是记录 listpack 的总字节数，2 个字节是记录 listpack 的元素数量。\n\n此外，listpack 的最后一个字节是用来标识 listpack 的结束，其默认值是宏定义 lp_eof。和 ziplist 列表项的结束标记一样，lp_eof 的值也是 255。\n\nunsigned char *lpnew(void) {\n    //分配lp_hrd_size+1\n    unsigned char *lp = lp_malloc(lp_hdr_size+1);\n    if (lp == null) return null;\n    //设置listpack的大小\n    lpsettotalbytes(lp,lp_hdr_size+1);\n    //设置listpack的元素个数，初始值为0\n    lpsetnumelements(lp,0);\n    //设置listpack的结尾标识为lp_eof，值为255\n    lp[lp_hdr_size] = lp_eof;\n    return lp;\n}\n\n\n你可以看看下面这张图，展示的就是大小为 lp_hdr_size 的 listpack 头和值为 255 的 listpack 尾。当有新元素插入时，该元素会被插在 listpack 头和尾之间。\n\n\n\n好了，了解了 listpack 的整体结构后，我们再来看下 listpack 列表项的设计。\n\n和 ziplist 列表项类似，listpack 列表项也包含了元数据信息和数据本身。不过，为了避免 ziplist 引起的连锁更新问题，listpack 中的每个列表项不再像 ziplist 列表项那样，保存其前一个列表项的长度，它只会包含三个方面内容，分别是当前元素的编码类型（entry-encoding）、元素数据 (entry-data)，以及编码类型和元素数据这两部分的长度 (entry-len)，如下图所示。\n\n\n\n这里，关于 listpack 列表项的设计，你需要重点掌握两方面的要点，分别是列表项元素的编码类型，以及列表项避免连锁更新的方法。下面我就带你具体了解下。\n\n\n# listpack 列表项编码方法\n\n我们先来看下 listpack 元素的编码类型。如果你看了 listpack.c 文件，你会发现该文件中有大量类似 lp_encodingxx_bit_int 和 lp_encodingxx_bit_str 的宏定义，如下所示：\n\n#define lp_encoding_7bit_uint 0\n#define lp_encoding_6bit_str 0x80\n#define lp_encoding_13bit_int 0xc0\n...\n#define lp_encoding_64bit_int 0xf4\n#define lp_encoding_32bit_str 0xf0\n\n\n这些宏定义其实就对应了 listpack 的元素编码类型。具体来说，listpack 元素会对不同长度的整数和字符串进行编码，这里我们分别来看下。\n\n首先，对于整数编码来说，当 listpack 元素的编码类型为 lp_encoding_7bit_uint 时，表示元素的实际数据是一个 7 bit 的无符号整数。又因为 lp_encoding_7bit_uint 本身的宏定义值为 0，所以编码类型的值也相应为 0，占 1 个 bit。\n\n此时，编码类型和元素实际数据共用 1 个字节，这个字节的最高位为 0，表示编码类型，后续的 7 位用来存储 7 bit 的无符号整数，如下图所示：\n\n\n\n而当编码类型为 lp_encoding_13bit_int 时，这表示元素的实际数据是 13 bit 的整数。同时，因为 lp_encoding_13bit_int 的宏定义值为 0xc0，转换为二进制值是 1100 0000，所以，这个二进制值中的后 5 位和后续的 1 个字节，共 13 位，会用来保存 13bit 的整数。而该二进制值中的前 3 位 110，则用来表示当前的编码类型。我画了下面这张图，你可以看下。\n\n\n\n好，在了解了 lp_encoding_7bit_uint 和 lp_encoding_13bit_int 这两种编码类型后，剩下的 lp_encoding_16bit_int、lp_encoding_24bit_int、lp_encoding_32bit_int 和 lp_encoding_64bit_int，你应该也就能知道它们的编码方式了。\n\n这四种类型是分别用 2 字节（16 bit）、3 字节（24 bit）、4 字节（32 bit）和 8 字节（64 bit）来保存整数数据。同时，它们的编码类型本身占 1 字节，编码类型值分别是它们的宏定义值。\n\n然后，对于字符串编码来说，一共有三种类型，分别是 lp_encoding_6bit_str、lp_encoding_12bit_str 和 lp_encoding_32bit_str。从刚才的介绍中，你可以看到，整数编码类型名称中 bit 前面的数字，表示的是整数的长度。因此类似的，字符串编码类型名称中 bit 前的数字，表示的就是字符串的长度。\n\n比如，当编码类型为 lp_encoding_6bit_str 时，编码类型占 1 字节。该类型的宏定义值是 0x80，对应的二进制值是 1000 0000，这其中的前 2 位是用来标识编码类型本身，而后 6 位保存的是字符串长度。然后，列表项中的数据部分保存了实际的字符串。\n\n下面的图展示了三种字符串编码类型和数据的布局，你可以看下。\n\n\n\n\n# listpack 避免连锁更新的实现方式\n\n最后，我们再来了解下 listpack 列表项是如何避免连锁更新的。\n\n在 listpack 中，因为每个列表项只记录自己的长度，而不会像 ziplist 中的列表项那样，会记录前一项的长度。所以，当我们在 listpack 中新增或修改元素时，实际上只会涉及每个列表项自己的操作，而不会影响后续列表项的长度变化，这就避免了连锁更新。\n\n不过，你可能会有疑问：如果 listpack 列表项只记录当前项的长度，那么 listpack 支持从左向右正向查询列表，或是从右向左反向查询列表吗？\n\n其实，listpack 是能支持正、反向查询列表的。\n\n当应用程序从左向右正向查询 listpack 时，我们可以先调用 lpfirst 函数。该函数的参数是指向 listpack 头的指针，它在执行时，会让指针向右偏移 lp_hdr_size 大小，也就是跳过 listpack 头。你可以看下 lpfirst 函数的代码，如下所示：\n\nunsigned char *lpfirst(unsigned char *lp) {\n    lp += lp_hdr_size; //跳过listpack头部6个字节\n    if (lp[0] == lp_eof) return null;  //如果已经是listpack的末尾结束字节，则返回null\n    return lp;\n}\n\n\n然后，再调用 lpnext 函数，该函数的参数包括了指向 listpack 某个列表项的指针。lpnext 函数会进一步调用 lpskip 函数，并传入当前列表项的指针，如下所示：\n\nunsigned char *lpnext(unsigned char *lp, unsigned char *p) {\n    ...\n    p = lpskip(p);  //调用lpskip函数，偏移指针指向下一个列表项\n    if (p[0] == lp_eof) return null;\n    return p;\n}\n\n\n最后，lpskip 函数会先后调用 lpcurrentencodedsize 和 lpencodebacklen 这两个函数。\n\nlpcurrentencodedsize 函数是根据当前列表项第 1 个字节的取值，来计算当前项的编码类型，并根据编码类型，计算当前项编码类型和实际数据的总长度。然后，lpencodebacklen 函数会根据编码类型和实际数据的长度之和，进一步计算列表项最后一部分 entry-len 本身的长度。\n\n这样一来，lpskip 函数就知道当前项的编码类型、实际数据和 entry-len 的总长度了，也就可以将当前项指针向右偏移相应的长度，从而实现查到下一个列表项的目的。\n\n下面代码展示了 lpencodebacklen 函数的基本计算逻辑，你可以看下。\n\nunsigned long lpencodebacklen(unsigned char *buf, uint64_t l) {\n    //编码类型和实际数据的总长度小于等于127，entry-len长度为1字节\n    if (l <= 127) {\n        ...\n        return 1;\n    } else if (l < 16383) { //编码类型和实际数据的总长度大于127但小于16383，entry-len长度为2字节\n       ...\n        return 2;\n    } else if (l < 2097151) {//编码类型和实际数据的总长度大于16383但小于2097151，entry-len长度为3字节\n       ...\n        return 3;\n    } else if (l < 268435455) { //编码类型和实际数据的总长度大于2097151但小于268435455，entry-len长度为4字节\n        ...\n        return 4;\n    } else { //否则，entry-len长度为5字节\n       ...\n        return 5;\n    }\n}\n\n\n我也画了一张图，展示了从左向右遍历 listpack 的基本过程，你可以再回顾下。\n\n\n\n好，了解了从左向右正向查询 listpack，我们再来看下从右向左反向查询 listpack。\n\n首先，我们根据 listpack 头中记录的 listpack 总长度，就可以直接定位到 listapck 的尾部结束标记。然后，我们可以调用 lpprev 函数，该函数的参数包括指向某个列表项的指针，并返回指向当前列表项前一项的指针。\n\nlpprev 函数中的关键一步就是调用 lpdecodebacklen 函数。lpdecodebacklen 函数会从右向左，逐个字节地读取当前列表项的 entry-len。\n\n那么，lpdecodebacklen 函数如何判断 entry-len 是否结束了呢？\n\n这就依赖于 entry-len 的编码方式了。entry-len 每个字节的最高位，是用来表示当前字节是否为 entry-len 的最后一个字节，这里存在两种情况，分别是：\n\n * 最高位为 1，表示 entry-len 还没有结束，当前字节的左边字节仍然表示 entry-len 的内容；\n * 最高位为 0，表示当前字节已经是 entry-len 最后一个字节了。\n\n而 entry-len 每个字节的低 7 位，则记录了实际的长度信息。这里你需要注意的是，entry-len 每个字节的低 7 位采用了大端模式存储，也就是说，entry-len 的低位字节保存在内存高地址上。\n\n我画了下面这张图，展示了 entry-len 这种特别的编码方式，你可以看下。\n\n\n\n实际上，正是因为有了 entry-len 的特别编码方式，lpdecodebacklen 函数就可以从当前列表项起始位置的指针开始，向左逐个字节解析，得到前一项的 entry-len 值。这也是 lpdecodebacklen 函数的返回值。而从刚才的介绍中，我们知道 entry-len 记录了编码类型和实际数据的长度之和。\n\n因此，lpprev 函数会再调用 lpencodebacklen 函数，来计算得到 entry-len 本身长度，这样一来，我们就可以得到前一项的总长度，而 lpprev 函数也就可以将指针指向前一项的起始位置了。所以按照这个方法，listpack 就实现了从右向左的查询功能。\n\n\n# 对比\n\n特性       ziplist                quicklist   listpack\n设计复杂度    较为复杂，包含前一个节点长度字段       复杂          更加简化，没有前一个节点长度字段\n内存占用     存在冗余字段，内存利用率较低         高           更加紧凑，内存占用低\n操作复杂度    插入、删除操作需要更新前向节点长度，较慢   中           操作简单高效，无需处理前向节点长度，但是也要移动\n内存移动问题   频繁插入删除可能导致大范围内存移动      中           仍存在内存移动问题，但操作更加简单\n适用场景     小 hash，小 zset          list        小 hash，小 zset，小 stream\n\n\n\n笔记\n\necho 认为，ziplist 和 listpack 适用于元素较少时的存储，一旦元素变多就需采用 quicklist 这种类似于 linkedlist 的结构来进行存储，但是 quicklist 的 node 有两种选择，分别是 ziplist 和 listpack，在最新的版本中貌似都是采用 listpack 来实现的\n\n\n# 总结\n\n本文从 ziplist 的设计不足出发，到学习 quicklist 和 listpack 的设计思想\n\n你要知道，ziplist 的不足主要在于一旦 ziplist 中元素个数多了，它的查找效率就会降低。而且如果在 ziplist 里新增或修改数据，ziplist 占用的内存空间还需要重新分配；更糟糕的是，ziplist 新增某个元素或修改某个元素时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起连锁更新问题，导致每个元素的空间都要重新分配，这就会导致 ziplist 的访问性能下降。\n\n所以，为了应对 ziplist 的问题，redis 先是在 3.0 版本中设计实现了 quicklist。quicklist 结构在 ziplist 基础上，使用链表将 ziplist 串联起来，链表的每个元素就是一个 ziplist。这种设计减少了数据插入时内存空间的重新分配，以及内存数据的拷贝。同时，quicklist 限制了每个节点上 ziplist 的大小，一旦一个 ziplist 过大，就会采用新增 quicklist 节点的方法。\n\n不过，又因为 quicklist 使用 quicklistnode 结构指向每个 ziplist，无疑增加了内存开销。为了减少内存开销，并进一步避免 ziplist 连锁更新问题，redis 在 5.0 版本中，就设计实现了 listpack 结构。listpack 结构沿用了 ziplist 紧凑型的内存布局，把每个元素都紧挨着放置\n\nlistpack 中每个列表项不再包含前一项的长度了，因此当某个列表项中的数据发生变化，导致列表项长度变化时，其他列表项的长度是不会受影响的，因而这就避免了 ziplist 面临的连锁更新问题。\n\n总而言之，redis 在内存紧凑型列表的设计与实现上，从 ziplist 到 quicklist，再到 listpack，你可以看到 redis 在内存空间开销和访问性能之间的设计取舍，这一系列的设计变化，是非常值得你学习的\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Hash 设计与实现",frontmatter:{title:"Hash 设计与实现",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d4311/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/02.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/05.Hash%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Redis 系统设计/02.二、基础知识/05.Hash 设计与实现.md",key:"v-0397123a",path:"/pages/2d4311/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:244},{level:2,title:"如何避免 hash 冲突",slug:"如何避免-hash-冲突",normalizedTitle:"如何避免 hash 冲突",charIndex:605},{level:2,title:"如何实现 rehash",slug:"如何实现-rehash",normalizedTitle:"如何实现 rehash",charIndex:1031},{level:3,title:"什么时候触发 rehash",slug:"什么时候触发-rehash",normalizedTitle:"什么时候触发 rehash",charIndex:1815},{level:3,title:"rehash 扩容扩多大？",slug:"rehash-扩容扩多大",normalizedTitle:"rehash 扩容扩多大？",charIndex:1833},{level:3,title:"渐进式 rehash 如何实现",slug:"渐进式-rehash-如何实现",normalizedTitle:"渐进式 rehash 如何实现",charIndex:5804},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:11418},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:11532}],headersStr:"前言 如何避免 hash 冲突 如何实现 rehash 什么时候触发 rehash rehash 扩容扩多大？ 渐进式 rehash 如何实现 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. 如何避免 Hash 表数据量增加导致哈希冲突的性能下降？\n 2. 为什么链式哈希能有效解决冲突，Redis 如何实现？\n 3. Hash 表扩容时，为什么直接 rehash 会影响性能？\n 4. Redis 如何优化 rehash 过程，避免主线程阻塞？\n 5. 渐进式 rehash 如何确保数据迁移时间有限？\n 6. Redis 如何调整 rehash 触发条件，平衡性能与内存？\n 7. rehash 进行时，如何确保新键值对正确存储？\n\n\n# 前言\n\n对于 Redis 键值数据库来说，Hash 表有两种主要场景\n\n * Hash 表既是键值对中的一种值类型\n * 同时，Redis 也使用一个全局 Hash 表来保存所有的键值对，从而既满足应用存取 Hash 结构数据需求，又能提供快速查询功能\n\n> Hash 表应用如此广泛的一个重要原因，就是从理论上来说，它能以 O(1) 的复杂度快速查询数据\n\n但是实际应用 Hash 表时有两个缺点：\n\n * 哈希冲突\n * rehash 开销\n\nRedis 为我们提供了一个经典的 Hash 表实现方案来解决上述问题\n\n * 针对 哈希冲突，Redis 采用了链式哈希\n * 针对 rehash 开销，Redis 采用了 渐进式 rehash 设计，进而缓解了 rehash 操作带来的额外开销对系统的性能影响\n\n\n# 如何避免 hash 冲突\n\n * 第一种方案，就是我接下来要给你介绍的链式哈希。这里你需要先知道，链式哈希的链 不能太长，否则会降低 Hash 表性能\n * 第二种方案，就是当链式哈希的链长达到一定长度时，我们可以使用 rehash。不过， 执行 rehash 本身开销比较大，所以就需要采用我稍后会给你介绍的渐进式 rehash 设计\n\n这样，当我们要查询 key5 时，可以先通过哈希函数计算，得到 key5 的哈希值被映射到了桶 9 中。然后，我们再逐一比较桶 9 中串接的 key，直到查找到 key5。如此一来，我们就能在链式哈希中找到所查的哈希项了。\n\n不过，链式哈希也存在局限性，那就是随着链表长度的增加，Hash 表在一个位置上查询哈希项的耗时就会增加，从而增加了 Hash 表的整体查询时间，这样也会导致 Hash 表的性能下降。\n\n所以 Redis 要控制 Hash 表的长度，就要在长度达到一定阈值时去进行 rehash\n\n\n# 如何实现 rehash\n\nrehash 操作，其实就是指扩大 Hash 表空间。而 Redis 实现 rehash 的基本思路是这样的：\n\n首先，Redis 准备了两个哈希表，用于 rehash 时交替保存数据。\n\nRedis 在 dict.h 文件中使用 dictht 结构体定义了 Hash 表。不过，在实际使用 Hash 表时，Redis 又在 dict.h 文件中，定义了一个 dict 结构体。这个结构体中有一个数组ht[2]，包含了两个 Hash 表 ht[0] 和 ht[1]\n\ntypedef struct dict {\n    dictType *type;\n    void *privdata;\n    //两个Hash表，交替使用，用于rehash操作\n    dictht ht[2];\n    // Hash表是否在进行rehash的标识，-1表示没有进行rehash\n    long rehashidx; /* rehashing not in progress if rehashidx == -1 */\n    int16_t pauserehash; /* If >0 rehashing is paused (<0 indicates coding error) */\n} dict;\n\n\n * 在正常服务请求阶段，所有的键值对写入哈希表 ht[0]\n\n * 当进行 rehash 时，键值对被迁移到哈希表 ht[1] 中\n\n * 当迁移完成后，ht[0] 的空间会被释放，并把 ht[1] 的地址赋值给 ht[0]，ht[1] 的表大小设置为 0。这样一来，又回到了正常服务请求的阶段，ht[0] 接收和服务请求，ht[1] 作为下一次 rehash 时的迁移表\n\n那么，在实现 rehash 时，需要解决哪些问题？\n\n * 什么时候触发 rehash？\n * rehash 扩容扩多大？\n * rehash 如何执行？\n\n\n# 什么时候触发 rehash\n\n首先要知道，Redis 用来判断是否触发 rehash 的函数是 _dictExpandIfNeeded。所以接 下来我们就先看看， _dictExpandIfNeeded 函数中进行扩容的触发条件；然后，我们再来了解下 _dictExpandIfNeeded 又是在哪些函数中被调用的。\n\n实际上， _dictExpandIfNeeded 函数中定义了三个扩容条件。\n\n * 条件一：ht[0] 的大小为 0。\n * 条件二：ht[0] 承载的元素个数已经超过了 ht[0] 的大小，同时 Hash 表可以进行扩容。\n * 条件三：ht[0] 承载的元素个数，是 ht[0] 的大小的 dict_force_resize_ratio 倍，其中， dict_force_resize_ratio 的默认值是 5\n\n/* Expand the hash table if needed */\nstatic int _dictExpandIfNeeded(dict *d)\n{\n    /* Incremental rehashing already in progress. Return. */\n    if (dictIsRehashing(d)) return DICT_OK;\n\n    /* If the hash table is empty expand it to the initial size. */\n    if (d->ht[0].size == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE);\n\n    /* If we reached the 1:1 ratio, and we are allowed to resize the hash\n     * table (global setting) or we should avoid it but the ratio between\n     * elements/buckets is over the \"safe\" threshold, we resize doubling\n     * the number of buckets. */\n    // ht[0]表使用的元素个数超过当前大小\n    // 并且可以扩容或者 ht[0]使用的元素个数/ht[0]表的大小 大于 dict_force_resize_ratio\n    // 并且能够允许扩展\n    if (d->ht[0].used >= d->ht[0].size &&\n        (dict_can_resize ||\n         d->ht[0].used/d->ht[0].size > dict_force_resize_ratio) &&\n        dictTypeExpandAllowed(d))\n    {\n        return dictExpand(d, d->ht[0].used + 1);\n    }\n    return DICT_OK;\n}\n\n\n * 对于条件一来说，此时 Hash 表是空的，所以 Redis 就需要将 Hash 表空间设置为初始大小，而这是初始化的工作，并不属于 rehash 操作。\n\n * 而条件二和三就对应了 rehash 的场景。因为在这两个条件中，都比较了 Hash 表当前承载 的元素个数d->ht[0].used和 Hash 表当前设定的大小d->ht[0].size，这两个值的比值一般称为负载因子（load factor）。也就是说，Redis 判断是否进行 rehash 的条 件，就是看 load factor 是否大于等于 1 和是否大于 5。\n\n提示\n\n当 load factor 大于 5 时，就表明 Hash 表已经过载比较严重了，需要立刻进行库扩容。而当 load factor 大于等于 1 时，Redis 还会再判断 dict_can_resize 这个变量值，查看当前是否可以进行扩容\n\n你可能要问了，这里的 dict_can_resize 变量值是啥呀？其实，这个变量值是在 dictEnableResize 和 dictDisableResize 两个函数中设置的，它们的作用分别是启用和禁止哈希表执行 rehash 功能，如下所示：\n\nvoid dictEnableResize(void) {\n    dict_can_resize = 1;\n}\n\nvoid dictDisableResize(void) {\n    dict_can_resize = 0;\n}\n\n\n然后，这两个函数又被封装在了 updateDictResizePolicy 函数中。\n\nupdateDictResizePolicy 函数是用来启用或禁用 rehash 扩容功能的，这个函数调用 dictEnableResize 函数启用扩容功能的条件是：\n\n * 当前没有 RDB 子进程，并且也没有 AOF 子进程。\n\n这就对应了 Redis 没有执行 RDB 快照和没有进行 AOF 重写的场景。你可以参考下面给出的代码：\n\nvoid updateDictResizePolicy(void) {\nif (server.rdb_child_pid == -1 && server.aof_child_pid == -1)\n  dictEnableResize();\nelse\n  dictDisableResize();\n}\n\n\n上述是 _dictExpandIfNeeded 对 rehash 的判断触发条件\n\n接下来，再来看下 Redis 会在哪些函数中，调用 _dictExpandIfNeeded 进行判断\n\n首先，通过在 dict.c 文件中查看 _dictExpandIfNeeded 的被调用关系，我们可以发现， _dictExpandIfNeeded 是被 _dictKeyIndex 函数调用的，而 _dictKeyIndex 函数又会被 dictAddRaw 函数调用，然后 dictAddRaw 会被以下三个函数调用。\n\n * dictAdd：用来往 Hash 表中添加一个键值对\n * dictRelace：用来往 Hash 表中添加一个键值对，或者键值对存在时，修改键值对\n * dictAddorFind：直接调用 dictAddRaw\n\n因此，当我们往 Redis 中写入新的键值对或是修改键值对时，Redis 都会判断下是否需要进行 rehash。这里你可以参考下面给出的示意图，其中就展示了 _dictExpandIfNeeded 被调用的关系。\n\n简而言之，Redis 中触发 rehash 操作的关键，就是 dictExpandIfNeeded 函数 和 updateDictResizePolicy 函数。\n\ndictExpandIfNeeded 函数会根据下述情况判断是否进行rehash\n\n * Hash 表的负载因子\n * RDB 和 AOF 的执行情况\n\n然后看第二个问题：rehash 扩容扩多大？\n\n\n# rehash 扩容扩多大？\n\n在 Redis 中，rehash 对 Hash 表空间的扩容是通过调用 dictExpand 函数来完成的。 dictExpand 函数的参数有两个\n\n * 一个是要扩容的 Hash 表\n * 另一个是要扩到的容量\n\n int dictExpand(dict *d, unsigned long size);\n\n\n对于一个 Hash 表来说\n\n 1. 我们就可以根据前面提到的 _dictExpandIfNeeded 函数， 来判断是否要对其进行扩容\n\n 2. 一旦判断要扩容，Redis 在执行 rehash 操作时，对 Hash 表扩容的思路也很简单，就是如果当前表的已用空间大小为 size，那么就将表扩容到 size*2 的大小\n\n如下所示，这里你可以看到，rehash 的扩容大小是当前 ht[0]已使用大小的 2 倍\n\ndictExpand(d, d->ht[0].used*2);\n\n\n而在 dictExpand 函数中，具体执行是由 _dictNextPower 函数完成的，以下代码显示的 Hash 表扩容的操作，就是从 Hash 表的初始大小DICT_HT_INITIAL_SIZE，不停地乘以 2，直到达到目标大小\n\nstatic unsigned long _dictNextPower(unsigned long size)\n{\n    // 哈希表的初始大小\n    unsigned long i = DICT_HT_INITIAL_SIZE;\n  \t// 如果要扩容的大小已经超过最大值，则返回最大值加1\n    if (size >= LONG_MAX) \n        return LONG_MAX + 1LU;\n    // 扩容大小没有超过最大值\n    while(1) {\n        if (i >= size)\n            return i;\n        // 每一步扩容都在现有大小基础上乘以2\n        i *= 2;\n    }\n}\n\n\n下面开始第三个问题，即 rehash 要如何执行？而这个问题，本质上就是 Redis 要如何实现渐进式 rehash 设计\n\n\n# 渐进式 rehash 如何实现\n\n为什么要实现渐进式 rehash\n\n因为，Hash 表在执行 rehash 时，由于 Hash 表空间扩大，原本映射到某一位置的键可能会被映射到一个新的位置上，因此，很多键就需要从原来的位置拷贝到新的位 置。而在键拷贝时，由于 Redis 主线程无法执行其他请求，所以键拷贝会阻塞主线程，这样就会产生 rehash 开销，而为了降低 rehash 开销，Redis 就提出了渐进式 rehash 的方法\n\n简述 「渐进式 rehash 」：Redis 并不会一次性把当前 Hash 表中的所有键， 都拷贝到新位置，而是会分批拷贝，每次的键拷贝只拷贝 Hash 表中一个 bucket 中的哈希项。这样一来，每次键拷贝的时长有限，对主线程的影响也就有限了。\n\n渐进式 rehash 在代码层面的实现，有两个关键函数：dictRehash 和 _dictRehashStep。\n\n我们先来看 dictRehash 函数，这个函数实际执行键拷贝，它的输入参数有两个，分别是 全局哈希表（即前面提到的 dict 结构体，包含了 ht[0]和 ht[1]）和需要进行键拷贝的桶数量（bucket 数量）。\n\ndictRehash 函数的整体逻辑包括三部分：\n\n 1. 该函数会执行一个循环，根据要进行键拷贝的 bucket 数量 n，依次完成这些 bucket 内部所有键的迁移。当然，如果 ht[0] 哈希表中的数据已经都迁移完成了，键拷贝的循环也会停止执行\n 2. 在完成了 n 个 bucket 拷贝后，dictRehash 函数的第二部分逻辑，就是判断 ht[0] 表中数据是否都已迁移完。如果都迁移完了，那么 ht[0] 的空间会被释放。因为 Redis 在处理请求时，代码逻辑中都是使用 ht[0]，所以当 rehash 执行完成后，虽然数据都在 ht[1] 中了，但 Redis 仍然会把 ht[1] 赋值给 ht[0]，以便其他部分的代码逻辑正常使用\n 3. 在 ht[1] 赋值给 ht[0] 后，它的大小就会被重置为 0，等待下一次 rehash。与此同时， 全局哈希表中的 rehashidx 变量会被标为 -1，表示 rehash 结束了（这里的 rehashidx 变量用来表示 rehash 的进度，稍后我会给你具体解释）。\n\n\n\nint dictRehash(dict *d, int n) {\n    int empty_visits = n*10; /* Max number of empty buckets to visit. */\n    if (!dictIsRehashing(d)) return 0;\n\n    // 主循环，根据要拷贝的bucket数量n，循环n次后停止或ht[0]中的数据迁移完停止\n    while(n-- && d->ht[0].used != 0) {\n        dictEntry *de, *nextde;\n\n        /* Note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */\n        assert(d->ht[0].size > (unsigned long)d->rehashidx);\n        while(d->ht[0].table[d->rehashidx] == NULL) {\n            d->rehashidx++;\n            if (--empty_visits == 0) return 1;\n        }\n        de = d->ht[0].table[d->rehashidx];\n        /* Move all the keys in this bucket from the old to the new hash HT */\n        while(de) {\n            uint64_t h;\n\n            nextde = de->next;\n            /* Get the index in the new hash table */\n            h = dictHashKey(d, de->key) & d->ht[1].sizemask;\n            de->next = d->ht[1].table[h];\n            d->ht[1].table[h] = de;\n            d->ht[0].used--;\n            d->ht[1].used++;\n            de = nextde;\n        }\n        d->ht[0].table[d->rehashidx] = NULL;\n        d->rehashidx++;\n    }\n\n    //判断ht[0]的数据是否迁移完成\n    /* Check if we already rehashed the whole table... */\n    if (d->ht[0].used == 0) {\n        // ht[0]迁移完后，释放ht[0]内存空间\n        zfree(d->ht[0].table);\n        // 让ht[0]指向ht[1]，以便接受正常的请求\n        d->ht[0] = d->ht[1];\n        // 重置ht[1]的大小为0\n        _dictReset(&d->ht[1]);\n        // 设置全局哈希表的rehashidx标识为-1，表示rehash结束\n        d->rehashidx = -1;\n        // 返回0，表示ht[0]中所有元素都迁移完\n        return 0;\n    }\n\n    //返回1，表示ht[0]中仍然有元素没有迁移完\n    /* More to rehash... */\n    return 1;\n}\n\n\n那么，渐进式 rehash 是如何按照 bucket 粒度拷贝数据的，这其实就和全局哈希表 dict 结构中的 rehashidx 变量相关了\n\nrehashidx 变量表示的是当前 rehash 在对哪个 bucket 做数据迁移。比如，当 rehashidx 等于 0 时，表示对 ht[0]中的第一个 bucket 进行数据迁移；当 rehashidx 等于 1 时，表 示对 ht[0] 中的第二个 bucket 进行数据迁移，以此类推。\n\n而 dictRehash 函数的主循环，首先会判断 rehashidx 指向的 bucket 是否为空，如果为空，那就将 rehashidx 的值加 1，检查下一个 bucket。\n\n那么，有没有可能连续几个 bucket 都为空呢？其实是有可能的，在这种情况下，渐进式 rehash 不会一直递增 rehashidx 进行检查。这是因为一旦执行了 rehash，Redis 主线程就无法处理其他请求了。\n\n所以，渐进式 rehash 在执行时设置了一个变量 empty_visits，用来表示已经检查过的空 bucket，当检查了一定数量的空 bucket 后，这一轮的 rehash 就停止执行，转而继续处理外来请求，避免了对 Redis 性能的影响。下面的代码显示了这部分逻辑，你可以看下。\n\n// 如果当前要迁移的 bucket 中没有元素\nwhile(d->ht[0].table[d->rehashidx] == NULL) {\n    d->rehashidx++;\n    if (--empty_visits == 0) return 1;\n}\n\n\n而如果 rehashidx 指向的 bucket 有数据可以迁移，那么 Redis 就会把这个 bucket 中的哈希项依次取出来，并根据 ht[1] 的表空间大小，重新计算哈希项在 ht[1] 中的 bucket 位置，然后把这个哈希项赋值到 ht[1] 对应 bucket 中\n\n这样，每做完一个哈希项的迁移，ht[0] 和 ht[1] 用来表示承载哈希项多少的变量 used，就 会分别减一和加一。当然，如果当前 rehashidx 指向的 bucket 中数据都迁移完了， rehashidx 就会递增加 1，指向下一个 bucket。下面的代码显示了这一迁移过程。\n\n while(n-- && d->ht[0].used != 0) {\n     dictEntry *de, *nextde;\n\n     /* Note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */\n     assert(d->ht[0].size > (unsigned long)d->rehashidx);\n     while(d->ht[0].table[d->rehashidx] == NULL) {\n         d->rehashidx++;\n         if (--empty_visits == 0) return 1;\n     }\n     // 获得哈希表中哈希项\n     de = d->ht[0].table[d->rehashidx];\n     /* Move all the keys in this bucket from the old to the new hash HT */\n     while(de) {\n         uint64_t h;\n     \t// 获得同一个bucket中下一个哈希项\n         nextde = de->next;\n         /* Get the index in the new hash table */\n         // 根据扩容后的哈希表ht[1]大小，计算当前哈希项在扩容后哈希表中的bucket位置\n         h = dictHashKey(d, de->key) & d->ht[1].sizemask;\n         // 将当前哈希项添加到扩容后的哈希表ht[1]中\n         de->next = d->ht[1].table[h];\n         d->ht[1].table[h] = de;\n         // 减少当前哈希表的哈希项个数\n         d->ht[0].used--;\n         // 增加扩容后哈希表的哈希项个数\n         d->ht[1].used++;\n         de = nextde;\n     }\n     // 如果当前bucket中已经没有哈希项了，将该bucket置为NULL\n     d->ht[0].table[d->rehashidx] = NULL;\n     // 将rehash加1，下一次将迁移下一个bucket中的元素\n     d->rehashidx++;\n }\n\n\n好了，到这里，我们就已经基本了解了 dictRehash 函数的全部逻辑。 现在我们知道，dictRehash 函数本身是按照 bucket 粒度执行哈希项迁移的，它内部执行的 bucket 迁移个数，主要由传入的循环次数变量 n 来决定。但凡 Redis 要进行 rehash 操作，最终都会调用 dictRehash 函数。\n\n接下来，我们来学习和渐进式 rehash 相关的第二个关键函数 _dictRehashStep，这个函数实现了每次只对一个 bucket 执行 rehash。 从 Redis 的源码中我们可以看到，一共会有 5 个函数通过调用 _dictRehashStep 函数，进而调用 dictRehash 函数，来执行 rehash，它们分别是：dictAddRaw， dictGenericDelete，dictFind，dictGetRandomKey，dictGetSomeKeys。\n\n其中，dictAddRaw 和 dictGenericDelete 函数，分别对应了往 Redis 中增加和删除键值对，而后三个函数则对应了在 Redis 中进行查询操作。下图展示了这些函数间的调用关系：\n\n但你要注意，不管是增删查哪种操作，这 5 个函数调用的 _dictRehashStep 函数，给 dictRehash 传入的循环次数变量 n 的值都为 1，下面的代码就显示了这一传参的情况\n\nstatic void _dictRehashStep(dict *d) {\n    // 给dictRehash传入的循环次数参数为1，表明每迁移完一个bucket ，就执行正常操作\n    if (d->pauserehash == 0) dictRehash(d,1);\n}\n\n\n这样一来，每次迁移完一个 bucket，Hash 表就会执行正常的增删查请求操作，这就是在代码层面实现渐进式 rehash 的方法\n\n\n# 总结\n\n 1. 通过「链表」解决Hash冲突\n 2. Redis 通过 「渐进式 rehash」 来解决大量数据 rehash 可能会导致的阻塞问题\n 3. 渐进式 rehash 按照 bucket 粒度拷贝数据的方法\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 如何避免 hash 表数据量增加导致哈希冲突的性能下降？\n 2. 为什么链式哈希能有效解决冲突，redis 如何实现？\n 3. hash 表扩容时，为什么直接 rehash 会影响性能？\n 4. redis 如何优化 rehash 过程，避免主线程阻塞？\n 5. 渐进式 rehash 如何确保数据迁移时间有限？\n 6. redis 如何调整 rehash 触发条件，平衡性能与内存？\n 7. rehash 进行时，如何确保新键值对正确存储？\n\n\n# 前言\n\n对于 redis 键值数据库来说，hash 表有两种主要场景\n\n * hash 表既是键值对中的一种值类型\n * 同时，redis 也使用一个全局 hash 表来保存所有的键值对，从而既满足应用存取 hash 结构数据需求，又能提供快速查询功能\n\n> hash 表应用如此广泛的一个重要原因，就是从理论上来说，它能以 o(1) 的复杂度快速查询数据\n\n但是实际应用 hash 表时有两个缺点：\n\n * 哈希冲突\n * rehash 开销\n\nredis 为我们提供了一个经典的 hash 表实现方案来解决上述问题\n\n * 针对 哈希冲突，redis 采用了链式哈希\n * 针对 rehash 开销，redis 采用了 渐进式 rehash 设计，进而缓解了 rehash 操作带来的额外开销对系统的性能影响\n\n\n# 如何避免 hash 冲突\n\n * 第一种方案，就是我接下来要给你介绍的链式哈希。这里你需要先知道，链式哈希的链 不能太长，否则会降低 hash 表性能\n * 第二种方案，就是当链式哈希的链长达到一定长度时，我们可以使用 rehash。不过， 执行 rehash 本身开销比较大，所以就需要采用我稍后会给你介绍的渐进式 rehash 设计\n\n这样，当我们要查询 key5 时，可以先通过哈希函数计算，得到 key5 的哈希值被映射到了桶 9 中。然后，我们再逐一比较桶 9 中串接的 key，直到查找到 key5。如此一来，我们就能在链式哈希中找到所查的哈希项了。\n\n不过，链式哈希也存在局限性，那就是随着链表长度的增加，hash 表在一个位置上查询哈希项的耗时就会增加，从而增加了 hash 表的整体查询时间，这样也会导致 hash 表的性能下降。\n\n所以 redis 要控制 hash 表的长度，就要在长度达到一定阈值时去进行 rehash\n\n\n# 如何实现 rehash\n\nrehash 操作，其实就是指扩大 hash 表空间。而 redis 实现 rehash 的基本思路是这样的：\n\n首先，redis 准备了两个哈希表，用于 rehash 时交替保存数据。\n\nredis 在 dict.h 文件中使用 dictht 结构体定义了 hash 表。不过，在实际使用 hash 表时，redis 又在 dict.h 文件中，定义了一个 dict 结构体。这个结构体中有一个数组ht[2]，包含了两个 hash 表 ht[0] 和 ht[1]\n\ntypedef struct dict {\n    dicttype *type;\n    void *privdata;\n    //两个hash表，交替使用，用于rehash操作\n    dictht ht[2];\n    // hash表是否在进行rehash的标识，-1表示没有进行rehash\n    long rehashidx; /* rehashing not in progress if rehashidx == -1 */\n    int16_t pauserehash; /* if >0 rehashing is paused (<0 indicates coding error) */\n} dict;\n\n\n * 在正常服务请求阶段，所有的键值对写入哈希表 ht[0]\n\n * 当进行 rehash 时，键值对被迁移到哈希表 ht[1] 中\n\n * 当迁移完成后，ht[0] 的空间会被释放，并把 ht[1] 的地址赋值给 ht[0]，ht[1] 的表大小设置为 0。这样一来，又回到了正常服务请求的阶段，ht[0] 接收和服务请求，ht[1] 作为下一次 rehash 时的迁移表\n\n那么，在实现 rehash 时，需要解决哪些问题？\n\n * 什么时候触发 rehash？\n * rehash 扩容扩多大？\n * rehash 如何执行？\n\n\n# 什么时候触发 rehash\n\n首先要知道，redis 用来判断是否触发 rehash 的函数是 _dictexpandifneeded。所以接 下来我们就先看看， _dictexpandifneeded 函数中进行扩容的触发条件；然后，我们再来了解下 _dictexpandifneeded 又是在哪些函数中被调用的。\n\n实际上， _dictexpandifneeded 函数中定义了三个扩容条件。\n\n * 条件一：ht[0] 的大小为 0。\n * 条件二：ht[0] 承载的元素个数已经超过了 ht[0] 的大小，同时 hash 表可以进行扩容。\n * 条件三：ht[0] 承载的元素个数，是 ht[0] 的大小的 dict_force_resize_ratio 倍，其中， dict_force_resize_ratio 的默认值是 5\n\n/* expand the hash table if needed */\nstatic int _dictexpandifneeded(dict *d)\n{\n    /* incremental rehashing already in progress. return. */\n    if (dictisrehashing(d)) return dict_ok;\n\n    /* if the hash table is empty expand it to the initial size. */\n    if (d->ht[0].size == 0) return dictexpand(d, dict_ht_initial_size);\n\n    /* if we reached the 1:1 ratio, and we are allowed to resize the hash\n     * table (global setting) or we should avoid it but the ratio between\n     * elements/buckets is over the \"safe\" threshold, we resize doubling\n     * the number of buckets. */\n    // ht[0]表使用的元素个数超过当前大小\n    // 并且可以扩容或者 ht[0]使用的元素个数/ht[0]表的大小 大于 dict_force_resize_ratio\n    // 并且能够允许扩展\n    if (d->ht[0].used >= d->ht[0].size &&\n        (dict_can_resize ||\n         d->ht[0].used/d->ht[0].size > dict_force_resize_ratio) &&\n        dicttypeexpandallowed(d))\n    {\n        return dictexpand(d, d->ht[0].used + 1);\n    }\n    return dict_ok;\n}\n\n\n * 对于条件一来说，此时 hash 表是空的，所以 redis 就需要将 hash 表空间设置为初始大小，而这是初始化的工作，并不属于 rehash 操作。\n\n * 而条件二和三就对应了 rehash 的场景。因为在这两个条件中，都比较了 hash 表当前承载 的元素个数d->ht[0].used和 hash 表当前设定的大小d->ht[0].size，这两个值的比值一般称为负载因子（load factor）。也就是说，redis 判断是否进行 rehash 的条 件，就是看 load factor 是否大于等于 1 和是否大于 5。\n\n提示\n\n当 load factor 大于 5 时，就表明 hash 表已经过载比较严重了，需要立刻进行库扩容。而当 load factor 大于等于 1 时，redis 还会再判断 dict_can_resize 这个变量值，查看当前是否可以进行扩容\n\n你可能要问了，这里的 dict_can_resize 变量值是啥呀？其实，这个变量值是在 dictenableresize 和 dictdisableresize 两个函数中设置的，它们的作用分别是启用和禁止哈希表执行 rehash 功能，如下所示：\n\nvoid dictenableresize(void) {\n    dict_can_resize = 1;\n}\n\nvoid dictdisableresize(void) {\n    dict_can_resize = 0;\n}\n\n\n然后，这两个函数又被封装在了 updatedictresizepolicy 函数中。\n\nupdatedictresizepolicy 函数是用来启用或禁用 rehash 扩容功能的，这个函数调用 dictenableresize 函数启用扩容功能的条件是：\n\n * 当前没有 rdb 子进程，并且也没有 aof 子进程。\n\n这就对应了 redis 没有执行 rdb 快照和没有进行 aof 重写的场景。你可以参考下面给出的代码：\n\nvoid updatedictresizepolicy(void) {\nif (server.rdb_child_pid == -1 && server.aof_child_pid == -1)\n  dictenableresize();\nelse\n  dictdisableresize();\n}\n\n\n上述是 _dictexpandifneeded 对 rehash 的判断触发条件\n\n接下来，再来看下 redis 会在哪些函数中，调用 _dictexpandifneeded 进行判断\n\n首先，通过在 dict.c 文件中查看 _dictexpandifneeded 的被调用关系，我们可以发现， _dictexpandifneeded 是被 _dictkeyindex 函数调用的，而 _dictkeyindex 函数又会被 dictaddraw 函数调用，然后 dictaddraw 会被以下三个函数调用。\n\n * dictadd：用来往 hash 表中添加一个键值对\n * dictrelace：用来往 hash 表中添加一个键值对，或者键值对存在时，修改键值对\n * dictaddorfind：直接调用 dictaddraw\n\n因此，当我们往 redis 中写入新的键值对或是修改键值对时，redis 都会判断下是否需要进行 rehash。这里你可以参考下面给出的示意图，其中就展示了 _dictexpandifneeded 被调用的关系。\n\n简而言之，redis 中触发 rehash 操作的关键，就是 dictexpandifneeded 函数 和 updatedictresizepolicy 函数。\n\ndictexpandifneeded 函数会根据下述情况判断是否进行rehash\n\n * hash 表的负载因子\n * rdb 和 aof 的执行情况\n\n然后看第二个问题：rehash 扩容扩多大？\n\n\n# rehash 扩容扩多大？\n\n在 redis 中，rehash 对 hash 表空间的扩容是通过调用 dictexpand 函数来完成的。 dictexpand 函数的参数有两个\n\n * 一个是要扩容的 hash 表\n * 另一个是要扩到的容量\n\n int dictexpand(dict *d, unsigned long size);\n\n\n对于一个 hash 表来说\n\n 1. 我们就可以根据前面提到的 _dictexpandifneeded 函数， 来判断是否要对其进行扩容\n\n 2. 一旦判断要扩容，redis 在执行 rehash 操作时，对 hash 表扩容的思路也很简单，就是如果当前表的已用空间大小为 size，那么就将表扩容到 size*2 的大小\n\n如下所示，这里你可以看到，rehash 的扩容大小是当前 ht[0]已使用大小的 2 倍\n\ndictexpand(d, d->ht[0].used*2);\n\n\n而在 dictexpand 函数中，具体执行是由 _dictnextpower 函数完成的，以下代码显示的 hash 表扩容的操作，就是从 hash 表的初始大小dict_ht_initial_size，不停地乘以 2，直到达到目标大小\n\nstatic unsigned long _dictnextpower(unsigned long size)\n{\n    // 哈希表的初始大小\n    unsigned long i = dict_ht_initial_size;\n  \t// 如果要扩容的大小已经超过最大值，则返回最大值加1\n    if (size >= long_max) \n        return long_max + 1lu;\n    // 扩容大小没有超过最大值\n    while(1) {\n        if (i >= size)\n            return i;\n        // 每一步扩容都在现有大小基础上乘以2\n        i *= 2;\n    }\n}\n\n\n下面开始第三个问题，即 rehash 要如何执行？而这个问题，本质上就是 redis 要如何实现渐进式 rehash 设计\n\n\n# 渐进式 rehash 如何实现\n\n为什么要实现渐进式 rehash\n\n因为，hash 表在执行 rehash 时，由于 hash 表空间扩大，原本映射到某一位置的键可能会被映射到一个新的位置上，因此，很多键就需要从原来的位置拷贝到新的位 置。而在键拷贝时，由于 redis 主线程无法执行其他请求，所以键拷贝会阻塞主线程，这样就会产生 rehash 开销，而为了降低 rehash 开销，redis 就提出了渐进式 rehash 的方法\n\n简述 「渐进式 rehash 」：redis 并不会一次性把当前 hash 表中的所有键， 都拷贝到新位置，而是会分批拷贝，每次的键拷贝只拷贝 hash 表中一个 bucket 中的哈希项。这样一来，每次键拷贝的时长有限，对主线程的影响也就有限了。\n\n渐进式 rehash 在代码层面的实现，有两个关键函数：dictrehash 和 _dictrehashstep。\n\n我们先来看 dictrehash 函数，这个函数实际执行键拷贝，它的输入参数有两个，分别是 全局哈希表（即前面提到的 dict 结构体，包含了 ht[0]和 ht[1]）和需要进行键拷贝的桶数量（bucket 数量）。\n\ndictrehash 函数的整体逻辑包括三部分：\n\n 1. 该函数会执行一个循环，根据要进行键拷贝的 bucket 数量 n，依次完成这些 bucket 内部所有键的迁移。当然，如果 ht[0] 哈希表中的数据已经都迁移完成了，键拷贝的循环也会停止执行\n 2. 在完成了 n 个 bucket 拷贝后，dictrehash 函数的第二部分逻辑，就是判断 ht[0] 表中数据是否都已迁移完。如果都迁移完了，那么 ht[0] 的空间会被释放。因为 redis 在处理请求时，代码逻辑中都是使用 ht[0]，所以当 rehash 执行完成后，虽然数据都在 ht[1] 中了，但 redis 仍然会把 ht[1] 赋值给 ht[0]，以便其他部分的代码逻辑正常使用\n 3. 在 ht[1] 赋值给 ht[0] 后，它的大小就会被重置为 0，等待下一次 rehash。与此同时， 全局哈希表中的 rehashidx 变量会被标为 -1，表示 rehash 结束了（这里的 rehashidx 变量用来表示 rehash 的进度，稍后我会给你具体解释）。\n\n\n\nint dictrehash(dict *d, int n) {\n    int empty_visits = n*10; /* max number of empty buckets to visit. */\n    if (!dictisrehashing(d)) return 0;\n\n    // 主循环，根据要拷贝的bucket数量n，循环n次后停止或ht[0]中的数据迁移完停止\n    while(n-- && d->ht[0].used != 0) {\n        dictentry *de, *nextde;\n\n        /* note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */\n        assert(d->ht[0].size > (unsigned long)d->rehashidx);\n        while(d->ht[0].table[d->rehashidx] == null) {\n            d->rehashidx++;\n            if (--empty_visits == 0) return 1;\n        }\n        de = d->ht[0].table[d->rehashidx];\n        /* move all the keys in this bucket from the old to the new hash ht */\n        while(de) {\n            uint64_t h;\n\n            nextde = de->next;\n            /* get the index in the new hash table */\n            h = dicthashkey(d, de->key) & d->ht[1].sizemask;\n            de->next = d->ht[1].table[h];\n            d->ht[1].table[h] = de;\n            d->ht[0].used--;\n            d->ht[1].used++;\n            de = nextde;\n        }\n        d->ht[0].table[d->rehashidx] = null;\n        d->rehashidx++;\n    }\n\n    //判断ht[0]的数据是否迁移完成\n    /* check if we already rehashed the whole table... */\n    if (d->ht[0].used == 0) {\n        // ht[0]迁移完后，释放ht[0]内存空间\n        zfree(d->ht[0].table);\n        // 让ht[0]指向ht[1]，以便接受正常的请求\n        d->ht[0] = d->ht[1];\n        // 重置ht[1]的大小为0\n        _dictreset(&d->ht[1]);\n        // 设置全局哈希表的rehashidx标识为-1，表示rehash结束\n        d->rehashidx = -1;\n        // 返回0，表示ht[0]中所有元素都迁移完\n        return 0;\n    }\n\n    //返回1，表示ht[0]中仍然有元素没有迁移完\n    /* more to rehash... */\n    return 1;\n}\n\n\n那么，渐进式 rehash 是如何按照 bucket 粒度拷贝数据的，这其实就和全局哈希表 dict 结构中的 rehashidx 变量相关了\n\nrehashidx 变量表示的是当前 rehash 在对哪个 bucket 做数据迁移。比如，当 rehashidx 等于 0 时，表示对 ht[0]中的第一个 bucket 进行数据迁移；当 rehashidx 等于 1 时，表 示对 ht[0] 中的第二个 bucket 进行数据迁移，以此类推。\n\n而 dictrehash 函数的主循环，首先会判断 rehashidx 指向的 bucket 是否为空，如果为空，那就将 rehashidx 的值加 1，检查下一个 bucket。\n\n那么，有没有可能连续几个 bucket 都为空呢？其实是有可能的，在这种情况下，渐进式 rehash 不会一直递增 rehashidx 进行检查。这是因为一旦执行了 rehash，redis 主线程就无法处理其他请求了。\n\n所以，渐进式 rehash 在执行时设置了一个变量 empty_visits，用来表示已经检查过的空 bucket，当检查了一定数量的空 bucket 后，这一轮的 rehash 就停止执行，转而继续处理外来请求，避免了对 redis 性能的影响。下面的代码显示了这部分逻辑，你可以看下。\n\n// 如果当前要迁移的 bucket 中没有元素\nwhile(d->ht[0].table[d->rehashidx] == null) {\n    d->rehashidx++;\n    if (--empty_visits == 0) return 1;\n}\n\n\n而如果 rehashidx 指向的 bucket 有数据可以迁移，那么 redis 就会把这个 bucket 中的哈希项依次取出来，并根据 ht[1] 的表空间大小，重新计算哈希项在 ht[1] 中的 bucket 位置，然后把这个哈希项赋值到 ht[1] 对应 bucket 中\n\n这样，每做完一个哈希项的迁移，ht[0] 和 ht[1] 用来表示承载哈希项多少的变量 used，就 会分别减一和加一。当然，如果当前 rehashidx 指向的 bucket 中数据都迁移完了， rehashidx 就会递增加 1，指向下一个 bucket。下面的代码显示了这一迁移过程。\n\n while(n-- && d->ht[0].used != 0) {\n     dictentry *de, *nextde;\n\n     /* note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */\n     assert(d->ht[0].size > (unsigned long)d->rehashidx);\n     while(d->ht[0].table[d->rehashidx] == null) {\n         d->rehashidx++;\n         if (--empty_visits == 0) return 1;\n     }\n     // 获得哈希表中哈希项\n     de = d->ht[0].table[d->rehashidx];\n     /* move all the keys in this bucket from the old to the new hash ht */\n     while(de) {\n         uint64_t h;\n     \t// 获得同一个bucket中下一个哈希项\n         nextde = de->next;\n         /* get the index in the new hash table */\n         // 根据扩容后的哈希表ht[1]大小，计算当前哈希项在扩容后哈希表中的bucket位置\n         h = dicthashkey(d, de->key) & d->ht[1].sizemask;\n         // 将当前哈希项添加到扩容后的哈希表ht[1]中\n         de->next = d->ht[1].table[h];\n         d->ht[1].table[h] = de;\n         // 减少当前哈希表的哈希项个数\n         d->ht[0].used--;\n         // 增加扩容后哈希表的哈希项个数\n         d->ht[1].used++;\n         de = nextde;\n     }\n     // 如果当前bucket中已经没有哈希项了，将该bucket置为null\n     d->ht[0].table[d->rehashidx] = null;\n     // 将rehash加1，下一次将迁移下一个bucket中的元素\n     d->rehashidx++;\n }\n\n\n好了，到这里，我们就已经基本了解了 dictrehash 函数的全部逻辑。 现在我们知道，dictrehash 函数本身是按照 bucket 粒度执行哈希项迁移的，它内部执行的 bucket 迁移个数，主要由传入的循环次数变量 n 来决定。但凡 redis 要进行 rehash 操作，最终都会调用 dictrehash 函数。\n\n接下来，我们来学习和渐进式 rehash 相关的第二个关键函数 _dictrehashstep，这个函数实现了每次只对一个 bucket 执行 rehash。 从 redis 的源码中我们可以看到，一共会有 5 个函数通过调用 _dictrehashstep 函数，进而调用 dictrehash 函数，来执行 rehash，它们分别是：dictaddraw， dictgenericdelete，dictfind，dictgetrandomkey，dictgetsomekeys。\n\n其中，dictaddraw 和 dictgenericdelete 函数，分别对应了往 redis 中增加和删除键值对，而后三个函数则对应了在 redis 中进行查询操作。下图展示了这些函数间的调用关系：\n\n但你要注意，不管是增删查哪种操作，这 5 个函数调用的 _dictrehashstep 函数，给 dictrehash 传入的循环次数变量 n 的值都为 1，下面的代码就显示了这一传参的情况\n\nstatic void _dictrehashstep(dict *d) {\n    // 给dictrehash传入的循环次数参数为1，表明每迁移完一个bucket ，就执行正常操作\n    if (d->pauserehash == 0) dictrehash(d,1);\n}\n\n\n这样一来，每次迁移完一个 bucket，hash 表就会执行正常的增删查请求操作，这就是在代码层面实现渐进式 rehash 的方法\n\n\n# 总结\n\n 1. 通过「链表」解决hash冲突\n 2. redis 通过 「渐进式 rehash」 来解决大量数据 rehash 可能会导致的阻塞问题\n 3. 渐进式 rehash 按照 bucket 粒度拷贝数据的方法\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"ZSet 设计与实现",frontmatter:{title:"ZSet 设计与实现",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d4312/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/02.%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/10.ZSet%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Redis 系统设计/02.二、基础知识/10.ZSet 设计与实现.md",key:"v-188a6e0c",path:"/pages/2d4312/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:244},{level:2,title:"Sorted Set 基本结构",slug:"sorted-set-基本结构",normalizedTitle:"sorted set 基本结构",charIndex:769},{level:2,title:"跳表的设计与实现",slug:"跳表的设计与实现",normalizedTitle:"跳表的设计与实现",charIndex:1618},{level:3,title:"跳表数据结构",slug:"跳表数据结构",normalizedTitle:"跳表数据结构",charIndex:2124},{level:3,title:"跳表结点查询",slug:"跳表结点查询",normalizedTitle:"跳表结点查询",charIndex:3619},{level:3,title:"跳表结点层数设置",slug:"跳表结点层数设置",normalizedTitle:"跳表结点层数设置",charIndex:4325},{level:2,title:"哈希表和跳表的组合使用",slug:"哈希表和跳表的组合使用",normalizedTitle:"哈希表和跳表的组合使用",charIndex:6213},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:8982},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:9771}],headersStr:"前言 Sorted Set 基本结构 跳表的设计与实现 跳表数据结构 跳表结点查询 跳表结点层数设置 哈希表和跳表的组合使用 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. Sorted Set 如何高效支持范围查询和单点查询？\n 2. Redis 为什么使用跳表而非红黑树实现有序集合？\n 3. 跳表的多层链表如何优化查找性能？\n 4. 跳表中随机生成结点层数有什么优势？如何影响插入和查询效率？\n 5. 跳表和哈希表在 Sorted Set 中如何协同工作保持数据一致性？\n 6. 若哈希表和跳表数据不一致，Sorted Set 是否还能高效查询？\n 7. 跳表和哈希表组合的设计对其他数据结构和系统开发的启示？\n\n\n# 前言\n\n有序集合（Sorted Set）：它本身是集合类型，同时也可以支持集合中的元素带有权重，并按权重排序\n\n但是，为什么 Sorted Set 能同时提供以下两种操作接口，以及它们的复杂度分别是 O(logN)+M 和 O(1) 呢？\n\n * ZRANGEBYSCORE：按照元素权重返回一个范围内的元素\n * ZSCORE：返回某个元素的权重值\n\n实际上，这个问题背后的本质是：为什么 Sorted Set 既能支持高效的范围查询，同时还能以 O(1) 复杂度获取元素权重值？\n\n这其实就和 Sorted Set 底层的设计实现有关了\n\n * Sorted Set 能支持范围查询，这是因为它的核心数据结构设计采用了跳表\n * 它又能以常数复杂度获取元素权重，这是因为它同时采用了哈希表进行索引\n\n那么，你是不是很好奇，Sorted Set 是如何把这两种数据结构结合在一起的？它们又是如何进行协作的呢？\n\n让 echo 来给你介绍下 Sorted Set 采用的双索引的设计思想和实现。理解和掌握这种双索引的设计思想，对于我们实现数据库系统是具有非常重要的参考价值的。\n\n好，接下来，我们就先来看看 Sorted Set 的基本结构\n\n\n# Sorted Set 基本结构\n\n> Redis 源码中，Sorted Set 的代码文件和其他数据类型不太一样，它并不像哈希表的 dict.c/dict.h，或是压缩列表的 ziplist.c/ziplist.h，具有专门的数据结构实现和定义文件\n> \n> Sorted Set 的实现代码在 t_zset.c 文件中，包括 Sorted Set 的各种操作实现，同时 Sorted Set 相关的结构定义在server.h文件中。如果你想要了解学习 Sorted Set 的模块和操作，注意要从 t_zset.c 和 server.h 这两个文件中查找\n\n我们可以先来看下它的结构定义。Sorted Set 结构体的名称为 zset，其中包含了两个成员，分别是哈希表 dict 和跳表 zsl，如下所示。\n\ntypedef struct zset {\n    dict *dict;\n    zskiplist *zsl;\n} zset;\n\n\nSorted Set 这种同时采用跳表和哈希表两个索引结构的设计思想。这种设计思想充分融合了：\n\n * 跳表高效支持范围查询（如 ZRANGEBYSCORE 操作）\n * 以及哈希表高效支持单点查询（如 ZSCORE 操作）的特征\n\n这样一来，我们就可以在一个数据结构中，同时高效支持范围查询和单点查询，这是单一索引结构比较难达到的效果\n\n提示\n\n感觉很多数据结构如果要支持 O(1)的查询复杂度的话，底层都得用 hash，比如常规 LRU 算法下的也有 hash 的影子 LRU 算法\n\n既然 Sorted Set 采用了跳表和哈希表两种索引结构来组织数据，我们在实现 Sorted Set 时就会面临以下两个问题：\n\n * 跳表或是哈希表中，各自保存了什么样的数据？\n * 跳表和哈希表保存的数据是如何保持一致的？\n\n因为我已经在 Hash 中给你介绍了 Redis 中哈希表的实现思路，所以接下来，echo 给你介绍下跳表的设计和实现\n\n\n# 跳表的设计与实现\n\n首先，我们来了解下什么是跳表（skiplist）。\n\n「跳表」其实是一种多层的有序链表\n\n> 为了便于说明，我把跳表中的层次从低到高排个序，最底下一层称为 level0，依次往上是 level1、level2 等\n\n下图展示的是一个 3 层的跳表。其中，头结点中包含了三个指针，分别作为 leve0 到 level2 上的头指针。\n\n\n\n可以看到，在 level 0 上一共有 7 个结点，分别是 3、11、23、33、42、51、62，这些结点会通过指针连接起来，同时头结点中的 level0 指针会指向结点 3。然后，在这 7 个结点中，结点 11、33 和 51 又都包含了一个指针，同样也依次连接起来，且头结点的 level 1 指针会指向结点 11。这样一来，这 3 个结点就组成了 level 1 上的所有结点。\n\n最后，结点 33 中还包含了一个指针，这个指针会指向尾结点，同时，头结点的 level 2 指针会指向结点 33，这就形成了 level 2，只不过 level 2 上只有 1 个结点 33。\n\n在对跳表有了直观印象后，我们再来看看跳表实现的具体数据结构\n\n\n# 跳表数据结构\n\n我们先来看下跳表结点的结构定义\n\ntypedef struct zskiplistNode {\n    //Sorted Set中的元素\n    sds ele;\n    //元素权重值\n    double score;\n    //后向指针\n    struct zskiplistNode *backward;\n    //节点的level数组，保存每层上的前向指针和跨度\n    struct zskiplistLevel {\n        struct zskiplistNode *forward;\n        unsigned long span;\n    } level[];\n} zskiplistNode;\n\n\n * 因为 Sorted Set 中既要保存元素，也要保存元素的权重，所以对应到跳表结点的结构定义中，就对应了 sds 类型的变量 ele，以及 double 类型的变量 score。此外，为了便于从跳表的尾结点进行倒序查找，每个跳表结点中还保存了一个后向指针（*backward），指向该结点的前一个结点。\n * 因为跳表是一个多层的有序链表，每一层也是由多个结点通过指针连接起来的。因此在跳表结点的结构定义中，还包含了一个 zskiplistLevel 结构体类型的 level 数组。\n\nlevel 数组中的每一个元素对应了一个 zskiplistLevel 结构体，也对应了跳表的一层。而 zskiplistLevel 结构体定义了一个指向下一结点的前向指针（*forward），这就使得结点可以在某一层上和后续结点连接起来。同时，zskiplistLevel 结构体中还定义了，这是用来记录结点在某一层上的 跨度 *forward 指针和该指针指向的结点之间，跨越了 level0 上的几个结点。\n\n我们来看下面这张图，其中就展示了 33 结点的 level 数组和跨度情况。可以看到，33 结点的 level 数组有三个元素，分别对应了三层 level 上的指针。此外，在 level 数组中，level 2、level1 和 level 0 的跨度 span 值依次是 3、2、1。\n\n\n\n最后，因为跳表中的结点都是按序排列的，所以，对于跳表中的某个结点，我们可以把从头结点到该结点的查询路径上，各个结点在所查询层次上的*forward 指针跨度，做一个累加。这个累加值就可以用来计算该结点在整个跳表中的顺序，另外这个结构特点还可以用来实现 Sorted Set 的 rank 操作，比如 ZRANK、ZREVRANK 等。\n\n了解了跳表结点的定义后，我们可以来看看跳表的定义。在跳表的结构中，定义了跳表的头结点和尾结点、跳表的长度，以及跳表的最大层数\n\ntypedef struct zskiplist {\n    struct zskiplistNode *header, *tail;\n    unsigned long length;\n    int level;\n} zskiplist;\n\n\n因为跳表的每个结点都是通过指针连接起来的，所以我们在使用跳表时，只需要从跳表结构体中获得头结点或尾结点，就可以通过结点指针访问到跳表中的各个结点\n\n那么，当我们在 Sorted Set 中查找元素时，就对应到了 Redis 在跳表中查找结点，而此时，查询代码是否需要像查询常规链表那样，逐一顺序查询比较链表中的每个结点呢？\n\n其实是不用的，因为这里的查询代码，可以使用跳表结点中的 level 数组来加速查询\n\n\n# 跳表结点查询\n\n事实上，当查询一个结点时，跳表会先从头结点的最高层开始，查找下一个结点。而由于跳表结点同时保存了元素和权重，所以跳表在比较结点时，相应地有两个判断条件：\n\n 1. 当查找到的结点保存的元素权重，比要查找的权重小时，跳表就会继续访问该层上的下一个结点。\n 2. 当查找到的结点保存的元素权重，等于要查找的权重时，跳表会再检查该结点保存的 SDS 类型数据，是否比要查找的 SDS 数据小。如果结点数据小于要查找的数据时，跳表仍然会继续访问该层上的下一个结点。\n\n但是，当上述两个条件都不满足时，跳表就会用到当前查找到的结点的 level 数组了。跳表会使用当前结点 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。\n\n这部分的代码逻辑如下所示，因为在跳表中进行查找、插入、更新或删除操作时，都需要用到查询的功能，你可以重点了解下。\n\n//获取跳表的表头\nx = zsl->header;\n//从最大层数开始逐一遍历\nfor (i = zsl->level-1; i >= 0; i--) {\n   ...\n   while (x->level[i].forward && (x->level[i].forward->score < score || (x->level[i].forward->score == score\n    && sdscmp(x->level[i].forward->ele,ele) < 0))) {\n      ...\n      x = x->level[i].forward;\n    }\n    ...\n}\n\n\n\n# 跳表结点层数设置\n\n这样一来，有了 level 数组之后，一个跳表结点就可以在多层上被访问到了。而一个结点的 level 数组的层数也就决定了，该结点可以在几层上被访问到。\n\n所以，当我们要决定结点层数时，实际上是要决定 level 数组具体有几层。\n\n一种设计方法是，让每一层上的结点数约是下一层上结点数的一半，就像下面这张图展示的。第 0 层上的结点数是 7，第 1 层上的结点数是 3，约是第 0 层上结点数的一半。而第 2 层上的结点就 33 一个，约是第 1 层结点数的一半。\n\n\n\n这种设计方法带来的好处是，当跳表从最高层开始进行查找时，由于每一层结点数都约是下一层结点数的一半，这种查找过程就类似于二分查找，查找复杂度可以降低到 O(logN)。\n\n但这种设计方法也会带来负面影响，那就是为了维持相邻两层上结点数的比例为 2:1，一旦有新的结点插入或是有结点被删除，那么插入或删除处的结点，及其后续结点的层数都需要进行调整，而这样就带来了额外的开销。\n\n我先来给你举个例子，看下不维持结点数比例的影响，这样虽然可以不调整层数，但是会增加查询复杂度。\n\n首先，假设当前跳表有 3 个结点，其数值分别是 3、11、23，如下图所示。\n\n\n\n接着，假设现在要插入一个结点 15，如果我们不调整其他结点的层数，而是直接插入结点 15 的话，那么插入后，跳表 level 0 和 level 1 两层上的结点数比例就变成了为 4:1，如下图所示。\n\n\n\n而假设我们持续插入多个结点，但是仍然不调整其他结点的层数，这样一来，level0 上的结点数就会越来越多，如下图所示。\n\n\n\n相应的，如果我们要查找大于 11 的结点，就需要在 level 0 的结点中依次顺序查找，复杂度就是 O(N) 了。所以，为了降低查询复杂度，我们就需要维持相邻层结点数间的关系。\n\n好，接下来，我们再来看下维持相邻层结点数为 2:1 时的影响。\n\n比如，我们可以把结点 23 的 level 数组中增加一层指针，如下图所示。这样一来，level 0 和 level 1 上的结点数就维持在了 2:1。但相应的代价就是，我们也需要给 level 数组重新分配空间，以便增加一层指针。\n\n\n\n类似的，如果我们要在有 7 个结点的跳表中删除结点 33，那么结点 33 后面的所有结点都要进行调整：\n\n\n\n调整后的跳表如下图所示。你可以看到，结点 42 和 62 都要新增 level 数组空间，这样能分别保存 3 层的指针和 2 层的指针，而结点 51 的 level 数组则需要减少一层。也就是说，这样的调整会带来额外的操作开销。\n\n\n\n因此，为了避免上述问题，跳表在创建结点时，采用的是另一种设计方法，即随机生成每个结点的层数。此时，相邻两层链表上的结点数并不需要维持在严格的 2:1 关系。这样一来，当新插入一个结点时，只需要修改前后结点的指针，而其他结点的层数就不需要随之改变了，这就降低了插入操作的复杂度。\n\n在 Redis 源码中，跳表结点层数是由 zslRandomLevel 函数决定。zslRandomLevel 函数会把层数初始化为 1，这也是结点的最小层数。然后，该函数会生成随机数，如果随机数的值小于 ZSKIPLIST_P（指跳表结点增加层数的概率，值为 0.25），那么层数就增加 1 层。因为随机数取值到[0,0.25) 范围内的概率不超过 25%，所以这也就表明了，每增加一层的概率不超过 25%。下面的代码展示了 zslRandomLevel 函数的执行逻辑，你可以看下。\n\n#define ZSKIPLIST_MAXLEVEL 64  //最大层数为64\n#define ZSKIPLIST_P 0.25       //随机数的值为0.25\nint zslRandomLevel(void) {\n    //初始化层为1\n    int level = 1;\n    while ((random()&0xFFFF) < (ZSKIPLIST_P * 0xFFFF))\n        level += 1;\n    return (level<ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;\n}\n\n\n好，现在我们就了解了跳表的基本结构、查询方式和结点层数设置方法，那么下面我们接着来学习下，Sorted Set 中是如何将跳表和哈希表组合起来使用的，以及是如何保持这两个索引结构中的数据是一致的。\n\n\n# 哈希表和跳表的组合使用\n\ntypedef struct zset {\n    dict *dict;\n    zskiplist *zsl;\n} zset;\n\n\nSorted Set 中已经同时包含了 hash 和 skiplist，这就是组合使用两者的第一步。然后，我们还可以在 Sorted Set 的创建代码（t_zset.c文件）中，进一步看到跳表和哈希表被相继创建\n\n当创建一个 zset 时，代码中会相继调用 **dictCreate 函数 **创建 zset 中的哈希表，以及调用 **zslCreate 函数 **创建跳表，如下所示。\n\n zs = zmalloc(sizeof(*zs));\n zs->dict = dictCreate(&zsetDictType,NULL);\n zs->zsl = zslCreate();\n\n\n我们要想组合使用它们，必须保持这两个索引结构中的数据一致。简单来说，这就需要我们在往跳表中插入数据时，同时也向哈希表中插入数据。\n\n而这种保持两个索引结构一致的做法其实也不难，当往 Sorted Set 中插入数据时，zsetAdd 函数就会被调用。所以，我们可以通过阅读 Sorted Set 的元素添加函数 zsetAdd 了解到。下面我们就来分析一下 zsetAdd 函数的执行过程。\n\n首先，zsetAdd 函数会判定 Sorted Set 采用的是 ziplist 还是 skiplist 的编码方式。zsetAdd 函数会判定 Sorted Set 采用的是 ziplist 还是 skiplist 的编码方式。\n\n注意，在不同编码方式下，zsetAdd 函数的执行逻辑也有所区别。这一讲我们重点关注的是 skiplist 的编码方式，所以接下来，我们就主要来看看当采用 skiplist 编码方式时，zsetAdd 函数的逻辑是什么样的。\n\nzsetAdd 函数会先使用哈希表的 dictFind 函数，查找要插入的元素是否存在。如果不存在，就直接调用跳表元素插入函数 zslInsert 和哈希表元素插入函数 dictAdd，将新元素分别插入到跳表和哈希表中。\n\n这里你需要注意的是，Redis 并没有把哈希表的操作嵌入到跳表本身的操作函数中，而是在 zsetAdd 函数中依次执行以上两个函数。这样设计的好处是保持了跳表和哈希表两者操作的独立性。\n\n * 然后，如果 zsetAdd 函数通过 dictFind 函数发现要插入的元素已经存在，那么 zsetAdd 函数会判断是否要增加元素的权重值\n\n如果权重值发生了变化，zsetAdd 函数就会调用 zslUpdateScore 函数，更新跳表中的元素权重值。紧接着，zsetAdd 函数会把哈希表中该元素（对应哈希表中的 key）的 value 指向跳表结点中的权重值，这样一来，哈希表中元素的权重值就可以保持最新值了。\n\n下面的代码显示了 zsetAdd 函数的执行流程，你可以看下。\n\n //如果采用ziplist编码方式时，zsetAdd函数的处理逻辑\n if (zobj->encoding == OBJ_ENCODING_ZIPLIST) {\n   ...\n}\n//如果采用skiplist编码方式时，zsetAdd函数的处理逻辑\nelse if (zobj->encoding == OBJ_ENCODING_SKIPLIST) {\n        zset *zs = zobj->ptr;\n        zskiplistNode *znode;\n        dictEntry *de;\n        //从哈希表中查询新增元素\n        de = dictFind(zs->dict,ele);\n        //如果能查询到该元素\n        if (de != NULL) {\n            /* NX? Return, same element already exists. */\n            if (nx) {\n                *flags |= ZADD_NOP;\n                return 1;\n            }\n            //从哈希表中查询元素的权重\n            curscore = *(double*)dictGetVal(de);\n\n\n            //如果要更新元素权重值\n            if (incr) {\n                //更新权重值\n               ...\n            }\n\n\n            //如果权重发生变化了\n            if (score != curscore) {\n                //更新跳表结点\n                znode = zslUpdateScore(zs->zsl,curscore,ele,score);\n                //让哈希表元素的值指向跳表结点的权重\n                dictGetVal(de) = &znode->score;\n                ...\n            }\n            return 1;\n        }\n       //如果新元素不存在\n        else if (!xx) {\n            ele = sdsdup(ele);\n            //新插入跳表结点\n            znode = zslInsert(zs->zsl,score,ele);\n            //新插入哈希表元素\n            serverAssert(dictAdd(zs->dict,ele,&znode->score) == DICT_OK);\n            ...\n            return 1;\n        }\n        ..\n\n\n总之，你可以记住的是，Sorted Set 先是通过在它的数据结构中同时定义了跳表和哈希表，来实现同时使用这两种索引结构。然后，Sorted Set 在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。\n\n这样一来，Sorted Set 既可以使用跳表支持数据的范围查询，还能使用哈希表支持根据元素直接查询它的权重。\n\n\n# 总结\n\n 1. Sorted Set 的设计目标 Redis 的 Sorted Set 数据类型需要同时支持两种查询需求：\n    * 范围查询：根据元素的权重范围进行查询。\n    * 单点查询：快速查找特定元素及其权重。\n 2. 跳表的设计\n    * 跳表结构：跳表是一个多层的有序链表，顶层结点数最少，底层结点数最多。\n    * 查询过程：查询时，从顶层开始，通过高层节点大跨度跳跃查找，如果找到第一个大于待查元素的结点，就转向下一层继续查找，直到找到待查元素。\n    * 优化查询效率：这种从高层到低层的分层查找方式，极大地减少了查询的时间开销，相比普通链表的线性查找，跳表的查询效率更高。\n    * 随机层数：跳表采用随机算法确定每个结点的层数，避免新增结点时发生连锁更新，提高插入效率。\n 3. 哈希表的引入\n    * 哈希表作为索引：Sorted Set 还将每个元素保存在哈希表中，元素作为哈希表的 key，其权重作为 value。\n    * 单点查询效率提升：通过哈希表可以直接查找到特定元素及其权重，相较于跳表的范围查找，哈希表的查找效率更高，更适合针对单个元素的查询。\n 4. 组合索引设计\n    * 跳表 + 哈希表：Redis Sorted Set 通过组合使用跳表和哈希表两种数据结构，实现了既支持范围查询（跳表），又能快速进行单点查询（哈希表）的设计。\n    * 设计优势：这种组合设计使得 Redis Sorted Set 能在不同的查询场景下兼顾效率，既避免了跳表插入时的性能问题，又利用哈希表提升了单点查询速度。\n 5. 设计思路的应用\n    * 系统开发启示：在实际系统开发中，组合使用多种索引结构可以有效提升数据管理的效率。Redis Sorted Set 的设计思路是一个典型案例，值得在其他开发场景中借鉴。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. sorted set 如何高效支持范围查询和单点查询？\n 2. redis 为什么使用跳表而非红黑树实现有序集合？\n 3. 跳表的多层链表如何优化查找性能？\n 4. 跳表中随机生成结点层数有什么优势？如何影响插入和查询效率？\n 5. 跳表和哈希表在 sorted set 中如何协同工作保持数据一致性？\n 6. 若哈希表和跳表数据不一致，sorted set 是否还能高效查询？\n 7. 跳表和哈希表组合的设计对其他数据结构和系统开发的启示？\n\n\n# 前言\n\n有序集合（sorted set）：它本身是集合类型，同时也可以支持集合中的元素带有权重，并按权重排序\n\n但是，为什么 sorted set 能同时提供以下两种操作接口，以及它们的复杂度分别是 o(logn)+m 和 o(1) 呢？\n\n * zrangebyscore：按照元素权重返回一个范围内的元素\n * zscore：返回某个元素的权重值\n\n实际上，这个问题背后的本质是：为什么 sorted set 既能支持高效的范围查询，同时还能以 o(1) 复杂度获取元素权重值？\n\n这其实就和 sorted set 底层的设计实现有关了\n\n * sorted set 能支持范围查询，这是因为它的核心数据结构设计采用了跳表\n * 它又能以常数复杂度获取元素权重，这是因为它同时采用了哈希表进行索引\n\n那么，你是不是很好奇，sorted set 是如何把这两种数据结构结合在一起的？它们又是如何进行协作的呢？\n\n让 echo 来给你介绍下 sorted set 采用的双索引的设计思想和实现。理解和掌握这种双索引的设计思想，对于我们实现数据库系统是具有非常重要的参考价值的。\n\n好，接下来，我们就先来看看 sorted set 的基本结构\n\n\n# sorted set 基本结构\n\n> redis 源码中，sorted set 的代码文件和其他数据类型不太一样，它并不像哈希表的 dict.c/dict.h，或是压缩列表的 ziplist.c/ziplist.h，具有专门的数据结构实现和定义文件\n> \n> sorted set 的实现代码在 t_zset.c 文件中，包括 sorted set 的各种操作实现，同时 sorted set 相关的结构定义在server.h文件中。如果你想要了解学习 sorted set 的模块和操作，注意要从 t_zset.c 和 server.h 这两个文件中查找\n\n我们可以先来看下它的结构定义。sorted set 结构体的名称为 zset，其中包含了两个成员，分别是哈希表 dict 和跳表 zsl，如下所示。\n\ntypedef struct zset {\n    dict *dict;\n    zskiplist *zsl;\n} zset;\n\n\nsorted set 这种同时采用跳表和哈希表两个索引结构的设计思想。这种设计思想充分融合了：\n\n * 跳表高效支持范围查询（如 zrangebyscore 操作）\n * 以及哈希表高效支持单点查询（如 zscore 操作）的特征\n\n这样一来，我们就可以在一个数据结构中，同时高效支持范围查询和单点查询，这是单一索引结构比较难达到的效果\n\n提示\n\n感觉很多数据结构如果要支持 o(1)的查询复杂度的话，底层都得用 hash，比如常规 lru 算法下的也有 hash 的影子 lru 算法\n\n既然 sorted set 采用了跳表和哈希表两种索引结构来组织数据，我们在实现 sorted set 时就会面临以下两个问题：\n\n * 跳表或是哈希表中，各自保存了什么样的数据？\n * 跳表和哈希表保存的数据是如何保持一致的？\n\n因为我已经在 hash 中给你介绍了 redis 中哈希表的实现思路，所以接下来，echo 给你介绍下跳表的设计和实现\n\n\n# 跳表的设计与实现\n\n首先，我们来了解下什么是跳表（skiplist）。\n\n「跳表」其实是一种多层的有序链表\n\n> 为了便于说明，我把跳表中的层次从低到高排个序，最底下一层称为 level0，依次往上是 level1、level2 等\n\n下图展示的是一个 3 层的跳表。其中，头结点中包含了三个指针，分别作为 leve0 到 level2 上的头指针。\n\n\n\n可以看到，在 level 0 上一共有 7 个结点，分别是 3、11、23、33、42、51、62，这些结点会通过指针连接起来，同时头结点中的 level0 指针会指向结点 3。然后，在这 7 个结点中，结点 11、33 和 51 又都包含了一个指针，同样也依次连接起来，且头结点的 level 1 指针会指向结点 11。这样一来，这 3 个结点就组成了 level 1 上的所有结点。\n\n最后，结点 33 中还包含了一个指针，这个指针会指向尾结点，同时，头结点的 level 2 指针会指向结点 33，这就形成了 level 2，只不过 level 2 上只有 1 个结点 33。\n\n在对跳表有了直观印象后，我们再来看看跳表实现的具体数据结构\n\n\n# 跳表数据结构\n\n我们先来看下跳表结点的结构定义\n\ntypedef struct zskiplistnode {\n    //sorted set中的元素\n    sds ele;\n    //元素权重值\n    double score;\n    //后向指针\n    struct zskiplistnode *backward;\n    //节点的level数组，保存每层上的前向指针和跨度\n    struct zskiplistlevel {\n        struct zskiplistnode *forward;\n        unsigned long span;\n    } level[];\n} zskiplistnode;\n\n\n * 因为 sorted set 中既要保存元素，也要保存元素的权重，所以对应到跳表结点的结构定义中，就对应了 sds 类型的变量 ele，以及 double 类型的变量 score。此外，为了便于从跳表的尾结点进行倒序查找，每个跳表结点中还保存了一个后向指针（*backward），指向该结点的前一个结点。\n * 因为跳表是一个多层的有序链表，每一层也是由多个结点通过指针连接起来的。因此在跳表结点的结构定义中，还包含了一个 zskiplistlevel 结构体类型的 level 数组。\n\nlevel 数组中的每一个元素对应了一个 zskiplistlevel 结构体，也对应了跳表的一层。而 zskiplistlevel 结构体定义了一个指向下一结点的前向指针（*forward），这就使得结点可以在某一层上和后续结点连接起来。同时，zskiplistlevel 结构体中还定义了，这是用来记录结点在某一层上的 跨度 *forward 指针和该指针指向的结点之间，跨越了 level0 上的几个结点。\n\n我们来看下面这张图，其中就展示了 33 结点的 level 数组和跨度情况。可以看到，33 结点的 level 数组有三个元素，分别对应了三层 level 上的指针。此外，在 level 数组中，level 2、level1 和 level 0 的跨度 span 值依次是 3、2、1。\n\n\n\n最后，因为跳表中的结点都是按序排列的，所以，对于跳表中的某个结点，我们可以把从头结点到该结点的查询路径上，各个结点在所查询层次上的*forward 指针跨度，做一个累加。这个累加值就可以用来计算该结点在整个跳表中的顺序，另外这个结构特点还可以用来实现 sorted set 的 rank 操作，比如 zrank、zrevrank 等。\n\n了解了跳表结点的定义后，我们可以来看看跳表的定义。在跳表的结构中，定义了跳表的头结点和尾结点、跳表的长度，以及跳表的最大层数\n\ntypedef struct zskiplist {\n    struct zskiplistnode *header, *tail;\n    unsigned long length;\n    int level;\n} zskiplist;\n\n\n因为跳表的每个结点都是通过指针连接起来的，所以我们在使用跳表时，只需要从跳表结构体中获得头结点或尾结点，就可以通过结点指针访问到跳表中的各个结点\n\n那么，当我们在 sorted set 中查找元素时，就对应到了 redis 在跳表中查找结点，而此时，查询代码是否需要像查询常规链表那样，逐一顺序查询比较链表中的每个结点呢？\n\n其实是不用的，因为这里的查询代码，可以使用跳表结点中的 level 数组来加速查询\n\n\n# 跳表结点查询\n\n事实上，当查询一个结点时，跳表会先从头结点的最高层开始，查找下一个结点。而由于跳表结点同时保存了元素和权重，所以跳表在比较结点时，相应地有两个判断条件：\n\n 1. 当查找到的结点保存的元素权重，比要查找的权重小时，跳表就会继续访问该层上的下一个结点。\n 2. 当查找到的结点保存的元素权重，等于要查找的权重时，跳表会再检查该结点保存的 sds 类型数据，是否比要查找的 sds 数据小。如果结点数据小于要查找的数据时，跳表仍然会继续访问该层上的下一个结点。\n\n但是，当上述两个条件都不满足时，跳表就会用到当前查找到的结点的 level 数组了。跳表会使用当前结点 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。\n\n这部分的代码逻辑如下所示，因为在跳表中进行查找、插入、更新或删除操作时，都需要用到查询的功能，你可以重点了解下。\n\n//获取跳表的表头\nx = zsl->header;\n//从最大层数开始逐一遍历\nfor (i = zsl->level-1; i >= 0; i--) {\n   ...\n   while (x->level[i].forward && (x->level[i].forward->score < score || (x->level[i].forward->score == score\n    && sdscmp(x->level[i].forward->ele,ele) < 0))) {\n      ...\n      x = x->level[i].forward;\n    }\n    ...\n}\n\n\n\n# 跳表结点层数设置\n\n这样一来，有了 level 数组之后，一个跳表结点就可以在多层上被访问到了。而一个结点的 level 数组的层数也就决定了，该结点可以在几层上被访问到。\n\n所以，当我们要决定结点层数时，实际上是要决定 level 数组具体有几层。\n\n一种设计方法是，让每一层上的结点数约是下一层上结点数的一半，就像下面这张图展示的。第 0 层上的结点数是 7，第 1 层上的结点数是 3，约是第 0 层上结点数的一半。而第 2 层上的结点就 33 一个，约是第 1 层结点数的一半。\n\n\n\n这种设计方法带来的好处是，当跳表从最高层开始进行查找时，由于每一层结点数都约是下一层结点数的一半，这种查找过程就类似于二分查找，查找复杂度可以降低到 o(logn)。\n\n但这种设计方法也会带来负面影响，那就是为了维持相邻两层上结点数的比例为 2:1，一旦有新的结点插入或是有结点被删除，那么插入或删除处的结点，及其后续结点的层数都需要进行调整，而这样就带来了额外的开销。\n\n我先来给你举个例子，看下不维持结点数比例的影响，这样虽然可以不调整层数，但是会增加查询复杂度。\n\n首先，假设当前跳表有 3 个结点，其数值分别是 3、11、23，如下图所示。\n\n\n\n接着，假设现在要插入一个结点 15，如果我们不调整其他结点的层数，而是直接插入结点 15 的话，那么插入后，跳表 level 0 和 level 1 两层上的结点数比例就变成了为 4:1，如下图所示。\n\n\n\n而假设我们持续插入多个结点，但是仍然不调整其他结点的层数，这样一来，level0 上的结点数就会越来越多，如下图所示。\n\n\n\n相应的，如果我们要查找大于 11 的结点，就需要在 level 0 的结点中依次顺序查找，复杂度就是 o(n) 了。所以，为了降低查询复杂度，我们就需要维持相邻层结点数间的关系。\n\n好，接下来，我们再来看下维持相邻层结点数为 2:1 时的影响。\n\n比如，我们可以把结点 23 的 level 数组中增加一层指针，如下图所示。这样一来，level 0 和 level 1 上的结点数就维持在了 2:1。但相应的代价就是，我们也需要给 level 数组重新分配空间，以便增加一层指针。\n\n\n\n类似的，如果我们要在有 7 个结点的跳表中删除结点 33，那么结点 33 后面的所有结点都要进行调整：\n\n\n\n调整后的跳表如下图所示。你可以看到，结点 42 和 62 都要新增 level 数组空间，这样能分别保存 3 层的指针和 2 层的指针，而结点 51 的 level 数组则需要减少一层。也就是说，这样的调整会带来额外的操作开销。\n\n\n\n因此，为了避免上述问题，跳表在创建结点时，采用的是另一种设计方法，即随机生成每个结点的层数。此时，相邻两层链表上的结点数并不需要维持在严格的 2:1 关系。这样一来，当新插入一个结点时，只需要修改前后结点的指针，而其他结点的层数就不需要随之改变了，这就降低了插入操作的复杂度。\n\n在 redis 源码中，跳表结点层数是由 zslrandomlevel 函数决定。zslrandomlevel 函数会把层数初始化为 1，这也是结点的最小层数。然后，该函数会生成随机数，如果随机数的值小于 zskiplist_p（指跳表结点增加层数的概率，值为 0.25），那么层数就增加 1 层。因为随机数取值到[0,0.25) 范围内的概率不超过 25%，所以这也就表明了，每增加一层的概率不超过 25%。下面的代码展示了 zslrandomlevel 函数的执行逻辑，你可以看下。\n\n#define zskiplist_maxlevel 64  //最大层数为64\n#define zskiplist_p 0.25       //随机数的值为0.25\nint zslrandomlevel(void) {\n    //初始化层为1\n    int level = 1;\n    while ((random()&0xffff) < (zskiplist_p * 0xffff))\n        level += 1;\n    return (level<zskiplist_maxlevel) ? level : zskiplist_maxlevel;\n}\n\n\n好，现在我们就了解了跳表的基本结构、查询方式和结点层数设置方法，那么下面我们接着来学习下，sorted set 中是如何将跳表和哈希表组合起来使用的，以及是如何保持这两个索引结构中的数据是一致的。\n\n\n# 哈希表和跳表的组合使用\n\ntypedef struct zset {\n    dict *dict;\n    zskiplist *zsl;\n} zset;\n\n\nsorted set 中已经同时包含了 hash 和 skiplist，这就是组合使用两者的第一步。然后，我们还可以在 sorted set 的创建代码（t_zset.c文件）中，进一步看到跳表和哈希表被相继创建\n\n当创建一个 zset 时，代码中会相继调用 **dictcreate 函数 **创建 zset 中的哈希表，以及调用 **zslcreate 函数 **创建跳表，如下所示。\n\n zs = zmalloc(sizeof(*zs));\n zs->dict = dictcreate(&zsetdicttype,null);\n zs->zsl = zslcreate();\n\n\n我们要想组合使用它们，必须保持这两个索引结构中的数据一致。简单来说，这就需要我们在往跳表中插入数据时，同时也向哈希表中插入数据。\n\n而这种保持两个索引结构一致的做法其实也不难，当往 sorted set 中插入数据时，zsetadd 函数就会被调用。所以，我们可以通过阅读 sorted set 的元素添加函数 zsetadd 了解到。下面我们就来分析一下 zsetadd 函数的执行过程。\n\n首先，zsetadd 函数会判定 sorted set 采用的是 ziplist 还是 skiplist 的编码方式。zsetadd 函数会判定 sorted set 采用的是 ziplist 还是 skiplist 的编码方式。\n\n注意，在不同编码方式下，zsetadd 函数的执行逻辑也有所区别。这一讲我们重点关注的是 skiplist 的编码方式，所以接下来，我们就主要来看看当采用 skiplist 编码方式时，zsetadd 函数的逻辑是什么样的。\n\nzsetadd 函数会先使用哈希表的 dictfind 函数，查找要插入的元素是否存在。如果不存在，就直接调用跳表元素插入函数 zslinsert 和哈希表元素插入函数 dictadd，将新元素分别插入到跳表和哈希表中。\n\n这里你需要注意的是，redis 并没有把哈希表的操作嵌入到跳表本身的操作函数中，而是在 zsetadd 函数中依次执行以上两个函数。这样设计的好处是保持了跳表和哈希表两者操作的独立性。\n\n * 然后，如果 zsetadd 函数通过 dictfind 函数发现要插入的元素已经存在，那么 zsetadd 函数会判断是否要增加元素的权重值\n\n如果权重值发生了变化，zsetadd 函数就会调用 zslupdatescore 函数，更新跳表中的元素权重值。紧接着，zsetadd 函数会把哈希表中该元素（对应哈希表中的 key）的 value 指向跳表结点中的权重值，这样一来，哈希表中元素的权重值就可以保持最新值了。\n\n下面的代码显示了 zsetadd 函数的执行流程，你可以看下。\n\n //如果采用ziplist编码方式时，zsetadd函数的处理逻辑\n if (zobj->encoding == obj_encoding_ziplist) {\n   ...\n}\n//如果采用skiplist编码方式时，zsetadd函数的处理逻辑\nelse if (zobj->encoding == obj_encoding_skiplist) {\n        zset *zs = zobj->ptr;\n        zskiplistnode *znode;\n        dictentry *de;\n        //从哈希表中查询新增元素\n        de = dictfind(zs->dict,ele);\n        //如果能查询到该元素\n        if (de != null) {\n            /* nx? return, same element already exists. */\n            if (nx) {\n                *flags |= zadd_nop;\n                return 1;\n            }\n            //从哈希表中查询元素的权重\n            curscore = *(double*)dictgetval(de);\n\n\n            //如果要更新元素权重值\n            if (incr) {\n                //更新权重值\n               ...\n            }\n\n\n            //如果权重发生变化了\n            if (score != curscore) {\n                //更新跳表结点\n                znode = zslupdatescore(zs->zsl,curscore,ele,score);\n                //让哈希表元素的值指向跳表结点的权重\n                dictgetval(de) = &znode->score;\n                ...\n            }\n            return 1;\n        }\n       //如果新元素不存在\n        else if (!xx) {\n            ele = sdsdup(ele);\n            //新插入跳表结点\n            znode = zslinsert(zs->zsl,score,ele);\n            //新插入哈希表元素\n            serverassert(dictadd(zs->dict,ele,&znode->score) == dict_ok);\n            ...\n            return 1;\n        }\n        ..\n\n\n总之，你可以记住的是，sorted set 先是通过在它的数据结构中同时定义了跳表和哈希表，来实现同时使用这两种索引结构。然后，sorted set 在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。\n\n这样一来，sorted set 既可以使用跳表支持数据的范围查询，还能使用哈希表支持根据元素直接查询它的权重。\n\n\n# 总结\n\n 1. sorted set 的设计目标 redis 的 sorted set 数据类型需要同时支持两种查询需求：\n    * 范围查询：根据元素的权重范围进行查询。\n    * 单点查询：快速查找特定元素及其权重。\n 2. 跳表的设计\n    * 跳表结构：跳表是一个多层的有序链表，顶层结点数最少，底层结点数最多。\n    * 查询过程：查询时，从顶层开始，通过高层节点大跨度跳跃查找，如果找到第一个大于待查元素的结点，就转向下一层继续查找，直到找到待查元素。\n    * 优化查询效率：这种从高层到低层的分层查找方式，极大地减少了查询的时间开销，相比普通链表的线性查找，跳表的查询效率更高。\n    * 随机层数：跳表采用随机算法确定每个结点的层数，避免新增结点时发生连锁更新，提高插入效率。\n 3. 哈希表的引入\n    * 哈希表作为索引：sorted set 还将每个元素保存在哈希表中，元素作为哈希表的 key，其权重作为 value。\n    * 单点查询效率提升：通过哈希表可以直接查找到特定元素及其权重，相较于跳表的范围查找，哈希表的查找效率更高，更适合针对单个元素的查询。\n 4. 组合索引设计\n    * 跳表 + 哈希表：redis sorted set 通过组合使用跳表和哈希表两种数据结构，实现了既支持范围查询（跳表），又能快速进行单点查询（哈希表）的设计。\n    * 设计优势：这种组合设计使得 redis sorted set 能在不同的查询场景下兼顾效率，既避免了跳表插入时的性能问题，又利用哈希表提升了单点查询速度。\n 5. 设计思路的应用\n    * 系统开发启示：在实际系统开发中，组合使用多种索引结构可以有效提升数据管理的效率。redis sorted set 的设计思路是一个典型案例，值得在其他开发场景中借鉴。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Linux 中的 IO 多路复用",frontmatter:{title:"Linux 中的 IO 多路复用",date:"2024-09-15T17:16:08.000Z",permalink:"/pages/34fa27"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/01.Linux%20%E4%B8%AD%E7%9A%84%20IO%20%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.html",relativePath:"Redis 系统设计/03.三、主线任务/01.Linux 中的 IO 多路复用.md",key:"v-dcb44cc4",path:"/pages/34fa27/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:255},{level:3,title:"简述传统 Socket 模型",slug:"简述传统-socket-模型",normalizedTitle:"简述传统 socket 模型",charIndex:618},{level:3,title:"展望 IO 多路复用",slug:"展望-io-多路复用",normalizedTitle:"展望 io 多路复用",charIndex:2239},{level:2,title:"Linux 如何实现 IO 多路复用",slug:"linux-如何实现-io-多路复用",normalizedTitle:"linux 如何实现 io 多路复用",charIndex:2826},{level:3,title:"select 机制：多路复用的基本实现",slug:"select-机制-多路复用的基本实现",normalizedTitle:"select 机制：多路复用的基本实现",charIndex:2849},{level:3,title:"poll 机制：不受限的文件描述符数量",slug:"poll-机制-不受限的文件描述符数量",normalizedTitle:"poll 机制：不受限的文件描述符数量",charIndex:5632},{level:3,title:"epoll 机制：避免遍历每个描述符",slug:"epoll-机制-避免遍历每个描述符",normalizedTitle:"epoll 机制：避免遍历每个描述符",charIndex:7999},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:10290},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10774}],headersStr:"前言 简述传统 Socket 模型 展望 IO 多路复用 Linux 如何实现 IO 多路复用 select 机制：多路复用的基本实现 poll 机制：不受限的文件描述符数量 epoll 机制：避免遍历每个描述符 总结 参考资料",content:"提出问题是一切智慧的开端\n\n * 为什么 Redis 这样高并发的应用选择了 epoll，而不是 select 或 poll？\n\n * 在设计一个高并发服务器时，如何选择最适合的 IO 模型？\n\n * 当服务器面对成千上万的连接时，什么样的机制可以让你高效管理这些连接？\n\n * 高并发场景中，epoll 是如何通过事件通知机制避免性能瓶颈的？\n\n * 多路复用机制会监听套接字上的哪些事件？\n\n * 多路复用机制可以监听多少个套接字？\n\n * 当有套接字就绪时，多路复用机制要如何找到就绪的套接字？\n\n\n# 前言\n\nRedis 作为一个 Client-Server 架构的数据库，其源码中少不了用来实现网络通信的部分。通常系统实现网络通信的基本方法是 使用 Socket 编程模型，但是，由于基本的 Socket 编程模型是单线程阻塞的 ....\n\n所以当要处理高并发请求时，有两种方案\n\n * 多线程，让每个线程负责处理一个客户端的请求。而 Redis 负责客户端请求解析和处理的线程只有一个，那么如果直接采用基本 Socket 模型，就会影响 Redis 支持高并发的客户端访问。\n * IO 多路复用，为了实现高并发的网络通信，我们常用的 Linux 操作系统，就提供了 select、poll 和 epoll 三种编程模型，而在 Linux 上运行的 Redis，通常就会采用其中的 epoll 模型来进行网络通信。\n\n\n# 简述传统 Socket 模型\n\n我们看下使用 Socket 模型实现网络通信时的关键操作，以此帮助我们分析 Socket 模型中的不足\n\n首先，当我们需要让服务器端和客户端进行通信时，可以在服务器端通过以下三步，来创建监听客户端连接的监听套接字（Listening Socket）：\n\n 1. 调用 socket 函数，创建一个套接字。我们通常把这个套接字称为主动套接字（Active Socket）\n 2. 调用 bind 函数，将主动套接字和当前服务器的 IP 和监听端口进行绑定\n 3. 调用 listen 函数，将主动套接字转换为监听套接字，开始监听客户端的连接\n 4. 调用 accept函数，在完成上述三步之后，服务器端就可以接收客户端的连接请求了。为了能及时地收到客户端的连接请求，我们可以运行一个循环流程，在该流程中调用 accept 函数，用于接收客户端连接请求\n 5. 最后，服务器端可以通过调用 recv 或 send 函数，在刚才返回的已连接套接字上，接收并处理读写请求，或是将数据发送给客户端\n\n注意\n\naccept 函数是阻塞函数，也就是说，如果此时一直没有客户端连接请求，那么，服务器端的执行流程会一直阻塞在 accept 函数。一旦有客户端连接请求到达，accept 将不再阻塞，而是处理连接请求，和客户端建立连接，并返回已连接套接字（Connected Socket）\n\nlistenSocket = socket(); //调用socket系统调用创建一个主动套接字\nbind(listenSocket);  //绑定地址和端口\nlisten(listenSocket); //将默认的主动套接字转换为服务器使用的被动套接字，也就是监听套接字\nwhile (1) { //循环监听是否有客户端连接请求到来\n   connSocket = accept(listenSocket); //接受客户端连接\n   recv(connsocket); //从客户端读取数据，只能同时处理一个客户端\n   send(connsocket); //给客户端返回数据，只能同时处理一个客户端\n}\n\n\n你会发现，虽然上述代码能够实现服务器端和客户端之间的通信，但是程序每调用一次 accept 函数，只能处理一个客户端连接\n\n因此，如果想要处理多个并发客户端的请求，我们就需要使用多线程的方法，来处理通过 accept 函数建立的多个客户端连接上的请求\n\n使用这种方法后，我们需要在 accept 函数返回已连接套接字后，创建一个线程，并将已连接套接字传递给创建的线程，由该线程负责这个连接套接字上后续的数据读写。同时，服务器端的执行流程会再次调用 accept 函数，等待下一个客户端连接\n\n以下给出的示例代码，就展示了使用多线程来提升服务器端的并发客户端处理能力：\n\nlistenSocket = socket(); //调用socket系统调用创建一个主动套接字\nbind(listenSocket);  //绑定地址和端口\nlisten(listenSocket); //将默认的主动套接字转换为服务器使用的被动套接字，即监听套接字\nwhile (1) { //循环监听是否有客户端连接到来\n   connSocket = accept(listenSocket); //接受客户端连接，返回已连接套接字\n   pthread_create(processData, connSocket); //创建新线程对已连接套接字进行处理\n\n}\n\n//处理已连接套接字上的读写请求\nprocessData(connSocket){\n   recv(connsocket); //从客户端读取数据，只能同时处理一个客户端\n   send(connsocket); //给客户端返回数据，只能同时处理一个客户端\n}\n\n\n\n# 展望 IO 多路复用\n\n虽然上述方法能提升服务器端的并发处理能力，遗憾的是\n\nRedis 的主执行流程是由一个线程在执行，无法使用多线程的方式来提升并发处理能力。\n\n所以，该方法对 Redis 并不起作用。那么，还有没有什么其他方法，能帮助 Redis 提升并发客户端的处理能力呢？\n\n这就要用到操作系统提供的 IO 多路复用功能了\n\n在基本的 Socket 编程模型中，accept 函数只能在一个监听套接字上监听客户端的连接，recv 函数也只能在一个已连接套接字上，等待客户端发送的请求。\n\n而 IO 多路复用机制，可以让程序通过调用多路复用函数，同时监听多个套接字上的请求。这里既可以包括监听套接字上的连接请求，也可以包括已连接套接字上的读写请求。这样当有一个或多个套接字上有请求时，多路复用函数就会返回。此时，程序就可以处理这些就绪套接字上的请求，比如读取就绪的已连接套接字上的请求内容。\n\n因为 Linux 操作系统在实际应用中比较广泛，所以讲解 Linux 上的 IO 多路复用机制。\n\nLinux 提供的 IO 多路复用机制主要有三种，分别是\n\n * select\n * poll\n * epoll\n\n下面，我们就分别来学习下这三种机制的实现思路和使用方法。然后，我们再来看看，为什么 Redis 通常是选择使用 epoll 这种机制来实现网络通信\n\n\n# Linux 如何实现 IO 多路复用\n\n\n# select 机制：多路复用的基本实现\n\nselect 机制中的一个重要函数就是 select 函数\n\n对于 select 函数来说，它的参数包括\n\n * 监听的文件描述符数量__nfds\n * 被监听描述符的三个集合*__readfds、*__writefds和*__exceptfds\n * 监听时阻塞等待的超时时长*__timeout\n\nint select (int __nfds, fd_set *__readfds, fd_set *__writefds, fd_set *__exceptfds, struct timeval *__timeout)\n\n\n注意\n\nLinux 针对每一个套接字都会有一个文件描述符，也就是一个非负整数，用来唯一标识该套接字，在多路复用机制的函数中，Linux 通常会用文件描述符作为参数。有了文件描述符，函数也就能找到对应的套接字，进而进行监听、读写等操作\n\nselect 函数的参数__readfds、__writefds和__exceptfds表示的是，被监听描述符的集合，其实就是被监听套接字的集合。那么，为什么会有三个集合呢？\n\n这就和我刚才提出的第一个问题相关，也就是多路复用机制会监听哪些事件。select 函数使用三个集合，表示监听的三类事件，分别是\n\n * 读数据事件（对应__readfds集合）\n * 写数据事件（对应__writefds集合）\n * 异常事件（对应__exceptfds集合）\n\n我们进一步可以看到，参数 readfds、writefds 和 exceptfds 的类型是 fd_set 结构体，它主要定义部分如下所示。其中，__fd_mask类型是 long int 类型的别名，FD_SETSIZE 和NFDBITS 这两个宏定义的大小默认为 1024 和 32\n\ntypedef struct {\n   …\n   __fd_mask  __fds_bits[__FD_SETSIZE / __NFDBITS];\n   …\n} fd_set\n\n\n所以，fd_set 结构体的定义，其实就是一个 long int 类型的数组，该数组中一共有 32 个元素（1024/32=32），每个元素是 32 位（long int 类型的大小），而每一位可以用来表示一个文件描述符的状态\n\n好了，了解了 fd_set 结构体的定义，我们就可以回答刚才提出的第二个问题了。select 函数对每一个描述符集合，都可以监听 1024 个描述符。\n\n接下来，我们再来了解下 如何使用 select 机制来实现网络通信。\n\n 1. 首先，我们在调用 select 函数前，可以先创建好传递给 select 函数的描述符集合，然后再创建监听套接字。而为了让创建的监听套接字能被 select 函数监控，我们需要把这个套接字的描述符加入到创建好的描述符集合中。\n 2. 然后，我们就可以调用 select 函数，并把创建好的描述符集合作为参数传递给 select 函数。程序在调用 select 函数后，会发生阻塞。而当 select 函数检测到有描述符就绪后，就会结束阻塞，并返回就绪的文件描述符个数。\n 3. 此时，我们就可以在描述符集合中查找哪些描述符就绪了。然后，我们对已就绪描述符对应的套接字进行处理。比如，如果是__readfds 集合中有描述符就绪，这就表明这些就绪描述符对应的套接字上，有读事件发生，此时，我们就在该套接字上读取数据\n\n而因为 select 函数一次可以监听 1024 个文件描述符的状态，所以 select 函数在返回时，也可能会一次返回多个就绪的文件描述符。这样一来，我们就可以使用一个循环流程，依次对就绪描述符对应的套接字进行读写或异常处理操作。\n\n\n\n下面的代码展示的是使用 select 函数，进行并发客户端处理的关键步骤和主要函数调用：\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\nfd_set rset;  //被监听的描述符集合，关注描述符上的读事件\n\nint max_fd = sock_fd\n\n//初始化rset数组，使用FD_ZERO宏设置每个元素为0\nFD_ZERO(&rset);\n//使用FD_SET宏设置rset数组中位置为sock_fd的文件描述符为1，表示需要监听该文件描述符\nFD_SET(sock_fd,&rset);\n\n//设置超时时间\nstruct timeval timeout;\ntimeout.tv_sec = 3;\ntimeout.tv_usec = 0;\n\nwhile(1) {\n   //调用select函数，检测rset数组保存的文件描述符是否已有读事件就绪，返回就绪的文件描述符个数\n   n = select(max_fd+1, &rset, NULL, NULL, &timeout);\n\n   //调用FD_ISSET宏，在rset数组中检测sock_fd对应的文件描述符是否就绪\n   if (FD_ISSET(sock_fd, &rset)) {\n       //如果sock_fd已经就绪，表明已有客户端连接；调用accept函数建立连接\n       conn_fd = accept();\n       //设置rset数组中位置为conn_fd的文件描述符为1，表示需要监听该文件描述符\n       FD_SET(conn_fd, &rset);\n   }\n\n   //依次检查已连接套接字的文件描述符\n   for (i = 0; i < maxfd; i++) {\n        //调用FD_ISSET宏，在rset数组中检测文件描述符是否就绪\n       if (FD_ISSET(i, &rset)) {\n         //有数据可读，进行读数据处理\n       }\n   }\n}\n\n\n不过，你或许会发现 select 函数存在 两个设计上的不足：\n\n * select 函数对单个进程能监听的文件描述符数量是有限制的，它能监听的文件描述符个数由__FD_SETSIZE 决定，默认值是 1024。\n * 当 select 函数返回后，我们需要遍历描述符集合，才能找到具体是哪些描述符就绪了。这个遍历过程会产生一定开销，从而降低程序的性能。\n\n所以，为了解决 select 函数受限于 1024 个文件描述符的不足，poll 函数对此做了改进\n\n\n# poll 机制：不受限的文件描述符数量\n\npoll 机制的主要函数是 poll 函数，我们先来看下它的原型定义，如下所示：\n\nint poll(struct pollfd *__fds, nfds_t __nfds, int __timeout);\n\n\n其中，参数 fds 是 pollfd 结构体数组，参数 nfds 表示的是 fds 数组的元素个数，而timeout 表示 poll 函数阻塞的超时时间。\n\npollfd 结构体里包含了要监听的描述符，以及该描述符上要监听的事件类型。这个我们可以从 pollfd 结构体的定义中看出来，如下所示。pollfd 结构体中包含了三个成员变量 fd、events 和 revents，分别表示要监听的文件描述符、要监听的事件类型和实际发生的事件类型。\n\nstruct pollfd {\n    int fd;         //进行监听的文件描述符\n    short int events;       //要监听的事件类型\n    short int revents;      //实际发生的事件类型\n};\n\n\npollfd 结构体中要监听和实际发生的事件类型，是通过以下三个宏定义来表示的，分别是 POLLRDNORM、POLLWRNORM 和 POLLERR，它们分别表示可读、可写和错误事件。\n\n#define POLLRDNORM  0x040       //可读事件\n#define POLLWRNORM  0x100       //可写事件\n#define POLLERR     0x008       //错误事件\n\n\n好了，了解了 poll 函数的参数后，我们来看下如何使用 poll 函数完成网络通信。这个流程主要可以分成三步：\n\n 1. 创建 pollfd 数组和监听套接字，并进行绑定；\n 2. 将监听套接字加入 pollfd 数组，并设置其监听读事件，也就是客户端的连接请求；\n 3. 循环调用 poll 函数，检测 pollfd 数组中是否有就绪的文件描述符。\n\n而在第三步的循环过程中，其处理逻辑又分成了两种情况：\n\n * 如果是连接套接字就绪，这表明是有客户端连接，我们可以调用 accept 接受连接，并创建已连接套接字，并将其加入 pollfd 数组，并监听读事件；\n * 如果是已连接套接字就绪，这表明客户端有读写请求，我们可以调用 recv/send 函数处理读写请求。\n\n我画了下面这张图，展示了使用 poll 函数的流程，你可以学习掌握下。\n\n\n\n另外，为了便于你掌握在代码中使用 poll 函数，我也写了一份示例代码，如下所示：\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\n//poll函数可以监听的文件描述符数量，可以大于1024\n#define MAX_OPEN = 2048\n\n//pollfd结构体数组，对应文件描述符\nstruct pollfd client[MAX_OPEN];\n\n//将创建的监听套接字加入pollfd数组，并监听其可读事件\nclient[0].fd = sock_fd;\nclient[0].events = POLLRDNORM;\nmaxfd = 0;\n\n//初始化client数组其他元素为-1\nfor (i = 1; i < MAX_OPEN; i++)\n    client[i].fd = -1;\n\nwhile(1) {\n   //调用poll函数，检测client数组里的文件描述符是否有就绪的，返回就绪的文件描述符个数\n   n = poll(client, maxfd+1, &timeout);\n   //如果监听套件字的文件描述符有可读事件，则进行处理\n   if (client[0].revents & POLLRDNORM) {\n       //有客户端连接；调用accept函数建立连接\n       conn_fd = accept();\n\n       //保存已建立连接套接字\n       for (i = 1; i < MAX_OPEN; i++){\n         if (client[i].fd < 0) {\n           client[i].fd = conn_fd; //将已建立连接的文件描述符保存到client数组\n           client[i].events = POLLRDNORM; //设置该文件描述符监听可读事件\n           break;\n          }\n       }\n       maxfd = i;\n   }\n\n   //依次检查已连接套接字的文件描述符\n   for (i = 1; i < MAX_OPEN; i++) {\n       if (client[i].revents & (POLLRDNORM | POLLERR)) {\n         //有数据可读或发生错误，进行读数据处理或错误处理\n       }\n   }\n}\n\n\n其实，和 select 函数相比，poll 函数的改进之处主要就在于，它允许一次监听超过 1024 个文件描述符。但是当调用了 poll 函数后，我们仍然需要遍历每个文件描述符，检测该描述符是否就绪，然后再进行处理。\n\n那么，有没有办法可以避免遍历每个描述符呢？ 这就是我接下来向你介绍的 epoll 机制\n\n\n# epoll 机制：避免遍历每个描述符\n\n首先，epoll 机制是使用 epoll_event 结构体，来记录待监听的文件描述符及其监听的事件类型的，这和 poll 机制中使用 pollfd 结构体比较类似。\n\n那么，对于 epoll_event 结构体来说，其中包含了 epoll_data_t 联合体变量，以及整数类型的 events 变量。epoll_data_t 联合体中有记录文件描述符的成员变量 fd，而 events 变量会取值使用不同的宏定义值，来表示 epoll_data_t 变量中的文件描述符所关注的事件类型，比如一些常见的事件类型包括以下这几种。\n\n * EPOLLIN：读事件，表示文件描述符对应套接字有数据可读。\n * EPOLLOUT：写事件，表示文件描述符对应套接字有数据要写。\n * EPOLLERR：错误事件，表示文件描述符对于套接字出错。\n\n下面的代码展示了 epoll_event 结构体以及 epoll_data 联合体的定义，你可以看下。\n\ntypedef union epoll_data\n{\n  ...\n  int fd;  //记录文件描述符\n  ...\n} epoll_data_t;\n\n\nstruct epoll_event\n{\n  uint32_t events;  //epoll监听的事件类型\n  epoll_data_t data; //应用程序数据\n};\n\n\n好了，现在我们知道，在使用 select 或 poll 函数的时候，创建好文件描述符集合或 pollfd 数组后，就可以往数组中添加我们需要监听的文件描述符。\n\n但是对于 epoll 机制来说，我们则需要先调用 epoll_create 函数，创建一个 epoll 实例。这个 epoll 实例内部维护了两个结构，分别是记录要监听的文件描述符和已经就绪的文件描述符，而对于已经就绪的文件描述符来说，它们会被返回给用户程序进行处理。\n\n所以，我们在使用 epoll 机制时，就不用像使用 select 和 poll 一样，遍历查询哪些文件描述符已经就绪了。这样一来， epoll 的效率就比 select 和 poll 有了更高的提升。\n\n在创建了 epoll 实例后，我们需要再使用 epoll_ctl 函数，给被监听的文件描述符添加监听事件类型，以及使用 epoll_wait 函数获取就绪的文件描述符。\n\n我画了一张图，展示了使用 epoll 进行网络通信的流程，你可以看下。\n\n\n\n下面的代码展示了使用 epoll 函数的流程，你也可以看下。\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\nepfd = epoll_create(EPOLL_SIZE); //创建epoll实例，\n//创建epoll_event结构体数组，保存套接字对应文件描述符和监听事件类型\nep_events = (epoll_event*)malloc(sizeof(epoll_event) * EPOLL_SIZE);\n\n//创建epoll_event变量\nstruct epoll_event ee\n//监听读事件\nee.events = EPOLLIN;\n//监听的文件描述符是刚创建的监听套接字\nee.data.fd = sock_fd;\n\n//将监听套接字加入到监听列表中\nepoll_ctl(epfd, EPOLL_CTL_ADD, sock_fd, &ee);\n\nwhile (1) {\n   //等待返回已经就绪的描述符\n   n = epoll_wait(epfd, ep_events, EPOLL_SIZE, -1);\n   //遍历所有就绪的描述符\n   for (int i = 0; i < n; i++) {\n       //如果是监听套接字描述符就绪，表明有一个新客户端连接到来\n       if (ep_events[i].data.fd == sock_fd) {\n          conn_fd = accept(sock_fd); //调用accept()建立连接\n          ee.events = EPOLLIN;\n          ee.data.fd = conn_fd;\n          //添加对新创建的已连接套接字描述符的监听，监听后续在已连接套接字上的读事件\n          epoll_ctl(epfd, EPOLL_CTL_ADD, conn_fd, &ee);\n\n       } else { //如果是已连接套接字描述符就绪，则可以读数据\n           ...//读取数据并处理\n       }\n   }\n}\n\n\n好了，到这里，你就了解了 epoll 函数的使用方法了。实际上，也正是因为 epoll 能自定义监听的描述符数量，以及可以直接返回就绪的描述符，Redis 在设计和实现网络通信框架时，就基于 epoll 机制中的 epoll_create、epoll_ctl 和 epoll_wait 等函数和读写事件，进行了封装开发，实现了用于网络通信的事件驱动框架，从而使得 Redis 虽然是单线程运行，但是仍然能高效应对高并发的客户端访问。\n\n\n# 总结\n\necho 给你介绍了 Redis 网络通信依赖的操作系统底层机制，也就是 IO 多路复用机制\n\n由于 Redis 是单线程程序，如果使用基本的 Socket 编程模型的话，只能对一个监听套接字或一个已连接套接字进行监听。而当 Redis 实例面临很多并发的客户端时，这种处理方式的效率就会很低。\n\n所以，和基本的 Socket 通信相比，使用 IO 多路复用机制，就可以一次性获得就绪的多个套接字，从而避免了逐个检测套接字的开销。\n\n我是以最常用的 Linux 操作系统为例，给你具体介绍了 Linux 系统提供的三种 IO 多路复用机制，分别是 select、poll 和 epoll。这三种机制在能监听的描述符数量和查找就绪描述符的方法上是不一样的\n\n多路复用机制   监听的文件描述符数量   查找就绪的文件描述符\nselect   最多 1024      遍历所有描述符\npoll     不受限          遍历所有描述符\nepoll    与使用          epoll_wait 自动返回就绪的描述符，未就绪的描述符不返回\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n * 为什么 redis 这样高并发的应用选择了 epoll，而不是 select 或 poll？\n\n * 在设计一个高并发服务器时，如何选择最适合的 io 模型？\n\n * 当服务器面对成千上万的连接时，什么样的机制可以让你高效管理这些连接？\n\n * 高并发场景中，epoll 是如何通过事件通知机制避免性能瓶颈的？\n\n * 多路复用机制会监听套接字上的哪些事件？\n\n * 多路复用机制可以监听多少个套接字？\n\n * 当有套接字就绪时，多路复用机制要如何找到就绪的套接字？\n\n\n# 前言\n\nredis 作为一个 client-server 架构的数据库，其源码中少不了用来实现网络通信的部分。通常系统实现网络通信的基本方法是 使用 socket 编程模型，但是，由于基本的 socket 编程模型是单线程阻塞的 ....\n\n所以当要处理高并发请求时，有两种方案\n\n * 多线程，让每个线程负责处理一个客户端的请求。而 redis 负责客户端请求解析和处理的线程只有一个，那么如果直接采用基本 socket 模型，就会影响 redis 支持高并发的客户端访问。\n * io 多路复用，为了实现高并发的网络通信，我们常用的 linux 操作系统，就提供了 select、poll 和 epoll 三种编程模型，而在 linux 上运行的 redis，通常就会采用其中的 epoll 模型来进行网络通信。\n\n\n# 简述传统 socket 模型\n\n我们看下使用 socket 模型实现网络通信时的关键操作，以此帮助我们分析 socket 模型中的不足\n\n首先，当我们需要让服务器端和客户端进行通信时，可以在服务器端通过以下三步，来创建监听客户端连接的监听套接字（listening socket）：\n\n 1. 调用 socket 函数，创建一个套接字。我们通常把这个套接字称为主动套接字（active socket）\n 2. 调用 bind 函数，将主动套接字和当前服务器的 ip 和监听端口进行绑定\n 3. 调用 listen 函数，将主动套接字转换为监听套接字，开始监听客户端的连接\n 4. 调用 accept函数，在完成上述三步之后，服务器端就可以接收客户端的连接请求了。为了能及时地收到客户端的连接请求，我们可以运行一个循环流程，在该流程中调用 accept 函数，用于接收客户端连接请求\n 5. 最后，服务器端可以通过调用 recv 或 send 函数，在刚才返回的已连接套接字上，接收并处理读写请求，或是将数据发送给客户端\n\n注意\n\naccept 函数是阻塞函数，也就是说，如果此时一直没有客户端连接请求，那么，服务器端的执行流程会一直阻塞在 accept 函数。一旦有客户端连接请求到达，accept 将不再阻塞，而是处理连接请求，和客户端建立连接，并返回已连接套接字（connected socket）\n\nlistensocket = socket(); //调用socket系统调用创建一个主动套接字\nbind(listensocket);  //绑定地址和端口\nlisten(listensocket); //将默认的主动套接字转换为服务器使用的被动套接字，也就是监听套接字\nwhile (1) { //循环监听是否有客户端连接请求到来\n   connsocket = accept(listensocket); //接受客户端连接\n   recv(connsocket); //从客户端读取数据，只能同时处理一个客户端\n   send(connsocket); //给客户端返回数据，只能同时处理一个客户端\n}\n\n\n你会发现，虽然上述代码能够实现服务器端和客户端之间的通信，但是程序每调用一次 accept 函数，只能处理一个客户端连接\n\n因此，如果想要处理多个并发客户端的请求，我们就需要使用多线程的方法，来处理通过 accept 函数建立的多个客户端连接上的请求\n\n使用这种方法后，我们需要在 accept 函数返回已连接套接字后，创建一个线程，并将已连接套接字传递给创建的线程，由该线程负责这个连接套接字上后续的数据读写。同时，服务器端的执行流程会再次调用 accept 函数，等待下一个客户端连接\n\n以下给出的示例代码，就展示了使用多线程来提升服务器端的并发客户端处理能力：\n\nlistensocket = socket(); //调用socket系统调用创建一个主动套接字\nbind(listensocket);  //绑定地址和端口\nlisten(listensocket); //将默认的主动套接字转换为服务器使用的被动套接字，即监听套接字\nwhile (1) { //循环监听是否有客户端连接到来\n   connsocket = accept(listensocket); //接受客户端连接，返回已连接套接字\n   pthread_create(processdata, connsocket); //创建新线程对已连接套接字进行处理\n\n}\n\n//处理已连接套接字上的读写请求\nprocessdata(connsocket){\n   recv(connsocket); //从客户端读取数据，只能同时处理一个客户端\n   send(connsocket); //给客户端返回数据，只能同时处理一个客户端\n}\n\n\n\n# 展望 io 多路复用\n\n虽然上述方法能提升服务器端的并发处理能力，遗憾的是\n\nredis 的主执行流程是由一个线程在执行，无法使用多线程的方式来提升并发处理能力。\n\n所以，该方法对 redis 并不起作用。那么，还有没有什么其他方法，能帮助 redis 提升并发客户端的处理能力呢？\n\n这就要用到操作系统提供的 io 多路复用功能了\n\n在基本的 socket 编程模型中，accept 函数只能在一个监听套接字上监听客户端的连接，recv 函数也只能在一个已连接套接字上，等待客户端发送的请求。\n\n而 io 多路复用机制，可以让程序通过调用多路复用函数，同时监听多个套接字上的请求。这里既可以包括监听套接字上的连接请求，也可以包括已连接套接字上的读写请求。这样当有一个或多个套接字上有请求时，多路复用函数就会返回。此时，程序就可以处理这些就绪套接字上的请求，比如读取就绪的已连接套接字上的请求内容。\n\n因为 linux 操作系统在实际应用中比较广泛，所以讲解 linux 上的 io 多路复用机制。\n\nlinux 提供的 io 多路复用机制主要有三种，分别是\n\n * select\n * poll\n * epoll\n\n下面，我们就分别来学习下这三种机制的实现思路和使用方法。然后，我们再来看看，为什么 redis 通常是选择使用 epoll 这种机制来实现网络通信\n\n\n# linux 如何实现 io 多路复用\n\n\n# select 机制：多路复用的基本实现\n\nselect 机制中的一个重要函数就是 select 函数\n\n对于 select 函数来说，它的参数包括\n\n * 监听的文件描述符数量__nfds\n * 被监听描述符的三个集合*__readfds、*__writefds和*__exceptfds\n * 监听时阻塞等待的超时时长*__timeout\n\nint select (int __nfds, fd_set *__readfds, fd_set *__writefds, fd_set *__exceptfds, struct timeval *__timeout)\n\n\n注意\n\nlinux 针对每一个套接字都会有一个文件描述符，也就是一个非负整数，用来唯一标识该套接字，在多路复用机制的函数中，linux 通常会用文件描述符作为参数。有了文件描述符，函数也就能找到对应的套接字，进而进行监听、读写等操作\n\nselect 函数的参数__readfds、__writefds和__exceptfds表示的是，被监听描述符的集合，其实就是被监听套接字的集合。那么，为什么会有三个集合呢？\n\n这就和我刚才提出的第一个问题相关，也就是多路复用机制会监听哪些事件。select 函数使用三个集合，表示监听的三类事件，分别是\n\n * 读数据事件（对应__readfds集合）\n * 写数据事件（对应__writefds集合）\n * 异常事件（对应__exceptfds集合）\n\n我们进一步可以看到，参数 readfds、writefds 和 exceptfds 的类型是 fd_set 结构体，它主要定义部分如下所示。其中，__fd_mask类型是 long int 类型的别名，fd_setsize 和nfdbits 这两个宏定义的大小默认为 1024 和 32\n\ntypedef struct {\n   …\n   __fd_mask  __fds_bits[__fd_setsize / __nfdbits];\n   …\n} fd_set\n\n\n所以，fd_set 结构体的定义，其实就是一个 long int 类型的数组，该数组中一共有 32 个元素（1024/32=32），每个元素是 32 位（long int 类型的大小），而每一位可以用来表示一个文件描述符的状态\n\n好了，了解了 fd_set 结构体的定义，我们就可以回答刚才提出的第二个问题了。select 函数对每一个描述符集合，都可以监听 1024 个描述符。\n\n接下来，我们再来了解下 如何使用 select 机制来实现网络通信。\n\n 1. 首先，我们在调用 select 函数前，可以先创建好传递给 select 函数的描述符集合，然后再创建监听套接字。而为了让创建的监听套接字能被 select 函数监控，我们需要把这个套接字的描述符加入到创建好的描述符集合中。\n 2. 然后，我们就可以调用 select 函数，并把创建好的描述符集合作为参数传递给 select 函数。程序在调用 select 函数后，会发生阻塞。而当 select 函数检测到有描述符就绪后，就会结束阻塞，并返回就绪的文件描述符个数。\n 3. 此时，我们就可以在描述符集合中查找哪些描述符就绪了。然后，我们对已就绪描述符对应的套接字进行处理。比如，如果是__readfds 集合中有描述符就绪，这就表明这些就绪描述符对应的套接字上，有读事件发生，此时，我们就在该套接字上读取数据\n\n而因为 select 函数一次可以监听 1024 个文件描述符的状态，所以 select 函数在返回时，也可能会一次返回多个就绪的文件描述符。这样一来，我们就可以使用一个循环流程，依次对就绪描述符对应的套接字进行读写或异常处理操作。\n\n\n\n下面的代码展示的是使用 select 函数，进行并发客户端处理的关键步骤和主要函数调用：\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\nfd_set rset;  //被监听的描述符集合，关注描述符上的读事件\n\nint max_fd = sock_fd\n\n//初始化rset数组，使用fd_zero宏设置每个元素为0\nfd_zero(&rset);\n//使用fd_set宏设置rset数组中位置为sock_fd的文件描述符为1，表示需要监听该文件描述符\nfd_set(sock_fd,&rset);\n\n//设置超时时间\nstruct timeval timeout;\ntimeout.tv_sec = 3;\ntimeout.tv_usec = 0;\n\nwhile(1) {\n   //调用select函数，检测rset数组保存的文件描述符是否已有读事件就绪，返回就绪的文件描述符个数\n   n = select(max_fd+1, &rset, null, null, &timeout);\n\n   //调用fd_isset宏，在rset数组中检测sock_fd对应的文件描述符是否就绪\n   if (fd_isset(sock_fd, &rset)) {\n       //如果sock_fd已经就绪，表明已有客户端连接；调用accept函数建立连接\n       conn_fd = accept();\n       //设置rset数组中位置为conn_fd的文件描述符为1，表示需要监听该文件描述符\n       fd_set(conn_fd, &rset);\n   }\n\n   //依次检查已连接套接字的文件描述符\n   for (i = 0; i < maxfd; i++) {\n        //调用fd_isset宏，在rset数组中检测文件描述符是否就绪\n       if (fd_isset(i, &rset)) {\n         //有数据可读，进行读数据处理\n       }\n   }\n}\n\n\n不过，你或许会发现 select 函数存在 两个设计上的不足：\n\n * select 函数对单个进程能监听的文件描述符数量是有限制的，它能监听的文件描述符个数由__fd_setsize 决定，默认值是 1024。\n * 当 select 函数返回后，我们需要遍历描述符集合，才能找到具体是哪些描述符就绪了。这个遍历过程会产生一定开销，从而降低程序的性能。\n\n所以，为了解决 select 函数受限于 1024 个文件描述符的不足，poll 函数对此做了改进\n\n\n# poll 机制：不受限的文件描述符数量\n\npoll 机制的主要函数是 poll 函数，我们先来看下它的原型定义，如下所示：\n\nint poll(struct pollfd *__fds, nfds_t __nfds, int __timeout);\n\n\n其中，参数 fds 是 pollfd 结构体数组，参数 nfds 表示的是 fds 数组的元素个数，而timeout 表示 poll 函数阻塞的超时时间。\n\npollfd 结构体里包含了要监听的描述符，以及该描述符上要监听的事件类型。这个我们可以从 pollfd 结构体的定义中看出来，如下所示。pollfd 结构体中包含了三个成员变量 fd、events 和 revents，分别表示要监听的文件描述符、要监听的事件类型和实际发生的事件类型。\n\nstruct pollfd {\n    int fd;         //进行监听的文件描述符\n    short int events;       //要监听的事件类型\n    short int revents;      //实际发生的事件类型\n};\n\n\npollfd 结构体中要监听和实际发生的事件类型，是通过以下三个宏定义来表示的，分别是 pollrdnorm、pollwrnorm 和 pollerr，它们分别表示可读、可写和错误事件。\n\n#define pollrdnorm  0x040       //可读事件\n#define pollwrnorm  0x100       //可写事件\n#define pollerr     0x008       //错误事件\n\n\n好了，了解了 poll 函数的参数后，我们来看下如何使用 poll 函数完成网络通信。这个流程主要可以分成三步：\n\n 1. 创建 pollfd 数组和监听套接字，并进行绑定；\n 2. 将监听套接字加入 pollfd 数组，并设置其监听读事件，也就是客户端的连接请求；\n 3. 循环调用 poll 函数，检测 pollfd 数组中是否有就绪的文件描述符。\n\n而在第三步的循环过程中，其处理逻辑又分成了两种情况：\n\n * 如果是连接套接字就绪，这表明是有客户端连接，我们可以调用 accept 接受连接，并创建已连接套接字，并将其加入 pollfd 数组，并监听读事件；\n * 如果是已连接套接字就绪，这表明客户端有读写请求，我们可以调用 recv/send 函数处理读写请求。\n\n我画了下面这张图，展示了使用 poll 函数的流程，你可以学习掌握下。\n\n\n\n另外，为了便于你掌握在代码中使用 poll 函数，我也写了一份示例代码，如下所示：\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\n//poll函数可以监听的文件描述符数量，可以大于1024\n#define max_open = 2048\n\n//pollfd结构体数组，对应文件描述符\nstruct pollfd client[max_open];\n\n//将创建的监听套接字加入pollfd数组，并监听其可读事件\nclient[0].fd = sock_fd;\nclient[0].events = pollrdnorm;\nmaxfd = 0;\n\n//初始化client数组其他元素为-1\nfor (i = 1; i < max_open; i++)\n    client[i].fd = -1;\n\nwhile(1) {\n   //调用poll函数，检测client数组里的文件描述符是否有就绪的，返回就绪的文件描述符个数\n   n = poll(client, maxfd+1, &timeout);\n   //如果监听套件字的文件描述符有可读事件，则进行处理\n   if (client[0].revents & pollrdnorm) {\n       //有客户端连接；调用accept函数建立连接\n       conn_fd = accept();\n\n       //保存已建立连接套接字\n       for (i = 1; i < max_open; i++){\n         if (client[i].fd < 0) {\n           client[i].fd = conn_fd; //将已建立连接的文件描述符保存到client数组\n           client[i].events = pollrdnorm; //设置该文件描述符监听可读事件\n           break;\n          }\n       }\n       maxfd = i;\n   }\n\n   //依次检查已连接套接字的文件描述符\n   for (i = 1; i < max_open; i++) {\n       if (client[i].revents & (pollrdnorm | pollerr)) {\n         //有数据可读或发生错误，进行读数据处理或错误处理\n       }\n   }\n}\n\n\n其实，和 select 函数相比，poll 函数的改进之处主要就在于，它允许一次监听超过 1024 个文件描述符。但是当调用了 poll 函数后，我们仍然需要遍历每个文件描述符，检测该描述符是否就绪，然后再进行处理。\n\n那么，有没有办法可以避免遍历每个描述符呢？ 这就是我接下来向你介绍的 epoll 机制\n\n\n# epoll 机制：避免遍历每个描述符\n\n首先，epoll 机制是使用 epoll_event 结构体，来记录待监听的文件描述符及其监听的事件类型的，这和 poll 机制中使用 pollfd 结构体比较类似。\n\n那么，对于 epoll_event 结构体来说，其中包含了 epoll_data_t 联合体变量，以及整数类型的 events 变量。epoll_data_t 联合体中有记录文件描述符的成员变量 fd，而 events 变量会取值使用不同的宏定义值，来表示 epoll_data_t 变量中的文件描述符所关注的事件类型，比如一些常见的事件类型包括以下这几种。\n\n * epollin：读事件，表示文件描述符对应套接字有数据可读。\n * epollout：写事件，表示文件描述符对应套接字有数据要写。\n * epollerr：错误事件，表示文件描述符对于套接字出错。\n\n下面的代码展示了 epoll_event 结构体以及 epoll_data 联合体的定义，你可以看下。\n\ntypedef union epoll_data\n{\n  ...\n  int fd;  //记录文件描述符\n  ...\n} epoll_data_t;\n\n\nstruct epoll_event\n{\n  uint32_t events;  //epoll监听的事件类型\n  epoll_data_t data; //应用程序数据\n};\n\n\n好了，现在我们知道，在使用 select 或 poll 函数的时候，创建好文件描述符集合或 pollfd 数组后，就可以往数组中添加我们需要监听的文件描述符。\n\n但是对于 epoll 机制来说，我们则需要先调用 epoll_create 函数，创建一个 epoll 实例。这个 epoll 实例内部维护了两个结构，分别是记录要监听的文件描述符和已经就绪的文件描述符，而对于已经就绪的文件描述符来说，它们会被返回给用户程序进行处理。\n\n所以，我们在使用 epoll 机制时，就不用像使用 select 和 poll 一样，遍历查询哪些文件描述符已经就绪了。这样一来， epoll 的效率就比 select 和 poll 有了更高的提升。\n\n在创建了 epoll 实例后，我们需要再使用 epoll_ctl 函数，给被监听的文件描述符添加监听事件类型，以及使用 epoll_wait 函数获取就绪的文件描述符。\n\n我画了一张图，展示了使用 epoll 进行网络通信的流程，你可以看下。\n\n\n\n下面的代码展示了使用 epoll 函数的流程，你也可以看下。\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\nepfd = epoll_create(epoll_size); //创建epoll实例，\n//创建epoll_event结构体数组，保存套接字对应文件描述符和监听事件类型\nep_events = (epoll_event*)malloc(sizeof(epoll_event) * epoll_size);\n\n//创建epoll_event变量\nstruct epoll_event ee\n//监听读事件\nee.events = epollin;\n//监听的文件描述符是刚创建的监听套接字\nee.data.fd = sock_fd;\n\n//将监听套接字加入到监听列表中\nepoll_ctl(epfd, epoll_ctl_add, sock_fd, &ee);\n\nwhile (1) {\n   //等待返回已经就绪的描述符\n   n = epoll_wait(epfd, ep_events, epoll_size, -1);\n   //遍历所有就绪的描述符\n   for (int i = 0; i < n; i++) {\n       //如果是监听套接字描述符就绪，表明有一个新客户端连接到来\n       if (ep_events[i].data.fd == sock_fd) {\n          conn_fd = accept(sock_fd); //调用accept()建立连接\n          ee.events = epollin;\n          ee.data.fd = conn_fd;\n          //添加对新创建的已连接套接字描述符的监听，监听后续在已连接套接字上的读事件\n          epoll_ctl(epfd, epoll_ctl_add, conn_fd, &ee);\n\n       } else { //如果是已连接套接字描述符就绪，则可以读数据\n           ...//读取数据并处理\n       }\n   }\n}\n\n\n好了，到这里，你就了解了 epoll 函数的使用方法了。实际上，也正是因为 epoll 能自定义监听的描述符数量，以及可以直接返回就绪的描述符，redis 在设计和实现网络通信框架时，就基于 epoll 机制中的 epoll_create、epoll_ctl 和 epoll_wait 等函数和读写事件，进行了封装开发，实现了用于网络通信的事件驱动框架，从而使得 redis 虽然是单线程运行，但是仍然能高效应对高并发的客户端访问。\n\n\n# 总结\n\necho 给你介绍了 redis 网络通信依赖的操作系统底层机制，也就是 io 多路复用机制\n\n由于 redis 是单线程程序，如果使用基本的 socket 编程模型的话，只能对一个监听套接字或一个已连接套接字进行监听。而当 redis 实例面临很多并发的客户端时，这种处理方式的效率就会很低。\n\n所以，和基本的 socket 通信相比，使用 io 多路复用机制，就可以一次性获得就绪的多个套接字，从而避免了逐个检测套接字的开销。\n\n我是以最常用的 linux 操作系统为例，给你具体介绍了 linux 系统提供的三种 io 多路复用机制，分别是 select、poll 和 epoll。这三种机制在能监听的描述符数量和查找就绪描述符的方法上是不一样的\n\n多路复用机制   监听的文件描述符数量   查找就绪的文件描述符\nselect   最多 1024      遍历所有描述符\npoll     不受限          遍历所有描述符\nepoll    与使用          epoll_wait 自动返回就绪的描述符，未就绪的描述符不返回\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Redis Server 初始化",frontmatter:{title:"Redis Server 初始化",date:"2024-09-16T03:04:10.000Z",permalink:"/pages/d4ecb9/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/03.Redis%20Server%20%E5%88%9D%E5%A7%8B%E5%8C%96.html",relativePath:"Redis 系统设计/03.三、主线任务/03.Redis Server 初始化.md",key:"v-57506a62",path:"/pages/d4ecb9/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:289},{level:2,title:"main 函数：Redis server 的入口",slug:"main-函数-redis-server-的入口",normalizedTitle:"main 函数：redis server 的入口",charIndex:713},{level:3,title:"阶段一：基本初始化",slug:"阶段一-基本初始化",normalizedTitle:"阶段一：基本初始化",charIndex:860},{level:3,title:"阶段二：检查哨兵模式，并检查是否要执行 RDB 检测或 AOF 检测",slug:"阶段二-检查哨兵模式-并检查是否要执行-rdb-检测或-aof-检测",normalizedTitle:"阶段二：检查哨兵模式，并检查是否要执行 rdb 检测或 aof 检测",charIndex:1515},{level:3,title:"阶段三：运行参数解析",slug:"阶段三-运行参数解析",normalizedTitle:"阶段三：运行参数解析",charIndex:2543},{level:3,title:"阶段四：初始化 server",slug:"阶段四-初始化-server",normalizedTitle:"阶段四：初始化 server",charIndex:2680},{level:3,title:"阶段五：执行事件驱动框架",slug:"阶段五-执行事件驱动框架",normalizedTitle:"阶段五：执行事件驱动框架",charIndex:2991},{level:2,title:"Redis 运行参数解析与设置",slug:"redis-运行参数解析与设置",normalizedTitle:"redis 运行参数解析与设置",charIndex:3237},{level:3,title:"Redis 的主要参数类型",slug:"redis-的主要参数类型",normalizedTitle:"redis 的主要参数类型",charIndex:3609},{level:3,title:"Redis 参数的设置方法",slug:"redis-参数的设置方法",normalizedTitle:"redis 参数的设置方法",charIndex:4111},{level:2,title:"initServer：初始化 Redis server",slug:"initserver-初始化-redis-server",normalizedTitle:"initserver：初始化 redis server",charIndex:6839},{level:2,title:"执行事件驱动框架",slug:"执行事件驱动框架",normalizedTitle:"执行事件驱动框架",charIndex:2995},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:9549},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10555}],headersStr:"前言 main 函数：Redis server 的入口 阶段一：基本初始化 阶段二：检查哨兵模式，并检查是否要执行 RDB 检测或 AOF 检测 阶段三：运行参数解析 阶段四：初始化 server 阶段五：执行事件驱动框架 Redis 运行参数解析与设置 Redis 的主要参数类型 Redis 参数的设置方法 initServer：初始化 Redis server 执行事件驱动框架 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. 如何高效执行网络服务器启动的每一步？Redis 的 main 函数能带来什么启示？\n 2. Redis Server 启动后如何确保关键参数正确加载？有哪些常见陷阱？\n 3. Redis 如何通过事件驱动模型处理高并发请求？从 main 函数能学到什么？\n 4. 为什么初始化和加载配置参数对 Redis 启动至关重要？它如何影响客户端请求处理？\n 5. 如何通过 initServer 函数理解并调优 Redis 配置参数以提升性能？\n 6. Redis 启动时如何决定加载 AOF 还是 RDB？这种设计对数据恢复的影响是什么？\n\n\n# 前言\n\n我们知道，main 函数是 Redis 整个运行程序的入口。同时，由于 Redis 是典型的 Client-Server 架构，一旦 Redis 实例开始运行，Redis server 也就会启动，所以 main 函数其实也会负责 Redis server 的启动运行。\n\n> main 函数是在 server.c 中\n\n你平常在设计或实现一个网络服务器程序时，可能会遇到一个问题，那就是\n\n服务器启动时，应该做哪些操作、有没有一个典型的参考实现\n\n所以我就从 main 函数开始，给你介绍下 Redis server 是如何在 main 函数中启动并完成初始化的。通过这节课内容的学习，你可以掌握 Redis 针对以下三个问题的实现思路：\n\n 1. Redis server 启动后具体会做哪些初始化操作？\n 2. Redis server 初始化时有哪些关键配置项？\n 3. Redis server 如何开始处理客户端请求？\n\n\n# main 函数：Redis server 的入口\n\n一般来说，一个使用 C 开发的系统软件启动运行的代码逻辑，都是实现在了 main 函数当中，所以 echo 找到了 main 函数，看看它的执行过程\n\n那么，对于 Redis 的 main 函数来说，我把它执行的工作分成了五个阶段。\n\n\n# 阶段一：基本初始化\n\n在这个阶段，main 函数主要是完成一些基本的初始化工作，包括设置 server 运行的时区、设置哈希函数的随机种子等。这部分工作的主要调用函数如下所示：\n\n//设置时区\nsetlocale(LC_COLLATE,"");\ntzset();\n...\n//设置随机种子\nchar hashseed[16];\ngetRandomHexChars(hashseed,sizeof(hashseed));\ndictSetHashFunctionSeed((uint8_t*)hashseed);\n\n\n这里，你需要注意的是，在 main 函数的开始部分，有一段宏定义覆盖的代码。这部分代码的作用是，如果定义了 REDIS_TEST 宏定义，并且 Redis server 启动时的参数符合测试参数，那么 main 函数就会执行相应的测试程序。\n\n这段宏定义的代码如以下所示，其中的示例代码就是调用 ziplist 的测试函数 ziplistTest：\n\n#ifdef REDIS_TEST\n//如果启动参数有test和ziplist，那么就调用ziplistTest函数进行ziplist的测试\nif (argc == 3 && !strcasecmp(argv[1], "test")) {\n  if (!strcasecmp(argv[2], "ziplist")) {\n     return ziplistTest(argc, argv);\n  }\n  ...\n}\n#endif\n\n\n\n# 阶段二：检查哨兵模式，并检查是否要执行 RDB 检测或 AOF 检测\n\nRedis server 启动后，可能是以哨兵模式运行的，而哨兵模式运行的 server 在参数初始化、参数设置，以及 server 启动过程中要执行的操作等方面，与普通模式 server 有所差别。所以，main 函数在执行过程中需要根据 Redis 配置的参数，检查是否设置了哨兵模式。\n\n如果有设置哨兵模式的话，main 函数会调用 initSentinelConfig 函数，对哨兵模式的参数进行初始化设置，以及调用 initSentinel 函数，初始化设置哨兵模式运行的 server。有关哨兵模式运行的 Redis server 相关机制，我会在第 21 讲中给你详细介绍。\n\n下面的代码展示了 main 函数中对哨兵模式的检查，以及对哨兵模式的初始化，你可以看下：\n\n...\n//判断server是否设置为哨兵模式\nif (server.sentinel_mode) {\n    initSentinelConfig();  //初始化哨兵的配置\n    initSentinel();   //初始化哨兵模式\n}\n...\n\n\n除了检查哨兵模式以外，main 函数还会检查是否要执行 RDB 检测或 AOF 检查，这对应了实际运行的程序是 redis-check-rdb 或 redis-check-aof。在这种情况下，main 函数会调用 redis_check_rdb_main 函数或 redis_check_aof_main 函数，检测 RDB 文件或 AOF 文件。你可以看看下面的代码，其中就展示了 main 函数对这部分内容的检查和调用：\n\n...\n//如果运行的是redis-check-rdb程序，调用redis_check_rdb_main函数检测RDB文件\nif (strstr(argv[0],"redis-check-rdb") != NULL)\n   redis_check_rdb_main(argc,argv,NULL);\n//如果运行的是redis-check-aof程序，调用redis_check_aof_main函数检测AOF文件\nelse if (strstr(argv[0],"redis-check-aof") != NULL)\n   redis_check_aof_main(argc,argv);\n...\n\n\n\n# 阶段三：运行参数解析\n\n在这一阶段，main 函数会对命令行传入的参数进行解析，并且调用 loadServerConfig 函数，对命令行参数和配置文件中的参数进行合并处理，然后为 Redis 各功能模块的关键参数设置合适的取值，以便 server 能高效地运行。\n\n\n# 阶段四：初始化 server\n\n在完成对运行参数的解析和设置后，main 函数会调用 initServer 函数，对 server 运行时的各种资源进行初始化工作。这主要包括了 server 资源管理所需的数据结构初始化、键值对数据库初始化、server 网络框架初始化等。\n\n而在调用完 initServer 后，main 函数还会再次判断当前 server 是否为哨兵模式。如果是哨兵模式，main 函数会调用 sentinelIsRunning 函数，设置启动哨兵模式。否则的话，main 函数会调用 loadDataFromDisk 函数，从磁盘上加载 AOF 或者是 RDB 文件，以便恢复之前的数据。\n\n\n# 阶段五：执行事件驱动框架\n\n为了能高效处理高并发的客户端连接请求，Redis 采用了事件驱动框架，来并发处理不同客户端的连接和读写请求。所以，main 函数执行到最后时，会调用 aeMain 函数进入事件驱动框架，开始循环处理各种触发的事件。\n\n我把刚才介绍的五个阶段涉及到的关键操作，画在了下面的图中，你可以再回顾下。\n\n那么，在这五个阶段当中，阶段三、四和五其实就包括了 Redis server 启动过程中的关键操作。所以接下来，我们就来依次学习下这三个阶段中的主要工作。\n\n\n\n\n# Redis 运行参数解析与设置\n\n我们知道，Redis 提供了丰富的功能，既支持多种键值对数据类型的读写访问，还支持数据持久化保存、主从复制、切片集群等。而这些功能的高效运行，其实都离不开相关功能模块的关键参数配置。\n\n举例来说，Redis 为了节省内存，设计了内存紧凑型的数据结构来保存 Hash、Sorted Set 等键值对类型。但是在使用了内存紧凑型的数据结构之后，如果往数据结构存入的元素个数过多或元素过大的话，键值对的访问性能反而会受到影响。因此，为了平衡内存使用量和系统访问性能，我们就可以通过参数，来设置和调节内存紧凑型数据结构的使用条件。\n\n也就是说，掌握这些关键参数的设置，可以帮助我们提升 Redis 实例的运行效率。\n\n不过，Redis 的参数有很多，我们无法在一节课中掌握所有的参数设置。所以下面，我们可以先来学习下 Redis 的主要参数类型，这样就能对各种参数形成一个全面的了解。同时，我也会给你介绍一些和 server 运行关系密切的参数及其设置方法，以便你可以配置好这些参数，让 server 高效运行起来。\n\n\n# Redis 的主要参数类型\n\n首先，Redis 运行所需的各种参数，都统一定义在了server.h文件的 redisServer 结构体中。根据参数作用的范围，我把各种参数划分为了七大类型，包括通用参数、数据结构参数、网络参数、持久化参数、主从复制参数、切片集群参数、性能优化参数。具体你可以参考下面表格中的内容。\n\n\n\n这样，如果你能按照上面的划分方法给 Redis 参数进行归类，那么你就可以发现，这些参数实际和 Redis 的主要功能机制是相对应的。所以，如果你要深入掌握这些参数的典型配置值，你就需要对相应功能机制的工作原理有所了解。我在接下来的课程中，也会在介绍 Redis 功能模块设计的同时，带你了解下其相应的典型参数配置。\n\n好，现在我们就了解了 Redis 的七大参数类型，以及它们基本的作用范围，那么下面我们就接着来学习下，Redis 是如何针对这些参数进行设置的。\n\n\n# Redis 参数的设置方法\n\nRedis 对运行参数的设置实际上会经过三轮赋值，分别是默认配置值、命令行启动参数，以及配置文件配置值。\n\n首先，Redis 在 main 函数中会先调用 initServerConfig 函数，为各种参数设置默认值。参数的默认值统一定义在 server.h 文件中，都是以 CONFIG_DEFAULT 开头的宏定义变量。下面的代码显示的是部分参数的默认值，你可以看下。\n\n#define CONFIG_DEFAULT_HZ        10   //server后台任务的默认运行频率\n#define CONFIG_MIN_HZ            1    // server后台任务的最小运行频率\n#define CONFIG_MAX_HZ            500 // server后台任务的最大运行频率\n#define CONFIG_DEFAULT_SERVER_PORT  6379  //server监听的默认TCP端口\n#define CONFIG_DEFAULT_CLIENT_TIMEOUT  0  //客户端超时时间，默认为0，表示没有超时限制\n\n\n在 server.h 中提供的默认参数值，一般都是典型的配置值。因此，如果你在部署使用 Redis 实例的过程中，对 Redis 的工作原理不是很了解，就可以使用代码中提供的默认配置。\n\n当然，如果你对 Redis 各功能模块的工作机制比较熟悉的话，也可以自行设置运行参数。你可以在启动 Redis 程序时，在命令行上设置运行参数的值。比如，如果你想将 Redis server 监听端口从默认的 6379 修改为 7379，就可以在命令行上设置 port 参数为 7379，如下所示：\n\n./redis-server --port 7379\n\n\n这里，你需要注意的是，Redis 的命令行参数设置需要使用**两个减号“–”**来表示相应的参数名，否则的话，Redis 就无法识别所设置的运行参数。\n\nRedis 在使用 initServerConfig 函数对参数设置默认配置值后，接下来，main 函数就会对 Redis 程序启动时的命令行参数进行逐一解析。\n\nmain 函数会把解析后的参数及参数值保存成字符串，接着，main 函数会调用 loadServerConfig 函数进行第二和第三轮的赋值。以下代码显示了 main 函数对命令行参数的解析，以及调用 loadServerConfig 函数的过程，你可以看下。\n\nint main(int argc, char **argv) {\n…\n//保存命令行参数\nfor (j = 0; j < argc; j++) server.exec_argv[j] = zstrdup(argv[j]);\n…\nif (argc >= 2) {\n   …\n   //对每个运行时参数进行解析\n   while(j != argc) {\n      …\n   }\n   …\n   //\n   loadServerConfig(configfile,options);\n}\n\n\n这里你要知道的是，loadServerConfig 函数是在config.c文件中实现的，该函数是以 Redis 配置文件和命令行参数的解析字符串为参数，将配置文件中的所有配置项读取出来，形成字符串。紧接着，loadServerConfig 函数会把解析后的命令行参数，追加到配置文件形成的配置项字符串。\n\n这样一来，配置项字符串就同时包含了配置文件中设置的参数，以及命令行设置的参数。\n\n最后，loadServerConfig 函数会进一步调用 loadServerConfigFromString 函数，对配置项字符串中的每一个配置项进行匹配。一旦匹配成功，loadServerConfigFromString 函数就会按照配置项的值设置 server 的参数。\n\n以下代码显示了 loadServerConfigFromString 函数的部分内容。这部分代码是使用了条件分支，来依次比较配置项是否是“timeout”和“tcp-keepalive”，如果匹配上了，就将 server 参数设置为配置项的值。\n\n同时，代码还会检查配置项的值是否合理，比如是否小于 0。如果参数值不合理，程序在运行时就会报错。另外对于其他的配置项，loadServerConfigFromString 函数还会继续使用 elseif 分支进行判断。\n\nloadServerConfigFromString(char *config) {\n   …\n   //参数名匹配，检查参数是否为“timeout“\n   if (!strcasecmp(argv[0],"timeout") && argc == 2) {\n            //设置server的maxidletime参数\n  server.maxidletime = atoi(argv[1]);\n  //检查参数值是否小于0，小于0则报错\n            if (server.maxidletime < 0) {\n                err = "Invalid timeout value"; goto loaderr;\n            }\n   }\n  //参数名匹配，检查参数是否为“tcp-keepalive“\n  else if (!strcasecmp(argv[0],"tcp-keepalive") && argc == 2) {\n            //设置server的tcpkeepalive参数\n  server.tcpkeepalive = atoi(argv[1]);\n  //检查参数值是否小于0，小于0则报错\n            if (server.tcpkeepalive < 0) {\n                err = "Invalid tcp-keepalive value"; goto loaderr;\n            }\n   }\n   …\n}\n\n\n好了，到这里，你应该就了解了 Redis server 运行参数配置的步骤，我也画了一张图，以便你更直观地理解这个过程。\n\n\n\n在完成参数配置后，main 函数会开始调用 initServer 函数，对 server 进行初始化。所以接下来，我们继续来了解 Redis server 初始化时的关键操作。\n\n\n# initServer：初始化 Redis server\n\nRedis server 的初始化操作，主要可以分成三个步骤。\n\n * 第一步，Redis server 运行时需要对多种资源进行管理。\n\n比如说，和 server 连接的客户端、从库等，Redis 用作缓存时的替换候选集，以及 server 运行时的状态信息，这些资源的管理信息都会在 initServer 函数中进行初始化。\n\n我给你举个例子，initServer 函数会创建链表来分别维护客户端和从库，并调用 evictionPoolAlloc 函数（在evict.c中）采样生成用于淘汰的候选 key 集合。同时，initServer 函数还会调用 resetServerStats 函数（在 server.c 中）重置 server 运行状态信息。\n\n * 第二步，在完成资源管理信息的初始化后，initServer 函数会对 Redis 数据库进行初始化。\n\n因为一个 Redis 实例可以同时运行多个数据库，所以 initServer 函数会使用一个循环，依次为每个数据库创建相应的数据结构。\n\n这个代码逻辑是实现在 initServer 函数中，它会为每个数据库执行初始化操作，包括创建全局哈希表，为过期 key、被 BLPOP 阻塞的 key、将被 PUSH 的 key 和被监听的 key 创建相应的信息表。\n\nfor (j = 0; j < server.dbnum; j++) {\n    //创建全局哈希表\n    server.db[j].dict = dictCreate(&dbDictType,NULL);\n    //创建过期key的信息表\n    server.db[j].expires = dictCreate(&keyptrDictType,NULL);\n    //为被BLPOP阻塞的key创建信息表\n    server.db[j].blocking_keys = dictCreate(&keylistDictType,NULL);\n    //为将执行PUSH的阻塞key创建信息表\n    server.db[j].ready_keys = dictCreate(&objectKeyPointerValueDictType,NULL);\n    //为被MULTI/WATCH操作监听的key创建信息表\n    server.db[j].watched_keys = dictCreate(&keylistDictType,NULL);\n    …\n}\n\n\n * 第三步，initServer 函数会为运行的 Redis server 创建事件驱动框架，并开始启动端口监听，用于接收外部请求。\n\n注意，为了高效处理高并发的外部请求，initServer 在创建的事件框架中，针对每个监听 IP 上可能发生的客户端连接，都创建了监听事件，用来监听客户端连接请求。同时，initServer 为监听事件设置了相应的 处理函数 acceptTcpHandler。\n\n这样一来，只要有客户端连接到 server 监听的 IP 和端口，事件驱动框架就会检测到有连接事件发生，然后调用 acceptTcpHandler 函数来处理具体的连接。你可以参考以下代码中展示的处理逻辑：\n\n//创建事件循环框架\nserver.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR);\n…\n//开始监听设置的网络端口\nif (server.port != 0 &&\n        listenToPort(server.port,server.ipfd,&server.ipfd_count) == C_ERR)\n        exit(1);\n…\n//为server后台任务创建定时事件\nif (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR) {\n        serverPanic("Can\'t create event loop timers.");\n        exit(1);\n}\n…\n//为每一个监听的IP设置连接事件的处理函数acceptTcpHandler\nfor (j = 0; j < server.ipfd_count; j++) {\n        if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,\n            acceptTcpHandler,NULL) == AE_ERR)\n       { … }\n}\n\n\n那么到这里，Redis server 在完成运行参数设置和初始化后，就可以开始处理客户端请求了。为了能持续地处理并发的客户端请求，server 在 main 函数的最后，会进入事件驱动循环机制。而这就是接下来，我们要了解的事件驱动框架的执行过程。\n\n\n# 执行事件驱动框架\n\n事件驱动框架是 Redis server 运行的核心。该框架一旦启动后，就会一直循环执行，每次循环会处理一批触发的网络读写事件。关于事件驱动框架本身的设计思想与实现方法，我会在后面给你具体讲解\n\n这节课，我们主要是学习 Redis 入口的 main 函数中，是如何转换到事件驱动框架进行执行的\n\n其实，进入事件驱动框架开始执行并不复杂，main 函数直接调用事件框架的 主体函数 aeMain（在ae.c文件中）后，就进入事件处理循环了。\n\n当然，在进入事件驱动循环前，main 函数会分别调用 aeSetBeforeSleepProc 和 aeSetAfterSleepProc 两个函数，来设置每次进入事件循环前 server 需要执行的操作，以及每次事件循环结束后 server 需要执行的操作。下面代码显示了这部分的执行逻辑，你可以看下。\n\nint main(int argc, char **argv) {\n    ···\n    aeSetBeforeSleepProc(server.el,beforeSleep);\n    aeSetAfterSleepProc(server.el,afterSleep);\n    aeMain(server.el);\n    aeDeleteEventLoop(server.el);\n\t···\n}\n\n\n\n# 总结\n\n今天这节课，我们通过 server.c 文件中 main 函数的设计和实现思路，了解了 Redis server 启动后的五个主要阶段。在这五个阶段中，运行参数解析、server 初始化和执行事件驱动框架则是 Redis sever 启动过程中的三个关键阶段。所以相应的，我们需要重点关注以下三个要点。\n\n第一，main 函数是使用 initServerConfig 给 server 运行参数设置默认值，然后会解析命令行参数，并通过 loadServerConfig 读取配置文件参数值，将命令行参数追加至配置项字符串。最后，Redis 会调用 loadServerConfigFromString 函数，来完成配置文件参数和命令行参数的设置。\n\n第二，在 Redis server 完成参数设置后，initServer 函数会被调用，用来初始化 server 资源管理的主要结构，同时会初始化数据库启动状态，以及完成 server 监听 IP 和端口的设置。\n\n第三，一旦 server 可以接收外部客户端的请求后，main 函数会把程序的主体控制权，交给事件驱动框架的入口函数，也就 aeMain 函数。aeMain 函数会一直循环执行，处理收到的客户端请求。到此为止，server.c 中的 main 函数功能就已经全部完成了，程序控制权也交给了事件驱动循环框架，Redis 也就可以正常处理客户端请求了。\n\n实际上，Redis server 的启动过程从基本的初始化操作，到命令行和配置文件的参数解析设置，再到初始化 server 各种数据结构，以及最后的执行事件驱动框架，这是一个典型的网络服务器执行过程，你在开发网络服务器时，就可以作为参考。\n\n而且，掌握了启动过程中的初始化操作，还可以帮你解答一些使用中的疑惑。比如，Redis 启动时是先读取 RDB 文件，还是先读取 AOF 文件。如果你了解了 Redis server 的启动过程，就可以从 loadDataFromDisk 函数中看到，Redis server 会先读取 AOF；而如果没有 AOF，则再读取 RDB。\n\n所以，掌握 Redis server 启动过程，有助于你更好地了解 Redis 运行细节，这样当你遇到问题时，就知道还可以从启动过程中去溯源 server 的各种初始状态，从而助力你更好地解决问题。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 如何高效执行网络服务器启动的每一步？redis 的 main 函数能带来什么启示？\n 2. redis server 启动后如何确保关键参数正确加载？有哪些常见陷阱？\n 3. redis 如何通过事件驱动模型处理高并发请求？从 main 函数能学到什么？\n 4. 为什么初始化和加载配置参数对 redis 启动至关重要？它如何影响客户端请求处理？\n 5. 如何通过 initserver 函数理解并调优 redis 配置参数以提升性能？\n 6. redis 启动时如何决定加载 aof 还是 rdb？这种设计对数据恢复的影响是什么？\n\n\n# 前言\n\n我们知道，main 函数是 redis 整个运行程序的入口。同时，由于 redis 是典型的 client-server 架构，一旦 redis 实例开始运行，redis server 也就会启动，所以 main 函数其实也会负责 redis server 的启动运行。\n\n> main 函数是在 server.c 中\n\n你平常在设计或实现一个网络服务器程序时，可能会遇到一个问题，那就是\n\n服务器启动时，应该做哪些操作、有没有一个典型的参考实现\n\n所以我就从 main 函数开始，给你介绍下 redis server 是如何在 main 函数中启动并完成初始化的。通过这节课内容的学习，你可以掌握 redis 针对以下三个问题的实现思路：\n\n 1. redis server 启动后具体会做哪些初始化操作？\n 2. redis server 初始化时有哪些关键配置项？\n 3. redis server 如何开始处理客户端请求？\n\n\n# main 函数：redis server 的入口\n\n一般来说，一个使用 c 开发的系统软件启动运行的代码逻辑，都是实现在了 main 函数当中，所以 echo 找到了 main 函数，看看它的执行过程\n\n那么，对于 redis 的 main 函数来说，我把它执行的工作分成了五个阶段。\n\n\n# 阶段一：基本初始化\n\n在这个阶段，main 函数主要是完成一些基本的初始化工作，包括设置 server 运行的时区、设置哈希函数的随机种子等。这部分工作的主要调用函数如下所示：\n\n//设置时区\nsetlocale(lc_collate,"");\ntzset();\n...\n//设置随机种子\nchar hashseed[16];\ngetrandomhexchars(hashseed,sizeof(hashseed));\ndictsethashfunctionseed((uint8_t*)hashseed);\n\n\n这里，你需要注意的是，在 main 函数的开始部分，有一段宏定义覆盖的代码。这部分代码的作用是，如果定义了 redis_test 宏定义，并且 redis server 启动时的参数符合测试参数，那么 main 函数就会执行相应的测试程序。\n\n这段宏定义的代码如以下所示，其中的示例代码就是调用 ziplist 的测试函数 ziplisttest：\n\n#ifdef redis_test\n//如果启动参数有test和ziplist，那么就调用ziplisttest函数进行ziplist的测试\nif (argc == 3 && !strcasecmp(argv[1], "test")) {\n  if (!strcasecmp(argv[2], "ziplist")) {\n     return ziplisttest(argc, argv);\n  }\n  ...\n}\n#endif\n\n\n\n# 阶段二：检查哨兵模式，并检查是否要执行 rdb 检测或 aof 检测\n\nredis server 启动后，可能是以哨兵模式运行的，而哨兵模式运行的 server 在参数初始化、参数设置，以及 server 启动过程中要执行的操作等方面，与普通模式 server 有所差别。所以，main 函数在执行过程中需要根据 redis 配置的参数，检查是否设置了哨兵模式。\n\n如果有设置哨兵模式的话，main 函数会调用 initsentinelconfig 函数，对哨兵模式的参数进行初始化设置，以及调用 initsentinel 函数，初始化设置哨兵模式运行的 server。有关哨兵模式运行的 redis server 相关机制，我会在第 21 讲中给你详细介绍。\n\n下面的代码展示了 main 函数中对哨兵模式的检查，以及对哨兵模式的初始化，你可以看下：\n\n...\n//判断server是否设置为哨兵模式\nif (server.sentinel_mode) {\n    initsentinelconfig();  //初始化哨兵的配置\n    initsentinel();   //初始化哨兵模式\n}\n...\n\n\n除了检查哨兵模式以外，main 函数还会检查是否要执行 rdb 检测或 aof 检查，这对应了实际运行的程序是 redis-check-rdb 或 redis-check-aof。在这种情况下，main 函数会调用 redis_check_rdb_main 函数或 redis_check_aof_main 函数，检测 rdb 文件或 aof 文件。你可以看看下面的代码，其中就展示了 main 函数对这部分内容的检查和调用：\n\n...\n//如果运行的是redis-check-rdb程序，调用redis_check_rdb_main函数检测rdb文件\nif (strstr(argv[0],"redis-check-rdb") != null)\n   redis_check_rdb_main(argc,argv,null);\n//如果运行的是redis-check-aof程序，调用redis_check_aof_main函数检测aof文件\nelse if (strstr(argv[0],"redis-check-aof") != null)\n   redis_check_aof_main(argc,argv);\n...\n\n\n\n# 阶段三：运行参数解析\n\n在这一阶段，main 函数会对命令行传入的参数进行解析，并且调用 loadserverconfig 函数，对命令行参数和配置文件中的参数进行合并处理，然后为 redis 各功能模块的关键参数设置合适的取值，以便 server 能高效地运行。\n\n\n# 阶段四：初始化 server\n\n在完成对运行参数的解析和设置后，main 函数会调用 initserver 函数，对 server 运行时的各种资源进行初始化工作。这主要包括了 server 资源管理所需的数据结构初始化、键值对数据库初始化、server 网络框架初始化等。\n\n而在调用完 initserver 后，main 函数还会再次判断当前 server 是否为哨兵模式。如果是哨兵模式，main 函数会调用 sentinelisrunning 函数，设置启动哨兵模式。否则的话，main 函数会调用 loaddatafromdisk 函数，从磁盘上加载 aof 或者是 rdb 文件，以便恢复之前的数据。\n\n\n# 阶段五：执行事件驱动框架\n\n为了能高效处理高并发的客户端连接请求，redis 采用了事件驱动框架，来并发处理不同客户端的连接和读写请求。所以，main 函数执行到最后时，会调用 aemain 函数进入事件驱动框架，开始循环处理各种触发的事件。\n\n我把刚才介绍的五个阶段涉及到的关键操作，画在了下面的图中，你可以再回顾下。\n\n那么，在这五个阶段当中，阶段三、四和五其实就包括了 redis server 启动过程中的关键操作。所以接下来，我们就来依次学习下这三个阶段中的主要工作。\n\n\n\n\n# redis 运行参数解析与设置\n\n我们知道，redis 提供了丰富的功能，既支持多种键值对数据类型的读写访问，还支持数据持久化保存、主从复制、切片集群等。而这些功能的高效运行，其实都离不开相关功能模块的关键参数配置。\n\n举例来说，redis 为了节省内存，设计了内存紧凑型的数据结构来保存 hash、sorted set 等键值对类型。但是在使用了内存紧凑型的数据结构之后，如果往数据结构存入的元素个数过多或元素过大的话，键值对的访问性能反而会受到影响。因此，为了平衡内存使用量和系统访问性能，我们就可以通过参数，来设置和调节内存紧凑型数据结构的使用条件。\n\n也就是说，掌握这些关键参数的设置，可以帮助我们提升 redis 实例的运行效率。\n\n不过，redis 的参数有很多，我们无法在一节课中掌握所有的参数设置。所以下面，我们可以先来学习下 redis 的主要参数类型，这样就能对各种参数形成一个全面的了解。同时，我也会给你介绍一些和 server 运行关系密切的参数及其设置方法，以便你可以配置好这些参数，让 server 高效运行起来。\n\n\n# redis 的主要参数类型\n\n首先，redis 运行所需的各种参数，都统一定义在了server.h文件的 redisserver 结构体中。根据参数作用的范围，我把各种参数划分为了七大类型，包括通用参数、数据结构参数、网络参数、持久化参数、主从复制参数、切片集群参数、性能优化参数。具体你可以参考下面表格中的内容。\n\n\n\n这样，如果你能按照上面的划分方法给 redis 参数进行归类，那么你就可以发现，这些参数实际和 redis 的主要功能机制是相对应的。所以，如果你要深入掌握这些参数的典型配置值，你就需要对相应功能机制的工作原理有所了解。我在接下来的课程中，也会在介绍 redis 功能模块设计的同时，带你了解下其相应的典型参数配置。\n\n好，现在我们就了解了 redis 的七大参数类型，以及它们基本的作用范围，那么下面我们就接着来学习下，redis 是如何针对这些参数进行设置的。\n\n\n# redis 参数的设置方法\n\nredis 对运行参数的设置实际上会经过三轮赋值，分别是默认配置值、命令行启动参数，以及配置文件配置值。\n\n首先，redis 在 main 函数中会先调用 initserverconfig 函数，为各种参数设置默认值。参数的默认值统一定义在 server.h 文件中，都是以 config_default 开头的宏定义变量。下面的代码显示的是部分参数的默认值，你可以看下。\n\n#define config_default_hz        10   //server后台任务的默认运行频率\n#define config_min_hz            1    // server后台任务的最小运行频率\n#define config_max_hz            500 // server后台任务的最大运行频率\n#define config_default_server_port  6379  //server监听的默认tcp端口\n#define config_default_client_timeout  0  //客户端超时时间，默认为0，表示没有超时限制\n\n\n在 server.h 中提供的默认参数值，一般都是典型的配置值。因此，如果你在部署使用 redis 实例的过程中，对 redis 的工作原理不是很了解，就可以使用代码中提供的默认配置。\n\n当然，如果你对 redis 各功能模块的工作机制比较熟悉的话，也可以自行设置运行参数。你可以在启动 redis 程序时，在命令行上设置运行参数的值。比如，如果你想将 redis server 监听端口从默认的 6379 修改为 7379，就可以在命令行上设置 port 参数为 7379，如下所示：\n\n./redis-server --port 7379\n\n\n这里，你需要注意的是，redis 的命令行参数设置需要使用**两个减号“–”**来表示相应的参数名，否则的话，redis 就无法识别所设置的运行参数。\n\nredis 在使用 initserverconfig 函数对参数设置默认配置值后，接下来，main 函数就会对 redis 程序启动时的命令行参数进行逐一解析。\n\nmain 函数会把解析后的参数及参数值保存成字符串，接着，main 函数会调用 loadserverconfig 函数进行第二和第三轮的赋值。以下代码显示了 main 函数对命令行参数的解析，以及调用 loadserverconfig 函数的过程，你可以看下。\n\nint main(int argc, char **argv) {\n…\n//保存命令行参数\nfor (j = 0; j < argc; j++) server.exec_argv[j] = zstrdup(argv[j]);\n…\nif (argc >= 2) {\n   …\n   //对每个运行时参数进行解析\n   while(j != argc) {\n      …\n   }\n   …\n   //\n   loadserverconfig(configfile,options);\n}\n\n\n这里你要知道的是，loadserverconfig 函数是在config.c文件中实现的，该函数是以 redis 配置文件和命令行参数的解析字符串为参数，将配置文件中的所有配置项读取出来，形成字符串。紧接着，loadserverconfig 函数会把解析后的命令行参数，追加到配置文件形成的配置项字符串。\n\n这样一来，配置项字符串就同时包含了配置文件中设置的参数，以及命令行设置的参数。\n\n最后，loadserverconfig 函数会进一步调用 loadserverconfigfromstring 函数，对配置项字符串中的每一个配置项进行匹配。一旦匹配成功，loadserverconfigfromstring 函数就会按照配置项的值设置 server 的参数。\n\n以下代码显示了 loadserverconfigfromstring 函数的部分内容。这部分代码是使用了条件分支，来依次比较配置项是否是“timeout”和“tcp-keepalive”，如果匹配上了，就将 server 参数设置为配置项的值。\n\n同时，代码还会检查配置项的值是否合理，比如是否小于 0。如果参数值不合理，程序在运行时就会报错。另外对于其他的配置项，loadserverconfigfromstring 函数还会继续使用 elseif 分支进行判断。\n\nloadserverconfigfromstring(char *config) {\n   …\n   //参数名匹配，检查参数是否为“timeout“\n   if (!strcasecmp(argv[0],"timeout") && argc == 2) {\n            //设置server的maxidletime参数\n  server.maxidletime = atoi(argv[1]);\n  //检查参数值是否小于0，小于0则报错\n            if (server.maxidletime < 0) {\n                err = "invalid timeout value"; goto loaderr;\n            }\n   }\n  //参数名匹配，检查参数是否为“tcp-keepalive“\n  else if (!strcasecmp(argv[0],"tcp-keepalive") && argc == 2) {\n            //设置server的tcpkeepalive参数\n  server.tcpkeepalive = atoi(argv[1]);\n  //检查参数值是否小于0，小于0则报错\n            if (server.tcpkeepalive < 0) {\n                err = "invalid tcp-keepalive value"; goto loaderr;\n            }\n   }\n   …\n}\n\n\n好了，到这里，你应该就了解了 redis server 运行参数配置的步骤，我也画了一张图，以便你更直观地理解这个过程。\n\n\n\n在完成参数配置后，main 函数会开始调用 initserver 函数，对 server 进行初始化。所以接下来，我们继续来了解 redis server 初始化时的关键操作。\n\n\n# initserver：初始化 redis server\n\nredis server 的初始化操作，主要可以分成三个步骤。\n\n * 第一步，redis server 运行时需要对多种资源进行管理。\n\n比如说，和 server 连接的客户端、从库等，redis 用作缓存时的替换候选集，以及 server 运行时的状态信息，这些资源的管理信息都会在 initserver 函数中进行初始化。\n\n我给你举个例子，initserver 函数会创建链表来分别维护客户端和从库，并调用 evictionpoolalloc 函数（在evict.c中）采样生成用于淘汰的候选 key 集合。同时，initserver 函数还会调用 resetserverstats 函数（在 server.c 中）重置 server 运行状态信息。\n\n * 第二步，在完成资源管理信息的初始化后，initserver 函数会对 redis 数据库进行初始化。\n\n因为一个 redis 实例可以同时运行多个数据库，所以 initserver 函数会使用一个循环，依次为每个数据库创建相应的数据结构。\n\n这个代码逻辑是实现在 initserver 函数中，它会为每个数据库执行初始化操作，包括创建全局哈希表，为过期 key、被 blpop 阻塞的 key、将被 push 的 key 和被监听的 key 创建相应的信息表。\n\nfor (j = 0; j < server.dbnum; j++) {\n    //创建全局哈希表\n    server.db[j].dict = dictcreate(&dbdicttype,null);\n    //创建过期key的信息表\n    server.db[j].expires = dictcreate(&keyptrdicttype,null);\n    //为被blpop阻塞的key创建信息表\n    server.db[j].blocking_keys = dictcreate(&keylistdicttype,null);\n    //为将执行push的阻塞key创建信息表\n    server.db[j].ready_keys = dictcreate(&objectkeypointervaluedicttype,null);\n    //为被multi/watch操作监听的key创建信息表\n    server.db[j].watched_keys = dictcreate(&keylistdicttype,null);\n    …\n}\n\n\n * 第三步，initserver 函数会为运行的 redis server 创建事件驱动框架，并开始启动端口监听，用于接收外部请求。\n\n注意，为了高效处理高并发的外部请求，initserver 在创建的事件框架中，针对每个监听 ip 上可能发生的客户端连接，都创建了监听事件，用来监听客户端连接请求。同时，initserver 为监听事件设置了相应的 处理函数 accepttcphandler。\n\n这样一来，只要有客户端连接到 server 监听的 ip 和端口，事件驱动框架就会检测到有连接事件发生，然后调用 accepttcphandler 函数来处理具体的连接。你可以参考以下代码中展示的处理逻辑：\n\n//创建事件循环框架\nserver.el = aecreateeventloop(server.maxclients+config_fdset_incr);\n…\n//开始监听设置的网络端口\nif (server.port != 0 &&\n        listentoport(server.port,server.ipfd,&server.ipfd_count) == c_err)\n        exit(1);\n…\n//为server后台任务创建定时事件\nif (aecreatetimeevent(server.el, 1, servercron, null, null) == ae_err) {\n        serverpanic("can\'t create event loop timers.");\n        exit(1);\n}\n…\n//为每一个监听的ip设置连接事件的处理函数accepttcphandler\nfor (j = 0; j < server.ipfd_count; j++) {\n        if (aecreatefileevent(server.el, server.ipfd[j], ae_readable,\n            accepttcphandler,null) == ae_err)\n       { … }\n}\n\n\n那么到这里，redis server 在完成运行参数设置和初始化后，就可以开始处理客户端请求了。为了能持续地处理并发的客户端请求，server 在 main 函数的最后，会进入事件驱动循环机制。而这就是接下来，我们要了解的事件驱动框架的执行过程。\n\n\n# 执行事件驱动框架\n\n事件驱动框架是 redis server 运行的核心。该框架一旦启动后，就会一直循环执行，每次循环会处理一批触发的网络读写事件。关于事件驱动框架本身的设计思想与实现方法，我会在后面给你具体讲解\n\n这节课，我们主要是学习 redis 入口的 main 函数中，是如何转换到事件驱动框架进行执行的\n\n其实，进入事件驱动框架开始执行并不复杂，main 函数直接调用事件框架的 主体函数 aemain（在ae.c文件中）后，就进入事件处理循环了。\n\n当然，在进入事件驱动循环前，main 函数会分别调用 aesetbeforesleepproc 和 aesetaftersleepproc 两个函数，来设置每次进入事件循环前 server 需要执行的操作，以及每次事件循环结束后 server 需要执行的操作。下面代码显示了这部分的执行逻辑，你可以看下。\n\nint main(int argc, char **argv) {\n    ···\n    aesetbeforesleepproc(server.el,beforesleep);\n    aesetaftersleepproc(server.el,aftersleep);\n    aemain(server.el);\n    aedeleteeventloop(server.el);\n\t···\n}\n\n\n\n# 总结\n\n今天这节课，我们通过 server.c 文件中 main 函数的设计和实现思路，了解了 redis server 启动后的五个主要阶段。在这五个阶段中，运行参数解析、server 初始化和执行事件驱动框架则是 redis sever 启动过程中的三个关键阶段。所以相应的，我们需要重点关注以下三个要点。\n\n第一，main 函数是使用 initserverconfig 给 server 运行参数设置默认值，然后会解析命令行参数，并通过 loadserverconfig 读取配置文件参数值，将命令行参数追加至配置项字符串。最后，redis 会调用 loadserverconfigfromstring 函数，来完成配置文件参数和命令行参数的设置。\n\n第二，在 redis server 完成参数设置后，initserver 函数会被调用，用来初始化 server 资源管理的主要结构，同时会初始化数据库启动状态，以及完成 server 监听 ip 和端口的设置。\n\n第三，一旦 server 可以接收外部客户端的请求后，main 函数会把程序的主体控制权，交给事件驱动框架的入口函数，也就 aemain 函数。aemain 函数会一直循环执行，处理收到的客户端请求。到此为止，server.c 中的 main 函数功能就已经全部完成了，程序控制权也交给了事件驱动循环框架，redis 也就可以正常处理客户端请求了。\n\n实际上，redis server 的启动过程从基本的初始化操作，到命令行和配置文件的参数解析设置，再到初始化 server 各种数据结构，以及最后的执行事件驱动框架，这是一个典型的网络服务器执行过程，你在开发网络服务器时，就可以作为参考。\n\n而且，掌握了启动过程中的初始化操作，还可以帮你解答一些使用中的疑惑。比如，redis 启动时是先读取 rdb 文件，还是先读取 aof 文件。如果你了解了 redis server 的启动过程，就可以从 loaddatafromdisk 函数中看到，redis server 会先读取 aof；而如果没有 aof，则再读取 rdb。\n\n所以，掌握 redis server 启动过程，有助于你更好地了解 redis 运行细节，这样当你遇到问题时，就知道还可以从启动过程中去溯源 server 的各种初始状态，从而助力你更好地解决问题。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Redis 的 Reactor 模型",frontmatter:{title:"Redis 的 Reactor 模型",date:"2024-09-15T17:26:35.000Z",permalink:"/pages/d6b00d"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/05.Redis%20%E7%9A%84%20Reactor%20%E6%A8%A1%E5%9E%8B.html",relativePath:"Redis 系统设计/03.三、主线任务/05.Redis 的 Reactor 模型.md",key:"v-a7acaf8a",path:"/pages/d6b00d/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:159},{level:2,title:"Reactor 模型的工作机制",slug:"reactor-模型的工作机制",normalizedTitle:"reactor 模型的工作机制",charIndex:362},{level:3,title:"事件类型",slug:"事件类型",normalizedTitle:"事件类型",charIndex:52},{level:3,title:"关键角色",slug:"关键角色",normalizedTitle:"关键角色",charIndex:75},{level:3,title:"事件驱动框架",slug:"事件驱动框架",normalizedTitle:"事件驱动框架",charIndex:1524},{level:2,title:"Redis 对 Reactor 模型的实现",slug:"redis-对-reactor-模型的实现",normalizedTitle:"redis 对 reactor 模型的实现",charIndex:2307},{level:3,title:"事件的数据结构定义：以 aeFileEvent 为例",slug:"事件的数据结构定义-以-aefileevent-为例",normalizedTitle:"事件的数据结构定义：以 aefileevent 为例",charIndex:2629},{level:3,title:"主循环：aeMain 函数",slug:"主循环-aemain-函数",normalizedTitle:"主循环：aemain 函数",charIndex:3733},{level:3,title:"事件捕获与分发：aeProcessEvents 函数",slug:"事件捕获与分发-aeprocessevents-函数",normalizedTitle:"事件捕获与分发：aeprocessevents 函数",charIndex:4686},{level:3,title:"事件注册：aeCreateFileEvent 函数",slug:"事件注册-aecreatefileevent-函数",normalizedTitle:"事件注册：aecreatefileevent 函数",charIndex:7314},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:438},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:9228}],headersStr:"前言 Reactor 模型的工作机制 事件类型 关键角色 事件驱动框架 Redis 对 Reactor 模型的实现 事件的数据结构定义：以 aeFileEvent 为例 主循环：aeMain 函数 事件捕获与分发：aeProcessEvents 函数 事件注册：aeCreateFileEvent 函数 总结 参考资料",content:'提出问题是一切智慧的开端\n\n * Reactor 模型中有哪些组件？\n * Reactor 模型中有哪些事件类型？\n * Reactor 模型中有哪些关键角色？每个角色的作用？\n * 代码整体控制逻辑有哪些重要部分？\n * Redis 如何实现 Reactor 模型？\n * 如何注册事件？如何循环监听事件？\n\n\n# 前言\n\nRedis的网络框架是实现了Reactor模型吗？\n\n建议分为两部分来思考：\n\n * Reactor 模型是什么\n * Redis 代码实现是如何与 Reactor 模型相对应的\n\nReactor 模型是高性能网络系统实现高并发请求处理的一个重要技术方案。掌握Reactor模型的设计思想与实现方法，可以指导你设计和实现自己的高并发系统。当你要处理成千上万的网络连接时，就不会一筹莫展了。\n\n\n# Reactor 模型的工作机制\n\n实际上，Reactor 模型就是网络服务器端用来处理高并发网络IO请求的一种编程模型。我把这个模型的特征用两个「三」来总结，也就是：\n\n * 三类处理事件，连接事件、写事件、读事件；\n * 三个关键角色，即 reactor、acceptor、handler。\n\n那么，Reactor 模型是如何基于这三类事件和三个角色来处理高并发请求的呢？下面我们就来具体了解下。\n\n\n# 事件类型\n\n我们先来看看这三类事件和 Reactor 模型的关系。\n\n其实，Reactor 模型处理的是客户端和服务器端的 交互过程，而这三类事件正好对应了客户端和服务器端交互过程中，不同类请求在服务器端引发的待处理事件：\n\n * 连接事件\n   * 当一个客户端要和服务器端进行交互时，客户端会向服务器端发送连接请求，以建立连接，这就对应了服务器端的一个连接事件\n * 写事件\n   * 一旦连接建立后，客户端会给服务器端发送读请求，以便读取数据。服务器端在处理读请求时，需要向客户端写回数据，这对应了服务器端的写事件\n * 读事件\n   * 无论客户端给服务器端发送读或写请求，服务器端都需要从客户端读取请求内容，所以在这里，读或写请求的读取就对应了服务器端的读事件\n\n如下所示的图例中，就展示了客户端和服务器端在交互过程中，不同类请求和Reactor模型事件的对应关系，你可以看下。\n\n\n\n好，在了解了 Reactor 模型的三类事件后，你现在可能还有一个疑问：这三类事件是由谁来处理的呢？\n\n\n# 关键角色\n\n这其实就是模型中 三个关键角色 的作用了：\n\n * acceptor ：连接事件由 acceptor 来处理，负责接收连接；acceptor 在接收连接后，会创建 handler，用于网络连接上对后续读写事件的处理；\n * handler：读写事件由 handler 处理；\n * reactor：在高并发场景中，连接事件、读写事件会同时发生，所以，我们需要有一个角色专门监听和分配事件，这就是 reactor 角色。当有连接请求时，reactor 将产生的连接事件交由 acceptor 处理；当有读写请求时，reactor 将读写事件交由 handler 处理。\n\n下图就展示了这三个角色之间的关系，以及它们和事件的关系，你可以看下\n\n\n\n事实上，这三个角色都是Reactor模型中要实现的功能的抽象。\n\n当我们遵循Reactor模型开发服务器端的网络框架时，就需要在编程的时候，在代码功能模块中实现reactor、acceptor和handler 的逻辑\n\n那么，现在我们已经知道，这三个角色是围绕事件的监听、转发和处理来进行交互的，那么在编程时，我们又该如何实现这三者的交互呢？这就离不开 事件驱动框架了\n\n\n# 事件驱动框架\n\n所谓的事件驱动框架，就是在实现 Reactor 模型时，需要实现的代码整体控制逻辑。\n\n简单来说，事件驱动框架包括了两部分\n\n * 一是 事件初始化\n * 二是 事件捕获、分发和处理主循环\n\n事件初始化是在服务器程序启动时就执行的，它的作用主要是 创建需要监听的事件类型，以及该类事件对应的handler。\n\n而一旦服务器完成初始化后，事件初始化也就相应完成了，服务器程序就需要进入到事件捕获、分发和处理的主循环中。\n\n在开发代码时，我们通常会用一个 while循环 来作为这个主循环。\n\n然后在这个主循环中，我们需要\n\n 1. 捕获发生的事件\n 2. 判断事件类型\n 3. 根据事件类型，调用在初始化时创建好的事件handler来实际处理事件\n\n比如说，当有连接事件发生时，服务器程序需要调用acceptor处理函数，创建和客户端的连接。而当有读事件发生时，就表明有读或写请求发送到了服务器端，服务器程序就要调用具体的请求处理函数，从客户端连接中读取请求内容，进而就完成了读事件的处理。这里你可以参考下面给出的图例，其中显示了事件驱动框架的基本执行过程：\n\n\n\n那么到这里，你应该就已经了解了Reactor模型的基本工作机制：客户端的不同类请求会在服务器端触发连接、读、写三类事件，这三类事件的监听、分发和处理又是由reactor、acceptor、handler三类角色来完成的，然后这三类角色会通过事件驱动框架来实现交互和事件处理。\n\n所以可见，实现一个Reactor模型的关键，就是要实现事件驱动框架。那么，如何开发实现一个事件驱动框架呢？\n\nRedis 提供了一个简洁但有效的参考实现，非常值得我们学习，而且也可以用于自己的网络系统开发。下面，我们就一起来学习下Redis中对 Reactor 模型的实现。\n\n\n# Redis 对 Reactor 模型的实现\n\n首先我们要知道的是，Redis 的网络框架实现了 Reactor 模型，并且自行开发实现了一个事件驱动框架。这个框架对应的Redis代码实现文件是ae.c，对应的头文件是ae.h。\n\n前面我们已经知道，事件驱动框架的实现离不开事件的定义，以及事件注册、捕获、分发和处理等一系列操作。当然，对于整个框架来说，还需要能一直运行，持续地响应发生的事件。\n\n那么由此，我们从ae.h头文件中就可以看到，Redis为了实现事件驱动框架，相应地定义了\n\n * 事件的数据结构\n * 框架主循环函数\n * 事件捕获分发函数\n * 事件\n * handler注册函数\n\n所以接下来，我们就依次来了解学习下\n\n\n# 事件的数据结构定义：以 aeFileEvent 为例\n\n在 Redis 事件驱动框架的实现当中，事件的数据结构是关联事件类型和事件处理函数的关键要素。而Redis的事件驱动框架定义了两类事件：\n\n * IO事件：对应 客户端发送的 网络请求\n * 时间事件：对应 Redis 自身的周期性操作\n\n注意：不同类型事件的数据结构定义是不一样的\n\n为了让你能够理解事件数据结构对框架的作用，我就以 IO 事件 aeFileEvent 为例，给你介绍下它的数据结构定义\n\ntypedef struct aeFileEvent {\n    int mask; /* one of AE_(READABLE|WRITABLE|BARRIER) */\n    aeFileProc *rfileProc;\n    aeFileProc *wfileProc;\n    void *clientData;\n} aeFileEvent;\n\n\n * **mask **是用来 表示事件类型 的掩码。对于 IO事件 来说，主要有 AE_READABLE、AE_WRITABLE 和 AE_BARRIER 三种类型事件。框架在分发事件时，依赖的就是结构体中的事件类型\n * rfileProc 和 wfileProce 分别是指向 AE_READABLE 和 AE_WRITABLE 这两类事件的处理函数，也就是 Reactor 模型中的 handler。框架在分发事件后，就需要调用结构体中定义的函数进行事件处理\n * 最后一个成员变量 clientData 是用来指向客户端私有数据的指针\n\n除了事件的数据结构以外，前面我还提到 Redis 在 ae.h 文件中，定义了支撑框架运行的主要函数\n\n * 负责框架主循环的 aeMain 函数\n * 负责事件捕获与分发的 aeProcessEvents 函数\n * 负责事件和 handler 注册的 aeCreateFileEvent 函数\n\n它们的原型定义如下\n\nvoid aeMain(aeEventLoop *eventLoop);\nint aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData);\nint aeProcessEvents(aeEventLoop *eventLoop, int flags);\n\n\n而这三个函数的实现，都是在对应的 ae.c 文件中，那么接下来，我就给你具体介绍下这三个函数的主体逻辑和关键流程\n\n\n# 主循环：aeMain 函数\n\n我们先来看下 aeMain 函数\n\naeMain 函数的逻辑很简单，就是 用一个循环不停地判断事件循环的停止标记。如果事件循环的停止标记被设置为 true，那么针对事件捕获、分发和处理的整个主循环就停止了；否则，主循环会一直执行。aeMain 函数的主体代码如下所示：\n\nvoid aeMain(aeEventLoop *eventLoop) {\n    eventLoop->stop = 0;\n    while (!eventLoop->stop) {\n        if (eventLoop->beforesleep != NULL)\n            eventLoop->beforesleep(eventLoop);\n        aeProcessEvents(eventLoop, AE_ALL_EVENTS|AE_CALL_AFTER_SLEEP);\n    }\n}\n\n\n那么这里你可能要问了，aeMain 函数是在哪里被调用的呢？\n\nint main(int argc, char **argv) {\n\t···\n\taeSetBeforeSleepProc(server.el,beforeSleep);\n    aeSetAfterSleepProc(server.el,afterSleep);\n    aeMain(server.el);\n    aeDeleteEventLoop(server.el);\n    return 0;\n}\n\n\n按照事件驱动框架的编程规范来说，框架主循环是在服务器程序初始化完成后，就会开始执行。因此，如果我们把目光转向 Redis 服务器初始化的函数，就会发现服务器程序的 main 函数在完成 Redis server 的初始化后，会调用 aeMain 函数开始执行事件驱动框架。如果你想具体查看main函数，main函数在server.c文件中，server.c主要用于初始化服务器和执行服务器整体控制流程，你可以回顾下。\n\n不过，既然aeMain函数包含了事件框架的主循环，**那么在主循环中，事件又是如何被捕获、分发和处理呢？**这就是由 aeProcessEvents 函数来完成的了\n\n\n# 事件捕获与分发：aeProcessEvents 函数\n\naeProcessEvents 函数实现的主要功能，包括\n\n * 捕获事件\n * 判断事件类型\n * 调用具体的事件处理函数，从而实现事件的处理\n\n从 aeProcessEvents 函数的主体结构中，我们可以看到主要有三个 if 条件分支，如下所示：\n\nint aeProcessEvents(aeEventLoop *eventLoop, int flags)\n{\n    int processed = 0, numevents;\n \n    /* 若没有事件处理，则立刻返回 */\n    if (!(flags & AE_TIME_EVENTS) && !(flags & AE_FILE_EVENTS)) return 0;\n    \n    /*如果有IO事件发生，或者紧急的时间事件发生，则开始处理*/\n    if (eventLoop->maxfd != -1 || ((flags & AE_TIME_EVENTS) && !(flags & AE_DONT_WAIT))) {\n       …\n    }\n    \n    /* 检查是否有时间事件，若有，则调用processTimeEvents函数处理 */\n    if (flags & AE_TIME_EVENTS)\n        processed += processTimeEvents(eventLoop);\n    \n    /* 返回已经处理的文件或时间*/\n    return processed; \n}\n\n\n这三个分支分别对应了以下三种情况：\n\n * 情况一：既没有时间事件，也没有网络事件\n * 情况二：有 IO 事件或者有需要紧急处理的时间事件\n * 情况三：只有普通的时间事件\n\n那么对于第一种情况来说，因为没有任何事件需要处理，aeProcessEvents 函数就会直接返回到 aeMain 的主循环，开始下一轮的循环；而对于第三种情况来说，该情况发生时只有普通时间事件发生，所以 aeMain 函数会调用专门处理时间事件的函数 processTimeEvents，对时间事件进行处理\n\n现在，我们再来看看第二种情况\n\n首先，当该情况发生时，Redis需要捕获发生的网络事件，并进行相应的处理。那么从Redis源码中我们可以分析得到，在这种情况下，aeApiPoll 函数会被调用，用来捕获事件，如下所示：\n\nint aeProcessEvents(aeEventLoop *eventLoop, int flags){\n   ...\n   if (eventLoop->maxfd != -1 || ((flags & AE_TIME_EVENTS) && !(flags & AE_DONT_WAIT))) {\n       ...\n       //调用 aeApiPoll 函数捕获事件\n       numevents = aeApiPoll(eventLoop, tvp);\n       ...\n    }\n    ...\n」\n\n\n那么，aeApiPoll是如何捕获事件呢？\n\n实际上，Redis是依赖于操作系统底层提供的 IO多路复用机制，来实现事件捕获，检查是否有新的连接、读写事件发生。为了适配不同的操作系统，Redis对不同操作系统实现的网络IO多路复用函数，都进行了统一的封装，封装后的代码分别通过以下四个文件中实现：\n\n * ae_epoll.c，对应Linux上的IO复用函数epoll；\n * ae_evport.c，对应Solaris上的IO复用函数evport；\n * ae_kqueue.c，对应macOS或FreeBSD上的IO复用函数kqueue；\n * ae_select.c，对应Linux（或Windows）的IO复用函数select。\n\n这样，在有了这些封装代码后，Redis 在不同的操作系统上调用 IO 多路复用 API 时，就可以通过统一的接口来进行调用了。\n\n不过看到这里，你可能还是不太明白 Redis 封装的具体操作，所以这里，我就以在服务器端最常用的 Linux 操作系统为例，给你介绍下Redis 是如何封装 Linux 上提供的 IO 复用 API 的。\n\n首先，Linux 上提供了epoll_wait API，用于检测内核中发生的网络IO事件。在ae_epoll.c文件中，aeApiPoll函数就是封装了对epoll_wait的调用。\n\n这个封装程序如下所示，其中你可以看到，在 aeApiPoll 函数中直接调用了 epoll_wait 函数，并将 epoll 返回的事件信息保存起来的逻辑：\n\nstatic int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) {\n    …\n    //调用 epoll_wait 获取监听到的事件\n    retval = epoll_wait(state->epfd,state->events,eventLoop->setsize,\n            tvp ? (tvp->tv_sec*1000 + tvp->tv_usec/1000) : -1);\n    if (retval > 0) {\n        int j;\n        //获得监听到的事件数量\n        numevents = retval;\n        //针对每一个事件，进行处理\n        for (j = 0; j < numevents; j++) {\n             //保存事件信息\n        }\n    }\n    return numevents;\n}\n\n\n为了让你更加清晰地理解，事件驱动框架是如何实现最终对epoll_wait的调用，这里我也放了一张示意图，你可以看看整个调用链是如何工作和实现的。\n\n\n\nOK，现在我们就已经在 aeMain 函数中，看到了 aeProcessEvents 函数被调用，并用于捕获和分发事件的基本处理逻辑。\n\n**那么，事件具体是由哪个函数来处理的呢？**这就和框架中的 aeCreateFileEvents 函数有关了。\n\n\n# 事件注册：aeCreateFileEvent 函数\n\n我们知道，当Redis启动后，服务器程序的 main 函数会调用 initSever 函数来进行初始化，而在初始化的过程中，aeCreateFileEvent 就会被 initServer 函数调用，用于注册要监听的事件，以及相应的事件处理函数。\n\n具体来说，在 initServer 函数的执行过程中，initServer 函数会根据启用的 IP 端口个数，为每个 IP 端口上的网络事件，调用aeCreateFileEvent，创建对 AE_READABLE 事件的监听，并且注册 AE_READABLE 事件的处理 handler，也就是 acceptTcpHandler 函数。这一过程如下图所示：\n\n\n\n所以这里我们可以看到，AE_READABLE 事件就是客户端的网络连接事件，而对应的处理函数就是接收 TCP 连接请求。下面的示例代码中，显示了 initServer 中调用 aeCreateFileEvent 的部分片段，你可以看下：\n\nvoid initServer(void) {\n    …\n    for (j = 0; j < server.ipfd_count; j++) {\n        if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,\n            acceptTcpHandler,NULL) == AE_ERR)\n            {\n                serverPanic("Unrecoverable error creating server.ipfd file event.");\n            }\n\t}\n\t…\n}\n\n\n**那么，aeCreateFileEvent 如何实现事件和处理函数的注册呢？ **这就和刚才我介绍的 Redis 对底层 IO 多路复用函数封装有关了，下面我仍然以Linux系统为例，来给你说明一下。\n\n首先，Linux提供了epoll_ctl API，用于增加新的观察事件。而 Redis 在此基础上，封装了 aeApiAddEvent 函数，对 epoll_ctl 进行调用。\n\n所以这样一来，aeCreateFileEvent 就会调用 aeApiAddEvent，然后 aeApiAddEvent 再通过调用 epoll_ctl，来注册希望监听的事件和相应的处理函数。等到 aeProceeEvents 函数捕获到实际事件时，它就会调用注册的函数对事件进行处理了。\n\n好了，到这里，我们就已经全部了解了Redis中实现事件驱动框架的三个关键函数：aeMain、aeProcessEvents，以及aeCreateFileEvent。当你要去实现一个事件驱动框架时，Redis的设计思想就具有很好的参考意义。\n\n最后我再带你来简单地回顾下，在实现事件驱动框架的时候，你需要先实现一个主循环函数（对应aeMain），负责一直运行框架。其次，你需要编写事件注册函数（对应aeCreateFileEvent），用来注册监听的事件和事件对应的处理函数。只有对事件和处理函数进行了注册，才能在事件发生时调用相应的函数进行处理。\n\n最后，你需要编写事件监听、分发函数（对应aeProcessEvents），负责调用操作系统底层函数来捕获网络连接、读、写事件，并分发给不同处理函数进一步处理。\n\n\n# 总结\n\nRedis 一直被称为单线程架构，按照我们通常的理解，单个线程只能处理单个客户端的请求，但是在实际使用时，我们会看到Redis能同时和成百上千个客户端进行交互，这就是因为Redis基于Reactor模型，实现了高性能的网络框架，通过事件驱动框架，Redis可以使用一个循环来不断捕获、分发和处理客户端产生的网络连接、数据读写事件。\n\n为了方便你从代码层面掌握Redis事件驱动框架的实现，我总结了一个表格，其中列出了Redis事件驱动框架的主要函数和功能、它们所属的C文件，以及这些函数本身是在Redis代码结构中的哪里被调用。你可以使用这张表格，来巩固今天这节课学习的事件驱动框架。\n\n我也再强调下，这节课我们主要关注的是，事件驱动框架的基本运行流程，并以客户端连接事件为例，将框架主循环、事件捕获分发和事件注册的关键步骤串起来，给你做了介绍。Redis事件驱动框架监听处理的事件，还包括客户端请求、服务器端写数据以及周期性操作等，这也是我下一节课要和你一起学习的主要内容。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n * reactor 模型中有哪些组件？\n * reactor 模型中有哪些事件类型？\n * reactor 模型中有哪些关键角色？每个角色的作用？\n * 代码整体控制逻辑有哪些重要部分？\n * redis 如何实现 reactor 模型？\n * 如何注册事件？如何循环监听事件？\n\n\n# 前言\n\nredis的网络框架是实现了reactor模型吗？\n\n建议分为两部分来思考：\n\n * reactor 模型是什么\n * redis 代码实现是如何与 reactor 模型相对应的\n\nreactor 模型是高性能网络系统实现高并发请求处理的一个重要技术方案。掌握reactor模型的设计思想与实现方法，可以指导你设计和实现自己的高并发系统。当你要处理成千上万的网络连接时，就不会一筹莫展了。\n\n\n# reactor 模型的工作机制\n\n实际上，reactor 模型就是网络服务器端用来处理高并发网络io请求的一种编程模型。我把这个模型的特征用两个「三」来总结，也就是：\n\n * 三类处理事件，连接事件、写事件、读事件；\n * 三个关键角色，即 reactor、acceptor、handler。\n\n那么，reactor 模型是如何基于这三类事件和三个角色来处理高并发请求的呢？下面我们就来具体了解下。\n\n\n# 事件类型\n\n我们先来看看这三类事件和 reactor 模型的关系。\n\n其实，reactor 模型处理的是客户端和服务器端的 交互过程，而这三类事件正好对应了客户端和服务器端交互过程中，不同类请求在服务器端引发的待处理事件：\n\n * 连接事件\n   * 当一个客户端要和服务器端进行交互时，客户端会向服务器端发送连接请求，以建立连接，这就对应了服务器端的一个连接事件\n * 写事件\n   * 一旦连接建立后，客户端会给服务器端发送读请求，以便读取数据。服务器端在处理读请求时，需要向客户端写回数据，这对应了服务器端的写事件\n * 读事件\n   * 无论客户端给服务器端发送读或写请求，服务器端都需要从客户端读取请求内容，所以在这里，读或写请求的读取就对应了服务器端的读事件\n\n如下所示的图例中，就展示了客户端和服务器端在交互过程中，不同类请求和reactor模型事件的对应关系，你可以看下。\n\n\n\n好，在了解了 reactor 模型的三类事件后，你现在可能还有一个疑问：这三类事件是由谁来处理的呢？\n\n\n# 关键角色\n\n这其实就是模型中 三个关键角色 的作用了：\n\n * acceptor ：连接事件由 acceptor 来处理，负责接收连接；acceptor 在接收连接后，会创建 handler，用于网络连接上对后续读写事件的处理；\n * handler：读写事件由 handler 处理；\n * reactor：在高并发场景中，连接事件、读写事件会同时发生，所以，我们需要有一个角色专门监听和分配事件，这就是 reactor 角色。当有连接请求时，reactor 将产生的连接事件交由 acceptor 处理；当有读写请求时，reactor 将读写事件交由 handler 处理。\n\n下图就展示了这三个角色之间的关系，以及它们和事件的关系，你可以看下\n\n\n\n事实上，这三个角色都是reactor模型中要实现的功能的抽象。\n\n当我们遵循reactor模型开发服务器端的网络框架时，就需要在编程的时候，在代码功能模块中实现reactor、acceptor和handler 的逻辑\n\n那么，现在我们已经知道，这三个角色是围绕事件的监听、转发和处理来进行交互的，那么在编程时，我们又该如何实现这三者的交互呢？这就离不开 事件驱动框架了\n\n\n# 事件驱动框架\n\n所谓的事件驱动框架，就是在实现 reactor 模型时，需要实现的代码整体控制逻辑。\n\n简单来说，事件驱动框架包括了两部分\n\n * 一是 事件初始化\n * 二是 事件捕获、分发和处理主循环\n\n事件初始化是在服务器程序启动时就执行的，它的作用主要是 创建需要监听的事件类型，以及该类事件对应的handler。\n\n而一旦服务器完成初始化后，事件初始化也就相应完成了，服务器程序就需要进入到事件捕获、分发和处理的主循环中。\n\n在开发代码时，我们通常会用一个 while循环 来作为这个主循环。\n\n然后在这个主循环中，我们需要\n\n 1. 捕获发生的事件\n 2. 判断事件类型\n 3. 根据事件类型，调用在初始化时创建好的事件handler来实际处理事件\n\n比如说，当有连接事件发生时，服务器程序需要调用acceptor处理函数，创建和客户端的连接。而当有读事件发生时，就表明有读或写请求发送到了服务器端，服务器程序就要调用具体的请求处理函数，从客户端连接中读取请求内容，进而就完成了读事件的处理。这里你可以参考下面给出的图例，其中显示了事件驱动框架的基本执行过程：\n\n\n\n那么到这里，你应该就已经了解了reactor模型的基本工作机制：客户端的不同类请求会在服务器端触发连接、读、写三类事件，这三类事件的监听、分发和处理又是由reactor、acceptor、handler三类角色来完成的，然后这三类角色会通过事件驱动框架来实现交互和事件处理。\n\n所以可见，实现一个reactor模型的关键，就是要实现事件驱动框架。那么，如何开发实现一个事件驱动框架呢？\n\nredis 提供了一个简洁但有效的参考实现，非常值得我们学习，而且也可以用于自己的网络系统开发。下面，我们就一起来学习下redis中对 reactor 模型的实现。\n\n\n# redis 对 reactor 模型的实现\n\n首先我们要知道的是，redis 的网络框架实现了 reactor 模型，并且自行开发实现了一个事件驱动框架。这个框架对应的redis代码实现文件是ae.c，对应的头文件是ae.h。\n\n前面我们已经知道，事件驱动框架的实现离不开事件的定义，以及事件注册、捕获、分发和处理等一系列操作。当然，对于整个框架来说，还需要能一直运行，持续地响应发生的事件。\n\n那么由此，我们从ae.h头文件中就可以看到，redis为了实现事件驱动框架，相应地定义了\n\n * 事件的数据结构\n * 框架主循环函数\n * 事件捕获分发函数\n * 事件\n * handler注册函数\n\n所以接下来，我们就依次来了解学习下\n\n\n# 事件的数据结构定义：以 aefileevent 为例\n\n在 redis 事件驱动框架的实现当中，事件的数据结构是关联事件类型和事件处理函数的关键要素。而redis的事件驱动框架定义了两类事件：\n\n * io事件：对应 客户端发送的 网络请求\n * 时间事件：对应 redis 自身的周期性操作\n\n注意：不同类型事件的数据结构定义是不一样的\n\n为了让你能够理解事件数据结构对框架的作用，我就以 io 事件 aefileevent 为例，给你介绍下它的数据结构定义\n\ntypedef struct aefileevent {\n    int mask; /* one of ae_(readable|writable|barrier) */\n    aefileproc *rfileproc;\n    aefileproc *wfileproc;\n    void *clientdata;\n} aefileevent;\n\n\n * **mask **是用来 表示事件类型 的掩码。对于 io事件 来说，主要有 ae_readable、ae_writable 和 ae_barrier 三种类型事件。框架在分发事件时，依赖的就是结构体中的事件类型\n * rfileproc 和 wfileproce 分别是指向 ae_readable 和 ae_writable 这两类事件的处理函数，也就是 reactor 模型中的 handler。框架在分发事件后，就需要调用结构体中定义的函数进行事件处理\n * 最后一个成员变量 clientdata 是用来指向客户端私有数据的指针\n\n除了事件的数据结构以外，前面我还提到 redis 在 ae.h 文件中，定义了支撑框架运行的主要函数\n\n * 负责框架主循环的 aemain 函数\n * 负责事件捕获与分发的 aeprocessevents 函数\n * 负责事件和 handler 注册的 aecreatefileevent 函数\n\n它们的原型定义如下\n\nvoid aemain(aeeventloop *eventloop);\nint aecreatefileevent(aeeventloop *eventloop, int fd, int mask, aefileproc *proc, void *clientdata);\nint aeprocessevents(aeeventloop *eventloop, int flags);\n\n\n而这三个函数的实现，都是在对应的 ae.c 文件中，那么接下来，我就给你具体介绍下这三个函数的主体逻辑和关键流程\n\n\n# 主循环：aemain 函数\n\n我们先来看下 aemain 函数\n\naemain 函数的逻辑很简单，就是 用一个循环不停地判断事件循环的停止标记。如果事件循环的停止标记被设置为 true，那么针对事件捕获、分发和处理的整个主循环就停止了；否则，主循环会一直执行。aemain 函数的主体代码如下所示：\n\nvoid aemain(aeeventloop *eventloop) {\n    eventloop->stop = 0;\n    while (!eventloop->stop) {\n        if (eventloop->beforesleep != null)\n            eventloop->beforesleep(eventloop);\n        aeprocessevents(eventloop, ae_all_events|ae_call_after_sleep);\n    }\n}\n\n\n那么这里你可能要问了，aemain 函数是在哪里被调用的呢？\n\nint main(int argc, char **argv) {\n\t···\n\taesetbeforesleepproc(server.el,beforesleep);\n    aesetaftersleepproc(server.el,aftersleep);\n    aemain(server.el);\n    aedeleteeventloop(server.el);\n    return 0;\n}\n\n\n按照事件驱动框架的编程规范来说，框架主循环是在服务器程序初始化完成后，就会开始执行。因此，如果我们把目光转向 redis 服务器初始化的函数，就会发现服务器程序的 main 函数在完成 redis server 的初始化后，会调用 aemain 函数开始执行事件驱动框架。如果你想具体查看main函数，main函数在server.c文件中，server.c主要用于初始化服务器和执行服务器整体控制流程，你可以回顾下。\n\n不过，既然aemain函数包含了事件框架的主循环，**那么在主循环中，事件又是如何被捕获、分发和处理呢？**这就是由 aeprocessevents 函数来完成的了\n\n\n# 事件捕获与分发：aeprocessevents 函数\n\naeprocessevents 函数实现的主要功能，包括\n\n * 捕获事件\n * 判断事件类型\n * 调用具体的事件处理函数，从而实现事件的处理\n\n从 aeprocessevents 函数的主体结构中，我们可以看到主要有三个 if 条件分支，如下所示：\n\nint aeprocessevents(aeeventloop *eventloop, int flags)\n{\n    int processed = 0, numevents;\n \n    /* 若没有事件处理，则立刻返回 */\n    if (!(flags & ae_time_events) && !(flags & ae_file_events)) return 0;\n    \n    /*如果有io事件发生，或者紧急的时间事件发生，则开始处理*/\n    if (eventloop->maxfd != -1 || ((flags & ae_time_events) && !(flags & ae_dont_wait))) {\n       …\n    }\n    \n    /* 检查是否有时间事件，若有，则调用processtimeevents函数处理 */\n    if (flags & ae_time_events)\n        processed += processtimeevents(eventloop);\n    \n    /* 返回已经处理的文件或时间*/\n    return processed; \n}\n\n\n这三个分支分别对应了以下三种情况：\n\n * 情况一：既没有时间事件，也没有网络事件\n * 情况二：有 io 事件或者有需要紧急处理的时间事件\n * 情况三：只有普通的时间事件\n\n那么对于第一种情况来说，因为没有任何事件需要处理，aeprocessevents 函数就会直接返回到 aemain 的主循环，开始下一轮的循环；而对于第三种情况来说，该情况发生时只有普通时间事件发生，所以 aemain 函数会调用专门处理时间事件的函数 processtimeevents，对时间事件进行处理\n\n现在，我们再来看看第二种情况\n\n首先，当该情况发生时，redis需要捕获发生的网络事件，并进行相应的处理。那么从redis源码中我们可以分析得到，在这种情况下，aeapipoll 函数会被调用，用来捕获事件，如下所示：\n\nint aeprocessevents(aeeventloop *eventloop, int flags){\n   ...\n   if (eventloop->maxfd != -1 || ((flags & ae_time_events) && !(flags & ae_dont_wait))) {\n       ...\n       //调用 aeapipoll 函数捕获事件\n       numevents = aeapipoll(eventloop, tvp);\n       ...\n    }\n    ...\n」\n\n\n那么，aeapipoll是如何捕获事件呢？\n\n实际上，redis是依赖于操作系统底层提供的 io多路复用机制，来实现事件捕获，检查是否有新的连接、读写事件发生。为了适配不同的操作系统，redis对不同操作系统实现的网络io多路复用函数，都进行了统一的封装，封装后的代码分别通过以下四个文件中实现：\n\n * ae_epoll.c，对应linux上的io复用函数epoll；\n * ae_evport.c，对应solaris上的io复用函数evport；\n * ae_kqueue.c，对应macos或freebsd上的io复用函数kqueue；\n * ae_select.c，对应linux（或windows）的io复用函数select。\n\n这样，在有了这些封装代码后，redis 在不同的操作系统上调用 io 多路复用 api 时，就可以通过统一的接口来进行调用了。\n\n不过看到这里，你可能还是不太明白 redis 封装的具体操作，所以这里，我就以在服务器端最常用的 linux 操作系统为例，给你介绍下redis 是如何封装 linux 上提供的 io 复用 api 的。\n\n首先，linux 上提供了epoll_wait api，用于检测内核中发生的网络io事件。在ae_epoll.c文件中，aeapipoll函数就是封装了对epoll_wait的调用。\n\n这个封装程序如下所示，其中你可以看到，在 aeapipoll 函数中直接调用了 epoll_wait 函数，并将 epoll 返回的事件信息保存起来的逻辑：\n\nstatic int aeapipoll(aeeventloop *eventloop, struct timeval *tvp) {\n    …\n    //调用 epoll_wait 获取监听到的事件\n    retval = epoll_wait(state->epfd,state->events,eventloop->setsize,\n            tvp ? (tvp->tv_sec*1000 + tvp->tv_usec/1000) : -1);\n    if (retval > 0) {\n        int j;\n        //获得监听到的事件数量\n        numevents = retval;\n        //针对每一个事件，进行处理\n        for (j = 0; j < numevents; j++) {\n             //保存事件信息\n        }\n    }\n    return numevents;\n}\n\n\n为了让你更加清晰地理解，事件驱动框架是如何实现最终对epoll_wait的调用，这里我也放了一张示意图，你可以看看整个调用链是如何工作和实现的。\n\n\n\nok，现在我们就已经在 aemain 函数中，看到了 aeprocessevents 函数被调用，并用于捕获和分发事件的基本处理逻辑。\n\n**那么，事件具体是由哪个函数来处理的呢？**这就和框架中的 aecreatefileevents 函数有关了。\n\n\n# 事件注册：aecreatefileevent 函数\n\n我们知道，当redis启动后，服务器程序的 main 函数会调用 initsever 函数来进行初始化，而在初始化的过程中，aecreatefileevent 就会被 initserver 函数调用，用于注册要监听的事件，以及相应的事件处理函数。\n\n具体来说，在 initserver 函数的执行过程中，initserver 函数会根据启用的 ip 端口个数，为每个 ip 端口上的网络事件，调用aecreatefileevent，创建对 ae_readable 事件的监听，并且注册 ae_readable 事件的处理 handler，也就是 accepttcphandler 函数。这一过程如下图所示：\n\n\n\n所以这里我们可以看到，ae_readable 事件就是客户端的网络连接事件，而对应的处理函数就是接收 tcp 连接请求。下面的示例代码中，显示了 initserver 中调用 aecreatefileevent 的部分片段，你可以看下：\n\nvoid initserver(void) {\n    …\n    for (j = 0; j < server.ipfd_count; j++) {\n        if (aecreatefileevent(server.el, server.ipfd[j], ae_readable,\n            accepttcphandler,null) == ae_err)\n            {\n                serverpanic("unrecoverable error creating server.ipfd file event.");\n            }\n\t}\n\t…\n}\n\n\n**那么，aecreatefileevent 如何实现事件和处理函数的注册呢？ **这就和刚才我介绍的 redis 对底层 io 多路复用函数封装有关了，下面我仍然以linux系统为例，来给你说明一下。\n\n首先，linux提供了epoll_ctl api，用于增加新的观察事件。而 redis 在此基础上，封装了 aeapiaddevent 函数，对 epoll_ctl 进行调用。\n\n所以这样一来，aecreatefileevent 就会调用 aeapiaddevent，然后 aeapiaddevent 再通过调用 epoll_ctl，来注册希望监听的事件和相应的处理函数。等到 aeproceeevents 函数捕获到实际事件时，它就会调用注册的函数对事件进行处理了。\n\n好了，到这里，我们就已经全部了解了redis中实现事件驱动框架的三个关键函数：aemain、aeprocessevents，以及aecreatefileevent。当你要去实现一个事件驱动框架时，redis的设计思想就具有很好的参考意义。\n\n最后我再带你来简单地回顾下，在实现事件驱动框架的时候，你需要先实现一个主循环函数（对应aemain），负责一直运行框架。其次，你需要编写事件注册函数（对应aecreatefileevent），用来注册监听的事件和事件对应的处理函数。只有对事件和处理函数进行了注册，才能在事件发生时调用相应的函数进行处理。\n\n最后，你需要编写事件监听、分发函数（对应aeprocessevents），负责调用操作系统底层函数来捕获网络连接、读、写事件，并分发给不同处理函数进一步处理。\n\n\n# 总结\n\nredis 一直被称为单线程架构，按照我们通常的理解，单个线程只能处理单个客户端的请求，但是在实际使用时，我们会看到redis能同时和成百上千个客户端进行交互，这就是因为redis基于reactor模型，实现了高性能的网络框架，通过事件驱动框架，redis可以使用一个循环来不断捕获、分发和处理客户端产生的网络连接、数据读写事件。\n\n为了方便你从代码层面掌握redis事件驱动框架的实现，我总结了一个表格，其中列出了redis事件驱动框架的主要函数和功能、它们所属的c文件，以及这些函数本身是在redis代码结构中的哪里被调用。你可以使用这张表格，来巩固今天这节课学习的事件驱动框架。\n\n我也再强调下，这节课我们主要关注的是，事件驱动框架的基本运行流程，并以客户端连接事件为例，将框架主循环、事件捕获分发和事件注册的关键步骤串起来，给你做了介绍。redis事件驱动框架监听处理的事件，还包括客户端请求、服务器端写数据以及周期性操作等，这也是我下一节课要和你一起学习的主要内容。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"第六课：Netty 如何高效发送网络数据",frontmatter:{title:"第六课：Netty 如何高效发送网络数据",date:"2024-09-19T11:12:27.000Z",permalink:"/pages/a73206/"},regularPath:"/Netty%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/25.%E5%9B%9B%E3%80%81%E6%B7%B1%E5%85%A5%20Netty%20%E6%A0%B8%E5%BF%83/20.%E7%AC%AC%E5%85%AD%E8%AF%BE%EF%BC%9ANetty%20%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E5%8F%91%E9%80%81%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE.html",relativePath:"Netty 系统设计/25.四、深入 Netty 核心/20.第六课：Netty 如何高效发送网络数据.md",key:"v-03fe0612",path:"/pages/a73206/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"1. ChannelHandlerContext",slug:"_1-channelhandlercontext",normalizedTitle:"1. channelhandlercontext",charIndex:1061},{level:2,title:"2. write 事件的传播",slug:"_2-write-事件的传播",normalizedTitle:"2. write 事件的传播",charIndex:2361},{level:2,title:"3. write 方法发送数据",slug:"_3-write-方法发送数据",normalizedTitle:"3. write 方法发送数据",charIndex:2991},{level:3,title:"3.1 findContextOutbound",slug:"_3-1-findcontextoutbound",normalizedTitle:"3.1 findcontextoutbound",charIndex:7144},{level:4,title:"3.1.1 掩码的巧妙应用",slug:"_3-1-1-掩码的巧妙应用",normalizedTitle:"3.1.1 掩码的巧妙应用",charIndex:8041},{level:4,title:"3.1.2 向前查找具有执行资格的 ChannelOutboundHandler",slug:"_3-1-2-向前查找具有执行资格的-channeloutboundhandler",normalizedTitle:"3.1.2 向前查找具有执行资格的 channeloutboundhandler",charIndex:10130},{level:4,title:"3.1.3 skipContext",slug:"_3-1-3-skipcontext",normalizedTitle:"3.1.3 skipcontext",charIndex:11584},{level:4,title:"3.1.4 向前传播write事件",slug:"_3-1-4-向前传播write事件",normalizedTitle:"3.1.4 向前传播write事件",charIndex:12933},{level:3,title:"3.1.5 触发nextChannelHandler的write方法回调",slug:"_3-1-5-触发nextchannelhandler的write方法回调",normalizedTitle:"3.1.5 触发nextchannelhandler的write方法回调",charIndex:15583},{level:2,title:"3.2 HeadContext",slug:"_3-2-headcontext",normalizedTitle:"3.2 headcontext",charIndex:18232},{level:3,title:"3.2.1 filterOutboundMessage",slug:"_3-2-1-filteroutboundmessage",normalizedTitle:"3.2.1 filteroutboundmessage",charIndex:20001},{level:3,title:"3.2.2 estimatorHandle计算当前msg的大小",slug:"_3-2-2-estimatorhandle计算当前msg的大小",normalizedTitle:"3.2.2 estimatorhandle计算当前msg的大小",charIndex:20874},{level:2,title:"3.3 ChannelOutboundBuffer",slug:"_3-3-channeloutboundbuffer",normalizedTitle:"3.3 channeloutboundbuffer",charIndex:22956},{level:3,title:"3.3.1 Entry",slug:"_3-3-1-entry",normalizedTitle:"3.3.1 entry",charIndex:24167},{level:3,title:"3.3.2 pendingSize的作用",slug:"_3-3-2-pendingsize的作用",normalizedTitle:"3.3.2 pendingsize的作用",charIndex:26716},{level:3,title:"3.3.3 高低水位线",slug:"_3-3-3-高低水位线",normalizedTitle:"3.3.3 高低水位线",charIndex:27912},{level:3,title:"3.3.4 Entry实例对象在JVM中占用内存大小",slug:"_3-3-4-entry实例对象在jvm中占用内存大小",normalizedTitle:"3.3.4 entry实例对象在jvm中占用内存大小",charIndex:28915},{level:4,title:"开启指针压缩 -XX:+UseCompressedOops",slug:"开启指针压缩-xx-usecompressedoops",normalizedTitle:"开启指针压缩 -xx:+usecompressedoops",charIndex:30982},{level:4,title:"关闭指针压缩 -XX:-UseCompressedOops",slug:"关闭指针压缩-xx-usecompressedoops",normalizedTitle:"关闭指针压缩 -xx:-usecompressedoops",charIndex:31912},{level:3,title:"3.3.5 向ChannelOutboundBuffer中缓存待发送数据",slug:"_3-3-5-向channeloutboundbuffer中缓存待发送数据",normalizedTitle:"3.3.5 向channeloutboundbuffer中缓存待发送数据",charIndex:32872},{level:4,title:"3.3.5.1 创建Entry对象来封装待发送数据信息",slug:"_3-3-5-1-创建entry对象来封装待发送数据信息",normalizedTitle:"3.3.5.1 创建entry对象来封装待发送数据信息",charIndex:33462},{level:4,title:"3.3.5.2 将Entry对象添加进ChannelOutboundBuffer中",slug:"_3-3-5-2-将entry对象添加进channeloutboundbuffer中",normalizedTitle:"3.3.5.2 将entry对象添加进channeloutboundbuffer中",charIndex:36274},{level:4,title:"3.3.5.3 incrementPendingOutboundBytes",slug:"_3-3-5-3-incrementpendingoutboundbytes",normalizedTitle:"3.3.5.3 incrementpendingoutboundbytes",charIndex:37378},{level:2,title:"4. flush",slug:"_4-flush",normalizedTitle:"4. flush",charIndex:41332},{level:3,title:"4.1 flush事件的传播",slug:"_4-1-flush事件的传播",normalizedTitle:"4.1 flush事件的传播",charIndex:41798},{level:3,title:"4.1.1 触发nextChannelHandler的flush方法回调",slug:"_4-1-1-触发nextchannelhandler的flush方法回调",normalizedTitle:"4.1.1 触发nextchannelhandler的flush方法回调",charIndex:43371},{level:3,title:"4.2 flush事件的处理",slug:"_4-2-flush事件的处理",normalizedTitle:"4.2 flush事件的处理",charIndex:44776},{level:4,title:"4.2.1 ChannelOutboundBuffer#addFlush",slug:"_4-2-1-channeloutboundbuffer-addflush",normalizedTitle:"4.2.1 channeloutboundbuffer#addflush",charIndex:46054},{level:3,title:"4.2.2 发送数据前的最后检查---flush0",slug:"_4-2-2-发送数据前的最后检查-flush0",normalizedTitle:"4.2.2 发送数据前的最后检查---flush0",charIndex:49909},{level:4,title:"4.2.2.1  ChannelOutboundBuffer#failFlushed",slug:"_4-2-2-1-channeloutboundbuffer-failflushed",normalizedTitle:"4.2.2.1  channeloutboundbuffer#failflushed",charIndex:null},{level:4,title:"4.2.2.2  ChannelOutboundBuffer#remove0",slug:"_4-2-2-2-channeloutboundbuffer-remove0",normalizedTitle:"4.2.2.2  channeloutboundbuffer#remove0",charIndex:null},{level:2,title:"5. 终于开始真正地发送数据了！",slug:"_5-终于开始真正地发送数据了",normalizedTitle:"5. 终于开始真正地发送数据了！",charIndex:54306},{level:3,title:"5.1 发送数据前的准备工作",slug:"_5-1-发送数据前的准备工作",normalizedTitle:"5.1 发送数据前的准备工作",charIndex:54912},{level:4,title:"5.1.1 获取write loop最大发送循环次数",slug:"_5-1-1-获取write-loop最大发送循环次数",normalizedTitle:"5.1.1 获取write loop最大发送循环次数",charIndex:56019},{level:4,title:"5.1.2 处理在一轮write loop中就发送完数据的情况",slug:"_5-1-2-处理在一轮write-loop中就发送完数据的情况",normalizedTitle:"5.1.2 处理在一轮write loop中就发送完数据的情况",charIndex:56294},{level:4,title:"5.1.3 获取本次write loop 最大允许发送字节数",slug:"_5-1-3-获取本次write-loop-最大允许发送字节数",normalizedTitle:"5.1.3 获取本次write loop 最大允许发送字节数",charIndex:56527},{level:4,title:"5.1.4 将待发送数据转换成 JDK NIO ByteBuffer",slug:"_5-1-4-将待发送数据转换成-jdk-nio-bytebuffer",normalizedTitle:"5.1.4 将待发送数据转换成 jdk nio bytebuffer",charIndex:57576},{level:3,title:"5.2 向JDK NIO SocketChannel发送数据",slug:"_5-2-向jdk-nio-socketchannel发送数据",normalizedTitle:"5.2 向jdk nio socketchannel发送数据",charIndex:58427},{level:4,title:"5.2.1 零拷贝发送网络文件",slug:"_5-2-1-零拷贝发送网络文件",normalizedTitle:"5.2.1 零拷贝发送网络文件",charIndex:59777},{level:4,title:"5.2.2 发送普通数据",slug:"_5-2-2-发送普通数据",normalizedTitle:"5.2.2 发送普通数据",charIndex:62132},{level:2,title:"6. 处理Socket可写但已经写满16次还没写完的情况",slug:"_6-处理socket可写但已经写满16次还没写完的情况",normalizedTitle:"6. 处理socket可写但已经写满16次还没写完的情况",charIndex:68764},{level:2,title:"7. OP_WRITE事件的处理",slug:"_7-op-write事件的处理",normalizedTitle:"7. op_write事件的处理",charIndex:74110},{level:2,title:"8. writeAndFlush",slug:"_8-writeandflush",normalizedTitle:"8. writeandflush",charIndex:77241},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:81566},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:82576}],headersStr:"前言 1. ChannelHandlerContext 2. write 事件的传播 3. write 方法发送数据 3.1 findContextOutbound 3.1.1 掩码的巧妙应用 3.1.2 向前查找具有执行资格的 ChannelOutboundHandler 3.1.3 skipContext 3.1.4 向前传播write事件 3.1.5 触发nextChannelHandler的write方法回调 3.2 HeadContext 3.2.1 filterOutboundMessage 3.2.2 estimatorHandle计算当前msg的大小 3.3 ChannelOutboundBuffer 3.3.1 Entry 3.3.2 pendingSize的作用 3.3.3 高低水位线 3.3.4 Entry实例对象在JVM中占用内存大小 开启指针压缩 -XX:+UseCompressedOops 关闭指针压缩 -XX:-UseCompressedOops 3.3.5 向ChannelOutboundBuffer中缓存待发送数据 3.3.5.1 创建Entry对象来封装待发送数据信息 3.3.5.2 将Entry对象添加进ChannelOutboundBuffer中 3.3.5.3 incrementPendingOutboundBytes 4. flush 4.1 flush事件的传播 4.1.1 触发nextChannelHandler的flush方法回调 4.2 flush事件的处理 4.2.1 ChannelOutboundBuffer#addFlush 4.2.2 发送数据前的最后检查---flush0 4.2.2.1  ChannelOutboundBuffer#failFlushed 4.2.2.2  ChannelOutboundBuffer#remove0 5. 终于开始真正地发送数据了！ 5.1 发送数据前的准备工作 5.1.1 获取write loop最大发送循环次数 5.1.2 处理在一轮write loop中就发送完数据的情况 5.1.3 获取本次write loop 最大允许发送字节数 5.1.4 将待发送数据转换成 JDK NIO ByteBuffer 5.2 向JDK NIO SocketChannel发送数据 5.2.1 零拷贝发送网络文件 5.2.2 发送普通数据 6. 处理Socket可写但已经写满16次还没写完的情况 7. OP_WRITE事件的处理 8. writeAndFlush 总结 参考资料",content:"# 前言\n\n\n\n在《Netty如何高效接收网络数据》一文中，我们介绍了 Netty 的 SubReactor 处理网络数据读取的完整过程，当 Netty 为我们读取了网络请求数据，并且我们在自己的业务线程中完成了业务处理后，就需要将业务处理结果返回给客户端了，那么本文我们就来介绍下 SubReactor 如何处理网络数据发送的整个过程。\n\n我们都知道 Netty 是一款高性能的异步事件驱动的网络通讯框架，既然是网络通讯框架那么它主要做的事情就是：\n\n * 接收客户端连接。\n * 读取连接上的网络请求数据。\n * 向连接发送网络响应数据。\n\n前边系列文章在介绍Netty的启动以及接收连接的过程中，我们只看到 OP_ACCEPT 事件以及 OP_READ 事件的注册，并未看到 OP_WRITE 事件的注册。\n\n * 那么在什么情况下 Netty 才会向 SubReactor 去注册 OP_WRITE 事件呢？\n * Netty 又是怎么对写操作做到异步处理的呢？\n\n本文笔者将会为大家一一揭晓这些谜底。我们还是以之前的 EchoServer 为例进行说明。\n\n@Sharable\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(ChannelHandlerContext ctx, Object msg) {\n        //此处的msg就是Netty在read loop中从NioSocketChannel中读取到的ByteBuffer\n        ctx.write(msg);\n    }\n\n}\n\n\n我们将在《Netty如何高效接收网络数据》一文中读取到的 ByteBuffer (这里的 Object msg)，直接发送回给客户端，用这个简单的例子来揭开 Netty 如何发送数据的序幕~~\n\n> 在实际开发中，我们首先要通过解码器将读取到的 ByteBuffer 解码转换为我们的业务 Request 类，然后在业务线程中做业务处理，在通过编码器对业务 Response 类编码为 ByteBuffer ，最后利用 ChannelHandlerContext ctx 的引用发送响应数据。\n\n> 本文我们只聚焦 Netty 写数据的过程，对于 Netty 编解码相关的内容，笔者会在后续的文章中专门介绍。\n\n\n\n\n# 1. ChannelHandlerContext\n\n\n\n通过前面几篇文章的介绍，我们知道 Netty 会为每个 Channel 分配一个 pipeline ，pipeline 是一个双向链表的结构。Netty 中产生的 IO 异步事件会在这个 pipeline 中传播。\n\nNetty 中的 IO 异步事件大体上分为两类：\n\n * inbound事件：入站事件，比如前边介绍的 ChannelActive 事件， ChannelRead 事件，它们会从 pipeline 的头结点 HeadContext 开始一直向后传播\n * outbound事件：出站事件，比如本文中即将要介绍到的 write事件 以及 flush 事件，出站事件会从相反的方向从后往前传播直到 HeadContext 。最终会在 HeadContext 中完成出站事件的处理。\n   * 本例中用到的 channelHandlerContext.write() 会使 write 事件从当前 ChannelHandler 也就是这里的 EchoServerHandler 开始沿着 pipeline 向前传播\n   * 而 channelHandlerContext.channel().write() 则会使 write 事件从 pipeline 的尾结点 TailContext 开始向前传播直到 HeadContext\n\n\n\n而 pipeline 这样一个双向链表数据结构中的类型正是 ChannelHandlerContext ，由 ChannelHandlerContext 包裹我们自定义的 IO 处理逻辑 ChannelHandler\n\n> ChannelHandler 并不需要感知到它所处的 pipeline 中的上下文信息，只需要专心处理好 IO 逻辑即可，关于 pipeline 的上下文信息全部封装在 ChannelHandlerContext中。\n\nChannelHandler 在 Netty 中的作用只是负责处理 IO 逻辑，比如编码，解码。它并不会感知到它在 pipeline 中的位置，更不会感知和它相邻的两个 ChannelHandler。事实上 ChannelHandler也并不需要去关心这些，它唯一需要关注的就是处理所关心的异步事件\n\n而 ChannelHandlerContext 中维护了 pipeline 这个双向链表中的 pre 以及 next 指针，这样可以方便的找到与其相邻的 ChannelHandler ，并可以过滤出一些符合执行条件的 ChannelHandler。正如它的命名一样， ChannelHandlerContext 正是起到了维护 ChannelHandler 上下文的一个作用。而 Netty 中的异步事件在 pipeline 中的传播靠的就是这个 ChannelHandlerContext 。\n\n> 这样设计就使得 ChannelHandlerContext 和 ChannelHandler 的职责单一，各司其职，具有高度的可扩展性。\n\n\n# 2. write 事件的传播\n\n我们无论是在业务线程或者是在 SubReactor 线程中完成业务处理后，都需要通过 channelHandlerContext 的引用将 write事件在 pipeline 中进行传播。然后在 pipeline 中相应的 ChannelHandler 中监听 write 事件从而可以对 write 事件进行自定义编排处理（比如我们常用的编码器），最终传播到 HeadContext 中执行发送数据的逻辑操作。\n\n前边也提到 Netty 中有两个触发 write 事件传播的方法，它们的传播处理逻辑都是一样的，只不过它们在 pipeline 中的传播起点是不同的。\n\n * channelHandlerContext.write() 方法会从当前 ChannelHandler 开始在 pipeline 中向前传播 write 事件直到 HeadContext。\n * channelHandlerContext.channel().write() 方法则会从 pipeline 的尾结点 TailContext 开始在 pipeline 中向前传播 write 事件直到 HeadContext 。\n\n\n\n在我们清楚了 write 事件的总体传播流程后，接下来就来看看在 write 事件传播的过程中Netty为我们作了些什么？这里我们以 channelHandlerContext.write() 方法为例说明。\n\n\n# 3. write 方法发送数据\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)\n\nabstract class AbstractChannelHandlerContext implements ChannelHandlerContext, ResourceLeakHint {\n\n    @Override\n    public ChannelFuture write(Object msg) {\n        return write(msg, newPromise());\n    }\n\n    @Override\n    public ChannelFuture write(final Object msg, final ChannelPromise promise) {\n        write(msg, false, promise);\n        return promise;\n    }\n\n}\n\n\n这里我们看到 Netty 的写操作是一个异步操作，当我们在业务线程中调用 channelHandlerContext.write() 后，Netty 会给我们返回一个 ChannelFuture，我们可以在这个 ChannelFutrue 中添加 ChannelFutureListener ，这样当 Netty 将我们要发送的数据发送到底层 Socket 中时，Netty 会通过 ChannelFutureListener 通知我们写入结果。\n\n@Override\npublic void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n    //此处的msg就是Netty在read loop中从NioSocketChannel中读取到的ByteBuffer\n    ChannelFuture future = ctx.write(msg);\n    future.addListener(new ChannelFutureListener() {\n        @Override\n        public void operationComplete(ChannelFuture future) throws Exception {\n            Throwable cause = future.cause();\n            if (cause != null) {\n                处理异常情况\n            } else {                    \n                写入Socket成功后，Netty会通知到这里\n            }\n        }\n    });\n}\n\n\n当异步事件在 pipeline 传播的过程中发生异常时，异步事件就会停止在 pipeline 中传播。所以我们在日常开发中，需要对写操作异常情况进行处理。\n\n * 其中 inbound 类异步事件发生异常时，会触发exceptionCaught事件传播。exceptionCaught 事件本身也是一种 inbound 事件，传播方向会从当前发生异常的 ChannelHandler 开始一直向后传播直到 TailContext。\n * 而 outbound 类异步事件发生异常时，则不会触发exceptionCaught事件传播。一般只是通知相关 ChannelFuture。但如果是 flush 事件在传播过程中发生异常，则会触发当前发生异常的 ChannelHandler 中 exceptionCaught 事件回调。\n\n我们继续回归到写操作的主线上来~~~\n\nprivate void write(Object msg, boolean flush, ChannelPromise promise) {\n    ObjectUtil.checkNotNull(msg, \"msg\");\n\n    ................省略检查promise的有效性...............\n\n        //flush = true 表示channelHandler中调用的是writeAndFlush方法，这里需要找到pipeline中覆盖write或者flush方法的channelHandler\n        //flush = false 表示调用的是write方法，只需要找到pipeline中覆盖write方法的channelHandler\n        final AbstractChannelHandlerContext next = findContextOutbound(flush ?\n                                                                       (MASK_WRITE | MASK_FLUSH) : MASK_WRITE);\n    //用于检查内存泄露\n    final Object m = pipeline.touch(msg, next);\n    //获取pipeline中下一个要被执行的channelHandler的executor\n    EventExecutor executor = next.executor();\n    //确保OutBound事件由ChannelHandler指定的executor执行\n    if (executor.inEventLoop()) {\n        //如果当前线程正是channelHandler指定的executor则直接执行\n        if (flush) {\n            next.invokeWriteAndFlush(m, promise);\n        } else {\n            next.invokeWrite(m, promise);\n        }\n    } else {\n        //如果当前线程不是ChannelHandler指定的executor,则封装成异步任务提交给指定executor执行，注意这里的executor不一定是reactor线程。\n        final WriteTask task = WriteTask.newInstance(next, m, promise, flush);\n        if (!safeExecute(executor, task, promise, m, !flush)) {\n            task.cancel();\n        }\n    }\n}\n\n\nwrite 事件要向前在 pipeline 中传播，就需要在 pipeline 上找到下一个具有执行资格的 ChannelHandler，因为位于当前 ChannelHandler 前边的可能是 ChannelInboundHandler 类型的也可能是 ChannelOutboundHandler 类型的 ChannelHandler ，或者有可能压根就不关心 write 事件的 ChannelHandler（没有实现write回调方法）。![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)\n\n这里我们就需要通过 findContextOutbound 方法在当前 ChannelHandler 的前边找到 ChannelOutboundHandler 类型并且覆盖实现 write 回调方法的 ChannelHandler 作为下一个要执行的对象。\n\n\n# 3.1 findContextOutbound\n\nprivate AbstractChannelHandlerContext findContextOutbound(int mask) {\n    AbstractChannelHandlerContext ctx = this;\n    //获取当前ChannelHandler的executor\n    EventExecutor currentExecutor = executor();\n    do {\n        //获取前一个ChannelHandler\n        ctx = ctx.prev;\n    } while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_OUTBOUND));\n    return ctx;\n}\n//判断前一个ChannelHandler是否具有响应Write事件的资格\nprivate static boolean skipContext(\n    AbstractChannelHandlerContext ctx, EventExecutor currentExecutor, int mask, int onlyMask) {\n\n    return (ctx.executionMask & (onlyMask | mask)) == 0 ||\n        (ctx.executor() == currentExecutor && (ctx.executionMask & mask) == 0);\n}\n\n\nfindContextOutbound 方法接收的参数是一个掩码，这个掩码表示要向前查找具有什么样执行资格的 ChannelHandler。因为我们这里调用的是 ChannelHandlerContext 的 write 方法所以 flush = false，传递进来的掩码为 MASK_WRITE，表示我们要向前查找覆盖实现了 write 回调方法的 ChannelOutboundHandler。\n\n# 3.1.1 掩码的巧妙应用\n\nNetty 中将 ChannelHandler 覆盖实现的一些异步事件回调方法用 int 型的掩码来表示，这样我们就可以通过这个掩码来判断当前 ChannelHandler 具有什么样的执行资格。\n\nfinal class ChannelHandlerMask {\n    ....................省略......................\n\n    static final int MASK_CHANNEL_ACTIVE = 1 << 3;\n    static final int MASK_CHANNEL_READ = 1 << 5;\n    static final int MASK_CHANNEL_READ_COMPLETE = 1 << 6;\n    static final int MASK_WRITE = 1 << 15;\n    static final int MASK_FLUSH = 1 << 16;\n\n   //outbound事件掩码集合\n   static final int MASK_ONLY_OUTBOUND =  MASK_BIND | MASK_CONNECT | MASK_DISCONNECT |\n            MASK_CLOSE | MASK_DEREGISTER | MASK_READ | MASK_WRITE | MASK_FLUSH;\n    ....................省略......................\n}\n\n\n在 ChannelHandler 被添加进 pipeline 的时候，Netty 会根据当前 ChannelHandler 的类型以及其覆盖实现的异步事件回调方法，通过 | 运算 向 ChannelHandlerContext#executionMask 字段添加该 ChannelHandler 的执行资格。\n\nabstract class AbstractChannelHandlerContext implements ChannelHandlerContext, ResourceLeakHint {\n\n    //ChannelHandler执行资格掩码\n    private final int executionMask;\n\n    ....................省略......................\n}\n\n\n类似的掩码用法其实我们在前边的文章?《一文聊透Netty核心引擎Reactor的运转架构》中也提到过，在 Channel 向对应的 Reactor 注册自己感兴趣的 IO 事件时，也是用到了一个 int 型的掩码 interestOps 来表示 Channel 感兴趣的 IO 事件集合。\n\n@Override\nprotected void doBeginRead() throws Exception {\n\n    final SelectionKey selectionKey = this.selectionKey;\n    if (!selectionKey.isValid()) {\n        return;\n    }\n\n    readPending = true;\n\n    final int interestOps = selectionKey.interestOps();\n    /**\n     * 1：ServerSocketChannel 初始化时 readInterestOp设置的是OP_ACCEPT事件\n     * 2：SocketChannel 初始化时 readInterestOp设置的是OP_READ事件\n     * */\n    if ((interestOps & readInterestOp) == 0) {\n        //注册监听OP_ACCEPT或者OP_READ事件\n        selectionKey.interestOps(interestOps | readInterestOp);\n    }\n}\n\n\n * 用 & 操作判断，某个事件是否在事件集合中：(readyOps & SelectionKey.OP_CONNECT) != 0\n * 用 | 操作向事件集合中添加事件：interestOps | readInterestOp\n * 从事件集合中删除某个事件，是通过先将要删除事件取反 ~ ，然后在和事件集合做 & 操作：ops &= ~SelectionKey.OP_CONNECT\n\n> 这部分内容笔者会在下篇文章全面介绍 pipeline 的时候详细讲解，大家这里只需要知道这里的掩码就是表示一个执行资格的集合。当前 ChannelHandler 的执行资格存放在它的 ChannelHandlerContext 中的 executionMask 字段中。\n\n# 3.1.2 向前查找具有执行资格的 ChannelOutboundHandler\n\nprivate AbstractChannelHandlerContext findContextOutbound(int mask) {\n    //当前ChannelHandler\n    AbstractChannelHandlerContext ctx = this;\n    //获取当前ChannelHandler的executor\n    EventExecutor currentExecutor = executor();\n    do {\n        //获取前一个ChannelHandler\n        ctx = ctx.prev;\n    } while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_OUTBOUND));\n    return ctx;\n}\n\n//判断前一个ChannelHandler是否具有响应Write事件的资格\nprivate static boolean skipContext(\n    AbstractChannelHandlerContext ctx, EventExecutor currentExecutor, int mask, int onlyMask) {\n\n    return (ctx.executionMask & (onlyMask | mask)) == 0 ||\n        (ctx.executor() == currentExecutor && (ctx.executionMask & mask) == 0);\n}\n\n\n前边我们提到 ChannelHandlerContext 不仅封装了 ChannelHandler 的执行资格掩码还可以感知到当前 ChannelHandler 在 pipeline 中的位置，因为 ChannelHandlerContext 中维护了前驱指针 prev 以及后驱指针 next。\n\n这里我们需要在 pipeline 中传播 write 事件，它是一种 outbound 事件，所以需要向前传播，这里通过 ChannelHandlerContext 的前驱指针 prev 拿到当前 ChannelHandler 在 pipeline 中的前一个节点。\n\nctx = ctx.prev;\n\n\n通过 skipContext 方法判断前驱节点是否具有执行的资格。如果没有执行资格则跳过继续向前查找。如果具有执行资格则返回并响应 write 事件。\n\n在 write 事件传播场景中，执行资格指的是前驱 ChannelHandler 是否是ChannelOutboundHandler 类型的，并且它是否覆盖实现了 write 事件回调方法。\n\npublic class EchoChannelHandler extends ChannelOutboundHandlerAdapter {\n\n    @Override\n    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception {\n        super.write(ctx, msg, promise);\n    }\n}\n\n\n# 3.1.3 skipContext\n\n该方法主要用来判断当前 ChannelHandler 的前驱节点是否具有 mask 掩码中包含的事件响应资格。\n\n方法参数中有两个比较重要的掩码：\n\n * int onlyMask：用来指定当前 ChannelHandler 需要符合的类型。其中MASK_ONLY_OUTBOUND 为 ChannelOutboundHandler 类型的掩码， MASK_ONLY_INBOUND 为 ChannelInboundHandler 类型的掩码。\n\nfinal class ChannelHandlerMask {\n\n    //outbound事件的掩码集合\n    static final int MASK_ONLY_OUTBOUND =  MASK_BIND | MASK_CONNECT | MASK_DISCONNECT |\n            MASK_CLOSE | MASK_DEREGISTER | MASK_READ | MASK_WRITE | MASK_FLUSH;\n\n    //inbound事件的掩码集合\n    static final int MASK_ONLY_INBOUND =  MASK_CHANNEL_REGISTERED |\n            MASK_CHANNEL_UNREGISTERED | MASK_CHANNEL_ACTIVE | MASK_CHANNEL_INACTIVE | MASK_CHANNEL_READ |\n            MASK_CHANNEL_READ_COMPLETE | MASK_USER_EVENT_TRIGGERED | MASK_CHANNEL_WRITABILITY_CHANGED;\n}\n\n\n比如本小节中我们是在介绍 write 事件的传播，那么就需要在当前ChannelHandler 前边首先是找到一个 ChannelOutboundHandler 类型的ChannelHandler。\n\nctx.executionMask & (onlyMask | mask)) == 0 用于判断前一个 ChannelHandler 是否为我们指定的 ChannelHandler 类型，在本小节中我们指定的是 onluMask = MASK_ONLY_OUTBOUND 即 ChannelOutboundHandler 类型。如果不是，这里就会直接跳过，继续在 pipeline 中向前查找。\n\n * int mask：用于指定前一个 ChannelHandler 需要实现的相关异步事件处理回调。在本小节中这里指定的是 MASK_WRITE ，即需要实现 write 回调方法。通过 (ctx.executionMask & mask) == 0 条件来判断前一个ChannelHandler 是否实现了 write 回调，如果没有实现这里就跳过，继续在 pipeline 中向前查找。\n\n> 关于 skipContext 方法的详细介绍，笔者还会在下篇文章全面介绍 pipeline的时候再次进行介绍，这里大家只需要明白该方法的核心逻辑即可。\n\n# 3.1.4 向前传播write事件\n\n通过 findContextOutbound 方法我们在 pipeline 中找到了下一个具有执行资格的 ChannelHandler，这里指的是下一个 ChannelOutboundHandler 类型并且覆盖实现了 write 方法的 ChannelHandler。\n\nNetty 紧接着会调用这个 nextChannelHandler 的 write 方法实现 write 事件在 pipeline 中的传播。\n\n//获取下一个要被执行的channelHandler指定的executor\nEventExecutor executor = next.executor();\n//确保outbound事件的执行 是由 channelHandler指定的executor执行的\nif (executor.inEventLoop()) {\n    //如果当前线程是指定的executor 则直接操作\n    if (flush) {\n        next.invokeWriteAndFlush(m, promise);\n    } else {\n        next.invokeWrite(m, promise);\n    }\n} else {\n    //如果当前线程不是channelHandler指定的executor，则封装程异步任务 提交给指定的executor执行\n    final WriteTask task = WriteTask.newInstance(next, m, promise, flush);\n    if (!safeExecute(executor, task, promise, m, !flush)) {\n        task.cancel();\n    }\n}\n\n\n在我们向 pipeline 添加 ChannelHandler 的时候可以通过ChannelPipeline#addLast(EventExecutorGroup,ChannelHandler......) 方法指定执行该 ChannelHandler 的executor。如果不特殊指定，那么执行该 ChannelHandler 的executor默认为该 Channel 绑定的 Reactor 线程。\n\n> 执行 ChannelHandler 中异步事件回调方法的线程必须是 ChannelHandler 指定的executor。\n\n所以这里首先我们需要获取在 findContextOutbound 方法查找出来的下一个符合执行条件的 ChannelHandler 指定的executor。\n\nEventExecutor executor = next.executor()\n\n\n并通过 executor.inEventLoop() 方法判断当前线程是否是该 ChannelHandler 指定的 executor。\n\n如果是，那么我们直接在当前线程中执行 ChannelHandler 中的 write 方法。\n\n如果不是，我们就需要将 ChannelHandler 对 write 事件的回调操作封装成异步任务 WriteTask 并提交给 ChannelHandler 指定的 executor 中，由 executor 负责执行。\n\n> 这里需要注意的是这个 executor 并不一定是 channel 绑定的 reactor 线程。它可以是我们自定义的线程池，不过需要我们通过 ChannelPipeline#addLast 方法进行指定，如果我们不指定，默认情况下执行 ChannelHandler 的 executor 才是 channel 绑定的 reactor 线程。\n\n> 这里Netty需要确保 outbound 事件是由 channelHandler 指定的 executor 执行的。\n\n这里有些同学可能会有疑问，如果我们向pipieline添加ChannelHandler的时候，为每个ChannelHandler指定不同的executor时，Netty如果确保线程安全呢？？\n\n大家还记得pipeline中的结构吗？\n\n客户端channel pipeline结构.png\n\noutbound 事件在 pipeline 中的传播最终会传播到 HeadContext 中，之前的系列文章我们提到过，HeadContext 中封装了 Channel 的 Unsafe 类负责 Channel 底层的 IO 操作。而 HeadContext 指定的 executor 正是对应 channel 绑定的 reactor 线程。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n所以最终在 netty 内核中执行写操作的线程一定是 reactor 线程从而保证了线程安全性。\n\n> 忘记这段内容的同学可以在回顾下?《Reactor在Netty中的实现(创建篇)》，类似的套路我们在介绍 NioServerSocketChannel 进行 bind 绑定以及 register 注册的时候都介绍过，只不过这里将 executor 扩展到了自定义线程池的范围。\n\n\n# 3.1.5 触发nextChannelHandler的write方法回调\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)write事件的传播1.png\n\n//如果当前线程是指定的executor 则直接操作\nif (flush) {\n    next.invokeWriteAndFlush(m, promise);\n} else {\n    next.invokeWrite(m, promise);\n}\n\n\n由于我们在示例 ChannelHandler 中调用的是 ChannelHandlerContext#write 方法，所以这里的 flush = false 。触发调用 nextChannelHandler 的 write 方法。\n\nvoid invokeWrite(Object msg, ChannelPromise promise) {\n    if (invokeHandler()) {\n        invokeWrite0(msg, promise);\n    } else {\n        // 当前channelHandler虽然添加到pipeline中，但是并没有调用handlerAdded\n        // 所以不能调用当前channelHandler中的回调方法，只能继续向前传递write事件\n        write(msg, promise);\n    }\n}\n\n\n这里首先需要通过 invokeHandler() 方法判断这个 nextChannelHandler 中的 handlerAdded 方法是否被回调过。因为 ChannelHandler 只有被正确的添加到对应的 ChannelHandlerContext 中并且准备好处理异步事件时， ChannelHandler#handlerAdded 方法才会被回调。\n\n> 这一部分内容笔者会在下一篇文章中详细为大家介绍，这里大家只需要了解调用 invokeHandler() 方法的目的就是为了确定 ChannelHandler 是否被正确的初始化。\n\nprivate boolean invokeHandler() {\n    // Store in local variable to reduce volatile reads.\n    int handlerState = this.handlerState;\n    return handlerState == ADD_COMPLETE || (!ordered && handlerState == ADD_PENDING);\n}\n\n\n只有触发了 handlerAdded 回调，ChannelHandler 的状态才能变成 ADD_COMPLETE 。\n\n如果 invokeHandler() 方法返回 false，那么我们就需要跳过这个nextChannelHandler，并调用 ChannelHandlerContext#write 方法继续向前传播 write 事件。\n\n@Override\npublic ChannelFuture write(final Object msg, final ChannelPromise promise) {\n    //继续向前传播write事件，回到流程起点\n    write(msg, false, promise);\n    return promise;\n}\n\n\n如果 invokeHandler() 返回 true ，说明这个 nextChannelHandler 已经在 pipeline 中被正确的初始化了，Netty 直接调用这个 ChannelHandler 的 write 方法，这样就实现了 write 事件从当前 ChannelHandler 传播到了nextChannelHandler。\n\nprivate void invokeWrite0(Object msg, ChannelPromise promise) {\n    try {\n        //调用当前ChannelHandler中的write方法\n        ((ChannelOutboundHandler) handler()).write(this, msg, promise);\n    } catch (Throwable t) {\n        notifyOutboundHandlerException(t, promise);\n    }\n}\n\n\n> 这里我们看到在 write 事件的传播过程中如果发生异常，那么 write 事件就会停止在 pipeline 中传播，并通知注册的 ChannelFutureListener。\n\n客户端channel pipeline结构.png\n\n从本文示例的 pipeline 结构中我们可以看到，当在 EchoServerHandler 调用 ChannelHandlerContext#write 方法后，write 事件会在 pipeline 中向前传播到 HeadContext 中，而在 HeadContext 中才是 Netty 真正处理 write 事件的地方。\n\n\n# 3.2 HeadContext\n\nfinal class HeadContext extends AbstractChannelHandlerContext\n            implements ChannelOutboundHandler, ChannelInboundHandler {\n          \n        @Override\n        public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) {\n            unsafe.write(msg, promise);\n        }\n }\n\n\nwrite 事件最终会在 pipeline 中传播到 HeadContext 里并回调 HeadContext 的 write 方法。并在 write 回调中调用 channel 的 unsafe 类执行底层的 write 操作。这里正是 write 事件在 pipeline 中的传播终点。\n\nChannelOutboundBuffer中缓存待发送数据.png\n\nprotected abstract class AbstractUnsafe implements Unsafe {\n    //待发送数据缓冲队列  Netty是全异步框架，所以这里需要一个缓冲队列来缓存用户需要发送的数据 \n    private volatile ChannelOutboundBuffer outboundBuffer = new ChannelOutboundBuffer(AbstractChannel.this);\n\n    @Override\n    public final void write(Object msg, ChannelPromise promise) {\n        assertEventLoop();\n        //获取当前channel对应的待发送数据缓冲队列（支持用户异步写入的核心关键）\n        ChannelOutboundBuffer outboundBuffer = this.outboundBuffer;\n\n        ..........省略..................\n\n            int size;\n        try {\n            //过滤message类型 这里只会接受DirectBuffer或者fileRegion类型的msg\n            msg = filterOutboundMessage(msg);\n            //计算当前msg的大小\n            size = pipeline.estimatorHandle().size(msg);\n            if (size < 0) {\n                size = 0;\n            }\n        } catch (Throwable t) {\n            ..........省略..................\n        }\n        //将msg 加入到Netty中的待写入数据缓冲队列ChannelOutboundBuffer中\n        outboundBuffer.addMessage(msg, size, promise);\n    }\n\n}\n\n\n众所周知 Netty 是一个异步事件驱动的网络框架，在 Netty 中所有的 IO 操作全部都是异步的，当然也包括本小节介绍的 write 操作，为了保证异步执行 write 操作，Netty 定义了一个待发送数据缓冲队列 ChannelOutboundBuffer ，Netty 将这些用户需要发送的网络数据在写入到 Socket 之前，先放在 ChannelOutboundBuffer 中缓存。\n\n> 每个客户端 NioSocketChannel 对应一个 ChannelOutboundBuffer 待发送数据缓冲队列\n\n\n# 3.2.1 filterOutboundMessage\n\nChannelOutboundBuffer 只会接受 ByteBuffer 类型以及 FileRegion 类型的 msg 数据。\n\n> FileRegion 是Netty定义的用来通过零拷贝的方式网络传输文件数据。本文我们主要聚焦普通网络数据 ByteBuffer 的发送。\n\n所以在将 msg 写入到 ChannelOutboundBuffer 之前，我们需要检查待写入 msg 的类型。确保是 ChannelOutboundBuffer 可接受的类型。\n\n    @Override\n    protected final Object filterOutboundMessage(Object msg) {\n        if (msg instanceof ByteBuf) {\n            ByteBuf buf = (ByteBuf) msg;\n            if (buf.isDirect()) {\n                return msg;\n            }\n\n            return newDirectBuffer(buf);\n        }\n\n        if (msg instanceof FileRegion) {\n            return msg;\n        }\n\n        throw new UnsupportedOperationException(\n                \"unsupported message type: \" + StringUtil.simpleClassName(msg) + EXPECTED_TYPES);\n    }\n\n\n在网络数据传输的过程中，Netty为了减少数据从 堆内内存 到 堆外内存 的拷贝以及缓解GC的压力，所以这里必须采用 DirectByteBuffer 使用堆外内存来存放网络发送数据。\n\n\n# 3.2.2 estimatorHandle计算当前msg的大小\n\npublic class DefaultChannelPipeline implements ChannelPipeline {\n    //原子更新estimatorHandle字段\n    private static final AtomicReferenceFieldUpdater<DefaultChannelPipeline, MessageSizeEstimator.Handle> ESTIMATOR =\n            AtomicReferenceFieldUpdater.newUpdater(\n                    DefaultChannelPipeline.class, MessageSizeEstimator.Handle.class, \"estimatorHandle\");\n\n    //计算要发送msg大小的handler\n    private volatile MessageSizeEstimator.Handle estimatorHandle;\n\n    final MessageSizeEstimator.Handle estimatorHandle() {\n        MessageSizeEstimator.Handle handle = estimatorHandle;\n        if (handle == null) {\n            handle = channel.config().getMessageSizeEstimator().newHandle();\n            if (!ESTIMATOR.compareAndSet(this, null, handle)) {\n                handle = estimatorHandle;\n            }\n        }\n        return handle;\n    }\n}\n\n\n在 pipeline 中会有一个 estimatorHandle 专门用来计算待发送 ByteBuffer 的大小。这个 estimatorHandle 会在 pipeline 对应的 Channel 中的配置类创建的时候被初始化。\n\n这里 estimatorHandle 的实际类型为DefaultMessageSizeEstimator#HandleImpl。\n\npublic final class DefaultMessageSizeEstimator implements MessageSizeEstimator {\n\n    private static final class HandleImpl implements Handle {\n        private final int unknownSize;\n\n        private HandleImpl(int unknownSize) {\n            this.unknownSize = unknownSize;\n        }\n\n        @Override\n        public int size(Object msg) {\n            if (msg instanceof ByteBuf) {\n                return ((ByteBuf) msg).readableBytes();\n            }\n            if (msg instanceof ByteBufHolder) {\n                return ((ByteBufHolder) msg).content().readableBytes();\n            }\n            if (msg instanceof FileRegion) {\n                return 0;\n            }\n            return unknownSize;\n        }\n    }\n\n\n这里我们看到 ByteBuffer 的大小即为 Buffer 中未读取的字节数 writerIndex - readerIndex 。\n\n当我们验证了待写入数据 msg 的类型以及计算了 msg 的大小后，我们就可以通过 ChannelOutboundBuffer#addMessage方法将 msg 写入到ChannelOutboundBuffer（待发送数据缓冲队列）中。\n\nwrite 事件处理的最终逻辑就是将待发送数据写入到 ChannelOutboundBuffer 中，下面我们就来看下这个 ChannelOutboundBuffer 内部结构到底是什么样子的？\n\n\n# 3.3 ChannelOutboundBuffer\n\nChannelOutboundBuffer 其实是一个单链表结构的缓冲队列，链表中的节点类型为 Entry ，由于 ChannelOutboundBuffer 在 Netty 中的作用就是缓存应用程序待发送的网络数据，所以 Entry 中封装的就是待写入 Socket 中的网络发送数据相关的信息，以及 ChannelHandlerContext#write 方法中返回给用户的 ChannelPromise 。这样可以在数据写入Socket之后异步通知应用程序。\n\n此外 ChannelOutboundBuffer 中还封装了三个重要的指针：\n\n * unflushedEntry ：该指针指向 ChannelOutboundBuffer 中第一个待发送数据的 Entry。\n * tailEntry：该指针指向 ChannelOutboundBuffer 中最后一个待发送数据的 Entry。通过 unflushedEntry 和 tailEntry 这两个指针，我们可以很方便的定位到待发送数据的 Entry 范围。\n * flushedEntry：当我们通过 flush 操作需要将 ChannelOutboundBuffer 中缓存的待发送数据发送到 Socket 中时，flushedEntry 指针会指向 unflushedEntry 的位置，这样 flushedEntry 指针和 tailEntry 指针之间的 Entry 就是我们即将发送到 Socket 中的网络数据。\n\n这三个指针在初始化的时候均为 null 。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)ChannelOutboundBuffer结构.png\n\n\n# 3.3.1 Entry\n\nEntry 作为 ChannelOutboundBuffer 链表结构中的节点元素类型，里边封装了待发送数据的各种信息，ChannelOutboundBuffer 其实就是对 Entry 结构的组织和操作。因此理解 Entry 结构是理解整个 ChannelOutboundBuffer 运作流程的基础。\n\n下面我们就来看下 Entry 结构具体封装了哪些待发送数据的信息。\n\nstatic final class Entry {\n    //Entry的对象池，用来创建和回收Entry对象\n    private static final ObjectPool<Entry> RECYCLER = ObjectPool.newPool(new ObjectCreator<Entry>() {\n        @Override\n        public Entry newObject(Handle<Entry> handle) {\n            return new Entry(handle);\n        }\n    });\n\n    //DefaultHandle用于回收对象\n    private final Handle<Entry> handle;\n    //ChannelOutboundBuffer下一个节点\n    Entry next;\n    //待发送数据\n    Object msg;\n    //msg 转换为 jdk nio 中的byteBuffer\n    ByteBuffer[] bufs;\n    ByteBuffer buf;\n    //异步write操作的future\n    ChannelPromise promise;\n    //已发送了多少\n    long progress;\n    //总共需要发送多少，不包含entry对象大小。\n    long total;\n    //pendingSize表示entry对象在堆中需要的内存总量 待发送数据大小 + entry对象本身在堆中占用内存大小（96）\n    int pendingSize;\n    //msg中包含了几个jdk nio bytebuffer\n    int count = -1;\n    //write操作是否被取消\n    boolean cancelled;\n}\n\n\n我们看到Entry结构中一共有12个字段，其中1个静态字段和11个实例字段。\n\n下面笔者就为大家介绍下这12个字段的含义及其作用，其中有些字段会在后面的场景中使用到，这里大家可能对有些字段理解起来比较模糊，不过没关系，这里能看懂多少是多少，不理解也没关系，这里介绍只是为了让大家混个眼熟，在后面流程的讲解中，笔者还会重新提到这些字段。\n\n * ObjectPool<Entry> RECYCLER：Entry 的对象池，负责创建管理 Entry 实例，由于 Netty 是一个网络框架，所以 IO 读写就成了它的核心操作，在一个支持高性能高吞吐的网络框架中，会有大量的 IO 读写操作，那么就会导致频繁的创建 Entry 对象。我们都知道，创建一个实例对象以及 GC 回收这些实例对象都是需要性能开销的，那么在大量频繁创建 Entry 对象的场景下，引入对象池来复用创建好的 Entry 对象实例可以抵消掉由频繁创建对象以及GC回收对象所带来的性能开销。\n\n> 关于对象池的详细内容，感兴趣的同学可以回看下笔者的这篇文章?《详解Recycler对象池的精妙设计与实现》\n\n * Handle<Entry> handle：默认实现类型为 DefaultHandle ，用于数据发送完毕后，对象池回收 Entry 对象。由对象池 RECYCLER 在创建 Entry 对象的时候传递进来。\n\n * Entry next：ChannelOutboundBuffer 是一个单链表的结构，这里的 next 指针用于指向当前 Entry 节点的后继节点。\n\n * Object msg：应用程序待发送的网络数据，这里 msg 的类型为 DirectByteBuffer 或者 FileRegion（用于通过零拷贝的方式网络传输文件）。\n\n * ByteBuffer[] bufs：这里的 ByteBuffer 类型为 JDK NIO 原生的 ByteBuffer 类型，因为 Netty 最终发送数据是通过 JDK NIO 底层的 SocketChannel 进行发送，所以需要将 Netty 中实现的 ByteBuffer 类型转换为 JDK NIO ByteBuffer 类型。应用程序发送的 ByteBuffer 可能是一个也可能是多个，如果发送多个就用 ByteBuffer[] bufs 封装在 Entry 对象中，如果是一个就用 ByteBuffer buf 封装。\n\n * int count：表示待发送数据 msg 中一共包含了多少个 ByteBuffer 需要发送。\n\n * ChannelPromise promise：ChannelHandlerContext#write 异步写操作返回的 ChannelFuture。当 Netty 将待发送数据写入到 Socket 中时会通过这个 ChannelPromise 通知应用程序发送结果。\n\n * long progress：表示当前的一个发送进度，已经发送了多少数据。\n\n * long total：Entry中总共需要发送多少数据。注意：这个字段并不包含 Entry 对象的内存占用大小。只是表示待发送网络数据的大小。\n\n * boolean cancelled：应用程序调用的 write 操作是否被取消。\n\n * int pendingSize：表示待发送数据的内存占用总量。待发送数据在内存中的占用量分为两部分：\n\n * * Entry对象中所封装的待发送网络数据大小。\n   * Entry对象本身在内存中的占用量。\n\nEntry内存占用总量.png\n\n\n# 3.3.2 pendingSize的作用\n\n想象一下这样的一个场景，当由于网络拥塞或者 Netty 客户端负载很高导致网络数据的接收速度以及处理速度越来越慢，TCP 的滑动窗口不断缩小以减少网络数据的发送直到为 0，而 Netty 服务端却有大量频繁的写操作，不断的写入到 ChannelOutboundBuffer 中。\n\n这样就导致了数据发送不出去但是 Netty 服务端又在不停的写数据，慢慢的就会撑爆 ChannelOutboundBuffer 导致OOM。这里主要指的是堆外内存的 OOM，因为 ChannelOutboundBuffer 中包裹的待发送数据全部存储在堆外内存中。\n\n所以 Netty 就必须限制 ChannelOutboundBuffer 中的待发送数据的内存占用总量，不能让它无限增长。Netty 中定义了高低水位线用来表示 ChannelOutboundBuffer 中的待发送数据的内存占用量的上限和下限。注意：这里的内存既包括 JVM 堆内存占用也包括堆外内存占用。\n\n * 当待发送数据的内存占用总量超过高水位线的时候，Netty 就会将 NioSocketChannel 的状态标记为不可写状态。否则就可能导致 OOM。\n * 当待发送数据的内存占用总量低于低水位线的时候，Netty 会再次将 NioSocketChannel 的状态标记为可写状态。\n\n那么我们用什么记录ChannelOutboundBuffer中的待发送数据的内存占用总量呢？\n\n答案就是本小节要介绍的 pendingSize 字段。在谈到待发送数据的内存占用量时大部分同学普遍都会有一个误解就是只计算待发送数据的大小（msg中包含的字节数） 而忽略了 Entry 实例对象本身在内存中的占用量。\n\n因为 Netty 会将待发送数据封装在 Entry 实例对象中，在大量频繁的写操作中会产生大量的 Entry 实例对象，所以 Entry 实例对象的内存占用是不可忽视的。\n\n否则就会导致明明还没有到达高水位线，但是由于大量的 Entry 实例对象存在，从而发生OOM。\n\n所以 pendingSize 的计算既要包含待发送数据的大小也要包含其 Entry 实例对象的内存占用大小，这样才能准确计算出 ChannelOutboundBuffer 中待发送数据的内存占用总量。\n\nChannelOutboundBuffer 中所有的 Entry 实例中的 pendingSize 之和就是待发送数据总的内存占用量。\n\npublic final class ChannelOutboundBuffer {\n  //ChannelOutboundBuffer中的待发送数据的内存占用总量\n  private volatile long totalPendingSize;\n\n}\n\n\n\n# 3.3.3 高低水位线\n\n上小节提到 Netty 为了防止 ChannelOutboundBuffer 中的待发送数据内存占用无限制的增长从而导致 OOM ，所以引入了高低水位线，作为待发送数据内存占用的上限和下限。\n\n那么高低水位线具体设置多大呢 ? 我们来看一下 DefaultChannelConfig 中的配置。\n\npublic class DefaultChannelConfig implements ChannelConfig {\n\n    //ChannelOutboundBuffer中的高低水位线\n    private volatile WriteBufferWaterMark writeBufferWaterMark = WriteBufferWaterMark.DEFAULT;\n\n}\npublic final class WriteBufferWaterMark {\n\n    private static final int DEFAULT_LOW_WATER_MARK = 32 * 1024;\n    private static final int DEFAULT_HIGH_WATER_MARK = 64 * 1024;\n\n    public static final WriteBufferWaterMark DEFAULT =\n            new WriteBufferWaterMark(DEFAULT_LOW_WATER_MARK, DEFAULT_HIGH_WATER_MARK, false);\n\n    WriteBufferWaterMark(int low, int high, boolean validate) {\n\n        ..........省略校验逻辑.........\n\n        this.low = low;\n        this.high = high;\n    }\n}\n\n\n我们看到 ChannelOutboundBuffer 中的高水位线设置的大小为 64 KB，低水位线设置的是 32 KB。\n\n这也就意味着每个 Channel 中的待发送数据如果超过 64 KB。Channel 的状态就会变为不可写状态。当内存占用量低于 32 KB时，Channel 的状态会再次变为可写状态。\n\n\n# 3.3.4 Entry实例对象在JVM中占用内存大小\n\n前边提到 pendingSize 的作用主要是记录当前待发送数据的内存占用总量从而可以预警 OOM 的发生。\n\n待发送数据的内存占用分为：待发送数据 msg 的内存占用大小以及 Entry 对象本身在JVM中的内存占用。\n\n那么 Entry 对象本身的内存占用我们该如何计算呢？\n\n要想搞清楚这个问题，大家需要先了解一下 Java 对象内存布局的相关知识。关于这部分背景知识，笔者已经在 ?《一文聊透对象在JVM中的内存布局，以及内存对齐和压缩指针的原理及应用》这篇文章中给出了详尽的阐述，想深入了解这块的同学可以看下这篇文章。\n\n这里笔者只从这篇文章中提炼一些关于计算 Java 对象占用内存大小相关的内容。\n\n在关于 Java 对象内存布局这篇文章中我们提到，对于Java普通对象来说内存中的布局由：对象头 + 实例数据区 + Padding，这三部分组成。\n\n其中对象头由存储对象运行时信息的 MarkWord 以及指向对象类型元信息的类型指针组成。\n\n> MarkWord 用来存放：hashcode，GC 分代年龄，锁状态标志，线程持有的锁，偏向线程 Id，偏向时间戳等。在 32 位操作系统和 64 位操作系统中 MarkWord 分别占用 4B 和 8B 大小的内存。\n\n> Java 对象头中的类型指针还有实例数据区的对象引用，在64 位系统中开启压缩指针的情况下（-XX:+UseCompressedOops）占用 4B 大小。在关闭压缩指针的情况下（-XX:-UseCompressedOops）占用 8B 大小。\n\n实例数据区用于存储 Java 类中定义的实例字段，包括所有父类中的实例字段以及对象引用。\n\n在实例数据区中对象字段之间的排列以及内存对齐需要遵循三个字段重排列规则：\n\n * 规则1：如果一个字段占用X个字节，那么这个字段的偏移量OFFSET需要对齐至NX。\n * 规则2：在开启了压缩指针的 64 位 JVM 中，Java 类中的第一个字段的 OFFSET 需要对齐至 4N，在关闭压缩指针的情况下类中第一个字段的OFFSET需要对齐至 8N。\n * 规则3：JVM 默认分配字段的顺序为：long / double，int / float，short / char，byte / boolean，oops(Ordianry Object Point 引用类型指针)，并且父类中定义的实例变量会出现在子类实例变量之前。当设置JVM参数 -XX +CompactFields 时（默认），占用内存小于 long / double 的字段会允许被插入到对象中第一个 long / double 字段之前的间隙中，以避免不必要的内存填充。\n\n还有一个重要规则就是 Java 虚拟机堆中对象的起始地址需要对齐至 8 的倍数（可由JVM参数 -XX:ObjectAlignmentInBytes 控制，默认为 8 ）。\n\n在了解上述字段排列以及对象之间的内存对齐规则后，我们分别以开启压缩指针和关闭压缩指针两种情况，来对 Entry 对象的内存布局进行分析并计算对象占用内存大小。\n\nstatic final class Entry {\n    .............省略static字段RECYCLER.........\n\n   \t//DefaultHandle用于回收对象\n   \tprivate final Handle<Entry> handle;\n    //ChannelOutboundBuffer下一个节点\n    Entry next;\n    //待发送数据\n    Object msg;\n    //msg 转换为 jdk nio 中的byteBuffer\n    ByteBuffer[] bufs;\n    ByteBuffer buf;\n    //异步write操作的future\n    ChannelPromise promise;\n    //已发送了多少\n    long progress;\n    //总共需要发送多少，不包含entry对象大小。\n    long total;\n    //pendingSize表示entry对象在堆中需要的内存总量 待发送数据大小 + entry对象本身在堆中占用内存大小（96）\n    int pendingSize;\n    //msg中包含了几个jdk nio bytebuffer\n    int count = -1;\n    //write操作是否被取消\n    boolean cancelled;\n}\n\n\n我们看到 Entry 对象中一共有 11 个实例字段，其中 2 个 long 型字段，2 个 int 型字段，1 个 boolean 型字段，6 个对象引用。\n\n默认情况下JVM参数 -XX +CompactFields 是开启的。\n\n# 开启指针压缩 -XX:+UseCompressedOops\n\nimage.png\n\nEntry 对象的内存布局中开头先是 8 个字节的 MarkWord，然后是 4 个字节的类型指针（开启压缩指针）。\n\n在实例数据区中对象的排列规则需要符合规则3，也就是字段之间的排列顺序需要遵循 long > int > boolean > oop(对象引用)。\n\n根据规则 3 Entry对象实例数据区第一个字段应该是 long progress，但根据规则1 long 型字段的 OFFSET 需要对齐至 8 的倍数，并且根据 规则2 在开启压缩指针的情况下，对象的第一个字段 OFFSET 需要对齐至 4 的倍数。所以字段long progress 的 OFFET = 16，这就必然导致了在对象头与字段 long progress 之间需要由 4 字节的字节填充（OFFET = 12处发生字节填充）。\n\n但是 JVM 默认开启了 -XX +CompactFields，根据 规则3 占用内存小于 long / double 的字段会允许被插入到对象中第一个 long / double 字段之前的间隙中，以避免不必要的内存填充。\n\n所以位于后边的字段 int pendingSize 插入到了 OFFET = 12 位置处，避免了不必要的字节填充。\n\n在 Entry 对象的实例数据区中紧接着基础类型字段后面跟着的就是 6 个对象引用字段(开启压缩指针占用 4 个字节)。\n\n大家一定注意到 OFFSET = 37 处本应该存放的是字段 private final Handle<Entry> handle 但是却被填充了 3 个字节。这是为什么呢？\n\n根据字段重排列规则1：引用字段 private final Handle<Entry> handle 占用 4 个字节（开启压缩指针的情况），所以需要对齐至4的倍数。所以需要填充3个字节，使得引用字段 private final Handle<Entry> handle 位于 OFFSET = 40 处。\n\n根据以上这些规则最终计算出来在开启压缩指针的情况下Entry对象在堆中占用内存大小为64字节\n\n# 关闭指针压缩 -XX:-UseCompressedOops\n\n在分析完 Entry 对象在开启压缩指针情况下的内存布局情况后，我想大家现在对前边介绍的字段重排列的三个规则理解更加清晰了，那么我们基于这个基础来分析下在关闭压缩指针的情况下 Entry 对象的内存布局。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)image.png\n\n首先 Entry 对象在内存布局中的开头依然是由 8 个字节的 MarkWord 还有 8 个字节的类型指针（关闭压缩指针）组成的对象头。\n\n我们看到在 OFFSET = 41 处发生了字节填充，原因是在关闭压缩指针的情况下，对象引用占用内存大小变为 8 个字节，根据规则1: 引用字段 private final Handle<Entry> handle 的 OFFET 需要对齐至 8 的倍数，所以需要在该引用字段之前填充 7 个字节，使得引用字段 private final Handle<Entry> handle 的OFFET = 48 。\n\n综合字段重排列的三个规则最终计算出来在关闭压缩指针的情况下Entry对象在堆中占用内存大小为96字节\n\n\n# 3.3.5 向ChannelOutboundBuffer中缓存待发送数据\n\n在介绍完 ChannelOutboundBuffer 的基本结构之后，下面就来到了 Netty 处理 write 事件的最后一步，我们来看下用户的待发送数据是如何被添加进 ChannelOutboundBuffer 中的。\n\npublic void addMessage(Object msg, int size, ChannelPromise promise) {\n    Entry entry = Entry.newInstance(msg, size, total(msg), promise);\n    if (tailEntry == null) {\n        flushedEntry = null;\n    } else {\n        Entry tail = tailEntry;\n        tail.next = entry;\n    }\n    tailEntry = entry;\n    if (unflushedEntry == null) {\n        unflushedEntry = entry;\n    }\n\n    incrementPendingOutboundBytes(entry.pendingSize, false);\n}\n\n\n# 3.3.5.1 创建Entry对象来封装待发送数据信息\n\n通过前边的介绍我们了解到当用户调用 ctx.write(msg) 之后，write 事件开始在pipeline中从当前 ChannelHandler开始一直向前进行传播，最终在 HeadContext 中将待发送数据写入到 channel 对应的写缓冲区 ChannelOutboundBuffer 中。\n\n而 ChannelOutboundBuffer 是由 Entry 结构组成的一个单链表，Entry 结构封装了用户待发送数据的各种信息。\n\n这里首先我们需要为待发送数据创建 Entry 对象，而在?《详解Recycler对象池的精妙设计与实现》一文中我们介绍对象池时，提到 Netty 作为一个高性能高吞吐的网络框架要面对海量的 IO 处理操作，这种场景下会频繁的创建大量的 Entry 对象，而对象的创建及其回收时需要性能开销的，尤其是在面对大量频繁的创建对象场景下，这种开销会进一步被放大，所以 Netty 引入了对象池来管理 Entry 对象实例从而避免 Entry 对象频繁创建以及 GC 带来的性能开销。\n\n既然 Entry 对象已经被对象池接管，那么它在对象池外面是不能被直接创建的，其构造函数是私有类型，并提供一个静态方法 newInstance 供外部线程从对象池中获取 Entry 对象。这在?《详解Recycler对象池的精妙设计与实现》一文中介绍池化对象的设计时也有提到过。\n\nstatic final class Entry {\n    //静态变量引用类型地址 这个是在Klass Point(类型指针)中定义 8字节（开启指针压缩 为4字节）\n    private static final ObjectPool<Entry> RECYCLER = ObjectPool.newPool(new ObjectCreator<Entry>() {\n        @Override\n        public Entry newObject(Handle<Entry> handle) {\n            return new Entry(handle);\n        }\n    });\n\n    //Entry对象只能通过对象池获取，不可外部自行创建\n    private Entry(Handle<Entry> handle) {\n        this.handle = handle;\n    }\n\n    //不考虑指针压缩的大小 entry对象在堆中占用的内存大小为96\n    //如果开启指针压缩，entry对象在堆中占用的内存大小 会是64  \n    static final int CHANNEL_OUTBOUND_BUFFER_ENTRY_OVERHEAD =\n        SystemPropertyUtil.getInt(\"io.netty.transport.outboundBufferEntrySizeOverhead\", 96);\n\n    static Entry newInstance(Object msg, int size, long total, ChannelPromise promise) {\n        Entry entry = RECYCLER.get();\n        entry.msg = msg;\n        //待发数据数据大小 + entry对象大小\n        entry.pendingSize = size + CHANNEL_OUTBOUND_BUFFER_ENTRY_OVERHEAD;\n        entry.total = total;\n        entry.promise = promise;\n        return entry;\n    }\n\n    .......................省略................\n\n}\n\n\n 1. 通过池化对象 Entry 中持有的对象池 RECYCLER ，从对象池中获取 Entry 对象实例。\n 2. 将用户待发送数据 msg（DirectByteBuffer），待发送数据大小：total ，本次发送数据的 channelFuture，以及该 Entry 对象的 pendingSize 统统封装在 Entry 对象实例的相应字段中。\n\n这里需要特殊说明一点的是关于 pendingSize 的计算方式，之前我们提到 pendingSize 中所计算的内存占用一共包含两部分：\n\n * 待发送网络数据大小\n * Entry 对象本身在内存中的占用量\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)Entry内存占用总量.png\n\n而在《3.3.4 Entry实例对象在JVM中占用内存大小》小节中我们介绍到，Entry 对象在内存中的占用大小在开启压缩指针的情况下（-XX:+UseCompressedOops）占用 64 字节，在关闭压缩指针的情况下（-XX:-UseCompressedOops）占用 96 字节。\n\n字段 CHANNEL_OUTBOUND_BUFFER_ENTRY_OVERHEAD 表示的就是 Entry 对象在内存中的占用大小，Netty这里默认是 96 字节，当然如果我们的应用程序开启了指针压缩，我们可以通过 JVM 启动参数 -D io.netty.transport.outboundBufferEntrySizeOverhead 指定为 64 字节。\n\n# 3.3.5.2 将Entry对象添加进ChannelOutboundBuffer中\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)ChannelOutboundBuffer结构.png\n\nif (tailEntry == null) {\n    flushedEntry = null;\n} else {\n    Entry tail = tailEntry;\n    tail.next = entry;\n}\ntailEntry = entry;\nif (unflushedEntry == null) {\n    unflushedEntry = entry;\n}\n\n\n在《3.3 ChannelOutboundBuffer》小节一开始，我们介绍了 ChannelOutboundBuffer 中最重要的三个指针，这里涉及到的两个指针分别是：\n\n * unflushedEntry ：指向 ChannelOutboundBuffer 中第一个未被 flush 进 Socket 的待发送数据。用来指示 ChannelOutboundBuffer 的第一个节点。\n * tailEntry：指向 ChannelOutboundBuffer 中最后一个节点。\n\n通过 unflushedEntry 和 tailEntry 可以定位出待发送数据的范围。Channel 中的每一次 write 事件，最终都会将待发送数据插入到 ChannelOutboundBuffer 的尾结点处。\n\n# 3.3.5.3 incrementPendingOutboundBytes\n\n在将 Entry 对象添加进 ChannelOutboundBuffer 之后，就需要更新用于记录当前 ChannelOutboundBuffer 中关于待发送数据所占内存总量的水位线指示。\n\n如果更新后的水位线超过了 Netty 指定的高水位线 DEFAULT_HIGH_WATER_MARK = 64 * 1024，则需要将当前 Channel 的状态设置为不可写，并在 pipeline 中传播 ChannelWritabilityChanged 事件，注意该事件是一个 inbound 事件。\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)响应channelWritabilityChanged事件.png\n\npublic final class ChannelOutboundBuffer {\n\n    //ChannelOutboundBuffer中的待发送数据的内存占用总量 : 所有Entry对象本身所占用内存大小 + 所有待发送数据的大小\n    private volatile long totalPendingSize;\n\n    //水位线指针\n    private static final AtomicLongFieldUpdater<ChannelOutboundBuffer> TOTAL_PENDING_SIZE_UPDATER =\n        AtomicLongFieldUpdater.newUpdater(ChannelOutboundBuffer.class, \"totalPendingSize\");\n\n    private void incrementPendingOutboundBytes(long size, boolean invokeLater) {\n        if (size == 0) {\n            return;\n        }\n        //更新总共待写入数据的大小\n        long newWriteBufferSize = TOTAL_PENDING_SIZE_UPDATER.addAndGet(this, size);\n        //如果待写入的数据 大于 高水位线 64 * 1024  则设置当前channel为不可写 由用户自己决定是否继续写入\n        if (newWriteBufferSize > channel.config().getWriteBufferHighWaterMark()) {\n            //设置当前channel状态为不可写，并触发fireChannelWritabilityChanged事件\n            setUnwritable(invokeLater);\n        }\n    }\n\n}\n\n\n> volatile 关键字在 Java 内存模型中只能保证变量的可见性，以及禁止指令重排序。但无法保证多线程更新的原子性，这里我们可以通过AtomicLongFieldUpdater 来帮助 totalPendingSize 字段实现原子性的更新。\n\n// 0表示channel可写，1表示channel不可写\nprivate volatile int unwritable;\n\nprivate static final AtomicIntegerFieldUpdater<ChannelOutboundBuffer> UNWRITABLE_UPDATER =\n    AtomicIntegerFieldUpdater.newUpdater(ChannelOutboundBuffer.class, \"unwritable\");\n\nprivate void setUnwritable(boolean invokeLater) {\n    for (;;) {\n        final int oldValue = unwritable;\n        final int newValue = oldValue | 1;\n        if (UNWRITABLE_UPDATER.compareAndSet(this, oldValue, newValue)) {\n            if (oldValue == 0) {\n                //触发fireChannelWritabilityChanged事件 表示当前channel变为不可写\n                fireChannelWritabilityChanged(invokeLater);\n            }\n            break;\n        }\n    }\n}\n\n\n当 ChannelOutboundBuffer 中的内存占用水位线 totalPendingSize 已经超过高水位线时，调用该方法将当前 Channel 的状态设置为不可写状态。\n\n> unwritable == 0 表示当前channel可写，unwritable == 1 表示当前channel不可写。\n\nchannel 可以通过调用 isWritable 方法来判断自身当前状态是否可写。\n\npublic boolean isWritable() {\n    return unwritable == 0;\n}\n\n\n当 Channel 的状态是首次从可写状态变为不可写状态时，就会在 channel 对应的 pipeline 中传播 ChannelWritabilityChanged 事件。\n\nprivate void fireChannelWritabilityChanged(boolean invokeLater) {\n    final ChannelPipeline pipeline = channel.pipeline();\n    if (invokeLater) {\n        Runnable task = fireChannelWritabilityChangedTask;\n        if (task == null) {\n            fireChannelWritabilityChangedTask = task = new Runnable() {\n                @Override\n                public void run() {\n                    pipeline.fireChannelWritabilityChanged();\n                }\n            };\n        }\n        channel.eventLoop().execute(task);\n    } else {\n        pipeline.fireChannelWritabilityChanged();\n    }\n}\n\n\n用户可以在自定义的 ChannelHandler 中实现 channelWritabilityChanged 事件回调方法，来针对 Channel 的可写状态变化做出不同的处理。\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelWritabilityChanged(ChannelHandlerContext ctx) throws Exception {\n\n        if (ctx.channel().isWritable()) {\n            ...........当前channel可写.........\n        } else {\n            ...........当前channel不可写.........\n        }\n    }\n\n}\n\n\n到这里 write 事件在 pipeline 中的传播，笔者就为大家介绍完了，下面我们来看下另一个重要的 flush 事件的处理过程。\n\n\n# 4. flush\n\n从前面 Netty 对 write 事件的处理过程中，我们可以看到当用户调用 ctx.write(msg) 方法之后，Netty 只是将用户要发送的数据临时写到 channel 对应的待发送缓冲队列 ChannelOutboundBuffer 中，然而并不会将数据写入 Socket 中。\n\n而当一次 read 事件完成之后，我们会调用 ctx.flush() 方法将 ChannelOutboundBuffer 中的待发送数据写入 Socket 中的发送缓冲区中，从而将数据发送出去。\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelReadComplete(ChannelHandlerContext ctx) {\n        //本次OP_READ事件处理完毕\n        ctx.flush();\n    }\n\n}\n\n\n\n# 4.1 flush事件的传播\n\npipeline结构.png\n\nflush 事件和 write 事件一样都是 oubound 事件，所以它们的传播方向都是从后往前在 pipeline 中传播。\n\n触发 flush 事件传播的同样也有两个方法：\n\n * channelHandlerContext.flush()：flush事件会从当前 channelHandler 开始在 pipeline 中向前传播直到 headContext。\n * channelHandlerContext.channel().flush()：flush 事件会从 pipeline 的尾结点 tailContext 处开始向前传播直到 headContext。\n\nabstract class AbstractChannelHandlerContext implements ChannelHandlerContext, ResourceLeakHint {\n\n    @Override\n    public ChannelHandlerContext flush() {\n        //向前查找覆盖flush方法的Outbound类型的ChannelHandler\n        final AbstractChannelHandlerContext next = findContextOutbound(MASK_FLUSH);\n        //获取执行ChannelHandler的executor,在初始化pipeline的时候设置，默认为Reactor线程\n        EventExecutor executor = next.executor();\n        if (executor.inEventLoop()) {\n            next.invokeFlush();\n        } else {\n            Tasks tasks = next.invokeTasks;\n            if (tasks == null) {\n                next.invokeTasks = tasks = new Tasks(next);\n            }\n            safeExecute(executor, tasks.invokeFlushTask, channel().voidPromise(), null, false);\n        }\n\n        return this;\n    }\n\n}\n\n\n这里的逻辑和 write 事件传播的逻辑基本一样，也是首先通过findContextOutbound(MASK_FLUSH) 方法从当前 ChannelHandler 开始从 pipeline 中向前查找出第一个 ChannelOutboundHandler 类型的并且实现 flush 事件回调方法的 ChannelHandler 。注意这里传入的执行资格掩码为 MASK_FLUSH。\n\n执行ChannelHandler中事件回调方法的线程必须是通过pipeline#addLast(EventExecutorGroup group, ChannelHandler... handlers)为 ChannelHandler 指定的 executor。如果不指定，默认的 executor 为 channel 绑定的 reactor 线程。\n\n如果当前线程不是 ChannelHandler 指定的 executor，则需要将 invokeFlush() 方法的调用封装成 Task 交给指定的 executor 执行。\n\n\n# 4.1.1 触发nextChannelHandler的flush方法回调\n\nprivate void invokeFlush() {\n    if (invokeHandler()) {\n        invokeFlush0();\n    } else {\n        //如果该ChannelHandler并没有加入到pipeline中则继续向前传递flush事件\n        flush();\n    }\n}\n\n\n这里和 write 事件的相关处理一样，首先也是需要调用 invokeHandler() 方法来判断这个 nextChannelHandler 是否在 pipeline 中被正确的初始化。\n\n如果 nextChannelHandler 中的 handlerAdded 方法并没有被回调过，那么这里就只能跳过 nextChannelHandler，并调用 ChannelHandlerContext#flush 方法继续向前传播flush事件。\n\n如果 nextChannelHandler 中的 handlerAdded 方法已经被回调过，说明 nextChannelHandler 在 pipeline 中已经被正确的初始化好，则直接调用nextChannelHandler 的 flush 事件回调方法。\n\nprivate void invokeFlush0() {\n    try {\n        ((ChannelOutboundHandler) handler()).flush(this);\n    } catch (Throwable t) {\n        invokeExceptionCaught(t);\n    }\n}\n\n\n这里有一点和 write 事件处理不同的是，当调用 nextChannelHandler 的 flush 回调出现异常的时候，会触发 nextChannelHandler 的 exceptionCaught 回调。\n\nprivate void invokeExceptionCaught(final Throwable cause) {\n    if (invokeHandler()) {\n        try {\n            handler().exceptionCaught(this, cause);\n        } catch (Throwable error) {\n            if (logger.isDebugEnabled()) {\n                logger.debug(....相关日志打印......);\n            } else if (logger.isWarnEnabled()) {\n                logger.warn(...相关日志打印......));\n            }\n        }\n    } else {\n        fireExceptionCaught(cause);\n    }\n}\n\n\n而其他 outbound 类事件比如 write 事件在传播的过程中发生异常，只是回调通知相关的 ChannelFuture。并不会触发 exceptionCaught 事件的传播。\n\n\n# 4.2 flush事件的处理\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)客户端channel pipeline结构.png\n\n最终flush事件会在pipeline中一直向前传播至HeadContext中，并在 HeadContext 里调用 channel 的 unsafe 类完成 flush 事件的最终处理逻辑。\n\nfinal class HeadContext extends AbstractChannelHandlerContext {\n\n    @Override\n    public void flush(ChannelHandlerContext ctx) {\n        unsafe.flush();\n    }\n\n}\n\n\n下面就真正到了 Netty 处理 flush 事件的地方。\n\nprotected abstract class AbstractUnsafe implements Unsafe {\n\n    @Override\n    public final void flush() {\n        assertEventLoop();\n\n        ChannelOutboundBuffer outboundBuffer = this.outboundBuffer;\n        //channel以关闭\n        if (outboundBuffer == null) {\n            return;\n        }\n        //将flushedEntry指针指向ChannelOutboundBuffer头结点，此时变为即将要flush进Socket的数据队列\n        outboundBuffer.addFlush();\n        //将待写数据写进Socket\n        flush0();\n    }\n\n}\n\n\n# 4.2.1 ChannelOutboundBuffer#addFlush\n\n![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)ChannelOutboundBuffer结构.png\n\n这里就到了真正要发送数据的时候了，在 addFlush 方法中会将 flushedEntry 指针指向 unflushedEntry 指针表示的第一个未被 flush 的 Entry 节点。并将 unflushedEntry 指针置为空，准备开始 flush 发送数据流程。\n\n> 此时 ChannelOutboundBuffer 由待发送数据的缓冲队列变为了即将要 flush 进 Socket 的数据队列\n\n这样在 flushedEntry 与 tailEntry 之间的 Entry 节点即为本次 flush 操作需要发送的数据范围。\n\npublic void addFlush() {\n    Entry entry = unflushedEntry;\n    if (entry != null) {\n        if (flushedEntry == null) {\n            flushedEntry = entry;\n        }\n        do {\n            flushed ++;\n            //如果当前entry对应的write操作被用户取消，则释放msg，并降低channelOutboundBuffer水位线\n            if (!entry.promise.setUncancellable()) {\n                int pending = entry.cancel();\n                decrementPendingOutboundBytes(pending, false, true);\n            }\n            entry = entry.next;\n        } while (entry != null);\n\n        // All flushed so reset unflushedEntry\n        unflushedEntry = null;\n    }\n}\n\n\n在 flush 发送数据流程开始时，数据的发送流程就不能被取消了，在这之前我们都是可以通过 ChannelPromise 取消数据发送流程的。\n\n所以这里需要对 ChannelOutboundBuffer 中所有 Entry 节点包裹的 ChannelPromise 设置为不可取消状态。\n\npublic interface Promise<V> extends Future<V> {\n\n    /**\n     * 设置当前future为不可取消状态\n     * \n     * 返回true的情况：\n     * 1：成功的将future设置为uncancellable\n     * 2：当future已经成功完成\n     * \n     * 返回false的情况：\n     * 1：future已经被取消，则不能在设置 uncancellable 状态\n     *\n     */\n    boolean setUncancellable();\n\n}\n\n\n如果这里的 setUncancellable() 方法返回 false 则说明在这之前用户已经将 ChannelPromise 取消掉了，接下来就需要调用 entry.cancel() 方法来释放为待发送数据 msg 分配的堆外内存。\n\nstatic final class Entry {\n    //write操作是否被取消\n    boolean cancelled;\n\n    int cancel() {\n        if (!cancelled) {\n            cancelled = true;\n            int pSize = pendingSize;\n\n            // release message and replace with an empty buffer\n            ReferenceCountUtil.safeRelease(msg);\n            msg = Unpooled.EMPTY_BUFFER;\n\n            pendingSize = 0;\n            total = 0;\n            progress = 0;\n            bufs = null;\n            buf = null;\n            return pSize;\n        }\n        return 0;\n    }\n\n}\n\n\n当 Entry 对象被取消后，就需要减少 ChannelOutboundBuffer 的内存占用总量的水位线 totalPendingSize。\n\nprivate static final AtomicLongFieldUpdater<ChannelOutboundBuffer> TOTAL_PENDING_SIZE_UPDATER =\n    AtomicLongFieldUpdater.newUpdater(ChannelOutboundBuffer.class, \"totalPendingSize\");\n\n//水位线指针.ChannelOutboundBuffer中的待发送数据的内存占用总量 : 所有Entry对象本身所占用内存大小 + 所有待发送数据的大小\nprivate volatile long totalPendingSize;\n\nprivate void decrementPendingOutboundBytes(long size, boolean invokeLater, boolean notifyWritability) {\n    if (size == 0) {\n        return;\n    }\n\n    long newWriteBufferSize = TOTAL_PENDING_SIZE_UPDATER.addAndGet(this, -size);\n    if (notifyWritability && newWriteBufferSize < channel.config().getWriteBufferLowWaterMark()) {\n        setWritable(invokeLater);\n    }\n}\n\n\n当更新后的水位线低于低水位线 DEFAULT_LOW_WATER_MARK = 32 * 1024 时，就将当前 channel 设置为可写状态。\n\nprivate void setWritable(boolean invokeLater) {\n    for (;;) {\n        final int oldValue = unwritable;\n        final int newValue = oldValue & ~1;\n        if (UNWRITABLE_UPDATER.compareAndSet(this, oldValue, newValue)) {\n            if (oldValue != 0 && newValue == 0) {\n                fireChannelWritabilityChanged(invokeLater);\n            }\n            break;\n        }\n    }\n}\n\n\n当 Channel 的状态是第一次从不可写状态变为可写状态时，Netty 会在 pipeline 中再次触发 ChannelWritabilityChanged 事件的传播。\n\n响应channelWritabilityChanged事件.png\n\n\n# 4.2.2 发送数据前的最后检查---flush0\n\nflush0 方法这里主要做的事情就是检查当 channel 的状态是否正常，如果 channel 状态一切正常，则调用 doWrite 方法发送数据。\n\nprotected abstract class AbstractUnsafe implements Unsafe {\n\n    //是否正在进行flush操作\n    private boolean inFlush0; \n\n    protected void flush0() {\n        if (inFlush0) {\n            // Avoid re-entrance\n            return;\n        }\n\n        final ChannelOutboundBuffer outboundBuffer = this.outboundBuffer;\n        //channel已经关闭或者outboundBuffer为空\n        if (outboundBuffer == null || outboundBuffer.isEmpty()) {\n            return;\n        }\n\n        inFlush0 = true;\n\n        if (!isActive()) {\n            try {\n                if (!outboundBuffer.isEmpty()) {\n                    if (isOpen()) {\n                        //当前channel处于disConnected状态  通知promise 写入失败 并触发channelWritabilityChanged事件\n                        outboundBuffer.failFlushed(new NotYetConnectedException(), true);\n                    } else {\n                        //当前channel处于关闭状态 通知promise 写入失败 但不触发channelWritabilityChanged事件\n                        outboundBuffer.failFlushed(newClosedChannelException(initialCloseCause, \"flush0()\"), false);\n                    }\n                }\n            } finally {\n                inFlush0 = false;\n            }\n            return;\n        }\n\n        try {\n            //写入Socket\n            doWrite(outboundBuffer);\n        } catch (Throwable t) {\n            handleWriteError(t);\n        } finally {\n            inFlush0 = false;\n        }\n    }\n\n}\n\n\n * outboundBuffer == null || outboundBuffer.isEmpty() ：如果 channel 已经关闭了或者对应写缓冲区中没有任何数据，那么就停止发送流程，直接 return。\n * !isActive() ：如果当前channel处于非活跃状态，则需要调用 outboundBuffer#failFlushed 通知 ChannelOutboundBuffer 中所有待发送操作对应的 channelPromise 向用户线程报告发送失败。并将待发送数据 Entry 对象从 ChannelOutboundBuffer 中删除，并释放待发送数据空间，回收 Entry 对象实例。\n\n还记得我们在?《Netty如何高效接收网络连接》一文中提到过的 NioSocketChannel 的 active 状态有哪些条件吗？？\n\n@Override\npublic boolean isActive() {\n    SocketChannel ch = javaChannel();\n    return ch.isOpen() && ch.isConnected();\n}\n\n\nNioSocketChannel 处于 active 状态的条件必须是当前 NioSocketChannel 是 open 的同时处于 connected 状态。\n\n * !isActive() && isOpen()：说明当前 channel 处于 disConnected 状态，这时通知给用户 channelPromise 的异常类型为 NotYetConnectedException ,并释放所有待发送数据占用的堆外内存，如果此时内存占用量低于低水位线，则设置 channel 为可写状态，并触发 channelWritabilityChanged 事件。\n\n> 当 channel 处于 disConnected 状态时，用户可以进行 write 操作但不能进行 flush 操作。\n\n * !isActive() && !isOpen() ：说明当前 channel 处于关闭状态，这时通知给用户 channelPromise 的异常类型为 newClosedChannelException ，因为 channel 已经关闭，所以这里并不会触发 channelWritabilityChanged 事件。\n * 当 channel 的这些异常状态校验通过之后，则调用 doWrite 方法将 ChannelOutboundBuffer 中的待发送数据写进底层 Socket 中。\n\n# 4.2.2.1 ChannelOutboundBuffer#failFlushed\n\npublic final class ChannelOutboundBuffer {\n\n    private boolean inFail;\n\n    void failFlushed(Throwable cause, boolean notify) {\n        if (inFail) {\n            return;\n        }\n\n        try {\n            inFail = true;\n            for (;;) {\n                if (!remove0(cause, notify)) {\n                    break;\n                }\n            }\n        } finally {\n            inFail = false;\n        }\n    }\n}\n\n\n该方法用于在 Netty 在发送数据的时候，如果发现当前 channel 处于非活跃状态，则将 ChannelOutboundBuffer 中 flushedEntry 与tailEntry 之间的 Entry 对象节点全部删除，并释放发送数据占用的内存空间，同时回收 Entry 对象实例。\n\n# 4.2.2.2 ChannelOutboundBuffer#remove0\n\nprivate boolean remove0(Throwable cause, boolean notifyWritability) {\n    Entry e = flushedEntry;\n    if (e == null) {\n        //清空当前reactor线程缓存的所有待发送数据\n        clearNioBuffers();\n        return false;\n    }\n    Object msg = e.msg;\n\n    ChannelPromise promise = e.promise;\n    int size = e.pendingSize;\n    //从channelOutboundBuffer中删除该Entry节点\n    removeEntry(e);\n\n    if (!e.cancelled) {\n        // only release message, fail and decrement if it was not canceled before.\n        //释放msg所占用的内存空间\n        ReferenceCountUtil.safeRelease(msg);\n        //编辑promise发送失败，并通知相应的Lisener\n        safeFail(promise, cause);\n        //由于msg得到释放，所以需要降低channelOutboundBuffer中的内存占用水位线，并根据notifyWritability决定是否触发ChannelWritabilityChanged事件\n        decrementPendingOutboundBytes(size, false, notifyWritability);\n    }\n\n    // recycle the entry\n    //回收Entry实例对象\n    e.recycle();\n\n    return true;\n}\n\n\n当一个 Entry 节点需要从 ChannelOutboundBuffer 中清除时，Netty 需要释放该 Entry 节点中包裹的发送数据 msg 所占用的内存空间。并标记对应的 promise 为失败同时通知对应的 listener ，由于 msg 得到释放，所以需要降低 channelOutboundBuffer 中的内存占用水位线，并根据 boolean notifyWritability 决定是否触发 ChannelWritabilityChanged 事件。最后需要将该 Entry 实例回收至 Recycler 对象池中。\n\n\n# 5. 终于开始真正地发送数据了！\n\n来到这里我们就真正进入到了 Netty 发送数据的核心处理逻辑，在?《Netty如何高效接收网络数据》一文中，笔者详细介绍了 Netty 读取数据的核心流程，Netty 会在一个 read loop 中不断循环读取 Socket 中的数据直到数据读取完毕或者读取次数已满 16 次，当循环读取了 16 次还没有读取完毕时，Netty 就不能在继续读了，因为 Netty 要保证 Reactor 线程可以均匀的处理注册在它上边的所有 Channel 中的 IO 事件。剩下未读取的数据等到下一次 read loop 在开始读取。\n\n除此之外，在每次 read loop 开始之前，Netty 都会分配一个初始化大小为 2048 的 DirectByteBuffer 来装载从 Socket 中读取到的数据，当整个 read loop 结束时，会根据本次读取数据的总量来判断是否为该 DirectByteBuffer 进行扩容或者缩容，目的是在下一次 read loop 的时候可以为其分配一个容量大小合适的 DirectByteBuffer 。\n\n其实 Netty 对发送数据的处理和对读取数据的处理核心逻辑都是一样的，这里大家可以将这两篇文章结合对比着看。\n\n但发送数据的细节会多一些，也会更复杂一些，由于这块逻辑整体稍微比较复杂，所以我们接下来还是分模块进行解析：\n\n\n# 5.1 发送数据前的准备工作\n\n@Override\nprotected void doWrite(ChannelOutboundBuffer in) throws Exception {\n    //获取NioSocketChannel中封装的jdk nio底层socketChannel\n    SocketChannel ch = javaChannel();\n    //最大写入次数 默认为16 目的是为了保证SubReactor可以平均的处理注册其上的所有Channel\n    int writeSpinCount = config().getWriteSpinCount();\n    do {\n        if (in.isEmpty()) {\n            // 如果全部数据已经写完 则移除OP_WRITE事件并直接退出writeLoop\n            clearOpWrite();             \n            return;\n        }\n\n        //  SO_SNDBUF设置的发送缓冲区大小 * 2 作为 最大写入字节数  293976 = 146988 << 1\n        int maxBytesPerGatheringWrite = ((NioSocketChannelConfig) config).getMaxBytesPerGatheringWrite();\n        // 将ChannelOutboundBuffer中缓存的DirectBuffer转换成JDK NIO 的 ByteBuffer\n        ByteBuffer[] nioBuffers = in.nioBuffers(1024, maxBytesPerGatheringWrite);\n        // ChannelOutboundBuffer中总共的DirectBuffer数\n        int nioBufferCnt = in.nioBufferCount();\n\n        switch (nioBufferCnt) {\n                .........向底层jdk nio socketChannel发送数据.........\n        }\n    } while (writeSpinCount > 0);\n\n    ............处理本轮write loop未写完的情况.......\n}\n\n\n这部分内容为 Netty 开始发送数据之前的准备工作：\n\n# 5.1.1 获取write loop最大发送循环次数\n\n从当前 NioSocketChannel 的配置类 NioSocketChannelConfig 中获取 write loop 最大循环写入次数，默认为 16。但也可以通过下面的方式进行自定义设置。\n\nServerBootstrap b = new ServerBootstrap();\nb.group(bossGroup, workerGroup)\n    .......\n    .childOption(ChannelOption.WRITE_SPIN_COUNT,自定义数值)\n\n\n# 5.1.2 处理在一轮write loop中就发送完数据的情况\n\n进入 write loop 之后首先需要判断当前 ChannelOutboundBuffer 中的数据是否已经写完了 in.isEmpty()) ，如果全部写完就需要清除当前 Channel 在 Reactor 上注册的 OP_WRITE 事件。\n\n> 这里大家可能会有疑问，目前我们还没有注册 OP_WRITE 事件到 Reactor 上，为啥要清除呢？别着急，笔者会在后面为大家揭晓答案。\n\n# 5.1.3 获取本次write loop 最大允许发送字节数\n\n从 ChannelConfig 中获取本次 write loop 最大允许发送的字节数 maxBytesPerGatheringWrite 。初始值为 SO_SNDBUF大小 * 2 = 293976 = 146988 << 1，最小值为 2048。\n\nprivate final class NioSocketChannelConfig extends DefaultSocketChannelConfig {\n    //293976 = 146988 << 1\n    //SO_SNDBUF设置的发送缓冲区大小 * 2 作为 最大写入字节数\n    //最小值为2048 \n    private volatile int maxBytesPerGatheringWrite = Integer.MAX_VALUE;\n    private NioSocketChannelConfig(NioSocketChannel channel, Socket javaSocket) {\n        super(channel, javaSocket);\n        calculateMaxBytesPerGatheringWrite();\n    }\n\n    private void calculateMaxBytesPerGatheringWrite() {\n        // 293976 = 146988 << 1\n        // SO_SNDBUF设置的发送缓冲区大小 * 2 作为 最大写入字节数\n        int newSendBufferSize = getSendBufferSize() << 1;\n        if (newSendBufferSize > 0) {\n            setMaxBytesPerGatheringWrite(newSendBufferSize);\n        }\n    }\n}\n\n\n我们可以通过如下的方式自定义配置 Socket 发送缓冲区大小。\n\nServerBootstrap b = new ServerBootstrap();\nb.group(bossGroup, workerGroup)\n    .......\n    .childOption(ChannelOption.SO_SNDBUF,自定义数值)\n\n\n# 5.1.4 将待发送数据转换成 JDK NIO ByteBuffer\n\n由于最终 Netty 会调用 JDK NIO 的 SocketChannel 发送数据，所以这里需要首先将当前 Channel 中的写缓冲队列 ChannelOutboundBuffer 里存储的 DirectByteBuffer（ Netty 中的 ByteBuffer 实现）转换成 JDK NIO 的 ByteBuffer 类型。最终将转换后的待发送数据存储在 ByteBuffer[] nioBuffers 数组中。这里通过调用 ChannelOutboundBuffer#nioBuffers 方法完成以上 ByteBuffer 类型的转换。\n\n * maxBytesPerGatheringWrite：表示本次 write loop 中最多从 ChannelOutboundBuffer 中转换 maxBytesPerGatheringWrite 个字节出来。也就是本次 write loop 最多能发送多少字节。\n * 1024: 本次 write loop 最多转换 1024 个 ByteBuffer（ JDK NIO 实现）。也就是说本次 write loop 最多批量发送多少个 ByteBuffer 。\n\n通过 ChannelOutboundBuffer#nioBufferCount() 获取本次 write loop 总共需要发送的 ByteBuffer 数量 nioBufferCnt 。注意这里已经变成了 JDK NIO 实现的 ByteBuffer 了。\n\n> 详细的 ByteBuffer 类型转换过程，笔者会在专门讲解 Buffer 设计的时候为大家全面细致地讲解，这里我们还是主要聚焦于发送数据流程的主线。\n\n当做完这些发送前的准备工作之后，接下来 Netty 就开始向 JDK NIO SocketChannel 发送这些已经转换好的 JDK NIO ByteBuffer 了。\n\n\n# 5.2 向JDK NIO SocketChannel发送数据\n\nflush流程.png\n\n@Override\nprotected void doWrite(ChannelOutboundBuffer in) throws Exception {      \n    SocketChannel ch = javaChannel();\n    int writeSpinCount = config().getWriteSpinCount();\n    do {\n\n        .........将待发送数据转换到JDK NIO ByteBuffer中.........\n\n            //本次write loop中需要发送的 JDK ByteBuffer个数\n            int nioBufferCnt = in.nioBufferCount();\n\n        switch (nioBufferCnt) {\n            case 0:\n                //这里主要是针对 网络传输文件数据 的处理 FileRegion                 \n                writeSpinCount -= doWrite0(in);\n                break;\n            case 1: {\n                .........处理单个NioByteBuffer发送的情况......\n                    break;\n            }\n            default: {\n                .........批量处理多个NioByteBuffers发送的情况......\n                    break;\n            }            \n        }\n    } while (writeSpinCount > 0);\n\n    ............处理本轮write loop未写完的情况.......\n}\n\n\n这里大家可能对 nioBufferCnt == 0 的情况比较有疑惑，明明之前已经校验过ChannelOutboundBuffer 不为空了，为什么这里从 ChannelOutboundBuffer 中获取到的 nioBuffer 个数依然为 0 呢？\n\n在前边我们介绍 Netty 对 write 事件的处理过程时提过， ChannelOutboundBuffer 中只支持 ByteBuf 类型和 FileRegion 类型，其中 ByteBuf 类型用于装载普通的发送数据，而 FileRegion 类型用于通过零拷贝的方式网络传输文件。\n\n而这里 ChannelOutboundBuffer 虽然不为空，但是装载的 NioByteBuffer 个数却为 0 说明 ChannelOutboundBuffer 中装载的是 FileRegion 类型，当前正在进行网络文件的传输。\n\ncase 0 的分支主要就是用于处理网络文件传输的情况。\n\n# 5.2.1 零拷贝发送网络文件\n\nprotected final int doWrite0(ChannelOutboundBuffer in) throws Exception {\n    Object msg = in.current();\n    if (msg == null) {\n        return 0;\n    }\n    return doWriteInternal(in, in.current());\n}\n\n\n这里需要特别注意的是用于文件传输的方法 doWriteInternal 中的返回值，理解这些返回值的具体情况有助于我们理解后面 write loop 的逻辑走向。\n\nprivate int doWriteInternal(ChannelOutboundBuffer in, Object msg) throws Exception {\n\n    if (msg instanceof ByteBuf) {\n\n        ..............忽略............\n\n    } else if (msg instanceof FileRegion) {\n        FileRegion region = (FileRegion) msg;\n        //文件已经传输完毕\n        if (region.transferred() >= region.count()) {\n            in.remove();\n            return 0;\n        }\n\n        //零拷贝的方式传输文件\n        long localFlushedAmount = doWriteFileRegion(region);\n        if (localFlushedAmount > 0) {\n            in.progress(localFlushedAmount);\n            if (region.transferred() >= region.count()) {\n                in.remove();\n            }\n            return 1;\n        }\n    } else {\n        // Should not reach here.\n        throw new Error();\n    }\n    //走到这里表示 此时Socket已经写不进去了 退出writeLoop，注册OP_WRITE事件\n    return WRITE_STATUS_SNDBUF_FULL;\n}\n\n\n最终会在 doWriteFileRegion 方法中通过 FileChannel#transferTo 方法底层用到的系统调用为 sendFile 实现零拷贝网络文件的传输。\n\npublic class NioSocketChannel extends AbstractNioByteChannel implements io.netty.channel.socket.SocketChannel {\n\n   @Override\n    protected long doWriteFileRegion(FileRegion region) throws Exception {\n        final long position = region.transferred();\n        return region.transferTo(javaChannel(), position);\n    }\n\n}\n\n\n> 关于 Netty 中涉及到的零拷贝，笔者会有一篇专门的文章为大家讲解，本文的主题我们还是先聚焦于把发送流程的主线打通。\n\n我们继续回到发送数据流程主线上来~~\n\n                case 0:\n                    //这里主要是针对 网络传输文件数据 的处理 FileRegion                 \n                    writeSpinCount -= doWrite0(in);\n                    break;\n\n\n * region.transferred() >= region.count() ：表示当前 FileRegion 中的文件数据已经传输完毕。那么在这种情况下本次 write loop 没有写入任何数据到 Socket ，所以返回 0 ，writeSpinCount - 0 意思就是本次 write loop 不算，继续循环。\n * localFlushedAmount > 0 ：表示本 write loop 中写入了一些数据到 Socket 中，会有返回 1，writeSpinCount - 1 减少一次 write loop 次数。\n * localFlushedAmount <= 0 ：表示当前 Socket 发送缓冲区已满，无法写入数据，那么就返回 WRITE_STATUS_SNDBUF_FULL = Integer.MAX_VALUE。writeSpinCount - Integer.MAX_VALUE 必然是负数，直接退出循环，向 Reactor 注册 OP_WRITE 事件并退出 flush 流程。等 Socket 发送缓冲区可写了，Reactor 会通知 channel 继续发送文件数据。记住这里，我们后面还会提到。\n\n# 5.2.2 发送普通数据\n\n剩下两个 case 1 和 default 分支主要就是处理 ByteBuffer 装载的普通数据发送逻辑。\n\n其中 case 1 表示当前 Channel 的 ChannelOutboundBuffer 中只包含了一个 NioByteBuffer 的情况。\n\ndefault 表示当前 Channel 的 ChannelOutboundBuffer 中包含了多个 NioByteBuffers 的情况。\n\n@Override\nprotected void doWrite(ChannelOutboundBuffer in) throws Exception {      \n    SocketChannel ch = javaChannel();\n    int writeSpinCount = config().getWriteSpinCount();\n    do {\n\n        .........将待发送数据转换到JDK NIO ByteBuffer中.........\n\n            //本次write loop中需要发送的 JDK ByteBuffer个数\n            int nioBufferCnt = in.nioBufferCount();\n\n        switch (nioBufferCnt) {\n            case 0:\n                ..........处理网络文件传输.........\n                    case 1: {\n                        ByteBuffer buffer = nioBuffers[0];\n                        int attemptedBytes = buffer.remaining();\n                        final int localWrittenBytes = ch.write(buffer);\n                        if (localWrittenBytes <= 0) {\n                            //如果当前Socket发送缓冲区满了写不进去了，则注册OP_WRITE事件，等待Socket发送缓冲区可写时 在写\n                            // SubReactor在处理OP_WRITE事件时，直接调用flush方法\n                            incompleteWrite(true);\n                            return;\n                        }\n                        //根据当前实际写入情况调整 maxBytesPerGatheringWrite数值\n                        adjustMaxBytesPerGatheringWrite(attemptedBytes, localWrittenBytes, maxBytesPerGatheringWrite);\n                        //如果ChannelOutboundBuffer中的某个Entry被全部写入 则删除该Entry\n                        // 如果Entry被写入了一部分 还有一部分未写入  则更新Entry中的readIndex 等待下次writeLoop继续写入\n                        in.removeBytes(localWrittenBytes);\n                        --writeSpinCount;\n                        break;\n                    }\n            default: {\n                // ChannelOutboundBuffer中总共待写入数据的字节数\n                long attemptedBytes = in.nioBufferSize();\n                //批量写入\n                final long localWrittenBytes = ch.write(nioBuffers, 0, nioBufferCnt);\n                if (localWrittenBytes <= 0) {\n                    incompleteWrite(true);\n                    return;\n                }\n                //根据实际写入情况调整一次写入数据大小的最大值\n                // maxBytesPerGatheringWrite决定每次可以从channelOutboundBuffer中获取多少发送数据\n                adjustMaxBytesPerGatheringWrite((int) attemptedBytes, (int) localWrittenBytes,\n                                                maxBytesPerGatheringWrite);\n                //移除全部写完的BUffer，如果只写了部分数据则更新buffer的readerIndex，下一个writeLoop写入\n                in.removeBytes(localWrittenBytes);\n                --writeSpinCount;\n                break;\n            }            \n        }\n    } while (writeSpinCount > 0);\n\n    ............处理本轮write loop未写完的情况.......\n}\n\n\ncase 1 和 default 这两个分支在处理发送数据时的逻辑是一样的，唯一的区别就是 case 1 是处理单个 NioByteBuffer 的发送，而 default 分支是批量处理多个 NioByteBuffers 的发送。\n\n下面笔者就以经常被触发到的 default 分支为例来为大家讲述 Netty 在处理数据发送时的逻辑细节：\n\n 1. 首先从当前 NioSocketChannel 中的 ChannelOutboundBuffer 中获取本次 write loop 需要发送的字节总量 attemptedBytes 。这个 nioBufferSize 是在前边介绍 ChannelOutboundBuffer#nioBuffers 方法转换 JDK NIO ByteBuffer 类型时被计算出来的。\n 2. 调用 JDK NIO 原生 SocketChannel 批量发送 nioBuffers 中的数据。并获取到本次 write loop 一共批量发送了多少字节 localWrittenBytes 。\n\n    /**\n     * @throws  NotYetConnectedException\n     *          If this channel is not yet connected\n     */\n    public abstract long write(ByteBuffer[] srcs, int offset, int length)\n        throws IOException;\n\n\n 1. localWrittenBytes <= 0 表示当前 Socket 的写缓存区 SEND_BUF 已满，写不进数据了。那么就需要向当前 NioSocketChannel 对应的 Reactor 注册 OP_WRITE 事件，并停止当前 flush 流程。当 Socket 的写缓冲区有容量可写时，epoll 会通知 reactor 线程继续写入。\n\n    protected final void incompleteWrite(boolean setOpWrite) {\n        // Did not write completely.\n        if (setOpWrite) {\n            //这里处理还没写满16次 但是socket缓冲区已满写不进去的情况 注册write事件\n            //什么时候socket可写了， epoll会通知reactor线程继续写\n            setOpWrite();\n        } else {\n              ...........目前还不需要关注这里.......\n        }\n    }\n\n\n向 Reactor 注册 OP_WRITE 事件：\n\n    protected final void setOpWrite() {\n        final SelectionKey key = selectionKey();\n        if (!key.isValid()) {\n            return;\n        }\n        final int interestOps = key.interestOps();\n        if ((interestOps & SelectionKey.OP_WRITE) == 0) {\n            key.interestOps(interestOps | SelectionKey.OP_WRITE);\n        }\n    }\n\n\n> 关于通过位运算来向 IO 事件集合 interestOps 添加监听 IO 事件的用法，在前边的文章中，笔者已经多次介绍过了，这里不再重复。\n\n 1. 根据本次 write loop 向 Socket 写缓冲区写入数据的情况，来调整下次 write loop 最大写入字节数。maxBytesPerGatheringWrite 决定每次 write loop 可以从 channelOutboundBuffer 中最多获取多少发送数据。初始值为 SO_SNDBUF大小 * 2 = 293976 = 146988 << 1，最小值为 2048。\n\n    public static final int MAX_BYTES_PER_GATHERING_WRITE_ATTEMPTED_LOW_THRESHOLD = 4096;\n\n    private void adjustMaxBytesPerGatheringWrite(int attempted, int written, int oldMaxBytesPerGatheringWrite) {\n        if (attempted == written) {\n            if (attempted << 1 > oldMaxBytesPerGatheringWrite) {\n                ((NioSocketChannelConfig) config).setMaxBytesPerGatheringWrite(attempted << 1);\n            }\n        } else if (attempted > MAX_BYTES_PER_GATHERING_WRITE_ATTEMPTED_LOW_THRESHOLD && written < attempted >>> 1) {\n            ((NioSocketChannelConfig) config).setMaxBytesPerGatheringWrite(attempted >>> 1);\n        }\n    }\n\n\n> 由于操作系统会动态调整 SO_SNDBUF 的大小，所以这里 netty 也需要根据操作系统的动态调整做出相应的调整，目的是尽量多的去写入数据。\n\nattempted == written 表示本次 write loop 尝试写入的数据能全部写入到 Socket 的写缓冲区中，那么下次 write loop 就应该尝试去写入更多的数据。\n\n那么这里的更多具体是多少呢？\n\nNetty 会将本次写入的数据量 written 扩大两倍，如果扩大两倍后的写入量大于本次 write loop 的最大限制写入量 maxBytesPerGatheringWrite，说明用户的写入需求很猛烈，Netty当然要满足这样的猛烈需求，那么就将当前 NioSocketChannelConfig 中的 maxBytesPerGatheringWrite 更新为本次 write loop 两倍的写入量大小。\n\n在下次 write loop 写入数据的时候，就会尝试从 ChannelOutboundBuffer 中加载最多 written * 2 大小的字节数。\n\n如果扩大两倍后的写入量依然小于等于本次 write loop 的最大限制写入量 maxBytesPerGatheringWrite，说明用户的写入需求还不是很猛烈，Netty 继续维持本次 maxBytesPerGatheringWrite 数值不变。\n\n如果本次写入的数据还不及尝试写入数据的 1 / 2 ：written < attempted >>> 1。说明当前 Socket 写缓冲区的可写容量不是很多了，下一次 write loop 就不要写这么多了尝试减少下次写入的量将下次 write loop 要写入的数据减小为 attempted 的1 / 2。当然也不能无限制的减小，最小值不能低于 2048。\n\n> 这里可以结合笔者前边的文章?《一文聊透ByteBuffer动态自适应扩缩容机制》中介绍到的 read loop 场景中的扩缩容一起对比着看。\n\n> read loop 中的扩缩容触发时机是在一个完整的 read loop 结束时候触发。而 write loop 中扩缩容的触发时机是在每次 write loop 发送完数据后，立即触发扩缩容判断。\n\n 1. 当本次 write loop 批量发送完 ChannelOutboundBuffer 中的数据之后，最后调用in.removeBytes(localWrittenBytes) 从 ChannelOutboundBuffer 中移除全部写完的 Entry ，如果只发送了 Entry 的部分数据则更新 Entry 对象中封装的 DirectByteBuffer 的 readerIndex，等待下一次 write loop 写入。\n\n到这里，write loop 中的发送数据的逻辑就介绍完了，接下来 Netty 会在 write loop 中循环地发送数据直到写满 16 次或者数据发送完毕。\n\n还有一种退出 write loop 的情况就是当 Socket 中的写缓冲区满了，无法在写入时。Netty 会退出 write loop 并向 reactor 注册 OP_WRITE 事件。\n\n但这其中还隐藏着一种情况就是如果 write loop 已经写满 16 次但还没写完数据并且此时 Socket 写缓冲区还没有满，还可以继续在写。那 Netty 会如何处理这种情况呢？\n\n\n# 6. 处理Socket可写但已经写满16次还没写完的情况\n\n    @Override\n    protected void doWrite(ChannelOutboundBuffer in) throws Exception {      \n        SocketChannel ch = javaChannel();\n        int writeSpinCount = config().getWriteSpinCount();\n        do {\n  \n            .........将待发送数据转换到JDK NIO ByteBuffer中.........\n\n            int nioBufferCnt = in.nioBufferCount();\n\n            switch (nioBufferCnt) {\n                case 0:\n                    //这里主要是针对 网络传输文件数据 的处理 FileRegion                 \n                    writeSpinCount -= doWrite0(in);\n                    break;\n                case 1: {\n                      .....发送单个nioBuffer....\n                }\n                default: {\n                      .....批量发送多个nioBuffers......\n                }            \n            }\n        } while (writeSpinCount > 0);\n        \n        //处理write loop结束 但数据还没写完的情况\n        incompleteWrite(writeSpinCount < 0);\n    }\n\n\n当 write loop 结束后，这时 writeSpinCount 的值会有两种情况：\n\n * writeSpinCount < 0：这种情况有点不好理解，我们在介绍 Netty 通过零拷贝的方式传输网络文件也就是这里的 case 0 分支逻辑时，详细介绍了 doWrite0 方法的几种返回值，当 Netty 在传输文件的过程中发现 Socket 缓冲区已满无法在继续写入数据时，会返回 WRITE_STATUS_SNDBUF_FULL = Integer.MAX_VALUE，这就使得 writeSpinCount的值 < 0。随后 break 掉 write loop 来到 incompleteWrite(writeSpinCount < 0) 方法中，最后会在 incompleteWrite 方法中向 reactor 注册 OP_WRITE 事件。当 Socket 缓冲区变得可写时，epoll 会通知 reactor 线程继续发送文件。\n\n    protected final void incompleteWrite(boolean setOpWrite) {\n        // Did not write completely.\n        if (setOpWrite) {\n            //这里处理还没写满16次 但是socket缓冲区已满写不进去的情况 注册write事件\n            // 什么时候socket可写了， epoll会通知reactor线程继续写\n            setOpWrite();\n        } else {\n            ..............\n        }\n    }\n\n\n * writeSpinCount == 0：这种情况很好理解，就是已经写满了 16 次，但是还没写完，同时 Socket 的写缓冲区未满，还可以继续写入。这种情况下即使 Socket 还可以继续写入，Netty 也不会再去写了，因为执行 flush 操作的是 reactor 线程，而 reactor 线程负责执行注册在它上边的所有 channel 的 IO 操作，Netty 不会允许 reactor 线程一直在一个 channel 上执行 IO 操作，reactor 线程的执行时间需要均匀的分配到每个 channel 上。所以这里 Netty 会停下，转而去处理其他 channel 上的 IO 事件。\n\n那么还没写完的数据，Netty会如何处理呢？\n\n    protected final void incompleteWrite(boolean setOpWrite) {\n        // Did not write completely.\n        if (setOpWrite) {\n            //这里处理还没写满16次 但是socket缓冲区已满写不进去的情况 注册write事件\n            // 什么时候socket可写了， epoll会通知reactor线程继续写\n            setOpWrite();\n        } else {\n            //这里处理的是socket缓冲区依然可写，但是写了16次还没写完，这时就不能在写了，reactor线程需要处理其他channel上的io事件\n\n            //因为此时socket是可写的，必须清除op_write事件，否则会一直不停地被通知\n            clearOpWrite();\n            //如果本次writeLoop还没写完，则提交flushTask到reactor           \n            eventLoop().execute(flushTask);\n\n        }\n\n\n这个方法的 if 分支逻辑，我们在介绍do {.....}while()循环体 write loop 中发送逻辑时已经提过，在 write loop 循环发送数据的过程中，如果发现 Socket 缓冲区已满，无法写入数据时（ localWrittenBytes <= 0），则需要向 reactor 注册 OP_WRITE 事件，等到 Socket 缓冲区变为可写状态时，epoll 会通知 reactor 线程继续写入剩下的数据。\n\n       do {\n            .........将待发送数据转换到JDK NIO ByteBuffer中.........\n\n            int nioBufferCnt = in.nioBufferCount();\n\n            switch (nioBufferCnt) {\n                case 0:\n                    writeSpinCount -= doWrite0(in);\n                    break;\n                case 1: {\n                    .....发送单个nioBuffer....\n                    final int localWrittenBytes = ch.write(buffer);\n                    if (localWrittenBytes <= 0) {\n                        incompleteWrite(true);\n                        return;\n                    }\n                    .................省略..............\n                    break;\n                }\n                default: {\n                    .....批量发送多个nioBuffers......\n                    final long localWrittenBytes = ch.write(nioBuffers, 0, nioBufferCnt);\n                    if (localWrittenBytes <= 0) {\n                        incompleteWrite(true);\n                        return;\n                    }\n                    .................省略..............\n                    break;\n                }\n            }\n        } while (writeSpinCount > 0);\n\n\n> 注意 if 分支处理的情况是还没写满 16 次，但是 Socket 缓冲区已满，无法写入的情况。\n\n而 else 分支正是处理我们这里正在讨论的情况即 Socket 缓冲区是可写的，但是已经写满 16 次，在本轮 write loop 中不能再继续写入的情况。\n\n这时 Netty 会将 channel 中剩下的待写数据的 flush 操作封装程 flushTask，丢进 reactor 的普通任务队列中，等待 reactor 执行完其他 channel 上的 io 操作后在回过头来执行未写完的 flush 任务。\n\n> 忘记 Reactor 整体运行逻辑的同学，可以在回看下笔者的这篇文章?《一文聊透Netty核心引擎Reactor的运转架构》\n\n    private final Runnable flushTask = new Runnable() {\n        @Override\n        public void run() {\n            ((AbstractNioUnsafe) unsafe()).flush0();\n        }\n    };\n\n\n这里我们看到 flushTask 中的任务是直接再次调用 flush0 继续回到发送数据的逻辑流程中。\n\n细心的同学可能会有疑问，为什么这里不在继续注册 OP_WRITE 事件而是通过向 reactor 提交一个 flushTask 来完成 channel 中剩下数据的写入呢？\n\n原因是这里我们讲的 else 分支是用来处理 Socket 缓冲区未满还是可写的，但是由于用户本次要发送的数据太多，导致写了 16 次还没写完的情形。\n\n既然当前 Socket 缓冲区是可写的，我们就不能注册 OP_WRITE 事件，否则这里一直会不停地收到 epoll 的通知。因为 JDK NIO Selector 默认的是 epoll 的水平触发。\n\n> 忘记水平触发和边缘触发这两种 epoll 工作模式的同学，可以在回看下笔者的这篇文章?《聊聊Netty那些事儿之从内核角度看IO模型》\n\n所以这里只能向 reactor 提交 flushTask 来继续完成剩下数据的写入，而不能注册 OP_WRITE 事件。\n\n> 注意：只有当 Socket 缓冲区已满导致无法写入时，Netty 才会去注册 OP_WRITE 事件。这和我们之前介绍的 OP_ACCEPT 事件和 OP_READ 事件的注册时机是不同的。\n\n这里大家可能还会有另一个疑问，就是为什么在向 reactor 提交 flushTask 之前需要清理 OP_WRITE 事件呢？ 我们并没有注册 OP_WRITE 事件呀~~\n\n    protected final void incompleteWrite(boolean setOpWrite) {\n        if (setOpWrite) {\n            ......省略......\n        } else {\n            clearOpWrite();  \n            eventLoop().execute(flushTask);\n        }\n\n\n在为大家解答这个疑问之前，笔者先为大家介绍下 Netty 是如何处理 OP_WRITE 事件的，当大家明白了 OP_WRITE 事件的处理逻辑后，这个疑问就自然解开了。\n\n\n# 7. OP_WRITE事件的处理\n\n在?《一文聊透Netty核心引擎Reactor的运转架构》一文中，我们介绍过，当 Reactor 监听到 channel 上有 IO 事件发生后，最终会在 processSelectedKey 方法中处理 channel 上的 IO 事件，其中 OP_ACCEPT 事件和 OP_READ 事件的处理过程，笔者已经在之前的系列文章中介绍过了，这里我们聚焦于 OP_WRITE 事件的处理。\n\npublic final class NioEventLoop extends SingleThreadEventLoop {\n\n   private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {\n        final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();\n\n        .............省略.......\n\n        try {\n            int readyOps = k.readyOps();\n\n            if ((readyOps & SelectionKey.OP_CONNECT) != 0) {\n                  ......处理connect事件......\n            }\n\n            if ((readyOps & SelectionKey.OP_WRITE) != 0) {\n                ch.unsafe().forceFlush();\n            }\n \n            if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {\n               ........处理accept和read事件.........\n            }\n        } catch (CancelledKeyException ignored) {\n            unsafe.close(unsafe.voidPromise());\n        }\n    }\n\n}\n\n\n这里我们看到当 OP_WRITE 事件发生后，Netty 直接调用 channel 的 forceFlush 方法。\n\n       @Override\n        public final void forceFlush() {\n            // directly call super.flush0() to force a flush now\n            super.flush0();\n        }\n\n\n其实 forceFlush 方法中并没有什么特殊的逻辑，直接调用 flush0 方法再次发起 flush 操作继续 channel 中剩下数据的写入。\n\n    @Override\n    protected void doWrite(ChannelOutboundBuffer in) throws Exception {      \n        SocketChannel ch = javaChannel();\n        int writeSpinCount = config().getWriteSpinCount();\n        do {\n            if (in.isEmpty()) {\n                clearOpWrite();\n                return;\n            }\n            .........将待发送数据转换到JDK NIO ByteBuffer中.........\n\n            int nioBufferCnt = in.nioBufferCount();\n\n            switch (nioBufferCnt) {\n                case 0:\n                      ......传输网络文件........\n                case 1: {\n                      .....发送单个nioBuffer....\n                }\n                default: {\n                      .....批量发送多个nioBuffers......\n                }            \n            }\n        } while (writeSpinCount > 0);\n        \n        //处理write loop结束 但数据还没写完的情况\n        incompleteWrite(writeSpinCount < 0);\n    }\n\n\n注意这里的 clearOpWrite() 方法，由于 channel 上的 OP_WRITE 事件就绪，表明此时 Socket 缓冲区变为可写状态，从而 Reactor 线程再次来到了 flush 流程中。\n\n当 ChannelOutboundBuffer 中的数据全部写完后 in.isEmpty() ，就需要清理 OP_WRITE 事件，因为此时 Socket 缓冲区是可写的，这种情况下当数据全部写完后，就需要取消对 OP_WRITE 事件的监听，否则 epoll 会不断的通知 Reactor。\n\n同理在 incompleteWrite 方法的 else 分支也需要执行 clearOpWrite() 方法取消对 OP_WRITE 事件的监听。\n\n    protected final void incompleteWrite(boolean setOpWrite) {\n\n        if (setOpWrite) {\n            // 这里处理还没写满16次 但是socket缓冲区已满写不进去的情况 注册write事件\n            // 什么时候socket可写了， epoll会通知reactor线程继续写\n            setOpWrite();\n        } else {\n            // 必须清除OP_WRITE事件，此时Socket对应的缓冲区依然是可写的，只不过当前channel写够了16次，被SubReactor限制了。\n            // 这样SubReactor可以腾出手来处理其他channel上的IO事件。这里如果不清除OP_WRITE事件，则会一直被通知。\n            clearOpWrite();\n\n            //如果本次writeLoop还没写完，则提交flushTask到SubReactor\n            //释放SubReactor让其可以继续处理其他Channel上的IO事件\n            eventLoop().execute(flushTask);\n        }\n    }\n\n\n\n# 8. writeAndFlush\n\n在我们讲完了 write 事件和 flush 事件的处理过程之后，writeAndFlush 就变得很简单了，它就是把 write 和 flush 流程结合起来，先触发 write 事件然后在触发 flush 事件。\n\n下面我们来看下 writeAndFlush 的具体逻辑处理：\n\npublic class EchoServerHandler extends ChannelInboundHandlerAdapter {\n\n    @Override\n    public void channelRead(final ChannelHandlerContext ctx, final Object msg) {\n        //此处的msg就是Netty在read loop中从NioSocketChannel中读取到ByteBuffer\n        ctx.writeAndFlush(msg);\n    }\n}\nabstract class AbstractChannelHandlerContext implements ChannelHandlerContext, ResourceLeakHint {\n\n    @Override\n    public ChannelFuture writeAndFlush(Object msg) {\n        return writeAndFlush(msg, newPromise());\n    }\n\n    @Override\n    public ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) {\n        write(msg, true, promise);\n        return promise;\n    }\n\n}\n\n\n这里可以看到 writeAndFlush 方法的处理入口和 write 事件的处理入口是一样的。唯一不同的是入口处理函数 write 方法的 boolean flush 入参不同，在 writeAndFlush 的处理中 flush = true。\n\n    private void write(Object msg, boolean flush, ChannelPromise promise) {\n        ObjectUtil.checkNotNull(msg, \"msg\");\n\n        ................省略检查promise的有效性...............\n\n        //flush = true 表示channelHandler中调用的是writeAndFlush方法，这里需要找到pipeline中覆盖write或者flush方法的channelHandler\n        //flush = false 表示调用的是write方法，只需要找到pipeline中覆盖write方法的channelHandler\n        final AbstractChannelHandlerContext next = findContextOutbound(flush ?\n                (MASK_WRITE | MASK_FLUSH) : MASK_WRITE);\n        //用于检查内存泄露\n        final Object m = pipeline.touch(msg, next);\n        //获取下一个要被执行的channelHandler的executor\n        EventExecutor executor = next.executor();\n        //确保OutBound事件由ChannelHandler指定的executor执行\n        if (executor.inEventLoop()) {\n            //如果当前线程正是channelHandler指定的executor则直接执行\n            if (flush) {\n                next.invokeWriteAndFlush(m, promise);\n            } else {\n                next.invokeWrite(m, promise);\n            }\n        } else {\n            //如果当前线程不是ChannelHandler指定的executor,则封装成异步任务提交给指定executor执行，注意这里的executor不一定是reactor线程。\n            final WriteTask task = WriteTask.newInstance(next, m, promise, flush);\n            if (!safeExecute(executor, task, promise, m, !flush)) {\n                task.cancel();\n            }\n        }\n    }\n\n\n由于在 writeAndFlush 流程的处理中，flush 标志被设置为 true，所以这里有两个地方会和 write 事件的处理有所不同。\n\n * findContextOutbound( MASK_WRITE | MASK_FLUSH )：这里在 pipeline 中向前查找的 ChanneOutboundHandler 需要实现 write 方法或者 flush 方法。这里需要注意的是 write 方法和 flush 方法只需要实现其中一个即可满足查找条件。因为一般我们自定义 ChannelOutboundHandler 时，都会继承 ChannelOutboundHandlerAdapter 类，而在 ChannelInboundHandlerAdapter 类中对于这些 outbound 事件都会有默认的实现。\n\npublic class ChannelOutboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelOutboundHandler {\n\n    @Skip\n    @Override\n    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception {\n        ctx.write(msg, promise);\n    }\n\n\n    @Skip\n    @Override\n    public void flush(ChannelHandlerContext ctx) throws Exception {\n        ctx.flush();\n    }\n\n}\n\n\n这样在后面传播 write 事件或者 flush 事件的时候，我们通过上面逻辑找出的 ChannelOutboundHandler 中可能只实现了一个 flush 方法或者 write 方法。不过这样没关系，如果这里在传播 outbound 事件的过程中，发现找出的 ChannelOutboundHandler 中并没有实现对应的 outbound 事件回调函数，那么就直接调用在 ChannelOutboundHandlerAdapter 中的默认实现。\n\n * 在向前传播 writeAndFlush 事件的时候会通过调用 ChannelHandlerContext 的 invokeWriteAndFlush 方法，先传播 write 事件 然后在传播 flush 事件。\n\nvoid invokeWriteAndFlush(Object msg, ChannelPromise promise) {\n    if (invokeHandler()) {\n        //向前传递write事件\n        invokeWrite0(msg, promise);\n        //向前传递flush事件\n        invokeFlush0();\n    } else {\n        writeAndFlush(msg, promise);\n    }\n}\n\nprivate void invokeWrite0(Object msg, ChannelPromise promise) {\n    try {\n        //调用当前ChannelHandler中的write方法\n        ((ChannelOutboundHandler) handler()).write(this, msg, promise);\n    } catch (Throwable t) {\n        notifyOutboundHandlerException(t, promise);\n    }\n}\n\nprivate void invokeFlush0() {\n    try {\n        ((ChannelOutboundHandler) handler()).flush(this);\n    } catch (Throwable t) {\n        invokeExceptionCaught(t);\n    }\n}\n\n\n这里我们看到了 writeAndFlush 的核心处理逻辑，首先向前传播 write 事件，经过 write 事件的流程处理后，最后向前传播 flush 事件。\n\n根据前边的介绍，这里在向前传播 write 事件的时候，可能查找出的 ChannelOutboundHandler 只是实现了 flush 方法，不过没关系，这里会直接调用 write 方法在 ChannelOutboundHandlerAdapter 父类中的默认实现。同理 flush 也是一样。\n\n----------------------------------------\n\n\n# 总结\n\n到这里，Netty 处理数据发送的整个完整流程，笔者就为大家详细地介绍完了，可以看到 Netty 在处理读取数据和处理发送数据的过程中，虽然核心逻辑都差不多，但是发送数据的过程明显细节比较多，而且更加复杂一些。\n\n这里笔者将读取数据和发送数据的不同之处总结如下几点供大家回忆对比：\n\n * 在每次 read loop 之前，会分配一个大小固定的 diretByteBuffer 用来装载读取数据。每轮 read loop 完全结束之后，才会决定是否对下一轮的读取过程分配的 directByteBuffer 进行扩容或者缩容。\n\n * 在每次 write loop 之前，都会获取本次 write loop 最大能够写入的字节数，根据这个最大写入字节数从 ChannelOutboundBuffer 中转换 JDK NIO ByteBuffer 。每次写入 Socket 之后都需要重新评估是否对这个最大写入字节数进行扩容或者缩容。\n\n * read loop 和 write loop 都被默认限定最多执行 16 次。\n\n * 在一个完整的 read loop 中，如果还读取不完数据，直接退出。等到 reactor 线程执行完其他 channel 上的 IO 事件再来读取未读完的数据。\n\n * 而在一个完整的 write loop 中，数据发送不完，则分两种情况。\n   \n   * Socket 缓冲区满无法在继续写入。这时就需要向 reactor 注册 OP_WRITE 事件。等 Socket 缓冲区变的可写时，epoll 通知 reactor 线程继续发送。\n   * Socket 缓冲区可写，但是由于发送数据太多，导致虽然写满 16 次但依然没有写完。这时就直接向 reactor 丢一个 flushTask 进去，等到 reactor 线程执行完其他 channel 上的 IO 事件，在回过头来执行 flushTask。\n\n * OP_READ 事件的注册是在 NioSocketChannel 被注册到对应的 Reactor 中时就会注册。而 OP_WRITE 事件只会在 Socket 缓冲区满的时候才会被注册。当 Socket 缓冲区再次变得可写时，要记得取消 OP_WRITE 事件的监听。否则的话就会一直被通知。\n\n好了，本文的全部内容就到这里了，我们下篇文章见~~~~\n\n\n# 参考资料\n\n一文搞懂Netty发送数据全流程 | 你想知道的细节全在这里 (qq.com)",normalizedContent:"# 前言\n\n\n\n在《netty如何高效接收网络数据》一文中，我们介绍了 netty 的 subreactor 处理网络数据读取的完整过程，当 netty 为我们读取了网络请求数据，并且我们在自己的业务线程中完成了业务处理后，就需要将业务处理结果返回给客户端了，那么本文我们就来介绍下 subreactor 如何处理网络数据发送的整个过程。\n\n我们都知道 netty 是一款高性能的异步事件驱动的网络通讯框架，既然是网络通讯框架那么它主要做的事情就是：\n\n * 接收客户端连接。\n * 读取连接上的网络请求数据。\n * 向连接发送网络响应数据。\n\n前边系列文章在介绍netty的启动以及接收连接的过程中，我们只看到 op_accept 事件以及 op_read 事件的注册，并未看到 op_write 事件的注册。\n\n * 那么在什么情况下 netty 才会向 subreactor 去注册 op_write 事件呢？\n * netty 又是怎么对写操作做到异步处理的呢？\n\n本文笔者将会为大家一一揭晓这些谜底。我们还是以之前的 echoserver 为例进行说明。\n\n@sharable\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(channelhandlercontext ctx, object msg) {\n        //此处的msg就是netty在read loop中从niosocketchannel中读取到的bytebuffer\n        ctx.write(msg);\n    }\n\n}\n\n\n我们将在《netty如何高效接收网络数据》一文中读取到的 bytebuffer (这里的 object msg)，直接发送回给客户端，用这个简单的例子来揭开 netty 如何发送数据的序幕~~\n\n> 在实际开发中，我们首先要通过解码器将读取到的 bytebuffer 解码转换为我们的业务 request 类，然后在业务线程中做业务处理，在通过编码器对业务 response 类编码为 bytebuffer ，最后利用 channelhandlercontext ctx 的引用发送响应数据。\n\n> 本文我们只聚焦 netty 写数据的过程，对于 netty 编解码相关的内容，笔者会在后续的文章中专门介绍。\n\n\n\n\n# 1. channelhandlercontext\n\n\n\n通过前面几篇文章的介绍，我们知道 netty 会为每个 channel 分配一个 pipeline ，pipeline 是一个双向链表的结构。netty 中产生的 io 异步事件会在这个 pipeline 中传播。\n\nnetty 中的 io 异步事件大体上分为两类：\n\n * inbound事件：入站事件，比如前边介绍的 channelactive 事件， channelread 事件，它们会从 pipeline 的头结点 headcontext 开始一直向后传播\n * outbound事件：出站事件，比如本文中即将要介绍到的 write事件 以及 flush 事件，出站事件会从相反的方向从后往前传播直到 headcontext 。最终会在 headcontext 中完成出站事件的处理。\n   * 本例中用到的 channelhandlercontext.write() 会使 write 事件从当前 channelhandler 也就是这里的 echoserverhandler 开始沿着 pipeline 向前传播\n   * 而 channelhandlercontext.channel().write() 则会使 write 事件从 pipeline 的尾结点 tailcontext 开始向前传播直到 headcontext\n\n\n\n而 pipeline 这样一个双向链表数据结构中的类型正是 channelhandlercontext ，由 channelhandlercontext 包裹我们自定义的 io 处理逻辑 channelhandler\n\n> channelhandler 并不需要感知到它所处的 pipeline 中的上下文信息，只需要专心处理好 io 逻辑即可，关于 pipeline 的上下文信息全部封装在 channelhandlercontext中。\n\nchannelhandler 在 netty 中的作用只是负责处理 io 逻辑，比如编码，解码。它并不会感知到它在 pipeline 中的位置，更不会感知和它相邻的两个 channelhandler。事实上 channelhandler也并不需要去关心这些，它唯一需要关注的就是处理所关心的异步事件\n\n而 channelhandlercontext 中维护了 pipeline 这个双向链表中的 pre 以及 next 指针，这样可以方便的找到与其相邻的 channelhandler ，并可以过滤出一些符合执行条件的 channelhandler。正如它的命名一样， channelhandlercontext 正是起到了维护 channelhandler 上下文的一个作用。而 netty 中的异步事件在 pipeline 中的传播靠的就是这个 channelhandlercontext 。\n\n> 这样设计就使得 channelhandlercontext 和 channelhandler 的职责单一，各司其职，具有高度的可扩展性。\n\n\n# 2. write 事件的传播\n\n我们无论是在业务线程或者是在 subreactor 线程中完成业务处理后，都需要通过 channelhandlercontext 的引用将 write事件在 pipeline 中进行传播。然后在 pipeline 中相应的 channelhandler 中监听 write 事件从而可以对 write 事件进行自定义编排处理（比如我们常用的编码器），最终传播到 headcontext 中执行发送数据的逻辑操作。\n\n前边也提到 netty 中有两个触发 write 事件传播的方法，它们的传播处理逻辑都是一样的，只不过它们在 pipeline 中的传播起点是不同的。\n\n * channelhandlercontext.write() 方法会从当前 channelhandler 开始在 pipeline 中向前传播 write 事件直到 headcontext。\n * channelhandlercontext.channel().write() 方法则会从 pipeline 的尾结点 tailcontext 开始在 pipeline 中向前传播 write 事件直到 headcontext 。\n\n\n\n在我们清楚了 write 事件的总体传播流程后，接下来就来看看在 write 事件传播的过程中netty为我们作了些什么？这里我们以 channelhandlercontext.write() 方法为例说明。\n\n\n# 3. write 方法发送数据\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)\n\nabstract class abstractchannelhandlercontext implements channelhandlercontext, resourceleakhint {\n\n    @override\n    public channelfuture write(object msg) {\n        return write(msg, newpromise());\n    }\n\n    @override\n    public channelfuture write(final object msg, final channelpromise promise) {\n        write(msg, false, promise);\n        return promise;\n    }\n\n}\n\n\n这里我们看到 netty 的写操作是一个异步操作，当我们在业务线程中调用 channelhandlercontext.write() 后，netty 会给我们返回一个 channelfuture，我们可以在这个 channelfutrue 中添加 channelfuturelistener ，这样当 netty 将我们要发送的数据发送到底层 socket 中时，netty 会通过 channelfuturelistener 通知我们写入结果。\n\n@override\npublic void channelread(final channelhandlercontext ctx, final object msg) {\n    //此处的msg就是netty在read loop中从niosocketchannel中读取到的bytebuffer\n    channelfuture future = ctx.write(msg);\n    future.addlistener(new channelfuturelistener() {\n        @override\n        public void operationcomplete(channelfuture future) throws exception {\n            throwable cause = future.cause();\n            if (cause != null) {\n                处理异常情况\n            } else {                    \n                写入socket成功后，netty会通知到这里\n            }\n        }\n    });\n}\n\n\n当异步事件在 pipeline 传播的过程中发生异常时，异步事件就会停止在 pipeline 中传播。所以我们在日常开发中，需要对写操作异常情况进行处理。\n\n * 其中 inbound 类异步事件发生异常时，会触发exceptioncaught事件传播。exceptioncaught 事件本身也是一种 inbound 事件，传播方向会从当前发生异常的 channelhandler 开始一直向后传播直到 tailcontext。\n * 而 outbound 类异步事件发生异常时，则不会触发exceptioncaught事件传播。一般只是通知相关 channelfuture。但如果是 flush 事件在传播过程中发生异常，则会触发当前发生异常的 channelhandler 中 exceptioncaught 事件回调。\n\n我们继续回归到写操作的主线上来~~~\n\nprivate void write(object msg, boolean flush, channelpromise promise) {\n    objectutil.checknotnull(msg, \"msg\");\n\n    ................省略检查promise的有效性...............\n\n        //flush = true 表示channelhandler中调用的是writeandflush方法，这里需要找到pipeline中覆盖write或者flush方法的channelhandler\n        //flush = false 表示调用的是write方法，只需要找到pipeline中覆盖write方法的channelhandler\n        final abstractchannelhandlercontext next = findcontextoutbound(flush ?\n                                                                       (mask_write | mask_flush) : mask_write);\n    //用于检查内存泄露\n    final object m = pipeline.touch(msg, next);\n    //获取pipeline中下一个要被执行的channelhandler的executor\n    eventexecutor executor = next.executor();\n    //确保outbound事件由channelhandler指定的executor执行\n    if (executor.ineventloop()) {\n        //如果当前线程正是channelhandler指定的executor则直接执行\n        if (flush) {\n            next.invokewriteandflush(m, promise);\n        } else {\n            next.invokewrite(m, promise);\n        }\n    } else {\n        //如果当前线程不是channelhandler指定的executor,则封装成异步任务提交给指定executor执行，注意这里的executor不一定是reactor线程。\n        final writetask task = writetask.newinstance(next, m, promise, flush);\n        if (!safeexecute(executor, task, promise, m, !flush)) {\n            task.cancel();\n        }\n    }\n}\n\n\nwrite 事件要向前在 pipeline 中传播，就需要在 pipeline 上找到下一个具有执行资格的 channelhandler，因为位于当前 channelhandler 前边的可能是 channelinboundhandler 类型的也可能是 channeloutboundhandler 类型的 channelhandler ，或者有可能压根就不关心 write 事件的 channelhandler（没有实现write回调方法）。![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)\n\n这里我们就需要通过 findcontextoutbound 方法在当前 channelhandler 的前边找到 channeloutboundhandler 类型并且覆盖实现 write 回调方法的 channelhandler 作为下一个要执行的对象。\n\n\n# 3.1 findcontextoutbound\n\nprivate abstractchannelhandlercontext findcontextoutbound(int mask) {\n    abstractchannelhandlercontext ctx = this;\n    //获取当前channelhandler的executor\n    eventexecutor currentexecutor = executor();\n    do {\n        //获取前一个channelhandler\n        ctx = ctx.prev;\n    } while (skipcontext(ctx, currentexecutor, mask, mask_only_outbound));\n    return ctx;\n}\n//判断前一个channelhandler是否具有响应write事件的资格\nprivate static boolean skipcontext(\n    abstractchannelhandlercontext ctx, eventexecutor currentexecutor, int mask, int onlymask) {\n\n    return (ctx.executionmask & (onlymask | mask)) == 0 ||\n        (ctx.executor() == currentexecutor && (ctx.executionmask & mask) == 0);\n}\n\n\nfindcontextoutbound 方法接收的参数是一个掩码，这个掩码表示要向前查找具有什么样执行资格的 channelhandler。因为我们这里调用的是 channelhandlercontext 的 write 方法所以 flush = false，传递进来的掩码为 mask_write，表示我们要向前查找覆盖实现了 write 回调方法的 channeloutboundhandler。\n\n# 3.1.1 掩码的巧妙应用\n\nnetty 中将 channelhandler 覆盖实现的一些异步事件回调方法用 int 型的掩码来表示，这样我们就可以通过这个掩码来判断当前 channelhandler 具有什么样的执行资格。\n\nfinal class channelhandlermask {\n    ....................省略......................\n\n    static final int mask_channel_active = 1 << 3;\n    static final int mask_channel_read = 1 << 5;\n    static final int mask_channel_read_complete = 1 << 6;\n    static final int mask_write = 1 << 15;\n    static final int mask_flush = 1 << 16;\n\n   //outbound事件掩码集合\n   static final int mask_only_outbound =  mask_bind | mask_connect | mask_disconnect |\n            mask_close | mask_deregister | mask_read | mask_write | mask_flush;\n    ....................省略......................\n}\n\n\n在 channelhandler 被添加进 pipeline 的时候，netty 会根据当前 channelhandler 的类型以及其覆盖实现的异步事件回调方法，通过 | 运算 向 channelhandlercontext#executionmask 字段添加该 channelhandler 的执行资格。\n\nabstract class abstractchannelhandlercontext implements channelhandlercontext, resourceleakhint {\n\n    //channelhandler执行资格掩码\n    private final int executionmask;\n\n    ....................省略......................\n}\n\n\n类似的掩码用法其实我们在前边的文章?《一文聊透netty核心引擎reactor的运转架构》中也提到过，在 channel 向对应的 reactor 注册自己感兴趣的 io 事件时，也是用到了一个 int 型的掩码 interestops 来表示 channel 感兴趣的 io 事件集合。\n\n@override\nprotected void dobeginread() throws exception {\n\n    final selectionkey selectionkey = this.selectionkey;\n    if (!selectionkey.isvalid()) {\n        return;\n    }\n\n    readpending = true;\n\n    final int interestops = selectionkey.interestops();\n    /**\n     * 1：serversocketchannel 初始化时 readinterestop设置的是op_accept事件\n     * 2：socketchannel 初始化时 readinterestop设置的是op_read事件\n     * */\n    if ((interestops & readinterestop) == 0) {\n        //注册监听op_accept或者op_read事件\n        selectionkey.interestops(interestops | readinterestop);\n    }\n}\n\n\n * 用 & 操作判断，某个事件是否在事件集合中：(readyops & selectionkey.op_connect) != 0\n * 用 | 操作向事件集合中添加事件：interestops | readinterestop\n * 从事件集合中删除某个事件，是通过先将要删除事件取反 ~ ，然后在和事件集合做 & 操作：ops &= ~selectionkey.op_connect\n\n> 这部分内容笔者会在下篇文章全面介绍 pipeline 的时候详细讲解，大家这里只需要知道这里的掩码就是表示一个执行资格的集合。当前 channelhandler 的执行资格存放在它的 channelhandlercontext 中的 executionmask 字段中。\n\n# 3.1.2 向前查找具有执行资格的 channeloutboundhandler\n\nprivate abstractchannelhandlercontext findcontextoutbound(int mask) {\n    //当前channelhandler\n    abstractchannelhandlercontext ctx = this;\n    //获取当前channelhandler的executor\n    eventexecutor currentexecutor = executor();\n    do {\n        //获取前一个channelhandler\n        ctx = ctx.prev;\n    } while (skipcontext(ctx, currentexecutor, mask, mask_only_outbound));\n    return ctx;\n}\n\n//判断前一个channelhandler是否具有响应write事件的资格\nprivate static boolean skipcontext(\n    abstractchannelhandlercontext ctx, eventexecutor currentexecutor, int mask, int onlymask) {\n\n    return (ctx.executionmask & (onlymask | mask)) == 0 ||\n        (ctx.executor() == currentexecutor && (ctx.executionmask & mask) == 0);\n}\n\n\n前边我们提到 channelhandlercontext 不仅封装了 channelhandler 的执行资格掩码还可以感知到当前 channelhandler 在 pipeline 中的位置，因为 channelhandlercontext 中维护了前驱指针 prev 以及后驱指针 next。\n\n这里我们需要在 pipeline 中传播 write 事件，它是一种 outbound 事件，所以需要向前传播，这里通过 channelhandlercontext 的前驱指针 prev 拿到当前 channelhandler 在 pipeline 中的前一个节点。\n\nctx = ctx.prev;\n\n\n通过 skipcontext 方法判断前驱节点是否具有执行的资格。如果没有执行资格则跳过继续向前查找。如果具有执行资格则返回并响应 write 事件。\n\n在 write 事件传播场景中，执行资格指的是前驱 channelhandler 是否是channeloutboundhandler 类型的，并且它是否覆盖实现了 write 事件回调方法。\n\npublic class echochannelhandler extends channeloutboundhandleradapter {\n\n    @override\n    public void write(channelhandlercontext ctx, object msg, channelpromise promise) throws exception {\n        super.write(ctx, msg, promise);\n    }\n}\n\n\n# 3.1.3 skipcontext\n\n该方法主要用来判断当前 channelhandler 的前驱节点是否具有 mask 掩码中包含的事件响应资格。\n\n方法参数中有两个比较重要的掩码：\n\n * int onlymask：用来指定当前 channelhandler 需要符合的类型。其中mask_only_outbound 为 channeloutboundhandler 类型的掩码， mask_only_inbound 为 channelinboundhandler 类型的掩码。\n\nfinal class channelhandlermask {\n\n    //outbound事件的掩码集合\n    static final int mask_only_outbound =  mask_bind | mask_connect | mask_disconnect |\n            mask_close | mask_deregister | mask_read | mask_write | mask_flush;\n\n    //inbound事件的掩码集合\n    static final int mask_only_inbound =  mask_channel_registered |\n            mask_channel_unregistered | mask_channel_active | mask_channel_inactive | mask_channel_read |\n            mask_channel_read_complete | mask_user_event_triggered | mask_channel_writability_changed;\n}\n\n\n比如本小节中我们是在介绍 write 事件的传播，那么就需要在当前channelhandler 前边首先是找到一个 channeloutboundhandler 类型的channelhandler。\n\nctx.executionmask & (onlymask | mask)) == 0 用于判断前一个 channelhandler 是否为我们指定的 channelhandler 类型，在本小节中我们指定的是 onlumask = mask_only_outbound 即 channeloutboundhandler 类型。如果不是，这里就会直接跳过，继续在 pipeline 中向前查找。\n\n * int mask：用于指定前一个 channelhandler 需要实现的相关异步事件处理回调。在本小节中这里指定的是 mask_write ，即需要实现 write 回调方法。通过 (ctx.executionmask & mask) == 0 条件来判断前一个channelhandler 是否实现了 write 回调，如果没有实现这里就跳过，继续在 pipeline 中向前查找。\n\n> 关于 skipcontext 方法的详细介绍，笔者还会在下篇文章全面介绍 pipeline的时候再次进行介绍，这里大家只需要明白该方法的核心逻辑即可。\n\n# 3.1.4 向前传播write事件\n\n通过 findcontextoutbound 方法我们在 pipeline 中找到了下一个具有执行资格的 channelhandler，这里指的是下一个 channeloutboundhandler 类型并且覆盖实现了 write 方法的 channelhandler。\n\nnetty 紧接着会调用这个 nextchannelhandler 的 write 方法实现 write 事件在 pipeline 中的传播。\n\n//获取下一个要被执行的channelhandler指定的executor\neventexecutor executor = next.executor();\n//确保outbound事件的执行 是由 channelhandler指定的executor执行的\nif (executor.ineventloop()) {\n    //如果当前线程是指定的executor 则直接操作\n    if (flush) {\n        next.invokewriteandflush(m, promise);\n    } else {\n        next.invokewrite(m, promise);\n    }\n} else {\n    //如果当前线程不是channelhandler指定的executor，则封装程异步任务 提交给指定的executor执行\n    final writetask task = writetask.newinstance(next, m, promise, flush);\n    if (!safeexecute(executor, task, promise, m, !flush)) {\n        task.cancel();\n    }\n}\n\n\n在我们向 pipeline 添加 channelhandler 的时候可以通过channelpipeline#addlast(eventexecutorgroup,channelhandler......) 方法指定执行该 channelhandler 的executor。如果不特殊指定，那么执行该 channelhandler 的executor默认为该 channel 绑定的 reactor 线程。\n\n> 执行 channelhandler 中异步事件回调方法的线程必须是 channelhandler 指定的executor。\n\n所以这里首先我们需要获取在 findcontextoutbound 方法查找出来的下一个符合执行条件的 channelhandler 指定的executor。\n\neventexecutor executor = next.executor()\n\n\n并通过 executor.ineventloop() 方法判断当前线程是否是该 channelhandler 指定的 executor。\n\n如果是，那么我们直接在当前线程中执行 channelhandler 中的 write 方法。\n\n如果不是，我们就需要将 channelhandler 对 write 事件的回调操作封装成异步任务 writetask 并提交给 channelhandler 指定的 executor 中，由 executor 负责执行。\n\n> 这里需要注意的是这个 executor 并不一定是 channel 绑定的 reactor 线程。它可以是我们自定义的线程池，不过需要我们通过 channelpipeline#addlast 方法进行指定，如果我们不指定，默认情况下执行 channelhandler 的 executor 才是 channel 绑定的 reactor 线程。\n\n> 这里netty需要确保 outbound 事件是由 channelhandler 指定的 executor 执行的。\n\n这里有些同学可能会有疑问，如果我们向pipieline添加channelhandler的时候，为每个channelhandler指定不同的executor时，netty如果确保线程安全呢？？\n\n大家还记得pipeline中的结构吗？\n\n客户端channel pipeline结构.png\n\noutbound 事件在 pipeline 中的传播最终会传播到 headcontext 中，之前的系列文章我们提到过，headcontext 中封装了 channel 的 unsafe 类负责 channel 底层的 io 操作。而 headcontext 指定的 executor 正是对应 channel 绑定的 reactor 线程。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n所以最终在 netty 内核中执行写操作的线程一定是 reactor 线程从而保证了线程安全性。\n\n> 忘记这段内容的同学可以在回顾下?《reactor在netty中的实现(创建篇)》，类似的套路我们在介绍 nioserversocketchannel 进行 bind 绑定以及 register 注册的时候都介绍过，只不过这里将 executor 扩展到了自定义线程池的范围。\n\n\n# 3.1.5 触发nextchannelhandler的write方法回调\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)write事件的传播1.png\n\n//如果当前线程是指定的executor 则直接操作\nif (flush) {\n    next.invokewriteandflush(m, promise);\n} else {\n    next.invokewrite(m, promise);\n}\n\n\n由于我们在示例 channelhandler 中调用的是 channelhandlercontext#write 方法，所以这里的 flush = false 。触发调用 nextchannelhandler 的 write 方法。\n\nvoid invokewrite(object msg, channelpromise promise) {\n    if (invokehandler()) {\n        invokewrite0(msg, promise);\n    } else {\n        // 当前channelhandler虽然添加到pipeline中，但是并没有调用handleradded\n        // 所以不能调用当前channelhandler中的回调方法，只能继续向前传递write事件\n        write(msg, promise);\n    }\n}\n\n\n这里首先需要通过 invokehandler() 方法判断这个 nextchannelhandler 中的 handleradded 方法是否被回调过。因为 channelhandler 只有被正确的添加到对应的 channelhandlercontext 中并且准备好处理异步事件时， channelhandler#handleradded 方法才会被回调。\n\n> 这一部分内容笔者会在下一篇文章中详细为大家介绍，这里大家只需要了解调用 invokehandler() 方法的目的就是为了确定 channelhandler 是否被正确的初始化。\n\nprivate boolean invokehandler() {\n    // store in local variable to reduce volatile reads.\n    int handlerstate = this.handlerstate;\n    return handlerstate == add_complete || (!ordered && handlerstate == add_pending);\n}\n\n\n只有触发了 handleradded 回调，channelhandler 的状态才能变成 add_complete 。\n\n如果 invokehandler() 方法返回 false，那么我们就需要跳过这个nextchannelhandler，并调用 channelhandlercontext#write 方法继续向前传播 write 事件。\n\n@override\npublic channelfuture write(final object msg, final channelpromise promise) {\n    //继续向前传播write事件，回到流程起点\n    write(msg, false, promise);\n    return promise;\n}\n\n\n如果 invokehandler() 返回 true ，说明这个 nextchannelhandler 已经在 pipeline 中被正确的初始化了，netty 直接调用这个 channelhandler 的 write 方法，这样就实现了 write 事件从当前 channelhandler 传播到了nextchannelhandler。\n\nprivate void invokewrite0(object msg, channelpromise promise) {\n    try {\n        //调用当前channelhandler中的write方法\n        ((channeloutboundhandler) handler()).write(this, msg, promise);\n    } catch (throwable t) {\n        notifyoutboundhandlerexception(t, promise);\n    }\n}\n\n\n> 这里我们看到在 write 事件的传播过程中如果发生异常，那么 write 事件就会停止在 pipeline 中传播，并通知注册的 channelfuturelistener。\n\n客户端channel pipeline结构.png\n\n从本文示例的 pipeline 结构中我们可以看到，当在 echoserverhandler 调用 channelhandlercontext#write 方法后，write 事件会在 pipeline 中向前传播到 headcontext 中，而在 headcontext 中才是 netty 真正处理 write 事件的地方。\n\n\n# 3.2 headcontext\n\nfinal class headcontext extends abstractchannelhandlercontext\n            implements channeloutboundhandler, channelinboundhandler {\n          \n        @override\n        public void write(channelhandlercontext ctx, object msg, channelpromise promise) {\n            unsafe.write(msg, promise);\n        }\n }\n\n\nwrite 事件最终会在 pipeline 中传播到 headcontext 里并回调 headcontext 的 write 方法。并在 write 回调中调用 channel 的 unsafe 类执行底层的 write 操作。这里正是 write 事件在 pipeline 中的传播终点。\n\nchanneloutboundbuffer中缓存待发送数据.png\n\nprotected abstract class abstractunsafe implements unsafe {\n    //待发送数据缓冲队列  netty是全异步框架，所以这里需要一个缓冲队列来缓存用户需要发送的数据 \n    private volatile channeloutboundbuffer outboundbuffer = new channeloutboundbuffer(abstractchannel.this);\n\n    @override\n    public final void write(object msg, channelpromise promise) {\n        asserteventloop();\n        //获取当前channel对应的待发送数据缓冲队列（支持用户异步写入的核心关键）\n        channeloutboundbuffer outboundbuffer = this.outboundbuffer;\n\n        ..........省略..................\n\n            int size;\n        try {\n            //过滤message类型 这里只会接受directbuffer或者fileregion类型的msg\n            msg = filteroutboundmessage(msg);\n            //计算当前msg的大小\n            size = pipeline.estimatorhandle().size(msg);\n            if (size < 0) {\n                size = 0;\n            }\n        } catch (throwable t) {\n            ..........省略..................\n        }\n        //将msg 加入到netty中的待写入数据缓冲队列channeloutboundbuffer中\n        outboundbuffer.addmessage(msg, size, promise);\n    }\n\n}\n\n\n众所周知 netty 是一个异步事件驱动的网络框架，在 netty 中所有的 io 操作全部都是异步的，当然也包括本小节介绍的 write 操作，为了保证异步执行 write 操作，netty 定义了一个待发送数据缓冲队列 channeloutboundbuffer ，netty 将这些用户需要发送的网络数据在写入到 socket 之前，先放在 channeloutboundbuffer 中缓存。\n\n> 每个客户端 niosocketchannel 对应一个 channeloutboundbuffer 待发送数据缓冲队列\n\n\n# 3.2.1 filteroutboundmessage\n\nchanneloutboundbuffer 只会接受 bytebuffer 类型以及 fileregion 类型的 msg 数据。\n\n> fileregion 是netty定义的用来通过零拷贝的方式网络传输文件数据。本文我们主要聚焦普通网络数据 bytebuffer 的发送。\n\n所以在将 msg 写入到 channeloutboundbuffer 之前，我们需要检查待写入 msg 的类型。确保是 channeloutboundbuffer 可接受的类型。\n\n    @override\n    protected final object filteroutboundmessage(object msg) {\n        if (msg instanceof bytebuf) {\n            bytebuf buf = (bytebuf) msg;\n            if (buf.isdirect()) {\n                return msg;\n            }\n\n            return newdirectbuffer(buf);\n        }\n\n        if (msg instanceof fileregion) {\n            return msg;\n        }\n\n        throw new unsupportedoperationexception(\n                \"unsupported message type: \" + stringutil.simpleclassname(msg) + expected_types);\n    }\n\n\n在网络数据传输的过程中，netty为了减少数据从 堆内内存 到 堆外内存 的拷贝以及缓解gc的压力，所以这里必须采用 directbytebuffer 使用堆外内存来存放网络发送数据。\n\n\n# 3.2.2 estimatorhandle计算当前msg的大小\n\npublic class defaultchannelpipeline implements channelpipeline {\n    //原子更新estimatorhandle字段\n    private static final atomicreferencefieldupdater<defaultchannelpipeline, messagesizeestimator.handle> estimator =\n            atomicreferencefieldupdater.newupdater(\n                    defaultchannelpipeline.class, messagesizeestimator.handle.class, \"estimatorhandle\");\n\n    //计算要发送msg大小的handler\n    private volatile messagesizeestimator.handle estimatorhandle;\n\n    final messagesizeestimator.handle estimatorhandle() {\n        messagesizeestimator.handle handle = estimatorhandle;\n        if (handle == null) {\n            handle = channel.config().getmessagesizeestimator().newhandle();\n            if (!estimator.compareandset(this, null, handle)) {\n                handle = estimatorhandle;\n            }\n        }\n        return handle;\n    }\n}\n\n\n在 pipeline 中会有一个 estimatorhandle 专门用来计算待发送 bytebuffer 的大小。这个 estimatorhandle 会在 pipeline 对应的 channel 中的配置类创建的时候被初始化。\n\n这里 estimatorhandle 的实际类型为defaultmessagesizeestimator#handleimpl。\n\npublic final class defaultmessagesizeestimator implements messagesizeestimator {\n\n    private static final class handleimpl implements handle {\n        private final int unknownsize;\n\n        private handleimpl(int unknownsize) {\n            this.unknownsize = unknownsize;\n        }\n\n        @override\n        public int size(object msg) {\n            if (msg instanceof bytebuf) {\n                return ((bytebuf) msg).readablebytes();\n            }\n            if (msg instanceof bytebufholder) {\n                return ((bytebufholder) msg).content().readablebytes();\n            }\n            if (msg instanceof fileregion) {\n                return 0;\n            }\n            return unknownsize;\n        }\n    }\n\n\n这里我们看到 bytebuffer 的大小即为 buffer 中未读取的字节数 writerindex - readerindex 。\n\n当我们验证了待写入数据 msg 的类型以及计算了 msg 的大小后，我们就可以通过 channeloutboundbuffer#addmessage方法将 msg 写入到channeloutboundbuffer（待发送数据缓冲队列）中。\n\nwrite 事件处理的最终逻辑就是将待发送数据写入到 channeloutboundbuffer 中，下面我们就来看下这个 channeloutboundbuffer 内部结构到底是什么样子的？\n\n\n# 3.3 channeloutboundbuffer\n\nchanneloutboundbuffer 其实是一个单链表结构的缓冲队列，链表中的节点类型为 entry ，由于 channeloutboundbuffer 在 netty 中的作用就是缓存应用程序待发送的网络数据，所以 entry 中封装的就是待写入 socket 中的网络发送数据相关的信息，以及 channelhandlercontext#write 方法中返回给用户的 channelpromise 。这样可以在数据写入socket之后异步通知应用程序。\n\n此外 channeloutboundbuffer 中还封装了三个重要的指针：\n\n * unflushedentry ：该指针指向 channeloutboundbuffer 中第一个待发送数据的 entry。\n * tailentry：该指针指向 channeloutboundbuffer 中最后一个待发送数据的 entry。通过 unflushedentry 和 tailentry 这两个指针，我们可以很方便的定位到待发送数据的 entry 范围。\n * flushedentry：当我们通过 flush 操作需要将 channeloutboundbuffer 中缓存的待发送数据发送到 socket 中时，flushedentry 指针会指向 unflushedentry 的位置，这样 flushedentry 指针和 tailentry 指针之间的 entry 就是我们即将发送到 socket 中的网络数据。\n\n这三个指针在初始化的时候均为 null 。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)channeloutboundbuffer结构.png\n\n\n# 3.3.1 entry\n\nentry 作为 channeloutboundbuffer 链表结构中的节点元素类型，里边封装了待发送数据的各种信息，channeloutboundbuffer 其实就是对 entry 结构的组织和操作。因此理解 entry 结构是理解整个 channeloutboundbuffer 运作流程的基础。\n\n下面我们就来看下 entry 结构具体封装了哪些待发送数据的信息。\n\nstatic final class entry {\n    //entry的对象池，用来创建和回收entry对象\n    private static final objectpool<entry> recycler = objectpool.newpool(new objectcreator<entry>() {\n        @override\n        public entry newobject(handle<entry> handle) {\n            return new entry(handle);\n        }\n    });\n\n    //defaulthandle用于回收对象\n    private final handle<entry> handle;\n    //channeloutboundbuffer下一个节点\n    entry next;\n    //待发送数据\n    object msg;\n    //msg 转换为 jdk nio 中的bytebuffer\n    bytebuffer[] bufs;\n    bytebuffer buf;\n    //异步write操作的future\n    channelpromise promise;\n    //已发送了多少\n    long progress;\n    //总共需要发送多少，不包含entry对象大小。\n    long total;\n    //pendingsize表示entry对象在堆中需要的内存总量 待发送数据大小 + entry对象本身在堆中占用内存大小（96）\n    int pendingsize;\n    //msg中包含了几个jdk nio bytebuffer\n    int count = -1;\n    //write操作是否被取消\n    boolean cancelled;\n}\n\n\n我们看到entry结构中一共有12个字段，其中1个静态字段和11个实例字段。\n\n下面笔者就为大家介绍下这12个字段的含义及其作用，其中有些字段会在后面的场景中使用到，这里大家可能对有些字段理解起来比较模糊，不过没关系，这里能看懂多少是多少，不理解也没关系，这里介绍只是为了让大家混个眼熟，在后面流程的讲解中，笔者还会重新提到这些字段。\n\n * objectpool<entry> recycler：entry 的对象池，负责创建管理 entry 实例，由于 netty 是一个网络框架，所以 io 读写就成了它的核心操作，在一个支持高性能高吞吐的网络框架中，会有大量的 io 读写操作，那么就会导致频繁的创建 entry 对象。我们都知道，创建一个实例对象以及 gc 回收这些实例对象都是需要性能开销的，那么在大量频繁创建 entry 对象的场景下，引入对象池来复用创建好的 entry 对象实例可以抵消掉由频繁创建对象以及gc回收对象所带来的性能开销。\n\n> 关于对象池的详细内容，感兴趣的同学可以回看下笔者的这篇文章?《详解recycler对象池的精妙设计与实现》\n\n * handle<entry> handle：默认实现类型为 defaulthandle ，用于数据发送完毕后，对象池回收 entry 对象。由对象池 recycler 在创建 entry 对象的时候传递进来。\n\n * entry next：channeloutboundbuffer 是一个单链表的结构，这里的 next 指针用于指向当前 entry 节点的后继节点。\n\n * object msg：应用程序待发送的网络数据，这里 msg 的类型为 directbytebuffer 或者 fileregion（用于通过零拷贝的方式网络传输文件）。\n\n * bytebuffer[] bufs：这里的 bytebuffer 类型为 jdk nio 原生的 bytebuffer 类型，因为 netty 最终发送数据是通过 jdk nio 底层的 socketchannel 进行发送，所以需要将 netty 中实现的 bytebuffer 类型转换为 jdk nio bytebuffer 类型。应用程序发送的 bytebuffer 可能是一个也可能是多个，如果发送多个就用 bytebuffer[] bufs 封装在 entry 对象中，如果是一个就用 bytebuffer buf 封装。\n\n * int count：表示待发送数据 msg 中一共包含了多少个 bytebuffer 需要发送。\n\n * channelpromise promise：channelhandlercontext#write 异步写操作返回的 channelfuture。当 netty 将待发送数据写入到 socket 中时会通过这个 channelpromise 通知应用程序发送结果。\n\n * long progress：表示当前的一个发送进度，已经发送了多少数据。\n\n * long total：entry中总共需要发送多少数据。注意：这个字段并不包含 entry 对象的内存占用大小。只是表示待发送网络数据的大小。\n\n * boolean cancelled：应用程序调用的 write 操作是否被取消。\n\n * int pendingsize：表示待发送数据的内存占用总量。待发送数据在内存中的占用量分为两部分：\n\n * * entry对象中所封装的待发送网络数据大小。\n   * entry对象本身在内存中的占用量。\n\nentry内存占用总量.png\n\n\n# 3.3.2 pendingsize的作用\n\n想象一下这样的一个场景，当由于网络拥塞或者 netty 客户端负载很高导致网络数据的接收速度以及处理速度越来越慢，tcp 的滑动窗口不断缩小以减少网络数据的发送直到为 0，而 netty 服务端却有大量频繁的写操作，不断的写入到 channeloutboundbuffer 中。\n\n这样就导致了数据发送不出去但是 netty 服务端又在不停的写数据，慢慢的就会撑爆 channeloutboundbuffer 导致oom。这里主要指的是堆外内存的 oom，因为 channeloutboundbuffer 中包裹的待发送数据全部存储在堆外内存中。\n\n所以 netty 就必须限制 channeloutboundbuffer 中的待发送数据的内存占用总量，不能让它无限增长。netty 中定义了高低水位线用来表示 channeloutboundbuffer 中的待发送数据的内存占用量的上限和下限。注意：这里的内存既包括 jvm 堆内存占用也包括堆外内存占用。\n\n * 当待发送数据的内存占用总量超过高水位线的时候，netty 就会将 niosocketchannel 的状态标记为不可写状态。否则就可能导致 oom。\n * 当待发送数据的内存占用总量低于低水位线的时候，netty 会再次将 niosocketchannel 的状态标记为可写状态。\n\n那么我们用什么记录channeloutboundbuffer中的待发送数据的内存占用总量呢？\n\n答案就是本小节要介绍的 pendingsize 字段。在谈到待发送数据的内存占用量时大部分同学普遍都会有一个误解就是只计算待发送数据的大小（msg中包含的字节数） 而忽略了 entry 实例对象本身在内存中的占用量。\n\n因为 netty 会将待发送数据封装在 entry 实例对象中，在大量频繁的写操作中会产生大量的 entry 实例对象，所以 entry 实例对象的内存占用是不可忽视的。\n\n否则就会导致明明还没有到达高水位线，但是由于大量的 entry 实例对象存在，从而发生oom。\n\n所以 pendingsize 的计算既要包含待发送数据的大小也要包含其 entry 实例对象的内存占用大小，这样才能准确计算出 channeloutboundbuffer 中待发送数据的内存占用总量。\n\nchanneloutboundbuffer 中所有的 entry 实例中的 pendingsize 之和就是待发送数据总的内存占用量。\n\npublic final class channeloutboundbuffer {\n  //channeloutboundbuffer中的待发送数据的内存占用总量\n  private volatile long totalpendingsize;\n\n}\n\n\n\n# 3.3.3 高低水位线\n\n上小节提到 netty 为了防止 channeloutboundbuffer 中的待发送数据内存占用无限制的增长从而导致 oom ，所以引入了高低水位线，作为待发送数据内存占用的上限和下限。\n\n那么高低水位线具体设置多大呢 ? 我们来看一下 defaultchannelconfig 中的配置。\n\npublic class defaultchannelconfig implements channelconfig {\n\n    //channeloutboundbuffer中的高低水位线\n    private volatile writebufferwatermark writebufferwatermark = writebufferwatermark.default;\n\n}\npublic final class writebufferwatermark {\n\n    private static final int default_low_water_mark = 32 * 1024;\n    private static final int default_high_water_mark = 64 * 1024;\n\n    public static final writebufferwatermark default =\n            new writebufferwatermark(default_low_water_mark, default_high_water_mark, false);\n\n    writebufferwatermark(int low, int high, boolean validate) {\n\n        ..........省略校验逻辑.........\n\n        this.low = low;\n        this.high = high;\n    }\n}\n\n\n我们看到 channeloutboundbuffer 中的高水位线设置的大小为 64 kb，低水位线设置的是 32 kb。\n\n这也就意味着每个 channel 中的待发送数据如果超过 64 kb。channel 的状态就会变为不可写状态。当内存占用量低于 32 kb时，channel 的状态会再次变为可写状态。\n\n\n# 3.3.4 entry实例对象在jvm中占用内存大小\n\n前边提到 pendingsize 的作用主要是记录当前待发送数据的内存占用总量从而可以预警 oom 的发生。\n\n待发送数据的内存占用分为：待发送数据 msg 的内存占用大小以及 entry 对象本身在jvm中的内存占用。\n\n那么 entry 对象本身的内存占用我们该如何计算呢？\n\n要想搞清楚这个问题，大家需要先了解一下 java 对象内存布局的相关知识。关于这部分背景知识，笔者已经在 ?《一文聊透对象在jvm中的内存布局，以及内存对齐和压缩指针的原理及应用》这篇文章中给出了详尽的阐述，想深入了解这块的同学可以看下这篇文章。\n\n这里笔者只从这篇文章中提炼一些关于计算 java 对象占用内存大小相关的内容。\n\n在关于 java 对象内存布局这篇文章中我们提到，对于java普通对象来说内存中的布局由：对象头 + 实例数据区 + padding，这三部分组成。\n\n其中对象头由存储对象运行时信息的 markword 以及指向对象类型元信息的类型指针组成。\n\n> markword 用来存放：hashcode，gc 分代年龄，锁状态标志，线程持有的锁，偏向线程 id，偏向时间戳等。在 32 位操作系统和 64 位操作系统中 markword 分别占用 4b 和 8b 大小的内存。\n\n> java 对象头中的类型指针还有实例数据区的对象引用，在64 位系统中开启压缩指针的情况下（-xx:+usecompressedoops）占用 4b 大小。在关闭压缩指针的情况下（-xx:-usecompressedoops）占用 8b 大小。\n\n实例数据区用于存储 java 类中定义的实例字段，包括所有父类中的实例字段以及对象引用。\n\n在实例数据区中对象字段之间的排列以及内存对齐需要遵循三个字段重排列规则：\n\n * 规则1：如果一个字段占用x个字节，那么这个字段的偏移量offset需要对齐至nx。\n * 规则2：在开启了压缩指针的 64 位 jvm 中，java 类中的第一个字段的 offset 需要对齐至 4n，在关闭压缩指针的情况下类中第一个字段的offset需要对齐至 8n。\n * 规则3：jvm 默认分配字段的顺序为：long / double，int / float，short / char，byte / boolean，oops(ordianry object point 引用类型指针)，并且父类中定义的实例变量会出现在子类实例变量之前。当设置jvm参数 -xx +compactfields 时（默认），占用内存小于 long / double 的字段会允许被插入到对象中第一个 long / double 字段之前的间隙中，以避免不必要的内存填充。\n\n还有一个重要规则就是 java 虚拟机堆中对象的起始地址需要对齐至 8 的倍数（可由jvm参数 -xx:objectalignmentinbytes 控制，默认为 8 ）。\n\n在了解上述字段排列以及对象之间的内存对齐规则后，我们分别以开启压缩指针和关闭压缩指针两种情况，来对 entry 对象的内存布局进行分析并计算对象占用内存大小。\n\nstatic final class entry {\n    .............省略static字段recycler.........\n\n   \t//defaulthandle用于回收对象\n   \tprivate final handle<entry> handle;\n    //channeloutboundbuffer下一个节点\n    entry next;\n    //待发送数据\n    object msg;\n    //msg 转换为 jdk nio 中的bytebuffer\n    bytebuffer[] bufs;\n    bytebuffer buf;\n    //异步write操作的future\n    channelpromise promise;\n    //已发送了多少\n    long progress;\n    //总共需要发送多少，不包含entry对象大小。\n    long total;\n    //pendingsize表示entry对象在堆中需要的内存总量 待发送数据大小 + entry对象本身在堆中占用内存大小（96）\n    int pendingsize;\n    //msg中包含了几个jdk nio bytebuffer\n    int count = -1;\n    //write操作是否被取消\n    boolean cancelled;\n}\n\n\n我们看到 entry 对象中一共有 11 个实例字段，其中 2 个 long 型字段，2 个 int 型字段，1 个 boolean 型字段，6 个对象引用。\n\n默认情况下jvm参数 -xx +compactfields 是开启的。\n\n# 开启指针压缩 -xx:+usecompressedoops\n\nimage.png\n\nentry 对象的内存布局中开头先是 8 个字节的 markword，然后是 4 个字节的类型指针（开启压缩指针）。\n\n在实例数据区中对象的排列规则需要符合规则3，也就是字段之间的排列顺序需要遵循 long > int > boolean > oop(对象引用)。\n\n根据规则 3 entry对象实例数据区第一个字段应该是 long progress，但根据规则1 long 型字段的 offset 需要对齐至 8 的倍数，并且根据 规则2 在开启压缩指针的情况下，对象的第一个字段 offset 需要对齐至 4 的倍数。所以字段long progress 的 offet = 16，这就必然导致了在对象头与字段 long progress 之间需要由 4 字节的字节填充（offet = 12处发生字节填充）。\n\n但是 jvm 默认开启了 -xx +compactfields，根据 规则3 占用内存小于 long / double 的字段会允许被插入到对象中第一个 long / double 字段之前的间隙中，以避免不必要的内存填充。\n\n所以位于后边的字段 int pendingsize 插入到了 offet = 12 位置处，避免了不必要的字节填充。\n\n在 entry 对象的实例数据区中紧接着基础类型字段后面跟着的就是 6 个对象引用字段(开启压缩指针占用 4 个字节)。\n\n大家一定注意到 offset = 37 处本应该存放的是字段 private final handle<entry> handle 但是却被填充了 3 个字节。这是为什么呢？\n\n根据字段重排列规则1：引用字段 private final handle<entry> handle 占用 4 个字节（开启压缩指针的情况），所以需要对齐至4的倍数。所以需要填充3个字节，使得引用字段 private final handle<entry> handle 位于 offset = 40 处。\n\n根据以上这些规则最终计算出来在开启压缩指针的情况下entry对象在堆中占用内存大小为64字节\n\n# 关闭指针压缩 -xx:-usecompressedoops\n\n在分析完 entry 对象在开启压缩指针情况下的内存布局情况后，我想大家现在对前边介绍的字段重排列的三个规则理解更加清晰了，那么我们基于这个基础来分析下在关闭压缩指针的情况下 entry 对象的内存布局。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)image.png\n\n首先 entry 对象在内存布局中的开头依然是由 8 个字节的 markword 还有 8 个字节的类型指针（关闭压缩指针）组成的对象头。\n\n我们看到在 offset = 41 处发生了字节填充，原因是在关闭压缩指针的情况下，对象引用占用内存大小变为 8 个字节，根据规则1: 引用字段 private final handle<entry> handle 的 offet 需要对齐至 8 的倍数，所以需要在该引用字段之前填充 7 个字节，使得引用字段 private final handle<entry> handle 的offet = 48 。\n\n综合字段重排列的三个规则最终计算出来在关闭压缩指针的情况下entry对象在堆中占用内存大小为96字节\n\n\n# 3.3.5 向channeloutboundbuffer中缓存待发送数据\n\n在介绍完 channeloutboundbuffer 的基本结构之后，下面就来到了 netty 处理 write 事件的最后一步，我们来看下用户的待发送数据是如何被添加进 channeloutboundbuffer 中的。\n\npublic void addmessage(object msg, int size, channelpromise promise) {\n    entry entry = entry.newinstance(msg, size, total(msg), promise);\n    if (tailentry == null) {\n        flushedentry = null;\n    } else {\n        entry tail = tailentry;\n        tail.next = entry;\n    }\n    tailentry = entry;\n    if (unflushedentry == null) {\n        unflushedentry = entry;\n    }\n\n    incrementpendingoutboundbytes(entry.pendingsize, false);\n}\n\n\n# 3.3.5.1 创建entry对象来封装待发送数据信息\n\n通过前边的介绍我们了解到当用户调用 ctx.write(msg) 之后，write 事件开始在pipeline中从当前 channelhandler开始一直向前进行传播，最终在 headcontext 中将待发送数据写入到 channel 对应的写缓冲区 channeloutboundbuffer 中。\n\n而 channeloutboundbuffer 是由 entry 结构组成的一个单链表，entry 结构封装了用户待发送数据的各种信息。\n\n这里首先我们需要为待发送数据创建 entry 对象，而在?《详解recycler对象池的精妙设计与实现》一文中我们介绍对象池时，提到 netty 作为一个高性能高吞吐的网络框架要面对海量的 io 处理操作，这种场景下会频繁的创建大量的 entry 对象，而对象的创建及其回收时需要性能开销的，尤其是在面对大量频繁的创建对象场景下，这种开销会进一步被放大，所以 netty 引入了对象池来管理 entry 对象实例从而避免 entry 对象频繁创建以及 gc 带来的性能开销。\n\n既然 entry 对象已经被对象池接管，那么它在对象池外面是不能被直接创建的，其构造函数是私有类型，并提供一个静态方法 newinstance 供外部线程从对象池中获取 entry 对象。这在?《详解recycler对象池的精妙设计与实现》一文中介绍池化对象的设计时也有提到过。\n\nstatic final class entry {\n    //静态变量引用类型地址 这个是在klass point(类型指针)中定义 8字节（开启指针压缩 为4字节）\n    private static final objectpool<entry> recycler = objectpool.newpool(new objectcreator<entry>() {\n        @override\n        public entry newobject(handle<entry> handle) {\n            return new entry(handle);\n        }\n    });\n\n    //entry对象只能通过对象池获取，不可外部自行创建\n    private entry(handle<entry> handle) {\n        this.handle = handle;\n    }\n\n    //不考虑指针压缩的大小 entry对象在堆中占用的内存大小为96\n    //如果开启指针压缩，entry对象在堆中占用的内存大小 会是64  \n    static final int channel_outbound_buffer_entry_overhead =\n        systempropertyutil.getint(\"io.netty.transport.outboundbufferentrysizeoverhead\", 96);\n\n    static entry newinstance(object msg, int size, long total, channelpromise promise) {\n        entry entry = recycler.get();\n        entry.msg = msg;\n        //待发数据数据大小 + entry对象大小\n        entry.pendingsize = size + channel_outbound_buffer_entry_overhead;\n        entry.total = total;\n        entry.promise = promise;\n        return entry;\n    }\n\n    .......................省略................\n\n}\n\n\n 1. 通过池化对象 entry 中持有的对象池 recycler ，从对象池中获取 entry 对象实例。\n 2. 将用户待发送数据 msg（directbytebuffer），待发送数据大小：total ，本次发送数据的 channelfuture，以及该 entry 对象的 pendingsize 统统封装在 entry 对象实例的相应字段中。\n\n这里需要特殊说明一点的是关于 pendingsize 的计算方式，之前我们提到 pendingsize 中所计算的内存占用一共包含两部分：\n\n * 待发送网络数据大小\n * entry 对象本身在内存中的占用量\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)entry内存占用总量.png\n\n而在《3.3.4 entry实例对象在jvm中占用内存大小》小节中我们介绍到，entry 对象在内存中的占用大小在开启压缩指针的情况下（-xx:+usecompressedoops）占用 64 字节，在关闭压缩指针的情况下（-xx:-usecompressedoops）占用 96 字节。\n\n字段 channel_outbound_buffer_entry_overhead 表示的就是 entry 对象在内存中的占用大小，netty这里默认是 96 字节，当然如果我们的应用程序开启了指针压缩，我们可以通过 jvm 启动参数 -d io.netty.transport.outboundbufferentrysizeoverhead 指定为 64 字节。\n\n# 3.3.5.2 将entry对象添加进channeloutboundbuffer中\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)channeloutboundbuffer结构.png\n\nif (tailentry == null) {\n    flushedentry = null;\n} else {\n    entry tail = tailentry;\n    tail.next = entry;\n}\ntailentry = entry;\nif (unflushedentry == null) {\n    unflushedentry = entry;\n}\n\n\n在《3.3 channeloutboundbuffer》小节一开始，我们介绍了 channeloutboundbuffer 中最重要的三个指针，这里涉及到的两个指针分别是：\n\n * unflushedentry ：指向 channeloutboundbuffer 中第一个未被 flush 进 socket 的待发送数据。用来指示 channeloutboundbuffer 的第一个节点。\n * tailentry：指向 channeloutboundbuffer 中最后一个节点。\n\n通过 unflushedentry 和 tailentry 可以定位出待发送数据的范围。channel 中的每一次 write 事件，最终都会将待发送数据插入到 channeloutboundbuffer 的尾结点处。\n\n# 3.3.5.3 incrementpendingoutboundbytes\n\n在将 entry 对象添加进 channeloutboundbuffer 之后，就需要更新用于记录当前 channeloutboundbuffer 中关于待发送数据所占内存总量的水位线指示。\n\n如果更新后的水位线超过了 netty 指定的高水位线 default_high_water_mark = 64 * 1024，则需要将当前 channel 的状态设置为不可写，并在 pipeline 中传播 channelwritabilitychanged 事件，注意该事件是一个 inbound 事件。\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)响应channelwritabilitychanged事件.png\n\npublic final class channeloutboundbuffer {\n\n    //channeloutboundbuffer中的待发送数据的内存占用总量 : 所有entry对象本身所占用内存大小 + 所有待发送数据的大小\n    private volatile long totalpendingsize;\n\n    //水位线指针\n    private static final atomiclongfieldupdater<channeloutboundbuffer> total_pending_size_updater =\n        atomiclongfieldupdater.newupdater(channeloutboundbuffer.class, \"totalpendingsize\");\n\n    private void incrementpendingoutboundbytes(long size, boolean invokelater) {\n        if (size == 0) {\n            return;\n        }\n        //更新总共待写入数据的大小\n        long newwritebuffersize = total_pending_size_updater.addandget(this, size);\n        //如果待写入的数据 大于 高水位线 64 * 1024  则设置当前channel为不可写 由用户自己决定是否继续写入\n        if (newwritebuffersize > channel.config().getwritebufferhighwatermark()) {\n            //设置当前channel状态为不可写，并触发firechannelwritabilitychanged事件\n            setunwritable(invokelater);\n        }\n    }\n\n}\n\n\n> volatile 关键字在 java 内存模型中只能保证变量的可见性，以及禁止指令重排序。但无法保证多线程更新的原子性，这里我们可以通过atomiclongfieldupdater 来帮助 totalpendingsize 字段实现原子性的更新。\n\n// 0表示channel可写，1表示channel不可写\nprivate volatile int unwritable;\n\nprivate static final atomicintegerfieldupdater<channeloutboundbuffer> unwritable_updater =\n    atomicintegerfieldupdater.newupdater(channeloutboundbuffer.class, \"unwritable\");\n\nprivate void setunwritable(boolean invokelater) {\n    for (;;) {\n        final int oldvalue = unwritable;\n        final int newvalue = oldvalue | 1;\n        if (unwritable_updater.compareandset(this, oldvalue, newvalue)) {\n            if (oldvalue == 0) {\n                //触发firechannelwritabilitychanged事件 表示当前channel变为不可写\n                firechannelwritabilitychanged(invokelater);\n            }\n            break;\n        }\n    }\n}\n\n\n当 channeloutboundbuffer 中的内存占用水位线 totalpendingsize 已经超过高水位线时，调用该方法将当前 channel 的状态设置为不可写状态。\n\n> unwritable == 0 表示当前channel可写，unwritable == 1 表示当前channel不可写。\n\nchannel 可以通过调用 iswritable 方法来判断自身当前状态是否可写。\n\npublic boolean iswritable() {\n    return unwritable == 0;\n}\n\n\n当 channel 的状态是首次从可写状态变为不可写状态时，就会在 channel 对应的 pipeline 中传播 channelwritabilitychanged 事件。\n\nprivate void firechannelwritabilitychanged(boolean invokelater) {\n    final channelpipeline pipeline = channel.pipeline();\n    if (invokelater) {\n        runnable task = firechannelwritabilitychangedtask;\n        if (task == null) {\n            firechannelwritabilitychangedtask = task = new runnable() {\n                @override\n                public void run() {\n                    pipeline.firechannelwritabilitychanged();\n                }\n            };\n        }\n        channel.eventloop().execute(task);\n    } else {\n        pipeline.firechannelwritabilitychanged();\n    }\n}\n\n\n用户可以在自定义的 channelhandler 中实现 channelwritabilitychanged 事件回调方法，来针对 channel 的可写状态变化做出不同的处理。\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelwritabilitychanged(channelhandlercontext ctx) throws exception {\n\n        if (ctx.channel().iswritable()) {\n            ...........当前channel可写.........\n        } else {\n            ...........当前channel不可写.........\n        }\n    }\n\n}\n\n\n到这里 write 事件在 pipeline 中的传播，笔者就为大家介绍完了，下面我们来看下另一个重要的 flush 事件的处理过程。\n\n\n# 4. flush\n\n从前面 netty 对 write 事件的处理过程中，我们可以看到当用户调用 ctx.write(msg) 方法之后，netty 只是将用户要发送的数据临时写到 channel 对应的待发送缓冲队列 channeloutboundbuffer 中，然而并不会将数据写入 socket 中。\n\n而当一次 read 事件完成之后，我们会调用 ctx.flush() 方法将 channeloutboundbuffer 中的待发送数据写入 socket 中的发送缓冲区中，从而将数据发送出去。\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelreadcomplete(channelhandlercontext ctx) {\n        //本次op_read事件处理完毕\n        ctx.flush();\n    }\n\n}\n\n\n\n# 4.1 flush事件的传播\n\npipeline结构.png\n\nflush 事件和 write 事件一样都是 oubound 事件，所以它们的传播方向都是从后往前在 pipeline 中传播。\n\n触发 flush 事件传播的同样也有两个方法：\n\n * channelhandlercontext.flush()：flush事件会从当前 channelhandler 开始在 pipeline 中向前传播直到 headcontext。\n * channelhandlercontext.channel().flush()：flush 事件会从 pipeline 的尾结点 tailcontext 处开始向前传播直到 headcontext。\n\nabstract class abstractchannelhandlercontext implements channelhandlercontext, resourceleakhint {\n\n    @override\n    public channelhandlercontext flush() {\n        //向前查找覆盖flush方法的outbound类型的channelhandler\n        final abstractchannelhandlercontext next = findcontextoutbound(mask_flush);\n        //获取执行channelhandler的executor,在初始化pipeline的时候设置，默认为reactor线程\n        eventexecutor executor = next.executor();\n        if (executor.ineventloop()) {\n            next.invokeflush();\n        } else {\n            tasks tasks = next.invoketasks;\n            if (tasks == null) {\n                next.invoketasks = tasks = new tasks(next);\n            }\n            safeexecute(executor, tasks.invokeflushtask, channel().voidpromise(), null, false);\n        }\n\n        return this;\n    }\n\n}\n\n\n这里的逻辑和 write 事件传播的逻辑基本一样，也是首先通过findcontextoutbound(mask_flush) 方法从当前 channelhandler 开始从 pipeline 中向前查找出第一个 channeloutboundhandler 类型的并且实现 flush 事件回调方法的 channelhandler 。注意这里传入的执行资格掩码为 mask_flush。\n\n执行channelhandler中事件回调方法的线程必须是通过pipeline#addlast(eventexecutorgroup group, channelhandler... handlers)为 channelhandler 指定的 executor。如果不指定，默认的 executor 为 channel 绑定的 reactor 线程。\n\n如果当前线程不是 channelhandler 指定的 executor，则需要将 invokeflush() 方法的调用封装成 task 交给指定的 executor 执行。\n\n\n# 4.1.1 触发nextchannelhandler的flush方法回调\n\nprivate void invokeflush() {\n    if (invokehandler()) {\n        invokeflush0();\n    } else {\n        //如果该channelhandler并没有加入到pipeline中则继续向前传递flush事件\n        flush();\n    }\n}\n\n\n这里和 write 事件的相关处理一样，首先也是需要调用 invokehandler() 方法来判断这个 nextchannelhandler 是否在 pipeline 中被正确的初始化。\n\n如果 nextchannelhandler 中的 handleradded 方法并没有被回调过，那么这里就只能跳过 nextchannelhandler，并调用 channelhandlercontext#flush 方法继续向前传播flush事件。\n\n如果 nextchannelhandler 中的 handleradded 方法已经被回调过，说明 nextchannelhandler 在 pipeline 中已经被正确的初始化好，则直接调用nextchannelhandler 的 flush 事件回调方法。\n\nprivate void invokeflush0() {\n    try {\n        ((channeloutboundhandler) handler()).flush(this);\n    } catch (throwable t) {\n        invokeexceptioncaught(t);\n    }\n}\n\n\n这里有一点和 write 事件处理不同的是，当调用 nextchannelhandler 的 flush 回调出现异常的时候，会触发 nextchannelhandler 的 exceptioncaught 回调。\n\nprivate void invokeexceptioncaught(final throwable cause) {\n    if (invokehandler()) {\n        try {\n            handler().exceptioncaught(this, cause);\n        } catch (throwable error) {\n            if (logger.isdebugenabled()) {\n                logger.debug(....相关日志打印......);\n            } else if (logger.iswarnenabled()) {\n                logger.warn(...相关日志打印......));\n            }\n        }\n    } else {\n        fireexceptioncaught(cause);\n    }\n}\n\n\n而其他 outbound 类事件比如 write 事件在传播的过程中发生异常，只是回调通知相关的 channelfuture。并不会触发 exceptioncaught 事件的传播。\n\n\n# 4.2 flush事件的处理\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)客户端channel pipeline结构.png\n\n最终flush事件会在pipeline中一直向前传播至headcontext中，并在 headcontext 里调用 channel 的 unsafe 类完成 flush 事件的最终处理逻辑。\n\nfinal class headcontext extends abstractchannelhandlercontext {\n\n    @override\n    public void flush(channelhandlercontext ctx) {\n        unsafe.flush();\n    }\n\n}\n\n\n下面就真正到了 netty 处理 flush 事件的地方。\n\nprotected abstract class abstractunsafe implements unsafe {\n\n    @override\n    public final void flush() {\n        asserteventloop();\n\n        channeloutboundbuffer outboundbuffer = this.outboundbuffer;\n        //channel以关闭\n        if (outboundbuffer == null) {\n            return;\n        }\n        //将flushedentry指针指向channeloutboundbuffer头结点，此时变为即将要flush进socket的数据队列\n        outboundbuffer.addflush();\n        //将待写数据写进socket\n        flush0();\n    }\n\n}\n\n\n# 4.2.1 channeloutboundbuffer#addflush\n\n![图片](data:image/svg+xml,%3c%3fxml version='1.0' encoding='utf-8'%3f%3e%3csvg width='1px' height='1px' viewbox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3ctitle%3e%3c/title%3e%3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3e%3cg transform='translate(-249.000000, -126.000000)' fill='%23ffffff'%3e%3crect x='249' y='126' width='1' height='1'%3e%3c/rect%3e%3c/g%3e%3c/g%3e%3c/svg%3e)channeloutboundbuffer结构.png\n\n这里就到了真正要发送数据的时候了，在 addflush 方法中会将 flushedentry 指针指向 unflushedentry 指针表示的第一个未被 flush 的 entry 节点。并将 unflushedentry 指针置为空，准备开始 flush 发送数据流程。\n\n> 此时 channeloutboundbuffer 由待发送数据的缓冲队列变为了即将要 flush 进 socket 的数据队列\n\n这样在 flushedentry 与 tailentry 之间的 entry 节点即为本次 flush 操作需要发送的数据范围。\n\npublic void addflush() {\n    entry entry = unflushedentry;\n    if (entry != null) {\n        if (flushedentry == null) {\n            flushedentry = entry;\n        }\n        do {\n            flushed ++;\n            //如果当前entry对应的write操作被用户取消，则释放msg，并降低channeloutboundbuffer水位线\n            if (!entry.promise.setuncancellable()) {\n                int pending = entry.cancel();\n                decrementpendingoutboundbytes(pending, false, true);\n            }\n            entry = entry.next;\n        } while (entry != null);\n\n        // all flushed so reset unflushedentry\n        unflushedentry = null;\n    }\n}\n\n\n在 flush 发送数据流程开始时，数据的发送流程就不能被取消了，在这之前我们都是可以通过 channelpromise 取消数据发送流程的。\n\n所以这里需要对 channeloutboundbuffer 中所有 entry 节点包裹的 channelpromise 设置为不可取消状态。\n\npublic interface promise<v> extends future<v> {\n\n    /**\n     * 设置当前future为不可取消状态\n     * \n     * 返回true的情况：\n     * 1：成功的将future设置为uncancellable\n     * 2：当future已经成功完成\n     * \n     * 返回false的情况：\n     * 1：future已经被取消，则不能在设置 uncancellable 状态\n     *\n     */\n    boolean setuncancellable();\n\n}\n\n\n如果这里的 setuncancellable() 方法返回 false 则说明在这之前用户已经将 channelpromise 取消掉了，接下来就需要调用 entry.cancel() 方法来释放为待发送数据 msg 分配的堆外内存。\n\nstatic final class entry {\n    //write操作是否被取消\n    boolean cancelled;\n\n    int cancel() {\n        if (!cancelled) {\n            cancelled = true;\n            int psize = pendingsize;\n\n            // release message and replace with an empty buffer\n            referencecountutil.saferelease(msg);\n            msg = unpooled.empty_buffer;\n\n            pendingsize = 0;\n            total = 0;\n            progress = 0;\n            bufs = null;\n            buf = null;\n            return psize;\n        }\n        return 0;\n    }\n\n}\n\n\n当 entry 对象被取消后，就需要减少 channeloutboundbuffer 的内存占用总量的水位线 totalpendingsize。\n\nprivate static final atomiclongfieldupdater<channeloutboundbuffer> total_pending_size_updater =\n    atomiclongfieldupdater.newupdater(channeloutboundbuffer.class, \"totalpendingsize\");\n\n//水位线指针.channeloutboundbuffer中的待发送数据的内存占用总量 : 所有entry对象本身所占用内存大小 + 所有待发送数据的大小\nprivate volatile long totalpendingsize;\n\nprivate void decrementpendingoutboundbytes(long size, boolean invokelater, boolean notifywritability) {\n    if (size == 0) {\n        return;\n    }\n\n    long newwritebuffersize = total_pending_size_updater.addandget(this, -size);\n    if (notifywritability && newwritebuffersize < channel.config().getwritebufferlowwatermark()) {\n        setwritable(invokelater);\n    }\n}\n\n\n当更新后的水位线低于低水位线 default_low_water_mark = 32 * 1024 时，就将当前 channel 设置为可写状态。\n\nprivate void setwritable(boolean invokelater) {\n    for (;;) {\n        final int oldvalue = unwritable;\n        final int newvalue = oldvalue & ~1;\n        if (unwritable_updater.compareandset(this, oldvalue, newvalue)) {\n            if (oldvalue != 0 && newvalue == 0) {\n                firechannelwritabilitychanged(invokelater);\n            }\n            break;\n        }\n    }\n}\n\n\n当 channel 的状态是第一次从不可写状态变为可写状态时，netty 会在 pipeline 中再次触发 channelwritabilitychanged 事件的传播。\n\n响应channelwritabilitychanged事件.png\n\n\n# 4.2.2 发送数据前的最后检查---flush0\n\nflush0 方法这里主要做的事情就是检查当 channel 的状态是否正常，如果 channel 状态一切正常，则调用 dowrite 方法发送数据。\n\nprotected abstract class abstractunsafe implements unsafe {\n\n    //是否正在进行flush操作\n    private boolean inflush0; \n\n    protected void flush0() {\n        if (inflush0) {\n            // avoid re-entrance\n            return;\n        }\n\n        final channeloutboundbuffer outboundbuffer = this.outboundbuffer;\n        //channel已经关闭或者outboundbuffer为空\n        if (outboundbuffer == null || outboundbuffer.isempty()) {\n            return;\n        }\n\n        inflush0 = true;\n\n        if (!isactive()) {\n            try {\n                if (!outboundbuffer.isempty()) {\n                    if (isopen()) {\n                        //当前channel处于disconnected状态  通知promise 写入失败 并触发channelwritabilitychanged事件\n                        outboundbuffer.failflushed(new notyetconnectedexception(), true);\n                    } else {\n                        //当前channel处于关闭状态 通知promise 写入失败 但不触发channelwritabilitychanged事件\n                        outboundbuffer.failflushed(newclosedchannelexception(initialclosecause, \"flush0()\"), false);\n                    }\n                }\n            } finally {\n                inflush0 = false;\n            }\n            return;\n        }\n\n        try {\n            //写入socket\n            dowrite(outboundbuffer);\n        } catch (throwable t) {\n            handlewriteerror(t);\n        } finally {\n            inflush0 = false;\n        }\n    }\n\n}\n\n\n * outboundbuffer == null || outboundbuffer.isempty() ：如果 channel 已经关闭了或者对应写缓冲区中没有任何数据，那么就停止发送流程，直接 return。\n * !isactive() ：如果当前channel处于非活跃状态，则需要调用 outboundbuffer#failflushed 通知 channeloutboundbuffer 中所有待发送操作对应的 channelpromise 向用户线程报告发送失败。并将待发送数据 entry 对象从 channeloutboundbuffer 中删除，并释放待发送数据空间，回收 entry 对象实例。\n\n还记得我们在?《netty如何高效接收网络连接》一文中提到过的 niosocketchannel 的 active 状态有哪些条件吗？？\n\n@override\npublic boolean isactive() {\n    socketchannel ch = javachannel();\n    return ch.isopen() && ch.isconnected();\n}\n\n\nniosocketchannel 处于 active 状态的条件必须是当前 niosocketchannel 是 open 的同时处于 connected 状态。\n\n * !isactive() && isopen()：说明当前 channel 处于 disconnected 状态，这时通知给用户 channelpromise 的异常类型为 notyetconnectedexception ,并释放所有待发送数据占用的堆外内存，如果此时内存占用量低于低水位线，则设置 channel 为可写状态，并触发 channelwritabilitychanged 事件。\n\n> 当 channel 处于 disconnected 状态时，用户可以进行 write 操作但不能进行 flush 操作。\n\n * !isactive() && !isopen() ：说明当前 channel 处于关闭状态，这时通知给用户 channelpromise 的异常类型为 newclosedchannelexception ，因为 channel 已经关闭，所以这里并不会触发 channelwritabilitychanged 事件。\n * 当 channel 的这些异常状态校验通过之后，则调用 dowrite 方法将 channeloutboundbuffer 中的待发送数据写进底层 socket 中。\n\n# 4.2.2.1 channeloutboundbuffer#failflushed\n\npublic final class channeloutboundbuffer {\n\n    private boolean infail;\n\n    void failflushed(throwable cause, boolean notify) {\n        if (infail) {\n            return;\n        }\n\n        try {\n            infail = true;\n            for (;;) {\n                if (!remove0(cause, notify)) {\n                    break;\n                }\n            }\n        } finally {\n            infail = false;\n        }\n    }\n}\n\n\n该方法用于在 netty 在发送数据的时候，如果发现当前 channel 处于非活跃状态，则将 channeloutboundbuffer 中 flushedentry 与tailentry 之间的 entry 对象节点全部删除，并释放发送数据占用的内存空间，同时回收 entry 对象实例。\n\n# 4.2.2.2 channeloutboundbuffer#remove0\n\nprivate boolean remove0(throwable cause, boolean notifywritability) {\n    entry e = flushedentry;\n    if (e == null) {\n        //清空当前reactor线程缓存的所有待发送数据\n        clearniobuffers();\n        return false;\n    }\n    object msg = e.msg;\n\n    channelpromise promise = e.promise;\n    int size = e.pendingsize;\n    //从channeloutboundbuffer中删除该entry节点\n    removeentry(e);\n\n    if (!e.cancelled) {\n        // only release message, fail and decrement if it was not canceled before.\n        //释放msg所占用的内存空间\n        referencecountutil.saferelease(msg);\n        //编辑promise发送失败，并通知相应的lisener\n        safefail(promise, cause);\n        //由于msg得到释放，所以需要降低channeloutboundbuffer中的内存占用水位线，并根据notifywritability决定是否触发channelwritabilitychanged事件\n        decrementpendingoutboundbytes(size, false, notifywritability);\n    }\n\n    // recycle the entry\n    //回收entry实例对象\n    e.recycle();\n\n    return true;\n}\n\n\n当一个 entry 节点需要从 channeloutboundbuffer 中清除时，netty 需要释放该 entry 节点中包裹的发送数据 msg 所占用的内存空间。并标记对应的 promise 为失败同时通知对应的 listener ，由于 msg 得到释放，所以需要降低 channeloutboundbuffer 中的内存占用水位线，并根据 boolean notifywritability 决定是否触发 channelwritabilitychanged 事件。最后需要将该 entry 实例回收至 recycler 对象池中。\n\n\n# 5. 终于开始真正地发送数据了！\n\n来到这里我们就真正进入到了 netty 发送数据的核心处理逻辑，在?《netty如何高效接收网络数据》一文中，笔者详细介绍了 netty 读取数据的核心流程，netty 会在一个 read loop 中不断循环读取 socket 中的数据直到数据读取完毕或者读取次数已满 16 次，当循环读取了 16 次还没有读取完毕时，netty 就不能在继续读了，因为 netty 要保证 reactor 线程可以均匀的处理注册在它上边的所有 channel 中的 io 事件。剩下未读取的数据等到下一次 read loop 在开始读取。\n\n除此之外，在每次 read loop 开始之前，netty 都会分配一个初始化大小为 2048 的 directbytebuffer 来装载从 socket 中读取到的数据，当整个 read loop 结束时，会根据本次读取数据的总量来判断是否为该 directbytebuffer 进行扩容或者缩容，目的是在下一次 read loop 的时候可以为其分配一个容量大小合适的 directbytebuffer 。\n\n其实 netty 对发送数据的处理和对读取数据的处理核心逻辑都是一样的，这里大家可以将这两篇文章结合对比着看。\n\n但发送数据的细节会多一些，也会更复杂一些，由于这块逻辑整体稍微比较复杂，所以我们接下来还是分模块进行解析：\n\n\n# 5.1 发送数据前的准备工作\n\n@override\nprotected void dowrite(channeloutboundbuffer in) throws exception {\n    //获取niosocketchannel中封装的jdk nio底层socketchannel\n    socketchannel ch = javachannel();\n    //最大写入次数 默认为16 目的是为了保证subreactor可以平均的处理注册其上的所有channel\n    int writespincount = config().getwritespincount();\n    do {\n        if (in.isempty()) {\n            // 如果全部数据已经写完 则移除op_write事件并直接退出writeloop\n            clearopwrite();             \n            return;\n        }\n\n        //  so_sndbuf设置的发送缓冲区大小 * 2 作为 最大写入字节数  293976 = 146988 << 1\n        int maxbytespergatheringwrite = ((niosocketchannelconfig) config).getmaxbytespergatheringwrite();\n        // 将channeloutboundbuffer中缓存的directbuffer转换成jdk nio 的 bytebuffer\n        bytebuffer[] niobuffers = in.niobuffers(1024, maxbytespergatheringwrite);\n        // channeloutboundbuffer中总共的directbuffer数\n        int niobuffercnt = in.niobuffercount();\n\n        switch (niobuffercnt) {\n                .........向底层jdk nio socketchannel发送数据.........\n        }\n    } while (writespincount > 0);\n\n    ............处理本轮write loop未写完的情况.......\n}\n\n\n这部分内容为 netty 开始发送数据之前的准备工作：\n\n# 5.1.1 获取write loop最大发送循环次数\n\n从当前 niosocketchannel 的配置类 niosocketchannelconfig 中获取 write loop 最大循环写入次数，默认为 16。但也可以通过下面的方式进行自定义设置。\n\nserverbootstrap b = new serverbootstrap();\nb.group(bossgroup, workergroup)\n    .......\n    .childoption(channeloption.write_spin_count,自定义数值)\n\n\n# 5.1.2 处理在一轮write loop中就发送完数据的情况\n\n进入 write loop 之后首先需要判断当前 channeloutboundbuffer 中的数据是否已经写完了 in.isempty()) ，如果全部写完就需要清除当前 channel 在 reactor 上注册的 op_write 事件。\n\n> 这里大家可能会有疑问，目前我们还没有注册 op_write 事件到 reactor 上，为啥要清除呢？别着急，笔者会在后面为大家揭晓答案。\n\n# 5.1.3 获取本次write loop 最大允许发送字节数\n\n从 channelconfig 中获取本次 write loop 最大允许发送的字节数 maxbytespergatheringwrite 。初始值为 so_sndbuf大小 * 2 = 293976 = 146988 << 1，最小值为 2048。\n\nprivate final class niosocketchannelconfig extends defaultsocketchannelconfig {\n    //293976 = 146988 << 1\n    //so_sndbuf设置的发送缓冲区大小 * 2 作为 最大写入字节数\n    //最小值为2048 \n    private volatile int maxbytespergatheringwrite = integer.max_value;\n    private niosocketchannelconfig(niosocketchannel channel, socket javasocket) {\n        super(channel, javasocket);\n        calculatemaxbytespergatheringwrite();\n    }\n\n    private void calculatemaxbytespergatheringwrite() {\n        // 293976 = 146988 << 1\n        // so_sndbuf设置的发送缓冲区大小 * 2 作为 最大写入字节数\n        int newsendbuffersize = getsendbuffersize() << 1;\n        if (newsendbuffersize > 0) {\n            setmaxbytespergatheringwrite(newsendbuffersize);\n        }\n    }\n}\n\n\n我们可以通过如下的方式自定义配置 socket 发送缓冲区大小。\n\nserverbootstrap b = new serverbootstrap();\nb.group(bossgroup, workergroup)\n    .......\n    .childoption(channeloption.so_sndbuf,自定义数值)\n\n\n# 5.1.4 将待发送数据转换成 jdk nio bytebuffer\n\n由于最终 netty 会调用 jdk nio 的 socketchannel 发送数据，所以这里需要首先将当前 channel 中的写缓冲队列 channeloutboundbuffer 里存储的 directbytebuffer（ netty 中的 bytebuffer 实现）转换成 jdk nio 的 bytebuffer 类型。最终将转换后的待发送数据存储在 bytebuffer[] niobuffers 数组中。这里通过调用 channeloutboundbuffer#niobuffers 方法完成以上 bytebuffer 类型的转换。\n\n * maxbytespergatheringwrite：表示本次 write loop 中最多从 channeloutboundbuffer 中转换 maxbytespergatheringwrite 个字节出来。也就是本次 write loop 最多能发送多少字节。\n * 1024: 本次 write loop 最多转换 1024 个 bytebuffer（ jdk nio 实现）。也就是说本次 write loop 最多批量发送多少个 bytebuffer 。\n\n通过 channeloutboundbuffer#niobuffercount() 获取本次 write loop 总共需要发送的 bytebuffer 数量 niobuffercnt 。注意这里已经变成了 jdk nio 实现的 bytebuffer 了。\n\n> 详细的 bytebuffer 类型转换过程，笔者会在专门讲解 buffer 设计的时候为大家全面细致地讲解，这里我们还是主要聚焦于发送数据流程的主线。\n\n当做完这些发送前的准备工作之后，接下来 netty 就开始向 jdk nio socketchannel 发送这些已经转换好的 jdk nio bytebuffer 了。\n\n\n# 5.2 向jdk nio socketchannel发送数据\n\nflush流程.png\n\n@override\nprotected void dowrite(channeloutboundbuffer in) throws exception {      \n    socketchannel ch = javachannel();\n    int writespincount = config().getwritespincount();\n    do {\n\n        .........将待发送数据转换到jdk nio bytebuffer中.........\n\n            //本次write loop中需要发送的 jdk bytebuffer个数\n            int niobuffercnt = in.niobuffercount();\n\n        switch (niobuffercnt) {\n            case 0:\n                //这里主要是针对 网络传输文件数据 的处理 fileregion                 \n                writespincount -= dowrite0(in);\n                break;\n            case 1: {\n                .........处理单个niobytebuffer发送的情况......\n                    break;\n            }\n            default: {\n                .........批量处理多个niobytebuffers发送的情况......\n                    break;\n            }            \n        }\n    } while (writespincount > 0);\n\n    ............处理本轮write loop未写完的情况.......\n}\n\n\n这里大家可能对 niobuffercnt == 0 的情况比较有疑惑，明明之前已经校验过channeloutboundbuffer 不为空了，为什么这里从 channeloutboundbuffer 中获取到的 niobuffer 个数依然为 0 呢？\n\n在前边我们介绍 netty 对 write 事件的处理过程时提过， channeloutboundbuffer 中只支持 bytebuf 类型和 fileregion 类型，其中 bytebuf 类型用于装载普通的发送数据，而 fileregion 类型用于通过零拷贝的方式网络传输文件。\n\n而这里 channeloutboundbuffer 虽然不为空，但是装载的 niobytebuffer 个数却为 0 说明 channeloutboundbuffer 中装载的是 fileregion 类型，当前正在进行网络文件的传输。\n\ncase 0 的分支主要就是用于处理网络文件传输的情况。\n\n# 5.2.1 零拷贝发送网络文件\n\nprotected final int dowrite0(channeloutboundbuffer in) throws exception {\n    object msg = in.current();\n    if (msg == null) {\n        return 0;\n    }\n    return dowriteinternal(in, in.current());\n}\n\n\n这里需要特别注意的是用于文件传输的方法 dowriteinternal 中的返回值，理解这些返回值的具体情况有助于我们理解后面 write loop 的逻辑走向。\n\nprivate int dowriteinternal(channeloutboundbuffer in, object msg) throws exception {\n\n    if (msg instanceof bytebuf) {\n\n        ..............忽略............\n\n    } else if (msg instanceof fileregion) {\n        fileregion region = (fileregion) msg;\n        //文件已经传输完毕\n        if (region.transferred() >= region.count()) {\n            in.remove();\n            return 0;\n        }\n\n        //零拷贝的方式传输文件\n        long localflushedamount = dowritefileregion(region);\n        if (localflushedamount > 0) {\n            in.progress(localflushedamount);\n            if (region.transferred() >= region.count()) {\n                in.remove();\n            }\n            return 1;\n        }\n    } else {\n        // should not reach here.\n        throw new error();\n    }\n    //走到这里表示 此时socket已经写不进去了 退出writeloop，注册op_write事件\n    return write_status_sndbuf_full;\n}\n\n\n最终会在 dowritefileregion 方法中通过 filechannel#transferto 方法底层用到的系统调用为 sendfile 实现零拷贝网络文件的传输。\n\npublic class niosocketchannel extends abstractniobytechannel implements io.netty.channel.socket.socketchannel {\n\n   @override\n    protected long dowritefileregion(fileregion region) throws exception {\n        final long position = region.transferred();\n        return region.transferto(javachannel(), position);\n    }\n\n}\n\n\n> 关于 netty 中涉及到的零拷贝，笔者会有一篇专门的文章为大家讲解，本文的主题我们还是先聚焦于把发送流程的主线打通。\n\n我们继续回到发送数据流程主线上来~~\n\n                case 0:\n                    //这里主要是针对 网络传输文件数据 的处理 fileregion                 \n                    writespincount -= dowrite0(in);\n                    break;\n\n\n * region.transferred() >= region.count() ：表示当前 fileregion 中的文件数据已经传输完毕。那么在这种情况下本次 write loop 没有写入任何数据到 socket ，所以返回 0 ，writespincount - 0 意思就是本次 write loop 不算，继续循环。\n * localflushedamount > 0 ：表示本 write loop 中写入了一些数据到 socket 中，会有返回 1，writespincount - 1 减少一次 write loop 次数。\n * localflushedamount <= 0 ：表示当前 socket 发送缓冲区已满，无法写入数据，那么就返回 write_status_sndbuf_full = integer.max_value。writespincount - integer.max_value 必然是负数，直接退出循环，向 reactor 注册 op_write 事件并退出 flush 流程。等 socket 发送缓冲区可写了，reactor 会通知 channel 继续发送文件数据。记住这里，我们后面还会提到。\n\n# 5.2.2 发送普通数据\n\n剩下两个 case 1 和 default 分支主要就是处理 bytebuffer 装载的普通数据发送逻辑。\n\n其中 case 1 表示当前 channel 的 channeloutboundbuffer 中只包含了一个 niobytebuffer 的情况。\n\ndefault 表示当前 channel 的 channeloutboundbuffer 中包含了多个 niobytebuffers 的情况。\n\n@override\nprotected void dowrite(channeloutboundbuffer in) throws exception {      \n    socketchannel ch = javachannel();\n    int writespincount = config().getwritespincount();\n    do {\n\n        .........将待发送数据转换到jdk nio bytebuffer中.........\n\n            //本次write loop中需要发送的 jdk bytebuffer个数\n            int niobuffercnt = in.niobuffercount();\n\n        switch (niobuffercnt) {\n            case 0:\n                ..........处理网络文件传输.........\n                    case 1: {\n                        bytebuffer buffer = niobuffers[0];\n                        int attemptedbytes = buffer.remaining();\n                        final int localwrittenbytes = ch.write(buffer);\n                        if (localwrittenbytes <= 0) {\n                            //如果当前socket发送缓冲区满了写不进去了，则注册op_write事件，等待socket发送缓冲区可写时 在写\n                            // subreactor在处理op_write事件时，直接调用flush方法\n                            incompletewrite(true);\n                            return;\n                        }\n                        //根据当前实际写入情况调整 maxbytespergatheringwrite数值\n                        adjustmaxbytespergatheringwrite(attemptedbytes, localwrittenbytes, maxbytespergatheringwrite);\n                        //如果channeloutboundbuffer中的某个entry被全部写入 则删除该entry\n                        // 如果entry被写入了一部分 还有一部分未写入  则更新entry中的readindex 等待下次writeloop继续写入\n                        in.removebytes(localwrittenbytes);\n                        --writespincount;\n                        break;\n                    }\n            default: {\n                // channeloutboundbuffer中总共待写入数据的字节数\n                long attemptedbytes = in.niobuffersize();\n                //批量写入\n                final long localwrittenbytes = ch.write(niobuffers, 0, niobuffercnt);\n                if (localwrittenbytes <= 0) {\n                    incompletewrite(true);\n                    return;\n                }\n                //根据实际写入情况调整一次写入数据大小的最大值\n                // maxbytespergatheringwrite决定每次可以从channeloutboundbuffer中获取多少发送数据\n                adjustmaxbytespergatheringwrite((int) attemptedbytes, (int) localwrittenbytes,\n                                                maxbytespergatheringwrite);\n                //移除全部写完的buffer，如果只写了部分数据则更新buffer的readerindex，下一个writeloop写入\n                in.removebytes(localwrittenbytes);\n                --writespincount;\n                break;\n            }            \n        }\n    } while (writespincount > 0);\n\n    ............处理本轮write loop未写完的情况.......\n}\n\n\ncase 1 和 default 这两个分支在处理发送数据时的逻辑是一样的，唯一的区别就是 case 1 是处理单个 niobytebuffer 的发送，而 default 分支是批量处理多个 niobytebuffers 的发送。\n\n下面笔者就以经常被触发到的 default 分支为例来为大家讲述 netty 在处理数据发送时的逻辑细节：\n\n 1. 首先从当前 niosocketchannel 中的 channeloutboundbuffer 中获取本次 write loop 需要发送的字节总量 attemptedbytes 。这个 niobuffersize 是在前边介绍 channeloutboundbuffer#niobuffers 方法转换 jdk nio bytebuffer 类型时被计算出来的。\n 2. 调用 jdk nio 原生 socketchannel 批量发送 niobuffers 中的数据。并获取到本次 write loop 一共批量发送了多少字节 localwrittenbytes 。\n\n    /**\n     * @throws  notyetconnectedexception\n     *          if this channel is not yet connected\n     */\n    public abstract long write(bytebuffer[] srcs, int offset, int length)\n        throws ioexception;\n\n\n 1. localwrittenbytes <= 0 表示当前 socket 的写缓存区 send_buf 已满，写不进数据了。那么就需要向当前 niosocketchannel 对应的 reactor 注册 op_write 事件，并停止当前 flush 流程。当 socket 的写缓冲区有容量可写时，epoll 会通知 reactor 线程继续写入。\n\n    protected final void incompletewrite(boolean setopwrite) {\n        // did not write completely.\n        if (setopwrite) {\n            //这里处理还没写满16次 但是socket缓冲区已满写不进去的情况 注册write事件\n            //什么时候socket可写了， epoll会通知reactor线程继续写\n            setopwrite();\n        } else {\n              ...........目前还不需要关注这里.......\n        }\n    }\n\n\n向 reactor 注册 op_write 事件：\n\n    protected final void setopwrite() {\n        final selectionkey key = selectionkey();\n        if (!key.isvalid()) {\n            return;\n        }\n        final int interestops = key.interestops();\n        if ((interestops & selectionkey.op_write) == 0) {\n            key.interestops(interestops | selectionkey.op_write);\n        }\n    }\n\n\n> 关于通过位运算来向 io 事件集合 interestops 添加监听 io 事件的用法，在前边的文章中，笔者已经多次介绍过了，这里不再重复。\n\n 1. 根据本次 write loop 向 socket 写缓冲区写入数据的情况，来调整下次 write loop 最大写入字节数。maxbytespergatheringwrite 决定每次 write loop 可以从 channeloutboundbuffer 中最多获取多少发送数据。初始值为 so_sndbuf大小 * 2 = 293976 = 146988 << 1，最小值为 2048。\n\n    public static final int max_bytes_per_gathering_write_attempted_low_threshold = 4096;\n\n    private void adjustmaxbytespergatheringwrite(int attempted, int written, int oldmaxbytespergatheringwrite) {\n        if (attempted == written) {\n            if (attempted << 1 > oldmaxbytespergatheringwrite) {\n                ((niosocketchannelconfig) config).setmaxbytespergatheringwrite(attempted << 1);\n            }\n        } else if (attempted > max_bytes_per_gathering_write_attempted_low_threshold && written < attempted >>> 1) {\n            ((niosocketchannelconfig) config).setmaxbytespergatheringwrite(attempted >>> 1);\n        }\n    }\n\n\n> 由于操作系统会动态调整 so_sndbuf 的大小，所以这里 netty 也需要根据操作系统的动态调整做出相应的调整，目的是尽量多的去写入数据。\n\nattempted == written 表示本次 write loop 尝试写入的数据能全部写入到 socket 的写缓冲区中，那么下次 write loop 就应该尝试去写入更多的数据。\n\n那么这里的更多具体是多少呢？\n\nnetty 会将本次写入的数据量 written 扩大两倍，如果扩大两倍后的写入量大于本次 write loop 的最大限制写入量 maxbytespergatheringwrite，说明用户的写入需求很猛烈，netty当然要满足这样的猛烈需求，那么就将当前 niosocketchannelconfig 中的 maxbytespergatheringwrite 更新为本次 write loop 两倍的写入量大小。\n\n在下次 write loop 写入数据的时候，就会尝试从 channeloutboundbuffer 中加载最多 written * 2 大小的字节数。\n\n如果扩大两倍后的写入量依然小于等于本次 write loop 的最大限制写入量 maxbytespergatheringwrite，说明用户的写入需求还不是很猛烈，netty 继续维持本次 maxbytespergatheringwrite 数值不变。\n\n如果本次写入的数据还不及尝试写入数据的 1 / 2 ：written < attempted >>> 1。说明当前 socket 写缓冲区的可写容量不是很多了，下一次 write loop 就不要写这么多了尝试减少下次写入的量将下次 write loop 要写入的数据减小为 attempted 的1 / 2。当然也不能无限制的减小，最小值不能低于 2048。\n\n> 这里可以结合笔者前边的文章?《一文聊透bytebuffer动态自适应扩缩容机制》中介绍到的 read loop 场景中的扩缩容一起对比着看。\n\n> read loop 中的扩缩容触发时机是在一个完整的 read loop 结束时候触发。而 write loop 中扩缩容的触发时机是在每次 write loop 发送完数据后，立即触发扩缩容判断。\n\n 1. 当本次 write loop 批量发送完 channeloutboundbuffer 中的数据之后，最后调用in.removebytes(localwrittenbytes) 从 channeloutboundbuffer 中移除全部写完的 entry ，如果只发送了 entry 的部分数据则更新 entry 对象中封装的 directbytebuffer 的 readerindex，等待下一次 write loop 写入。\n\n到这里，write loop 中的发送数据的逻辑就介绍完了，接下来 netty 会在 write loop 中循环地发送数据直到写满 16 次或者数据发送完毕。\n\n还有一种退出 write loop 的情况就是当 socket 中的写缓冲区满了，无法在写入时。netty 会退出 write loop 并向 reactor 注册 op_write 事件。\n\n但这其中还隐藏着一种情况就是如果 write loop 已经写满 16 次但还没写完数据并且此时 socket 写缓冲区还没有满，还可以继续在写。那 netty 会如何处理这种情况呢？\n\n\n# 6. 处理socket可写但已经写满16次还没写完的情况\n\n    @override\n    protected void dowrite(channeloutboundbuffer in) throws exception {      \n        socketchannel ch = javachannel();\n        int writespincount = config().getwritespincount();\n        do {\n  \n            .........将待发送数据转换到jdk nio bytebuffer中.........\n\n            int niobuffercnt = in.niobuffercount();\n\n            switch (niobuffercnt) {\n                case 0:\n                    //这里主要是针对 网络传输文件数据 的处理 fileregion                 \n                    writespincount -= dowrite0(in);\n                    break;\n                case 1: {\n                      .....发送单个niobuffer....\n                }\n                default: {\n                      .....批量发送多个niobuffers......\n                }            \n            }\n        } while (writespincount > 0);\n        \n        //处理write loop结束 但数据还没写完的情况\n        incompletewrite(writespincount < 0);\n    }\n\n\n当 write loop 结束后，这时 writespincount 的值会有两种情况：\n\n * writespincount < 0：这种情况有点不好理解，我们在介绍 netty 通过零拷贝的方式传输网络文件也就是这里的 case 0 分支逻辑时，详细介绍了 dowrite0 方法的几种返回值，当 netty 在传输文件的过程中发现 socket 缓冲区已满无法在继续写入数据时，会返回 write_status_sndbuf_full = integer.max_value，这就使得 writespincount的值 < 0。随后 break 掉 write loop 来到 incompletewrite(writespincount < 0) 方法中，最后会在 incompletewrite 方法中向 reactor 注册 op_write 事件。当 socket 缓冲区变得可写时，epoll 会通知 reactor 线程继续发送文件。\n\n    protected final void incompletewrite(boolean setopwrite) {\n        // did not write completely.\n        if (setopwrite) {\n            //这里处理还没写满16次 但是socket缓冲区已满写不进去的情况 注册write事件\n            // 什么时候socket可写了， epoll会通知reactor线程继续写\n            setopwrite();\n        } else {\n            ..............\n        }\n    }\n\n\n * writespincount == 0：这种情况很好理解，就是已经写满了 16 次，但是还没写完，同时 socket 的写缓冲区未满，还可以继续写入。这种情况下即使 socket 还可以继续写入，netty 也不会再去写了，因为执行 flush 操作的是 reactor 线程，而 reactor 线程负责执行注册在它上边的所有 channel 的 io 操作，netty 不会允许 reactor 线程一直在一个 channel 上执行 io 操作，reactor 线程的执行时间需要均匀的分配到每个 channel 上。所以这里 netty 会停下，转而去处理其他 channel 上的 io 事件。\n\n那么还没写完的数据，netty会如何处理呢？\n\n    protected final void incompletewrite(boolean setopwrite) {\n        // did not write completely.\n        if (setopwrite) {\n            //这里处理还没写满16次 但是socket缓冲区已满写不进去的情况 注册write事件\n            // 什么时候socket可写了， epoll会通知reactor线程继续写\n            setopwrite();\n        } else {\n            //这里处理的是socket缓冲区依然可写，但是写了16次还没写完，这时就不能在写了，reactor线程需要处理其他channel上的io事件\n\n            //因为此时socket是可写的，必须清除op_write事件，否则会一直不停地被通知\n            clearopwrite();\n            //如果本次writeloop还没写完，则提交flushtask到reactor           \n            eventloop().execute(flushtask);\n\n        }\n\n\n这个方法的 if 分支逻辑，我们在介绍do {.....}while()循环体 write loop 中发送逻辑时已经提过，在 write loop 循环发送数据的过程中，如果发现 socket 缓冲区已满，无法写入数据时（ localwrittenbytes <= 0），则需要向 reactor 注册 op_write 事件，等到 socket 缓冲区变为可写状态时，epoll 会通知 reactor 线程继续写入剩下的数据。\n\n       do {\n            .........将待发送数据转换到jdk nio bytebuffer中.........\n\n            int niobuffercnt = in.niobuffercount();\n\n            switch (niobuffercnt) {\n                case 0:\n                    writespincount -= dowrite0(in);\n                    break;\n                case 1: {\n                    .....发送单个niobuffer....\n                    final int localwrittenbytes = ch.write(buffer);\n                    if (localwrittenbytes <= 0) {\n                        incompletewrite(true);\n                        return;\n                    }\n                    .................省略..............\n                    break;\n                }\n                default: {\n                    .....批量发送多个niobuffers......\n                    final long localwrittenbytes = ch.write(niobuffers, 0, niobuffercnt);\n                    if (localwrittenbytes <= 0) {\n                        incompletewrite(true);\n                        return;\n                    }\n                    .................省略..............\n                    break;\n                }\n            }\n        } while (writespincount > 0);\n\n\n> 注意 if 分支处理的情况是还没写满 16 次，但是 socket 缓冲区已满，无法写入的情况。\n\n而 else 分支正是处理我们这里正在讨论的情况即 socket 缓冲区是可写的，但是已经写满 16 次，在本轮 write loop 中不能再继续写入的情况。\n\n这时 netty 会将 channel 中剩下的待写数据的 flush 操作封装程 flushtask，丢进 reactor 的普通任务队列中，等待 reactor 执行完其他 channel 上的 io 操作后在回过头来执行未写完的 flush 任务。\n\n> 忘记 reactor 整体运行逻辑的同学，可以在回看下笔者的这篇文章?《一文聊透netty核心引擎reactor的运转架构》\n\n    private final runnable flushtask = new runnable() {\n        @override\n        public void run() {\n            ((abstractniounsafe) unsafe()).flush0();\n        }\n    };\n\n\n这里我们看到 flushtask 中的任务是直接再次调用 flush0 继续回到发送数据的逻辑流程中。\n\n细心的同学可能会有疑问，为什么这里不在继续注册 op_write 事件而是通过向 reactor 提交一个 flushtask 来完成 channel 中剩下数据的写入呢？\n\n原因是这里我们讲的 else 分支是用来处理 socket 缓冲区未满还是可写的，但是由于用户本次要发送的数据太多，导致写了 16 次还没写完的情形。\n\n既然当前 socket 缓冲区是可写的，我们就不能注册 op_write 事件，否则这里一直会不停地收到 epoll 的通知。因为 jdk nio selector 默认的是 epoll 的水平触发。\n\n> 忘记水平触发和边缘触发这两种 epoll 工作模式的同学，可以在回看下笔者的这篇文章?《聊聊netty那些事儿之从内核角度看io模型》\n\n所以这里只能向 reactor 提交 flushtask 来继续完成剩下数据的写入，而不能注册 op_write 事件。\n\n> 注意：只有当 socket 缓冲区已满导致无法写入时，netty 才会去注册 op_write 事件。这和我们之前介绍的 op_accept 事件和 op_read 事件的注册时机是不同的。\n\n这里大家可能还会有另一个疑问，就是为什么在向 reactor 提交 flushtask 之前需要清理 op_write 事件呢？ 我们并没有注册 op_write 事件呀~~\n\n    protected final void incompletewrite(boolean setopwrite) {\n        if (setopwrite) {\n            ......省略......\n        } else {\n            clearopwrite();  \n            eventloop().execute(flushtask);\n        }\n\n\n在为大家解答这个疑问之前，笔者先为大家介绍下 netty 是如何处理 op_write 事件的，当大家明白了 op_write 事件的处理逻辑后，这个疑问就自然解开了。\n\n\n# 7. op_write事件的处理\n\n在?《一文聊透netty核心引擎reactor的运转架构》一文中，我们介绍过，当 reactor 监听到 channel 上有 io 事件发生后，最终会在 processselectedkey 方法中处理 channel 上的 io 事件，其中 op_accept 事件和 op_read 事件的处理过程，笔者已经在之前的系列文章中介绍过了，这里我们聚焦于 op_write 事件的处理。\n\npublic final class nioeventloop extends singlethreadeventloop {\n\n   private void processselectedkey(selectionkey k, abstractniochannel ch) {\n        final abstractniochannel.niounsafe unsafe = ch.unsafe();\n\n        .............省略.......\n\n        try {\n            int readyops = k.readyops();\n\n            if ((readyops & selectionkey.op_connect) != 0) {\n                  ......处理connect事件......\n            }\n\n            if ((readyops & selectionkey.op_write) != 0) {\n                ch.unsafe().forceflush();\n            }\n \n            if ((readyops & (selectionkey.op_read | selectionkey.op_accept)) != 0 || readyops == 0) {\n               ........处理accept和read事件.........\n            }\n        } catch (cancelledkeyexception ignored) {\n            unsafe.close(unsafe.voidpromise());\n        }\n    }\n\n}\n\n\n这里我们看到当 op_write 事件发生后，netty 直接调用 channel 的 forceflush 方法。\n\n       @override\n        public final void forceflush() {\n            // directly call super.flush0() to force a flush now\n            super.flush0();\n        }\n\n\n其实 forceflush 方法中并没有什么特殊的逻辑，直接调用 flush0 方法再次发起 flush 操作继续 channel 中剩下数据的写入。\n\n    @override\n    protected void dowrite(channeloutboundbuffer in) throws exception {      \n        socketchannel ch = javachannel();\n        int writespincount = config().getwritespincount();\n        do {\n            if (in.isempty()) {\n                clearopwrite();\n                return;\n            }\n            .........将待发送数据转换到jdk nio bytebuffer中.........\n\n            int niobuffercnt = in.niobuffercount();\n\n            switch (niobuffercnt) {\n                case 0:\n                      ......传输网络文件........\n                case 1: {\n                      .....发送单个niobuffer....\n                }\n                default: {\n                      .....批量发送多个niobuffers......\n                }            \n            }\n        } while (writespincount > 0);\n        \n        //处理write loop结束 但数据还没写完的情况\n        incompletewrite(writespincount < 0);\n    }\n\n\n注意这里的 clearopwrite() 方法，由于 channel 上的 op_write 事件就绪，表明此时 socket 缓冲区变为可写状态，从而 reactor 线程再次来到了 flush 流程中。\n\n当 channeloutboundbuffer 中的数据全部写完后 in.isempty() ，就需要清理 op_write 事件，因为此时 socket 缓冲区是可写的，这种情况下当数据全部写完后，就需要取消对 op_write 事件的监听，否则 epoll 会不断的通知 reactor。\n\n同理在 incompletewrite 方法的 else 分支也需要执行 clearopwrite() 方法取消对 op_write 事件的监听。\n\n    protected final void incompletewrite(boolean setopwrite) {\n\n        if (setopwrite) {\n            // 这里处理还没写满16次 但是socket缓冲区已满写不进去的情况 注册write事件\n            // 什么时候socket可写了， epoll会通知reactor线程继续写\n            setopwrite();\n        } else {\n            // 必须清除op_write事件，此时socket对应的缓冲区依然是可写的，只不过当前channel写够了16次，被subreactor限制了。\n            // 这样subreactor可以腾出手来处理其他channel上的io事件。这里如果不清除op_write事件，则会一直被通知。\n            clearopwrite();\n\n            //如果本次writeloop还没写完，则提交flushtask到subreactor\n            //释放subreactor让其可以继续处理其他channel上的io事件\n            eventloop().execute(flushtask);\n        }\n    }\n\n\n\n# 8. writeandflush\n\n在我们讲完了 write 事件和 flush 事件的处理过程之后，writeandflush 就变得很简单了，它就是把 write 和 flush 流程结合起来，先触发 write 事件然后在触发 flush 事件。\n\n下面我们来看下 writeandflush 的具体逻辑处理：\n\npublic class echoserverhandler extends channelinboundhandleradapter {\n\n    @override\n    public void channelread(final channelhandlercontext ctx, final object msg) {\n        //此处的msg就是netty在read loop中从niosocketchannel中读取到bytebuffer\n        ctx.writeandflush(msg);\n    }\n}\nabstract class abstractchannelhandlercontext implements channelhandlercontext, resourceleakhint {\n\n    @override\n    public channelfuture writeandflush(object msg) {\n        return writeandflush(msg, newpromise());\n    }\n\n    @override\n    public channelfuture writeandflush(object msg, channelpromise promise) {\n        write(msg, true, promise);\n        return promise;\n    }\n\n}\n\n\n这里可以看到 writeandflush 方法的处理入口和 write 事件的处理入口是一样的。唯一不同的是入口处理函数 write 方法的 boolean flush 入参不同，在 writeandflush 的处理中 flush = true。\n\n    private void write(object msg, boolean flush, channelpromise promise) {\n        objectutil.checknotnull(msg, \"msg\");\n\n        ................省略检查promise的有效性...............\n\n        //flush = true 表示channelhandler中调用的是writeandflush方法，这里需要找到pipeline中覆盖write或者flush方法的channelhandler\n        //flush = false 表示调用的是write方法，只需要找到pipeline中覆盖write方法的channelhandler\n        final abstractchannelhandlercontext next = findcontextoutbound(flush ?\n                (mask_write | mask_flush) : mask_write);\n        //用于检查内存泄露\n        final object m = pipeline.touch(msg, next);\n        //获取下一个要被执行的channelhandler的executor\n        eventexecutor executor = next.executor();\n        //确保outbound事件由channelhandler指定的executor执行\n        if (executor.ineventloop()) {\n            //如果当前线程正是channelhandler指定的executor则直接执行\n            if (flush) {\n                next.invokewriteandflush(m, promise);\n            } else {\n                next.invokewrite(m, promise);\n            }\n        } else {\n            //如果当前线程不是channelhandler指定的executor,则封装成异步任务提交给指定executor执行，注意这里的executor不一定是reactor线程。\n            final writetask task = writetask.newinstance(next, m, promise, flush);\n            if (!safeexecute(executor, task, promise, m, !flush)) {\n                task.cancel();\n            }\n        }\n    }\n\n\n由于在 writeandflush 流程的处理中，flush 标志被设置为 true，所以这里有两个地方会和 write 事件的处理有所不同。\n\n * findcontextoutbound( mask_write | mask_flush )：这里在 pipeline 中向前查找的 channeoutboundhandler 需要实现 write 方法或者 flush 方法。这里需要注意的是 write 方法和 flush 方法只需要实现其中一个即可满足查找条件。因为一般我们自定义 channeloutboundhandler 时，都会继承 channeloutboundhandleradapter 类，而在 channelinboundhandleradapter 类中对于这些 outbound 事件都会有默认的实现。\n\npublic class channeloutboundhandleradapter extends channelhandleradapter implements channeloutboundhandler {\n\n    @skip\n    @override\n    public void write(channelhandlercontext ctx, object msg, channelpromise promise) throws exception {\n        ctx.write(msg, promise);\n    }\n\n\n    @skip\n    @override\n    public void flush(channelhandlercontext ctx) throws exception {\n        ctx.flush();\n    }\n\n}\n\n\n这样在后面传播 write 事件或者 flush 事件的时候，我们通过上面逻辑找出的 channeloutboundhandler 中可能只实现了一个 flush 方法或者 write 方法。不过这样没关系，如果这里在传播 outbound 事件的过程中，发现找出的 channeloutboundhandler 中并没有实现对应的 outbound 事件回调函数，那么就直接调用在 channeloutboundhandleradapter 中的默认实现。\n\n * 在向前传播 writeandflush 事件的时候会通过调用 channelhandlercontext 的 invokewriteandflush 方法，先传播 write 事件 然后在传播 flush 事件。\n\nvoid invokewriteandflush(object msg, channelpromise promise) {\n    if (invokehandler()) {\n        //向前传递write事件\n        invokewrite0(msg, promise);\n        //向前传递flush事件\n        invokeflush0();\n    } else {\n        writeandflush(msg, promise);\n    }\n}\n\nprivate void invokewrite0(object msg, channelpromise promise) {\n    try {\n        //调用当前channelhandler中的write方法\n        ((channeloutboundhandler) handler()).write(this, msg, promise);\n    } catch (throwable t) {\n        notifyoutboundhandlerexception(t, promise);\n    }\n}\n\nprivate void invokeflush0() {\n    try {\n        ((channeloutboundhandler) handler()).flush(this);\n    } catch (throwable t) {\n        invokeexceptioncaught(t);\n    }\n}\n\n\n这里我们看到了 writeandflush 的核心处理逻辑，首先向前传播 write 事件，经过 write 事件的流程处理后，最后向前传播 flush 事件。\n\n根据前边的介绍，这里在向前传播 write 事件的时候，可能查找出的 channeloutboundhandler 只是实现了 flush 方法，不过没关系，这里会直接调用 write 方法在 channeloutboundhandleradapter 父类中的默认实现。同理 flush 也是一样。\n\n----------------------------------------\n\n\n# 总结\n\n到这里，netty 处理数据发送的整个完整流程，笔者就为大家详细地介绍完了，可以看到 netty 在处理读取数据和处理发送数据的过程中，虽然核心逻辑都差不多，但是发送数据的过程明显细节比较多，而且更加复杂一些。\n\n这里笔者将读取数据和发送数据的不同之处总结如下几点供大家回忆对比：\n\n * 在每次 read loop 之前，会分配一个大小固定的 diretbytebuffer 用来装载读取数据。每轮 read loop 完全结束之后，才会决定是否对下一轮的读取过程分配的 directbytebuffer 进行扩容或者缩容。\n\n * 在每次 write loop 之前，都会获取本次 write loop 最大能够写入的字节数，根据这个最大写入字节数从 channeloutboundbuffer 中转换 jdk nio bytebuffer 。每次写入 socket 之后都需要重新评估是否对这个最大写入字节数进行扩容或者缩容。\n\n * read loop 和 write loop 都被默认限定最多执行 16 次。\n\n * 在一个完整的 read loop 中，如果还读取不完数据，直接退出。等到 reactor 线程执行完其他 channel 上的 io 事件再来读取未读完的数据。\n\n * 而在一个完整的 write loop 中，数据发送不完，则分两种情况。\n   \n   * socket 缓冲区满无法在继续写入。这时就需要向 reactor 注册 op_write 事件。等 socket 缓冲区变的可写时，epoll 通知 reactor 线程继续发送。\n   * socket 缓冲区可写，但是由于发送数据太多，导致虽然写满 16 次但依然没有写完。这时就直接向 reactor 丢一个 flushtask 进去，等到 reactor 线程执行完其他 channel 上的 io 事件，在回过头来执行 flushtask。\n\n * op_read 事件的注册是在 niosocketchannel 被注册到对应的 reactor 中时就会注册。而 op_write 事件只会在 socket 缓冲区满的时候才会被注册。当 socket 缓冲区再次变得可写时，要记得取消 op_write 事件的监听。否则的话就会一直被通知。\n\n好了，本文的全部内容就到这里了，我们下篇文章见~~~~\n\n\n# 参考资料\n\n一文搞懂netty发送数据全流程 | 你想知道的细节全在这里 (qq.com)",charsets:{cjk:!0},lastUpdated:"2024/09/19, 08:16:36",lastUpdatedTimestamp:1726733796e3},{title:"深入 Redis 事件驱动框架",frontmatter:{title:"深入 Redis 事件驱动框架",date:"2024-09-15T01:36:53.000Z",permalink:"/pages/264b06/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/08.%E6%B7%B1%E5%85%A5%20Redis%20%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%A1%86%E6%9E%B6.html",relativePath:"Redis 系统设计/03.三、主线任务/08.深入 Redis 事件驱动框架.md",key:"v-24e06c7a",path:"/pages/264b06/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:140},{level:2,title:"事件概述",slug:"事件概述",normalizedTitle:"事件概述",charIndex:495},{level:3,title:"文件事件处理",slug:"文件事件处理",normalizedTitle:"文件事件处理",charIndex:691},{level:3,title:"时间事件处理器",slug:"时间事件处理器",normalizedTitle:"时间事件处理器",charIndex:1167},{level:3,title:"核心源码的伪代码（自顶向下）",slug:"核心源码的伪代码-自顶向下",normalizedTitle:"核心源码的伪代码（自顶向下）",charIndex:2344},{level:2,title:"事件驱动框架循环流程的初始化",slug:"事件驱动框架循环流程的初始化",normalizedTitle:"事件驱动框架循环流程的初始化",charIndex:3482},{level:3,title:"aeEventLoop 结构体与初始化",slug:"aeeventloop-结构体与初始化",normalizedTitle:"aeeventloop 结构体与初始化",charIndex:3583},{level:3,title:"aeCreateEventLoop 函数的初始化操作",slug:"aecreateeventloop-函数的初始化操作",normalizedTitle:"aecreateeventloop 函数的初始化操作",charIndex:4371},{level:2,title:"IO 事件处理",slug:"io-事件处理",normalizedTitle:"io 事件处理",charIndex:7675},{level:3,title:"IO 事件创建",slug:"io-事件创建",normalizedTitle:"io 事件创建",charIndex:8332},{level:3,title:"读事件处理",slug:"读事件处理",normalizedTitle:"读事件处理",charIndex:11606},{level:3,title:"写事件处理",slug:"写事件处理",normalizedTitle:"写事件处理",charIndex:12550},{level:2,title:"时间事件处理",slug:"时间事件处理",normalizedTitle:"时间事件处理",charIndex:1167},{level:3,title:"时间事件定义",slug:"时间事件定义",normalizedTitle:"时间事件定义",charIndex:17487},{level:3,title:"时间事件创建",slug:"时间事件创建",normalizedTitle:"时间事件创建",charIndex:1324},{level:3,title:"时间事件回调函数",slug:"时间事件回调函数",normalizedTitle:"时间事件回调函数",charIndex:18951},{level:3,title:"时间事件的触发处理",slug:"时间事件的触发处理",normalizedTitle:"时间事件的触发处理",charIndex:20051},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:21002},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:21070}],headersStr:"前言 事件概述 文件事件处理 时间事件处理器 核心源码的伪代码（自顶向下） 事件驱动框架循环流程的初始化 aeEventLoop 结构体与初始化 aeCreateEventLoop 函数的初始化操作 IO 事件处理 IO 事件创建 读事件处理 写事件处理 时间事件处理 时间事件定义 时间事件创建 时间事件回调函数 时间事件的触发处理 总结 参考资料",content:"提出问题是一切智慧的开端\n\n * Redis 事件驱动框架有哪些事件？\n * 这些事件的创建和处理又分别对应了 Redis 源码中的哪些具体操作？\n * 如何在一个框架中，同时处理 IO 事件 和 时间事件？\n * Redis 事件驱动框架有哪些核心函数？struct？\n\n\n# 前言\n\n前置知识\n\n * Linux 中的 IO 多路复用机制\n * Redis 的 Reactor 模型\n\n在 Redis 的 Reactor 模型 主要关注的是框架基本流程，其中介绍了事件驱动框架基于的 Reactor 模型，并以 IO 事件中的客户端连接事件为例，给你介绍了框架运行的基本流程：从 server 初始化时调用 aeCreateFileEvent 函数注册监听事件，到 server 初始化完成后调用 aeMain 函数，而 aeMain 函数循环执行 aeProceeEvent 函数，来捕获和处理客户端请求触发的事件。\n\n本文，echo 带你深入 Redis 事件驱动框架 ，给你介绍下 Redis 事件驱动框架中的两大类事件类型：IO 事件和时间事件，以及它们相应的处理机制\n\n\n# 事件概述\n\n * 文件事件「IO 事件」：Redis 服务器通过套接字与客户端进行连接，而 文件事件就是服务器对套接字操作的抽象。服务器与客户端的通信会产生相应的文件事件，而服务器则通过监听并处理这些事件来完成一系列网络通信操作。\n\n * 时间事件：Redis 服务器中的一些操作「比如 servercron 函数」需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象。\n\n\n# 文件事件处理\n\n\n\n文件事件处理器有四个组成部分：\n\n * 套接字：文件事件是对套接字操作的抽象，每当一个套接字准备好执行连接应答、写入、读取、关闭等操作时，就会产生一个文件事件\n * IO 多路复用：IO 多路复用程序负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。\n * 文件事件分派器：尽管多个文件事件可能会并发地出现，但 IO 多路复用程序总是会将所有产生事件的套接字都放到一个队列里面，然后通过这个队列，以有序(sequentially)、同步(synchronously)、每次一个套接字的方式向文件事件分派器传送套接字。当上一个套接字产生的事件被处理完毕之后(该套接字为事件所关联的事件处理器执行完毕)，I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字\n * 事件处理器：文件事件分派器接收 I/O 多路复用程序传来的套接字，并根据套接字产生的事件的类调用相应的事件处理器。服务器会为执行不同任务的套接字关联不同的事件处理器，这些处理器是一个个函数它们定义了某个事件发生时，服务器应该执行的动作\n\n\n\n\n# 时间事件处理器\n\n时间事件分为两类：\n\n * 定时事件：让一段程序在指定的时间之后执行一次。比如说，让程序 X 在当前时间的 30 毫秒之后执行一次。\n * 周期性事件：让一段程序每隔指定时间就执行一次。比如说，让程序 Y 每隔 30 毫秒就执行一次。\n\n一个时间事件主要由以下三个属性组成\n\n * id：服务器为时间事件创建的全局唯一 ID(标识号)。ID 号按从小到大的顺序递增新事件的 ID 号比旧事件的 ID 号要大。\n * when：毫秒精度的 UNIX 时间戳，记录了时间事件的到达(arrive)时间。\n * timeProc：时间事件处理器，一个函数。当时间事件到达时，服务器就会调用相应的处理器来处理事件。\n\n> 一个时间事件是定时事件还是周期性事件取决于时间事件处理器的返回值:如果事件处理器返回 ae.h/AENOMORE，那么这个事件为定时事件:该事件在达到一次之后就会被删除，之后不再到达。如果事件处理器返回一个非 AENOMORE 的整数值，那么这个事件为周期性时间:当一个时间事件到达之后，服务器会根据事件处理器返回的值，对时间事件的 when 属性进行更新，让这个事件在一段时间之后再次到达，并以这种方式一直更新并运行下去。\n\n\n\nredis 中的所有时间事件都放在一个无序链表中，服务器将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，它就遍历整个链表，查找所有已到达的时间事件，并调用相应的事件处理器。\n\n注意\n\n我们说保存时间事件的链表为无序链表，指的不是链表不按 ID 排序，而是说该链表不按 when 属性的大小排序。正因为链表没有按 when 属性进行排序，所以当时间事件执行器运行的时候，它必须遍历链表中的所有时间事件，这样才能确保服务器中所有已到达的时间事件都会被处理。\n\n正常模式下的 Redis 服务器只使用 serverCron 一个时间事件，而在 benchmark 模式下，服务器也只使用两个时间事件。在这种情况下，服务器几乎是将无序链表退化成一个指针来使用，所以使用无序链表来保存时间事件，并不影响事件执行的性能。\n\n> serverCron\n> \n> 持续运行的 Redis 服务器需要定期对自身的资源和状态进行检查和调整，从而确保服务器可以长期、稳定地运行，这些定期操作由 redis.c/serverCron 函数负责执行，它的主要工作包括:\n> \n>  * 更新服务器的各类统计信息，比如时间、内存占用、数据库占用情况等。\n>  * 清理数据库中的过期键值对。关闭和清理连接失效的客户端。\n>  * 尝试进行 AOF 或 RDB 持久化操作。\n>  * 如果服务器是主服务器，那么对从服务器进行定期同步。\n>  * 如果处于集群模式，对集群进行定期同步和连接测试。\n\n\n# 核心源码的伪代码（自顶向下）\n\n\n\ndef main():\n    #初始化服务器\n    init server()\n    #一直处理事件，直到服务器关闭为止\n    while server_is_not_shutdown():\n    \taeProcessEvents()\n    #服务器关闭，执行清理操作\n    clean server()\n\n\ndef aeProcessEvents():\n    #获取到达时间离当前时间最接近的时间事件\n    time_event=aeSearchNearestTimer()\n\n    #计算最接近的时间事件距离到达还有多少毫秒\n    remaind_ms=time_event.when - unix_ts_now()\n\n    #如果事件已到达，那么remaind ms的值可能为负数，将它设定为0\n    if remaind_ms < 0:\n    \tremaind_ms = 0\n\n    #根据remaind_ms的值，创建timeval结构\n    timeval=create_timeval_with_ms(remaind_ms)\n\n    #阻塞并等待文件事件产生，最大阻塞时间由传入的timeval结构决定#如果remaind_ms的值为0，那么aeApiPo1l调用之后马上返回，不阻塞\n    aeApiPoll(timeval)\n\n    #处理所有已产生的文件事件\n    processFileEvents()\n\n    #处理所有已到达的时间事件\n    processTimeEvents()\n\n\ndef processTimeEvents():\n\t#遍历服务器中的所有时间事件\n\tfor time_event in all_time_event():\n\t#检查事件是否已经到达\n\t\tif time_event.when <= unix_ts_now():\n\t\t\t#事件已到达\n\t\t\t#执行事件处理器，并获取返回值\n\t\t\tretval=time_event.timeProc()\n\t\t\t#如果这是一个定时事件\n\t\t\tif retval==AE_NOMORE:\n\t\t\t\t#那么将该事件从服务器中删除\n\t\t\t\tdelete_time_event_from_server(time event)\n\t\t\t#如果这是一个周期性事件\n\t\t\telse:\n            #那么按照事件处理器的返回值更新时间事件的when 属性\n            #让这个事件在指定的时间之后再次到达\n            \tupdate_when(time_event,retval)\n\n\n\n# 事件驱动框架循环流程的初始化\n\n为了对这两类事件有个相对全面的了解，接下来，我们先从事件驱动框架循环流程的数据结构及其初始化开始学起，因为这里面就包含了针对这两类事件的数据结构定义和初始化操作\n\n\n# aeEventLoop 结构体与初始化\n\n首先，我们来看下 Redis 事件驱动框架循环流程对应的数据结构 aeEventLoop\n\n这个结构体是在事件驱动框架代码 ae.h 中定义的，记录了框架循环运行过程中的信息，其中，就包含了记录两类事件的变量，分别是：\n\n * aeFileEvent 类型的指针 *events，表示 IO 事件。之所以类型名称为 aeFileEvent，是因为所有的 IO 事件都会用文件描述符进行标识\n * aeTimeEvent 类型的指针 *timeEventHead，表示时间事件，即按一定时间周期触发的事件\n\n此外，aeEventLoop 结构体中还有一个 aeFiredEvent 类型的指针 *fired，这个并不是一类专门的事件类型，它只是用来记录已触发事件对应的文件描述符信息\n\n下面的代码显示了 Redis 中事件循环的结构体定义，你可以看下\n\ntypedef struct aeEventLoop {\n    …\n    aeFileEvent *events; //IO事件数组\n    aeFiredEvent *fired; //已触发事件数组\n    aeTimeEvent *timeEventHead; //记录时间事件的链表头\n    …\n    void *apidata; //和API调用接口相关的数据\n    aeBeforeSleepProc *beforesleep; //进入事件循环流程前执行的函数\n    aeBeforeSleepProc *aftersleep;  //退出事件循环流程后执行的函数\n    \n} aeEventLoop;\n\n\n了解了 aeEventLoop 结构体后，我们再来看下，这个结构体是如何初始化的，这其中就包括了 IO 事件数组和时间事件链表的初始化。\n\n\n# aeCreateEventLoop 函数的初始化操作\n\n因为 Redis server 在完成初始化后，就要开始运行事件驱动框架的循环流程，所以，aeEventLoop 结构体在server.c的 initServer 函数中，就通过调用 **aeCreateEventLoop 函数 **进行初始化了。这个函数的参数只有一个，是 setsize\n\n下面的代码展示了 initServer 函数中对 aeCreateEventLoop 函数的调用。\n\ninitServer() {\n    …\n    //调用 aeCreateEventLoop 函数创建 aeEventLoop 结构体，并赋值给 server 结构的 el 变量\n    server.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR);\n    …\n}\n\n\n从这里我们可以看到 参数 setsize 的大小，其实是由 server 结构的 maxclients 变量和宏定义 CONFIG_FDSET_INCR 共同决定的。其中，maxclients 变量的值大小，可以在 Redis 的配置文件 redis.conf 中进行定义，默认值是 1000。而宏定义 CONFIG_FDSET_INCR 的大小，等于宏定义 CONFIG_MIN_RESERVED_FDS 的值再加上 96，如下所示，这里的两个宏定义都是在server.h文件中定义的。\n\n#define CONFIG_MIN_RESERVED_FDS 32\n#define CONFIG_FDSET_INCR (CONFIG_MIN_RESERVED_FDS+96)\n\n\n好了，到这里，你可能有疑问了：aeCreateEventLoop 函数的参数 setsize，设置为最大客户端数量加上一个宏定义值，可是这个参数有什么用呢？这就和 aeCreateEventLoop 函数具体执行的初始化操作有关了。\n\n接下来，我们就来看下 aeCreateEventLoop 函数执行的操作，大致可以分成以下三个步骤。\n\n第一步，aeCreateEventLoop 函数会创建一个 aeEventLoop 结构体类型的变量 eventLoop。然后，该函数会给 eventLoop 的成员变量分配内存空间，比如，按照传入的参数 setsize，给 IO 事件数组和已触发事件数组分配相应的内存空间。此外，该函数还会给 eventLoop 的成员变量赋初始值\n\n第二步，aeCreateEventLoop 函数会调用 aeApiCreate 函数。aeApiCreate 函数封装了操作系统提供的 IO 多路复用函数，假设 Redis 运行在 Linux 操作系统上，并且 IO 多路复用机制是 epoll，那么此时，aeApiCreate 函数就会调用 epoll_create 创建 epoll 实例，同时会创建 epoll_event 结构的数组，数组大小等于参数 setsize\n\n这里你需要注意，aeApiCreate 函数是把创建的 epoll 实例描述符和 epoll_event 数组，保存在了 aeApiState 结构体类型的变量 state，如下所示：\n\ntypedef struct aeApiState {  //aeApiState结构体定义\n    int epfd;   //epoll实例的描述符\n    struct epoll_event *events;   //epoll_event结构体数组，记录监听事件\n} aeApiState;\n\nstatic int aeApiCreate(aeEventLoop *eventLoop) {\n    aeApiState *state = zmalloc(sizeof(aeApiState));\n    ...\n    //将epoll_event数组保存在aeApiState结构体变量state中\n    state->events = zmalloc(sizeof(struct epoll_event)*eventLoop->setsize);\n    ...\n    //将epoll实例描述符保存在aeApiState结构体变量state中\n    state->epfd = epoll_create(1024);\n    ···\n}\n\n\n紧接着，aeApiCreate 函数把 state 变量赋值给 eventLoop 中的 apidata 。这样一来，eventLoop 结构体中就有了 epoll 实例和 epoll_event 数组的信息，这样就可以用来基于 epoll 创建和处理事件了。我一会儿还会给你具体介绍。\n\neventLoop->apidata = state;\n\n\n第三步，aeCreateEventLoop 函数会把所有网络 IO 事件对应文件描述符的掩码，初始化为 AE_NONE，表示暂时不对任何事件进行监听\n\n我把 aeCreateEventLoop 函数的主要部分代码放在这里，你可以看下。\n\naeEventLoop *aeCreateEventLoop(int setsize) {\n    aeEventLoop *eventLoop;\n    int i;\n\n    //给eventLoop变量分配内存空间\n\tif ((eventLoop = zmalloc(sizeof(*eventLoop))) == NULL) goto err;\n\n\t//给IO事件、已触发事件分配内存空间\n    eventLoop->events = zmalloc(sizeof(aeFileEvent)*setsize);\n    eventLoop->fired = zmalloc(sizeof(aeFiredEvent)*setsize);\n    …\n    eventLoop->setsize = setsize;\n    eventLoop->lastTime = time(NULL);\n\n    //设置时间事件的链表头为NULL\n    eventLoop->timeEventHead = NULL;\n\t…\n\t//调用aeApiCreate函数，去实际调用操作系统提供的IO多路复用函数\n\tif (aeApiCreate(eventLoop) == -1) goto err;\n\n    //将所有网络IO事件对应文件描述符的掩码设置为AE_NONE\n    for (i = 0; i < setsize; i++)\n        eventLoop->events[i].mask = AE_NONE;\n    return eventLoop;\n\n    //初始化失败后的处理逻辑，\n    err:\n    …\n}\n\n\n好，那么从 aeCreateEventLoop 函数的执行流程中，我们其实可以看到以下 两个关键点：\n\n * 事件驱动框架监听的 IO 事件数组大小就等于参数 setsize，这样决定了和 Redis server 连接的客户端数量。所以，当你遇到客户端连接 Redis 时报错“max number of clients reached”，你就可以去 redis.conf 文件修改 maxclients 配置项，以扩充框架能监听的客户端数量。\n * 当使用 Linux 系统的 epoll 机制时，框架循环流程初始化操作，会通过 aeApiCreate 函数创建 epoll_event 结构数组，并调用 epoll_create 函数创建 epoll 实例，这都是使用 epoll 机制的准备工作要求\n\n到这里，框架就可以创建和处理具体的 IO 事件和时间事件了。所以接下来，我们就先来了解下 IO 事件及其处理机制。\n\n\n# IO 事件处理\n\nRedis 的 IO 事件主要包括三类，分别是\n\n * 可读事件：从客户端读取数据\n * 可写事件：向客户端写入数据\n * 屏障事件：屏障事件的主要作用是用来反转事件的处理顺序。比如在默认情况下，Redis 会先给客户端返回结果，但是如果面临需要把数据尽快写入磁盘的情况，Redis 就会用到屏障事件，把写数据和回复客户端的顺序做下调整，先把数据落盘，再给客户端回复。\n\n在 Redis 源码中，IO 事件的数据结构是 aeFileEvent 结构体，IO 事件的创建是通过 aeCreateFileEvent 函数来完成的。下面的代码展示了 aeFileEvent 结构体的定义，你可以再回顾下：\n\ntypedef struct aeFileEvent {\n    int mask; //掩码标记，包括可读事件、可写事件和屏障事件\n    aeFileProc *rfileProc;   //处理可读事件的回调函数\n    aeFileProc *wfileProc;   //处理可写事件的回调函数\n    void *clientData;  //私有数据\n} aeFileEvent;\n\n\n而对于 aeCreateFileEvent 函数来说，在上节课我们已经了解了它是通过 aeApiAddEvent 函数来完成事件注册的。那么接下来，我们再从代码级别看下它是如何执行的，这可以帮助我们更加透彻地理解，事件驱动框架对 IO 事件监听是如何基于 epoll 机制对应封装的。\n\n\n# IO 事件创建\n\n首先，我们来看 aeCreateFileEvent 函数，如下所示：\n\nint aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData)\n{\n    if (fd >= eventLoop->setsize) {\n        errno = ERANGE;\n        return AE_ERR;\n    }\n    aeFileEvent *fe = &eventLoop->events[fd];\n\n    if (aeApiAddEvent(eventLoop, fd, mask) == -1)\n        return AE_ERR;\n    fe->mask |= mask;\n    if (mask & AE_READABLE) fe->rfileProc = proc;\n    if (mask & AE_WRITABLE) fe->wfileProc = proc;\n    fe->clientData = clientData;\n    if (fd > eventLoop->maxfd)\n        eventLoop->maxfd = fd;\n    return AE_OK;\n}\n\n\n这个函数的参数有 5 个，分别是\n\n * 循环流程结构体*eventLoop\n * IO 事件对应的文件描述符 fd\n * 事件类型掩码 mask\n * 事件处理回调函数*proc\n * 事件私有数据*clientData。\n\n因为循环流程结构体 *eventLoop 中有 IO 事件数组，这个数组的元素是 aeFileEvent 类型，所以，每个数组元素都对应记录了一个文件描述符（比如一个套接字）相关联的监听事件类型和回调函数。\n\naeCreateFileEvent 函数会先根据传入的文件描述符 fd，在 eventLoop 的 IO 事件数组中，获取该描述符关联的 IO 事件指针变量*fe，如下所示：\n\naeFileEvent *fe = &eventLoop->events[fd];\n\n\n紧接着，aeCreateFileEvent 函数会调用 aeApiAddEvent 函数，添加要监听的事件：\n\nif (aeApiAddEvent(eventLoop, fd, mask) == -1)\n   return AE_ERR;\n\n\naeApiAddEvent 函数实际上会调用操作系统提供的 IO 多路复用函数，来完成事件的添加。我们还是假设 Redis 实例运行在使用 epoll 机制的 Linux 上，那么 aeApiAddEvent 函数就会调用 epoll_ctl 函数，添加要监听的事件。我在xxx中其实已经给你介绍过 epoll_ctl 函数，这个函数会接收 4 个参数，分别是：\n\n * epoll 实例；\n * 要执行的操作类型（是添加还是修改）；\n * 要监听的文件描述符；\n * epoll_event 类型变量\n\n那么，这个调用过程是如何准备 epoll_ctl 函数需要的参数，从而完成执行的呢？\n\n首先，epoll 实例是我刚才给你介绍的 aeCreateEventLoop 函数，它是通过调用 aeApiCreate 函数来创建的，保存在了 eventLoop 结构体的 apidata 变量中，类型是 aeApiState。所以，aeApiAddEvent 函数会先获取该变量，如下所示：\n\nstatic int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) {\n    //从eventLoop结构体中获取aeApiState变量，里面保存了epoll实例\n\taeApiState *state = eventLoop->apidata;\n    ...\n }\n\n\n其次，对于要执行的操作类型的设置，aeApiAddEvent 函数会根据传入的文件描述符 fd，在 eventLoop 结构体中 IO 事件数组中查找该 fd。因为 IO 事件数组的每个元素，都对应了一个文件描述符，而该数组初始化时，每个元素的值都设置为了 AE_NONE。\n\n所以，如果要监听的文件描述符 fd 在数组中的类型不是 AE_NONE，则表明该描述符已做过设置，那么操作类型就是修改操作，对应 epoll 机制中的宏定义 EPOLL_CTL_MOD。否则，操作类型就是添加操作，对应 epoll 机制中的宏定义 EPOLL_CTL_ADD。这部分代码如下所示：\n\n//如果文件描述符fd对应的IO事件已存在，则操作类型为修改，否则为添加\n int op = eventLoop->events[fd].mask == AE_NONE ?\n            EPOLL_CTL_ADD : EPOLL_CTL_MOD;\n\n\n第三，epoll_ctl 函数需要的监听文件描述符，就是 aeApiAddEvent 函数接收到的参数 fd。\n\n最后，epoll_ctl 函数还需要一个 epoll_event 类型变量，因此 aeApiAddEvent 函数在调用 epoll_ctl 函数前，会新创建 epoll_event 类型**变量 ee。**然后，aeApiAddEvent 函数会设置变量 ee 中的监听事件类型和监听文件描述符。\n\naeApiAddEvent 函数的参数 mask，表示的是要监听的事件类型掩码。所以，aeApiAddEvent 函数会根据掩码值是可读（AE_READABLE）或可写（AE_WRITABLE）事件，来设置 ee 监听的事件类型是 EPOLLIN 还是 EPOLLOUT。这样一来，Redis 事件驱动框架中的读写事件就能够和 epoll 机制中的读写事件对应上来。下面的代码展示了这部分逻辑，你可以看下。\n\n…\nstruct epoll_event ee = {0}; //创建epoll_event类型变量\n…\n//将可读或可写IO事件类型转换为epoll监听的类型EPOLLIN或EPOLLOUT\nif (mask & AE_READABLE) ee.events |= EPOLLIN;\nif (mask & AE_WRITABLE) ee.events |= EPOLLOUT;\nee.data.fd = fd;  //将要监听的文件描述符赋值给ee\n…\n\n\n好了，到这里，aeApiAddEvent 函数就准备好了 epoll 实例、操作类型、监听文件描述符以及 epoll_event 类型变量，然后，它就会调用 epoll_ctl 开始实际创建监听事件了，如下所示：\n\nstatic int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) {\n    ...\n    //调用epoll_ctl实际创建监听事件\n    if (epoll_ctl(state->epfd,op,fd,&ee) == -1) return -1;\n        return 0;\n}\n\n\n了解了这些代码后，我们可以学习到事件驱动框架是如何基于 epoll，封装实现了 IO 事件的创建。那么，在 Redis server 启动运行后，最开始监听的 IO 事件是可读事件，对应于客户端的连接请求。具体是 initServer 函数调用了 aeCreateFileEvent 函数，创建可读事件，并设置回调函数为 acceptTcpHandler，用来处理客户端连接\n\n接下来，我们再来看下一旦有了客户端连接请求后，IO 事件具体是如何处理的呢？\n\n\n# 读事件处理\n\n当 Redis server 接收到客户端的连接请求时，就会使用注册好的 acceptTcpHandler 函数 进行处理\n\nacceptTcpHandler 函数会接受客户端连接，并创建已连接套接字 cfd。然后，acceptCommonHandler 函数会被调用，同时，刚刚创建的已连接套接字 cfd 会作为参数，传递给 acceptCommonHandler 函数。\n\nacceptCommonHandler 函数会调用 createClient 函数创建客户端。而在 createClient 函数中，我们就会看到，aeCreateFileEvent 函数被再次调用了\n\n此时，aeCreateFileEvent 函数会针对已连接套接字上，创建监听事件，类型为 AE_READABLE，回调函数是 readQueryFromClient\n\n好了，到这里，事件驱动框架就增加了对一个客户端已连接套接字的监听。一旦客户端有请求发送到 server，框架就会回调 readQueryFromClient 函数处理请求。这样一来，客户端请求就能通过事件驱动框架进行处理了。\n\n下面代码展示了 createClient 函数调用 aeCreateFileEvent 的过程，你可以看下。\n\nclient *createClient(int fd) {\n…\nif (fd != -1) {\n        …\n        //调用aeCreateFileEvent，监听读事件，对应客户端读写请求，使用readQueryFromclient回调函数处理\n        if (aeCreateFileEvent(server.el,fd,AE_READABLE,\n            readQueryFromClient, c) == AE_ERR)\n        {\n            close(fd);\n            zfree(c);\n            return NULL;\n        } }\n…\n}\n\n\n为了便于你掌握从监听客户端连接请求到监听客户端常规读写请求的事件创建过程，我画了下面这张图，你可以看下\n\n\n\n\n# 写事件处理\n\nRedis 实例在收到客户端请求后，会在处理客户端命令后，将要返回的数据写入客户端输出缓冲区。下图就展示了这个过程的函数调用逻辑：\n\n\n\n而在 Redis 事件驱动框架每次循环进入事件处理函数前，也就是在框架主函数 aeMain 中调用 aeProcessEvents，来处理监听到的已触发事件或是到时的时间事件之前，都会调用 server.c 文件中的 beforeSleep 函数，进行一些任务处理，这其中就包括了调用 handleClientsWithPendingWrites 函数，它会将 Redis sever 客户端缓冲区中的数据写回客户端。\n\n下面给出的代码是事件驱动框架的主函数 aeMain。在该函数每次调用 aeProcessEvents 函数前，就会调用 beforeSleep 函数，你可以看下。\n\nvoid aeMain(aeEventLoop *eventLoop) {\n    eventLoop->stop = 0;\n\twhile (!eventLoop->stop) {\n\t    //如果beforeSleep函数不为空，则调用beforeSleep函数\n        if (eventLoop->beforesleep != NULL)\n            eventLoop->beforesleep(eventLoop);\n        //调用完beforeSleep函数，再处理事件\n        aeProcessEvents(eventLoop, AE_ALL_EVENTS|AE_CALL_AFTER_SLEEP);\n    }\n}\n\n\n这里你要知道，beforeSleep 函数调用的 handleClientsWithPendingWrites 函数，会遍历每一个待写回数据的客户端，然后调用 writeToClient 函数，将客户端输出缓冲区中的数据写回。下面这张图展示了这个流程，你可以看下。\n\n\n\n/* \n * 该函数在进入事件循环之前调用，\n * 目的是尽可能直接将响应数据写入客户端的输出缓冲区，\n * 避免使用系统调用来安装可写事件处理程序，\n * 从而提高效率。 \n */\nint handleClientsWithPendingWrites(void) {\n    listIter li;\n    listNode *ln;\n    int processed = listLength(server.clients_pending_write); // 获取当前待写客户端队列的长度\n\n    listRewind(server.clients_pending_write, &li); // 将迭代器设置为从队列头开始\n    while ((ln = listNext(&li))) { // 遍历待写客户端的队列\n        client *c = listNodeValue(ln); // 获取当前节点的客户端\n        c->flags &= ~CLIENT_PENDING_WRITE; // 清除客户端的“待写”标志\n        listUnlinkNode(server.clients_pending_write, ln); // 从待写客户端队列中移除当前节点\n\n        /* 如果客户端是受保护的，不执行任何操作以避免写入错误或重新创建处理程序 */\n        if (c->flags & CLIENT_PROTECTED) continue;\n\n        /* 如果客户端即将关闭，不需要写入操作 */\n        if (c->flags & CLIENT_CLOSE_ASAP) continue;\n\n        /* 尝试将缓冲区中的数据写入客户端的套接字。\n         * 如果写入失败，则跳过该客户端继续处理下一个。 */\n        if (writeToClient(c, 0) == C_ERR) continue;\n\n        /* 如果经过上述同步写入后仍有数据需要输出到客户端，\n         * 则需要为该客户端安装一个可写事件处理程序，以便稍后继续写入。 */\n        if (clientHasPendingReplies(c)) {\n            installClientWriteHandler(c);\n        }\n    }\n    return processed; // 返回处理的客户端数量\n}\n\n\n不过，如果输出缓冲区的数据还没有写完，此时，handleClientsWithPendingWrites 函数就会调用 aeCreateFileEvent 函数，创建可写事件，并设置回调函数 sendReplyToClient。sendReplyToClient 函数里面会调用 writeToClient 函数写回数据。\n\n> aeCreateFileEvent 是 Redis 中的一个底层函数，用于向事件循环中注册一个新的文件事件。文件事件可以是“可读事件”（数据到来时触发）或“可写事件”（缓冲区空闲时触发）。在上述场景中，aeCreateFileEvent 创建的是一个“可写事件”。\n> \n> 当客户端的输出缓冲区还未完全发送完数据时，Redis 不会立刻阻塞，而是通过创建“可写事件”来处理这个情况。这个可写事件表示，当 Redis 发现客户端可以继续接收数据时（输出缓冲区空闲），它就会自动触发这个事件。\n> \n> 当可写事件触发时，Redis 会调用 sendReplyToClient 函数。这个函数负责将剩余的数据从输出缓冲区发送给客户端。具体来说，它内部会调用 writeToClient 函数来真正执行数据发送的操作。\n\necho 认为\n\n>  1. 输出缓冲区：当 Redis 需要将数据返回给客户端时，数据会先存放在一个输出缓冲区中，然后再通过网络传输给客户端。\n>  2. 缓冲区未写完：这个情况可能发生在以下几种情况下：\n>     * 客户端网络不畅：客户端处理速度较慢，或者网络带宽不足，导致一次只能从缓冲区接收一部分数据，剩余的数据暂时无法发送。(可能是 TCP 的滑动窗口中的接收方的接收窗口跟不上)\n>     * 大数据量传输：如果 Redis 需要发送的数据量很大，比如一个大的查询结果，Redis 可能无法在一次 write 操作中将所有数据写入客户端的网络套接字，只能先写入一部分，剩下的放在缓冲区里等待下一次写入。\n>  3. 处理机制：\n>     * 当 Redis 发现缓冲区中的数据没有写完（例如，writeToClient 函数尝试发送数据时只能写入一部分），它不会等待或阻塞主线程。\n>     * 此时 Redis 会调用 aeCreateFileEvent，创建一个可写事件，表示客户端还未完全接收数据。当客户端准备好接收更多数据时，这个可写事件会触发，回调函数 sendReplyToClient 会再次被调用，尝试将剩下的数据发送给客户端。\n> \n>  * 避免阻塞主线程：Redis 是单线程的，如果由于网络问题或客户端处理能力限制，主线程被阻塞在一个客户端的发送过程中，其他客户端的请求就无法得到及时处理。\n>  * 提高性能和吞吐量：通过异步的方式处理缓冲区的剩余数据发送，Redis 能在高并发的情况下更高效地处理多个客户端的请求。\n\nint handleClientsWithPendingWrites(void) {\n    listIter li;\n\tlistNode *ln;\n\t…\n    //获取待写回的客户端列表\n\tlistRewind(server.clients_pending_write,&li);\n\t//遍历每一个待写回的客户端\n\twhile((ln = listNext(&li))) {\n\t   client *c = listNodeValue(ln);\n\t   …\n\t   //调用writeToClient将当前客户端的输出缓冲区数据写回\n\t   if (writeToClient(c->fd,c,0) == C_ERR) continue;\n\t   //如果还有待写回数据\n\t   if (clientHasPendingReplies(c)) {\n\t            int ae_flags = AE_WRITABLE;\n\t            //创建可写事件的监听，以及设置回调函数\n\t             if (aeCreateFileEvent(server.el, c->fd, ae_flags,\n\t                sendReplyToClient, c) == AE_ERR)\n\t            {\n\t                   …\n\t            }\n\t  } }\n}\n\n\n好了，我们刚才了解的是读写事件对应的回调处理函数。实际上，为了能及时处理这些事件，Redis 事件驱动框架的 aeMain 函数还会循环 调用 aeProcessEvents 函数，来检测已触发的事件，并调用相应的回调函数进行处理。\n\n从 aeProcessEvents 函数的代码中，我们可以看到该函数会调用 aeApiPoll 函数，查询监听的文件描述符中，有哪些已经就绪。一旦有描述符就绪，aeProcessEvents 函数就会根据事件的可读或可写类型，调用相应的回调函数进行处理。aeProcessEvents 函数调用的基本流程如下所示：\n\nint aeProcessEvents(aeEventLoop *eventLoop, int flags){\n…\n//调用aeApiPoll获取就绪的描述符\nnumevents = aeApiPoll(eventLoop, tvp);\n…\nfor (j = 0; j < numevents; j++) {\n\taeFileEvent *fe = &eventLoop->events[eventLoop->fired[j].fd];\n\t…\n    //如果触发的是可读事件，调用事件注册时设置的读事件回调处理函数\n\tif (!invert && fe->mask & mask & AE_READABLE) {\n\t      fe->rfileProc(eventLoop,fd,fe->clientData,mask);\n\t                fired++;\n\t}\n\n    //如果触发的是可写事件，调用事件注册时设置的写事件回调处理函数\n\tif (fe->mask & mask & AE_WRITABLE) {\n\t                if (!fired || fe->wfileProc != fe->rfileProc) {\n\t                    fe->wfileProc(eventLoop,fd,fe->clientData,mask);\n\t                    fired++;\n\t                }\n\t            }\n\t…\n\t} }\n\t…\n}\n\n\n到这里，我们就了解了 IO 事件的创建函数 aeCreateFileEvent，以及在处理客户端请求时对应的读写事件和它们的处理函数。那么接下来，我们再来看看事件驱动框架中的时间事件是怎么创建和处理的。\n\n\n# 时间事件处理\n\n其实，相比于 IO 事件有可读、可写、屏障类型，以及不同类型 IO 事件有不同回调函数来说，时间事件的处理就比较简单了。下面，我们就来分别学习下它的定义、创建、回调函数和触发处理。\n\n\n# 时间事件定义\n\n首先，我们来看下时间事件的结构体定义，代码如下所示：\n\ntypedef struct aeTimeEvent {\n    long long id; //时间事件ID\n    long when_sec; //事件到达的秒级时间戳\n    long when_ms; //事件到达的毫秒级时间戳\n    aeTimeProc *timeProc; //时间事件触发后的处理函数\n    aeEventFinalizerProc *finalizerProc;  //事件结束后的处理函数\n    void *clientData; //事件相关的私有数据\n    struct aeTimeEvent *prev;  //时间事件链表的前向指针\n    struct aeTimeEvent *next;  //时间事件链表的后向指针\n} aeTimeEvent;\n\n\n时间事件结构体中主要的变量，包括以秒记录和以毫秒记录的时间事件触发时的时间戳 when_sec 和 when_ms，以及时间事件触发后的处理函数*timeProc。另外，在时间事件的结构体中，还包含了前向和后向指针*prev和*next，这表明时间事件是以链表的形式组织起来的。\n\n在了解了时间事件结构体的定义以后，我们接着来看下，时间事件是如何创建的。\n\n\n# 时间事件创建\n\n与 IO 事件创建使用 aeCreateFileEvent 函数类似，时间事件的创建函数是 aeCreateTimeEvent 函数。这个函数的原型定义如下所示：\n\nlong long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds, aeTimeProc *proc, void *clientData, aeEventFinalizerProc *finalizerProc)\n\n\n在它的参数中，有两个需要我们重点了解下，以便于我们理解时间事件的处理。\n\n * 一个是milliseconds，这是所创建时间事件的触发时间距离当前时间的时长，是用毫秒表示的。\n * 另一个是***proc**，这是所创建时间事件触发后的回调函数。\n\naeCreateTimeEvent 函数的执行逻辑不复杂，主要就是创建一个时间事件的变量 te，对它进行初始化，并把它插入到框架循环流程结构体 eventLoop 中的时间事件链表中。在这个过程中，aeCreateTimeEvent 函数会调用 aeAddMillisecondsToNow 函数，根据传入的 milliseconds 参数，计算所创建时间事件具体的触发时间戳，并赋值给 te。\n\n实际上，Redis server 在初始化时，除了创建监听的 IO 事件外，也会调用 aeCreateTimeEvent 函数创建时间事件。下面代码显示了 initServer 函数对 aeCreateTimeEvent 函数的调用：\n\ninitServer() {\n    …\n    //创建时间事件\n    if (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR){\n    … //报错信息\n    }\n}\n\n\n从代码中，我们可以看到，时间事件触发后的回调函数是 serverCron。所以接下来，我们就来了解下 serverCron 函数。\n\n\n# 时间事件回调函数\n\nserverCron 函数是在 server.c 文件中实现的。一方面，它会顺序调用一些函数，来实现时间事件被触发后，执行一些后台任务。比如，serverCron 函数会检查是否有进程结束信号，若有就执行 server 关闭操作。serverCron 会调用 databaseCron 函数，处理过期 key 或进行 rehash 等。你可以参考下面给出的代码：\n\n//如果收到进程结束信号，则执行server关闭操作\nif (server.shutdown_asap) {\n    if (prepareForShutdown(SHUTDOWN_NOFLAGS) == C_OK) exit(0);\n    ...\n}\n...\nclientCron();  //执行客户端的异步操作\ndatabaseCron(); //执行数据库的后台操作\n\n\n另一方面，serverCron 函数还会以不同的频率周期性执行一些任务，这是通过执行宏 run_with_period 来实现的。\n\nrunwith_period 宏定义如下，该宏定义会根据 Redis 实例配置文件 redis.conf 中定义的 hz 值，来判断参数_ms表示的时间戳是否到达。一旦到达，serverCron 就可以执行相应的任务了。\n\n#define run_with_period(_ms_) if ((_ms_ <= 1000/server.hz) || !(server.cronloops%((_ms_)/(1000/server.hz))))\n\n\n比如，serverCron 函数中会以 1 秒 1 次的频率，检查 AOF 文件是否有写错误。如果有的话，serverCron 就会调用 flushAppendOnlyFile 函数，再次刷回 AOF 文件的缓存数据。下面的代码展示了这一周期性任务：\n\nserverCron() {\n   …\n   //每1秒执行1次，检查AOF是否有写错误\n   run_with_period(1000) {\n        if (server.aof_last_write_status == C_ERR)\n            flushAppendOnlyFile(0);\n    }\n   …\n}\n\n\n如果你想了解更多的周期性任务，可以再详细阅读下 serverCron 函数中，以 run_with_period 宏定义包含的代码块。\n\n好了，了解了时间事件触发后的回调函数 serverCron，我们最后来看下，时间事件是如何触发处理的。\n\n\n# 时间事件的触发处理\n\n其实，时间事件的检测触发比较简单，事件驱动框架的 aeMain 函数会循环调用 aeProcessEvents 函数，来处理各种事件。而 aeProcessEvents 函数在执行流程的最后，会调用 processTimeEvents 函数处理相应到时的任务。\n\naeProcessEvents(){\n    …\n    //检测时间事件是否触发\n    if (flags & AE_TIME_EVENTS)\n            processed += processTimeEvents(eventLoop);\n    …\n}\n\n\n那么，具体到 proecessTimeEvent 函数来说，它的基本流程就是从时间事件链表上逐一取出每一个事件，然后根据当前时间判断该事件的触发时间戳是否已满足。如果已满足，那么就调用该事件对应的回调函数进行处理。这样一来，周期性任务就能在不断循环执行的 aeProcessEvents 函数中，得到执行了。\n\n下面的代码显示了 processTimeEvents 函数的基本流程，你可以再看下。\n\nstatic int processTimeEvents(aeEventLoop *eventLoop) {\n    ...\n    te = eventLoop->timeEventHead;  //从时间事件链表中取出事件\n    while(te) {\n       ...\n      aeGetTime(&now_sec, &now_ms);  //获取当前时间\n      if (now_sec > te->when_sec || (now_sec == te->when_sec && now_ms >= te->when_ms))   //如果当前时间已经满足当前事件的触发时间戳\n      {\n         ...\n        retval = te->timeProc(eventLoop, id, te->clientData); //调用注册的回调函数处理\n        ...\n      }\n      te = te->next;   //获取下一个时间事件\n      ...\n}\n\n\n\n# 总结\n\n 1. 文件事件处理流程和时间事件处理流程\n 2. 文件事件和时间事件如何共同工作的\n 3. 事件驱动框架的初始化过程\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n * redis 事件驱动框架有哪些事件？\n * 这些事件的创建和处理又分别对应了 redis 源码中的哪些具体操作？\n * 如何在一个框架中，同时处理 io 事件 和 时间事件？\n * redis 事件驱动框架有哪些核心函数？struct？\n\n\n# 前言\n\n前置知识\n\n * linux 中的 io 多路复用机制\n * redis 的 reactor 模型\n\n在 redis 的 reactor 模型 主要关注的是框架基本流程，其中介绍了事件驱动框架基于的 reactor 模型，并以 io 事件中的客户端连接事件为例，给你介绍了框架运行的基本流程：从 server 初始化时调用 aecreatefileevent 函数注册监听事件，到 server 初始化完成后调用 aemain 函数，而 aemain 函数循环执行 aeproceeevent 函数，来捕获和处理客户端请求触发的事件。\n\n本文，echo 带你深入 redis 事件驱动框架 ，给你介绍下 redis 事件驱动框架中的两大类事件类型：io 事件和时间事件，以及它们相应的处理机制\n\n\n# 事件概述\n\n * 文件事件「io 事件」：redis 服务器通过套接字与客户端进行连接，而 文件事件就是服务器对套接字操作的抽象。服务器与客户端的通信会产生相应的文件事件，而服务器则通过监听并处理这些事件来完成一系列网络通信操作。\n\n * 时间事件：redis 服务器中的一些操作「比如 servercron 函数」需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象。\n\n\n# 文件事件处理\n\n\n\n文件事件处理器有四个组成部分：\n\n * 套接字：文件事件是对套接字操作的抽象，每当一个套接字准备好执行连接应答、写入、读取、关闭等操作时，就会产生一个文件事件\n * io 多路复用：io 多路复用程序负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。\n * 文件事件分派器：尽管多个文件事件可能会并发地出现，但 io 多路复用程序总是会将所有产生事件的套接字都放到一个队列里面，然后通过这个队列，以有序(sequentially)、同步(synchronously)、每次一个套接字的方式向文件事件分派器传送套接字。当上一个套接字产生的事件被处理完毕之后(该套接字为事件所关联的事件处理器执行完毕)，i/o 多路复用程序才会继续向文件事件分派器传送下一个套接字\n * 事件处理器：文件事件分派器接收 i/o 多路复用程序传来的套接字，并根据套接字产生的事件的类调用相应的事件处理器。服务器会为执行不同任务的套接字关联不同的事件处理器，这些处理器是一个个函数它们定义了某个事件发生时，服务器应该执行的动作\n\n\n\n\n# 时间事件处理器\n\n时间事件分为两类：\n\n * 定时事件：让一段程序在指定的时间之后执行一次。比如说，让程序 x 在当前时间的 30 毫秒之后执行一次。\n * 周期性事件：让一段程序每隔指定时间就执行一次。比如说，让程序 y 每隔 30 毫秒就执行一次。\n\n一个时间事件主要由以下三个属性组成\n\n * id：服务器为时间事件创建的全局唯一 id(标识号)。id 号按从小到大的顺序递增新事件的 id 号比旧事件的 id 号要大。\n * when：毫秒精度的 unix 时间戳，记录了时间事件的到达(arrive)时间。\n * timeproc：时间事件处理器，一个函数。当时间事件到达时，服务器就会调用相应的处理器来处理事件。\n\n> 一个时间事件是定时事件还是周期性事件取决于时间事件处理器的返回值:如果事件处理器返回 ae.h/aenomore，那么这个事件为定时事件:该事件在达到一次之后就会被删除，之后不再到达。如果事件处理器返回一个非 aenomore 的整数值，那么这个事件为周期性时间:当一个时间事件到达之后，服务器会根据事件处理器返回的值，对时间事件的 when 属性进行更新，让这个事件在一段时间之后再次到达，并以这种方式一直更新并运行下去。\n\n\n\nredis 中的所有时间事件都放在一个无序链表中，服务器将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，它就遍历整个链表，查找所有已到达的时间事件，并调用相应的事件处理器。\n\n注意\n\n我们说保存时间事件的链表为无序链表，指的不是链表不按 id 排序，而是说该链表不按 when 属性的大小排序。正因为链表没有按 when 属性进行排序，所以当时间事件执行器运行的时候，它必须遍历链表中的所有时间事件，这样才能确保服务器中所有已到达的时间事件都会被处理。\n\n正常模式下的 redis 服务器只使用 servercron 一个时间事件，而在 benchmark 模式下，服务器也只使用两个时间事件。在这种情况下，服务器几乎是将无序链表退化成一个指针来使用，所以使用无序链表来保存时间事件，并不影响事件执行的性能。\n\n> servercron\n> \n> 持续运行的 redis 服务器需要定期对自身的资源和状态进行检查和调整，从而确保服务器可以长期、稳定地运行，这些定期操作由 redis.c/servercron 函数负责执行，它的主要工作包括:\n> \n>  * 更新服务器的各类统计信息，比如时间、内存占用、数据库占用情况等。\n>  * 清理数据库中的过期键值对。关闭和清理连接失效的客户端。\n>  * 尝试进行 aof 或 rdb 持久化操作。\n>  * 如果服务器是主服务器，那么对从服务器进行定期同步。\n>  * 如果处于集群模式，对集群进行定期同步和连接测试。\n\n\n# 核心源码的伪代码（自顶向下）\n\n\n\ndef main():\n    #初始化服务器\n    init server()\n    #一直处理事件，直到服务器关闭为止\n    while server_is_not_shutdown():\n    \taeprocessevents()\n    #服务器关闭，执行清理操作\n    clean server()\n\n\ndef aeprocessevents():\n    #获取到达时间离当前时间最接近的时间事件\n    time_event=aesearchnearesttimer()\n\n    #计算最接近的时间事件距离到达还有多少毫秒\n    remaind_ms=time_event.when - unix_ts_now()\n\n    #如果事件已到达，那么remaind ms的值可能为负数，将它设定为0\n    if remaind_ms < 0:\n    \tremaind_ms = 0\n\n    #根据remaind_ms的值，创建timeval结构\n    timeval=create_timeval_with_ms(remaind_ms)\n\n    #阻塞并等待文件事件产生，最大阻塞时间由传入的timeval结构决定#如果remaind_ms的值为0，那么aeapipo1l调用之后马上返回，不阻塞\n    aeapipoll(timeval)\n\n    #处理所有已产生的文件事件\n    processfileevents()\n\n    #处理所有已到达的时间事件\n    processtimeevents()\n\n\ndef processtimeevents():\n\t#遍历服务器中的所有时间事件\n\tfor time_event in all_time_event():\n\t#检查事件是否已经到达\n\t\tif time_event.when <= unix_ts_now():\n\t\t\t#事件已到达\n\t\t\t#执行事件处理器，并获取返回值\n\t\t\tretval=time_event.timeproc()\n\t\t\t#如果这是一个定时事件\n\t\t\tif retval==ae_nomore:\n\t\t\t\t#那么将该事件从服务器中删除\n\t\t\t\tdelete_time_event_from_server(time event)\n\t\t\t#如果这是一个周期性事件\n\t\t\telse:\n            #那么按照事件处理器的返回值更新时间事件的when 属性\n            #让这个事件在指定的时间之后再次到达\n            \tupdate_when(time_event,retval)\n\n\n\n# 事件驱动框架循环流程的初始化\n\n为了对这两类事件有个相对全面的了解，接下来，我们先从事件驱动框架循环流程的数据结构及其初始化开始学起，因为这里面就包含了针对这两类事件的数据结构定义和初始化操作\n\n\n# aeeventloop 结构体与初始化\n\n首先，我们来看下 redis 事件驱动框架循环流程对应的数据结构 aeeventloop\n\n这个结构体是在事件驱动框架代码 ae.h 中定义的，记录了框架循环运行过程中的信息，其中，就包含了记录两类事件的变量，分别是：\n\n * aefileevent 类型的指针 *events，表示 io 事件。之所以类型名称为 aefileevent，是因为所有的 io 事件都会用文件描述符进行标识\n * aetimeevent 类型的指针 *timeeventhead，表示时间事件，即按一定时间周期触发的事件\n\n此外，aeeventloop 结构体中还有一个 aefiredevent 类型的指针 *fired，这个并不是一类专门的事件类型，它只是用来记录已触发事件对应的文件描述符信息\n\n下面的代码显示了 redis 中事件循环的结构体定义，你可以看下\n\ntypedef struct aeeventloop {\n    …\n    aefileevent *events; //io事件数组\n    aefiredevent *fired; //已触发事件数组\n    aetimeevent *timeeventhead; //记录时间事件的链表头\n    …\n    void *apidata; //和api调用接口相关的数据\n    aebeforesleepproc *beforesleep; //进入事件循环流程前执行的函数\n    aebeforesleepproc *aftersleep;  //退出事件循环流程后执行的函数\n    \n} aeeventloop;\n\n\n了解了 aeeventloop 结构体后，我们再来看下，这个结构体是如何初始化的，这其中就包括了 io 事件数组和时间事件链表的初始化。\n\n\n# aecreateeventloop 函数的初始化操作\n\n因为 redis server 在完成初始化后，就要开始运行事件驱动框架的循环流程，所以，aeeventloop 结构体在server.c的 initserver 函数中，就通过调用 **aecreateeventloop 函数 **进行初始化了。这个函数的参数只有一个，是 setsize\n\n下面的代码展示了 initserver 函数中对 aecreateeventloop 函数的调用。\n\ninitserver() {\n    …\n    //调用 aecreateeventloop 函数创建 aeeventloop 结构体，并赋值给 server 结构的 el 变量\n    server.el = aecreateeventloop(server.maxclients+config_fdset_incr);\n    …\n}\n\n\n从这里我们可以看到 参数 setsize 的大小，其实是由 server 结构的 maxclients 变量和宏定义 config_fdset_incr 共同决定的。其中，maxclients 变量的值大小，可以在 redis 的配置文件 redis.conf 中进行定义，默认值是 1000。而宏定义 config_fdset_incr 的大小，等于宏定义 config_min_reserved_fds 的值再加上 96，如下所示，这里的两个宏定义都是在server.h文件中定义的。\n\n#define config_min_reserved_fds 32\n#define config_fdset_incr (config_min_reserved_fds+96)\n\n\n好了，到这里，你可能有疑问了：aecreateeventloop 函数的参数 setsize，设置为最大客户端数量加上一个宏定义值，可是这个参数有什么用呢？这就和 aecreateeventloop 函数具体执行的初始化操作有关了。\n\n接下来，我们就来看下 aecreateeventloop 函数执行的操作，大致可以分成以下三个步骤。\n\n第一步，aecreateeventloop 函数会创建一个 aeeventloop 结构体类型的变量 eventloop。然后，该函数会给 eventloop 的成员变量分配内存空间，比如，按照传入的参数 setsize，给 io 事件数组和已触发事件数组分配相应的内存空间。此外，该函数还会给 eventloop 的成员变量赋初始值\n\n第二步，aecreateeventloop 函数会调用 aeapicreate 函数。aeapicreate 函数封装了操作系统提供的 io 多路复用函数，假设 redis 运行在 linux 操作系统上，并且 io 多路复用机制是 epoll，那么此时，aeapicreate 函数就会调用 epoll_create 创建 epoll 实例，同时会创建 epoll_event 结构的数组，数组大小等于参数 setsize\n\n这里你需要注意，aeapicreate 函数是把创建的 epoll 实例描述符和 epoll_event 数组，保存在了 aeapistate 结构体类型的变量 state，如下所示：\n\ntypedef struct aeapistate {  //aeapistate结构体定义\n    int epfd;   //epoll实例的描述符\n    struct epoll_event *events;   //epoll_event结构体数组，记录监听事件\n} aeapistate;\n\nstatic int aeapicreate(aeeventloop *eventloop) {\n    aeapistate *state = zmalloc(sizeof(aeapistate));\n    ...\n    //将epoll_event数组保存在aeapistate结构体变量state中\n    state->events = zmalloc(sizeof(struct epoll_event)*eventloop->setsize);\n    ...\n    //将epoll实例描述符保存在aeapistate结构体变量state中\n    state->epfd = epoll_create(1024);\n    ···\n}\n\n\n紧接着，aeapicreate 函数把 state 变量赋值给 eventloop 中的 apidata 。这样一来，eventloop 结构体中就有了 epoll 实例和 epoll_event 数组的信息，这样就可以用来基于 epoll 创建和处理事件了。我一会儿还会给你具体介绍。\n\neventloop->apidata = state;\n\n\n第三步，aecreateeventloop 函数会把所有网络 io 事件对应文件描述符的掩码，初始化为 ae_none，表示暂时不对任何事件进行监听\n\n我把 aecreateeventloop 函数的主要部分代码放在这里，你可以看下。\n\naeeventloop *aecreateeventloop(int setsize) {\n    aeeventloop *eventloop;\n    int i;\n\n    //给eventloop变量分配内存空间\n\tif ((eventloop = zmalloc(sizeof(*eventloop))) == null) goto err;\n\n\t//给io事件、已触发事件分配内存空间\n    eventloop->events = zmalloc(sizeof(aefileevent)*setsize);\n    eventloop->fired = zmalloc(sizeof(aefiredevent)*setsize);\n    …\n    eventloop->setsize = setsize;\n    eventloop->lasttime = time(null);\n\n    //设置时间事件的链表头为null\n    eventloop->timeeventhead = null;\n\t…\n\t//调用aeapicreate函数，去实际调用操作系统提供的io多路复用函数\n\tif (aeapicreate(eventloop) == -1) goto err;\n\n    //将所有网络io事件对应文件描述符的掩码设置为ae_none\n    for (i = 0; i < setsize; i++)\n        eventloop->events[i].mask = ae_none;\n    return eventloop;\n\n    //初始化失败后的处理逻辑，\n    err:\n    …\n}\n\n\n好，那么从 aecreateeventloop 函数的执行流程中，我们其实可以看到以下 两个关键点：\n\n * 事件驱动框架监听的 io 事件数组大小就等于参数 setsize，这样决定了和 redis server 连接的客户端数量。所以，当你遇到客户端连接 redis 时报错“max number of clients reached”，你就可以去 redis.conf 文件修改 maxclients 配置项，以扩充框架能监听的客户端数量。\n * 当使用 linux 系统的 epoll 机制时，框架循环流程初始化操作，会通过 aeapicreate 函数创建 epoll_event 结构数组，并调用 epoll_create 函数创建 epoll 实例，这都是使用 epoll 机制的准备工作要求\n\n到这里，框架就可以创建和处理具体的 io 事件和时间事件了。所以接下来，我们就先来了解下 io 事件及其处理机制。\n\n\n# io 事件处理\n\nredis 的 io 事件主要包括三类，分别是\n\n * 可读事件：从客户端读取数据\n * 可写事件：向客户端写入数据\n * 屏障事件：屏障事件的主要作用是用来反转事件的处理顺序。比如在默认情况下，redis 会先给客户端返回结果，但是如果面临需要把数据尽快写入磁盘的情况，redis 就会用到屏障事件，把写数据和回复客户端的顺序做下调整，先把数据落盘，再给客户端回复。\n\n在 redis 源码中，io 事件的数据结构是 aefileevent 结构体，io 事件的创建是通过 aecreatefileevent 函数来完成的。下面的代码展示了 aefileevent 结构体的定义，你可以再回顾下：\n\ntypedef struct aefileevent {\n    int mask; //掩码标记，包括可读事件、可写事件和屏障事件\n    aefileproc *rfileproc;   //处理可读事件的回调函数\n    aefileproc *wfileproc;   //处理可写事件的回调函数\n    void *clientdata;  //私有数据\n} aefileevent;\n\n\n而对于 aecreatefileevent 函数来说，在上节课我们已经了解了它是通过 aeapiaddevent 函数来完成事件注册的。那么接下来，我们再从代码级别看下它是如何执行的，这可以帮助我们更加透彻地理解，事件驱动框架对 io 事件监听是如何基于 epoll 机制对应封装的。\n\n\n# io 事件创建\n\n首先，我们来看 aecreatefileevent 函数，如下所示：\n\nint aecreatefileevent(aeeventloop *eventloop, int fd, int mask, aefileproc *proc, void *clientdata)\n{\n    if (fd >= eventloop->setsize) {\n        errno = erange;\n        return ae_err;\n    }\n    aefileevent *fe = &eventloop->events[fd];\n\n    if (aeapiaddevent(eventloop, fd, mask) == -1)\n        return ae_err;\n    fe->mask |= mask;\n    if (mask & ae_readable) fe->rfileproc = proc;\n    if (mask & ae_writable) fe->wfileproc = proc;\n    fe->clientdata = clientdata;\n    if (fd > eventloop->maxfd)\n        eventloop->maxfd = fd;\n    return ae_ok;\n}\n\n\n这个函数的参数有 5 个，分别是\n\n * 循环流程结构体*eventloop\n * io 事件对应的文件描述符 fd\n * 事件类型掩码 mask\n * 事件处理回调函数*proc\n * 事件私有数据*clientdata。\n\n因为循环流程结构体 *eventloop 中有 io 事件数组，这个数组的元素是 aefileevent 类型，所以，每个数组元素都对应记录了一个文件描述符（比如一个套接字）相关联的监听事件类型和回调函数。\n\naecreatefileevent 函数会先根据传入的文件描述符 fd，在 eventloop 的 io 事件数组中，获取该描述符关联的 io 事件指针变量*fe，如下所示：\n\naefileevent *fe = &eventloop->events[fd];\n\n\n紧接着，aecreatefileevent 函数会调用 aeapiaddevent 函数，添加要监听的事件：\n\nif (aeapiaddevent(eventloop, fd, mask) == -1)\n   return ae_err;\n\n\naeapiaddevent 函数实际上会调用操作系统提供的 io 多路复用函数，来完成事件的添加。我们还是假设 redis 实例运行在使用 epoll 机制的 linux 上，那么 aeapiaddevent 函数就会调用 epoll_ctl 函数，添加要监听的事件。我在xxx中其实已经给你介绍过 epoll_ctl 函数，这个函数会接收 4 个参数，分别是：\n\n * epoll 实例；\n * 要执行的操作类型（是添加还是修改）；\n * 要监听的文件描述符；\n * epoll_event 类型变量\n\n那么，这个调用过程是如何准备 epoll_ctl 函数需要的参数，从而完成执行的呢？\n\n首先，epoll 实例是我刚才给你介绍的 aecreateeventloop 函数，它是通过调用 aeapicreate 函数来创建的，保存在了 eventloop 结构体的 apidata 变量中，类型是 aeapistate。所以，aeapiaddevent 函数会先获取该变量，如下所示：\n\nstatic int aeapiaddevent(aeeventloop *eventloop, int fd, int mask) {\n    //从eventloop结构体中获取aeapistate变量，里面保存了epoll实例\n\taeapistate *state = eventloop->apidata;\n    ...\n }\n\n\n其次，对于要执行的操作类型的设置，aeapiaddevent 函数会根据传入的文件描述符 fd，在 eventloop 结构体中 io 事件数组中查找该 fd。因为 io 事件数组的每个元素，都对应了一个文件描述符，而该数组初始化时，每个元素的值都设置为了 ae_none。\n\n所以，如果要监听的文件描述符 fd 在数组中的类型不是 ae_none，则表明该描述符已做过设置，那么操作类型就是修改操作，对应 epoll 机制中的宏定义 epoll_ctl_mod。否则，操作类型就是添加操作，对应 epoll 机制中的宏定义 epoll_ctl_add。这部分代码如下所示：\n\n//如果文件描述符fd对应的io事件已存在，则操作类型为修改，否则为添加\n int op = eventloop->events[fd].mask == ae_none ?\n            epoll_ctl_add : epoll_ctl_mod;\n\n\n第三，epoll_ctl 函数需要的监听文件描述符，就是 aeapiaddevent 函数接收到的参数 fd。\n\n最后，epoll_ctl 函数还需要一个 epoll_event 类型变量，因此 aeapiaddevent 函数在调用 epoll_ctl 函数前，会新创建 epoll_event 类型**变量 ee。**然后，aeapiaddevent 函数会设置变量 ee 中的监听事件类型和监听文件描述符。\n\naeapiaddevent 函数的参数 mask，表示的是要监听的事件类型掩码。所以，aeapiaddevent 函数会根据掩码值是可读（ae_readable）或可写（ae_writable）事件，来设置 ee 监听的事件类型是 epollin 还是 epollout。这样一来，redis 事件驱动框架中的读写事件就能够和 epoll 机制中的读写事件对应上来。下面的代码展示了这部分逻辑，你可以看下。\n\n…\nstruct epoll_event ee = {0}; //创建epoll_event类型变量\n…\n//将可读或可写io事件类型转换为epoll监听的类型epollin或epollout\nif (mask & ae_readable) ee.events |= epollin;\nif (mask & ae_writable) ee.events |= epollout;\nee.data.fd = fd;  //将要监听的文件描述符赋值给ee\n…\n\n\n好了，到这里，aeapiaddevent 函数就准备好了 epoll 实例、操作类型、监听文件描述符以及 epoll_event 类型变量，然后，它就会调用 epoll_ctl 开始实际创建监听事件了，如下所示：\n\nstatic int aeapiaddevent(aeeventloop *eventloop, int fd, int mask) {\n    ...\n    //调用epoll_ctl实际创建监听事件\n    if (epoll_ctl(state->epfd,op,fd,&ee) == -1) return -1;\n        return 0;\n}\n\n\n了解了这些代码后，我们可以学习到事件驱动框架是如何基于 epoll，封装实现了 io 事件的创建。那么，在 redis server 启动运行后，最开始监听的 io 事件是可读事件，对应于客户端的连接请求。具体是 initserver 函数调用了 aecreatefileevent 函数，创建可读事件，并设置回调函数为 accepttcphandler，用来处理客户端连接\n\n接下来，我们再来看下一旦有了客户端连接请求后，io 事件具体是如何处理的呢？\n\n\n# 读事件处理\n\n当 redis server 接收到客户端的连接请求时，就会使用注册好的 accepttcphandler 函数 进行处理\n\naccepttcphandler 函数会接受客户端连接，并创建已连接套接字 cfd。然后，acceptcommonhandler 函数会被调用，同时，刚刚创建的已连接套接字 cfd 会作为参数，传递给 acceptcommonhandler 函数。\n\nacceptcommonhandler 函数会调用 createclient 函数创建客户端。而在 createclient 函数中，我们就会看到，aecreatefileevent 函数被再次调用了\n\n此时，aecreatefileevent 函数会针对已连接套接字上，创建监听事件，类型为 ae_readable，回调函数是 readqueryfromclient\n\n好了，到这里，事件驱动框架就增加了对一个客户端已连接套接字的监听。一旦客户端有请求发送到 server，框架就会回调 readqueryfromclient 函数处理请求。这样一来，客户端请求就能通过事件驱动框架进行处理了。\n\n下面代码展示了 createclient 函数调用 aecreatefileevent 的过程，你可以看下。\n\nclient *createclient(int fd) {\n…\nif (fd != -1) {\n        …\n        //调用aecreatefileevent，监听读事件，对应客户端读写请求，使用readqueryfromclient回调函数处理\n        if (aecreatefileevent(server.el,fd,ae_readable,\n            readqueryfromclient, c) == ae_err)\n        {\n            close(fd);\n            zfree(c);\n            return null;\n        } }\n…\n}\n\n\n为了便于你掌握从监听客户端连接请求到监听客户端常规读写请求的事件创建过程，我画了下面这张图，你可以看下\n\n\n\n\n# 写事件处理\n\nredis 实例在收到客户端请求后，会在处理客户端命令后，将要返回的数据写入客户端输出缓冲区。下图就展示了这个过程的函数调用逻辑：\n\n\n\n而在 redis 事件驱动框架每次循环进入事件处理函数前，也就是在框架主函数 aemain 中调用 aeprocessevents，来处理监听到的已触发事件或是到时的时间事件之前，都会调用 server.c 文件中的 beforesleep 函数，进行一些任务处理，这其中就包括了调用 handleclientswithpendingwrites 函数，它会将 redis sever 客户端缓冲区中的数据写回客户端。\n\n下面给出的代码是事件驱动框架的主函数 aemain。在该函数每次调用 aeprocessevents 函数前，就会调用 beforesleep 函数，你可以看下。\n\nvoid aemain(aeeventloop *eventloop) {\n    eventloop->stop = 0;\n\twhile (!eventloop->stop) {\n\t    //如果beforesleep函数不为空，则调用beforesleep函数\n        if (eventloop->beforesleep != null)\n            eventloop->beforesleep(eventloop);\n        //调用完beforesleep函数，再处理事件\n        aeprocessevents(eventloop, ae_all_events|ae_call_after_sleep);\n    }\n}\n\n\n这里你要知道，beforesleep 函数调用的 handleclientswithpendingwrites 函数，会遍历每一个待写回数据的客户端，然后调用 writetoclient 函数，将客户端输出缓冲区中的数据写回。下面这张图展示了这个流程，你可以看下。\n\n\n\n/* \n * 该函数在进入事件循环之前调用，\n * 目的是尽可能直接将响应数据写入客户端的输出缓冲区，\n * 避免使用系统调用来安装可写事件处理程序，\n * 从而提高效率。 \n */\nint handleclientswithpendingwrites(void) {\n    listiter li;\n    listnode *ln;\n    int processed = listlength(server.clients_pending_write); // 获取当前待写客户端队列的长度\n\n    listrewind(server.clients_pending_write, &li); // 将迭代器设置为从队列头开始\n    while ((ln = listnext(&li))) { // 遍历待写客户端的队列\n        client *c = listnodevalue(ln); // 获取当前节点的客户端\n        c->flags &= ~client_pending_write; // 清除客户端的“待写”标志\n        listunlinknode(server.clients_pending_write, ln); // 从待写客户端队列中移除当前节点\n\n        /* 如果客户端是受保护的，不执行任何操作以避免写入错误或重新创建处理程序 */\n        if (c->flags & client_protected) continue;\n\n        /* 如果客户端即将关闭，不需要写入操作 */\n        if (c->flags & client_close_asap) continue;\n\n        /* 尝试将缓冲区中的数据写入客户端的套接字。\n         * 如果写入失败，则跳过该客户端继续处理下一个。 */\n        if (writetoclient(c, 0) == c_err) continue;\n\n        /* 如果经过上述同步写入后仍有数据需要输出到客户端，\n         * 则需要为该客户端安装一个可写事件处理程序，以便稍后继续写入。 */\n        if (clienthaspendingreplies(c)) {\n            installclientwritehandler(c);\n        }\n    }\n    return processed; // 返回处理的客户端数量\n}\n\n\n不过，如果输出缓冲区的数据还没有写完，此时，handleclientswithpendingwrites 函数就会调用 aecreatefileevent 函数，创建可写事件，并设置回调函数 sendreplytoclient。sendreplytoclient 函数里面会调用 writetoclient 函数写回数据。\n\n> aecreatefileevent 是 redis 中的一个底层函数，用于向事件循环中注册一个新的文件事件。文件事件可以是“可读事件”（数据到来时触发）或“可写事件”（缓冲区空闲时触发）。在上述场景中，aecreatefileevent 创建的是一个“可写事件”。\n> \n> 当客户端的输出缓冲区还未完全发送完数据时，redis 不会立刻阻塞，而是通过创建“可写事件”来处理这个情况。这个可写事件表示，当 redis 发现客户端可以继续接收数据时（输出缓冲区空闲），它就会自动触发这个事件。\n> \n> 当可写事件触发时，redis 会调用 sendreplytoclient 函数。这个函数负责将剩余的数据从输出缓冲区发送给客户端。具体来说，它内部会调用 writetoclient 函数来真正执行数据发送的操作。\n\necho 认为\n\n>  1. 输出缓冲区：当 redis 需要将数据返回给客户端时，数据会先存放在一个输出缓冲区中，然后再通过网络传输给客户端。\n>  2. 缓冲区未写完：这个情况可能发生在以下几种情况下：\n>     * 客户端网络不畅：客户端处理速度较慢，或者网络带宽不足，导致一次只能从缓冲区接收一部分数据，剩余的数据暂时无法发送。(可能是 tcp 的滑动窗口中的接收方的接收窗口跟不上)\n>     * 大数据量传输：如果 redis 需要发送的数据量很大，比如一个大的查询结果，redis 可能无法在一次 write 操作中将所有数据写入客户端的网络套接字，只能先写入一部分，剩下的放在缓冲区里等待下一次写入。\n>  3. 处理机制：\n>     * 当 redis 发现缓冲区中的数据没有写完（例如，writetoclient 函数尝试发送数据时只能写入一部分），它不会等待或阻塞主线程。\n>     * 此时 redis 会调用 aecreatefileevent，创建一个可写事件，表示客户端还未完全接收数据。当客户端准备好接收更多数据时，这个可写事件会触发，回调函数 sendreplytoclient 会再次被调用，尝试将剩下的数据发送给客户端。\n> \n>  * 避免阻塞主线程：redis 是单线程的，如果由于网络问题或客户端处理能力限制，主线程被阻塞在一个客户端的发送过程中，其他客户端的请求就无法得到及时处理。\n>  * 提高性能和吞吐量：通过异步的方式处理缓冲区的剩余数据发送，redis 能在高并发的情况下更高效地处理多个客户端的请求。\n\nint handleclientswithpendingwrites(void) {\n    listiter li;\n\tlistnode *ln;\n\t…\n    //获取待写回的客户端列表\n\tlistrewind(server.clients_pending_write,&li);\n\t//遍历每一个待写回的客户端\n\twhile((ln = listnext(&li))) {\n\t   client *c = listnodevalue(ln);\n\t   …\n\t   //调用writetoclient将当前客户端的输出缓冲区数据写回\n\t   if (writetoclient(c->fd,c,0) == c_err) continue;\n\t   //如果还有待写回数据\n\t   if (clienthaspendingreplies(c)) {\n\t            int ae_flags = ae_writable;\n\t            //创建可写事件的监听，以及设置回调函数\n\t             if (aecreatefileevent(server.el, c->fd, ae_flags,\n\t                sendreplytoclient, c) == ae_err)\n\t            {\n\t                   …\n\t            }\n\t  } }\n}\n\n\n好了，我们刚才了解的是读写事件对应的回调处理函数。实际上，为了能及时处理这些事件，redis 事件驱动框架的 aemain 函数还会循环 调用 aeprocessevents 函数，来检测已触发的事件，并调用相应的回调函数进行处理。\n\n从 aeprocessevents 函数的代码中，我们可以看到该函数会调用 aeapipoll 函数，查询监听的文件描述符中，有哪些已经就绪。一旦有描述符就绪，aeprocessevents 函数就会根据事件的可读或可写类型，调用相应的回调函数进行处理。aeprocessevents 函数调用的基本流程如下所示：\n\nint aeprocessevents(aeeventloop *eventloop, int flags){\n…\n//调用aeapipoll获取就绪的描述符\nnumevents = aeapipoll(eventloop, tvp);\n…\nfor (j = 0; j < numevents; j++) {\n\taefileevent *fe = &eventloop->events[eventloop->fired[j].fd];\n\t…\n    //如果触发的是可读事件，调用事件注册时设置的读事件回调处理函数\n\tif (!invert && fe->mask & mask & ae_readable) {\n\t      fe->rfileproc(eventloop,fd,fe->clientdata,mask);\n\t                fired++;\n\t}\n\n    //如果触发的是可写事件，调用事件注册时设置的写事件回调处理函数\n\tif (fe->mask & mask & ae_writable) {\n\t                if (!fired || fe->wfileproc != fe->rfileproc) {\n\t                    fe->wfileproc(eventloop,fd,fe->clientdata,mask);\n\t                    fired++;\n\t                }\n\t            }\n\t…\n\t} }\n\t…\n}\n\n\n到这里，我们就了解了 io 事件的创建函数 aecreatefileevent，以及在处理客户端请求时对应的读写事件和它们的处理函数。那么接下来，我们再来看看事件驱动框架中的时间事件是怎么创建和处理的。\n\n\n# 时间事件处理\n\n其实，相比于 io 事件有可读、可写、屏障类型，以及不同类型 io 事件有不同回调函数来说，时间事件的处理就比较简单了。下面，我们就来分别学习下它的定义、创建、回调函数和触发处理。\n\n\n# 时间事件定义\n\n首先，我们来看下时间事件的结构体定义，代码如下所示：\n\ntypedef struct aetimeevent {\n    long long id; //时间事件id\n    long when_sec; //事件到达的秒级时间戳\n    long when_ms; //事件到达的毫秒级时间戳\n    aetimeproc *timeproc; //时间事件触发后的处理函数\n    aeeventfinalizerproc *finalizerproc;  //事件结束后的处理函数\n    void *clientdata; //事件相关的私有数据\n    struct aetimeevent *prev;  //时间事件链表的前向指针\n    struct aetimeevent *next;  //时间事件链表的后向指针\n} aetimeevent;\n\n\n时间事件结构体中主要的变量，包括以秒记录和以毫秒记录的时间事件触发时的时间戳 when_sec 和 when_ms，以及时间事件触发后的处理函数*timeproc。另外，在时间事件的结构体中，还包含了前向和后向指针*prev和*next，这表明时间事件是以链表的形式组织起来的。\n\n在了解了时间事件结构体的定义以后，我们接着来看下，时间事件是如何创建的。\n\n\n# 时间事件创建\n\n与 io 事件创建使用 aecreatefileevent 函数类似，时间事件的创建函数是 aecreatetimeevent 函数。这个函数的原型定义如下所示：\n\nlong long aecreatetimeevent(aeeventloop *eventloop, long long milliseconds, aetimeproc *proc, void *clientdata, aeeventfinalizerproc *finalizerproc)\n\n\n在它的参数中，有两个需要我们重点了解下，以便于我们理解时间事件的处理。\n\n * 一个是milliseconds，这是所创建时间事件的触发时间距离当前时间的时长，是用毫秒表示的。\n * 另一个是***proc**，这是所创建时间事件触发后的回调函数。\n\naecreatetimeevent 函数的执行逻辑不复杂，主要就是创建一个时间事件的变量 te，对它进行初始化，并把它插入到框架循环流程结构体 eventloop 中的时间事件链表中。在这个过程中，aecreatetimeevent 函数会调用 aeaddmillisecondstonow 函数，根据传入的 milliseconds 参数，计算所创建时间事件具体的触发时间戳，并赋值给 te。\n\n实际上，redis server 在初始化时，除了创建监听的 io 事件外，也会调用 aecreatetimeevent 函数创建时间事件。下面代码显示了 initserver 函数对 aecreatetimeevent 函数的调用：\n\ninitserver() {\n    …\n    //创建时间事件\n    if (aecreatetimeevent(server.el, 1, servercron, null, null) == ae_err){\n    … //报错信息\n    }\n}\n\n\n从代码中，我们可以看到，时间事件触发后的回调函数是 servercron。所以接下来，我们就来了解下 servercron 函数。\n\n\n# 时间事件回调函数\n\nservercron 函数是在 server.c 文件中实现的。一方面，它会顺序调用一些函数，来实现时间事件被触发后，执行一些后台任务。比如，servercron 函数会检查是否有进程结束信号，若有就执行 server 关闭操作。servercron 会调用 databasecron 函数，处理过期 key 或进行 rehash 等。你可以参考下面给出的代码：\n\n//如果收到进程结束信号，则执行server关闭操作\nif (server.shutdown_asap) {\n    if (prepareforshutdown(shutdown_noflags) == c_ok) exit(0);\n    ...\n}\n...\nclientcron();  //执行客户端的异步操作\ndatabasecron(); //执行数据库的后台操作\n\n\n另一方面，servercron 函数还会以不同的频率周期性执行一些任务，这是通过执行宏 run_with_period 来实现的。\n\nrunwith_period 宏定义如下，该宏定义会根据 redis 实例配置文件 redis.conf 中定义的 hz 值，来判断参数_ms表示的时间戳是否到达。一旦到达，servercron 就可以执行相应的任务了。\n\n#define run_with_period(_ms_) if ((_ms_ <= 1000/server.hz) || !(server.cronloops%((_ms_)/(1000/server.hz))))\n\n\n比如，servercron 函数中会以 1 秒 1 次的频率，检查 aof 文件是否有写错误。如果有的话，servercron 就会调用 flushappendonlyfile 函数，再次刷回 aof 文件的缓存数据。下面的代码展示了这一周期性任务：\n\nservercron() {\n   …\n   //每1秒执行1次，检查aof是否有写错误\n   run_with_period(1000) {\n        if (server.aof_last_write_status == c_err)\n            flushappendonlyfile(0);\n    }\n   …\n}\n\n\n如果你想了解更多的周期性任务，可以再详细阅读下 servercron 函数中，以 run_with_period 宏定义包含的代码块。\n\n好了，了解了时间事件触发后的回调函数 servercron，我们最后来看下，时间事件是如何触发处理的。\n\n\n# 时间事件的触发处理\n\n其实，时间事件的检测触发比较简单，事件驱动框架的 aemain 函数会循环调用 aeprocessevents 函数，来处理各种事件。而 aeprocessevents 函数在执行流程的最后，会调用 processtimeevents 函数处理相应到时的任务。\n\naeprocessevents(){\n    …\n    //检测时间事件是否触发\n    if (flags & ae_time_events)\n            processed += processtimeevents(eventloop);\n    …\n}\n\n\n那么，具体到 proecesstimeevent 函数来说，它的基本流程就是从时间事件链表上逐一取出每一个事件，然后根据当前时间判断该事件的触发时间戳是否已满足。如果已满足，那么就调用该事件对应的回调函数进行处理。这样一来，周期性任务就能在不断循环执行的 aeprocessevents 函数中，得到执行了。\n\n下面的代码显示了 processtimeevents 函数的基本流程，你可以再看下。\n\nstatic int processtimeevents(aeeventloop *eventloop) {\n    ...\n    te = eventloop->timeeventhead;  //从时间事件链表中取出事件\n    while(te) {\n       ...\n      aegettime(&now_sec, &now_ms);  //获取当前时间\n      if (now_sec > te->when_sec || (now_sec == te->when_sec && now_ms >= te->when_ms))   //如果当前时间已经满足当前事件的触发时间戳\n      {\n         ...\n        retval = te->timeproc(eventloop, id, te->clientdata); //调用注册的回调函数处理\n        ...\n      }\n      te = te->next;   //获取下一个时间事件\n      ...\n}\n\n\n\n# 总结\n\n 1. 文件事件处理流程和时间事件处理流程\n 2. 文件事件和时间事件如何共同工作的\n 3. 事件驱动框架的初始化过程\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Redis 的执行模式",frontmatter:{title:"Redis 的执行模式",date:"2024-09-15T20:23:53.000Z",permalink:"/pages/e6d8ef/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/09.Redis%20%E7%9A%84%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%BC%8F.html",relativePath:"Redis 系统设计/03.三、主线任务/09.Redis 的执行模式.md",key:"v-b6e5e50e",path:"/pages/e6d8ef/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:268},{level:2,title:"从 bio.c 文件学习 Redis 的后台线程",slug:"从-bio-c-文件学习-redis-的后台线程",normalizedTitle:"从 bio.c 文件学习 redis 的后台线程",charIndex:621},{level:3,title:"bioInit 函数：初始化数组",slug:"bioinit-函数-初始化数组",normalizedTitle:"bioinit 函数：初始化数组",charIndex:1958},{level:3,title:"bioInit 函数：设置线程属性并创建线程",slug:"bioinit-函数-设置线程属性并创建线程",normalizedTitle:"bioinit 函数：设置线程属性并创建线程",charIndex:3142},{level:3,title:"bioProcessBackgroundJobs 函数：处理后台任务",slug:"bioprocessbackgroundjobs-函数-处理后台任务",normalizedTitle:"bioprocessbackgroundjobs 函数：处理后台任务",charIndex:4792},{level:3,title:"bioCreateBackgroundJob 函数：创建后台任务",slug:"biocreatebackgroundjob-函数-创建后台任务",normalizedTitle:"biocreatebackgroundjob 函数：创建后台任务",charIndex:7208},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:8813},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:9128}],headersStr:"前言 从 bio.c 文件学习 Redis 的后台线程 bioInit 函数：初始化数组 bioInit 函数：设置线程属性并创建线程 bioProcessBackgroundJobs 函数：处理后台任务 bioCreateBackgroundJob 函数：创建后台任务 总结 参考资料",content:'提出问题是一切智慧的开端\n\n * 为什么 Redis 的核心流程采用单线程 IO 架构，而不是多线程？\n * 哪些操作可能导致 Redis 主线程阻塞？如何用后台线程避免？\n * Redis 后台线程如何启动、管理，并与主线程协同？\n * Redis 的惰性删除机制是如何工作的？对性能有何影响？\n * 生产环境中如何根据 Redis 模型优化性能？\n * Redis 后台线程的创建机制及其与任务队列的交互？\n * 如果 Redis 的后台任务队列满了，会发生什么？\n * Redis 如何通过任务优先级或类型优化资源管理？\n\n\n# 前言\n\n所谓的执行模型，就是指 Redis 运行时使用的进程、子进程和线程的个数，以及它们各自负责的工作任务\n\n你在实际使用 Redis 的时候，可能经常会听到类似“Redis 是单线程”、“Redis 的主 IO 线程”、“Redis 包含多线程”等不同说法。我也听到不少同学提出困惑和疑问：Redis 到底是不是一个单线程的程序？\n\n其实，彻底理解这个问题，有助于指导我们保持 Redis 高性能、低延迟的特性。如果说 Redis 就是单线程程序，那么，我们就需要避免所有容易引起线程阻塞的操作；而如果说 Redis 不只是单线程，还有其他线程在工作，那么，我们就需要了解多线程各自负责什么任务，负责请求解析和数据读写的线程有几个，有哪些操作是后台线程在完成，而不会影响请求解析和数据读写的。\n\n\n# 从 bio.c 文件学习 Redis 的后台线程\n\n我们先来看下 main 函数在初始化过程最后调用的 InitServerLast 函数。InitServerLast 函数的作用是进一步调用 bioInit 函数，来创建后台线程，让 Redis 把部分任务交给后台线程处理。这个过程如下所示。\n\nvoid InitServerLast() {\n    bioInit();\n    …\n}\n\n\nbioInit函数是在bio.c文件中实现的，它的主要作用调用pthread_create函数创建多个后台线程。不过在具体了解bioInit函数之前，我们先来看下 bio.c 文件中定义的主要数组，这也是在bioInit函数中要进行初始化的。\n\nbio.c 文件针对要创建的线程，定义了pthread_t类型的数组bio_threads，用来保存创建的线程描述符。此外，bio.c 文件还创建了一个保存互斥锁的数组bio_mutex，以及两个保存条件变量的数组bio_newjob_cond和bio_step_cond。以下代码展示了这些数组的创建逻辑，你可以看下。\n\n//保存线程描述符的数组\nstatic pthread_t bio_threads[BIO_NUM_OPS];\n//保存互斥锁的数组\nstatic pthread_mutex_t bio_mutex[BIO_NUM_OPS];\n//保存条件变量的两个数组\nstatic pthread_cond_t bio_newjob_cond[BIO_NUM_OPS];\nstatic pthread_cond_t bio_step_cond[BIO_NUM_OPS];\n\n\n从中你可以注意到，这些数组的大小都是宏定义 BIO_NUM_OPS，这个宏定义是在bio.h文件中定义的，默认值为 3。\n\n同时在 bio.h 文件中，你还可以看到另外三个宏定义，分别是 BIO_CLOSE_FILE、BIO_AOF_FSYNC 和 BIO_LAZY_FREE。它们的代码如下所示：\n\n#define BIO_CLOSE_FILE    0 /* Deferred close(2) syscall. */\n#define BIO_AOF_FSYNC    1 /* Deferred AOF fsync. */\n#define BIO_LAZY_FREE     2 /* Deferred objects freeing. */\n#define BIO_NUM_OPS       3\n\n\n其中，BIO_NUM_OPS 表示的是 Redis 后台任务的类型有三种。而 BIO_CLOSE_FILE、BIO_AOF_FSYNC 和 BIO_LAZY_FREE，它们分别表示三种后台任务的操作码，这些操作码可以用来标识不同的任务。\n\n * BIO_CLOSE_FILE：文件关闭后台任务\n * BIO_AOF_FSYNC：AOF 日志同步写回后台任务\n * BIO_LAZY_FREE：惰性删除后台任务\n\n实际上，bio.c 文件创建的线程数组、互斥锁数组和条件变量数组，大小都是包含三个元素，也正是对应了这三种任务。\n\n\n# bioInit 函数：初始化数组\n\n接下来，我们再来了解下 bio.c 文件中的初始化和线程创建函数bioInit。我刚才也给你介绍过这个函数，它是main函数执行完server初始化后，通过InitServerLast函数调用的。也就是说，Redis 在完成 server 初始化后，就会创建线程来执行后台任务。\n\n所以，Redis 在运行时其实已经不止是单个线程（也就是主 IO 线程）在运行了，还会有后台线程在运行。\n\nbioInit函数首先会初始化互斥锁数组和条件变量数组。然后，该函数会调用listCreate函数，给bio_jobs这个数组的每个元素创建一个列表，同时给bio_pending数组的每个元素赋值为 0。这部分代码如下所示：\n\nfor (j = 0; j < BIO_NUM_OPS; j++) {\n    pthread_mutex_init(&bio_mutex[j],NULL);\n    pthread_cond_init(&bio_newjob_cond[j],NULL);\n    pthread_cond_init(&bio_step_cond[j],NULL);\n    bio_jobs[j] = listCreate();\n    bio_pending[j] = 0;\n}\n\n\n那么，要想了解给bio_jobs数组和bio_pending数组元素赋值的作用，我们就需要先搞清楚这两个数组的含义：\n\n * bio_jobs 数组的元素是bio_jobs结构体类型，用来表示后台任务。该结构体的成员变量包括了后台任务的创建时间 time，以及任务的参数。为该数组的每个元素创建一个列表，其实就是为每个后台线程创建一个要处理的任务列表。\n * bio_pending 数组的元素类型是unsigned long long，用来表示每种任务中，处于等待状态的任务个数。将该数组每个元素初始化为 0，其实就是表示初始时，每种任务都没有待处理的具体任务。\n\nstruct bio_job {\n    time_t time; //任务创建时间\n    void *arg1, *arg2, *arg3;  //任务参数\n};\n//以后台线程方式运行的任务列表\nstatic list *bio_jobs[BIO_NUM_OPS];\n//被阻塞的后台任务数组\nstatic unsigned long long bio_pending[BIO_NUM_OPS];\n\n\n好了，到这里，你就了解了bioInit函数执行时，会把线程互斥锁、条件变量对应数组初始化为 NULL，同时会给每个后台线程创建一个任务列表（对应bio_jobs数组的元素），以及会设置每种任务的待处理个数为 0（对应 bio_pending 数组的元素）。\n\n\n# bioInit 函数：设置线程属性并创建线程\n\n在完成了初始化之后，接下来，bioInit 函数会先通过 pthread_attr_t 类型的变量，给线程设置属性。然后，bioInit 函数会调用前面我提到的 pthread_create 函数来创建线程。\n\n不过，为了能更好地理解 bioInit 函数设置线程属性和创建线程的过程，我们需要先对 pthread_create 函数本身有所了解，该函数的原型如下所示：\n\nint  pthread_create(pthread_t *tidp, const  pthread_attr_t *attr,( void *)(*start_routine)( void *), void  *arg);\n\n\n可以看到，pthread_create 函数一共有 4 个参数，分别是：\n\n * *tidp，指向线程数据结构 pthread_t 的指针；\n * *attr，指向线程属性结构 pthread_attr_t 的指针；\n * *start_routine，线程所要运行的函数的起始地址，也是指向函数的指针；\n * *arg，传给运行函数的参数。\n\n了解了pthread_create函数之后，我们来看下bioInit函数的具体操作。\n\n首先，bioInit函数会调用pthread_attr_init函数，初始化线程属性变量attr，然后调用pthread_attr_getstacksize函数，获取线程的栈大小这一属性的当前值，并根据当前栈大小和REDIS_THREAD_STACK_SIZE宏定义的大小（默认值为 4MB），来计算最终的栈大小属性值。紧接着，bioInit函数会调用pthread_attr_setstacksize函数，来设置栈大小这一属性值。\n\n下面的代码展示了线程属性的获取、计算和设置逻辑，你可以看下。\n\npthread_attr_init(&attr);\npthread_attr_getstacksize(&attr,&stacksize);\nif (!stacksize) stacksize = 1; /针对Solaris系统做处理\n    while (stacksize < REDIS_THREAD_STACK_SIZE) stacksize *= 2;\n    pthread_attr_setstacksize(&attr, stacksize);\n\n\n我也画了一张图，展示了线程属性的这一操作过程，你可以看下。\n\n\n\n在完成线程属性的设置后，接下来，bioInit函数会通过一个 for 循环，来依次为每种后台任务创建一个线程。循环的次数是由BIO_NUM_OPS宏定义决定的，也就是 3 次。相应的，bioInit函数就会调用 3 次pthread_create函数，并创建 3 个线程。bioInit 函数让这 3 个线程执行的函数都是**bioProcessBackgroundJobs**。\n\n不过这里要注意一点，就是在这三次线程的创建过程中，传给这个函数的参数分别是 0、1、2。这个创建过程如下所示：\n\nfor (j = 0; j < BIO_NUM_OPS; j++) {\n    void *arg = (void*)(unsigned long) j;\n    if (pthread_create(&thread,&attr,bioProcessBackgroundJobs,arg) != 0) {\n    \t… //报错信息\n    }\n    bio_threads[j] = thread;\n}\n\n\n你看了这个代码，可能会有一个小疑问：为什么创建的 3 个线程，它们所运行的 bioProcessBackgroundJobs 函数接收的参数分别是 0、1、2 呢？\n\n这就和 bioProcessBackgroundJobs 函数的实现有关了，我们来具体看下。\n\n\n# bioProcessBackgroundJobs 函数：处理后台任务\n\n首先，bioProcessBackgroundJobs函数会把接收到的参数 arg，转成unsigned long类型，并赋值给 type 变量，如下所示：\n\nvoid *bioProcessBackgroundJobs(void *arg) {\n    …\n\tunsigned long type = (unsigned long) arg;\n\t…\n}\n\n\n而type 变量表示的就是后台任务的操作码。这也是我刚才给你介绍的三种后台任务类型 BIO_CLOSE_FILE、BIO_AOF_FSYNC 和 BIO_LAZY_FREE 对应的操作码，它们的取值分别为 0、1、2。\n\nbioProcessBackgroundJobs 函数的主要执行逻辑是一个 while(1)的循环。在这个循环中，bioProcessBackgroundJobs 函数会从 bio_jobs 这个数组中取出相应任务，并根据任务类型，调用具体的函数来执行。\n\n我刚才已经介绍过，bio_jobs数组的每一个元素是一个队列。而因为bio_jobs数组的元素个数，等于后台任务的类型个数（也就是 BIO_NUM_OPS），所以，bio_jobs数组的每个元素，实际上是对应了某一种后台任务的任务队列。\n\n在了解了这一点后，我们就容易理解bioProcessBackgroundJobs函数中的 while 循环了。因为传给bioProcessBackgroundJobs函数的参数，分别是 0、1、2，对应了三种任务类型，所以在这个循环中，bioProcessBackgroundJobs函数会一直不停地从某一种任务队列中，取出一个任务来执行。\n\n同时，bioProcessBackgroundJobs 函数会根据传入的任务操作类型调用相应函数，具体来说：\n\n * 任务类型是 BIO_CLOSE_FILE，则调用 close 函数；\n * 任务类型是 BIO_AOF_FSYNC，则调用 redis_fsync 函数；\n * 任务类型是 BIO_LAZY_FREE，则再根据参数个数等情况，分别调用 lazyfreeFreeObjectFromBioThread、lazyfreeFreeDatabaseFromBioThread 和 lazyfreeFreeSlotsMapFromBioThread 这三个函数。\n\n最后，当某个任务执行完成后，bioProcessBackgroundJobs 函数会从任务队列中，把这个任务对应的数据结构删除。我把这部分代码放在这里，你可以看下。\n\nwhile(1) {\n        listNode *ln;\n\n        …\n        //从类型为type的任务队列中获取第一个任务\n        ln = listFirst(bio_jobs[type]);\n        job = ln->value;\n\n        …\n        //判断当前处理的后台任务类型是哪一种\n        if (type == BIO_CLOSE_FILE) {\n            close((long)job->arg1);  //如果是关闭文件任务，那就调用close函数\n        } else if (type == BIO_AOF_FSYNC) {\n            redis_fsync((long)job->arg1); //如果是AOF同步写任务，那就调用redis_fsync函数\n        } else if (type == BIO_LAZY_FREE) {\n            //如果是惰性删除任务，那根据任务的参数分别调用不同的惰性删除函数执行\n            if (job->arg1)\n                lazyfreeFreeObjectFromBioThread(job->arg1);\n            else if (job->arg2 && job->arg3)\n                lazyfreeFreeDatabaseFromBioThread(job->arg2,job->arg3);\n            else if (job->arg3)\n                lazyfreeFreeSlotsMapFromBioThread(job->arg3);\n        } else {\n            serverPanic("Wrong job type in bioProcessBackgroundJobs().");\n        }\n        …\n        //任务执行完成后，调用 listDelNode 在任务队列中删除该任务\n        listDelNode(bio_jobs[type],ln);\n        //将对应的等待任务个数减一。\n        bio_pending[type]--;\n        …\n}\n\n\n所以说，bioInit 函数其实就是创建了 3 个线程，每个线程不停地去查看任务队列中是否有任务，如果有任务，就调用具体函数执行。\n\n你可以再参考回顾下图所展示的bioInit函数和bioProcessBackgroundJobs函数的基本处理流程。\n\n\n\n不过接下来你或许还会疑惑：既然 bioProcessBackgroundJobs 函数是负责执行任务的，那么哪个函数负责生成任务呢？\n\n这就是下面，我要给你介绍的 后台任务创建函数 bioCreateBackgroundJob\n\n\n# bioCreateBackgroundJob 函数：创建后台任务\n\nbioCreateBackgroundJob函数的原型如下，它会接收 4 个参数，其中，参数 type 表示该后台任务的类型，剩下来的 3 个参数，则对应了后台任务函数的参数，如下所示：\n\nvoid bioCreateBackgroundJob(int type, void *arg1, void *arg2, void *arg3)\n\n\nbioCreateBackgroundJob函数在执行时，会先创建bio_job，这是后台任务对应的数据结构。然后，后台任务数据结构中的参数，会被设置为bioCreateBackgroundJob函数传入的参数 arg1、arg2 和 arg3。\n\n最后，bioCreateBackgroundJob函数调用listAddNodeTail函数，将刚才创建的任务加入到对应的bio_jobs队列中，同时，将bio_pending数组的对应值加 1，表示有个任务在等待执行。\n\n{\n    //创建新的任务\n    struct bio_job *job = zmalloc(sizeof(*job));\n    //设置任务数据结构中的参数\n    job->time = time(NULL);\n    job->arg1 = arg1;\n    job->arg2 = arg2;\n    job->arg3 = arg3;\n    pthread_mutex_lock(&bio_mutex[type]);\n    listAddNodeTail(bio_jobs[type],job);  //将任务加到bio_jobs数组的对应任务列表中\n    bio_pending[type]++; //将对应任务列表上等待处理的任务个数加1\n    pthread_cond_signal(&bio_newjob_cond[type]);\n    pthread_mutex_unlock(&bio_mutex[type]);\n}\n\n\n好了，这样一来，当 Redis 进程想要启动一个后台任务时，只要调用bioCreateBackgroundJob函数，并设置好该任务对应的类型和参数即可。然后，bioCreateBackgroundJob函数就会把创建好的任务数据结构，放到后台任务对应的队列中。另一方面，bioInit函数在 Redis server 启动时，创建的线程会不断地轮询后台任务队列，一旦发现有任务可以执行，就会将该任务取出并执行。\n\n其实，这种设计方式是典型的生产者-消费者模型。bioCreateBackgroundJob函数是生产者，负责往每种任务队列中加入要执行的后台任务，而bioProcessBackgroundJobs函数是消费者，负责从每种任务队列中取出任务来执行。然后 Redis 创建的后台线程，会调用bioProcessBackgroundJobs函数，从而实现一直循环检查任务队列。\n\n下图展示的就是bioCreateBackgroundJob和bioProcessBackgroundJobs两者间的生产者-消费者模型，你可以看下。\n\n\n\n好了，到这里，我们就学习了 Redis 后台线程的创建和运行机制。简单来说，主要是以下三个关键点：\n\n * Redis 是先通过 bioInit 函数初始化和创建后台线程\n * 后台线程运行的是 bioProcessBackgroundJobs 函数，这个函数会轮询任务队列，并根据要处理的任务类型，调用相应函数进行处理\n * 后台线程要处理的任务是由 bioCreateBackgroundJob函数来创建的，这些任务创建后会被放到任务队列中，等待bioProcessBackgroundJobs 函数处理\n\n\n# 总结\n\necho 你介绍了 Redis 的执行模型，并且也从源码的角度出发，通过分析代码，带你了解了 Redis 进程创建、以子进程方式创建的守护进程、以及后台线程和它们负责的工作任务。同时，这也解答了你在面试中可能经常会被问到的问题：Redis 是单线程程序吗？\n\n事实上，Redis server 启动后，它的主要工作包括接收客户端请求、解析请求和进行数据读写等操作，是由单线程来执行的，这也是我们常说 Redis 是单线程程序的原因。\n\n但是，学完这节课你应该也知道，Redis 还启动了 3 个线程来执行文件关闭、AOF 同步写和惰性删除等操作，从这个角度来说，Redis 又不能算单线程程序，它还是有多线程的。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n * 为什么 redis 的核心流程采用单线程 io 架构，而不是多线程？\n * 哪些操作可能导致 redis 主线程阻塞？如何用后台线程避免？\n * redis 后台线程如何启动、管理，并与主线程协同？\n * redis 的惰性删除机制是如何工作的？对性能有何影响？\n * 生产环境中如何根据 redis 模型优化性能？\n * redis 后台线程的创建机制及其与任务队列的交互？\n * 如果 redis 的后台任务队列满了，会发生什么？\n * redis 如何通过任务优先级或类型优化资源管理？\n\n\n# 前言\n\n所谓的执行模型，就是指 redis 运行时使用的进程、子进程和线程的个数，以及它们各自负责的工作任务\n\n你在实际使用 redis 的时候，可能经常会听到类似“redis 是单线程”、“redis 的主 io 线程”、“redis 包含多线程”等不同说法。我也听到不少同学提出困惑和疑问：redis 到底是不是一个单线程的程序？\n\n其实，彻底理解这个问题，有助于指导我们保持 redis 高性能、低延迟的特性。如果说 redis 就是单线程程序，那么，我们就需要避免所有容易引起线程阻塞的操作；而如果说 redis 不只是单线程，还有其他线程在工作，那么，我们就需要了解多线程各自负责什么任务，负责请求解析和数据读写的线程有几个，有哪些操作是后台线程在完成，而不会影响请求解析和数据读写的。\n\n\n# 从 bio.c 文件学习 redis 的后台线程\n\n我们先来看下 main 函数在初始化过程最后调用的 initserverlast 函数。initserverlast 函数的作用是进一步调用 bioinit 函数，来创建后台线程，让 redis 把部分任务交给后台线程处理。这个过程如下所示。\n\nvoid initserverlast() {\n    bioinit();\n    …\n}\n\n\nbioinit函数是在bio.c文件中实现的，它的主要作用调用pthread_create函数创建多个后台线程。不过在具体了解bioinit函数之前，我们先来看下 bio.c 文件中定义的主要数组，这也是在bioinit函数中要进行初始化的。\n\nbio.c 文件针对要创建的线程，定义了pthread_t类型的数组bio_threads，用来保存创建的线程描述符。此外，bio.c 文件还创建了一个保存互斥锁的数组bio_mutex，以及两个保存条件变量的数组bio_newjob_cond和bio_step_cond。以下代码展示了这些数组的创建逻辑，你可以看下。\n\n//保存线程描述符的数组\nstatic pthread_t bio_threads[bio_num_ops];\n//保存互斥锁的数组\nstatic pthread_mutex_t bio_mutex[bio_num_ops];\n//保存条件变量的两个数组\nstatic pthread_cond_t bio_newjob_cond[bio_num_ops];\nstatic pthread_cond_t bio_step_cond[bio_num_ops];\n\n\n从中你可以注意到，这些数组的大小都是宏定义 bio_num_ops，这个宏定义是在bio.h文件中定义的，默认值为 3。\n\n同时在 bio.h 文件中，你还可以看到另外三个宏定义，分别是 bio_close_file、bio_aof_fsync 和 bio_lazy_free。它们的代码如下所示：\n\n#define bio_close_file    0 /* deferred close(2) syscall. */\n#define bio_aof_fsync    1 /* deferred aof fsync. */\n#define bio_lazy_free     2 /* deferred objects freeing. */\n#define bio_num_ops       3\n\n\n其中，bio_num_ops 表示的是 redis 后台任务的类型有三种。而 bio_close_file、bio_aof_fsync 和 bio_lazy_free，它们分别表示三种后台任务的操作码，这些操作码可以用来标识不同的任务。\n\n * bio_close_file：文件关闭后台任务\n * bio_aof_fsync：aof 日志同步写回后台任务\n * bio_lazy_free：惰性删除后台任务\n\n实际上，bio.c 文件创建的线程数组、互斥锁数组和条件变量数组，大小都是包含三个元素，也正是对应了这三种任务。\n\n\n# bioinit 函数：初始化数组\n\n接下来，我们再来了解下 bio.c 文件中的初始化和线程创建函数bioinit。我刚才也给你介绍过这个函数，它是main函数执行完server初始化后，通过initserverlast函数调用的。也就是说，redis 在完成 server 初始化后，就会创建线程来执行后台任务。\n\n所以，redis 在运行时其实已经不止是单个线程（也就是主 io 线程）在运行了，还会有后台线程在运行。\n\nbioinit函数首先会初始化互斥锁数组和条件变量数组。然后，该函数会调用listcreate函数，给bio_jobs这个数组的每个元素创建一个列表，同时给bio_pending数组的每个元素赋值为 0。这部分代码如下所示：\n\nfor (j = 0; j < bio_num_ops; j++) {\n    pthread_mutex_init(&bio_mutex[j],null);\n    pthread_cond_init(&bio_newjob_cond[j],null);\n    pthread_cond_init(&bio_step_cond[j],null);\n    bio_jobs[j] = listcreate();\n    bio_pending[j] = 0;\n}\n\n\n那么，要想了解给bio_jobs数组和bio_pending数组元素赋值的作用，我们就需要先搞清楚这两个数组的含义：\n\n * bio_jobs 数组的元素是bio_jobs结构体类型，用来表示后台任务。该结构体的成员变量包括了后台任务的创建时间 time，以及任务的参数。为该数组的每个元素创建一个列表，其实就是为每个后台线程创建一个要处理的任务列表。\n * bio_pending 数组的元素类型是unsigned long long，用来表示每种任务中，处于等待状态的任务个数。将该数组每个元素初始化为 0，其实就是表示初始时，每种任务都没有待处理的具体任务。\n\nstruct bio_job {\n    time_t time; //任务创建时间\n    void *arg1, *arg2, *arg3;  //任务参数\n};\n//以后台线程方式运行的任务列表\nstatic list *bio_jobs[bio_num_ops];\n//被阻塞的后台任务数组\nstatic unsigned long long bio_pending[bio_num_ops];\n\n\n好了，到这里，你就了解了bioinit函数执行时，会把线程互斥锁、条件变量对应数组初始化为 null，同时会给每个后台线程创建一个任务列表（对应bio_jobs数组的元素），以及会设置每种任务的待处理个数为 0（对应 bio_pending 数组的元素）。\n\n\n# bioinit 函数：设置线程属性并创建线程\n\n在完成了初始化之后，接下来，bioinit 函数会先通过 pthread_attr_t 类型的变量，给线程设置属性。然后，bioinit 函数会调用前面我提到的 pthread_create 函数来创建线程。\n\n不过，为了能更好地理解 bioinit 函数设置线程属性和创建线程的过程，我们需要先对 pthread_create 函数本身有所了解，该函数的原型如下所示：\n\nint  pthread_create(pthread_t *tidp, const  pthread_attr_t *attr,( void *)(*start_routine)( void *), void  *arg);\n\n\n可以看到，pthread_create 函数一共有 4 个参数，分别是：\n\n * *tidp，指向线程数据结构 pthread_t 的指针；\n * *attr，指向线程属性结构 pthread_attr_t 的指针；\n * *start_routine，线程所要运行的函数的起始地址，也是指向函数的指针；\n * *arg，传给运行函数的参数。\n\n了解了pthread_create函数之后，我们来看下bioinit函数的具体操作。\n\n首先，bioinit函数会调用pthread_attr_init函数，初始化线程属性变量attr，然后调用pthread_attr_getstacksize函数，获取线程的栈大小这一属性的当前值，并根据当前栈大小和redis_thread_stack_size宏定义的大小（默认值为 4mb），来计算最终的栈大小属性值。紧接着，bioinit函数会调用pthread_attr_setstacksize函数，来设置栈大小这一属性值。\n\n下面的代码展示了线程属性的获取、计算和设置逻辑，你可以看下。\n\npthread_attr_init(&attr);\npthread_attr_getstacksize(&attr,&stacksize);\nif (!stacksize) stacksize = 1; /针对solaris系统做处理\n    while (stacksize < redis_thread_stack_size) stacksize *= 2;\n    pthread_attr_setstacksize(&attr, stacksize);\n\n\n我也画了一张图，展示了线程属性的这一操作过程，你可以看下。\n\n\n\n在完成线程属性的设置后，接下来，bioinit函数会通过一个 for 循环，来依次为每种后台任务创建一个线程。循环的次数是由bio_num_ops宏定义决定的，也就是 3 次。相应的，bioinit函数就会调用 3 次pthread_create函数，并创建 3 个线程。bioinit 函数让这 3 个线程执行的函数都是**bioprocessbackgroundjobs**。\n\n不过这里要注意一点，就是在这三次线程的创建过程中，传给这个函数的参数分别是 0、1、2。这个创建过程如下所示：\n\nfor (j = 0; j < bio_num_ops; j++) {\n    void *arg = (void*)(unsigned long) j;\n    if (pthread_create(&thread,&attr,bioprocessbackgroundjobs,arg) != 0) {\n    \t… //报错信息\n    }\n    bio_threads[j] = thread;\n}\n\n\n你看了这个代码，可能会有一个小疑问：为什么创建的 3 个线程，它们所运行的 bioprocessbackgroundjobs 函数接收的参数分别是 0、1、2 呢？\n\n这就和 bioprocessbackgroundjobs 函数的实现有关了，我们来具体看下。\n\n\n# bioprocessbackgroundjobs 函数：处理后台任务\n\n首先，bioprocessbackgroundjobs函数会把接收到的参数 arg，转成unsigned long类型，并赋值给 type 变量，如下所示：\n\nvoid *bioprocessbackgroundjobs(void *arg) {\n    …\n\tunsigned long type = (unsigned long) arg;\n\t…\n}\n\n\n而type 变量表示的就是后台任务的操作码。这也是我刚才给你介绍的三种后台任务类型 bio_close_file、bio_aof_fsync 和 bio_lazy_free 对应的操作码，它们的取值分别为 0、1、2。\n\nbioprocessbackgroundjobs 函数的主要执行逻辑是一个 while(1)的循环。在这个循环中，bioprocessbackgroundjobs 函数会从 bio_jobs 这个数组中取出相应任务，并根据任务类型，调用具体的函数来执行。\n\n我刚才已经介绍过，bio_jobs数组的每一个元素是一个队列。而因为bio_jobs数组的元素个数，等于后台任务的类型个数（也就是 bio_num_ops），所以，bio_jobs数组的每个元素，实际上是对应了某一种后台任务的任务队列。\n\n在了解了这一点后，我们就容易理解bioprocessbackgroundjobs函数中的 while 循环了。因为传给bioprocessbackgroundjobs函数的参数，分别是 0、1、2，对应了三种任务类型，所以在这个循环中，bioprocessbackgroundjobs函数会一直不停地从某一种任务队列中，取出一个任务来执行。\n\n同时，bioprocessbackgroundjobs 函数会根据传入的任务操作类型调用相应函数，具体来说：\n\n * 任务类型是 bio_close_file，则调用 close 函数；\n * 任务类型是 bio_aof_fsync，则调用 redis_fsync 函数；\n * 任务类型是 bio_lazy_free，则再根据参数个数等情况，分别调用 lazyfreefreeobjectfrombiothread、lazyfreefreedatabasefrombiothread 和 lazyfreefreeslotsmapfrombiothread 这三个函数。\n\n最后，当某个任务执行完成后，bioprocessbackgroundjobs 函数会从任务队列中，把这个任务对应的数据结构删除。我把这部分代码放在这里，你可以看下。\n\nwhile(1) {\n        listnode *ln;\n\n        …\n        //从类型为type的任务队列中获取第一个任务\n        ln = listfirst(bio_jobs[type]);\n        job = ln->value;\n\n        …\n        //判断当前处理的后台任务类型是哪一种\n        if (type == bio_close_file) {\n            close((long)job->arg1);  //如果是关闭文件任务，那就调用close函数\n        } else if (type == bio_aof_fsync) {\n            redis_fsync((long)job->arg1); //如果是aof同步写任务，那就调用redis_fsync函数\n        } else if (type == bio_lazy_free) {\n            //如果是惰性删除任务，那根据任务的参数分别调用不同的惰性删除函数执行\n            if (job->arg1)\n                lazyfreefreeobjectfrombiothread(job->arg1);\n            else if (job->arg2 && job->arg3)\n                lazyfreefreedatabasefrombiothread(job->arg2,job->arg3);\n            else if (job->arg3)\n                lazyfreefreeslotsmapfrombiothread(job->arg3);\n        } else {\n            serverpanic("wrong job type in bioprocessbackgroundjobs().");\n        }\n        …\n        //任务执行完成后，调用 listdelnode 在任务队列中删除该任务\n        listdelnode(bio_jobs[type],ln);\n        //将对应的等待任务个数减一。\n        bio_pending[type]--;\n        …\n}\n\n\n所以说，bioinit 函数其实就是创建了 3 个线程，每个线程不停地去查看任务队列中是否有任务，如果有任务，就调用具体函数执行。\n\n你可以再参考回顾下图所展示的bioinit函数和bioprocessbackgroundjobs函数的基本处理流程。\n\n\n\n不过接下来你或许还会疑惑：既然 bioprocessbackgroundjobs 函数是负责执行任务的，那么哪个函数负责生成任务呢？\n\n这就是下面，我要给你介绍的 后台任务创建函数 biocreatebackgroundjob\n\n\n# biocreatebackgroundjob 函数：创建后台任务\n\nbiocreatebackgroundjob函数的原型如下，它会接收 4 个参数，其中，参数 type 表示该后台任务的类型，剩下来的 3 个参数，则对应了后台任务函数的参数，如下所示：\n\nvoid biocreatebackgroundjob(int type, void *arg1, void *arg2, void *arg3)\n\n\nbiocreatebackgroundjob函数在执行时，会先创建bio_job，这是后台任务对应的数据结构。然后，后台任务数据结构中的参数，会被设置为biocreatebackgroundjob函数传入的参数 arg1、arg2 和 arg3。\n\n最后，biocreatebackgroundjob函数调用listaddnodetail函数，将刚才创建的任务加入到对应的bio_jobs队列中，同时，将bio_pending数组的对应值加 1，表示有个任务在等待执行。\n\n{\n    //创建新的任务\n    struct bio_job *job = zmalloc(sizeof(*job));\n    //设置任务数据结构中的参数\n    job->time = time(null);\n    job->arg1 = arg1;\n    job->arg2 = arg2;\n    job->arg3 = arg3;\n    pthread_mutex_lock(&bio_mutex[type]);\n    listaddnodetail(bio_jobs[type],job);  //将任务加到bio_jobs数组的对应任务列表中\n    bio_pending[type]++; //将对应任务列表上等待处理的任务个数加1\n    pthread_cond_signal(&bio_newjob_cond[type]);\n    pthread_mutex_unlock(&bio_mutex[type]);\n}\n\n\n好了，这样一来，当 redis 进程想要启动一个后台任务时，只要调用biocreatebackgroundjob函数，并设置好该任务对应的类型和参数即可。然后，biocreatebackgroundjob函数就会把创建好的任务数据结构，放到后台任务对应的队列中。另一方面，bioinit函数在 redis server 启动时，创建的线程会不断地轮询后台任务队列，一旦发现有任务可以执行，就会将该任务取出并执行。\n\n其实，这种设计方式是典型的生产者-消费者模型。biocreatebackgroundjob函数是生产者，负责往每种任务队列中加入要执行的后台任务，而bioprocessbackgroundjobs函数是消费者，负责从每种任务队列中取出任务来执行。然后 redis 创建的后台线程，会调用bioprocessbackgroundjobs函数，从而实现一直循环检查任务队列。\n\n下图展示的就是biocreatebackgroundjob和bioprocessbackgroundjobs两者间的生产者-消费者模型，你可以看下。\n\n\n\n好了，到这里，我们就学习了 redis 后台线程的创建和运行机制。简单来说，主要是以下三个关键点：\n\n * redis 是先通过 bioinit 函数初始化和创建后台线程\n * 后台线程运行的是 bioprocessbackgroundjobs 函数，这个函数会轮询任务队列，并根据要处理的任务类型，调用相应函数进行处理\n * 后台线程要处理的任务是由 biocreatebackgroundjob函数来创建的，这些任务创建后会被放到任务队列中，等待bioprocessbackgroundjobs 函数处理\n\n\n# 总结\n\necho 你介绍了 redis 的执行模型，并且也从源码的角度出发，通过分析代码，带你了解了 redis 进程创建、以子进程方式创建的守护进程、以及后台线程和它们负责的工作任务。同时，这也解答了你在面试中可能经常会被问到的问题：redis 是单线程程序吗？\n\n事实上，redis server 启动后，它的主要工作包括接收客户端请求、解析请求和进行数据读写等操作，是由单线程来执行的，这也是我们常说 redis 是单线程程序的原因。\n\n但是，学完这节课你应该也知道，redis 还启动了 3 个线程来执行文件关闭、aof 同步写和惰性删除等操作，从这个角度来说，redis 又不能算单线程程序，它还是有多线程的。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Redis 多IO线程",frontmatter:{title:"Redis 多IO线程",date:"2024-09-17T20:15:03.000Z",permalink:"/pages/0850b6/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%89%E3%80%81%E4%B8%BB%E7%BA%BF%E4%BB%BB%E5%8A%A1/16.Redis%20%E5%A4%9AIO%E7%BA%BF%E7%A8%8B.html",relativePath:"Redis 系统设计/03.三、主线任务/16.Redis 多IO线程.md",key:"v-58840c14",path:"/pages/0850b6/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:353},{level:2,title:"单线程 IO 及其缺陷",slug:"单线程-io-及其缺陷",normalizedTitle:"单线程 io 及其缺陷",charIndex:705},{level:3,title:"异步处理",slug:"异步处理",normalizedTitle:"异步处理",charIndex:721},{level:3,title:"事件驱动",slug:"事件驱动",normalizedTitle:"事件驱动",charIndex:1411},{level:3,title:"单线程 IO 的瓶颈",slug:"单线程-io-的瓶颈",normalizedTitle:"单线程 io 的瓶颈",charIndex:2646},{level:2,title:"Redis 多线程 IO 的工作原理",slug:"redis-多线程-io-的工作原理",normalizedTitle:"redis 多线程 io 的工作原理",charIndex:611},{level:2,title:"Redis 多线程 IO 核心源码解析",slug:"redis-多线程-io-核心源码解析",normalizedTitle:"redis 多线程 io 核心源码解析",charIndex:634},{level:3,title:"1、初始化 IO 线程",slug:"_1、初始化-io-线程",normalizedTitle:"1、初始化 io 线程",charIndex:4119},{level:3,title:"2. 多线程读",slug:"_2-多线程读",normalizedTitle:"2. 多线程读",charIndex:9938},{level:4,title:"入队：如何推迟客户端读操作？",slug:"入队-如何推迟客户端读操作",normalizedTitle:"入队：如何推迟客户端读操作？",charIndex:10477},{level:4,title:"分配：如何将待读客户端分配给 IO 线程执行？",slug:"分配-如何将待读客户端分配给-io-线程执行",normalizedTitle:"分配：如何将待读客户端分配给 io 线程执行？",charIndex:12673},{level:3,title:"3、多线程写",slug:"_3、多线程写",normalizedTitle:"3、多线程写",charIndex:16350},{level:4,title:"入队：如何决定是否推迟客户端写操作？",slug:"入队-如何决定是否推迟客户端写操作",normalizedTitle:"入队：如何决定是否推迟客户端写操作？",charIndex:16360},{level:4,title:"分配：如何将待读客户端分配给 10 个线程执行？",slug:"分配-如何将待读客户端分配给-10-个线程执行",normalizedTitle:"分配：如何将待读客户端分配给 10 个线程执行？",charIndex:18267},{level:2,title:"Redis 多线程 IO 的性能调优与实际问题",slug:"redis-多线程-io-的性能调优与实际问题",normalizedTitle:"redis 多线程 io 的性能调优与实际问题",charIndex:19946},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1627},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:21906}],headersStr:"前言 单线程 IO 及其缺陷 异步处理 事件驱动 单线程 IO 的瓶颈 Redis 多线程 IO 的工作原理 Redis 多线程 IO 核心源码解析 1、初始化 IO 线程 2. 多线程读 入队：如何推迟客户端读操作？ 分配：如何将待读客户端分配给 IO 线程执行？ 3、多线程写 入队：如何决定是否推迟客户端写操作？ 分配：如何将待读客户端分配给 10 个线程执行？ Redis 多线程 IO 的性能调优与实际问题 总结 参考文献",content:'提出问题是一切智慧的开端\n\n 1. 为什么 Redis 从单线程演变到多线程？单线程模式的瓶颈在哪里？\n 2. Redis 6.0 引入多线程 IO 的主要原理是什么？它如何提升性能？\n 3. 多线程 IO 是如何在 Redis 中分担读写任务的？与单线程 IO 有哪些关键区别？\n 4. 在 Redis 6.0 中，哪些情况下适合启用多线程 IO，线程数该如何配置？\n 5. Redis 如何利用多线程机制分配和处理客户端请求？具体流程是怎样的？\n 6. 多线程 IO 如何解决单线程模式下的性能瓶颈？有哪些场景下效果最显著？\n 7. 什么是 Redis 多线程 IO 的主要性能优化点？如何避免潜在的问题？\n 8. 多 IO 线程对 Redis 命令执行的影响有哪些？是否会带来新的并发挑战？\n\n\n# 前言\n\n复杂的架构系统通常是逐渐演进的，从单线程到多线程，从单体应用到复杂功能的分布式系统，Redis 也经历了类似的发展历程。\n\n在单线程模式下，Redis 能够实现极高的吞吐量，但在某些情况下，处理时间可能会显著增加，导致性能下降。为了解决这些问题，Redis 引入了后台线程来处理一些耗时的操作。随着对更高吞吐量的需求增加，网络模块也成为瓶颈，因此 Redis 在 6.0 版本中引入了多线程来解决这个问题——这也是本文主要探讨的内容。\n\n本文内容包括：\n\n 1. 早期单线程 IO 处理过程及其缺点\n 2. Redis 多线程 IO 的工作原理\n 3. Redis 多线程 IO 核心源码解析\n\n注意\n\n请下载 Redis 6.0.15 的源码，以便查看与多 IO 线程机制相关的代码。\n\n\n# 单线程 IO 及其缺陷\n\n\n# 异步处理\n\nRedis 的核心负载由单线程处理，但为何其性能仍能如此优异？\n\n * 纯内存操作：Redis 的操作大多在内存中完成。\n * 非阻塞 IO：Redis 使用非阻塞的 IO 机制。\n * 异步 IO 处理：每个命令在接收、处理和返回的过程中，经过多个“不连续”的步骤。\n\n> 需要特别指出的是，此处的“异步处理”并非指同步/异步 IO，而是指 IO 处理过程的异步化，即各个处理步骤之间不是同步执行的，而是通过事件循环机制和非阻塞 IO，使 Redis 能在单线程环境下高效处理多个请求。\n\n假设客户端发送以下命令：\n\nGET key-how-to-be-a-better-man\n\n\nRedis 的回应是：\n\n努力加把劲把文章写完\n\n\n处理这个命令的过程包括以下几个步骤：\n\n * 接收：通过 TCP 接收命令，可能经历多次 TCP 包、确认应答 (ack) 和 IO 操作。\n * 解析：从接收到的数据中提取命令。\n * 执行：读取指定位置的值。\n * 返回：通过 TCP 返回值给客户端。如果值较大，IO 负载也更重。\n\n其中，解析和执行步骤主要是 CPU 和内存操作，而接收和返回主要涉及 IO 操作，这是我们关注的重点。以接收为例，Redis 采用了两种策略：\n\n * 同步处理：在接收完整命令之前，始终保持等待状态，接收到完整命令后才进行处理，然后返回结果。在网络状况不佳时，这种方法可能会导致较长的等待时间。\n * 异步处理：通过非阻塞 IO 和事件循环机制，在处理一个请求时，Redis 可以继续处理其他请求，从而避免了阻塞等待。Redis 使用高效的事件驱动机制（如 epoll）来监控 IO 事件，从而提高单线程下的并发处理能力。\n\n以下是对异步处理的类比：\n\n * 同步：当聊天框显示“正在输入”时，你需要等对方输入完成后，才能回答对方的问题。完成回答后，才会转向其他人。\n * 异步：当聊天框显示“正在输入”时，你可以回答其他已完成输入的问题，而不必等对方输入完成，待对方输入完成后再继续回答其他问题。\n\n显然，异步处理的效率更高，因为同步处理在等待上浪费了时间。异步处理策略总结如下：\n\n * 在网络包到达时立即读取并放入缓冲区，读取完成后立即进行其他操作，而不等待下一个包。\n * 解析缓冲区中的数据是否完整。若数据完整，则执行命令；若不完整，则继续处理其他任务。\n * 数据完整后立即执行命令，将结果放入缓冲区。\n * 将数据返回给客户端。如果一次不能全部发送，则等到可以发送时再继续发送，直到全部发送完毕。\n\n\n# 事件驱动\n\n尽管异步处理避免了零散的等待时间，但如何得知“网络包有数据”或“下次可以发送数据”呢？如果通过轮询检查这些时机，效率会很低。Redis 使用事件驱动机制来解决这一问题。\n\n事件驱动框架能够高效地通知 Redis 在何时需要处理事件。Redis 通过事件驱动机制（如 epoll）来监听和处理事件。Linux 中的 epoll 机制专为高效通知而设计。Redis 基于 epoll 等机制抽象出了一套事件驱动框架，整个服务器由事件驱动，当事件发生时进行处理，无事件时则处于空闲状态。\n\n * 可读事件：表示对应的 socket 中有新的 TCP 数据包到达。\n * 可写事件：表示对应的 socket 的写缓冲区已经空了（数据已通过网络发送给客户端）。\n\n处理流程如下：\n\n\n\n * aeMain() 内部为一个死循环，在 epoll_wait 处短暂休眠。\n * epoll_wait 返回当前可读、可写的 socket 列表。\n * beforeSleep 是进入休眠前执行的逻辑，主要是将数据回写到 socket。\n * 核心逻辑由 IO 事件触发，可能是可读事件，也可能是可写事件，否则执行定时任务。\n * 第一次的 IO 可读事件是监听 socket（如监听 6379 端口），当有握手请求时，执行 accept 调用，获取一个连接 socket，并注册可读回调 createClient，此后客户端与 Redis 的数据通过该 socket 进行传输。\n * 一个完整的命令可能通过多次 readQueryFromClient 读取完毕，意味着会有多次可读 IO 事件。\n * 命令执行结果也可能需要通过多次写操作完成。\n * 命令执行完毕后，对应的连接会被加入 clients_pending_write，beforeSleep 会尝试回写到 socket，若写不完则注册可写事件，下次继续写。\n * 整个过程中的 IO 全部是同步非阻塞的，没有时间浪费。\n\n\n# 单线程 IO 的瓶颈\n\n尽管单线程 IO 处理过程避免了等待时间的浪费，并能实现较高的 QPS，但仍然存在一些瓶颈：\n\n * 仅使用一个 CPU 核心（忽略后台线程）。\n * 当数据量较大时，Redis 的 QPS 可能会显著下降，有时一个大的 key 会拖垮整个系统。\n * 难以进一步提升 QPS。\n\nRedis 主线程的时间消耗主要集中在以下两个方面：\n\n * 逻辑计算消耗\n * 同步 IO 读写消耗，包括数据拷贝的消耗。\n\n当数据量较大时，瓶颈主要出现在同步 IO 上（假设带宽和内存充足）。主要的消耗包括：\n\n * 从 socket 中读取请求数据时，会将数据从内核态拷贝到用户态（read 调用）。\n * 将数据回写到 socket 时，会将数据从用户态拷贝到内核态（write 调用）。\n\n这些数据读写操作会占用大量 CPU 时间，并直接导致性能瓶颈。如果能通过多线程来分担这些消耗，Redis 的吞吐量有望得到显著提升，这也是 Redis 引入多线程 IO 的主要目的。\n\n\n# Redis 多线程 IO 的工作原理\n\n接下来将目光放到： 如何用多线程分担IO的负荷。其做法用简单的话来说就是：\n\n * 用一组单独的线程专门进行 read/write socket读写调用 （同步IO）\n * 读回调函数中不再读数据，而是将对应的连接追加到可读 clients_pending_read 的链表\n * 主线程在 beforeSleep 中将IO读任务分给IO线程组\n * 主线程自己也处理一个 IO 读任务，并自旋式等 IO 线程组处理完，再继续往下\n * 主线程在 beforeSleep 中将 IO 写任务分给IO线程组\n * 主线程自己也处理一个 IO 写任务，并自旋式等 IO 线程组处理完，再继续往下\n * IO线程组要么同时在读，要么同时在写\n * 命令的执行由主线程串行执行（保持单线程）\n * IO线程数量可配置\n\n完整流程图如下：\n\n\n\nbeforesleep 中，先让 IO 线程读数据，然后再让 IO 线程写数据。 读写时，多线程能并发执行，利用多核。\n\n 1. 将读任务均匀分发到各个IO线程的任务链表 io_threads_list[i]，将 io_threads_pending[i] 设置为对应的任务数，此时 IO 线程将从死循环中被激活，开始执行任务，执行完毕后，会将 io_threads_pending[i] 清零。 函数名为： handleClientsWithPendingReadsUsingThreads\n 2. 将写任务均匀分发到各个IO线程的任务链表 io_threads_list[i]，将io_threads_pending[i] 设置为对应的任务数，此时IO线程将从死循环中被激活，开始执行任务，执行完毕后，会将 io_threads_pending[i]清零。 函数名为： handleClientsWithPendingWritesUsingThreads\n 3. beforeSleep中主线程也会执行其中一个任务，执行完后自旋等待 IO 线程处理完。\n 4. 读任务要么在 beforeSleep 中被执行，要么在 IO 线程被执行，不会再在读回调中执行\n 5. 写任务会分散到 beforeSleep、IO线程、写回调中执行\n 6. 主线程和 IO 线程交互是无锁的，通过标志位设置进行，不会同时写任务链表\n\n\n# Redis 多线程 IO 核心源码解析\n\n\n# 1、初始化 IO 线程\n\n在 Redis 的执行模式 中提到：Redis 5.0 版本中的三个后台线程是在 server.c 文件的 main 函数启动的最后阶段调用 InitServerLast 函数来初始化的，而 InitServerLast 函数则进一步调用 bioInit 函数来完成初始化。\n\n在 Redis 6.0 中，InitServerLast 函数在调用 bioInit 后，新增了对 initThreadedIO 函数的调用，以初始化多线程 IO 机制。\n\ninitThreadedIO 函数用于初始化多 IO 线程，其代码实现如下所示：\n\n// server.c#InitServerLast\nvoid InitServerLast() {\n    bioInit();\n    initThreadedIO();\n    set_jemalloc_bg_thread(server.jemalloc_bg_thread);\n    server.initial_memory_usage = zmalloc_used_memory();\n}\n\n\n> bioInit 函数用于初始化 Redis 的后台 IO 线程，处理如 RDB/AOF 持久化等耗时操作。而 initThreadedIO 函数在此基础上进一步初始化多线程 IO 机制，以支持更高效的客户端请求处理。\n\ninitThreadedIO 函数的主要任务是初始化 IO 线程，其代码实现如下：\n\n// networking.c#initThreadedIO\nvoid initThreadedIO(void) {\n    server.io_threads_active = 0; /* 初始化时线程未激活。 */\n\n    /* 如果用户选择了单线程，则不创建额外线程：\n     * 所有 I/O 操作将由主线程处理。 */\n    if (server.io_threads_num == 1) return;\n\n    if (server.io_threads_num > IO_THREADS_MAX_NUM) {\n        serverLog(LL_WARNING,"致命错误：配置了过多的 I/O 线程。"\n                             "最大允许数量为 %d。", IO_THREADS_MAX_NUM);\n        exit(1);\n    }\n\n    /* 创建并初始化 I/O 线程。 */\n    for (int i = 0; i < server.io_threads_num; i++) {\n        /* 对所有线程（包括主线程）执行的操作。 */\n        io_threads_list[i] = listCreate();\n        if (i == 0) continue; /* 线程 0 是主线程。 */\n\n        /* 对额外线程执行的操作。 */\n        pthread_t tid;\n        pthread_mutex_init(&io_threads_mutex[i],NULL);\n        setIOPendingCount(i, 0);\n        pthread_mutex_lock(&io_threads_mutex[i]); /* 线程将被暂停。 */\n        // 创建线程，并指定处理方法 IOThreadMain\n        if (pthread_create(&tid,NULL,IOThreadMain,(void*)(long)i) != 0) {\n            serverLog(LL_WARNING,"致命错误：无法初始化 IO 线程。");\n            exit(1);\n        }\n        io_threads[i] = tid;\n    }\n}\n\n\n 1. 首先，initThreadedIO 函数会设置 IO 线程的激活标志。\n 2. 随后，initThreadedIO 函数对设置的 IO 线程数量进行检查：\n    1. 如果 IO 线程数量为 1，则表示只有一个主线程，initThreadedIO 函数将直接返回。在这种情况下，Redis server 的 IO 线程配置与 Redis 6.0 之前的版本相同。\n    2. 如果 IO 线程数量超过宏定义 IO_THREADS_MAX_NUM（默认值为 128），initThreadedIO 函数会报错并退出程序。\n    3. 如果 IO 线程数量在 1 和 IO_THREADS_MAX_NUM 之间，initThreadedIO 函数会执行一个循环，该循环次数等于设置的 IO 线程数量（注意，i == 0 表示主线程）。\n\n> IO_THREADS_MAX_NUM 是一个宏定义，表示 Redis 支持的最大 IO 线程数量，默认值为 128。这一限制旨在防止过多线程对系统性能造成负担。\n\n在该循环中，initThreadedIO 函数会对以下四个数组进行初始化：\n\n * io_threads_list 数组：保存每个 IO 线程要处理的客户端列表，数组的每个元素初始化为一个 List 类型的列表。\n * io_threads_pending 数组：保存等待每个 IO 线程处理的客户端数量。\n * io_threads_mutex 数组：保存线程的互斥锁。\n * io_threads 数组：保存每个 IO 线程的描述符。\n\n这些数组的定义都在 networking.c 文件中，如下所示：\n\npthread_t io_threads[IO_THREADS_MAX_NUM];   // 记录线程描述符的数组\npthread_mutex_t io_threads_mutex[IO_THREADS_MAX_NUM];  // 记录线程互斥锁的数组\n_Atomic unsigned long io_threads_pending[IO_THREADS_MAX_NUM];  // 记录线程待处理的客户端数量\nlist *io_threads_list[IO_THREADS_MAX_NUM];  // 记录线程对应处理的客户端列表\n\n\n在对这些数组进行初始化的同时，initThreadedIO 函数还会根据 IO 线程数量，调用 pthread_create 函数创建相应数量的线程。\n\n在 for 循环中，pthread_create 函数用于创建线程。每个线程执行 IOThreadMain 函数来处理客户端请求。如果 pthread_create 返回非零值，则说明线程创建失败，此时 Redis 会记录错误并退出。\n\n因此，initThreadedIO 函数创建的线程运行的函数是 IOThreadMain，参数为当前创建线程的编号。需要注意的是，该编号从 1 开始，而编号为 0 的线程实际上是运行 Redis server 主流程的主线程。\n\nvoid *IOThreadMain(void *myid) {\n    /* ID 是线程编号（从 0 到 server.iothreads_num-1） */\n    long id = (unsigned long)myid;\n    char thdname[16];\n\n    snprintf(thdname, sizeof(thdname), "io_thd_%ld", id);\n    redis_set_thread_title(thdname);\n    redisSetCpuAffinity(server.server_cpulist);\n    makeThreadKillable();\n\n    while(1) {\n        /* 等待开始 */\n        for (int j = 0; j < 1000000; j++) {\n            if (getIOPendingCount(id) != 0) break;\n        }\n\n        /* 给主线程一个机会来停止此线程。 */\n        if (getIOPendingCount(id) == 0) {\n            pthread_mutex_lock(&io_threads_mutex[id]);\n            pthread_mutex_unlock(&io_threads_mutex[id]);\n            continue;\n        }\n\n        serverAssert(getIOPendingCount(id) != 0);\n\n        if (tio_debug) printf("[%ld] %d to handle\\n", id, (int)listLength(io_threads_list[id]));\n\n        /* 处理：注意主线程不会在我们将待处理数量减少到 0 之前触碰我们的列表 */\n        listIter li;\n        listNode *ln;\n        listRewind(io_threads_list[id],&li);\n        while((ln = listNext(&li))) {\n            client *c = listNodeValue(ln);\n            if (io_threads_op == IO_THREADS_OP_WRITE) {\n                writeToClient(c,0);\n            } else if (io_threads_op == IO_THREADS_OP_READ) {\n                readQueryFromClient(c->conn);\n            } else {\n                serverPanic("io_threads_op 值未知");\n            }\n        }\n        listEmpty(io_threads_list[id]);\n        setIOPendingCount(id, 0);\n\n        if (tio_debug) printf("[%ld] Done\\n", id);\n    }\n}\n\n\nIOThreadMain 函数也在 networking.c 文件中定义，其主要逻辑为一个 while(1) 循环。\n\nIOThreadMain 函数在循环中处理 io_threads_list 数组中每个线程的客户端请求。\n\n正如之前所述，io_threads_list 数组中为每个 IO 线程使用一个列表记录待处理的客户端。因此，IOThreadMain 函数会从每个 IO 线程对应的列表中取出待处理的客户端，并根据操作类型执行相应操作。操作类型由变量 io_threads_op 表示，其值有两种：\n\n * io_threads_op 的值为宏定义 IO_THREADS_OP_WRITE：表示该 IO 线程进行写操作，将数据从 Redis 写回客户端，线程会调用 writeToClient 函数。\n * io_threads_op 的值为宏定义 IO_THREADS_OP_READ：表示该 IO 线程进行读操作，从客户端读取数据，线程会调用 readQueryFromClient 函数。\n\n笔记\n\n如果您对 Java 编程熟悉，可以将 IOThreadMain 函数视为 Runnable 的具体实现。其核心逻辑在于 while(1) 无限循环中。根据源码，IO 线程从 io_threads_list 队列（或列表）中获取待处理的客户端，并根据操作类型选择具体的执行逻辑。这是一种典型的 生产者-消费者模型，主线程负责投递事件，IO 线程负责消费事件（主线程也参与）。\n\n我绘制了下图，以展示 IOThreadMain 函数的基本流程，请参考：\n\n\n\n如上所示，每个 IO 线程在运行过程中，会不断检查是否有待处理的客户端请求。如果存在待处理的客户端，线程会根据操作类型，从客户端读取数据或将数据写回客户端。这些操作涉及 Redis 与客户端之间的 I/O 交互，因此这些线程被称为 IO 线程。\n\n在此，您可能会产生一些疑问：IO 线程如何将客户端添加到 io_threads_list 数组中？\n\n这涉及 Redis server 的全局变量 server。server 变量中包含两个 List 类型的成员变量：clients_pending_write 和 clients_pending_read，分别记录待写回数据的客户端和待读取数据的客户端，如下所示：\n\nstruct redisServer {\n    ...\n    // 待写回数据的客户端\n    list *clients_pending_write;  \n    // 待读取数据的客户端\n    list *clients_pending_read;\n    ...\n}\n\n\nRedis server 在接收客户端请求和返回数据的过程中，会根据特定条件推迟客户端的读写操作，并将这些客户端分别保存到这两个列表中。随后，在每次进入事件循环前，Redis server 会将列表中的客户端添加到 io_threads_list 数组中，由 IO 线程进行处理。\n\n接下来，我们将探讨 Redis 如何推迟客户端的读写操作，并将这些客户端添加到 clients_pending_write 和 clients_pending_read 列表中。\n\n\n# 2. 多线程读\n\n在早期的单线程版本中，当多路复用检测到客户端数据准备就绪时，主事件循环会轮询处理这些就绪的客户端，步骤如下：\n\n 1. 读取数据\n 2. 解析数据\n 3. 执行命令\n 4. 将数据写回客户端缓冲区\n 5. 等待下一轮主事件循环\n 6. 将客户端缓冲数据写回客户端\n\n在多线程模式下（假设配置了多线程读），上述流程有所变化：数据读取和解析操作将被分配给多个 IO 线程（包括主线程）。\n\n所有就绪客户端将暂存至队列中：\n\nstruct redisServer {  \n   ...\n   list *clients_pending_read;\n   ...\n}\n\n\n处理流程如下：\n\n 1. 主线程开始监听 IO 事件\n 2. 主线程调用 readQueryFromClient\n 3. postponeClientRead 将客户端添加至 clients_pending_read\n 4. handleClientsWithPendingReadsUsingThreads 将 clients_pending_read 列表中的客户端分配给所有 IO 线程\n 5. 主线程阻塞并等待所有 IO 线程完成读取\n 6. 主线程循环遍历并处理所有读取到的数据\n\n# 入队：如何推迟客户端读操作？\n\nRedis server 在与客户端建立连接后，会开始监听客户端的可读事件。处理可读事件的回调函数是 readQueryFromClient。我在某处已介绍了这一过程，您可以再次回顾。\n\n在 Redis 6.0 版本中，readQueryFromClient 函数首先从传入的参数 conn 中获取客户端 c，然后调用 postponeClientRead 函数来判断是否推迟从客户端读取数据。执行逻辑如下：\n\nvoid readQueryFromClient(connection *conn) {\n    client *c = connGetPrivateData(conn);  // 从连接数据结构中获取客户端\n    ...\n    if (postponeClientRead(c)) return;  // 判断是否推迟从客户端读取数据\n    ...\n}\n\n\n接下来，我们将分析 postponeClientRead 函数的执行逻辑。该函数会根据以下四个条件判断是否可以推迟从客户端读取数据：\n\n条件一：全局变量 server 的 io_threads_active 值为 1\n\n这表示多 IO 线程已激活。正如前述，该变量在 initThreadedIO 函数中初始化为 0，表明多 IO 线程初始化后默认未激活（后文将详细介绍何时将该变量值设置为 1）。\n\n条件二：全局变量 server 的 io_threads_do_read 值为 1\n\n这表示多 IO 线程可以处理延后执行的客户端读操作。该变量在 Redis 配置文件 redis.conf 中通过 io-threads-do-reads 配置项设置，默认为 no，即多 IO 线程机制默认不用于客户端读操作。若要启用多 IO 线程处理客户端读操作，需将 io-threads-do-reads 配置项设为 yes。\n\n条件三：ProcessingEventsWhileBlocked 变量值为 0\n\n这表示 processEventsWhileBlocked 函数未在执行中。ProcessingEventsWhileBlocked 是一个全局变量，当 processEventsWhileBlocked 函数执行时，该变量值为 1，函数执行完成后值为 0。processEventsWhileBlocked 函数在 networking.c 文件中实现，主要用于在 Redis 读取 RDB 或 AOF 文件时处理事件，避免因读取文件阻塞 Redis 导致事件处理延迟。因此，当 processEventsWhileBlocked 函数处理客户端可读事件时，这些客户端读操作不会被推迟。\n\n条件四：客户端当前标识不能包含 CLIENT_MASTER、CLIENT_SLAVE 和 CLIENT_PENDING_READ\n\n其中，CLIENT_MASTER 和 CLIENT_SLAVE 标识表示客户端用于主从复制，这些客户端的读操作不会被推迟。CLIENT_PENDING_READ 标识表示客户端已设置为推迟读操作，因此，对于已带有 CLIENT_PENDING_READ 标识的客户端，postponeClientRead 函数不会再次推迟其读操作。\n\n只有当上述四个条件均满足时，postponeClientRead 函数才会推迟当前客户端的读操作。具体来说，postponeClientRead 函数会为该客户端设置 CLIENT_PENDING_READ 标识，并调用 listAddNodeHead 函数，将客户端添加到全局变量 server 的 clients_pending_read 列表中。\n\n以下是 postponeClientRead 函数的代码：\n\nint postponeClientRead(client *c) {\n    // 判断 IO 线程是否激活\n    if (server.io_threads_active && server.io_threads_do_reads &&      \n         !ProcessingEventsWhileBlocked &&\n        !(c->flags & (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_PENDING_READ)))\n    {\n        c->flags |= CLIENT_PENDING_READ; // 设置客户端标识为 CLIENT_PENDING_READ，表示推迟该客户端的读操作\n        listAddNodeHead(server.clients_pending_read,c); // 将客户端添加到 clients_pending_read 列表中\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\n\n综上所述，Redis 在客户端读事件回调函数 readQueryFromClient 中，通过调用 postponeClientRead 函数来判断并推迟客户端读操作。接下来，我们将探讨 Redis 如何推迟客户端写操作。\n\n# 分配：如何将待读客户端分配给 IO 线程执行？\n\n首先，我们需要了解 handleClientsWithPendingReadsUsingThreads 函数的作用。该函数在 beforeSleep 函数中被调用。\n\n在 Redis 6.0 版本的实现中，事件驱动框架通过调用 aeMain 函数执行事件循环，aeMain 函数进一步调用 aeProcessEvents 处理各种事件。在 aeProcessEvents 实际调用 aeApiPoll 捕获 IO 事件之前，beforeSleep 函数会被执行。\n\n该过程如图所示：\n\n\n\nhandleClientsWithPendingReadsUsingThreads 函数的执行逻辑可分为四个步骤：\n\n第一步，该函数首先检查全局变量 server 的 io_threads_active 成员变量，确认 IO 线程是否激活，同时依据 io_threads_do_reads 成员变量判断是否允许 IO 线程处理待读客户端。只有在 IO 线程被激活并且允许处理待读客户端的情况下，handleClientsWithPendingReadsUsingThreads 函数才会继续执行，否则函数将直接返回。判断逻辑如下：\n\nif (!server.io_threads_active || !server.io_threads_do_reads) \n\treturn 0;\n\n\n第二步，函数获取 clients_pending_read 列表的长度，表示待处理客户端的数量。随后，函数从 clients_pending_read 列表中逐一取出待处理的客户端，并通过客户端在列表中的序号对 IO 线程数量进行取模运算。\n\n通过这种方式，客户端将被分配给对应的 IO 线程。接着，函数会调用 listAddNodeTail 将分配好的客户端添加到 io_threads_list 数组的相应元素中。io_threads_list 数组的每个元素是一个列表，保存了每个 IO 线程需要处理的客户端。\n\n以下是具体的示例：\n\n假设 IO 线程数量为 3，而 clients_pending_read 列表中有 5 个客户端，其序号分别为 0、1、2、3 和 4。在此步骤中，这些客户端的序号对线程数量 3 取模的结果分别是 0、1、2、0、1，这对应了处理这些客户端的 IO 线程编号。也就是说，客户端 0 由线程 0 处理，客户端 1 由线程 1 处理，以此类推。客户端的分配方式实际上是一种 轮询 方式。\n\n下图展示了这种分配结果：\n\n\n\n以下代码展示了如何以轮询方式将客户端分配给 IO 线程的执行逻辑：\n\nint processed = listLength(server.clients_pending_read);\nlistRewind(server.clients_pending_read, &li);\nint item_id = 0;\nwhile ((ln = listNext(&li))) {\n    client *c = listNodeValue(ln);\n    int target_id = item_id % server.io_threads_num;\n    listAddNodeTail(io_threads_list[target_id], c);\n    item_id++;\n}\n\n\n当 handleClientsWithPendingReadsUsingThreads 函数完成客户端的 IO 线程分配后，它会将 IO 线程的操作标识设置为 读操作，即 IO_THREADS_OP_READ。然后，它会遍历 io_threads_list 数组中的每个元素列表，记录每个线程待处理客户端的数量，并赋值给 io_threads_pending 数组。具体过程如下：\n\nio_threads_op = IO_THREADS_OP_READ;\nfor (int j = 1; j < server.io_threads_num; j++) {\n    int count = listLength(io_threads_list[j]);\n    io_threads_pending[j] = count;\n}\n\n\n第三步，函数会将 io_threads_list 数组中的 0 号列表（即 io_threads_list[0]）中的客户端逐一取出，并调用 readQueryFromClient 函数进行处理。\n\n需要注意的是，handleClientsWithPendingReadsUsingThreads 函数本身由 IO 主线程执行，而 io_threads_list 数组中的 0 号线程即为 IO 主线程，因此此步骤是由主线程处理其待读客户端：\n\nlistRewind(io_threads_list[0], &li);  // 获取 0 号列表中的所有客户端\nwhile ((ln = listNext(&li))) {\n    client *c = listNodeValue(ln);\n    readQueryFromClient(c->conn);\n}\nlistEmpty(io_threads_list[0]); // 处理完后，清空 0 号列表\n\n\n接下来，handleClientsWithPendingReadsUsingThreads 函数会进入一个 while(1) 循环，等待所有 IO 线程完成对待读客户端的处理，如下所示：\n\nwhile (1) {\n    unsigned long pending = 0;\n    for (int j = 1; j < server.io_threads_num; j++)\n        pending += io_threads_pending[j];\n    if (pending == 0) break;\n}\n\n\n第四步，函数会再次遍历 clients_pending_read 列表，逐一取出其中的客户端。接着，函数会检查客户端是否有 CLIENT_PENDING_COMMAND 标识。如果存在，说明该客户端的命令已被某个 IO 线程解析，可以执行。\n\n此时，handleClientsWithPendingReadsUsingThreads 函数会调用 processCommandAndResetClient 执行命令，并直接调用 processInputBuffer 解析客户端中所有命令并执行。\n\n相关代码如下：\n\nwhile (listLength(server.clients_pending_read)) {\n    ln = listFirst(server.clients_pending_read);\n    client *c = listNodeValue(ln);\n    ...\n    // 如果命令已解析，则执行该命令\n    if (c->flags & CLIENT_PENDING_COMMAND) {\n        c->flags &= ~CLIENT_PENDING_COMMAND;\n        if (processCommandAndResetClient(c) == C_ERR) {\n            continue;\n        }\n    }\n    // 解析并执行所有命令\n    processInputBuffer(c);\n}\n\n\n至此，你已经了解了如何将 clients_pending_read 列表中的待读客户端通过上述四个步骤分配给 IO 线程进行处理。下图展示了这一主要过程，你可以进一步回顾：\n\n\n\n接下来，我们将探讨待写客户端的分配和处理方式。\n\n需要注意的是，当 待处理客户端 数量较少时，Redis 认为不需要多线程共同处理，所有任务将由主线程完成：\n\nint stopThreadedIOIfNeeded(void) {\n    int pending = listLength(server.clients_pending_write);\n\n    if (server.io_threads_num == 1) return 1;\n\n    if (pending < (server.io_threads_num * 2)) {\n        if (server.io_threads_active) stopThreadedIO();\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\n\n当 待处理客户端数量 小于 2倍的 IO 线程数 时，所有客户端数据将由主线程处理。\n\n\n# 3、多线程写\n\n# 入队：如何决定是否推迟客户端写操作？\n\n在 Redis 中，当执行客户端命令后，需要向客户端返回结果时，会调用 addReply 函数将待返回的结果写入客户端的输出缓冲区。\n\n在 addReply 函数的开始部分，该函数会调用 prepareClientToWrite 函数来判断是否需要推迟执行客户端的写操作。以下代码展示了 addReply 函数如何调用 prepareClientToWrite 函数：\n\nvoid addReply(client *c, robj *obj) {\n    if (prepareClientToWrite(c) != C_OK) return;\n    ...\n}\n\n\n接下来，我们来看一下 prepareClientToWrite 函数。该函数根据客户端的设置进行一系列判断。其中，clientHasPendingReplies 函数会被调用，用于检查当前客户端的输出缓冲区中是否还有待写回的数据。\n\n如果缓冲区中没有待写回的数据，prepareClientToWrite 函数会进一步调用 clientInstallWriteHandler 函数，以判断是否能够推迟客户端的写操作。以下代码展示了这一调用过程：\n\nint prepareClientToWrite(client *c) {\n    ...\n    // 如果客户端没有待写回数据，则调用 clientInstallWriteHandler 函数\n    if (!clientHasPendingReplies(c)) clientInstallWriteHandler(c);\n    return C_OK;\n}\n\n\n因此，推迟客户端写操作的最终决定由 clientInstallWriteHandler 函数做出。该函数会检查两个条件：\n\n * 条件一：客户端未设置 CLIENT_PENDING_WRITE 标识，即尚未推迟过写操作。\n * 条件二：客户端所在实例未进行主从复制，或即使正在进行主从复制，客户端所在实例作为从节点且全量复制的 RDB 文件已传输完成，可以接收请求。\n\n当上述两个条件都满足时，clientInstallWriteHandler 函数会将客户端标识设置为 CLIENT_PENDING_WRITE，以表示推迟该客户端的写操作。同时，该函数会将客户端添加到全局变量 server 的待写回客户端列表 clients_pending_write 中。\n\nvoid clientInstallWriteHandler(client *c) {\n    // 如果客户端没有设置过 CLIENT_PENDING_WRITE 标识，且客户端不在主从复制中，或作为从节点且已接收请求\n    if (!(c->flags & CLIENT_PENDING_WRITE) &&\n        (c->replstate == REPL_STATE_NONE ||\n         (c->replstate == SLAVE_STATE_ONLINE && !c->repl_put_online_on_ack)))\n    {\n        // 将客户端标识设置为待写回，即 CLIENT_PENDING_WRITE\n        c->flags |= CLIENT_PENDING_WRITE;\n        listAddNodeHead(server.clients_pending_write, c);  // 将客户端添加到 clients_pending_write 列表\n    }\n}\n\n\n为帮助理解，我绘制了一张图，展示了 Redis 推迟客户端写操作的函数调用关系，供参考。\n\n\n\n然而，当 Redis 使用 clients_pending_read 和 clients_pending_write 两个列表保存推迟执行的客户端时，这些客户端如何分配给多个 I/O 线程进行处理呢？ 这涉及到以下两个函数：\n\n * handleClientsWithPendingReadsUsingThreads 函数：负责将 clients_pending_read 列表中的客户端分配给 I/O 线程处理。\n * handleClientsWithPendingWritesUsingThreads 函数：负责将 clients_pending_write 列表中的客户端分配给 I/O 线程处理。\n\n接下来，我们将详细介绍这两个函数的具体操作。\n\n# 分配：如何将待读客户端分配给 10 个线程执行？\n\n与待读客户端的分配类似，待写客户端的分配处理由 handleClientsWithPendingWritesUsingThreads 函数完成，该函数同样在 beforeSleep 函数中被调用。\n\nhandleClientsWithPendingWritesUsingThreads 函数的主要流程可以分为四个步骤，其中第二、第三和第四步的执行逻辑与 handleClientsWithPendingReadsUsingThreads 函数类似。\n\n简言之，在第二步中，handleClientsWithPendingWritesUsingThreads 函数会将待写客户端按照 轮询方式 分配给 I/O 线程，并将其添加到 io_threads_list 数组的各个元素中。\n\n在第三步中，handleClientsWithPendingWritesUsingThreads 函数会让主 I/O 线程处理其待写客户端，并执行 while(1) 循环以等待所有 I/O 线程完成处理。\n\n在第四步中，handleClientsWithPendingWritesUsingThreads 函数会再次检查 clients_pending_write 列表中是否还有待写客户端。如果存在且这些客户端仍有数据待写，函数会调用 connSetWriteHandler 函数注册可写事件，该事件的回调函数为 sendReplyToClient。\n\n当事件循环流程再次执行时，sendReplyToClient 函数会被调用，它会直接调用 writeToClient 函数，将客户端缓冲区中的数据写回。\n\n需要注意的是，connSetWriteHandler 函数最终会映射为 connSocketSetWriteHandler 函数，后者在 connection.c 文件中实现。connSocketSetWriteHandler 函数会调用 aeCreateFileEvent 函数创建 AE_WRITABLE 事件，这即为可写事件的注册（有关 aeCreateFileEvent 函数的使用，可以参见第 11 讲）。\n\n与 handleClientsWithPendingReadsUsingThreads 函数不同的是，在第一步中，handleClientsWithPendingWritesUsingThreads 函数会判断 I/O 线程数量是否为 1，或待写客户端数量是否少于 I/O 线程数量的两倍。\n\n如果满足上述任一条件，则 handleClientsWithPendingWritesUsingThreads 函数不会采用多线程处理客户端，而是调用 handleClientsWithPendingWrites 函数由主 I/O 线程直接处理待写客户端。这主要是为了在待写客户端数量较少时，节省 CPU 开销。\n\n以下是条件判断逻辑：\n\nif (server.io_threads_num == 1 || stopThreadedIOIfNeeded()) {\n    return handleClientsWithPendingWrites();\n}\n\n\n此外，handleClientsWithPendingWritesUsingThreads 函数在第一步中还会判断 I/O 线程是否已激活。如果未激活，则调用 startThreadedIO 函数，将全局变量 server 的 io_threads_active 成员变量设置为 1，以表示 I/O 线程已激活。此判断操作如下：\n\nif (!server.io_threads_active) startThreadedIO();\n\n\n总之，Redis 通过 handleClientsWithPendingWritesUsingThreads 函数将待写客户端按轮询方式分配给各个 I/O 线程，并由这些线程负责数据的写回。\n\n\n# Redis 多线程 IO 的性能调优与实际问题\n\nredis 默认情况下不会开启多线程处理，官方也建议，除非性能达到瓶颈，否则没必要开启多线程。\n\n配置多少合适？\n\n官方文档 redis.conf 中介绍有：\n\n> By default threading is disabled, we suggest enabling it only in machines that have at least 4 or more cores, leaving at least one spare core. Using more than 8 threads is unlikely to help much. We also recommend using threaded I/O only if you actually have performance problems, with Redis instances being able to use a quite big percentage of CPU time, otherwise there is no point in using this feature.\n> \n> So for instance if you have a four cores boxes, try to use 2 or 3 I/O threads, if you have a 8 cores, try to use 6 threads. In order to enable I/O threads use the following configuration directive:\n\nCPU 4 核以上，才考虑开启多线程，其中：\n\n * 4 核开启 2 - 3 个 IO 线程\n * 8 核 开启 6 个 IO 线程\n * 超过 8 个 IO 线程，性能提升已经不大\n\n值得注意的是，以上的 IO 线程其实包含了主线程。\n\n配置：\n\n开启多线程：配置 io-thread 即可。io-thread = 1 表示只使用主线程\n\n> io-threads 4\n\n开启之后，默认写操作会通过多线程来处理，而读操作则不会。\n\n如果读操作也想要开启多线程，则需要配置：\n\n> io-threads-do-reads yes\n\n\n# 总结\n\n今天这节课，我给你介绍了 Redis 6.0 中新设计实现的多 IO 线程机制。这个机制的设计主要是为了使用多个 IO 线程，来并发处理客户端读取数据、解析命令和写回数据。使用了多线程后，Redis 就可以充分利用服务器的多核特性，从而提高 IO 效率。\n\n总结来说，Redis 6.0 先是在初始化过程中，根据用户设置的 IO 线程数量，创建对应数量的 IO 线程。\n\n当 Redis server 初始化完成后正常运行时，它会在 readQueryFromClient 函数中通过调用 postponeClientRead 函数来决定是否推迟客户端读操作。同时，Redis server 会在 addReply 函数中通过调用 prepareClientToWrite 函数，来决定是否推迟客户端写操作。而待读写的客户端会被分别加入到 clients_pending_read 和 clients_pending_write 两个列表中。\n\n这样，每当 Redis server 要进入事件循环流程前，都会在 beforeSleep 函数中分别调用 handleClientsWithPendingReadsUsingThreads 函数和 handleClientsWithPendingWritesUsingThreads 函数，将待读写客户端以轮询方式分配给 IO 线程，加入到 IO 线程的待处理客户端列表 io_threads_list 中。\n\n而 IO 线程一旦运行后，本身会一直检测 io_threads_list 中的客户端，如果有待读写客户端，IO 线程就会调用 readQueryFromClient 或 writeToClient 函数来进行处理。\n\n最后，我也想再提醒你一下，多 IO 线程本身并不会执行命令，它们只是利用多核并行地读取数据和解析命令，或是将 server 数据写回（下节课我还会结合分布式锁的原子性保证，来给你介绍这一部分的源码实现。）。所以，Redis 执行命令的线程还是主线程。这一点对于你理解多 IO 线程机制很重要，可以避免你误解 Redis 有多线程同时执行命令。\n\n这样一来，我们原来针对 Redis 单个主线程做的优化仍然有效，比如避免 bigkey、避免阻塞操作等。\n\n\n# 参考文献\n\n * redis 6.0之多线程，深入解读 - 知乎 (zhihu.com)\n * Redis 6.0 多线程IO处理过程详解 (zhihu.com)\n * Redis 源码剖析与实战 (geekbang.org)',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 为什么 redis 从单线程演变到多线程？单线程模式的瓶颈在哪里？\n 2. redis 6.0 引入多线程 io 的主要原理是什么？它如何提升性能？\n 3. 多线程 io 是如何在 redis 中分担读写任务的？与单线程 io 有哪些关键区别？\n 4. 在 redis 6.0 中，哪些情况下适合启用多线程 io，线程数该如何配置？\n 5. redis 如何利用多线程机制分配和处理客户端请求？具体流程是怎样的？\n 6. 多线程 io 如何解决单线程模式下的性能瓶颈？有哪些场景下效果最显著？\n 7. 什么是 redis 多线程 io 的主要性能优化点？如何避免潜在的问题？\n 8. 多 io 线程对 redis 命令执行的影响有哪些？是否会带来新的并发挑战？\n\n\n# 前言\n\n复杂的架构系统通常是逐渐演进的，从单线程到多线程，从单体应用到复杂功能的分布式系统，redis 也经历了类似的发展历程。\n\n在单线程模式下，redis 能够实现极高的吞吐量，但在某些情况下，处理时间可能会显著增加，导致性能下降。为了解决这些问题，redis 引入了后台线程来处理一些耗时的操作。随着对更高吞吐量的需求增加，网络模块也成为瓶颈，因此 redis 在 6.0 版本中引入了多线程来解决这个问题——这也是本文主要探讨的内容。\n\n本文内容包括：\n\n 1. 早期单线程 io 处理过程及其缺点\n 2. redis 多线程 io 的工作原理\n 3. redis 多线程 io 核心源码解析\n\n注意\n\n请下载 redis 6.0.15 的源码，以便查看与多 io 线程机制相关的代码。\n\n\n# 单线程 io 及其缺陷\n\n\n# 异步处理\n\nredis 的核心负载由单线程处理，但为何其性能仍能如此优异？\n\n * 纯内存操作：redis 的操作大多在内存中完成。\n * 非阻塞 io：redis 使用非阻塞的 io 机制。\n * 异步 io 处理：每个命令在接收、处理和返回的过程中，经过多个“不连续”的步骤。\n\n> 需要特别指出的是，此处的“异步处理”并非指同步/异步 io，而是指 io 处理过程的异步化，即各个处理步骤之间不是同步执行的，而是通过事件循环机制和非阻塞 io，使 redis 能在单线程环境下高效处理多个请求。\n\n假设客户端发送以下命令：\n\nget key-how-to-be-a-better-man\n\n\nredis 的回应是：\n\n努力加把劲把文章写完\n\n\n处理这个命令的过程包括以下几个步骤：\n\n * 接收：通过 tcp 接收命令，可能经历多次 tcp 包、确认应答 (ack) 和 io 操作。\n * 解析：从接收到的数据中提取命令。\n * 执行：读取指定位置的值。\n * 返回：通过 tcp 返回值给客户端。如果值较大，io 负载也更重。\n\n其中，解析和执行步骤主要是 cpu 和内存操作，而接收和返回主要涉及 io 操作，这是我们关注的重点。以接收为例，redis 采用了两种策略：\n\n * 同步处理：在接收完整命令之前，始终保持等待状态，接收到完整命令后才进行处理，然后返回结果。在网络状况不佳时，这种方法可能会导致较长的等待时间。\n * 异步处理：通过非阻塞 io 和事件循环机制，在处理一个请求时，redis 可以继续处理其他请求，从而避免了阻塞等待。redis 使用高效的事件驱动机制（如 epoll）来监控 io 事件，从而提高单线程下的并发处理能力。\n\n以下是对异步处理的类比：\n\n * 同步：当聊天框显示“正在输入”时，你需要等对方输入完成后，才能回答对方的问题。完成回答后，才会转向其他人。\n * 异步：当聊天框显示“正在输入”时，你可以回答其他已完成输入的问题，而不必等对方输入完成，待对方输入完成后再继续回答其他问题。\n\n显然，异步处理的效率更高，因为同步处理在等待上浪费了时间。异步处理策略总结如下：\n\n * 在网络包到达时立即读取并放入缓冲区，读取完成后立即进行其他操作，而不等待下一个包。\n * 解析缓冲区中的数据是否完整。若数据完整，则执行命令；若不完整，则继续处理其他任务。\n * 数据完整后立即执行命令，将结果放入缓冲区。\n * 将数据返回给客户端。如果一次不能全部发送，则等到可以发送时再继续发送，直到全部发送完毕。\n\n\n# 事件驱动\n\n尽管异步处理避免了零散的等待时间，但如何得知“网络包有数据”或“下次可以发送数据”呢？如果通过轮询检查这些时机，效率会很低。redis 使用事件驱动机制来解决这一问题。\n\n事件驱动框架能够高效地通知 redis 在何时需要处理事件。redis 通过事件驱动机制（如 epoll）来监听和处理事件。linux 中的 epoll 机制专为高效通知而设计。redis 基于 epoll 等机制抽象出了一套事件驱动框架，整个服务器由事件驱动，当事件发生时进行处理，无事件时则处于空闲状态。\n\n * 可读事件：表示对应的 socket 中有新的 tcp 数据包到达。\n * 可写事件：表示对应的 socket 的写缓冲区已经空了（数据已通过网络发送给客户端）。\n\n处理流程如下：\n\n\n\n * aemain() 内部为一个死循环，在 epoll_wait 处短暂休眠。\n * epoll_wait 返回当前可读、可写的 socket 列表。\n * beforesleep 是进入休眠前执行的逻辑，主要是将数据回写到 socket。\n * 核心逻辑由 io 事件触发，可能是可读事件，也可能是可写事件，否则执行定时任务。\n * 第一次的 io 可读事件是监听 socket（如监听 6379 端口），当有握手请求时，执行 accept 调用，获取一个连接 socket，并注册可读回调 createclient，此后客户端与 redis 的数据通过该 socket 进行传输。\n * 一个完整的命令可能通过多次 readqueryfromclient 读取完毕，意味着会有多次可读 io 事件。\n * 命令执行结果也可能需要通过多次写操作完成。\n * 命令执行完毕后，对应的连接会被加入 clients_pending_write，beforesleep 会尝试回写到 socket，若写不完则注册可写事件，下次继续写。\n * 整个过程中的 io 全部是同步非阻塞的，没有时间浪费。\n\n\n# 单线程 io 的瓶颈\n\n尽管单线程 io 处理过程避免了等待时间的浪费，并能实现较高的 qps，但仍然存在一些瓶颈：\n\n * 仅使用一个 cpu 核心（忽略后台线程）。\n * 当数据量较大时，redis 的 qps 可能会显著下降，有时一个大的 key 会拖垮整个系统。\n * 难以进一步提升 qps。\n\nredis 主线程的时间消耗主要集中在以下两个方面：\n\n * 逻辑计算消耗\n * 同步 io 读写消耗，包括数据拷贝的消耗。\n\n当数据量较大时，瓶颈主要出现在同步 io 上（假设带宽和内存充足）。主要的消耗包括：\n\n * 从 socket 中读取请求数据时，会将数据从内核态拷贝到用户态（read 调用）。\n * 将数据回写到 socket 时，会将数据从用户态拷贝到内核态（write 调用）。\n\n这些数据读写操作会占用大量 cpu 时间，并直接导致性能瓶颈。如果能通过多线程来分担这些消耗，redis 的吞吐量有望得到显著提升，这也是 redis 引入多线程 io 的主要目的。\n\n\n# redis 多线程 io 的工作原理\n\n接下来将目光放到： 如何用多线程分担io的负荷。其做法用简单的话来说就是：\n\n * 用一组单独的线程专门进行 read/write socket读写调用 （同步io）\n * 读回调函数中不再读数据，而是将对应的连接追加到可读 clients_pending_read 的链表\n * 主线程在 beforesleep 中将io读任务分给io线程组\n * 主线程自己也处理一个 io 读任务，并自旋式等 io 线程组处理完，再继续往下\n * 主线程在 beforesleep 中将 io 写任务分给io线程组\n * 主线程自己也处理一个 io 写任务，并自旋式等 io 线程组处理完，再继续往下\n * io线程组要么同时在读，要么同时在写\n * 命令的执行由主线程串行执行（保持单线程）\n * io线程数量可配置\n\n完整流程图如下：\n\n\n\nbeforesleep 中，先让 io 线程读数据，然后再让 io 线程写数据。 读写时，多线程能并发执行，利用多核。\n\n 1. 将读任务均匀分发到各个io线程的任务链表 io_threads_list[i]，将 io_threads_pending[i] 设置为对应的任务数，此时 io 线程将从死循环中被激活，开始执行任务，执行完毕后，会将 io_threads_pending[i] 清零。 函数名为： handleclientswithpendingreadsusingthreads\n 2. 将写任务均匀分发到各个io线程的任务链表 io_threads_list[i]，将io_threads_pending[i] 设置为对应的任务数，此时io线程将从死循环中被激活，开始执行任务，执行完毕后，会将 io_threads_pending[i]清零。 函数名为： handleclientswithpendingwritesusingthreads\n 3. beforesleep中主线程也会执行其中一个任务，执行完后自旋等待 io 线程处理完。\n 4. 读任务要么在 beforesleep 中被执行，要么在 io 线程被执行，不会再在读回调中执行\n 5. 写任务会分散到 beforesleep、io线程、写回调中执行\n 6. 主线程和 io 线程交互是无锁的，通过标志位设置进行，不会同时写任务链表\n\n\n# redis 多线程 io 核心源码解析\n\n\n# 1、初始化 io 线程\n\n在 redis 的执行模式 中提到：redis 5.0 版本中的三个后台线程是在 server.c 文件的 main 函数启动的最后阶段调用 initserverlast 函数来初始化的，而 initserverlast 函数则进一步调用 bioinit 函数来完成初始化。\n\n在 redis 6.0 中，initserverlast 函数在调用 bioinit 后，新增了对 initthreadedio 函数的调用，以初始化多线程 io 机制。\n\ninitthreadedio 函数用于初始化多 io 线程，其代码实现如下所示：\n\n// server.c#initserverlast\nvoid initserverlast() {\n    bioinit();\n    initthreadedio();\n    set_jemalloc_bg_thread(server.jemalloc_bg_thread);\n    server.initial_memory_usage = zmalloc_used_memory();\n}\n\n\n> bioinit 函数用于初始化 redis 的后台 io 线程，处理如 rdb/aof 持久化等耗时操作。而 initthreadedio 函数在此基础上进一步初始化多线程 io 机制，以支持更高效的客户端请求处理。\n\ninitthreadedio 函数的主要任务是初始化 io 线程，其代码实现如下：\n\n// networking.c#initthreadedio\nvoid initthreadedio(void) {\n    server.io_threads_active = 0; /* 初始化时线程未激活。 */\n\n    /* 如果用户选择了单线程，则不创建额外线程：\n     * 所有 i/o 操作将由主线程处理。 */\n    if (server.io_threads_num == 1) return;\n\n    if (server.io_threads_num > io_threads_max_num) {\n        serverlog(ll_warning,"致命错误：配置了过多的 i/o 线程。"\n                             "最大允许数量为 %d。", io_threads_max_num);\n        exit(1);\n    }\n\n    /* 创建并初始化 i/o 线程。 */\n    for (int i = 0; i < server.io_threads_num; i++) {\n        /* 对所有线程（包括主线程）执行的操作。 */\n        io_threads_list[i] = listcreate();\n        if (i == 0) continue; /* 线程 0 是主线程。 */\n\n        /* 对额外线程执行的操作。 */\n        pthread_t tid;\n        pthread_mutex_init(&io_threads_mutex[i],null);\n        setiopendingcount(i, 0);\n        pthread_mutex_lock(&io_threads_mutex[i]); /* 线程将被暂停。 */\n        // 创建线程，并指定处理方法 iothreadmain\n        if (pthread_create(&tid,null,iothreadmain,(void*)(long)i) != 0) {\n            serverlog(ll_warning,"致命错误：无法初始化 io 线程。");\n            exit(1);\n        }\n        io_threads[i] = tid;\n    }\n}\n\n\n 1. 首先，initthreadedio 函数会设置 io 线程的激活标志。\n 2. 随后，initthreadedio 函数对设置的 io 线程数量进行检查：\n    1. 如果 io 线程数量为 1，则表示只有一个主线程，initthreadedio 函数将直接返回。在这种情况下，redis server 的 io 线程配置与 redis 6.0 之前的版本相同。\n    2. 如果 io 线程数量超过宏定义 io_threads_max_num（默认值为 128），initthreadedio 函数会报错并退出程序。\n    3. 如果 io 线程数量在 1 和 io_threads_max_num 之间，initthreadedio 函数会执行一个循环，该循环次数等于设置的 io 线程数量（注意，i == 0 表示主线程）。\n\n> io_threads_max_num 是一个宏定义，表示 redis 支持的最大 io 线程数量，默认值为 128。这一限制旨在防止过多线程对系统性能造成负担。\n\n在该循环中，initthreadedio 函数会对以下四个数组进行初始化：\n\n * io_threads_list 数组：保存每个 io 线程要处理的客户端列表，数组的每个元素初始化为一个 list 类型的列表。\n * io_threads_pending 数组：保存等待每个 io 线程处理的客户端数量。\n * io_threads_mutex 数组：保存线程的互斥锁。\n * io_threads 数组：保存每个 io 线程的描述符。\n\n这些数组的定义都在 networking.c 文件中，如下所示：\n\npthread_t io_threads[io_threads_max_num];   // 记录线程描述符的数组\npthread_mutex_t io_threads_mutex[io_threads_max_num];  // 记录线程互斥锁的数组\n_atomic unsigned long io_threads_pending[io_threads_max_num];  // 记录线程待处理的客户端数量\nlist *io_threads_list[io_threads_max_num];  // 记录线程对应处理的客户端列表\n\n\n在对这些数组进行初始化的同时，initthreadedio 函数还会根据 io 线程数量，调用 pthread_create 函数创建相应数量的线程。\n\n在 for 循环中，pthread_create 函数用于创建线程。每个线程执行 iothreadmain 函数来处理客户端请求。如果 pthread_create 返回非零值，则说明线程创建失败，此时 redis 会记录错误并退出。\n\n因此，initthreadedio 函数创建的线程运行的函数是 iothreadmain，参数为当前创建线程的编号。需要注意的是，该编号从 1 开始，而编号为 0 的线程实际上是运行 redis server 主流程的主线程。\n\nvoid *iothreadmain(void *myid) {\n    /* id 是线程编号（从 0 到 server.iothreads_num-1） */\n    long id = (unsigned long)myid;\n    char thdname[16];\n\n    snprintf(thdname, sizeof(thdname), "io_thd_%ld", id);\n    redis_set_thread_title(thdname);\n    redissetcpuaffinity(server.server_cpulist);\n    makethreadkillable();\n\n    while(1) {\n        /* 等待开始 */\n        for (int j = 0; j < 1000000; j++) {\n            if (getiopendingcount(id) != 0) break;\n        }\n\n        /* 给主线程一个机会来停止此线程。 */\n        if (getiopendingcount(id) == 0) {\n            pthread_mutex_lock(&io_threads_mutex[id]);\n            pthread_mutex_unlock(&io_threads_mutex[id]);\n            continue;\n        }\n\n        serverassert(getiopendingcount(id) != 0);\n\n        if (tio_debug) printf("[%ld] %d to handle\\n", id, (int)listlength(io_threads_list[id]));\n\n        /* 处理：注意主线程不会在我们将待处理数量减少到 0 之前触碰我们的列表 */\n        listiter li;\n        listnode *ln;\n        listrewind(io_threads_list[id],&li);\n        while((ln = listnext(&li))) {\n            client *c = listnodevalue(ln);\n            if (io_threads_op == io_threads_op_write) {\n                writetoclient(c,0);\n            } else if (io_threads_op == io_threads_op_read) {\n                readqueryfromclient(c->conn);\n            } else {\n                serverpanic("io_threads_op 值未知");\n            }\n        }\n        listempty(io_threads_list[id]);\n        setiopendingcount(id, 0);\n\n        if (tio_debug) printf("[%ld] done\\n", id);\n    }\n}\n\n\niothreadmain 函数也在 networking.c 文件中定义，其主要逻辑为一个 while(1) 循环。\n\niothreadmain 函数在循环中处理 io_threads_list 数组中每个线程的客户端请求。\n\n正如之前所述，io_threads_list 数组中为每个 io 线程使用一个列表记录待处理的客户端。因此，iothreadmain 函数会从每个 io 线程对应的列表中取出待处理的客户端，并根据操作类型执行相应操作。操作类型由变量 io_threads_op 表示，其值有两种：\n\n * io_threads_op 的值为宏定义 io_threads_op_write：表示该 io 线程进行写操作，将数据从 redis 写回客户端，线程会调用 writetoclient 函数。\n * io_threads_op 的值为宏定义 io_threads_op_read：表示该 io 线程进行读操作，从客户端读取数据，线程会调用 readqueryfromclient 函数。\n\n笔记\n\n如果您对 java 编程熟悉，可以将 iothreadmain 函数视为 runnable 的具体实现。其核心逻辑在于 while(1) 无限循环中。根据源码，io 线程从 io_threads_list 队列（或列表）中获取待处理的客户端，并根据操作类型选择具体的执行逻辑。这是一种典型的 生产者-消费者模型，主线程负责投递事件，io 线程负责消费事件（主线程也参与）。\n\n我绘制了下图，以展示 iothreadmain 函数的基本流程，请参考：\n\n\n\n如上所示，每个 io 线程在运行过程中，会不断检查是否有待处理的客户端请求。如果存在待处理的客户端，线程会根据操作类型，从客户端读取数据或将数据写回客户端。这些操作涉及 redis 与客户端之间的 i/o 交互，因此这些线程被称为 io 线程。\n\n在此，您可能会产生一些疑问：io 线程如何将客户端添加到 io_threads_list 数组中？\n\n这涉及 redis server 的全局变量 server。server 变量中包含两个 list 类型的成员变量：clients_pending_write 和 clients_pending_read，分别记录待写回数据的客户端和待读取数据的客户端，如下所示：\n\nstruct redisserver {\n    ...\n    // 待写回数据的客户端\n    list *clients_pending_write;  \n    // 待读取数据的客户端\n    list *clients_pending_read;\n    ...\n}\n\n\nredis server 在接收客户端请求和返回数据的过程中，会根据特定条件推迟客户端的读写操作，并将这些客户端分别保存到这两个列表中。随后，在每次进入事件循环前，redis server 会将列表中的客户端添加到 io_threads_list 数组中，由 io 线程进行处理。\n\n接下来，我们将探讨 redis 如何推迟客户端的读写操作，并将这些客户端添加到 clients_pending_write 和 clients_pending_read 列表中。\n\n\n# 2. 多线程读\n\n在早期的单线程版本中，当多路复用检测到客户端数据准备就绪时，主事件循环会轮询处理这些就绪的客户端，步骤如下：\n\n 1. 读取数据\n 2. 解析数据\n 3. 执行命令\n 4. 将数据写回客户端缓冲区\n 5. 等待下一轮主事件循环\n 6. 将客户端缓冲数据写回客户端\n\n在多线程模式下（假设配置了多线程读），上述流程有所变化：数据读取和解析操作将被分配给多个 io 线程（包括主线程）。\n\n所有就绪客户端将暂存至队列中：\n\nstruct redisserver {  \n   ...\n   list *clients_pending_read;\n   ...\n}\n\n\n处理流程如下：\n\n 1. 主线程开始监听 io 事件\n 2. 主线程调用 readqueryfromclient\n 3. postponeclientread 将客户端添加至 clients_pending_read\n 4. handleclientswithpendingreadsusingthreads 将 clients_pending_read 列表中的客户端分配给所有 io 线程\n 5. 主线程阻塞并等待所有 io 线程完成读取\n 6. 主线程循环遍历并处理所有读取到的数据\n\n# 入队：如何推迟客户端读操作？\n\nredis server 在与客户端建立连接后，会开始监听客户端的可读事件。处理可读事件的回调函数是 readqueryfromclient。我在某处已介绍了这一过程，您可以再次回顾。\n\n在 redis 6.0 版本中，readqueryfromclient 函数首先从传入的参数 conn 中获取客户端 c，然后调用 postponeclientread 函数来判断是否推迟从客户端读取数据。执行逻辑如下：\n\nvoid readqueryfromclient(connection *conn) {\n    client *c = conngetprivatedata(conn);  // 从连接数据结构中获取客户端\n    ...\n    if (postponeclientread(c)) return;  // 判断是否推迟从客户端读取数据\n    ...\n}\n\n\n接下来，我们将分析 postponeclientread 函数的执行逻辑。该函数会根据以下四个条件判断是否可以推迟从客户端读取数据：\n\n条件一：全局变量 server 的 io_threads_active 值为 1\n\n这表示多 io 线程已激活。正如前述，该变量在 initthreadedio 函数中初始化为 0，表明多 io 线程初始化后默认未激活（后文将详细介绍何时将该变量值设置为 1）。\n\n条件二：全局变量 server 的 io_threads_do_read 值为 1\n\n这表示多 io 线程可以处理延后执行的客户端读操作。该变量在 redis 配置文件 redis.conf 中通过 io-threads-do-reads 配置项设置，默认为 no，即多 io 线程机制默认不用于客户端读操作。若要启用多 io 线程处理客户端读操作，需将 io-threads-do-reads 配置项设为 yes。\n\n条件三：processingeventswhileblocked 变量值为 0\n\n这表示 processeventswhileblocked 函数未在执行中。processingeventswhileblocked 是一个全局变量，当 processeventswhileblocked 函数执行时，该变量值为 1，函数执行完成后值为 0。processeventswhileblocked 函数在 networking.c 文件中实现，主要用于在 redis 读取 rdb 或 aof 文件时处理事件，避免因读取文件阻塞 redis 导致事件处理延迟。因此，当 processeventswhileblocked 函数处理客户端可读事件时，这些客户端读操作不会被推迟。\n\n条件四：客户端当前标识不能包含 client_master、client_slave 和 client_pending_read\n\n其中，client_master 和 client_slave 标识表示客户端用于主从复制，这些客户端的读操作不会被推迟。client_pending_read 标识表示客户端已设置为推迟读操作，因此，对于已带有 client_pending_read 标识的客户端，postponeclientread 函数不会再次推迟其读操作。\n\n只有当上述四个条件均满足时，postponeclientread 函数才会推迟当前客户端的读操作。具体来说，postponeclientread 函数会为该客户端设置 client_pending_read 标识，并调用 listaddnodehead 函数，将客户端添加到全局变量 server 的 clients_pending_read 列表中。\n\n以下是 postponeclientread 函数的代码：\n\nint postponeclientread(client *c) {\n    // 判断 io 线程是否激活\n    if (server.io_threads_active && server.io_threads_do_reads &&      \n         !processingeventswhileblocked &&\n        !(c->flags & (client_master|client_slave|client_pending_read)))\n    {\n        c->flags |= client_pending_read; // 设置客户端标识为 client_pending_read，表示推迟该客户端的读操作\n        listaddnodehead(server.clients_pending_read,c); // 将客户端添加到 clients_pending_read 列表中\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\n\n综上所述，redis 在客户端读事件回调函数 readqueryfromclient 中，通过调用 postponeclientread 函数来判断并推迟客户端读操作。接下来，我们将探讨 redis 如何推迟客户端写操作。\n\n# 分配：如何将待读客户端分配给 io 线程执行？\n\n首先，我们需要了解 handleclientswithpendingreadsusingthreads 函数的作用。该函数在 beforesleep 函数中被调用。\n\n在 redis 6.0 版本的实现中，事件驱动框架通过调用 aemain 函数执行事件循环，aemain 函数进一步调用 aeprocessevents 处理各种事件。在 aeprocessevents 实际调用 aeapipoll 捕获 io 事件之前，beforesleep 函数会被执行。\n\n该过程如图所示：\n\n\n\nhandleclientswithpendingreadsusingthreads 函数的执行逻辑可分为四个步骤：\n\n第一步，该函数首先检查全局变量 server 的 io_threads_active 成员变量，确认 io 线程是否激活，同时依据 io_threads_do_reads 成员变量判断是否允许 io 线程处理待读客户端。只有在 io 线程被激活并且允许处理待读客户端的情况下，handleclientswithpendingreadsusingthreads 函数才会继续执行，否则函数将直接返回。判断逻辑如下：\n\nif (!server.io_threads_active || !server.io_threads_do_reads) \n\treturn 0;\n\n\n第二步，函数获取 clients_pending_read 列表的长度，表示待处理客户端的数量。随后，函数从 clients_pending_read 列表中逐一取出待处理的客户端，并通过客户端在列表中的序号对 io 线程数量进行取模运算。\n\n通过这种方式，客户端将被分配给对应的 io 线程。接着，函数会调用 listaddnodetail 将分配好的客户端添加到 io_threads_list 数组的相应元素中。io_threads_list 数组的每个元素是一个列表，保存了每个 io 线程需要处理的客户端。\n\n以下是具体的示例：\n\n假设 io 线程数量为 3，而 clients_pending_read 列表中有 5 个客户端，其序号分别为 0、1、2、3 和 4。在此步骤中，这些客户端的序号对线程数量 3 取模的结果分别是 0、1、2、0、1，这对应了处理这些客户端的 io 线程编号。也就是说，客户端 0 由线程 0 处理，客户端 1 由线程 1 处理，以此类推。客户端的分配方式实际上是一种 轮询 方式。\n\n下图展示了这种分配结果：\n\n\n\n以下代码展示了如何以轮询方式将客户端分配给 io 线程的执行逻辑：\n\nint processed = listlength(server.clients_pending_read);\nlistrewind(server.clients_pending_read, &li);\nint item_id = 0;\nwhile ((ln = listnext(&li))) {\n    client *c = listnodevalue(ln);\n    int target_id = item_id % server.io_threads_num;\n    listaddnodetail(io_threads_list[target_id], c);\n    item_id++;\n}\n\n\n当 handleclientswithpendingreadsusingthreads 函数完成客户端的 io 线程分配后，它会将 io 线程的操作标识设置为 读操作，即 io_threads_op_read。然后，它会遍历 io_threads_list 数组中的每个元素列表，记录每个线程待处理客户端的数量，并赋值给 io_threads_pending 数组。具体过程如下：\n\nio_threads_op = io_threads_op_read;\nfor (int j = 1; j < server.io_threads_num; j++) {\n    int count = listlength(io_threads_list[j]);\n    io_threads_pending[j] = count;\n}\n\n\n第三步，函数会将 io_threads_list 数组中的 0 号列表（即 io_threads_list[0]）中的客户端逐一取出，并调用 readqueryfromclient 函数进行处理。\n\n需要注意的是，handleclientswithpendingreadsusingthreads 函数本身由 io 主线程执行，而 io_threads_list 数组中的 0 号线程即为 io 主线程，因此此步骤是由主线程处理其待读客户端：\n\nlistrewind(io_threads_list[0], &li);  // 获取 0 号列表中的所有客户端\nwhile ((ln = listnext(&li))) {\n    client *c = listnodevalue(ln);\n    readqueryfromclient(c->conn);\n}\nlistempty(io_threads_list[0]); // 处理完后，清空 0 号列表\n\n\n接下来，handleclientswithpendingreadsusingthreads 函数会进入一个 while(1) 循环，等待所有 io 线程完成对待读客户端的处理，如下所示：\n\nwhile (1) {\n    unsigned long pending = 0;\n    for (int j = 1; j < server.io_threads_num; j++)\n        pending += io_threads_pending[j];\n    if (pending == 0) break;\n}\n\n\n第四步，函数会再次遍历 clients_pending_read 列表，逐一取出其中的客户端。接着，函数会检查客户端是否有 client_pending_command 标识。如果存在，说明该客户端的命令已被某个 io 线程解析，可以执行。\n\n此时，handleclientswithpendingreadsusingthreads 函数会调用 processcommandandresetclient 执行命令，并直接调用 processinputbuffer 解析客户端中所有命令并执行。\n\n相关代码如下：\n\nwhile (listlength(server.clients_pending_read)) {\n    ln = listfirst(server.clients_pending_read);\n    client *c = listnodevalue(ln);\n    ...\n    // 如果命令已解析，则执行该命令\n    if (c->flags & client_pending_command) {\n        c->flags &= ~client_pending_command;\n        if (processcommandandresetclient(c) == c_err) {\n            continue;\n        }\n    }\n    // 解析并执行所有命令\n    processinputbuffer(c);\n}\n\n\n至此，你已经了解了如何将 clients_pending_read 列表中的待读客户端通过上述四个步骤分配给 io 线程进行处理。下图展示了这一主要过程，你可以进一步回顾：\n\n\n\n接下来，我们将探讨待写客户端的分配和处理方式。\n\n需要注意的是，当 待处理客户端 数量较少时，redis 认为不需要多线程共同处理，所有任务将由主线程完成：\n\nint stopthreadedioifneeded(void) {\n    int pending = listlength(server.clients_pending_write);\n\n    if (server.io_threads_num == 1) return 1;\n\n    if (pending < (server.io_threads_num * 2)) {\n        if (server.io_threads_active) stopthreadedio();\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\n\n当 待处理客户端数量 小于 2倍的 io 线程数 时，所有客户端数据将由主线程处理。\n\n\n# 3、多线程写\n\n# 入队：如何决定是否推迟客户端写操作？\n\n在 redis 中，当执行客户端命令后，需要向客户端返回结果时，会调用 addreply 函数将待返回的结果写入客户端的输出缓冲区。\n\n在 addreply 函数的开始部分，该函数会调用 prepareclienttowrite 函数来判断是否需要推迟执行客户端的写操作。以下代码展示了 addreply 函数如何调用 prepareclienttowrite 函数：\n\nvoid addreply(client *c, robj *obj) {\n    if (prepareclienttowrite(c) != c_ok) return;\n    ...\n}\n\n\n接下来，我们来看一下 prepareclienttowrite 函数。该函数根据客户端的设置进行一系列判断。其中，clienthaspendingreplies 函数会被调用，用于检查当前客户端的输出缓冲区中是否还有待写回的数据。\n\n如果缓冲区中没有待写回的数据，prepareclienttowrite 函数会进一步调用 clientinstallwritehandler 函数，以判断是否能够推迟客户端的写操作。以下代码展示了这一调用过程：\n\nint prepareclienttowrite(client *c) {\n    ...\n    // 如果客户端没有待写回数据，则调用 clientinstallwritehandler 函数\n    if (!clienthaspendingreplies(c)) clientinstallwritehandler(c);\n    return c_ok;\n}\n\n\n因此，推迟客户端写操作的最终决定由 clientinstallwritehandler 函数做出。该函数会检查两个条件：\n\n * 条件一：客户端未设置 client_pending_write 标识，即尚未推迟过写操作。\n * 条件二：客户端所在实例未进行主从复制，或即使正在进行主从复制，客户端所在实例作为从节点且全量复制的 rdb 文件已传输完成，可以接收请求。\n\n当上述两个条件都满足时，clientinstallwritehandler 函数会将客户端标识设置为 client_pending_write，以表示推迟该客户端的写操作。同时，该函数会将客户端添加到全局变量 server 的待写回客户端列表 clients_pending_write 中。\n\nvoid clientinstallwritehandler(client *c) {\n    // 如果客户端没有设置过 client_pending_write 标识，且客户端不在主从复制中，或作为从节点且已接收请求\n    if (!(c->flags & client_pending_write) &&\n        (c->replstate == repl_state_none ||\n         (c->replstate == slave_state_online && !c->repl_put_online_on_ack)))\n    {\n        // 将客户端标识设置为待写回，即 client_pending_write\n        c->flags |= client_pending_write;\n        listaddnodehead(server.clients_pending_write, c);  // 将客户端添加到 clients_pending_write 列表\n    }\n}\n\n\n为帮助理解，我绘制了一张图，展示了 redis 推迟客户端写操作的函数调用关系，供参考。\n\n\n\n然而，当 redis 使用 clients_pending_read 和 clients_pending_write 两个列表保存推迟执行的客户端时，这些客户端如何分配给多个 i/o 线程进行处理呢？ 这涉及到以下两个函数：\n\n * handleclientswithpendingreadsusingthreads 函数：负责将 clients_pending_read 列表中的客户端分配给 i/o 线程处理。\n * handleclientswithpendingwritesusingthreads 函数：负责将 clients_pending_write 列表中的客户端分配给 i/o 线程处理。\n\n接下来，我们将详细介绍这两个函数的具体操作。\n\n# 分配：如何将待读客户端分配给 10 个线程执行？\n\n与待读客户端的分配类似，待写客户端的分配处理由 handleclientswithpendingwritesusingthreads 函数完成，该函数同样在 beforesleep 函数中被调用。\n\nhandleclientswithpendingwritesusingthreads 函数的主要流程可以分为四个步骤，其中第二、第三和第四步的执行逻辑与 handleclientswithpendingreadsusingthreads 函数类似。\n\n简言之，在第二步中，handleclientswithpendingwritesusingthreads 函数会将待写客户端按照 轮询方式 分配给 i/o 线程，并将其添加到 io_threads_list 数组的各个元素中。\n\n在第三步中，handleclientswithpendingwritesusingthreads 函数会让主 i/o 线程处理其待写客户端，并执行 while(1) 循环以等待所有 i/o 线程完成处理。\n\n在第四步中，handleclientswithpendingwritesusingthreads 函数会再次检查 clients_pending_write 列表中是否还有待写客户端。如果存在且这些客户端仍有数据待写，函数会调用 connsetwritehandler 函数注册可写事件，该事件的回调函数为 sendreplytoclient。\n\n当事件循环流程再次执行时，sendreplytoclient 函数会被调用，它会直接调用 writetoclient 函数，将客户端缓冲区中的数据写回。\n\n需要注意的是，connsetwritehandler 函数最终会映射为 connsocketsetwritehandler 函数，后者在 connection.c 文件中实现。connsocketsetwritehandler 函数会调用 aecreatefileevent 函数创建 ae_writable 事件，这即为可写事件的注册（有关 aecreatefileevent 函数的使用，可以参见第 11 讲）。\n\n与 handleclientswithpendingreadsusingthreads 函数不同的是，在第一步中，handleclientswithpendingwritesusingthreads 函数会判断 i/o 线程数量是否为 1，或待写客户端数量是否少于 i/o 线程数量的两倍。\n\n如果满足上述任一条件，则 handleclientswithpendingwritesusingthreads 函数不会采用多线程处理客户端，而是调用 handleclientswithpendingwrites 函数由主 i/o 线程直接处理待写客户端。这主要是为了在待写客户端数量较少时，节省 cpu 开销。\n\n以下是条件判断逻辑：\n\nif (server.io_threads_num == 1 || stopthreadedioifneeded()) {\n    return handleclientswithpendingwrites();\n}\n\n\n此外，handleclientswithpendingwritesusingthreads 函数在第一步中还会判断 i/o 线程是否已激活。如果未激活，则调用 startthreadedio 函数，将全局变量 server 的 io_threads_active 成员变量设置为 1，以表示 i/o 线程已激活。此判断操作如下：\n\nif (!server.io_threads_active) startthreadedio();\n\n\n总之，redis 通过 handleclientswithpendingwritesusingthreads 函数将待写客户端按轮询方式分配给各个 i/o 线程，并由这些线程负责数据的写回。\n\n\n# redis 多线程 io 的性能调优与实际问题\n\nredis 默认情况下不会开启多线程处理，官方也建议，除非性能达到瓶颈，否则没必要开启多线程。\n\n配置多少合适？\n\n官方文档 redis.conf 中介绍有：\n\n> by default threading is disabled, we suggest enabling it only in machines that have at least 4 or more cores, leaving at least one spare core. using more than 8 threads is unlikely to help much. we also recommend using threaded i/o only if you actually have performance problems, with redis instances being able to use a quite big percentage of cpu time, otherwise there is no point in using this feature.\n> \n> so for instance if you have a four cores boxes, try to use 2 or 3 i/o threads, if you have a 8 cores, try to use 6 threads. in order to enable i/o threads use the following configuration directive:\n\ncpu 4 核以上，才考虑开启多线程，其中：\n\n * 4 核开启 2 - 3 个 io 线程\n * 8 核 开启 6 个 io 线程\n * 超过 8 个 io 线程，性能提升已经不大\n\n值得注意的是，以上的 io 线程其实包含了主线程。\n\n配置：\n\n开启多线程：配置 io-thread 即可。io-thread = 1 表示只使用主线程\n\n> io-threads 4\n\n开启之后，默认写操作会通过多线程来处理，而读操作则不会。\n\n如果读操作也想要开启多线程，则需要配置：\n\n> io-threads-do-reads yes\n\n\n# 总结\n\n今天这节课，我给你介绍了 redis 6.0 中新设计实现的多 io 线程机制。这个机制的设计主要是为了使用多个 io 线程，来并发处理客户端读取数据、解析命令和写回数据。使用了多线程后，redis 就可以充分利用服务器的多核特性，从而提高 io 效率。\n\n总结来说，redis 6.0 先是在初始化过程中，根据用户设置的 io 线程数量，创建对应数量的 io 线程。\n\n当 redis server 初始化完成后正常运行时，它会在 readqueryfromclient 函数中通过调用 postponeclientread 函数来决定是否推迟客户端读操作。同时，redis server 会在 addreply 函数中通过调用 prepareclienttowrite 函数，来决定是否推迟客户端写操作。而待读写的客户端会被分别加入到 clients_pending_read 和 clients_pending_write 两个列表中。\n\n这样，每当 redis server 要进入事件循环流程前，都会在 beforesleep 函数中分别调用 handleclientswithpendingreadsusingthreads 函数和 handleclientswithpendingwritesusingthreads 函数，将待读写客户端以轮询方式分配给 io 线程，加入到 io 线程的待处理客户端列表 io_threads_list 中。\n\n而 io 线程一旦运行后，本身会一直检测 io_threads_list 中的客户端，如果有待读写客户端，io 线程就会调用 readqueryfromclient 或 writetoclient 函数来进行处理。\n\n最后，我也想再提醒你一下，多 io 线程本身并不会执行命令，它们只是利用多核并行地读取数据和解析命令，或是将 server 数据写回（下节课我还会结合分布式锁的原子性保证，来给你介绍这一部分的源码实现。）。所以，redis 执行命令的线程还是主线程。这一点对于你理解多 io 线程机制很重要，可以避免你误解 redis 有多线程同时执行命令。\n\n这样一来，我们原来针对 redis 单个主线程做的优化仍然有效，比如避免 bigkey、避免阻塞操作等。\n\n\n# 参考文献\n\n * redis 6.0之多线程，深入解读 - 知乎 (zhihu.com)\n * redis 6.0 多线程io处理过程详解 (zhihu.com)\n * redis 源码剖析与实战 (geekbang.org)',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"LRU 策略",frontmatter:{title:"LRU 策略",date:"2024-09-15T23:59:53.000Z",permalink:"/pages/b43a19/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E5%9B%9B%E3%80%81%E6%94%AF%E7%BA%BF%E4%BB%BB%E5%8A%A1/05.LRU%20%E7%AD%96%E7%95%A5.html",relativePath:"Redis 系统设计/04.四、支线任务/05.LRU 策略.md",key:"v-0c8edaf7",path:"/pages/b43a19/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:399},{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:924},{level:2,title:"全局 LRU 时钟值的计算",slug:"全局-lru-时钟值的计算",normalizedTitle:"全局 lru 时钟值的计算",charIndex:1210},{level:2,title:"键值对中 LRU 时钟值的初始化与更新",slug:"键值对中-lru-时钟值的初始化与更新",normalizedTitle:"键值对中 lru 时钟值的初始化与更新",charIndex:4327},{level:2,title:"近似 LRU 算法的实际执行",slug:"近似-lru-算法的实际执行",normalizedTitle:"近似 lru 算法的实际执行",charIndex:1360},{level:3,title:"When：什么时候执行",slug:"when-什么时候执行",normalizedTitle:"when：什么时候执行",charIndex:7381},{level:3,title:"How：如何执行",slug:"how-如何执行",normalizedTitle:"how：如何执行",charIndex:7396},{level:4,title:"判断当前内存使用情况",slug:"判断当前内存使用情况",normalizedTitle:"判断当前内存使用情况",charIndex:8701},{level:4,title:"更新待淘汰的候选键值对集合",slug:"更新待淘汰的候选键值对集合",normalizedTitle:"更新待淘汰的候选键值对集合",charIndex:8716},{level:4,title:"选择被淘汰的键值对并删除",slug:"选择被淘汰的键值对并删除",normalizedTitle:"选择被淘汰的键值对并删除",charIndex:8734},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:17247},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:17886}],headersStr:"前言 概述 全局 LRU 时钟值的计算 键值对中 LRU 时钟值的初始化与更新 近似 LRU 算法的实际执行 When：什么时候执行 How：如何执行 判断当前内存使用情况 更新待淘汰的候选键值对集合 选择被淘汰的键值对并删除 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. 为什么严格 LRU 算法在 Redis 中性能开销高？近似 LRU 如何避免问题？\n 2. Redis 如何利用全局 LRU 时钟判断淘汰数据？其优势与局限？\n 3. 为什么近似 LRU 算法中要使用“采样”策略？设计权衡是什么？\n 4. Redis 的 LRU 时钟精度为 1 秒，访问间隔小于 1 秒会影响淘汰准确性吗？\n 5. Redis 何时触发内存淘汰机制？与 Lua 脚本执行状态有何关联？\n 6. Redis 如何找到“最近最少使用”的数据？EvictionPoolLRU 数组设计考量是什么？\n 7. Redis 通过全局 LRU 时钟如何更新键值对的访问时间戳？哪些操作会更新？\n 8. EvictionPoolLRU 数组的固定大小会导致淘汰不准确吗？\n 9. Redis 为什么区分“同步删除”和“异步删除”？各自适用场景是什么？\n\n\n# 前言\n\nLRU，全称 Least Recently Used，最近最少使用，在Redis中语义就是 优先淘汰最近最不常用的数据。\n\n你觉得有哪几种方式可以实现 LRU 淘汰策略？\n\n * 「最直观的想法」：记录下每个 key 最近一次的访问时间 timestamp，timestamp 最小的 Key，就是最近未使用的，然后择时删除。**但是 **首先需要存储每个 Key 和它的 timestamp。其次，还要比较 timestamp 得出最小值。代价很大，不现实。\n\n * 「双链表+HashMap」：echo 的 LRU 算法详解，不记录具体的访问时间点(unix timestamp)，而是记录空闲时间 idle time：idle time 越小，意味着是最近被访问的\n\n你其实可以发现，如果要严格按照 LRU 基本算法「双链表+HashMap」来实现的话，你需要在代码中实现如下内容：\n\n * 要为 Redis 可容纳的所有数据维护一个链表\n * 每当有新数据插入或是现有数据被再次访问时，需要执行多次链表操作\n\nRedis 并没有采用常见的 LRU 实现，而是采用**「近似 LRU 算法」**，听 echo 娓娓道来....\n\n\n# 概述\n\nRedis 的 内存淘汰主要和两个 redis.conf 中的配置有关：\n\n * maxmemory，该配置项设定了 Redis server 可以使用的最大内存容量，一旦 server 使用的实际内存量超出该阈值时，server 就会根据 maxmemory-policy 配置项定义的策略，执行内存淘汰操作\n * maxmemory-policy，该配置项设定了 Redis server 的内存淘汰策略，主要包括近似 LRU 算法、LFU 算法、按 TTL 值淘汰和随机淘汰等几种算法\n\n我们把 Redis 对近似 LRU 算法的实现分成三个部分。\n\n * 全局 LRU 时钟值的计算：这部分包括，Redis 源码为了实现近似 LRU 算法的效果，是 如何计算全局 LRU 时钟值 的，以用来判断数据访问的时效性\n * 键值对 LRU 时钟值的初始化与更新：这部分包括，Redis 源码在哪些函数中对每个键值对对应的 LRU 时钟值，进行初始化与更新\n * 近似 LRU 算法的实际执行：这部分包括，Redis 源码具体如何执行近似 LRU 算法，也就是何时触发数据淘汰，以及实际淘汰的机制是怎么实现的。\n\n上述三部分的整体流程：Redis 在某个时刻去取 全局LRU时钟值 来刷新 键值对的LRU时钟值，然后在某个时刻根据这个时钟值去 淘汰数据\n\n\n# 全局 LRU 时钟值的计算\n\n虽然 Redis 使用了近似 LRU 算法，但是，这个算法仍然需要区分不同数据的访问时效性，也就是说，Redis 需要知道数据的最近一次访问时间。因此，Redis 就设计了 LRU 时钟来记录数据每次访问的时间戳。\n\nRedis 在源码中对于每个键值对中的值，会使用一个 redisObject 结构体来保存指向值的指针。\n\n那么，redisObject 结构体除了记录值的指针以外，还会 使用 24 bits 来保存 LRU 时钟信息，对应的是 LRU 成员变量。所以这样一来，每个键值对都会把它最近一次被访问的时间戳，记录在 LRU 变量当中。\n\ntypedef struct redisObject {\n    unsigned type:4;\n    unsigned encoding:4;\n    unsigned lru:LRU_BITS;  //记录LRU信息，宏定义LRU_BITS是24 bits\n    int refcount;\n    void *ptr;\n} robj;\n\n\n但是，每个键值对的 LRU 时钟值具体是 如何计算 的呢？其实，Redis server 使用了一个实例级别的 全局 LRU 时钟，每个键值对的 LRU 时钟值会根据全局 LRU 时钟进行设置\n\n这个全局 LRU 时钟保存在了 Redis 全局变量 server 的成员变量 lruclock 中。当 Redis server 启动后，调用 initServerConfig 函数初始化各项参数时，就会对这个全局 LRU 时钟 lruclock 进行设置。具体来说，initServerConfig 函数是调用 getLRUClock 函数，来设置 lruclock 的值，如下所示：\n\n// 调用getLRUClock函数计算全局LRU时钟值\nunsigned int lruclock = getLRUClock();\n//设置lruclock为刚计算的LRU时钟值\natomicSet(server.lruclock,lruclock);\n\n\n所以，全局 LRU 时钟值就是通过 getLRUClock 函数计算得到的。\n\ngetLRUClock 函数是在 evict.c 文件中实现的，它会调用 mstime 函数（在 server.c 文件中）获得以毫秒为单位计算的 UNIX 时间戳，然后将这个 UNIX 时间戳除以宏定义 LRU_CLOCK_RESOLUTION。宏定义 LRU_CLOCK_RESOLUTION 是在 server.h 文件中定义的，它表示的是以毫秒为单位的 LRU 时钟精度，也就是以毫秒为单位来表示的 LRU 时钟最小单位。\n\n因为 LRU_CLOCK_RESOLUTION 的默认值是 1000，所以，LRU 时钟精度就是 1000 毫秒，也就是 1 秒。\n\n这样一来，你需要注意的就是，如果一个数据前后两次访问的时间间隔小于 1 秒，那么这两次访问的时间戳就是一样的。因为 LRU 时钟的精度就是 1 秒，它无法区分间隔小于 1 秒的不同时间戳。\n\n了解了宏定义 LRU_CLOCK_RESOLUTION 的含义之后，我们再来看下 getLRUClock 函数中的计算。\n\n 1. 首先，getLRUClock 函数将获得的 UNIX 时间戳，除以 LRU_CLOCK_RESOLUTION 后，就得到了以 LRU 时钟精度来计算的 UNIX 时间戳，也就是当前的 LRU 时钟值。\n 2. 紧接着，getLRUClock 函数会把 LRU 时钟值和宏定义 LRU_CLOCK_MAX 做与运算，其中宏定义 LRU_CLOCK_MAX 表示的是 LRU 时钟能表示的最大值。\n\n/* Return the LRU clock, based on the clock resolution. This is a time\n * in a reduced-bits format that can be used to set and check the\n * object->lru field of redisObject structures. */\nunsigned int getLRUClock(void) {\n    return (mstime()/LRU_CLOCK_RESOLUTION) & LRU_CLOCK_MAX;\n}\n\n\n#define LRU_BITS 24\n#define LRU_CLOCK_MAX ((1<<LRU_BITS)-1) /* Max value of obj->lru */\n#define LRU_CLOCK_RESOLUTION 1000 /* LRU clock resolution in ms */\n\n\n所以现在，你就知道了在默认情况下，全局 LRU 时钟值是 以 1 秒为精度 来计算的 UNIX 时间戳，并且它是在 initServerConfig 函数中进行了初始化。\n\n那么接下来，你可能还会困惑的问题是：在 Redis server 的运行过程中，全局 LRU 时钟值是如何更新的呢？\n\n这就和 Redis server 在事件驱动框架中，定期运行的时间事件所对应的 serverCron 函数有关了\n\nserverCron 函数作为时间事件的回调函数，本身会按照一定的频率周期性执行，其频率值是由 Redis 配置文件 redis.conf 中的 hz 配置项决定的。hz 配置项的默认值是 10，这表示 serverCron 函数会每 100 毫秒（1 秒 /10 = 100 毫秒）运行一次。\n\n这样，在 serverCron 函数中，全局 LRU 时钟值就会按照这个函数的执行频率，定期调用 getLRUClock 函数进行更新，如下所示：\n\nint serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {\n    ...\n   \t//默认情况下，每100毫秒调用getLRUClock函数更新一次全局LRU时钟值\n    unsigned int lruclock = getLRUClock(); \n    \n    //设置lruclock变量\n    atomicSet(server.lruclock,lruclock); \n    ...\n}\n\n\n所以这样一来，每个键值对就可以从全局 LRU 时钟获取最新的访问时间戳了\n\n好，那么接下来，我们就来了解下，对于每个键值对来说，它对应的 redisObject 结构体中的 lru 变量，是在哪些函数中进行初始化和更新的\n\n\n# 键值对中 LRU 时钟值的初始化与更新\n\n首先，对于一个键值对来说，它的 LRU 时钟值最初是在这个键值对被创建的时候，进行初始化设置的，这个初始化操作是在 createObject 函数中调用的。\n\ncreateObject 函数实现在 object.c 文件当中，当 Redis 要创建一个键值对时，就会调用这个函数。\n\nrobj *createObject(int type, void *ptr) {\n    robj *o = zmalloc(sizeof(*o));\n    o->type = type;\n    o->encoding = OBJ_ENCODING_RAW;\n    o->ptr = ptr;\n    o->refcount = 1;\n\n    /* Set the LRU to the current lruclock (minutes resolution), or\n     * alternatively the LFU counter. */\n    if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n        o->lru = (LFUGetTimeInMinutes()<<8) | LFU_INIT_VAL;\n    } else {\n        o->lru = LRU_CLOCK();\n    }\n    return o;\n}\n\n\n而 createObject 函数除了会给 redisObject 结构体分配内存空间之外，它还会根据我刚才提到的 maxmemory_policy 配置项的值，来初始化设置 redisObject 结构体中的 lru 变量。\n\n具体来说，就是如果 maxmemory_policy 配置为使用 LFU 策略，那么 lru 变量值会被初始化设置为 LFU 算法的计算值。而如果 maxmemory_policy 配置项没有使用 LFU 策略，那么，createObject 函数就会调用 LRU_CLOCK 函数来设置 lru 变量的值，也就是键值对对应的 LRU 时钟值。\n\nLRU_CLOCK 函数是在 evict.c 文件中实现的，它的作用就是返回当前的全局 LRU 时钟值。因为一个键值对一旦被创建，也就相当于有了一次访问，所以它对应的 LRU 时钟值就表示了它的访问时间戳。\n\n/* This function is used to obtain the current LRU clock.\n * If the current resolution is lower than the frequency we refresh the\n * LRU clock (as it should be in production servers) we return the\n * precomputed value, otherwise we need to resort to a system call. */\nunsigned int LRU_CLOCK(void) {\n    unsigned int lruclock;\n    if (1000/server.hz <= LRU_CLOCK_RESOLUTION) {\n        atomicGet(server.lruclock,lruclock);\n    } else {\n        lruclock = getLRUClock();\n    }\n    return lruclock;\n}\n\n\n那么到这里，又出现了一个新的问题：一个键值对的 LRU 时钟值又是在什么时候被再次更新的呢？\n\n其实，只要一个键值对被访问了，它的 LRU 时钟值就会被更新。而当一个键值对被访问时，访问操作最终都会调用 lookupKey 函数。\n\nlookupKey 函数是在 db.c 文件中实现的，它会从全局哈希表中查找要访问的键值对。如果该键值对存在，那么 lookupKey 函数就会根据 maxmemory_policy 的配置值，来更新键值对的 LRU 时钟值，也就是它的访问时间戳。\n\n而当 maxmemory_policy 没有配置为 LFU 策略时，lookupKey 函数就会调用 LRU_CLOCK 函数，来获取当前的全局 LRU 时钟值，并将其赋值给键值对的 redisObject 结构体中的 lru 变量，如下所示：\n\n/* Low level key lookup API, not actually called directly from commands\n * implementations that should instead rely on lookupKeyRead(),\n * lookupKeyWrite() and lookupKeyReadWithFlags(). */\nrobj *lookupKey(redisDb *db, robj *key, int flags) {\n    dictEntry *de = dictFind(db->dict,key->ptr);\n    if (de) {\n        // 获取键值对对应的redisObject结构体\n        robj *val = dictGetVal(de);\n\n        /* Update the access time for the ageing algorithm.\n         * Don't do it if we have a saving child, as this will trigger\n         * a copy on write madness. */\n        if (!hasActiveChildProcess() && !(flags & LOOKUP_NOTOUCH)){\n            if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n                // 如果使用了LFU策略，更新LFU计数值\n                updateLFU(val);\n            } else {\n                 // 否则，调用LRU_CLOCK函数获取全局LRU时钟值\n                val->lru = LRU_CLOCK();\n            }\n        }\n        return val;\n    } else {\n        return NULL;\n    }\n}\n\n\n这样一来，每个键值对一旦被访问，就能获得最新的访问时间戳了\n\n不过现在，你可能要问了：这些访问时间戳最终是如何被用于近似 LRU 算法，来进行数据淘汰的呢？接下来，我们就来学习下近似 LRU 算法的实际执行过程\n\n\n# 近似 LRU 算法的实际执行\n\n现在我们已经知道，Redis 之所以实现近似 LRU 算法的目的，是为了减少内存资源和操作时间上的开销。那么在这里，我们其实可以从两个方面来了解近似 LRU 算法的执行过程，分别是\n\n * When：什么时候执行\n * How：如何执行\n\n\n# When：什么时候执行\n\n近似 LRU 算法的主要逻辑是在 freeMemoryIfNeeded 函数中实现的，而这个函数本身是在 evict.c 文件中实现。\n\n首先，近似 LRU 算法的主要逻辑是在 freeMemoryIfNeeded 函数中实现的，而这个函数本身是在 evict.c 文件中实现。\n\nfreeMemoryIfNeeded 函数是被 freeMemoryIfNeededAndSafe 函数（在 evict.c 文件中）调用，而 freeMemoryIfNeededAndSafe 函数又是被 processCommand 函数所调用的。你可以参考下面的图，展示了这三者的调用关系。\n\n\n\n所以，我们看到 processCommand 函数，就应该知道这个函数是 Redis 处理每个命令时都会被调用的。\n\n那么，processCommand 函数在执行的时候，实际上会根据两个条件来判断是否调用 freeMemoryIfNeededAndSafe 函数。\n\n * 条件一：设置了 maxmemory 配置项为非 0 值。\n * 条件二：Lua 脚本没有在超时运行。\n\n如果这两个条件成立，那么 processCommand 函数就会调用 freeMemoryIfNeededAndSafe 函数，如下所示：\n\nif (server.maxmemory && !server.lua_timedout) {\n    \n        int out_of_memory = freeMemoryIfNeededAndSafe() == C_ERR;\n    \n}\n\n\n也就是说，只有在这两个条件都不成立的情况下，freeMemoryIfNeeded 函数才会被调用。下面的代码展示了 freeMemoryIfNeededAndSafe 函数的执行逻辑，你可以看下。\n\n * 条件一：Lua 脚本在超时运行\n * 条件二：Redis server 正在加载数据\n\n也就是说，只有在这两个条件都不成立的情况下，freeMemoryIfNeeded 函数才会被调用。下面的代码展示了 freeMemoryIfNeededAndSafe 函数的执行逻辑，你可以看下。\n\nint freeMemoryIfNeededAndSafe(void) {\n    if (server.lua_timedout || server.loading) return C_OK;\n    return freeMemoryIfNeeded();\n}\n\n\n这样，一旦 freeMemoryIfNeeded 函数被调用了，并且 maxmemory-policy 被设置为了 allkeys-lru 或 volatile-lru，那么近似 LRU 算法就开始被触发执行了。接下来，我们就来看下近似 LRU 算法具体是如何执行的，也就是来了解 freeMemoryIfNeeded 函数的主要执行流程。\n\n\n# How：如何执行\n\n近似 LRU 算法的执行可以分成三大步骤，分别是\n\n 1. 判断当前内存使用情况\n 2. 更新待淘汰的候选键值对集合\n 3. 选择被淘汰的键值对并删除\n\n# 判断当前内存使用情况\n\n * 首先，freeMemoryIfNeeded 函数会调用 getMaxmemoryState 函数，评估当前的内存使用情况。getMaxmemoryState 函数是在 evict.c 文件中实现的，它会判断当前 Redis server 使用的内存容量是否超过了 maxmemory 配置的值。\n * 如果当前内存使用量没有超过 maxmemory，那么，getMaxmemoryState 函数会返回 C_OK，紧接着，freeMemoryIfNeeded 函数也会直接返回了。\n\nint freeMemoryIfNeeded(void) {\n    ...\n    if (getMaxmemoryState(&mem_reported,NULL,&mem_tofree,NULL) == C_OK)\n            return C_OK;\n    ...\n}\n\n\n这里，你需要注意的是，getMaxmemoryState 函数在评估当前内存使用情况的时候，如果发现已用内存超出了 maxmemory，它就会计算需要释放的内存量。这个释放的内存大小等于已使用的内存量减去 maxmemory。不过，已使用的内存量并不包括用于主从复制的复制缓冲区大小，这是 getMaxmemoryState 函数，通过调用 freeMemoryGetNotCountedMemory 函数来计算的。\n\n/* Get the memory status from the point of view of the maxmemory directive:\n * if the memory used is under the maxmemory setting then C_OK is returned.\n * Otherwise, if we are over the memory limit, the function returns\n * C_ERR.\n *\n * The function may return additional info via reference, only if the\n * pointers to the respective arguments is not NULL. Certain fields are\n * populated only when C_ERR is returned:\n *\n *  'total'     total amount of bytes used.\n *              (Populated both for C_ERR and C_OK)\n *\n *  'logical'   the amount of memory used minus the slaves/AOF buffers.\n *              (Populated when C_ERR is returned)\n *\n *  'tofree'    the amount of memory that should be released\n *              in order to return back into the memory limits.\n *              (Populated when C_ERR is returned)\n *\n *  'level'     this usually ranges from 0 to 1, and reports the amount of\n *              memory currently used. May be > 1 if we are over the memory\n *              limit.\n *              (Populated both for C_ERR and C_OK)\n */\nint getMaxmemoryState(size_t *total, size_t *logical, size_t *tofree, float *level) {\n    size_t mem_reported, mem_used, mem_tofree;\n\n    /* Check if we are over the memory usage limit. If we are not, no need\n     * to subtract the slaves output buffers. We can just return ASAP. */\n    // 计算已使用的内存量\n    mem_reported = zmalloc_used_memory();\n    if (total) *total = mem_reported;\n\n    /* We may return ASAP if there is no need to compute the level. */\n    int return_ok_asap = !server.maxmemory || mem_reported <= server.maxmemory;\n    if (return_ok_asap && !level) return C_OK;\n\n    /* Remove the size of slaves output buffers and AOF buffer from the\n     * count of used memory. */\n    // 将用于主从复制的复制缓冲区大小和AOF缓冲区大小从已使用内存量中扣除\n    mem_used = mem_reported;\n    size_t overhead = freeMemoryGetNotCountedMemory();\n    mem_used = (mem_used > overhead) ? mem_used-overhead : 0;\n\n\n    /* Compute the ratio of memory usage. */\n    // 计算内存使用率。\n    if (level) {\n        if (!server.maxmemory) {\n            *level = 0;\n        } else {\n            *level = (float)mem_used / (float)server.maxmemory;\n        }\n    }\n\n    if (return_ok_asap) return C_OK;\n\n    /* Check if we are still over the memory limit. */\n    // 检查我们是否仍然超过内存限制。\n    if (mem_used <= server.maxmemory) return C_OK;\n\n    // 计算需要释放的内存量\n    /* Compute how much memory we need to free. */\n    mem_tofree = mem_used - server.maxmemory;\n\n    if (logical) *logical = mem_used;\n    if (tofree) *tofree = mem_tofree;\n\n    return C_ERR;\n}\n\n\n而如果当前 server 使用的内存量，的确已经超出 maxmemory 的上限了，那么 freeMemoryIfNeeded 函数就会执行一个 while 循环，来淘汰数据释放内存。\n\n其实，为了淘汰数据，Redis 定义了一个数组 EvictionPoolLRU，用来保存待淘汰的候选键值对。这个数组的元素类型是 evictionPoolEntry 结构体，该结构体保存了待淘汰键值对的空闲时间 idle、对应的 key 等信息。以下代码展示了 EvictionPoolLRU 数组和 evictionPoolEntry 结构体，它们都是在 evict.c 文件中定义的。\n\nstruct evictionPoolEntry {\n    // 待淘汰的键值对的空闲时间\n    unsigned long long idle;    \n    // 待淘汰的键值对的key\n    sds key;                    \n    // 缓存的SDS对象\n    sds cached;                 \n    // 待淘汰键值对的key所在的数据库ID\n    int dbid;                   \n};\n\nstatic struct evictionPoolEntry *EvictionPoolLRU;\n\n\n这样，Redis server 在执行 initSever 函数进行初始化时，会调用 evictionPoolAlloc 函数（在 evict.c 文件中）为 EvictionPoolLRU 数组分配内存空间，该数组的大小由宏定义 EVPOOL_SIZE（在 evict.c 文件中）决定，默认是 16 个元素，也就是可以保存 16 个待淘汰的候选键值对。\n\n#define EVPOOL_SIZE 16\n\n/* Create a new eviction pool. */\nvoid evictionPoolAlloc(void) {\n    struct evictionPoolEntry *ep;\n    int j;\n\n    ep = zmalloc(sizeof(*ep)*EVPOOL_SIZE);\n    for (j = 0; j < EVPOOL_SIZE; j++) {\n        ep[j].idle = 0;\n        ep[j].key = NULL;\n        ep[j].cached = sdsnewlen(NULL,EVPOOL_CACHED_SDS_SIZE);\n        ep[j].dbid = 0;\n    }\n    EvictionPoolLRU = ep;\n}\n\n\n那么，freeMemoryIfNeeded 函数在淘汰数据的循环流程中，就会更新这个待淘汰的候选键值对集合，也就是 EvictionPoolLRU 数组。下面我就来给你具体介绍一下。\n\n# 更新待淘汰的候选键值对集合\n\n首先，freeMemoryIfNeeded 函数会调用 evictionPoolPopulate 函数（在 evict.c 文件中），而 evictionPoolPopulate 函数会先调用 dictGetSomeKeys 函数（在 dict.c 文件中），从待采样的哈希表中随机获取一定数量的 key。\n\n不过，这里还有两个地方你需要注意下。\n\n第一点，dictGetSomeKeys 函数采样的哈希表，是由 maxmemory_policy 配置项来决定的。\n\n如果 maxmemory_policy 配置的是 allkeys_lru，那么待采样哈希表就是 Redis server 的全局哈希表，也就是在所有键值对中进行采样；否则，待采样哈希表就是保存着设置了过期时间的 key 的哈希表。\n\n以下代码是 freeMemoryIfNeeded 函数中对 evictionPoolPopulate 函数的调用过程，你可以看下。\n\n/* We don't want to make local-db choices when expiring keys,\n * so to start populate the eviction pool sampling keys from\n * every DB. */\nfor (i = 0; i < server.dbnum; i++) {\n    // 对Redis server上的每一个数据库都执行\n    db = server.db+i;\n    // 根据淘汰策略，决定使用全局哈希表还是设置了过期时间的key的哈希表\n    dict = (server.maxmemory_policy & MAXMEMORY_FLAG_ALLKEYS) ?\n            db->dict : db->expires;\n    // 将选择的哈希表dict传入evictionPoolPopulate函数，同时将全局哈希表也传给evictionPoolPopulate函数\n    if ((keys = dictSize(dict)) != 0) {\n        evictionPoolPopulate(i, dict, db->dict, pool);\n        total_keys += keys;\n    }\n}\n\n\n第二点，dictGetSomeKeys 函数采样的 key 的数量，是由 redis.conf 中的配置项 maxmemory-samples 决定的，该配置项的默认值是 5。下面代码就展示了 evictionPoolPopulate 函数对 dictGetSomeKeys 函数的调用：\n\nvoid evictionPoolPopulate(int dbid, dict *sampledict, dict *keydict, struct evictionPoolEntry *pool) {\n    ...\n    //采样后的集合，大小为maxmemory_samples\n    dictEntry *samples[server.maxmemory_samples]; \n    \n    //将待采样的哈希表sampledict、采样后的集合samples、以及采样数量maxmemory_samples，作为参数传给dictGetSomeKeys\n    count = dictGetSomeKeys(sampledict,samples,server.maxmemory_samples);\n    ...\n}\n\n\n如此一来，dictGetSomeKeys 函数就能返回采样的键值对集合了。然后，evictionPoolPopulate 函数会根据实际采样到的键值对数量 count，执行一个循环。\n\nfor (j = 0; j < count; j++) {\n    ...\n    if (server.maxmemory_policy & MAXMEMORY_FLAG_LRU) {\n    \tidle = estimateObjectIdleTime(o);\n    }\n...\n\n\n紧接着，evictionPoolPopulate 函数会遍历待淘汰的候选键值对集合，也就是 EvictionPoolLRU 数组。在遍历过程中，它会尝试把采样的每一个键值对插入 EvictionPoolLRU 数组，这主要取决于以下两个条件之一：\n\n * 一是，它能在数组中找到一个尚未插入键值对的空位\n * 二是，它能在数组中找到一个空闲时间小于采样键值对空闲时间的键值对\n\n这两个条件有一个成立的话，evictionPoolPopulate 函数就可以把采样键值对插入 EvictionPoolLRU 数组。等所有采样键值对都处理完后，evictionPoolPopulate 函数就完成对待淘汰候选键值对集合的更新了。\n\n接下来，freeMemoryIfNeeded 函数，就可以开始选择最终被淘汰的键值对了。\n\n# 选择被淘汰的键值对并删除\n\n因为 evictionPoolPopulate 函数已经更新了 EvictionPoolLRU 数组，而且这个数组里面的 key，是按照空闲时间从小到大排好序了。所以，freeMemoryIfNeeded 函数会遍历一次 EvictionPoolLRU 数组，从数组的最后一个 key 开始选择，如果选到的 key 不是空值，那么就把它作为最终淘汰的 key。\n\n// 从数组最后一个key开始查找\n/* Go backward from best to worst element to evict. */\nfor (k = EVPOOL_SIZE-1; k >= 0; k--) {\n    // 当前key为空值，则查找下一个key\n    if (pool[k].key == NULL) continue;\n    bestdbid = pool[k].dbid;\n    // 从全局哈希表或是expire哈希表中，获取当前key对应的键值对；并将当前key从EvictionPoolLRU数组删除\n    if (server.maxmemory_policy & MAXMEMORY_FLAG_ALLKEYS) {\n        de = dictFind(server.db[pool[k].dbid].dict,\n            pool[k].key);\n    } else {\n        de = dictFind(server.db[pool[k].dbid].expires,\n            pool[k].key);\n    }\n\n    /* Remove the entry from the pool. */\n    if (pool[k].key != pool[k].cached)\n        sdsfree(pool[k].key);\n    pool[k].key = NULL;\n    pool[k].idle = 0;\n\n    /* If the key exists, is our pick. Otherwise it is\n     * a ghost and we need to try the next element. */\n    // 如果当前key对应的键值对不为空，选择当前key为被淘汰的key\n    if (de) {\n        bestkey = dictGetKey(de);\n        break;\n    } else {\n        //否则，继续查找下个key\n        /* Ghost... Iterate again. */\n    }\n}\n\n\n最后，一旦选到了被淘汰的 key，freeMemoryIfNeeded 函数就会 根据 Redis server 的惰性删除配置，来执行同步删除或异步删除，如下所示：\n\nif (bestkey) {\n    db = server.db+bestdbid;\n    robj *keyobj = createStringObject(bestkey,sdslen(bestkey));        //将删除key的信息传递给从库和AOF文件\n    propagateExpire(db,keyobj,server.lazyfree_lazy_eviction);\n    //如果配置了惰性删除，则进行异步删除\n    if (server.lazyfree_lazy_eviction)\n    \tdbAsyncDelete(db,keyobj);\n    else  //否则进行同步删除\n    \tdbSyncDelete(db,keyobj);\n}\n\n\n好了，到这里，freeMemoryIfNeeded 函数就淘汰了一个 key。而如果此时，释放的内存空间还不够，也就是说没有达到我前面介绍的待释放空间，那么 freeMemoryIfNeeded 函数还会重复执行前面所说的更新待淘汰候选键值对集合、选择最终淘汰 key 的过程，直到满足待释放空间的大小要求。\n\n下图就展示了 freeMemoryIfNeeded 函数涉及的基本流程，你可以再来整体回顾下。\n\n\n\n所以，你会发现\n\n近似 LRU 算法并没有使用耗时耗空间的链表，而是使用了固定大小的待淘汰数据集合，每次随机选择一些 key 加入待淘汰数据集合中。最后，再按照待淘汰集合中 key 的空闲时间长度，删除空闲时间最长的 key。\n\n这样一来，Redis 就近似实现了 LRU 算法的效果了。\n\n\n# 总结\n\n在本文中，我们深入探讨了 Redis 中LRU（Least Recently Used）算法的实现方式，以及其在内存淘汰策略中的应用。\n\n 1. 我们了解了传统LRU算法，尽管它通过维护一个精确的双向链表来记录每个键的访问时间顺序，但在实际大规模应用中会带来严重的性能开销。这种实现需要频繁更新链表，尤其是在高并发情况下，操作成本极高且资源占用过多\n 2. 为了应对这些问题，Redis采用了近似LRU算法，通过全局LRU时钟和随机采样的方式，有效降低了资源消耗。全局时钟以秒为精度，尽管存在微小的时间戳冲突风险，但它大大减少了为每个键记录精确时间的开销。而通过随机采样选择淘汰对象，Redis避免了遍历所有数据带来的性能瓶颈，进一步提高了算法效率。\n 3. 探讨了Redis的内存管理机制，尤其是它何时以及如何触发内存淘汰。通过EvictionPoolLRU数组维护待淘汰键值对的集合，Redis确保可以在较小的时间复杂度内找到空闲时间最长的键。这个固定大小的数组设计避免了复杂的链表操作，实现了在性能和准确性之间的平衡。\n 4. Redis提供了灵活的内存淘汰策略，如同步删除和异步删除，使得在不同使用场景下可以自由选择最适合的策略，进一步优化了内存管理的灵活性。\n 5. Redis通过这种近似LRU策略实现了高效、低成本的内存管理机制，为大规模高并发场景提供了强有力的支持。这一设计展示了在资源受限的环境中，如何在性能与实现成本之间做出合理的权衡与优化\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 为什么严格 lru 算法在 redis 中性能开销高？近似 lru 如何避免问题？\n 2. redis 如何利用全局 lru 时钟判断淘汰数据？其优势与局限？\n 3. 为什么近似 lru 算法中要使用“采样”策略？设计权衡是什么？\n 4. redis 的 lru 时钟精度为 1 秒，访问间隔小于 1 秒会影响淘汰准确性吗？\n 5. redis 何时触发内存淘汰机制？与 lua 脚本执行状态有何关联？\n 6. redis 如何找到“最近最少使用”的数据？evictionpoollru 数组设计考量是什么？\n 7. redis 通过全局 lru 时钟如何更新键值对的访问时间戳？哪些操作会更新？\n 8. evictionpoollru 数组的固定大小会导致淘汰不准确吗？\n 9. redis 为什么区分“同步删除”和“异步删除”？各自适用场景是什么？\n\n\n# 前言\n\nlru，全称 least recently used，最近最少使用，在redis中语义就是 优先淘汰最近最不常用的数据。\n\n你觉得有哪几种方式可以实现 lru 淘汰策略？\n\n * 「最直观的想法」：记录下每个 key 最近一次的访问时间 timestamp，timestamp 最小的 key，就是最近未使用的，然后择时删除。**但是 **首先需要存储每个 key 和它的 timestamp。其次，还要比较 timestamp 得出最小值。代价很大，不现实。\n\n * 「双链表+hashmap」：echo 的 lru 算法详解，不记录具体的访问时间点(unix timestamp)，而是记录空闲时间 idle time：idle time 越小，意味着是最近被访问的\n\n你其实可以发现，如果要严格按照 lru 基本算法「双链表+hashmap」来实现的话，你需要在代码中实现如下内容：\n\n * 要为 redis 可容纳的所有数据维护一个链表\n * 每当有新数据插入或是现有数据被再次访问时，需要执行多次链表操作\n\nredis 并没有采用常见的 lru 实现，而是采用**「近似 lru 算法」**，听 echo 娓娓道来....\n\n\n# 概述\n\nredis 的 内存淘汰主要和两个 redis.conf 中的配置有关：\n\n * maxmemory，该配置项设定了 redis server 可以使用的最大内存容量，一旦 server 使用的实际内存量超出该阈值时，server 就会根据 maxmemory-policy 配置项定义的策略，执行内存淘汰操作\n * maxmemory-policy，该配置项设定了 redis server 的内存淘汰策略，主要包括近似 lru 算法、lfu 算法、按 ttl 值淘汰和随机淘汰等几种算法\n\n我们把 redis 对近似 lru 算法的实现分成三个部分。\n\n * 全局 lru 时钟值的计算：这部分包括，redis 源码为了实现近似 lru 算法的效果，是 如何计算全局 lru 时钟值 的，以用来判断数据访问的时效性\n * 键值对 lru 时钟值的初始化与更新：这部分包括，redis 源码在哪些函数中对每个键值对对应的 lru 时钟值，进行初始化与更新\n * 近似 lru 算法的实际执行：这部分包括，redis 源码具体如何执行近似 lru 算法，也就是何时触发数据淘汰，以及实际淘汰的机制是怎么实现的。\n\n上述三部分的整体流程：redis 在某个时刻去取 全局lru时钟值 来刷新 键值对的lru时钟值，然后在某个时刻根据这个时钟值去 淘汰数据\n\n\n# 全局 lru 时钟值的计算\n\n虽然 redis 使用了近似 lru 算法，但是，这个算法仍然需要区分不同数据的访问时效性，也就是说，redis 需要知道数据的最近一次访问时间。因此，redis 就设计了 lru 时钟来记录数据每次访问的时间戳。\n\nredis 在源码中对于每个键值对中的值，会使用一个 redisobject 结构体来保存指向值的指针。\n\n那么，redisobject 结构体除了记录值的指针以外，还会 使用 24 bits 来保存 lru 时钟信息，对应的是 lru 成员变量。所以这样一来，每个键值对都会把它最近一次被访问的时间戳，记录在 lru 变量当中。\n\ntypedef struct redisobject {\n    unsigned type:4;\n    unsigned encoding:4;\n    unsigned lru:lru_bits;  //记录lru信息，宏定义lru_bits是24 bits\n    int refcount;\n    void *ptr;\n} robj;\n\n\n但是，每个键值对的 lru 时钟值具体是 如何计算 的呢？其实，redis server 使用了一个实例级别的 全局 lru 时钟，每个键值对的 lru 时钟值会根据全局 lru 时钟进行设置\n\n这个全局 lru 时钟保存在了 redis 全局变量 server 的成员变量 lruclock 中。当 redis server 启动后，调用 initserverconfig 函数初始化各项参数时，就会对这个全局 lru 时钟 lruclock 进行设置。具体来说，initserverconfig 函数是调用 getlruclock 函数，来设置 lruclock 的值，如下所示：\n\n// 调用getlruclock函数计算全局lru时钟值\nunsigned int lruclock = getlruclock();\n//设置lruclock为刚计算的lru时钟值\natomicset(server.lruclock,lruclock);\n\n\n所以，全局 lru 时钟值就是通过 getlruclock 函数计算得到的。\n\ngetlruclock 函数是在 evict.c 文件中实现的，它会调用 mstime 函数（在 server.c 文件中）获得以毫秒为单位计算的 unix 时间戳，然后将这个 unix 时间戳除以宏定义 lru_clock_resolution。宏定义 lru_clock_resolution 是在 server.h 文件中定义的，它表示的是以毫秒为单位的 lru 时钟精度，也就是以毫秒为单位来表示的 lru 时钟最小单位。\n\n因为 lru_clock_resolution 的默认值是 1000，所以，lru 时钟精度就是 1000 毫秒，也就是 1 秒。\n\n这样一来，你需要注意的就是，如果一个数据前后两次访问的时间间隔小于 1 秒，那么这两次访问的时间戳就是一样的。因为 lru 时钟的精度就是 1 秒，它无法区分间隔小于 1 秒的不同时间戳。\n\n了解了宏定义 lru_clock_resolution 的含义之后，我们再来看下 getlruclock 函数中的计算。\n\n 1. 首先，getlruclock 函数将获得的 unix 时间戳，除以 lru_clock_resolution 后，就得到了以 lru 时钟精度来计算的 unix 时间戳，也就是当前的 lru 时钟值。\n 2. 紧接着，getlruclock 函数会把 lru 时钟值和宏定义 lru_clock_max 做与运算，其中宏定义 lru_clock_max 表示的是 lru 时钟能表示的最大值。\n\n/* return the lru clock, based on the clock resolution. this is a time\n * in a reduced-bits format that can be used to set and check the\n * object->lru field of redisobject structures. */\nunsigned int getlruclock(void) {\n    return (mstime()/lru_clock_resolution) & lru_clock_max;\n}\n\n\n#define lru_bits 24\n#define lru_clock_max ((1<<lru_bits)-1) /* max value of obj->lru */\n#define lru_clock_resolution 1000 /* lru clock resolution in ms */\n\n\n所以现在，你就知道了在默认情况下，全局 lru 时钟值是 以 1 秒为精度 来计算的 unix 时间戳，并且它是在 initserverconfig 函数中进行了初始化。\n\n那么接下来，你可能还会困惑的问题是：在 redis server 的运行过程中，全局 lru 时钟值是如何更新的呢？\n\n这就和 redis server 在事件驱动框架中，定期运行的时间事件所对应的 servercron 函数有关了\n\nservercron 函数作为时间事件的回调函数，本身会按照一定的频率周期性执行，其频率值是由 redis 配置文件 redis.conf 中的 hz 配置项决定的。hz 配置项的默认值是 10，这表示 servercron 函数会每 100 毫秒（1 秒 /10 = 100 毫秒）运行一次。\n\n这样，在 servercron 函数中，全局 lru 时钟值就会按照这个函数的执行频率，定期调用 getlruclock 函数进行更新，如下所示：\n\nint servercron(struct aeeventloop *eventloop, long long id, void *clientdata) {\n    ...\n   \t//默认情况下，每100毫秒调用getlruclock函数更新一次全局lru时钟值\n    unsigned int lruclock = getlruclock(); \n    \n    //设置lruclock变量\n    atomicset(server.lruclock,lruclock); \n    ...\n}\n\n\n所以这样一来，每个键值对就可以从全局 lru 时钟获取最新的访问时间戳了\n\n好，那么接下来，我们就来了解下，对于每个键值对来说，它对应的 redisobject 结构体中的 lru 变量，是在哪些函数中进行初始化和更新的\n\n\n# 键值对中 lru 时钟值的初始化与更新\n\n首先，对于一个键值对来说，它的 lru 时钟值最初是在这个键值对被创建的时候，进行初始化设置的，这个初始化操作是在 createobject 函数中调用的。\n\ncreateobject 函数实现在 object.c 文件当中，当 redis 要创建一个键值对时，就会调用这个函数。\n\nrobj *createobject(int type, void *ptr) {\n    robj *o = zmalloc(sizeof(*o));\n    o->type = type;\n    o->encoding = obj_encoding_raw;\n    o->ptr = ptr;\n    o->refcount = 1;\n\n    /* set the lru to the current lruclock (minutes resolution), or\n     * alternatively the lfu counter. */\n    if (server.maxmemory_policy & maxmemory_flag_lfu) {\n        o->lru = (lfugettimeinminutes()<<8) | lfu_init_val;\n    } else {\n        o->lru = lru_clock();\n    }\n    return o;\n}\n\n\n而 createobject 函数除了会给 redisobject 结构体分配内存空间之外，它还会根据我刚才提到的 maxmemory_policy 配置项的值，来初始化设置 redisobject 结构体中的 lru 变量。\n\n具体来说，就是如果 maxmemory_policy 配置为使用 lfu 策略，那么 lru 变量值会被初始化设置为 lfu 算法的计算值。而如果 maxmemory_policy 配置项没有使用 lfu 策略，那么，createobject 函数就会调用 lru_clock 函数来设置 lru 变量的值，也就是键值对对应的 lru 时钟值。\n\nlru_clock 函数是在 evict.c 文件中实现的，它的作用就是返回当前的全局 lru 时钟值。因为一个键值对一旦被创建，也就相当于有了一次访问，所以它对应的 lru 时钟值就表示了它的访问时间戳。\n\n/* this function is used to obtain the current lru clock.\n * if the current resolution is lower than the frequency we refresh the\n * lru clock (as it should be in production servers) we return the\n * precomputed value, otherwise we need to resort to a system call. */\nunsigned int lru_clock(void) {\n    unsigned int lruclock;\n    if (1000/server.hz <= lru_clock_resolution) {\n        atomicget(server.lruclock,lruclock);\n    } else {\n        lruclock = getlruclock();\n    }\n    return lruclock;\n}\n\n\n那么到这里，又出现了一个新的问题：一个键值对的 lru 时钟值又是在什么时候被再次更新的呢？\n\n其实，只要一个键值对被访问了，它的 lru 时钟值就会被更新。而当一个键值对被访问时，访问操作最终都会调用 lookupkey 函数。\n\nlookupkey 函数是在 db.c 文件中实现的，它会从全局哈希表中查找要访问的键值对。如果该键值对存在，那么 lookupkey 函数就会根据 maxmemory_policy 的配置值，来更新键值对的 lru 时钟值，也就是它的访问时间戳。\n\n而当 maxmemory_policy 没有配置为 lfu 策略时，lookupkey 函数就会调用 lru_clock 函数，来获取当前的全局 lru 时钟值，并将其赋值给键值对的 redisobject 结构体中的 lru 变量，如下所示：\n\n/* low level key lookup api, not actually called directly from commands\n * implementations that should instead rely on lookupkeyread(),\n * lookupkeywrite() and lookupkeyreadwithflags(). */\nrobj *lookupkey(redisdb *db, robj *key, int flags) {\n    dictentry *de = dictfind(db->dict,key->ptr);\n    if (de) {\n        // 获取键值对对应的redisobject结构体\n        robj *val = dictgetval(de);\n\n        /* update the access time for the ageing algorithm.\n         * don't do it if we have a saving child, as this will trigger\n         * a copy on write madness. */\n        if (!hasactivechildprocess() && !(flags & lookup_notouch)){\n            if (server.maxmemory_policy & maxmemory_flag_lfu) {\n                // 如果使用了lfu策略，更新lfu计数值\n                updatelfu(val);\n            } else {\n                 // 否则，调用lru_clock函数获取全局lru时钟值\n                val->lru = lru_clock();\n            }\n        }\n        return val;\n    } else {\n        return null;\n    }\n}\n\n\n这样一来，每个键值对一旦被访问，就能获得最新的访问时间戳了\n\n不过现在，你可能要问了：这些访问时间戳最终是如何被用于近似 lru 算法，来进行数据淘汰的呢？接下来，我们就来学习下近似 lru 算法的实际执行过程\n\n\n# 近似 lru 算法的实际执行\n\n现在我们已经知道，redis 之所以实现近似 lru 算法的目的，是为了减少内存资源和操作时间上的开销。那么在这里，我们其实可以从两个方面来了解近似 lru 算法的执行过程，分别是\n\n * when：什么时候执行\n * how：如何执行\n\n\n# when：什么时候执行\n\n近似 lru 算法的主要逻辑是在 freememoryifneeded 函数中实现的，而这个函数本身是在 evict.c 文件中实现。\n\n首先，近似 lru 算法的主要逻辑是在 freememoryifneeded 函数中实现的，而这个函数本身是在 evict.c 文件中实现。\n\nfreememoryifneeded 函数是被 freememoryifneededandsafe 函数（在 evict.c 文件中）调用，而 freememoryifneededandsafe 函数又是被 processcommand 函数所调用的。你可以参考下面的图，展示了这三者的调用关系。\n\n\n\n所以，我们看到 processcommand 函数，就应该知道这个函数是 redis 处理每个命令时都会被调用的。\n\n那么，processcommand 函数在执行的时候，实际上会根据两个条件来判断是否调用 freememoryifneededandsafe 函数。\n\n * 条件一：设置了 maxmemory 配置项为非 0 值。\n * 条件二：lua 脚本没有在超时运行。\n\n如果这两个条件成立，那么 processcommand 函数就会调用 freememoryifneededandsafe 函数，如下所示：\n\nif (server.maxmemory && !server.lua_timedout) {\n    \n        int out_of_memory = freememoryifneededandsafe() == c_err;\n    \n}\n\n\n也就是说，只有在这两个条件都不成立的情况下，freememoryifneeded 函数才会被调用。下面的代码展示了 freememoryifneededandsafe 函数的执行逻辑，你可以看下。\n\n * 条件一：lua 脚本在超时运行\n * 条件二：redis server 正在加载数据\n\n也就是说，只有在这两个条件都不成立的情况下，freememoryifneeded 函数才会被调用。下面的代码展示了 freememoryifneededandsafe 函数的执行逻辑，你可以看下。\n\nint freememoryifneededandsafe(void) {\n    if (server.lua_timedout || server.loading) return c_ok;\n    return freememoryifneeded();\n}\n\n\n这样，一旦 freememoryifneeded 函数被调用了，并且 maxmemory-policy 被设置为了 allkeys-lru 或 volatile-lru，那么近似 lru 算法就开始被触发执行了。接下来，我们就来看下近似 lru 算法具体是如何执行的，也就是来了解 freememoryifneeded 函数的主要执行流程。\n\n\n# how：如何执行\n\n近似 lru 算法的执行可以分成三大步骤，分别是\n\n 1. 判断当前内存使用情况\n 2. 更新待淘汰的候选键值对集合\n 3. 选择被淘汰的键值对并删除\n\n# 判断当前内存使用情况\n\n * 首先，freememoryifneeded 函数会调用 getmaxmemorystate 函数，评估当前的内存使用情况。getmaxmemorystate 函数是在 evict.c 文件中实现的，它会判断当前 redis server 使用的内存容量是否超过了 maxmemory 配置的值。\n * 如果当前内存使用量没有超过 maxmemory，那么，getmaxmemorystate 函数会返回 c_ok，紧接着，freememoryifneeded 函数也会直接返回了。\n\nint freememoryifneeded(void) {\n    ...\n    if (getmaxmemorystate(&mem_reported,null,&mem_tofree,null) == c_ok)\n            return c_ok;\n    ...\n}\n\n\n这里，你需要注意的是，getmaxmemorystate 函数在评估当前内存使用情况的时候，如果发现已用内存超出了 maxmemory，它就会计算需要释放的内存量。这个释放的内存大小等于已使用的内存量减去 maxmemory。不过，已使用的内存量并不包括用于主从复制的复制缓冲区大小，这是 getmaxmemorystate 函数，通过调用 freememorygetnotcountedmemory 函数来计算的。\n\n/* get the memory status from the point of view of the maxmemory directive:\n * if the memory used is under the maxmemory setting then c_ok is returned.\n * otherwise, if we are over the memory limit, the function returns\n * c_err.\n *\n * the function may return additional info via reference, only if the\n * pointers to the respective arguments is not null. certain fields are\n * populated only when c_err is returned:\n *\n *  'total'     total amount of bytes used.\n *              (populated both for c_err and c_ok)\n *\n *  'logical'   the amount of memory used minus the slaves/aof buffers.\n *              (populated when c_err is returned)\n *\n *  'tofree'    the amount of memory that should be released\n *              in order to return back into the memory limits.\n *              (populated when c_err is returned)\n *\n *  'level'     this usually ranges from 0 to 1, and reports the amount of\n *              memory currently used. may be > 1 if we are over the memory\n *              limit.\n *              (populated both for c_err and c_ok)\n */\nint getmaxmemorystate(size_t *total, size_t *logical, size_t *tofree, float *level) {\n    size_t mem_reported, mem_used, mem_tofree;\n\n    /* check if we are over the memory usage limit. if we are not, no need\n     * to subtract the slaves output buffers. we can just return asap. */\n    // 计算已使用的内存量\n    mem_reported = zmalloc_used_memory();\n    if (total) *total = mem_reported;\n\n    /* we may return asap if there is no need to compute the level. */\n    int return_ok_asap = !server.maxmemory || mem_reported <= server.maxmemory;\n    if (return_ok_asap && !level) return c_ok;\n\n    /* remove the size of slaves output buffers and aof buffer from the\n     * count of used memory. */\n    // 将用于主从复制的复制缓冲区大小和aof缓冲区大小从已使用内存量中扣除\n    mem_used = mem_reported;\n    size_t overhead = freememorygetnotcountedmemory();\n    mem_used = (mem_used > overhead) ? mem_used-overhead : 0;\n\n\n    /* compute the ratio of memory usage. */\n    // 计算内存使用率。\n    if (level) {\n        if (!server.maxmemory) {\n            *level = 0;\n        } else {\n            *level = (float)mem_used / (float)server.maxmemory;\n        }\n    }\n\n    if (return_ok_asap) return c_ok;\n\n    /* check if we are still over the memory limit. */\n    // 检查我们是否仍然超过内存限制。\n    if (mem_used <= server.maxmemory) return c_ok;\n\n    // 计算需要释放的内存量\n    /* compute how much memory we need to free. */\n    mem_tofree = mem_used - server.maxmemory;\n\n    if (logical) *logical = mem_used;\n    if (tofree) *tofree = mem_tofree;\n\n    return c_err;\n}\n\n\n而如果当前 server 使用的内存量，的确已经超出 maxmemory 的上限了，那么 freememoryifneeded 函数就会执行一个 while 循环，来淘汰数据释放内存。\n\n其实，为了淘汰数据，redis 定义了一个数组 evictionpoollru，用来保存待淘汰的候选键值对。这个数组的元素类型是 evictionpoolentry 结构体，该结构体保存了待淘汰键值对的空闲时间 idle、对应的 key 等信息。以下代码展示了 evictionpoollru 数组和 evictionpoolentry 结构体，它们都是在 evict.c 文件中定义的。\n\nstruct evictionpoolentry {\n    // 待淘汰的键值对的空闲时间\n    unsigned long long idle;    \n    // 待淘汰的键值对的key\n    sds key;                    \n    // 缓存的sds对象\n    sds cached;                 \n    // 待淘汰键值对的key所在的数据库id\n    int dbid;                   \n};\n\nstatic struct evictionpoolentry *evictionpoollru;\n\n\n这样，redis server 在执行 initsever 函数进行初始化时，会调用 evictionpoolalloc 函数（在 evict.c 文件中）为 evictionpoollru 数组分配内存空间，该数组的大小由宏定义 evpool_size（在 evict.c 文件中）决定，默认是 16 个元素，也就是可以保存 16 个待淘汰的候选键值对。\n\n#define evpool_size 16\n\n/* create a new eviction pool. */\nvoid evictionpoolalloc(void) {\n    struct evictionpoolentry *ep;\n    int j;\n\n    ep = zmalloc(sizeof(*ep)*evpool_size);\n    for (j = 0; j < evpool_size; j++) {\n        ep[j].idle = 0;\n        ep[j].key = null;\n        ep[j].cached = sdsnewlen(null,evpool_cached_sds_size);\n        ep[j].dbid = 0;\n    }\n    evictionpoollru = ep;\n}\n\n\n那么，freememoryifneeded 函数在淘汰数据的循环流程中，就会更新这个待淘汰的候选键值对集合，也就是 evictionpoollru 数组。下面我就来给你具体介绍一下。\n\n# 更新待淘汰的候选键值对集合\n\n首先，freememoryifneeded 函数会调用 evictionpoolpopulate 函数（在 evict.c 文件中），而 evictionpoolpopulate 函数会先调用 dictgetsomekeys 函数（在 dict.c 文件中），从待采样的哈希表中随机获取一定数量的 key。\n\n不过，这里还有两个地方你需要注意下。\n\n第一点，dictgetsomekeys 函数采样的哈希表，是由 maxmemory_policy 配置项来决定的。\n\n如果 maxmemory_policy 配置的是 allkeys_lru，那么待采样哈希表就是 redis server 的全局哈希表，也就是在所有键值对中进行采样；否则，待采样哈希表就是保存着设置了过期时间的 key 的哈希表。\n\n以下代码是 freememoryifneeded 函数中对 evictionpoolpopulate 函数的调用过程，你可以看下。\n\n/* we don't want to make local-db choices when expiring keys,\n * so to start populate the eviction pool sampling keys from\n * every db. */\nfor (i = 0; i < server.dbnum; i++) {\n    // 对redis server上的每一个数据库都执行\n    db = server.db+i;\n    // 根据淘汰策略，决定使用全局哈希表还是设置了过期时间的key的哈希表\n    dict = (server.maxmemory_policy & maxmemory_flag_allkeys) ?\n            db->dict : db->expires;\n    // 将选择的哈希表dict传入evictionpoolpopulate函数，同时将全局哈希表也传给evictionpoolpopulate函数\n    if ((keys = dictsize(dict)) != 0) {\n        evictionpoolpopulate(i, dict, db->dict, pool);\n        total_keys += keys;\n    }\n}\n\n\n第二点，dictgetsomekeys 函数采样的 key 的数量，是由 redis.conf 中的配置项 maxmemory-samples 决定的，该配置项的默认值是 5。下面代码就展示了 evictionpoolpopulate 函数对 dictgetsomekeys 函数的调用：\n\nvoid evictionpoolpopulate(int dbid, dict *sampledict, dict *keydict, struct evictionpoolentry *pool) {\n    ...\n    //采样后的集合，大小为maxmemory_samples\n    dictentry *samples[server.maxmemory_samples]; \n    \n    //将待采样的哈希表sampledict、采样后的集合samples、以及采样数量maxmemory_samples，作为参数传给dictgetsomekeys\n    count = dictgetsomekeys(sampledict,samples,server.maxmemory_samples);\n    ...\n}\n\n\n如此一来，dictgetsomekeys 函数就能返回采样的键值对集合了。然后，evictionpoolpopulate 函数会根据实际采样到的键值对数量 count，执行一个循环。\n\nfor (j = 0; j < count; j++) {\n    ...\n    if (server.maxmemory_policy & maxmemory_flag_lru) {\n    \tidle = estimateobjectidletime(o);\n    }\n...\n\n\n紧接着，evictionpoolpopulate 函数会遍历待淘汰的候选键值对集合，也就是 evictionpoollru 数组。在遍历过程中，它会尝试把采样的每一个键值对插入 evictionpoollru 数组，这主要取决于以下两个条件之一：\n\n * 一是，它能在数组中找到一个尚未插入键值对的空位\n * 二是，它能在数组中找到一个空闲时间小于采样键值对空闲时间的键值对\n\n这两个条件有一个成立的话，evictionpoolpopulate 函数就可以把采样键值对插入 evictionpoollru 数组。等所有采样键值对都处理完后，evictionpoolpopulate 函数就完成对待淘汰候选键值对集合的更新了。\n\n接下来，freememoryifneeded 函数，就可以开始选择最终被淘汰的键值对了。\n\n# 选择被淘汰的键值对并删除\n\n因为 evictionpoolpopulate 函数已经更新了 evictionpoollru 数组，而且这个数组里面的 key，是按照空闲时间从小到大排好序了。所以，freememoryifneeded 函数会遍历一次 evictionpoollru 数组，从数组的最后一个 key 开始选择，如果选到的 key 不是空值，那么就把它作为最终淘汰的 key。\n\n// 从数组最后一个key开始查找\n/* go backward from best to worst element to evict. */\nfor (k = evpool_size-1; k >= 0; k--) {\n    // 当前key为空值，则查找下一个key\n    if (pool[k].key == null) continue;\n    bestdbid = pool[k].dbid;\n    // 从全局哈希表或是expire哈希表中，获取当前key对应的键值对；并将当前key从evictionpoollru数组删除\n    if (server.maxmemory_policy & maxmemory_flag_allkeys) {\n        de = dictfind(server.db[pool[k].dbid].dict,\n            pool[k].key);\n    } else {\n        de = dictfind(server.db[pool[k].dbid].expires,\n            pool[k].key);\n    }\n\n    /* remove the entry from the pool. */\n    if (pool[k].key != pool[k].cached)\n        sdsfree(pool[k].key);\n    pool[k].key = null;\n    pool[k].idle = 0;\n\n    /* if the key exists, is our pick. otherwise it is\n     * a ghost and we need to try the next element. */\n    // 如果当前key对应的键值对不为空，选择当前key为被淘汰的key\n    if (de) {\n        bestkey = dictgetkey(de);\n        break;\n    } else {\n        //否则，继续查找下个key\n        /* ghost... iterate again. */\n    }\n}\n\n\n最后，一旦选到了被淘汰的 key，freememoryifneeded 函数就会 根据 redis server 的惰性删除配置，来执行同步删除或异步删除，如下所示：\n\nif (bestkey) {\n    db = server.db+bestdbid;\n    robj *keyobj = createstringobject(bestkey,sdslen(bestkey));        //将删除key的信息传递给从库和aof文件\n    propagateexpire(db,keyobj,server.lazyfree_lazy_eviction);\n    //如果配置了惰性删除，则进行异步删除\n    if (server.lazyfree_lazy_eviction)\n    \tdbasyncdelete(db,keyobj);\n    else  //否则进行同步删除\n    \tdbsyncdelete(db,keyobj);\n}\n\n\n好了，到这里，freememoryifneeded 函数就淘汰了一个 key。而如果此时，释放的内存空间还不够，也就是说没有达到我前面介绍的待释放空间，那么 freememoryifneeded 函数还会重复执行前面所说的更新待淘汰候选键值对集合、选择最终淘汰 key 的过程，直到满足待释放空间的大小要求。\n\n下图就展示了 freememoryifneeded 函数涉及的基本流程，你可以再来整体回顾下。\n\n\n\n所以，你会发现\n\n近似 lru 算法并没有使用耗时耗空间的链表，而是使用了固定大小的待淘汰数据集合，每次随机选择一些 key 加入待淘汰数据集合中。最后，再按照待淘汰集合中 key 的空闲时间长度，删除空闲时间最长的 key。\n\n这样一来，redis 就近似实现了 lru 算法的效果了。\n\n\n# 总结\n\n在本文中，我们深入探讨了 redis 中lru（least recently used）算法的实现方式，以及其在内存淘汰策略中的应用。\n\n 1. 我们了解了传统lru算法，尽管它通过维护一个精确的双向链表来记录每个键的访问时间顺序，但在实际大规模应用中会带来严重的性能开销。这种实现需要频繁更新链表，尤其是在高并发情况下，操作成本极高且资源占用过多\n 2. 为了应对这些问题，redis采用了近似lru算法，通过全局lru时钟和随机采样的方式，有效降低了资源消耗。全局时钟以秒为精度，尽管存在微小的时间戳冲突风险，但它大大减少了为每个键记录精确时间的开销。而通过随机采样选择淘汰对象，redis避免了遍历所有数据带来的性能瓶颈，进一步提高了算法效率。\n 3. 探讨了redis的内存管理机制，尤其是它何时以及如何触发内存淘汰。通过evictionpoollru数组维护待淘汰键值对的集合，redis确保可以在较小的时间复杂度内找到空闲时间最长的键。这个固定大小的数组设计避免了复杂的链表操作，实现了在性能和准确性之间的平衡。\n 4. redis提供了灵活的内存淘汰策略，如同步删除和异步删除，使得在不同使用场景下可以自由选择最适合的策略，进一步优化了内存管理的灵活性。\n 5. redis通过这种近似lru策略实现了高效、低成本的内存管理机制，为大规模高并发场景提供了强有力的支持。这一设计展示了在资源受限的环境中，如何在性能与实现成本之间做出合理的权衡与优化\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"LFU 策略",frontmatter:{title:"LFU 策略",date:"2024-09-15T23:59:53.000Z",permalink:"/pages/b43a89/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E5%9B%9B%E3%80%81%E6%94%AF%E7%BA%BF%E4%BB%BB%E5%8A%A1/10.LFU%20%E7%AD%96%E7%95%A5.html",relativePath:"Redis 系统设计/04.四、支线任务/10.LFU 策略.md",key:"v-41c60043",path:"/pages/b43a89/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:311},{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:510},{level:2,title:"LFU 算法的实现",slug:"lfu-算法的实现",normalizedTitle:"lfu 算法的实现",charIndex:1011},{level:3,title:"键值对访问频率记录",slug:"键值对访问频率记录",normalizedTitle:"键值对访问频率记录",charIndex:1247},{level:3,title:"键值对访问频率的初始化与更新",slug:"键值对访问频率的初始化与更新",normalizedTitle:"键值对访问频率的初始化与更新",charIndex:1719},{level:4,title:"第一步，衰减访问次数",slug:"第一步-衰减访问次数",normalizedTitle:"第一步，衰减访问次数",charIndex:4252},{level:4,title:"第二步，根据当前访问更新访问次数",slug:"第二步-根据当前访问更新访问次数",normalizedTitle:"第二步，根据当前访问更新访问次数",charIndex:7252},{level:4,title:"第三步，更新 lru 变量值",slug:"第三步-更新-lru-变量值",normalizedTitle:"第三步，更新 lru 变量值",charIndex:9016},{level:3,title:"LFU 算法淘汰数据",slug:"lfu-算法淘汰数据",normalizedTitle:"lfu 算法淘汰数据",charIndex:1279},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:9719},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:11203}],headersStr:"前言 概述 LFU 算法的实现 键值对访问频率记录 键值对访问频率的初始化与更新 第一步，衰减访问次数 第二步，根据当前访问更新访问次数 第三步，更新 lru 变量值 LFU 算法淘汰数据 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. 在高频访问与低频访问的数据中，如何有效决定哪个数据应该优先淘汰？\n 2. 为什么 LFU 算法比 LRU 更适合处理那些偶尔被频繁访问的数据？\n 3. LFU 算法如何在缓存淘汰中既考虑访问频率，又避免单纯的访问次数统计？\n 4. 当缓存容量不足时，Redis 的 LFU 算法是如何动态调整和减少数据误判的？\n 5. 如何在 Redis 中通过 LFU 算法记录访问频率，并避免访问次数无限增长？\n 6. 如何通过调整 LFU 的衰减机制来平衡频繁访问和长时间未访问的数据淘汰？\n 7. Redis 是如何利用 LFU 算法实现缓存淘汰的同时，确保系统性能和内存的高效使用？\n\n\n# 前言\n\nRedis 在 4.0 版本后，还引入了 LFU 算法，也就是，最不频繁使用（Least Frequently Used，LFU）\n\n * LFU 算法在进行数据淘汰时，会把最不频繁访问的数据淘汰掉\n * 而 LRU 算法是把最近最少使用的数据淘汰掉，看起来也是淘汰不频繁访问的数据。\n\nLFU 算法和 LRU 算法的区别到底有哪些呢？我们在实际场景中，需要使用 LFU 算法吗？\n\n\n# 概述\n\n因为 LFU 算法是根据 数据访问的频率 来选择被淘汰数据的，所以 LFU 算法会 记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。\n\n不过，访问次数和访问频率还不能完全等同。\n\n访问频率是指在一定时间内的访问次数，也就是说，在计算访问频率时，我们不仅需要记录访问次数，还要记录这些访问是在多长时间内执行的。否则，如果只记录访问次数的话，就缺少了时间维度的信息，进而就无法按照频率来淘汰数据了\n\n> 我来给你举个例子，假设数据 A 在 15 分钟内访问了 15 次，数据 B 在 5 分钟内访问了 10 次。如果只是按访问次数来统计的话，数据 A 的访问次数大于数据 B，所以淘汰数据时会优先淘汰数据 B。不过，如果按照访问频率来统计的话，数据 A 的访问频率是 1 分钟访问 1 次，而数据 B 的访问频率是 1 分钟访问 2 次，所以按访问频率淘汰数据的话，数据 A 应该被淘汰掉。 所以说，当要实现 LFU 算法时，我们需要能统计到数据的访问频率，而不是简单地记录数据访问次数就行。\n\n那么接下来，我们就来学习下 Redis 是如何实现 LFU 算法的\n\n\n# LFU 算法的实现\n\n首先，LFU 算法的启用，是通过设置 Redis 配置文件 redis.conf 中的 maxmemory 和 maxmemory-policy。\n\n * maxmemory 设置为 Redis 会用的最大内存容量\n * maxmemory-policy 可以设置为 allkeys-lfu 或是 volatile-lfu，表示淘汰的键值对会分别从所有键值对或是设置了过期时间的键值对中筛选\n\nLFU 算法的实现可以分成三部分内容，分别是\n\n 1. 键值对访问频率记录\n 2. 键值对访问频率初始化和更新\n 3. LFU 算法淘汰数据\n\n\n# 键值对访问频率记录\n\n每个键值对的值都对应了一个redisObject结构体，其中有一个 24 bits 的 lru 变量。\n\nlru 变量在 LRU 算法实现时，是用来记录数据的访问时间戳。因为 Redis server 每次运行时，只能将 maxmemory-policy 配置项设置为使用一种淘汰策略，所以，LRU 算法和 LFU 算法并不会同时使用。而为了节省内存开销，Redis 源码就复用了 lru 变量来记录 LFU 算法所需的访问频率信息。\n\n但是如何在LRU对访问时间的记录之上，再记录其访问频率呢？\n\n具体来说，当 lru 变量用来记录 LFU 算法的所需信息时，它会这样使用这珍贵的 24 bits\n\n * 低 8 bits：作为计数器，来记录键值对的访问次数\n * 高 16 bits：记录访问的时间戳\n\n好，了解了 LFU 算法所需的访问频率是如何记录的，接下来，我们再来看下键值对的访问频率是如何初始化和更新的。\n\n\n# 键值对访问频率的初始化与更新\n\n首先，我们要知道，LFU 算法和 LRU 算法的基本步骤，实际上是在相同的入口函数中执行的。围绕 LRU 算法的实现，我们已经了解到这些基本步骤包括数据访问信息的初始化、访问信息更新，以及实际淘汰数据。这些步骤对应的入口函数如下表所示，你也可以再去回顾下内容。\n\n了解了这些入口函数后，我们再去分析 LFU 算法的实现，就容易找到对应的函数了。\n\n对于键值对访问频率的初始化来说，当一个键值对被创建后，createObject 函数就会被调用，用来分配 redisObject 结构体的空间和设置初始化值。如果 Redis 将 maxmemory-policy 设置为 LFU 算法，那么，键值对 redisObject 结构体中的 lru 变量初始化值，会由两部分组成：\n\n * 第一部分是 lru 变量的高 16 位，是以 1 分钟为精度的 UNIX 时间戳。这是通过调用 LFUGetTimeInMinutes 函数（在 evict.c 文件中）计算得到的。\n * 第二部分是 lru 变量的低 8 位，被设置为宏定义 LFU_INIT_VAL（在 server.h 文件中），默认值为 5。\n\n你会发现，这和我刚才给你介绍的键值对访问频率记录是一致的，也就是说，当使用 LFU 算法时，lru 变量包括了键值对的访问时间戳和访问次数。以下代码也展示了这部分的执行逻辑，你可以看下。\n\nrobj *createObject(int type, void *ptr) {\n    robj *o = zmalloc(sizeof(*o));\n    o->type = type;\n    o->encoding = OBJ_ENCODING_RAW;\n    o->ptr = ptr;\n    o->refcount = 1;\n\n    /* Set the LRU to the current lruclock (minutes resolution), or\n     * alternatively the LFU counter. */\n    // 使用LFU算法时，lru变量包括以分钟为精度的UNIX时间戳和访问次数5\n    if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n        o->lru = (LFUGetTimeInMinutes()<<8) | LFU_INIT_VAL;\n    } else {\n        o->lru = LRU_CLOCK();\n    }\n    return o;\n}\n\n\n/* Return the current time in minutes, just taking the least significant\n * 16 bits. The returned time is suitable to be stored as LDT (last decrement\n * time) for the LFU implementation. */\nunsigned long LFUGetTimeInMinutes(void) {\n    return (server.unixtime/60) & 65535;\n}\n\n\n#define LFU_INIT_VAL 5\n\n\n下面，我们再来看下键值对访问频率的更新。\n\n当一个键值对被访问时，Redis 会调用 lookupKey 函数进行查找。当 maxmemory-policy 设置使用 LFU 算法时，lookupKey 函数会调用 updateLFU 函数来更新键值对的访问频率，也就是 lru 变量值，如下所示：\n\n/* Low level key lookup API, not actually called directly from commands\n * implementations that should instead rely on lookupKeyRead(),\n * lookupKeyWrite() and lookupKeyReadWithFlags(). */\nrobj *lookupKey(redisDb *db, robj *key, int flags) {\n    dictEntry *de = dictFind(db->dict,key->ptr);\n    if (de) {\n        robj *val = dictGetVal(de);\n\n        /* Update the access time for the ageing algorithm.\n         * Don't do it if we have a saving child, as this will trigger\n         * a copy on write madness. */\n        if (!hasActiveChildProcess() && !(flags & LOOKUP_NOTOUCH)){\n            // 使用LFU算法时，调用updateLFU函数更新访问频率\n            if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n                updateLFU(val);\n            } else {\n                // 使用LRU算法时，调用LRU_CLOCK\n                val->lru = LRU_CLOCK();\n            }\n        }\n        return val;\n    } else {\n        return NULL;\n    }\n}\n\n\nupdateLFU 函数是在 db.c 文件中实现的，它的执行逻辑比较明确，一共分成三步。\n\n# 第一步，衰减访问次数\n\nupdateLFU 函数首先会调用 LFUDecrAndReturn 函数（在 evict.c 文件中），对键值对的访问次数进行衰减操作，如下所示：\n\n/* Update LFU when an object is accessed.\n * Firstly, decrement the counter if the decrement time is reached.\n * Then logarithmically increment the counter, and update the access time. */\nvoid updateLFU(robj *val) {\n    // 首先，递减计数器\n    unsigned long counter = LFUDecrAndReturn(val);\n    // 然后以logN级别递增计数器，并更新访问次数。\n    counter = LFULogIncr(counter);\n    val->lru = (LFUGetTimeInMinutes()<<8) | counter;\n}\n\n\n看到这里，你可能会有疑问：访问键值对时不是要增加键值对的访问次数吗，为什么要先衰减访问次数呢？\n\n其实，这就是我在前面一开始和你介绍的，LFU 算法是根据访问频率来淘汰数据的，而不只是访问次数。访问频率需要考虑键值对的访问是多长时间段内发生的。键值对的先前访问距离当前时间越长，那么这个键值对的访问频率相应地也就会降低。\n\n我给你举个例子，假设数据 A 在时刻 T 到 T+10 分钟这段时间内，被访问了 30 次，那么，这段时间内数据 A 的访问频率可以计算为 3 次 / 分钟（30 次 /10 分钟 = 3 次 / 分钟）。\n\n紧接着，在 T+10 分钟到 T+20 分钟这段时间内，数据 A 没有再被访问，那么此时，如果我们计算数据 A 在 T 到 T+20 分钟这段时间内的访问频率，它的访问频率就会降为 1.5 次 / 分钟（30 次 /20 分钟 = 1.5 次 / 分钟）。以此类推，随着时间的推移，如果数据 A 在 T+10 分钟后一直没有新的访问，那么它的访问频率就会逐步降低。这就是所谓的访问频率衰减。\n\n因为 Redis 是使用 lru 变量中的访问次数来表示访问频率，所以在每次更新键值对的访问频率时，就会通过 LFUDecrAndReturn 函数对访问次数进行衰减。\n\n具体来说，LFUDecrAndReturn 函数会首先获取当前键值对的上一次访问时间，这是保存在 lru 变量高 16 位上的值。然后，LFUDecrAndReturn 函数会根据全局变量 server 的 lru_decay_time 成员变量的取值，来计算衰减的大小 num_period。\n\n这个计算过程会判断 lfu_decay_time 的值是否为 0。如果 lfu_decay_time 值为 0，那么衰减大小也为 0。此时，访问次数不进行衰减。\n\n否则的话，LFUDecrAndReturn 函数会调用 LFUTimeElapsed 函数（在 evict.c 文件中），计算距离键值对的上一次访问已经过去的时长。这个时长也是以 1 分钟为精度来计算的。有了距离上次访问的时长后，LFUDecrAndReturn 函数会把这个时长除以 lfu_decay_time 的值，并把结果作为访问次数的衰减大小。\n\n这里，你需要注意的是，lfu_decay_time 变量值，是由 redis.conf 文件中的配置项 lfu-decay-time 来决定的。Redis 在初始化时，会通过 initServerConfig 函数来设置 lfu_decay_time 变量的值，默认值为 1。所以，在默认情况下，访问次数的衰减大小就是等于上一次访问距离当前的分钟数。比如，假设上一次访问是 10 分钟前，那么在默认情况下，访问次数的衰减大小就等于 10。\n\n当然，如果上一次访问距离当前的分钟数，已经超过访问次数的值了，那么访问次数就会被设置为 0，这就表示键值对已经很长时间没有被访问了。\n\n下面的代码展示了 LFUDecrAndReturn 函数的执行逻辑，你可以看下。\n\n/* If the object decrement time is reached decrement the LFU counter but\n * do not update LFU fields of the object, we update the access time\n * and counter in an explicit way when the object is really accessed.\n * And we will times halve the counter according to the times of\n * elapsed time than server.lfu_decay_time.\n * Return the object frequency counter.\n *\n * This function is used in order to scan the dataset for the best object\n * to fit: as we check for the candidate, we incrementally decrement the\n * counter of the scanned objects if needed. */\nunsigned long LFUDecrAndReturn(robj *o) {\n    // 获取当前键值对的上一次访问时间，lru右移8位，相当于保留的是前面16位的时间戳\n    unsigned long ldt = o->lru >> 8;\n    // 获取当前的访问次数，相当于后8位与255做与运算，即得到计数器\n    unsigned long counter = o->lru & 255;\n    // 计算衰减大小\n    unsigned long num_periods = server.lfu_decay_time ? LFUTimeElapsed(ldt) / server.lfu_decay_time : 0;\n    // 如果衰减大小不为0\n    if (num_periods)\n        // 如果衰减大小小于当前访问次数，那么，衰减后的访问次数是当前访问次数减去衰减大小；否则，衰减后的访问次数等于0\n        counter = (num_periods > counter) ? 0 : counter - num_periods;\n    // 如果衰减大小为0，则返回原来的访问次数\n    return counter;\n}\n\n\n好了，到这里，updateLFU 函数就通过 LFUDecrAndReturn 函数，完成了键值对访问次数的衰减。紧接着，updateLFU 函数还是会基于键值对当前的这次访问，来更新它的访问次数。\n\n# 第二步，根据当前访问更新访问次数\n\n在这一步中，updateLFU 函数会调用 LFULogIncr 函数，来增加键值对的访问次数，如下所示：\n\n/* Logarithmically increment a counter. The greater is the current counter value\n * the less likely is that it gets really implemented. Saturate it at 255. */\n// 对数递增计数值\n//核心就是访问次数越大，访问次数被递增的可能性越小，最大 255，此外你可以在配置 redis.conf 中写明访问多少次递增多少。\nuint8_t LFULogIncr(uint8_t counter) {\n    // 到最大值了，不能在增加了\n    if (counter == 255) return 255;\n    //    rand()产生一个0-0x7fff的随机数,一个随机数去除以 RAND_MAX也就是Ox7FFF，也就是随机概率\n    double r = (double)rand()/RAND_MAX;\n    // 减去新对象初始化的基数值 (LFU_INIT_VAL 默认是 5)\n    double baseval = counter - LFU_INIT_VAL;\n    // baseval 如果小于零，说明这个对象快不行了，不过本次 incr 将会延长它的寿命\n    if (baseval < 0) baseval = 0;\n    // baseval * LFU 对数计数器因子 + 1保证分母大于1\n    // 当 baseval 特别大时，最大是 (255-5)，p 值会非常小，很难会走到 counter++ 这一步\n    // p 就是 counter 通往 [+1] 权力的门缝，baseval 越大，这个门缝越窄，通过就越艰难\n    double p = 1.0/(baseval*server.lfu_log_factor+1);\n    // 如果随机概率小于当前计算的访问概率，那么访问次数加1\n    if (r < p) counter++;\n    return counter;\n}\n\n\n * 第一个分支对应了当前访问次数等于最大值 255 的情况。此时，LFULogIncr 函数不再增加访问次数。\n\n * 第二个分支对应了当前访问次数小于 255 的情况。此时，LFULogIncr 函数会计算一个阈值 p，以及一个取值为 0 到 1 之间的随机概率值 r。如果概率 r 小于阈值 p，那么 LFULogIncr 函数才会将访问次数加 1。否则的话，LFULogIncr 函数会返回当前的访问次数，不做更新。\n\n从这里你可以看到，因为概率值 r 是随机定的，所以，阈值 p 的大小就决定了访问次数增加的难度。阈值 p 越小，概率值 r 小于 p 的可能性也越小，此时，访问次数也越难增加；相反，如果阈值 p 越大，概率值 r 小于 p 的可能性就越大，访问次数就越容易增加。\n\n而阈值 p 的值大小，其实是由两个因素决定的。一个是当前访问次数和宏定义 LFU_INIT_VAL 的差值 baseval，另一个是 reids.conf 文件中定义的配置项 lfu-log-factor。\n\n当计算阈值 p 时，我们是把 baseval 和 lfu-log-factor 乘积后，加上 1，然后再取其倒数。所以，baseval 或者 lfu-log-factor 越大，那么其倒数就越小，也就是阈值 p 就越小；反之，阈值 p 就越大。也就是说，这里其实就对应了两种影响因素。\n\n * baseval 的大小：这反映了当前访问次数的多少。比如，访问次数越多的键值对，它的访问次数再增加的难度就会越大；(有点类似指数退避算法)\n * lfu-log-factor 的大小：这是可以被设置的。也就是说，Redis 源码提供了让我们人为调节访问次数增加难度的方法。\n\n这样，等到 LFULogIncr 函数执行完成后，键值对的访问次数就算更新完了。\n\n# 第三步，更新 lru 变量值\n\n最后，到这一步，updateLFU 函数已经完成了键值对访问次数的更新。接着，它就会调用 LFUGetTimeInMinutes 函数，来获取当前的时间戳，并和更新后的访问次数组合，形成最新的访问频率信息，赋值给键值对的 lru 变量，如下所示：\n\nvoid updateLFU(robj *val) {\n    ...\n    val->lru = (LFUGetTimeInMinutes()<<8) | counter;\n}\n\n\n好了，到这里，你就了解了，Redis 源码在更新键值对访问频率时，对于访问次数，它是先按照上次访问距离当前的时长，来对访问次数进行衰减。然后，再按照一定概率增加访问次数。这样的设计方法，就既包含了访问的时间段对访问频率的影响，也避免了 8 bits 计数器对访问次数的影响。而对于访问时间来说，Redis 还会获取最新访问时间戳并更新到 lru 变量中\n\n那么最后，我们再来看下 Redis 是如何基于 LFU 算法淘汰数据的\n\n\n# LFU 算法淘汰数据\n\n在实现使用 LFU 算法淘汰数据时，Redis 是采用了和实现近似 LRU 算法相同的方法。也就是说，Redis 会使用一个全局数组 EvictionPoolLRU，来保存待淘汰候选键值对集合。然后，在 processCommand 函数处理每个命令时，它会调用 freeMemoryIfNeededAndSafe 函数和 freeMemoryIfNeeded 函数，来执行具体的数据淘汰流程。\n\n这个淘汰流程我在上篇文章已经给你介绍过了，你可以再去整体回顾下。这里，我也再简要总结下，也就是分成三个步骤：\n\n * 第一步，调用 getMaxmemoryState 函数计算待释放的内存空间；\n * 第二步，调用 evictionPoolPopulate 函数随机采样键值对，并插入到待淘汰集合 EvictionPoolLRU 中；\n * 第三步，遍历待淘汰集合 EvictionPoolLRU，选择实际被淘汰数据，并删除。\n\n虽然这个基本流程和 LRU 算法相同，但是你要注意，LFU 算法在淘汰数据时，在第二步的 evictionPoolPopulate 函数中，使用了不同的方法来计算每个待淘汰键值对的空闲时间\n\n具体来说，在实现 LRU 算法时，待淘汰候选键值对集合 EvictionPoolLRU 中的每个元素，都使用成员变量 idle 来记录它距离上次访问的空闲时间。\n\n而当实现 LFU 算法时，因为 LFU 算法会对访问次数进行衰减和按概率增加，所以，它是使用访问次数来近似表示访问频率的。相应的，LFU 算法其实是用 255 减去键值对的访问次数，这样来计算 EvictionPoolLRU 数组中每个元素的 idle 变量值的。而且，在计算 idle 变量值前，LFU 算法还会调用 LFUDecrAndReturn 函数，衰减一次键值对的访问次数，以便能更加准确地反映实际选择待淘汰数据时，数据的访问频率。\n\n下面的代码展示了 LFU 算法计算 idle 变量值的过程，你可以看下。\n\nif (server.maxmemory_policy & MAXMEMORY_FLAG_LRU) {\n    idle = estimateObjectIdleTime(o);\n} else if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n    idle = 255-LFUDecrAndReturn(o);\n}\n\n\n所以说，当 LFU 算法按照访问频率，计算了待淘汰键值对集合中每个元素的 idle 值后，键值对访问次数越大，它的 idle 值就越小，反之 idle 值越大。而 EvictionPoolLRU 数组中的元素，是按 idle 值从小到大来排序的。最后当 freeMemoryIfNeeded 函数按照 idle 值从大到小，遍历 EvictionPoolLRU 数组，选择实际被淘汰的键值对时，它就能选出访问次数小的键值对了，也就是把访问频率低的键值对淘汰出去。\n\n这样，Redis 就完成了按访问频率来淘汰数据的操作了。\n\n\n# 总结\n\n 1. LFU 是在 Redis 4.0 新增的淘汰策略，它涉及的巧妙之处在于，其复用了 redisObject 结构的 lru 字段，把这个字段「一分为二」，高 16 位保存最后访问时间和低 8 位保存访问次数\n 2. key 的访问次数不能只增不减，它需要根据时间间隔来做衰减，才能达到 LFU 的目的\n 3. 每次在访问一个 key 时，会**「懒惰」**更新这个 key 的访问次数：先衰减访问次数，再更新访问次数\n 4. 衰减访问次数，会根据时间间隔计算，间隔时间越久，衰减越厉害\n 5. 因为 redisObject lru 字段宽度限制，这个访问次数是有上限的（8 bit 最大值 255），所以递增访问次数时，会根据「当前」访问次数和「概率」的方式做递增，访问次数越大，递增因子越大，递增概率越低\n 6. Redis 实现的 LFU 算法也是**「近似」**LFU，是在性能和内存方面平衡的结果\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 在高频访问与低频访问的数据中，如何有效决定哪个数据应该优先淘汰？\n 2. 为什么 lfu 算法比 lru 更适合处理那些偶尔被频繁访问的数据？\n 3. lfu 算法如何在缓存淘汰中既考虑访问频率，又避免单纯的访问次数统计？\n 4. 当缓存容量不足时，redis 的 lfu 算法是如何动态调整和减少数据误判的？\n 5. 如何在 redis 中通过 lfu 算法记录访问频率，并避免访问次数无限增长？\n 6. 如何通过调整 lfu 的衰减机制来平衡频繁访问和长时间未访问的数据淘汰？\n 7. redis 是如何利用 lfu 算法实现缓存淘汰的同时，确保系统性能和内存的高效使用？\n\n\n# 前言\n\nredis 在 4.0 版本后，还引入了 lfu 算法，也就是，最不频繁使用（least frequently used，lfu）\n\n * lfu 算法在进行数据淘汰时，会把最不频繁访问的数据淘汰掉\n * 而 lru 算法是把最近最少使用的数据淘汰掉，看起来也是淘汰不频繁访问的数据。\n\nlfu 算法和 lru 算法的区别到底有哪些呢？我们在实际场景中，需要使用 lfu 算法吗？\n\n\n# 概述\n\n因为 lfu 算法是根据 数据访问的频率 来选择被淘汰数据的，所以 lfu 算法会 记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。\n\n不过，访问次数和访问频率还不能完全等同。\n\n访问频率是指在一定时间内的访问次数，也就是说，在计算访问频率时，我们不仅需要记录访问次数，还要记录这些访问是在多长时间内执行的。否则，如果只记录访问次数的话，就缺少了时间维度的信息，进而就无法按照频率来淘汰数据了\n\n> 我来给你举个例子，假设数据 a 在 15 分钟内访问了 15 次，数据 b 在 5 分钟内访问了 10 次。如果只是按访问次数来统计的话，数据 a 的访问次数大于数据 b，所以淘汰数据时会优先淘汰数据 b。不过，如果按照访问频率来统计的话，数据 a 的访问频率是 1 分钟访问 1 次，而数据 b 的访问频率是 1 分钟访问 2 次，所以按访问频率淘汰数据的话，数据 a 应该被淘汰掉。 所以说，当要实现 lfu 算法时，我们需要能统计到数据的访问频率，而不是简单地记录数据访问次数就行。\n\n那么接下来，我们就来学习下 redis 是如何实现 lfu 算法的\n\n\n# lfu 算法的实现\n\n首先，lfu 算法的启用，是通过设置 redis 配置文件 redis.conf 中的 maxmemory 和 maxmemory-policy。\n\n * maxmemory 设置为 redis 会用的最大内存容量\n * maxmemory-policy 可以设置为 allkeys-lfu 或是 volatile-lfu，表示淘汰的键值对会分别从所有键值对或是设置了过期时间的键值对中筛选\n\nlfu 算法的实现可以分成三部分内容，分别是\n\n 1. 键值对访问频率记录\n 2. 键值对访问频率初始化和更新\n 3. lfu 算法淘汰数据\n\n\n# 键值对访问频率记录\n\n每个键值对的值都对应了一个redisobject结构体，其中有一个 24 bits 的 lru 变量。\n\nlru 变量在 lru 算法实现时，是用来记录数据的访问时间戳。因为 redis server 每次运行时，只能将 maxmemory-policy 配置项设置为使用一种淘汰策略，所以，lru 算法和 lfu 算法并不会同时使用。而为了节省内存开销，redis 源码就复用了 lru 变量来记录 lfu 算法所需的访问频率信息。\n\n但是如何在lru对访问时间的记录之上，再记录其访问频率呢？\n\n具体来说，当 lru 变量用来记录 lfu 算法的所需信息时，它会这样使用这珍贵的 24 bits\n\n * 低 8 bits：作为计数器，来记录键值对的访问次数\n * 高 16 bits：记录访问的时间戳\n\n好，了解了 lfu 算法所需的访问频率是如何记录的，接下来，我们再来看下键值对的访问频率是如何初始化和更新的。\n\n\n# 键值对访问频率的初始化与更新\n\n首先，我们要知道，lfu 算法和 lru 算法的基本步骤，实际上是在相同的入口函数中执行的。围绕 lru 算法的实现，我们已经了解到这些基本步骤包括数据访问信息的初始化、访问信息更新，以及实际淘汰数据。这些步骤对应的入口函数如下表所示，你也可以再去回顾下内容。\n\n了解了这些入口函数后，我们再去分析 lfu 算法的实现，就容易找到对应的函数了。\n\n对于键值对访问频率的初始化来说，当一个键值对被创建后，createobject 函数就会被调用，用来分配 redisobject 结构体的空间和设置初始化值。如果 redis 将 maxmemory-policy 设置为 lfu 算法，那么，键值对 redisobject 结构体中的 lru 变量初始化值，会由两部分组成：\n\n * 第一部分是 lru 变量的高 16 位，是以 1 分钟为精度的 unix 时间戳。这是通过调用 lfugettimeinminutes 函数（在 evict.c 文件中）计算得到的。\n * 第二部分是 lru 变量的低 8 位，被设置为宏定义 lfu_init_val（在 server.h 文件中），默认值为 5。\n\n你会发现，这和我刚才给你介绍的键值对访问频率记录是一致的，也就是说，当使用 lfu 算法时，lru 变量包括了键值对的访问时间戳和访问次数。以下代码也展示了这部分的执行逻辑，你可以看下。\n\nrobj *createobject(int type, void *ptr) {\n    robj *o = zmalloc(sizeof(*o));\n    o->type = type;\n    o->encoding = obj_encoding_raw;\n    o->ptr = ptr;\n    o->refcount = 1;\n\n    /* set the lru to the current lruclock (minutes resolution), or\n     * alternatively the lfu counter. */\n    // 使用lfu算法时，lru变量包括以分钟为精度的unix时间戳和访问次数5\n    if (server.maxmemory_policy & maxmemory_flag_lfu) {\n        o->lru = (lfugettimeinminutes()<<8) | lfu_init_val;\n    } else {\n        o->lru = lru_clock();\n    }\n    return o;\n}\n\n\n/* return the current time in minutes, just taking the least significant\n * 16 bits. the returned time is suitable to be stored as ldt (last decrement\n * time) for the lfu implementation. */\nunsigned long lfugettimeinminutes(void) {\n    return (server.unixtime/60) & 65535;\n}\n\n\n#define lfu_init_val 5\n\n\n下面，我们再来看下键值对访问频率的更新。\n\n当一个键值对被访问时，redis 会调用 lookupkey 函数进行查找。当 maxmemory-policy 设置使用 lfu 算法时，lookupkey 函数会调用 updatelfu 函数来更新键值对的访问频率，也就是 lru 变量值，如下所示：\n\n/* low level key lookup api, not actually called directly from commands\n * implementations that should instead rely on lookupkeyread(),\n * lookupkeywrite() and lookupkeyreadwithflags(). */\nrobj *lookupkey(redisdb *db, robj *key, int flags) {\n    dictentry *de = dictfind(db->dict,key->ptr);\n    if (de) {\n        robj *val = dictgetval(de);\n\n        /* update the access time for the ageing algorithm.\n         * don't do it if we have a saving child, as this will trigger\n         * a copy on write madness. */\n        if (!hasactivechildprocess() && !(flags & lookup_notouch)){\n            // 使用lfu算法时，调用updatelfu函数更新访问频率\n            if (server.maxmemory_policy & maxmemory_flag_lfu) {\n                updatelfu(val);\n            } else {\n                // 使用lru算法时，调用lru_clock\n                val->lru = lru_clock();\n            }\n        }\n        return val;\n    } else {\n        return null;\n    }\n}\n\n\nupdatelfu 函数是在 db.c 文件中实现的，它的执行逻辑比较明确，一共分成三步。\n\n# 第一步，衰减访问次数\n\nupdatelfu 函数首先会调用 lfudecrandreturn 函数（在 evict.c 文件中），对键值对的访问次数进行衰减操作，如下所示：\n\n/* update lfu when an object is accessed.\n * firstly, decrement the counter if the decrement time is reached.\n * then logarithmically increment the counter, and update the access time. */\nvoid updatelfu(robj *val) {\n    // 首先，递减计数器\n    unsigned long counter = lfudecrandreturn(val);\n    // 然后以logn级别递增计数器，并更新访问次数。\n    counter = lfulogincr(counter);\n    val->lru = (lfugettimeinminutes()<<8) | counter;\n}\n\n\n看到这里，你可能会有疑问：访问键值对时不是要增加键值对的访问次数吗，为什么要先衰减访问次数呢？\n\n其实，这就是我在前面一开始和你介绍的，lfu 算法是根据访问频率来淘汰数据的，而不只是访问次数。访问频率需要考虑键值对的访问是多长时间段内发生的。键值对的先前访问距离当前时间越长，那么这个键值对的访问频率相应地也就会降低。\n\n我给你举个例子，假设数据 a 在时刻 t 到 t+10 分钟这段时间内，被访问了 30 次，那么，这段时间内数据 a 的访问频率可以计算为 3 次 / 分钟（30 次 /10 分钟 = 3 次 / 分钟）。\n\n紧接着，在 t+10 分钟到 t+20 分钟这段时间内，数据 a 没有再被访问，那么此时，如果我们计算数据 a 在 t 到 t+20 分钟这段时间内的访问频率，它的访问频率就会降为 1.5 次 / 分钟（30 次 /20 分钟 = 1.5 次 / 分钟）。以此类推，随着时间的推移，如果数据 a 在 t+10 分钟后一直没有新的访问，那么它的访问频率就会逐步降低。这就是所谓的访问频率衰减。\n\n因为 redis 是使用 lru 变量中的访问次数来表示访问频率，所以在每次更新键值对的访问频率时，就会通过 lfudecrandreturn 函数对访问次数进行衰减。\n\n具体来说，lfudecrandreturn 函数会首先获取当前键值对的上一次访问时间，这是保存在 lru 变量高 16 位上的值。然后，lfudecrandreturn 函数会根据全局变量 server 的 lru_decay_time 成员变量的取值，来计算衰减的大小 num_period。\n\n这个计算过程会判断 lfu_decay_time 的值是否为 0。如果 lfu_decay_time 值为 0，那么衰减大小也为 0。此时，访问次数不进行衰减。\n\n否则的话，lfudecrandreturn 函数会调用 lfutimeelapsed 函数（在 evict.c 文件中），计算距离键值对的上一次访问已经过去的时长。这个时长也是以 1 分钟为精度来计算的。有了距离上次访问的时长后，lfudecrandreturn 函数会把这个时长除以 lfu_decay_time 的值，并把结果作为访问次数的衰减大小。\n\n这里，你需要注意的是，lfu_decay_time 变量值，是由 redis.conf 文件中的配置项 lfu-decay-time 来决定的。redis 在初始化时，会通过 initserverconfig 函数来设置 lfu_decay_time 变量的值，默认值为 1。所以，在默认情况下，访问次数的衰减大小就是等于上一次访问距离当前的分钟数。比如，假设上一次访问是 10 分钟前，那么在默认情况下，访问次数的衰减大小就等于 10。\n\n当然，如果上一次访问距离当前的分钟数，已经超过访问次数的值了，那么访问次数就会被设置为 0，这就表示键值对已经很长时间没有被访问了。\n\n下面的代码展示了 lfudecrandreturn 函数的执行逻辑，你可以看下。\n\n/* if the object decrement time is reached decrement the lfu counter but\n * do not update lfu fields of the object, we update the access time\n * and counter in an explicit way when the object is really accessed.\n * and we will times halve the counter according to the times of\n * elapsed time than server.lfu_decay_time.\n * return the object frequency counter.\n *\n * this function is used in order to scan the dataset for the best object\n * to fit: as we check for the candidate, we incrementally decrement the\n * counter of the scanned objects if needed. */\nunsigned long lfudecrandreturn(robj *o) {\n    // 获取当前键值对的上一次访问时间，lru右移8位，相当于保留的是前面16位的时间戳\n    unsigned long ldt = o->lru >> 8;\n    // 获取当前的访问次数，相当于后8位与255做与运算，即得到计数器\n    unsigned long counter = o->lru & 255;\n    // 计算衰减大小\n    unsigned long num_periods = server.lfu_decay_time ? lfutimeelapsed(ldt) / server.lfu_decay_time : 0;\n    // 如果衰减大小不为0\n    if (num_periods)\n        // 如果衰减大小小于当前访问次数，那么，衰减后的访问次数是当前访问次数减去衰减大小；否则，衰减后的访问次数等于0\n        counter = (num_periods > counter) ? 0 : counter - num_periods;\n    // 如果衰减大小为0，则返回原来的访问次数\n    return counter;\n}\n\n\n好了，到这里，updatelfu 函数就通过 lfudecrandreturn 函数，完成了键值对访问次数的衰减。紧接着，updatelfu 函数还是会基于键值对当前的这次访问，来更新它的访问次数。\n\n# 第二步，根据当前访问更新访问次数\n\n在这一步中，updatelfu 函数会调用 lfulogincr 函数，来增加键值对的访问次数，如下所示：\n\n/* logarithmically increment a counter. the greater is the current counter value\n * the less likely is that it gets really implemented. saturate it at 255. */\n// 对数递增计数值\n//核心就是访问次数越大，访问次数被递增的可能性越小，最大 255，此外你可以在配置 redis.conf 中写明访问多少次递增多少。\nuint8_t lfulogincr(uint8_t counter) {\n    // 到最大值了，不能在增加了\n    if (counter == 255) return 255;\n    //    rand()产生一个0-0x7fff的随机数,一个随机数去除以 rand_max也就是ox7fff，也就是随机概率\n    double r = (double)rand()/rand_max;\n    // 减去新对象初始化的基数值 (lfu_init_val 默认是 5)\n    double baseval = counter - lfu_init_val;\n    // baseval 如果小于零，说明这个对象快不行了，不过本次 incr 将会延长它的寿命\n    if (baseval < 0) baseval = 0;\n    // baseval * lfu 对数计数器因子 + 1保证分母大于1\n    // 当 baseval 特别大时，最大是 (255-5)，p 值会非常小，很难会走到 counter++ 这一步\n    // p 就是 counter 通往 [+1] 权力的门缝，baseval 越大，这个门缝越窄，通过就越艰难\n    double p = 1.0/(baseval*server.lfu_log_factor+1);\n    // 如果随机概率小于当前计算的访问概率，那么访问次数加1\n    if (r < p) counter++;\n    return counter;\n}\n\n\n * 第一个分支对应了当前访问次数等于最大值 255 的情况。此时，lfulogincr 函数不再增加访问次数。\n\n * 第二个分支对应了当前访问次数小于 255 的情况。此时，lfulogincr 函数会计算一个阈值 p，以及一个取值为 0 到 1 之间的随机概率值 r。如果概率 r 小于阈值 p，那么 lfulogincr 函数才会将访问次数加 1。否则的话，lfulogincr 函数会返回当前的访问次数，不做更新。\n\n从这里你可以看到，因为概率值 r 是随机定的，所以，阈值 p 的大小就决定了访问次数增加的难度。阈值 p 越小，概率值 r 小于 p 的可能性也越小，此时，访问次数也越难增加；相反，如果阈值 p 越大，概率值 r 小于 p 的可能性就越大，访问次数就越容易增加。\n\n而阈值 p 的值大小，其实是由两个因素决定的。一个是当前访问次数和宏定义 lfu_init_val 的差值 baseval，另一个是 reids.conf 文件中定义的配置项 lfu-log-factor。\n\n当计算阈值 p 时，我们是把 baseval 和 lfu-log-factor 乘积后，加上 1，然后再取其倒数。所以，baseval 或者 lfu-log-factor 越大，那么其倒数就越小，也就是阈值 p 就越小；反之，阈值 p 就越大。也就是说，这里其实就对应了两种影响因素。\n\n * baseval 的大小：这反映了当前访问次数的多少。比如，访问次数越多的键值对，它的访问次数再增加的难度就会越大；(有点类似指数退避算法)\n * lfu-log-factor 的大小：这是可以被设置的。也就是说，redis 源码提供了让我们人为调节访问次数增加难度的方法。\n\n这样，等到 lfulogincr 函数执行完成后，键值对的访问次数就算更新完了。\n\n# 第三步，更新 lru 变量值\n\n最后，到这一步，updatelfu 函数已经完成了键值对访问次数的更新。接着，它就会调用 lfugettimeinminutes 函数，来获取当前的时间戳，并和更新后的访问次数组合，形成最新的访问频率信息，赋值给键值对的 lru 变量，如下所示：\n\nvoid updatelfu(robj *val) {\n    ...\n    val->lru = (lfugettimeinminutes()<<8) | counter;\n}\n\n\n好了，到这里，你就了解了，redis 源码在更新键值对访问频率时，对于访问次数，它是先按照上次访问距离当前的时长，来对访问次数进行衰减。然后，再按照一定概率增加访问次数。这样的设计方法，就既包含了访问的时间段对访问频率的影响，也避免了 8 bits 计数器对访问次数的影响。而对于访问时间来说，redis 还会获取最新访问时间戳并更新到 lru 变量中\n\n那么最后，我们再来看下 redis 是如何基于 lfu 算法淘汰数据的\n\n\n# lfu 算法淘汰数据\n\n在实现使用 lfu 算法淘汰数据时，redis 是采用了和实现近似 lru 算法相同的方法。也就是说，redis 会使用一个全局数组 evictionpoollru，来保存待淘汰候选键值对集合。然后，在 processcommand 函数处理每个命令时，它会调用 freememoryifneededandsafe 函数和 freememoryifneeded 函数，来执行具体的数据淘汰流程。\n\n这个淘汰流程我在上篇文章已经给你介绍过了，你可以再去整体回顾下。这里，我也再简要总结下，也就是分成三个步骤：\n\n * 第一步，调用 getmaxmemorystate 函数计算待释放的内存空间；\n * 第二步，调用 evictionpoolpopulate 函数随机采样键值对，并插入到待淘汰集合 evictionpoollru 中；\n * 第三步，遍历待淘汰集合 evictionpoollru，选择实际被淘汰数据，并删除。\n\n虽然这个基本流程和 lru 算法相同，但是你要注意，lfu 算法在淘汰数据时，在第二步的 evictionpoolpopulate 函数中，使用了不同的方法来计算每个待淘汰键值对的空闲时间\n\n具体来说，在实现 lru 算法时，待淘汰候选键值对集合 evictionpoollru 中的每个元素，都使用成员变量 idle 来记录它距离上次访问的空闲时间。\n\n而当实现 lfu 算法时，因为 lfu 算法会对访问次数进行衰减和按概率增加，所以，它是使用访问次数来近似表示访问频率的。相应的，lfu 算法其实是用 255 减去键值对的访问次数，这样来计算 evictionpoollru 数组中每个元素的 idle 变量值的。而且，在计算 idle 变量值前，lfu 算法还会调用 lfudecrandreturn 函数，衰减一次键值对的访问次数，以便能更加准确地反映实际选择待淘汰数据时，数据的访问频率。\n\n下面的代码展示了 lfu 算法计算 idle 变量值的过程，你可以看下。\n\nif (server.maxmemory_policy & maxmemory_flag_lru) {\n    idle = estimateobjectidletime(o);\n} else if (server.maxmemory_policy & maxmemory_flag_lfu) {\n    idle = 255-lfudecrandreturn(o);\n}\n\n\n所以说，当 lfu 算法按照访问频率，计算了待淘汰键值对集合中每个元素的 idle 值后，键值对访问次数越大，它的 idle 值就越小，反之 idle 值越大。而 evictionpoollru 数组中的元素，是按 idle 值从小到大来排序的。最后当 freememoryifneeded 函数按照 idle 值从大到小，遍历 evictionpoollru 数组，选择实际被淘汰的键值对时，它就能选出访问次数小的键值对了，也就是把访问频率低的键值对淘汰出去。\n\n这样，redis 就完成了按访问频率来淘汰数据的操作了。\n\n\n# 总结\n\n 1. lfu 是在 redis 4.0 新增的淘汰策略，它涉及的巧妙之处在于，其复用了 redisobject 结构的 lru 字段，把这个字段「一分为二」，高 16 位保存最后访问时间和低 8 位保存访问次数\n 2. key 的访问次数不能只增不减，它需要根据时间间隔来做衰减，才能达到 lfu 的目的\n 3. 每次在访问一个 key 时，会**「懒惰」**更新这个 key 的访问次数：先衰减访问次数，再更新访问次数\n 4. 衰减访问次数，会根据时间间隔计算，间隔时间越久，衰减越厉害\n 5. 因为 redisobject lru 字段宽度限制，这个访问次数是有上限的（8 bit 最大值 255），所以递增访问次数时，会根据「当前」访问次数和「概率」的方式做递增，访问次数越大，递增因子越大，递增概率越低\n 6. redis 实现的 lfu 算法也是**「近似」**lfu，是在性能和内存方面平衡的结果\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Redis 过期策略",frontmatter:{title:"Redis 过期策略",date:"2024-09-16T03:23:25.000Z",permalink:"/pages/f44fbe/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E5%9B%9B%E3%80%81%E6%94%AF%E7%BA%BF%E4%BB%BB%E5%8A%A1/13.Redis%20%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5.html",relativePath:"Redis 系统设计/04.四、支线任务/13.Redis 过期策略.md",key:"v-0b6a31ab",path:"/pages/f44fbe/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:242},{level:2,title:"过期键初始化结构",slug:"过期键初始化结构",normalizedTitle:"过期键初始化结构",charIndex:302},{level:2,title:"过期策略",slug:"过期策略",normalizedTitle:"过期策略",charIndex:398},{level:3,title:"惰性删除",slug:"惰性删除",normalizedTitle:"惰性删除",charIndex:53},{level:3,title:"定期删除",slug:"定期删除",normalizedTitle:"定期删除",charIndex:58},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6752},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:6940}],headersStr:"前言 过期键初始化结构 过期策略 惰性删除 定期删除 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. Redis 内存接近上限时，过期键未及时清理会带来什么影响？\n 2. 惰性删除和定期删除能确保所有过期键都被清理吗？\n 3. 为什么 Redis 不选定时删除，而用惰性和定期删除的组合？\n 4. 哪些场景下，过期键清理不及时会导致性能问题？如何避免？\n 5. 定期删除随机抽取过期键，这样能防止过期键堆积吗？\n 6. 如果某个键从未被访问，惰性删除是否会让它一直占用内存？\n 7. Redis 高负载时，过期键清理频率会下降吗？如何优化？\n\n\n# 前言\n\n在 Redis 中我们可以给一些元素设置过期时间，那当它过期之后 Redis 是如何处理这些过期键呢？\n\n\n# 过期键初始化结构\n\nRedis 之所以能知道那些键值过期，是因为在 Redis 中维护了一个字典，存储了所有设置了过期时间的键值，我们称之为过期字典。\n\n过期键判断流程如下图所示：\n\n![内存过期策略-过期键判断流程.png](https://learn.lianglianglee.com/专栏/Redis 核心原理与实战/assets/3bf71ae0-5de7-11ea-9e57-957b6467a3fc)\n\n过期键存储在 redisDb 结构中，源代码在 src/server.h 文件中：\n\n/* Redis database representation. There are multiple databases identified\n * by integers from 0 (the default database) up to the max configured\n * database. The database number is the 'id' field in the structure. */\ntypedef struct redisDb {\n    dict *dict;                 /* 数据库键空间，存放着所有的键值对 */\n    dict *expires;              /* 键的过期时间 */\n    dict *blocking_keys;        /* Keys with clients waiting for data (BLPOP)*/\n    dict *ready_keys;           /* Blocked keys that received a PUSH */\n    dict *watched_keys;         /* WATCHED keys for MULTI/EXEC CAS */\n    int id;                     /* Database ID */\n    long long avg_ttl;          /* Average TTL, just for stats */\n    list *defrag_later;         /* List of key names to attempt to defrag one by one, gradually. */\n} redisDb;\n\n\n过期键数据结构如下图所示：\n\n\n\n\n# 过期策略\n\nRedis 会删除已过期的键值，以此来减少 Redis 的空间占用，但因为 Redis 本身是单线的，如果因为删除操作而影响主业务的执行就得不偿失了，为此 Redis 需要制定多个（过期）删除策略来保证糟糕的事情不会发生。\n\n常见的过期策略有以下三种：\n\n * 定时删除：在设置键值过期时间时，创建一个定时事件，当过期时间到达时，由事件处理器自动执行键的删除操作\n * 惰性删除：不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null\n * 定期删除：每隔一段时间检查一次数据库，随机删除一些过期键。\n\nRedis 中采用了 惰性删除+定期删除 策略\n\n> 定时删除虽然 可以保证内存可以被尽快地释放，但是，在 Redis 高负载的情况下或有大量过期键需要同时处理时，会造成 Redis 服务器卡顿，影响主业务执行\n\n\n# 惰性删除\n\n不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null。\n\n * **优点：**因为每次访问时，才会判断过期键，所以此策略只会使用很少的系统资源。\n * **缺点：**系统占用空间删除不及时，导致空间利用率降低，造成了一定的空间浪费。\n\n惰性删除的源码位于 src/db.c 文件的 expireIfNeeded 方法中，源码如下：\n\nint expireIfNeeded(redisDb *db, robj *key) {\n    // 判断键是否过期\n    if (!keyIsExpired(db,key)) return 0;\n    if (server.masterhost != NULL) return 1;\n    /* 删除过期键 */\n    // 增加过期键个数\n    server.stat_expiredkeys++;\n    // 传播键过期的消息\n    propagateExpire(db,key,server.lazyfree_lazy_expire);\n    notifyKeyspaceEvent(NOTIFY_EXPIRED,\n        \"expired\",key,db->id);\n    // server.lazyfree_lazy_expire 为 1 表示异步删除（懒空间释放），反之同步删除\n    return server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) :\n                                         dbSyncDelete(db,key);\n}\n// 判断键是否过期\nint keyIsExpired(redisDb *db, robj *key) {\n    mstime_t when = getExpire(db,key);\n    if (when < 0) return 0; /* No expire for this key */\n    /* Don't expire anything while loading. It will be done later. */\n    if (server.loading) return 0;\n    mstime_t now = server.lua_caller ? server.lua_time_start : mstime();\n    return now > when;\n}\n// 获取键的过期时间\nlong long getExpire(redisDb *db, robj *key) {\n    dictEntry *de;\n    /* No expire? return ASAP */\n    if (dictSize(db->expires) == 0 ||\n       (de = dictFind(db->expires,key->ptr)) == NULL) return -1;\n    /* The entry was found in the expire dict, this means it should also\n     * be present in the main dict (safety check). */\n    serverAssertWithInfo(NULL,key,dictFind(db->dict,key->ptr) != NULL);\n    return dictGetSignedIntegerVal(de);\n}\n\n\n所有对数据库的读写命令在执行之前，都会调用 expireIfNeeded 方法判断键值是否过期，过期则会从数据库中删除，反之则不做任何处理。\n\n惰性删除执行流程，如下图所示：\n\n\n\n\n# 定期删除\n\n每隔一段时间检查一次数据库，随机删除一些过期键。\n\nRedis 默认每秒进行 10 次过期扫描，此配置可通过 Redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10。\n\n注意\n\nRedis 每次扫描并不是遍历过期字典中的所有键，而是采用随机抽取判断并删除过期键的形式执行的。\n\n定期删除流程如下\n\n 1. 从过期字典中随机取出 20 个键；\n 2. 删除这 20 个键中过期的键；\n 3. 如果过期 key 的比例超过 25%，重复步骤 1。\n\n同时为了保证过期扫描不会出现循环过度，导致线程卡死现象，算法还增加了扫描时间的上限，默认不会超过 25ms\n\n\n\n * **优点：**通过限制删除操作的时长和频率，来减少删除操作对 Redis 主业务的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。\n * **缺点：**内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少。\n\n定期删除的核心源码在 src/expire.c 文件下的 activeExpireCycle 方法中，源码如下：\n\nvoid activeExpireCycle(int type) {\n    static unsigned int current_db = 0; /* 上次定期删除遍历到的数据库ID */\n    static int timelimit_exit = 0;      /* Time limit hit in previous call? */\n    static long long last_fast_cycle = 0; /* 上一次执行快速定期删除的时间点 */\n    int j, iteration = 0;\n    int dbs_per_call = CRON_DBS_PER_CALL; // 每次定期删除，遍历的数据库的数量\n    long long start = ustime(), timelimit, elapsed;\n    if (clientsArePaused()) return;\n    if (type == ACTIVE_EXPIRE_CYCLE_FAST) {\n        if (!timelimit_exit) return;\n        // ACTIVE_EXPIRE_CYCLE_FAST_DURATION 是快速定期删除的执行时长\n        if (start < last_fast_cycle + ACTIVE_EXPIRE_CYCLE_FAST_DURATION*2) return;\n        last_fast_cycle = start;\n    }\n    if (dbs_per_call > server.dbnum || timelimit_exit)\n        dbs_per_call = server.dbnum;\n    // 慢速定期删除的执行时长\n    timelimit = 1000000*ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC/server.hz/100;\n    timelimit_exit = 0;\n    if (timelimit <= 0) timelimit = 1;\n    if (type == ACTIVE_EXPIRE_CYCLE_FAST)\n        timelimit = ACTIVE_EXPIRE_CYCLE_FAST_DURATION; /* 删除操作的执行时长 */\n    long total_sampled = 0;\n    long total_expired = 0;\n    for (j = 0; j < dbs_per_call && timelimit_exit == 0; j++) {\n        int expired;\n        redisDb *db = server.db+(current_db % server.dbnum);\n        current_db++;\n        do {\n            // .......\n            expired = 0;\n            ttl_sum = 0;\n            ttl_samples = 0;\n            // 每个数据库中检查的键的数量\n            if (num > ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP)\n                num = ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP;\n            // 从数据库中随机选取 num 个键进行检查\n            while (num--) {\n                dictEntry *de;\n                long long ttl;\n                if ((de = dictGetRandomKey(db->expires)) == NULL) break;\n                ttl = dictGetSignedInteger\n                // 过期检查，并对过期键进行删除\n                if (activeExpireCycleTryExpire(db,de,now)) expired++;\n                if (ttl > 0) {\n                    /* We want the average TTL of keys yet not expired. */\n                    ttl_sum += ttl;\n                    ttl_samples++;\n                }\n                total_sampled++;\n            }\n            total_expired += expired;\n            if (ttl_samples) {\n                long long avg_ttl = ttl_sum/ttl_samples;\n                if (db->avg_ttl == 0) db->avg_ttl = avg_ttl;\n                db->avg_ttl = (db->avg_ttl/50)*49 + (avg_ttl/50);\n            }\n            if ((iteration & 0xf) == 0) { /* check once every 16 iterations. */\n                elapsed = ustime()-start;\n                if (elapsed > timelimit) {\n                    timelimit_exit = 1;\n                    server.stat_expired_time_cap_reached_count++;\n                    break;\n                }\n            }\n            /* 每次检查只删除 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP/4 个过期键 */\n        } while (expired > ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP/4);\n    }\n    // .......\n}\n\n\n\n# 总结\n\n 1. Redis 是通过设置过期字典的形式来判断过期键的\n 2. Redis 采用的是惰性删除和定期删除的形式删除过期键的\n 3. Redis 的定期删除策略并不会遍历删除每个过期键，而是采用随机抽取的方式删除过期键\n 4. 为了保证过期扫描不影响 Redis 主业务，Redis 的定期删除策略中还提供了最大执行时间，以保证 Redis 正常并高效地运行\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. redis 内存接近上限时，过期键未及时清理会带来什么影响？\n 2. 惰性删除和定期删除能确保所有过期键都被清理吗？\n 3. 为什么 redis 不选定时删除，而用惰性和定期删除的组合？\n 4. 哪些场景下，过期键清理不及时会导致性能问题？如何避免？\n 5. 定期删除随机抽取过期键，这样能防止过期键堆积吗？\n 6. 如果某个键从未被访问，惰性删除是否会让它一直占用内存？\n 7. redis 高负载时，过期键清理频率会下降吗？如何优化？\n\n\n# 前言\n\n在 redis 中我们可以给一些元素设置过期时间，那当它过期之后 redis 是如何处理这些过期键呢？\n\n\n# 过期键初始化结构\n\nredis 之所以能知道那些键值过期，是因为在 redis 中维护了一个字典，存储了所有设置了过期时间的键值，我们称之为过期字典。\n\n过期键判断流程如下图所示：\n\n![内存过期策略-过期键判断流程.png](https://learn.lianglianglee.com/专栏/redis 核心原理与实战/assets/3bf71ae0-5de7-11ea-9e57-957b6467a3fc)\n\n过期键存储在 redisdb 结构中，源代码在 src/server.h 文件中：\n\n/* redis database representation. there are multiple databases identified\n * by integers from 0 (the default database) up to the max configured\n * database. the database number is the 'id' field in the structure. */\ntypedef struct redisdb {\n    dict *dict;                 /* 数据库键空间，存放着所有的键值对 */\n    dict *expires;              /* 键的过期时间 */\n    dict *blocking_keys;        /* keys with clients waiting for data (blpop)*/\n    dict *ready_keys;           /* blocked keys that received a push */\n    dict *watched_keys;         /* watched keys for multi/exec cas */\n    int id;                     /* database id */\n    long long avg_ttl;          /* average ttl, just for stats */\n    list *defrag_later;         /* list of key names to attempt to defrag one by one, gradually. */\n} redisdb;\n\n\n过期键数据结构如下图所示：\n\n\n\n\n# 过期策略\n\nredis 会删除已过期的键值，以此来减少 redis 的空间占用，但因为 redis 本身是单线的，如果因为删除操作而影响主业务的执行就得不偿失了，为此 redis 需要制定多个（过期）删除策略来保证糟糕的事情不会发生。\n\n常见的过期策略有以下三种：\n\n * 定时删除：在设置键值过期时间时，创建一个定时事件，当过期时间到达时，由事件处理器自动执行键的删除操作\n * 惰性删除：不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null\n * 定期删除：每隔一段时间检查一次数据库，随机删除一些过期键。\n\nredis 中采用了 惰性删除+定期删除 策略\n\n> 定时删除虽然 可以保证内存可以被尽快地释放，但是，在 redis 高负载的情况下或有大量过期键需要同时处理时，会造成 redis 服务器卡顿，影响主业务执行\n\n\n# 惰性删除\n\n不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null。\n\n * **优点：**因为每次访问时，才会判断过期键，所以此策略只会使用很少的系统资源。\n * **缺点：**系统占用空间删除不及时，导致空间利用率降低，造成了一定的空间浪费。\n\n惰性删除的源码位于 src/db.c 文件的 expireifneeded 方法中，源码如下：\n\nint expireifneeded(redisdb *db, robj *key) {\n    // 判断键是否过期\n    if (!keyisexpired(db,key)) return 0;\n    if (server.masterhost != null) return 1;\n    /* 删除过期键 */\n    // 增加过期键个数\n    server.stat_expiredkeys++;\n    // 传播键过期的消息\n    propagateexpire(db,key,server.lazyfree_lazy_expire);\n    notifykeyspaceevent(notify_expired,\n        \"expired\",key,db->id);\n    // server.lazyfree_lazy_expire 为 1 表示异步删除（懒空间释放），反之同步删除\n    return server.lazyfree_lazy_expire ? dbasyncdelete(db,key) :\n                                         dbsyncdelete(db,key);\n}\n// 判断键是否过期\nint keyisexpired(redisdb *db, robj *key) {\n    mstime_t when = getexpire(db,key);\n    if (when < 0) return 0; /* no expire for this key */\n    /* don't expire anything while loading. it will be done later. */\n    if (server.loading) return 0;\n    mstime_t now = server.lua_caller ? server.lua_time_start : mstime();\n    return now > when;\n}\n// 获取键的过期时间\nlong long getexpire(redisdb *db, robj *key) {\n    dictentry *de;\n    /* no expire? return asap */\n    if (dictsize(db->expires) == 0 ||\n       (de = dictfind(db->expires,key->ptr)) == null) return -1;\n    /* the entry was found in the expire dict, this means it should also\n     * be present in the main dict (safety check). */\n    serverassertwithinfo(null,key,dictfind(db->dict,key->ptr) != null);\n    return dictgetsignedintegerval(de);\n}\n\n\n所有对数据库的读写命令在执行之前，都会调用 expireifneeded 方法判断键值是否过期，过期则会从数据库中删除，反之则不做任何处理。\n\n惰性删除执行流程，如下图所示：\n\n\n\n\n# 定期删除\n\n每隔一段时间检查一次数据库，随机删除一些过期键。\n\nredis 默认每秒进行 10 次过期扫描，此配置可通过 redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10。\n\n注意\n\nredis 每次扫描并不是遍历过期字典中的所有键，而是采用随机抽取判断并删除过期键的形式执行的。\n\n定期删除流程如下\n\n 1. 从过期字典中随机取出 20 个键；\n 2. 删除这 20 个键中过期的键；\n 3. 如果过期 key 的比例超过 25%，重复步骤 1。\n\n同时为了保证过期扫描不会出现循环过度，导致线程卡死现象，算法还增加了扫描时间的上限，默认不会超过 25ms\n\n\n\n * **优点：**通过限制删除操作的时长和频率，来减少删除操作对 redis 主业务的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。\n * **缺点：**内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少。\n\n定期删除的核心源码在 src/expire.c 文件下的 activeexpirecycle 方法中，源码如下：\n\nvoid activeexpirecycle(int type) {\n    static unsigned int current_db = 0; /* 上次定期删除遍历到的数据库id */\n    static int timelimit_exit = 0;      /* time limit hit in previous call? */\n    static long long last_fast_cycle = 0; /* 上一次执行快速定期删除的时间点 */\n    int j, iteration = 0;\n    int dbs_per_call = cron_dbs_per_call; // 每次定期删除，遍历的数据库的数量\n    long long start = ustime(), timelimit, elapsed;\n    if (clientsarepaused()) return;\n    if (type == active_expire_cycle_fast) {\n        if (!timelimit_exit) return;\n        // active_expire_cycle_fast_duration 是快速定期删除的执行时长\n        if (start < last_fast_cycle + active_expire_cycle_fast_duration*2) return;\n        last_fast_cycle = start;\n    }\n    if (dbs_per_call > server.dbnum || timelimit_exit)\n        dbs_per_call = server.dbnum;\n    // 慢速定期删除的执行时长\n    timelimit = 1000000*active_expire_cycle_slow_time_perc/server.hz/100;\n    timelimit_exit = 0;\n    if (timelimit <= 0) timelimit = 1;\n    if (type == active_expire_cycle_fast)\n        timelimit = active_expire_cycle_fast_duration; /* 删除操作的执行时长 */\n    long total_sampled = 0;\n    long total_expired = 0;\n    for (j = 0; j < dbs_per_call && timelimit_exit == 0; j++) {\n        int expired;\n        redisdb *db = server.db+(current_db % server.dbnum);\n        current_db++;\n        do {\n            // .......\n            expired = 0;\n            ttl_sum = 0;\n            ttl_samples = 0;\n            // 每个数据库中检查的键的数量\n            if (num > active_expire_cycle_lookups_per_loop)\n                num = active_expire_cycle_lookups_per_loop;\n            // 从数据库中随机选取 num 个键进行检查\n            while (num--) {\n                dictentry *de;\n                long long ttl;\n                if ((de = dictgetrandomkey(db->expires)) == null) break;\n                ttl = dictgetsignedinteger\n                // 过期检查，并对过期键进行删除\n                if (activeexpirecycletryexpire(db,de,now)) expired++;\n                if (ttl > 0) {\n                    /* we want the average ttl of keys yet not expired. */\n                    ttl_sum += ttl;\n                    ttl_samples++;\n                }\n                total_sampled++;\n            }\n            total_expired += expired;\n            if (ttl_samples) {\n                long long avg_ttl = ttl_sum/ttl_samples;\n                if (db->avg_ttl == 0) db->avg_ttl = avg_ttl;\n                db->avg_ttl = (db->avg_ttl/50)*49 + (avg_ttl/50);\n            }\n            if ((iteration & 0xf) == 0) { /* check once every 16 iterations. */\n                elapsed = ustime()-start;\n                if (elapsed > timelimit) {\n                    timelimit_exit = 1;\n                    server.stat_expired_time_cap_reached_count++;\n                    break;\n                }\n            }\n            /* 每次检查只删除 active_expire_cycle_lookups_per_loop/4 个过期键 */\n        } while (expired > active_expire_cycle_lookups_per_loop/4);\n    }\n    // .......\n}\n\n\n\n# 总结\n\n 1. redis 是通过设置过期字典的形式来判断过期键的\n 2. redis 采用的是惰性删除和定期删除的形式删除过期键的\n 3. redis 的定期删除策略并不会遍历删除每个过期键，而是采用随机抽取的方式删除过期键\n 4. 为了保证过期扫描不影响 redis 主业务，redis 的定期删除策略中还提供了最大执行时间，以保证 redis 正常并高效地运行\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"RDB 持久化",frontmatter:{title:"RDB 持久化",date:"2024-09-16T03:23:41.000Z",permalink:"/pages/9b17a6/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E5%9B%9B%E3%80%81%E6%94%AF%E7%BA%BF%E4%BB%BB%E5%8A%A1/15.RDB%20%E6%8C%81%E4%B9%85%E5%8C%96.html",relativePath:"Redis 系统设计/04.四、支线任务/15.RDB 持久化.md",key:"v-7411b637",path:"/pages/9b17a6/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:285},{level:2,title:"RDB 创建的入口函数和触发时机",slug:"rdb-创建的入口函数和触发时机",normalizedTitle:"rdb 创建的入口函数和触发时机",charIndex:369},{level:2,title:"RDB 文件是如何生成的",slug:"rdb-文件是如何生成的",normalizedTitle:"rdb 文件是如何生成的",charIndex:3241},{level:3,title:"生成文件头",slug:"生成文件头",normalizedTitle:"生成文件头",charIndex:4870},{level:3,title:"生成文件数据部分",slug:"生成文件数据部分",normalizedTitle:"生成文件数据部分",charIndex:8125},{level:3,title:"生成文件尾",slug:"生成文件尾",normalizedTitle:"生成文件尾",charIndex:11525},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:11973},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:12733}],headersStr:"前言 RDB 创建的入口函数和触发时机 RDB 文件是如何生成的 生成文件头 生成文件数据部分 生成文件尾 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. Redis 是如何通过 RDB 文件来实现数据持久化的？其背后有哪些关键过程？\n 2. 除了 save 和 bgsave 命令，Redis 在什么场景下还会创建 RDB 文件？\n 3. RDB 文件中，为什么需要为不同内容设定操作码？这些操作码如何帮助解析数据？\n 4. rdbSaveRio 函数生成 RDB 文件时，如何保证数据的完整性和效率？\n 5. RDB 文件为什么要自包含所有键值对的类型、长度和数据？这对恢复数据有什么好处？\n 6. 如何通过解析 RDB 文件来发现 Redis 中的大 key，从而优化内存？\n\n\n# 前言\n\n我们来到了一个新的模块「可靠性保证模块」\n\n我们就先从RDB文件的生成开始学起。下面呢，我先带你来了解下RDB创建的入口函数，以及调用这些函数的地方。\n\n\n# RDB 创建的入口函数和触发时机\n\nRedis 源码中用来创建 RDB 文件的函数有三个，它们都是在rdb.c文件中实现的，接下来我就带你具体了解下。\n\n * rdbSave 函数\n\n这是 Redis server 在本地磁盘创建 RDB 文件的入口函数。它对应了 Redis 的 save 命令，会在 save 命令的实现函数 saveCommand（在 rdb.c 文件中）中被调用。而 rdbSave 函数最终会调用 rdbSaveRio 函数（在 rdb.c 文件中）来实际创建 RDB 文件。rdbSaveRio 函数的执行逻辑就体现了 RDB 文件的格式和生成过程，我稍后向你介绍\n\n * rdbSaveBackground 函数\n\n这是 Redis server 使用后台子进程方式，在本地磁盘创建 RDB 文件的入口函数。它对应了 Redis 的 bgsave 命令，会在 bgsave 命令的实现函数 bgsaveCommand（在 rdb.c 文件中）中被调用。这个函数会调用 fork 创建一个子进程，让子进程调用 rdbSave 函数来继续创建 RDB 文件，而父进程，也就是主线程本身可以继续处理客户端请求。\n\n下面的代码展示了 rdbSaveBackground 函数创建子进程的过程，你可以看下\n\nint rdbSaveBackground(char *filename, rdbSaveInfo *rsi) {\n    ...\n    if ((childpid = fork()) == 0) {  //子进程的代码执行分支\n       ...\n       retval = rdbSave(filename,rsi);  //调用rdbSave函数创建RDB文件\n       ...\n       exitFromChild((retval == C_OK) ? 0 : 1);  //子进程退出\n    } else {\n       ...  //父进程代码执行分支\n    }\n}\n\n\n * rdbSaveToSlavesSockets 函数\n\n这是 Redis server 在采用不落盘方式传输 RDB 文件进行主从复制时，创建 RDB 文件的入口函数。它会被 startBgsaveForReplication 函数调用（在replication.c文件中）。而 startBgsaveForReplication 函数会被 replication.c 文件中的 syncCommand 函数和 replicationCron 函数调用，这对应了 Redis server 执行主从复制命令，以及周期性检测主从复制状态时触发 RDB 生成。\n\n和 rdbSaveBackground 函数类似，rdbSaveToSlavesSockets 函数也是通过 fork 创建子进程，让子进程生成 RDB。不过和 rdbSaveBackground 函数不同的是，rdbSaveToSlavesSockets 函数是通过网络以字节流的形式，直接发送 RDB 文件的二进制数据给从节点。\n\n而为了让从节点能够识别用来同步数据的 RDB 内容，rdbSaveToSlavesSockets 函数调用 rdbSaveRioWithEOFMark 函数（在 rdb.c 文件中），在 RDB 二进制数据的前后加上了标识字符串，如下图所示：\n\n\n\n以下代码也展示了rdbSaveRioWithEOFMark函数的基本执行逻辑。你可以看到，它除了写入前后标识字符串之外，还是会调用rdbSaveRio函数实际生成RDB内容。\n\nint rdbSaveRioWithEOFMark(rio *rdb, int *error, rdbSaveInfo *rsi) {\n    ...\n    getRandomHexChars(eofmark,RDB_EOF_MARK_SIZE); //随机生成40字节的16进制字符串，保存在eofmark中，宏定义RDB_EOF_MARK_SIZE的值为40\n    if (rioWrite(rdb,"$EOF:",5) == 0) goto werr;  //写入$EOF\n    if (rioWrite(rdb,eofmark,RDB_EOF_MARK_SIZE) == 0) goto werr; //写入40字节的16进制字符串eofmark\n    if (rioWrite(rdb,"\\r\\n",2) == 0) goto werr; //写入\\r\\n\n    if (rdbSaveRio(rdb,error,RDB_SAVE_NONE,rsi) == C_ERR) goto werr; //生成RDB内容\n    if (rioWrite(rdb,eofmark,RDB_EOF_MARK_SIZE) == 0) goto werr; //再次写入40字节的16进制字符串eofmark\n    ...\n}\n\n\n好了，了解了 RDB 文件创建的三个入口函数后，我们也看到了，RDB 文件创建的三个时机，分别是 save 命令执行、bgsave 命令执行以及主从复制。那么，除了这三个时机外，在 Redis 源码中，还有哪些地方会触发 RDB 文件创建呢？\n\n实际上，因为 rdbSaveToSlavesSockets 函数只会在主从复制时调用，所以，我们只要通过在 Redis 源码中查找 rdbSave、rdbSaveBackground 这两个函数，就可以了解触发 RDB 文件创建的其他时机。\n\n那么经过查找，我们可以发现在 Redis 源码中，rdbSave 还会在 flushallCommand 函数（在db.c文件中）、prepareForShutdown 函数（在server.c文件中）中被调用。这也就是说，Redis 在执行 flushall 命令以及正常关闭时，会创建 RDB 文件。\n\n对于 rdbSaveBackground 函数来说，它除了在执行 bgsave 命令时被调用，当主从复制采用落盘文件方式传输 RDB 时，它也会被 startBgsaveForReplication 函数调用。此外，Redis server 运行时的周期性执行函数 serverCron（在server.c文件中），也会调用 rdbSaveBackground 函数来创建 RDB 文件。\n\n为了便于你掌握 RDB 文件创建的整体情况，我画了下面这张图，展示了 Redis 源码中创建 RDB 文件的函数调用关系，你可以看下。\n\n\n\n好了，到这里，你可以看到，实际最终生成 RDB 文件的函数是 rdbSaveRio。所以接下来，我们就来看看 rdbSaveRio 函数的执行过程。同时，我还会给你介绍 RDB 文件的格式是如何组织的\n\n\n# RDB 文件是如何生成的\n\n不过在了解 rdbSaveRio 函数具体是如何生成 RDB 文件之前，你还需要先了解下 RDB 文件的基本组成部分。这样，你就可以按照 RDB 文件的组成部分，依次了解 rdbSaveRio 函数的执行逻辑了。\n\n那么，一个 RDB 文件主要是由三个部分组成的。\n\n * 文件头：这部分内容保存了 Redis 的魔数、RDB 版本、Redis 版本、RDB 文件创建时间、键值对占用的内存大小等信息。\n * 文件数据部分：这部分保存了 Redis 数据库实际的所有键值对。\n * 文件尾：这部分保存了 RDB 文件的结束标识符，以及整个文件的校验值。这个校验值用来在 Redis server 加载 RDB 文件后，检查文件是否被篡改过。\n\n下图就展示了 RDB 文件的组成，你可以看下。\n\n\n\n好，接下来，我们就来看看 rdbSaveRio 函数是如何生成 RDB 文件中的每一部分的。这里，为了方便你理解 RDB 文件格式以及文件内容，你可以先按照如下步骤准备一个 RDB 文件。\n\n第一步，在你电脑上 Redis 的目录下，启动一个用来测试的 Redis server，可以执行如下命令：\n\n好，接下来，我们就来看看 rdbSaveRio 函数是如何生成 RDB 文件中的每一部分的。这里，为了方便你理解 RDB 文件格式以及文件内容，你可以先按照如下步骤准备一个 RDB 文件。\n\n第一步，在你电脑上 Redis 的目录下，启动一个用来测试的 Redis server，可以执行如下命令：\n\n./redis-server\n\n\n第二步，执行 flushall 命令，清空当前的数据库：\n\n./redis-cli flushall\n\n\n第三步，使用 redis-cli 登录刚启动的 Redis server，执行 set 命令插入一个 String 类型的键值对，再执行 hmset 命令插入一个 Hash 类型的键值对。执行 save 命令，将当前数据库内容保存到 RDB 文件中。这个过程如下所示：\n\n127.0.0.1:6379>set hello redis\nOK\n127.0.0.1:6379>hmset userinfo uid 1 name zs age 32\nOK\n127.0.0.1:6379> save\nOK\n\n\n好了，到这里，你就可以在刚才执行 redis-cli 命令的目录下，找见刚生成的 RDB 文件，文件名应该是 dump.rdb。\n\n不过，因为 RDB 文件实际是一个二进制数据组成的文件，所以如果你使用一般的文本编辑软件，比如 Linux 系统上的 Vim，在打开 RDB 文件时，你会看到文件中都是乱码。所以这里，我给你提供一个小工具，如果你想查看 RDB 文件中二进制数据和对应的 ASCII 字符，你可以使用 Linux 上的 od 命令，这个命令可以用不同进制的方式展示数据，并显示对应的 ASCII 字符。\n\n比如，你可以执行如下的命令，读取 dump.rdb 文件，并用十六进制展示文件内容，同时文件中每个字节对应的 ASCII 字符也会被对应显示出来。\n\nod -A x -t x1c -v dump.rdb\n\n\n以下代码展示的就是我用 od 命令，查看刚才生成的 dump.rdb 文件后，输出的从文件头开始的部分内容。你可以看到这四行结果中，第一和第三行是用十六进制显示的 dump.rdb 文件的字节内容，这里每两个十六进制数对应了一个字节。而第二和第四行是 od 命令生成的每个字节所对应的 ASCII 字符。\n\n\n\n这也就是说，在刚才生成的 RDB 文件中，如果想要转换成 ASCII 字符，它的文件头内容其实就已经包含了 REDIS 的字符串和一些数字，而这正是 RDB 文件头包含的内容。\n\n那么下面，我们就来看看 RDB 文件的文件头是如何生成的。\n\n\n# 生成文件头\n\n就像刚才给你介绍的，RDB 文件头的内容首先是魔数，这对应记录了 RDB 文件的版本。在 rdbSaveRio 函数中，魔数是通过 snprintf 函数生成的，它的具体内容是字符串“REDIS”，再加上 RDB 版本的宏定义 RDB_VERSION（在rdb.h文件中，值为 9）。然后，rdbSaveRio 函数会调用 rdbWriteRaw 函数（在 rdb.c 文件中），将魔数写入 RDB 文件，如下所示：\n\nsnprintf(magic,sizeof(magic),"REDIS%04d",RDB_VERSION);  //生成魔数magic\nif (rdbWriteRaw(rdb,magic,9) == -1) goto werr;  //将magic写入RDB文件\n\n\n刚才用来写入魔数的 rdbWriteRaw 函数，它实际会调用 rioWrite 函数（在 rdb.h 文件中）来完成写入。而 rioWrite 函数是 RDB 文件内容的最终写入函数，它负责根据要写入数据的长度，把待写入缓冲区中的内容写入 RDB。这里，你需要注意的是，RDB 文件生成过程中，会有不同的函数负责写入不同部分的内容，不过这些函数最终都还是调用 rioWrite 函数，来完成数据的实际写入的。\n\n好了，当在 RDB 文件头中写入魔数后，rdbSaveRio 函数紧接着会调用 rdbSaveInfoAuxFields 函数，将和 Redis server 相关的一些属性信息写入 RDB 文件头，如下所示：\n\nif (rdbSaveInfoAuxFields(rdb,flags,rsi) == -1) goto werr; //写入属性信息\n\n\nrdbSaveInfoAuxFields 函数是在 rdb.c 文件中实现的，它会使用键值对的形式，在 RDB 文件头中记录 Redis server 的属性信息。下表中列出了 RDB 文件头记录的一些主要信息，以及它们对应的键和值，你可以看下。\n\n\n\n那么，当属性值为字符串时，rdbSaveInfoAuxFields 函数会调用 rdbSaveAuxFieldStrStr 函数写入属性信息；而当属性值为整数时，rdbSaveInfoAuxFields 函数会调用 rdbSaveAuxFieldStrInt 函数写入属性信息，如下所示：\n\nif (rdbSaveAuxFieldStrStr(rdb,"redis-ver",REDIS_VERSION) == -1) return -1;\nif (rdbSaveAuxFieldStrInt(rdb,"redis-bits",redis_bits) == -1) return -1;\nif (rdbSaveAuxFieldStrInt(rdb,"ctime",time(NULL)) == -1) return -1;\nif (rdbSaveAuxFieldStrInt(rdb,"used-mem",zmalloc_used_memory()) == -1) return -1;\n\n\n这里，无论是 rdbSaveAuxFieldStrStr 函数还是 rdbSaveAuxFieldStrInt 函数，它们都会调用 rdbSaveAuxField 函数来写入属性值。rdbSaveAuxField 函数是在 rdb.c 文件中实现的，它会分三步来完成一个属性信息的写入。\n\n第一步，它调用 rdbSaveType 函数写入一个操作码。这个操作码的目的，是用来在 RDB 文件中标识接下来的内容是什么。当写入属性信息时，这个操作码对应了宏定义 RDB_OPCODE_AUX（在 rdb.h 文件中），值为 250，对应的十六进制值为 FA。这样一来，就方便我们解析 RDB 文件了。比如，在读取 RDB 文件时，如果程序读取到 FA 这个字节，那么，这就表明接下来的内容是一个属性信息。\n\n这里，你需要注意的是，RDB 文件使用了多个操作码，来标识文件中的不同内容。它们都是在 rdb.h 文件中定义的，下面的代码中展示了部分操作码，你可以看下。\n\n#define RDB_OPCODE_IDLE       248   //标识LRU空闲时间\n#define RDB_OPCODE_FREQ       249   //标识LFU访问频率信息\n#define RDB_OPCODE_AUX        250   //标识RDB文件头的属性信息\n#define RDB_OPCODE_EXPIRETIME_MS 252    //标识以毫秒记录的过期时间\n#define RDB_OPCODE_SELECTDB   254   //标识文件中后续键值对所属的数据库编号\n#define RDB_OPCODE_EOF        255   //标识RDB文件结束，用在文件尾\n\n\n第二步，rdbSaveAuxField 函数调用 rdbSaveRawString 函数（在 rdb.c 文件中）写入属性信息的键，而键通常是一个字符串。rdbSaveRawString 函数是用来写入字符串的通用函数，它会先记录字符串长度，然后再记录实际字符串，如下图所示。这个长度信息是为了解析 RDB 文件时，程序可以基于它知道当前读取的字符串应该读取多少个字节。\n\n\n\n不过，为了节省 RDB 文件消耗的空间，如果字符串中记录的实际是一个整数，rdbSaveRawString 函数还会调用 rdbTryIntegerEncoding 函数（在 rdb.c 文件中），尝试用紧凑结构对字符串进行编码。具体做法你可以进一步阅读 rdbTryIntegerEncoding 函数。\n\n下图展示了 rdbSaveRawString 函数的基本执行逻辑，你可以看下。其中，它调用 rdbSaveLen 函数写入字符串长度，调用 rdbWriteRaw 函数写入实际数据。\n\n\n\n第三步，rdbSaveAuxField 函数就需要写入属性信息的值了。因为属性信息的值通常也是字符串，所以和第二步写入属性信息的键类似，rdbSaveAuxField 函数会调用 rdbSaveRawString 函数来写入属性信息的值。\n\n下面的代码展示了 rdbSaveAuxField 函数的执行整体过程，你可以再回顾下。\n\nssize_t rdbSaveAuxField(rio *rdb, void *key, size_t keylen, void *val, size_t vallen) {\n    ssize_t ret, len = 0;\n    //写入操作码\n    if ((ret = rdbSaveType(rdb,RDB_OPCODE_AUX)) == -1) return -1;\n    len += ret;\n    //写入属性信息中的键\n    if ((ret = rdbSaveRawString(rdb,key,keylen)) == -1) return -1;\n    len += ret;\n    //写入属性信息中的值\n    if ((ret = rdbSaveRawString(rdb,val,vallen)) == -1) return -1;\n    len += ret;\n    return len;\n}\n\n\n到这里，RDB 文件头的内容已经写完了。我把刚才创建的 RDB 文件头的部分内容，画在了下图当中，并且标识了十六进制对应的 ASCII 字符以及一些关键信息，你可以结合图例来理解刚才介绍的代码。\n\n\n\n这样接下来，rdbSaveRio 函数就要开始写入实际的键值对了，这也是文件中实际记录数据的部分。下面，我们就来具体看下。\n\n\n# 生成文件数据部分\n\n因为 Redis server 上的键值对可能被保存在不同的数据库中，所以，rdbSaveRio 函数会执行一个循环，遍历每个数据库，将其中的键值对写入 RDB 文件。\n\n在这个循环流程中，rdbSaveRio 函数会先将 **SELECTDB 操作码 **和对应的数据库编号写入 RDB 文件，这样一来，程序在解析 RDB 文件时，就可以知道接下来的键值对是属于哪个数据库的了。这个过程如下所示：\n\n...\nfor (j = 0; j < server.dbnum; j++) { //循环遍历每一个数据库\n    ...\n    //写入SELECTDB操作码\n    if (rdbSaveType(rdb,RDB_OPCODE_SELECTDB) == -1) goto werr;\n    if (rdbSaveLen(rdb,j) == -1) goto werr;  //写入当前数据库编号j\n    ...\n}\n\n\n下图展示了刚才我创建的 RDB 文件中 SELECTDB 操作码的信息，你可以看到，数据库编号为 0。\n\n\n\n紧接着，rdbSaveRio 函数会写入 RESIZEDB 操作码，用来标识全局哈希表和过期 key 哈希表中键值对数量的记录，这个过程的执行代码如下所示：\n\n...\ndb_size = dictSize(db->dict);   //获取全局哈希表大小\nexpires_size = dictSize(db->expires);  //获取过期key哈希表的大小\nif (rdbSaveType(rdb,RDB_OPCODE_RESIZEDB) == -1) goto werr;  //写入RESIZEDB操作码\nif (rdbSaveLen(rdb,db_size) == -1) goto werr;  //写入全局哈希表大小\nif (rdbSaveLen(rdb,expires_size) == -1) goto werr; //写入过期key哈希表大小\n...\n\n\n我也把刚才创建的 RDB 文件中，RESIZEDB 操作码的内容画在了下图中，你可以看下。\n\n\n\n你可以看到，在 RESIZEDB 操作码后，紧接着记录的是全局哈希表中的键值对，它的数量是 2，然后是过期 key 哈希表中的键值对，其数量为 0。我们刚才在生成 RDB 文件前，只插入了两个键值对，所以，RDB 文件中记录的信息和我们刚才的操作结果是一致的。\n\n好了，在记录完这些信息后，rdbSaveRio 函数会接着执行一个循环流程，在该流程中，rdbSaveRio 函数会取出当前数据库中的每一个键值对，并调用 rdbSaveKeyValuePair 函数（在 rdb.c 文件中），将它写入 RDB 文件。这个基本的循环流程如下所示：\n\n while((de = dictNext(di)) != NULL) {  //读取数据库中的每一个键值对\n    sds keystr = dictGetKey(de);  //获取键值对的key\n    robj key, *o = dictGetVal(de);  //获取键值对的value\n    initStaticStringObject(key,keystr);  //为key生成String对象\n    expire = getExpire(db,&key);  //获取键值对的过期时间\n    //把key和value写入RDB文件\n    if (rdbSaveKeyValuePair(rdb,&key,o,expire) == -1) goto werr;\n    ...\n}\n\n\n这里，rdbSaveKeyValuePair 函数主要是负责将键值对实际写入 RDB 文件。它会先将键值对的过期时间、LRU 空闲时间或是 LFU 访问频率写入 RDB 文件。在写入这些信息时，rdbSaveKeyValuePair 函数都会先调用 rdbSaveType 函数，写入标识这些信息的操作码，你可以看下下面的代码。\n\nif (expiretime != -1) {\n    //写入过期时间操作码标识\n   if (rdbSaveType(rdb,RDB_OPCODE_EXPIRETIME_MS) == -1) return -1;\n   if (rdbSaveMillisecondTime(rdb,expiretime) == -1) return -1;\n}\nif (savelru) {\n   ...\n   //写入LRU空闲时间操作码标识\n   if (rdbSaveType(rdb,RDB_OPCODE_IDLE) == -1) return -1;\n   if (rdbSaveLen(rdb,idletime) == -1) return -1;\n}\nif (savelfu) {\n   ...\n   //写入LFU访问频率操作码标识\n   if (rdbSaveType(rdb,RDB_OPCODE_FREQ) == -1) return -1;\n   if (rdbWriteRaw(rdb,buf,1) == -1) return -1;\n}\n\n\n好了，到这里，rdbSaveKeyValuePair 函数就要开始实际写入键值对了。为了便于解析 RDB 文件时恢复键值对，rdbSaveKeyValuePair 函数会先调用 rdbSaveObjectType 函数，写入键值对的类型标识；然后调用 rdbSaveStringObject 写入键值对的 key；最后，它会调用 rdbSaveObject 函数写入键值对的 value。这个过程如下所示，这几个函数都是在 rdb.c 文件中实现的：\n\nif (rdbSaveObjectType(rdb,val) == -1) return -1;  //写入键值对的类型标识\nif (rdbSaveStringObject(rdb,key) == -1) return -1; //写入键值对的key\nif (rdbSaveObject(rdb,val,key) == -1) return -1; //写入键值对的value\n\n\n这里，你需要注意的是，rdbSaveObjectType 函数会根据键值对的 value 类型，来决定写入到 RDB 中的键值对类型标识，这些类型标识在 rdb.h 文件中有对应的宏定义。比如，我在刚才创建 RDB 文件前，写入的键值对分别是 String 类型和 Hash 类型，而 Hash 类型因为它包含的元素个数不多，所以默认采用 ziplist 数据结构来保存。这两个类型标识对应的数值如下所示：\n\n#define RDB_TYPE_STRING   0\n#define RDB_TYPE_HASH_ZIPLIST  13\n\n\n我把刚才写入的 String 类型键值对“hello”“redis”在 RDB 文件中对应的记录内容，画在了下图中，你可以看下。\n\n\n\n你可以看到，这个键值对的开头类型标识就是 0，和刚才介绍的 RDB_TYPE_STRING 宏定义的值是一致的。而紧接着的 key 和 value，它们都会先记录长度信息，然后才记录实际内容。\n\n因为键值对的 key 都是 String 类型，所以 rdbSaveKeyValuePair 函数就用 rdbSaveStringObject 函数来写入了。而键值对的 value 有不同的类型，所以，rdbSaveObject 函数会根据 value 的类型，执行不同的代码分支，将 value 底层数据结构中的内容写入 RDB。\n\n好了，到这里，我们就了解了 rdbSaveKeyValuePair 函数是如何将键值对写入 RDB 文件中的了。在这个过程中，除了键值对类型、键值对的 key 和 value 会被记录以外，键值对的过期时间、LRU 空闲时间或是 LFU 访问频率也都会记录到 RDB 文件中。这就生成 RDB 文件的数据部分。\n\n最后，我们再来看下 RDB 文件尾的生成。\n\n\n# 生成文件尾\n\n当所有键值对都写入 RDB 文件后，**rdbSaveRio 函数 **就可以写入文件尾内容了。文件尾的内容比较简单，主要包括两个部分，一个是 RDB 文件结束的操作码标识，另一个是 RDB 文件的校验值。\n\nrdbSaveRio 函数会先调用 rdbSaveType 函数，写入文件结束操作码 RDB_OPCODE_EOF，然后调用 rioWrite 写入检验值，如下所示：\n\n...\n//写入结束操作码\nif (rdbSaveType(rdb,RDB_OPCODE_EOF) == -1) goto werr;\n\n//写入校验值\ncksum = rdb->cksum;\nmemrev64ifbe(&cksum);\nif (rioWrite(rdb,&cksum,8) == 0) goto werr;\n...\n\n\n下图展示了我刚才生成的 RDB 文件的文件尾，你可以看下。\n\n\n\n这样，我们也就整体了解了 RDB 文件从文件头、文件数据部分再到文件尾的整个生成过程了。\n\n\n# 总结\n\n 1. 创建 RDB 文件的三个入口函数分别是 rdbSave、rdbSaveBackground、rdbSaveToSlavesSockets，它们在 Redis 源码中被调用的地方，也就是触发 RDB 文件生成的时机\n\n 2. RDB 文件的基本组成，并且也要结合 rdbSaveRio 函数的执行流程，来掌握 RDB 文件头、文件数据部分和文件尾这三个部分的生成。我总结了以下两点，方便你对 RDB 文件结构和内容有个整体把握：\n    \n    * RDB 文件使用多种操作码来标识 Redis 不同的属性信息，以及使用类型码来标识不同 value 类型；\n    \n    * RDB 文件内容是自包含的，也就是说，无论是属性信息还是键值对，RDB 文件都会按照类型、长度、实际数据的格式来记录，这样方便程序对 RDB 文件的解析。\n\n 3. RDB 文件包含了 Redis 数据库某一时刻的所有键值对，以及这些键值对的类型、大小、过期时间等信息。\n\n当你了解了 RDB 文件的格式和生成方法后，其实你就可以根据需求，开发解析 RDB 文件的程序或是加载 RDB 文件的程序了\n\n比如，你可以在 RDB 文件中查找内存空间消耗大的键值对，也就是在优化 Redis 性能时通常需要查找的 bigkey；你也可以分析不同类型键值对的数量、空间占用等分布情况，来了解业务数据的特点；你还可以自行加载 RDB 文件，用于测试或故障排查。\n\n当然，这里我也再给你一个小提示，就是在你实际开发 RDB 文件分析工具之前，可以看下redis-rdb-tools这个工具，它能够帮助你分析 RDB 文件中的内容。而如果它还不能满足你的定制化需求，你就可以用上这节课学习的内容，来开发自己的 RDB 分析工具了\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. redis 是如何通过 rdb 文件来实现数据持久化的？其背后有哪些关键过程？\n 2. 除了 save 和 bgsave 命令，redis 在什么场景下还会创建 rdb 文件？\n 3. rdb 文件中，为什么需要为不同内容设定操作码？这些操作码如何帮助解析数据？\n 4. rdbsaverio 函数生成 rdb 文件时，如何保证数据的完整性和效率？\n 5. rdb 文件为什么要自包含所有键值对的类型、长度和数据？这对恢复数据有什么好处？\n 6. 如何通过解析 rdb 文件来发现 redis 中的大 key，从而优化内存？\n\n\n# 前言\n\n我们来到了一个新的模块「可靠性保证模块」\n\n我们就先从rdb文件的生成开始学起。下面呢，我先带你来了解下rdb创建的入口函数，以及调用这些函数的地方。\n\n\n# rdb 创建的入口函数和触发时机\n\nredis 源码中用来创建 rdb 文件的函数有三个，它们都是在rdb.c文件中实现的，接下来我就带你具体了解下。\n\n * rdbsave 函数\n\n这是 redis server 在本地磁盘创建 rdb 文件的入口函数。它对应了 redis 的 save 命令，会在 save 命令的实现函数 savecommand（在 rdb.c 文件中）中被调用。而 rdbsave 函数最终会调用 rdbsaverio 函数（在 rdb.c 文件中）来实际创建 rdb 文件。rdbsaverio 函数的执行逻辑就体现了 rdb 文件的格式和生成过程，我稍后向你介绍\n\n * rdbsavebackground 函数\n\n这是 redis server 使用后台子进程方式，在本地磁盘创建 rdb 文件的入口函数。它对应了 redis 的 bgsave 命令，会在 bgsave 命令的实现函数 bgsavecommand（在 rdb.c 文件中）中被调用。这个函数会调用 fork 创建一个子进程，让子进程调用 rdbsave 函数来继续创建 rdb 文件，而父进程，也就是主线程本身可以继续处理客户端请求。\n\n下面的代码展示了 rdbsavebackground 函数创建子进程的过程，你可以看下\n\nint rdbsavebackground(char *filename, rdbsaveinfo *rsi) {\n    ...\n    if ((childpid = fork()) == 0) {  //子进程的代码执行分支\n       ...\n       retval = rdbsave(filename,rsi);  //调用rdbsave函数创建rdb文件\n       ...\n       exitfromchild((retval == c_ok) ? 0 : 1);  //子进程退出\n    } else {\n       ...  //父进程代码执行分支\n    }\n}\n\n\n * rdbsavetoslavessockets 函数\n\n这是 redis server 在采用不落盘方式传输 rdb 文件进行主从复制时，创建 rdb 文件的入口函数。它会被 startbgsaveforreplication 函数调用（在replication.c文件中）。而 startbgsaveforreplication 函数会被 replication.c 文件中的 synccommand 函数和 replicationcron 函数调用，这对应了 redis server 执行主从复制命令，以及周期性检测主从复制状态时触发 rdb 生成。\n\n和 rdbsavebackground 函数类似，rdbsavetoslavessockets 函数也是通过 fork 创建子进程，让子进程生成 rdb。不过和 rdbsavebackground 函数不同的是，rdbsavetoslavessockets 函数是通过网络以字节流的形式，直接发送 rdb 文件的二进制数据给从节点。\n\n而为了让从节点能够识别用来同步数据的 rdb 内容，rdbsavetoslavessockets 函数调用 rdbsaveriowitheofmark 函数（在 rdb.c 文件中），在 rdb 二进制数据的前后加上了标识字符串，如下图所示：\n\n\n\n以下代码也展示了rdbsaveriowitheofmark函数的基本执行逻辑。你可以看到，它除了写入前后标识字符串之外，还是会调用rdbsaverio函数实际生成rdb内容。\n\nint rdbsaveriowitheofmark(rio *rdb, int *error, rdbsaveinfo *rsi) {\n    ...\n    getrandomhexchars(eofmark,rdb_eof_mark_size); //随机生成40字节的16进制字符串，保存在eofmark中，宏定义rdb_eof_mark_size的值为40\n    if (riowrite(rdb,"$eof:",5) == 0) goto werr;  //写入$eof\n    if (riowrite(rdb,eofmark,rdb_eof_mark_size) == 0) goto werr; //写入40字节的16进制字符串eofmark\n    if (riowrite(rdb,"\\r\\n",2) == 0) goto werr; //写入\\r\\n\n    if (rdbsaverio(rdb,error,rdb_save_none,rsi) == c_err) goto werr; //生成rdb内容\n    if (riowrite(rdb,eofmark,rdb_eof_mark_size) == 0) goto werr; //再次写入40字节的16进制字符串eofmark\n    ...\n}\n\n\n好了，了解了 rdb 文件创建的三个入口函数后，我们也看到了，rdb 文件创建的三个时机，分别是 save 命令执行、bgsave 命令执行以及主从复制。那么，除了这三个时机外，在 redis 源码中，还有哪些地方会触发 rdb 文件创建呢？\n\n实际上，因为 rdbsavetoslavessockets 函数只会在主从复制时调用，所以，我们只要通过在 redis 源码中查找 rdbsave、rdbsavebackground 这两个函数，就可以了解触发 rdb 文件创建的其他时机。\n\n那么经过查找，我们可以发现在 redis 源码中，rdbsave 还会在 flushallcommand 函数（在db.c文件中）、prepareforshutdown 函数（在server.c文件中）中被调用。这也就是说，redis 在执行 flushall 命令以及正常关闭时，会创建 rdb 文件。\n\n对于 rdbsavebackground 函数来说，它除了在执行 bgsave 命令时被调用，当主从复制采用落盘文件方式传输 rdb 时，它也会被 startbgsaveforreplication 函数调用。此外，redis server 运行时的周期性执行函数 servercron（在server.c文件中），也会调用 rdbsavebackground 函数来创建 rdb 文件。\n\n为了便于你掌握 rdb 文件创建的整体情况，我画了下面这张图，展示了 redis 源码中创建 rdb 文件的函数调用关系，你可以看下。\n\n\n\n好了，到这里，你可以看到，实际最终生成 rdb 文件的函数是 rdbsaverio。所以接下来，我们就来看看 rdbsaverio 函数的执行过程。同时，我还会给你介绍 rdb 文件的格式是如何组织的\n\n\n# rdb 文件是如何生成的\n\n不过在了解 rdbsaverio 函数具体是如何生成 rdb 文件之前，你还需要先了解下 rdb 文件的基本组成部分。这样，你就可以按照 rdb 文件的组成部分，依次了解 rdbsaverio 函数的执行逻辑了。\n\n那么，一个 rdb 文件主要是由三个部分组成的。\n\n * 文件头：这部分内容保存了 redis 的魔数、rdb 版本、redis 版本、rdb 文件创建时间、键值对占用的内存大小等信息。\n * 文件数据部分：这部分保存了 redis 数据库实际的所有键值对。\n * 文件尾：这部分保存了 rdb 文件的结束标识符，以及整个文件的校验值。这个校验值用来在 redis server 加载 rdb 文件后，检查文件是否被篡改过。\n\n下图就展示了 rdb 文件的组成，你可以看下。\n\n\n\n好，接下来，我们就来看看 rdbsaverio 函数是如何生成 rdb 文件中的每一部分的。这里，为了方便你理解 rdb 文件格式以及文件内容，你可以先按照如下步骤准备一个 rdb 文件。\n\n第一步，在你电脑上 redis 的目录下，启动一个用来测试的 redis server，可以执行如下命令：\n\n好，接下来，我们就来看看 rdbsaverio 函数是如何生成 rdb 文件中的每一部分的。这里，为了方便你理解 rdb 文件格式以及文件内容，你可以先按照如下步骤准备一个 rdb 文件。\n\n第一步，在你电脑上 redis 的目录下，启动一个用来测试的 redis server，可以执行如下命令：\n\n./redis-server\n\n\n第二步，执行 flushall 命令，清空当前的数据库：\n\n./redis-cli flushall\n\n\n第三步，使用 redis-cli 登录刚启动的 redis server，执行 set 命令插入一个 string 类型的键值对，再执行 hmset 命令插入一个 hash 类型的键值对。执行 save 命令，将当前数据库内容保存到 rdb 文件中。这个过程如下所示：\n\n127.0.0.1:6379>set hello redis\nok\n127.0.0.1:6379>hmset userinfo uid 1 name zs age 32\nok\n127.0.0.1:6379> save\nok\n\n\n好了，到这里，你就可以在刚才执行 redis-cli 命令的目录下，找见刚生成的 rdb 文件，文件名应该是 dump.rdb。\n\n不过，因为 rdb 文件实际是一个二进制数据组成的文件，所以如果你使用一般的文本编辑软件，比如 linux 系统上的 vim，在打开 rdb 文件时，你会看到文件中都是乱码。所以这里，我给你提供一个小工具，如果你想查看 rdb 文件中二进制数据和对应的 ascii 字符，你可以使用 linux 上的 od 命令，这个命令可以用不同进制的方式展示数据，并显示对应的 ascii 字符。\n\n比如，你可以执行如下的命令，读取 dump.rdb 文件，并用十六进制展示文件内容，同时文件中每个字节对应的 ascii 字符也会被对应显示出来。\n\nod -a x -t x1c -v dump.rdb\n\n\n以下代码展示的就是我用 od 命令，查看刚才生成的 dump.rdb 文件后，输出的从文件头开始的部分内容。你可以看到这四行结果中，第一和第三行是用十六进制显示的 dump.rdb 文件的字节内容，这里每两个十六进制数对应了一个字节。而第二和第四行是 od 命令生成的每个字节所对应的 ascii 字符。\n\n\n\n这也就是说，在刚才生成的 rdb 文件中，如果想要转换成 ascii 字符，它的文件头内容其实就已经包含了 redis 的字符串和一些数字，而这正是 rdb 文件头包含的内容。\n\n那么下面，我们就来看看 rdb 文件的文件头是如何生成的。\n\n\n# 生成文件头\n\n就像刚才给你介绍的，rdb 文件头的内容首先是魔数，这对应记录了 rdb 文件的版本。在 rdbsaverio 函数中，魔数是通过 snprintf 函数生成的，它的具体内容是字符串“redis”，再加上 rdb 版本的宏定义 rdb_version（在rdb.h文件中，值为 9）。然后，rdbsaverio 函数会调用 rdbwriteraw 函数（在 rdb.c 文件中），将魔数写入 rdb 文件，如下所示：\n\nsnprintf(magic,sizeof(magic),"redis%04d",rdb_version);  //生成魔数magic\nif (rdbwriteraw(rdb,magic,9) == -1) goto werr;  //将magic写入rdb文件\n\n\n刚才用来写入魔数的 rdbwriteraw 函数，它实际会调用 riowrite 函数（在 rdb.h 文件中）来完成写入。而 riowrite 函数是 rdb 文件内容的最终写入函数，它负责根据要写入数据的长度，把待写入缓冲区中的内容写入 rdb。这里，你需要注意的是，rdb 文件生成过程中，会有不同的函数负责写入不同部分的内容，不过这些函数最终都还是调用 riowrite 函数，来完成数据的实际写入的。\n\n好了，当在 rdb 文件头中写入魔数后，rdbsaverio 函数紧接着会调用 rdbsaveinfoauxfields 函数，将和 redis server 相关的一些属性信息写入 rdb 文件头，如下所示：\n\nif (rdbsaveinfoauxfields(rdb,flags,rsi) == -1) goto werr; //写入属性信息\n\n\nrdbsaveinfoauxfields 函数是在 rdb.c 文件中实现的，它会使用键值对的形式，在 rdb 文件头中记录 redis server 的属性信息。下表中列出了 rdb 文件头记录的一些主要信息，以及它们对应的键和值，你可以看下。\n\n\n\n那么，当属性值为字符串时，rdbsaveinfoauxfields 函数会调用 rdbsaveauxfieldstrstr 函数写入属性信息；而当属性值为整数时，rdbsaveinfoauxfields 函数会调用 rdbsaveauxfieldstrint 函数写入属性信息，如下所示：\n\nif (rdbsaveauxfieldstrstr(rdb,"redis-ver",redis_version) == -1) return -1;\nif (rdbsaveauxfieldstrint(rdb,"redis-bits",redis_bits) == -1) return -1;\nif (rdbsaveauxfieldstrint(rdb,"ctime",time(null)) == -1) return -1;\nif (rdbsaveauxfieldstrint(rdb,"used-mem",zmalloc_used_memory()) == -1) return -1;\n\n\n这里，无论是 rdbsaveauxfieldstrstr 函数还是 rdbsaveauxfieldstrint 函数，它们都会调用 rdbsaveauxfield 函数来写入属性值。rdbsaveauxfield 函数是在 rdb.c 文件中实现的，它会分三步来完成一个属性信息的写入。\n\n第一步，它调用 rdbsavetype 函数写入一个操作码。这个操作码的目的，是用来在 rdb 文件中标识接下来的内容是什么。当写入属性信息时，这个操作码对应了宏定义 rdb_opcode_aux（在 rdb.h 文件中），值为 250，对应的十六进制值为 fa。这样一来，就方便我们解析 rdb 文件了。比如，在读取 rdb 文件时，如果程序读取到 fa 这个字节，那么，这就表明接下来的内容是一个属性信息。\n\n这里，你需要注意的是，rdb 文件使用了多个操作码，来标识文件中的不同内容。它们都是在 rdb.h 文件中定义的，下面的代码中展示了部分操作码，你可以看下。\n\n#define rdb_opcode_idle       248   //标识lru空闲时间\n#define rdb_opcode_freq       249   //标识lfu访问频率信息\n#define rdb_opcode_aux        250   //标识rdb文件头的属性信息\n#define rdb_opcode_expiretime_ms 252    //标识以毫秒记录的过期时间\n#define rdb_opcode_selectdb   254   //标识文件中后续键值对所属的数据库编号\n#define rdb_opcode_eof        255   //标识rdb文件结束，用在文件尾\n\n\n第二步，rdbsaveauxfield 函数调用 rdbsaverawstring 函数（在 rdb.c 文件中）写入属性信息的键，而键通常是一个字符串。rdbsaverawstring 函数是用来写入字符串的通用函数，它会先记录字符串长度，然后再记录实际字符串，如下图所示。这个长度信息是为了解析 rdb 文件时，程序可以基于它知道当前读取的字符串应该读取多少个字节。\n\n\n\n不过，为了节省 rdb 文件消耗的空间，如果字符串中记录的实际是一个整数，rdbsaverawstring 函数还会调用 rdbtryintegerencoding 函数（在 rdb.c 文件中），尝试用紧凑结构对字符串进行编码。具体做法你可以进一步阅读 rdbtryintegerencoding 函数。\n\n下图展示了 rdbsaverawstring 函数的基本执行逻辑，你可以看下。其中，它调用 rdbsavelen 函数写入字符串长度，调用 rdbwriteraw 函数写入实际数据。\n\n\n\n第三步，rdbsaveauxfield 函数就需要写入属性信息的值了。因为属性信息的值通常也是字符串，所以和第二步写入属性信息的键类似，rdbsaveauxfield 函数会调用 rdbsaverawstring 函数来写入属性信息的值。\n\n下面的代码展示了 rdbsaveauxfield 函数的执行整体过程，你可以再回顾下。\n\nssize_t rdbsaveauxfield(rio *rdb, void *key, size_t keylen, void *val, size_t vallen) {\n    ssize_t ret, len = 0;\n    //写入操作码\n    if ((ret = rdbsavetype(rdb,rdb_opcode_aux)) == -1) return -1;\n    len += ret;\n    //写入属性信息中的键\n    if ((ret = rdbsaverawstring(rdb,key,keylen)) == -1) return -1;\n    len += ret;\n    //写入属性信息中的值\n    if ((ret = rdbsaverawstring(rdb,val,vallen)) == -1) return -1;\n    len += ret;\n    return len;\n}\n\n\n到这里，rdb 文件头的内容已经写完了。我把刚才创建的 rdb 文件头的部分内容，画在了下图当中，并且标识了十六进制对应的 ascii 字符以及一些关键信息，你可以结合图例来理解刚才介绍的代码。\n\n\n\n这样接下来，rdbsaverio 函数就要开始写入实际的键值对了，这也是文件中实际记录数据的部分。下面，我们就来具体看下。\n\n\n# 生成文件数据部分\n\n因为 redis server 上的键值对可能被保存在不同的数据库中，所以，rdbsaverio 函数会执行一个循环，遍历每个数据库，将其中的键值对写入 rdb 文件。\n\n在这个循环流程中，rdbsaverio 函数会先将 **selectdb 操作码 **和对应的数据库编号写入 rdb 文件，这样一来，程序在解析 rdb 文件时，就可以知道接下来的键值对是属于哪个数据库的了。这个过程如下所示：\n\n...\nfor (j = 0; j < server.dbnum; j++) { //循环遍历每一个数据库\n    ...\n    //写入selectdb操作码\n    if (rdbsavetype(rdb,rdb_opcode_selectdb) == -1) goto werr;\n    if (rdbsavelen(rdb,j) == -1) goto werr;  //写入当前数据库编号j\n    ...\n}\n\n\n下图展示了刚才我创建的 rdb 文件中 selectdb 操作码的信息，你可以看到，数据库编号为 0。\n\n\n\n紧接着，rdbsaverio 函数会写入 resizedb 操作码，用来标识全局哈希表和过期 key 哈希表中键值对数量的记录，这个过程的执行代码如下所示：\n\n...\ndb_size = dictsize(db->dict);   //获取全局哈希表大小\nexpires_size = dictsize(db->expires);  //获取过期key哈希表的大小\nif (rdbsavetype(rdb,rdb_opcode_resizedb) == -1) goto werr;  //写入resizedb操作码\nif (rdbsavelen(rdb,db_size) == -1) goto werr;  //写入全局哈希表大小\nif (rdbsavelen(rdb,expires_size) == -1) goto werr; //写入过期key哈希表大小\n...\n\n\n我也把刚才创建的 rdb 文件中，resizedb 操作码的内容画在了下图中，你可以看下。\n\n\n\n你可以看到，在 resizedb 操作码后，紧接着记录的是全局哈希表中的键值对，它的数量是 2，然后是过期 key 哈希表中的键值对，其数量为 0。我们刚才在生成 rdb 文件前，只插入了两个键值对，所以，rdb 文件中记录的信息和我们刚才的操作结果是一致的。\n\n好了，在记录完这些信息后，rdbsaverio 函数会接着执行一个循环流程，在该流程中，rdbsaverio 函数会取出当前数据库中的每一个键值对，并调用 rdbsavekeyvaluepair 函数（在 rdb.c 文件中），将它写入 rdb 文件。这个基本的循环流程如下所示：\n\n while((de = dictnext(di)) != null) {  //读取数据库中的每一个键值对\n    sds keystr = dictgetkey(de);  //获取键值对的key\n    robj key, *o = dictgetval(de);  //获取键值对的value\n    initstaticstringobject(key,keystr);  //为key生成string对象\n    expire = getexpire(db,&key);  //获取键值对的过期时间\n    //把key和value写入rdb文件\n    if (rdbsavekeyvaluepair(rdb,&key,o,expire) == -1) goto werr;\n    ...\n}\n\n\n这里，rdbsavekeyvaluepair 函数主要是负责将键值对实际写入 rdb 文件。它会先将键值对的过期时间、lru 空闲时间或是 lfu 访问频率写入 rdb 文件。在写入这些信息时，rdbsavekeyvaluepair 函数都会先调用 rdbsavetype 函数，写入标识这些信息的操作码，你可以看下下面的代码。\n\nif (expiretime != -1) {\n    //写入过期时间操作码标识\n   if (rdbsavetype(rdb,rdb_opcode_expiretime_ms) == -1) return -1;\n   if (rdbsavemillisecondtime(rdb,expiretime) == -1) return -1;\n}\nif (savelru) {\n   ...\n   //写入lru空闲时间操作码标识\n   if (rdbsavetype(rdb,rdb_opcode_idle) == -1) return -1;\n   if (rdbsavelen(rdb,idletime) == -1) return -1;\n}\nif (savelfu) {\n   ...\n   //写入lfu访问频率操作码标识\n   if (rdbsavetype(rdb,rdb_opcode_freq) == -1) return -1;\n   if (rdbwriteraw(rdb,buf,1) == -1) return -1;\n}\n\n\n好了，到这里，rdbsavekeyvaluepair 函数就要开始实际写入键值对了。为了便于解析 rdb 文件时恢复键值对，rdbsavekeyvaluepair 函数会先调用 rdbsaveobjecttype 函数，写入键值对的类型标识；然后调用 rdbsavestringobject 写入键值对的 key；最后，它会调用 rdbsaveobject 函数写入键值对的 value。这个过程如下所示，这几个函数都是在 rdb.c 文件中实现的：\n\nif (rdbsaveobjecttype(rdb,val) == -1) return -1;  //写入键值对的类型标识\nif (rdbsavestringobject(rdb,key) == -1) return -1; //写入键值对的key\nif (rdbsaveobject(rdb,val,key) == -1) return -1; //写入键值对的value\n\n\n这里，你需要注意的是，rdbsaveobjecttype 函数会根据键值对的 value 类型，来决定写入到 rdb 中的键值对类型标识，这些类型标识在 rdb.h 文件中有对应的宏定义。比如，我在刚才创建 rdb 文件前，写入的键值对分别是 string 类型和 hash 类型，而 hash 类型因为它包含的元素个数不多，所以默认采用 ziplist 数据结构来保存。这两个类型标识对应的数值如下所示：\n\n#define rdb_type_string   0\n#define rdb_type_hash_ziplist  13\n\n\n我把刚才写入的 string 类型键值对“hello”“redis”在 rdb 文件中对应的记录内容，画在了下图中，你可以看下。\n\n\n\n你可以看到，这个键值对的开头类型标识就是 0，和刚才介绍的 rdb_type_string 宏定义的值是一致的。而紧接着的 key 和 value，它们都会先记录长度信息，然后才记录实际内容。\n\n因为键值对的 key 都是 string 类型，所以 rdbsavekeyvaluepair 函数就用 rdbsavestringobject 函数来写入了。而键值对的 value 有不同的类型，所以，rdbsaveobject 函数会根据 value 的类型，执行不同的代码分支，将 value 底层数据结构中的内容写入 rdb。\n\n好了，到这里，我们就了解了 rdbsavekeyvaluepair 函数是如何将键值对写入 rdb 文件中的了。在这个过程中，除了键值对类型、键值对的 key 和 value 会被记录以外，键值对的过期时间、lru 空闲时间或是 lfu 访问频率也都会记录到 rdb 文件中。这就生成 rdb 文件的数据部分。\n\n最后，我们再来看下 rdb 文件尾的生成。\n\n\n# 生成文件尾\n\n当所有键值对都写入 rdb 文件后，**rdbsaverio 函数 **就可以写入文件尾内容了。文件尾的内容比较简单，主要包括两个部分，一个是 rdb 文件结束的操作码标识，另一个是 rdb 文件的校验值。\n\nrdbsaverio 函数会先调用 rdbsavetype 函数，写入文件结束操作码 rdb_opcode_eof，然后调用 riowrite 写入检验值，如下所示：\n\n...\n//写入结束操作码\nif (rdbsavetype(rdb,rdb_opcode_eof) == -1) goto werr;\n\n//写入校验值\ncksum = rdb->cksum;\nmemrev64ifbe(&cksum);\nif (riowrite(rdb,&cksum,8) == 0) goto werr;\n...\n\n\n下图展示了我刚才生成的 rdb 文件的文件尾，你可以看下。\n\n\n\n这样，我们也就整体了解了 rdb 文件从文件头、文件数据部分再到文件尾的整个生成过程了。\n\n\n# 总结\n\n 1. 创建 rdb 文件的三个入口函数分别是 rdbsave、rdbsavebackground、rdbsavetoslavessockets，它们在 redis 源码中被调用的地方，也就是触发 rdb 文件生成的时机\n\n 2. rdb 文件的基本组成，并且也要结合 rdbsaverio 函数的执行流程，来掌握 rdb 文件头、文件数据部分和文件尾这三个部分的生成。我总结了以下两点，方便你对 rdb 文件结构和内容有个整体把握：\n    \n    * rdb 文件使用多种操作码来标识 redis 不同的属性信息，以及使用类型码来标识不同 value 类型；\n    \n    * rdb 文件内容是自包含的，也就是说，无论是属性信息还是键值对，rdb 文件都会按照类型、长度、实际数据的格式来记录，这样方便程序对 rdb 文件的解析。\n\n 3. rdb 文件包含了 redis 数据库某一时刻的所有键值对，以及这些键值对的类型、大小、过期时间等信息。\n\n当你了解了 rdb 文件的格式和生成方法后，其实你就可以根据需求，开发解析 rdb 文件的程序或是加载 rdb 文件的程序了\n\n比如，你可以在 rdb 文件中查找内存空间消耗大的键值对，也就是在优化 redis 性能时通常需要查找的 bigkey；你也可以分析不同类型键值对的数量、空间占用等分布情况，来了解业务数据的特点；你还可以自行加载 rdb 文件，用于测试或故障排查。\n\n当然，这里我也再给你一个小提示，就是在你实际开发 rdb 文件分析工具之前，可以看下redis-rdb-tools这个工具，它能够帮助你分析 rdb 文件中的内容。而如果它还不能满足你的定制化需求，你就可以用上这节课学习的内容，来开发自己的 rdb 分析工具了\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"AOF 持久化",frontmatter:{title:"AOF 持久化",date:"2024-09-16T03:23:41.000Z",permalink:"/pages/9b17a7/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E5%9B%9B%E3%80%81%E6%94%AF%E7%BA%BF%E4%BB%BB%E5%8A%A1/17.AOF%20%E6%8C%81%E4%B9%85%E5%8C%96.html",relativePath:"Redis 系统设计/04.四、支线任务/17.AOF 持久化.md",key:"v-7a26fbf7",path:"/pages/9b17a7/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:327},{level:2,title:"AOF 核心流程",slug:"aof-核心流程",normalizedTitle:"aof 核心流程",charIndex:695},{level:3,title:"命令追加",slug:"命令追加",normalizedTitle:"命令追加",charIndex:876},{level:3,title:"写入与同步",slug:"写入与同步",normalizedTitle:"写入与同步",charIndex:1043},{level:3,title:"持久化的效率和安全性",slug:"持久化的效率和安全性",normalizedTitle:"持久化的效率和安全性",charIndex:2266},{level:4,title:"如果 Redis 宕机了，操作系统没有宕机，会有数据丢失吗？",slug:"如果-redis-宕机了-操作系统没有宕机-会有数据丢失吗",normalizedTitle:"如果 redis 宕机了，操作系统没有宕机，会有数据丢失吗？",charIndex:2960},{level:4,title:"为什么在 always 下也可能会丢失一个事件循环中所产生的数据？",slug:"为什么在-always-下也可能会丢失一个事件循环中所产生的数据",normalizedTitle:"为什么在 always 下也可能会丢失一个事件循环中所产生的数据？",charIndex:3046},{level:3,title:"AOF 文件载入",slug:"aof-文件载入",normalizedTitle:"aof 文件载入",charIndex:3548},{level:2,title:"AOF 重写",slug:"aof-重写",normalizedTitle:"aof 重写",charIndex:18},{level:3,title:"AOF 重写函数与触发时机",slug:"aof-重写函数与触发时机",normalizedTitle:"aof 重写函数与触发时机",charIndex:3666},{level:3,title:"AOF 重写的基本过程",slug:"aof-重写的基本过程",normalizedTitle:"aof 重写的基本过程",charIndex:8343},{level:2,title:"深入重写缓冲区",slug:"深入重写缓冲区",normalizedTitle:"深入重写缓冲区",charIndex:10743},{level:3,title:"如何使用管道进行父子进程间通信？",slug:"如何使用管道进行父子进程间通信",normalizedTitle:"如何使用管道进行父子进程间通信？",charIndex:10755},{level:3,title:"AOF 重写子进程如何使用管道和父进程交互？",slug:"aof-重写子进程如何使用管道和父进程交互",normalizedTitle:"aof 重写子进程如何使用管道和父进程交互？",charIndex:12531},{level:4,title:"操作命令传输管道的使用",slug:"操作命令传输管道的使用",normalizedTitle:"操作命令传输管道的使用",charIndex:14521},{level:4,title:"ACK 管道的使用",slug:"ack-管道的使用",normalizedTitle:"ack 管道的使用",charIndex:18925},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:8083},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:21203}],headersStr:"前言 AOF 核心流程 命令追加 写入与同步 持久化的效率和安全性 如果 Redis 宕机了，操作系统没有宕机，会有数据丢失吗？ 为什么在 always 下也可能会丢失一个事件循环中所产生的数据？ AOF 文件载入 AOF 重写 AOF 重写函数与触发时机 AOF 重写的基本过程 深入重写缓冲区 如何使用管道进行父子进程间通信？ AOF 重写子进程如何使用管道和父进程交互？ 操作命令传输管道的使用 ACK 管道的使用 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. AOF 重写会在什么条件下触发？这些条件如何影响 Redis 性能？\n 2. 在 AOF 重写时，Redis 如何确保新的写操作不会丢失？\n 3. 为什么 Redis 要在 AOF 重写中使用管道通信？这个机制解决了哪些问题？\n 4. 即使操作系统没宕机，为什么在 AOF 模式下仍可能存在数据丢失风险？\n 5. AOF 重写如何确保写入日志的完整性？子进程是怎么处理新的写操作的？\n 6. 在什么情况下，AOF 重写可能会导致 Redis 性能变慢？\n 7. always 模式下，数据为什么还可能丢失？如何平衡 AOF 的性能与安全？\n 8. 当 Redis 变慢时，如何判断是否是 AOF 重写导致的？\n\n\n# 前言\n\n我们知道，Redis 除了使用内存快照 RDB 来保证数据可靠性之外，还可以使用 AOF 日志。不过，RDB 文件是将某一时刻的内存数据保存成一个文件，而 AOF 日志则会记录接收到的所有写操作。如果 Redis server 的写请求很多，那么 AOF 日志中记录的操作也会越来越多，进而就导致 AOF 日志文件越来越大。\n\n然后，为了避免产生过大的 AOF 日志文件，Redis 会对 AOF 文件进行重写，也就是针对当前数据库中每个键值对的最新内容，记录它的插入操作，而不再记录它的历史写操作了。这样一来，重写后的 AOF 日志文件就能变小了。\n\n那么，AOF 重写在哪些时候会被触发呢？以及 AOF 重写需要写文件，这个过程会阻塞 Redis 的主线程，进而影响 Redis 的性能吗？\n\necho 接下来就给你介绍下 AOF 核心流程以及重写的实现过程，通过了解它的实现，我们就可以清楚地了解到 AOF 重写过程的表现，以及它对 Redis server 的影响。这样，当你再遇到 Redis server 性能变慢的问题时，你就可以排查是否是 AOF 重写导致的了。\n\n好，接下来，我们先来看下 AOF 核心流程\n\n\n# AOF 核心流程\n\nAOF 持久化分为三个步骤\n\n * 命令追加\n * 文件写入\n * 文件同步\n\n\n# 命令追加\n\n当 AOF 持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的 aof buf 缓冲区的末尾\n\nstruct redisServer{\n\t...\n\t//AOF 缓冲区\n\tSDS aof_buf;\n\t...\n}\n\n\n\n# 写入与同步\n\nRedis 的服务器进程就是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行像 servercron 函数这样需要定时运行的函数\n\n因为服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到 aof buf 缓冲区里面，所以在服务器每次结束一个事件循环之前，它都会调用 flushAppendonlyFile 函数，考虑是否需要将 aof buf 缓冲区中的内容写入和保存到 AOF 文件里面，这个过程的伪代码如下\n\ndef eventloop():\n\twhile True:\n        # 在处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中\n\t\tprocessFileEvents()\n\t\tpricessTimeEvents()\n        # 考虑是否将 aof_buf 中的内容写入和保存到 AOF 文件里\n\t\tflushAppendOnlyFile();\n\n\nflushAppendOnlyFile 的行为由在 redis.conf 中的 appendfsync选项的值来决定\n\nAPPENDFSYNC 选项的值   FLUSHAPPENDONLYFILE 函数行为\nalways             将 AOF 缓冲区中的所有内容写入并同步到 AOF 文件。\neverysec           将 AOF 缓冲区中的所有内容写入到 AOF 文件，如果上次同步 AOF 文件的时间距离现在超过一秒钟，那么再次对\n                   AOF 文件进行同步，并且这个同步操作是由一个线程专门负责执行的。\nno                 将 AOF 缓冲区中的所有内容写入到 AOF 文件，但并不对 AOF 文件进行同步，何时同步由操作系统来决定。\n\n默认值是 everysec\n\n文件的写入和同步\n\n为了提高文件的写入效率，在现代操作系统中，当用户调用 write 函数，将一些数据写入到文件的时候，操作系统通常会将写入数据暂时保存在一个内存缓冲区里面等到缓冲区的空间被填满、或者超过了指定的时限之后，才真正地将缓冲区中的数据写入到磁盘里面。 这种做法虽然提高了效率，但也为写入数据带来了安全问题，因为如果计算机发生停机，那么保存在内存缓冲区里面的写入数据将会丢失。 为此，系统提供了 fsync 和 fdatasync 两个同步函数，它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里面，从而确保写入数据的安全性\n\n如果这时 flushAppendonlyFile 函数被调用，假设服务器当前 appendfsyne 选项的值为 everysec，并且距离上次同步 AOF 文件已经超过一秒钟，那么服务器会先将 aof buf 中的内容写人到 AOF 文件中，然后再对 AOF 文件进行同步。\n\n\n# 持久化的效率和安全性\n\n服务器配置 appendfsync 选项的值直接决定 AOF 持久化功能的效率和安全性。\n\n * 当 appendfsync 的值为 always 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 AOF 文件，并且同步 AOF 文件，所以 always 的效率是 appendfsync 选项三个值当中最慢的一个，但从安全性来说，always 也是最安全的，因为即使出现故障停机，AOF 持久化也只会丢失一个事件循环中所产生的命令数据\n * 当 appendfsync 的值为 everysec 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 AOF 文件，并且每隔一秒就要在子线程中对 AOF 文件进行一次同步。从效率上来讲，everysec 模式足够快，并且就算出现故障停机，数据库也只丢失一秒钟的命令数据。\n * 当 appendfsync 的值为 no 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 AOF 文件，至于何时对 AOF 文件进行同步，则由操作系统控制。因为处于 no 模式下的 flushappendOnlyFile 调用无须执行同步操作，所以该模式下的 AOF 文件写入速度总是最快的，不过因为这种模式会在系统缓存中积累一段时间的写入数据，所以该模式的单次同步时长通常是三种模式中时间最长的。从平摊操作的角度来看，no 模式和 everysec 模式的效率类似，当出现故障停机时，使用 no 模式的服务器将丢失上次同步 AOF 文件之后的所有写命令数据\n\n# 如果 Redis 宕机了，操作系统没有宕机，会有数据丢失吗？\n\n不一定 因为写入了 系统缓存 操作系统如果刷盘成功 就不会有丢失，redis 宕机不影响操作系统刷盘\n\n# 为什么在 always 下也可能会丢失一个事件循环中所产生的数据？\n\n如果客户端没有收到 OK 响应，可能会丢失一条数据\n\n但是\n\n在 appendfsync 为 always 模式下，如果客户端收到 Redis 返回的 OK 响应，意味着该命令的结果已经成功写入到 AOF 文件并同步到磁盘\n\n * 在 always 模式下，Redis 在每个命令执行后都会立刻将该命令追加到 aof_buf 缓冲区，然后立即将缓冲区中的内容写入 AOF 文件，并执行 fsync 操作（即将数据同步到磁盘）\n * 当 fsync 成功完成后，Redis 才会返回 OK 响应给客户端\n\n因此，在 always 模式下，如果客户端收到 OK 响应，意味着：\n\n 1. 命令已经被执行。\n 2. 命令的结果已经成功追加到 AOF 文件。\n 3. AOF 文件已经通过 fsync 操作将数据同步到了磁盘。\n\n正因为 always 模式确保每条命令在返回 OK 之前都已经被同步到磁盘，所以从客户端的视角来看，只要收到了 OK，就可以认为该数据已经被持久化到磁盘，不会因为 Redis 崩溃或服务器断电等原因丢失\n\n\n# AOF 文件载入\n\n\n\n 1. 创建一个不带网络连接的伪客户端\n 2. 从 AOF 文件中分析并读取出一条写命令\n 3. 使用伪客户端执行被读出的写命令\n 4. 重复 2 3，知道所有写命令被处理完毕为止\n\n\n# AOF 重写\n\n\n# AOF 重写函数与触发时机\n\n首先，实现 AOF 重写的函数是 rewriteAppendOnlyFileBackground，它是在aof.c文件中实现的。在这个函数中，会调用 fork 函数创建一个 AOF 重写子进程，来实际执行重写操作。关于这个函数的具体实现，我稍后会给你详细介绍。这里呢，我们先来看看，这个函数会被哪些函数调用，这样我们就可以了解 AOF 重写的触发时机了。\n\n实际上，rewriteAppendOnlyFileBackground 函数一共会在三个函数中被调用。\n\n**第一个是 bgrewriteaofCommand 函数。**这个函数是在 aof.c 文件中实现的，对应了我们在 Redis server 上执行 bgrewriteaof 命令，也就是说，我们手动触发了 AOF rewrite 的执行。\n\n不过，即使我们手动执行了 bgrewriteaof 命令，bgrewriteaofCommand 函数也会根据以下两个条件，来判断是否实际执行 AOF 重写。\n\n * **条件一：当前是否已经有 AOF 重写的子进程正在执行。**如果有的话，那么 bgrewriteaofCommand 函数就不再执行 AOF 重写了。\n * **条件二：当前是否有创建 RDB 的子进程正在执行。**如果有的话，bgrewriteaofCommand 函数会把全局变量 server 的 aof_rewrite_scheduled 成员变量设置为 1，这个标志表明 Redis server 已经将 AOF 重写设为待调度运行，等后续条件满足时，它就会实际执行 AOF 重写（我们一会儿就会看到，当 aof_rewrite_scheduled 设置为 1 以后，Redis server 会在哪些条件下实际执行重写操作）。\n\n所以这也就是说，只有当前既没有 AOF 重写子进程也没有 RDB 子进程，bgrewriteaofCommand 函数才会立即调用 rewriteAppendOnlyFileBackground 函数，实际执行 AOF 重写。\n\n以下代码展示了 bgrewriteaofCommand 函数的基本执行逻辑，你可以看下。\n\nvoid bgrewriteaofCommand(client *c) {\n    if (server.aof_child_pid != -1) {\n        .. //有AOF重写子进程，因此不执行重写\n    } else if (server.rdb_child_pid != -1) {\n        server.aof_rewrite_scheduled = 1; //有RDB子进程，将AOF重写设置为待调度运行\n        ...\n    } else if (rewriteAppendOnlyFileBackground() == C_OK) { //实际执行AOF重写\n        ...\n    }\n    ...\n}\n\n\n**第二个是 startAppendOnly 函数。**这个函数也是在 aof.c 文件中实现的，它本身会被 configSetCommand 函数（在config.c文件中）和 restartAOFAfterSYNC 函数（在replication.c文件中）调用。\n\n首先，对于 configSetCommand 函数来说，它对应了我们在 Redis 中执行 config 命令启用 AOF 功能，如下所示：\n\nconfig set appendonly yes\n\n\n这样，一旦 AOF 功能启用后，configSetCommand 函数就会调用 startAppendOnly 函数，执行一次 AOF 重写。\n\n而对于 restartAOFAfterSYNC 函数来说，它会在主从节点的复制过程中被调用。简单来说，就是当主从节点在进行复制时，如果从节点的 AOF 选项被打开，那么在加载解析 RDB 文件时，AOF 选项就会被关闭。然后，无论从节点是否成功加载了 RDB 文件，restartAOFAfterSYNC 函数都会被调用，用来恢复被关闭的 AOF 功能。\n\n那么在这个过程中，restartAOFAfterSYNC 函数就会调用 startAppendOnly 函数，并进一步调用 rewriteAppendOnlyFileBackground 函数，来执行一次 AOF 重写。\n\n这里你要注意，和 bgrewriteaofCommand 函数类似，startAppendOnly 函数也会判断当前是否有 RDB 子进程在执行，如果有的话，它会将 AOF 重写设置为待调度执行。除此之外，如果 startAppendOnly 函数检测到有 AOF 重写子进程在执行，那么它就会把该子进程先 kill 掉，然后再调用 rewriteAppendOnlyFileBackground 函数进行 AOF 重写。\n\n所以到这里，我们其实可以发现，无论是 bgrewriteaofCommand 函数还是 startAppendOnly 函数，当它们检测到有 RDB 子进程在执行的时候，就会把 aof_rewrite_scheduled 变量设置为 1，这表示 AOF 重写操作将在条件满足时再被执行。\n\n**那么，Redis server 什么时候会再检查 AOF 重写操作的条件是否满足呢？**这就和 rewriteAppendOnlyFileBackground 函数被调用的第三个函数，serverCron 函数相关了。\n\n**第三个是 serverCron 函数。**在 Redis server 运行时，serverCron 函数是会被周期性执行的。然后它在执行的过程中，会做两次判断来决定是否执行 AOF 重写。\n\n首先，serverCron 函数会检测当前是否没有 RDB 子进程和 AOF 重写子进程在执行，并检测是否有 AOF 重写操作被设置为了待调度执行，也就是 aof_rewrite_scheduled 变量值为 1。\n\n如果这三个条件都满足，那么 serverCron 函数就会调用 rewriteAppendOnlyFileBackground 函数来执行 AOF 重写。serverCron 函数里面的这部分执行逻辑如下所示：\n\n//如果没有 RDB 子进程，也没有 AOF 重写子进程，并且 AOF 重写被设置为待调度执行，那么调用 rewriteAppendOnlyFileBackground 函数进行 AOF 重写\n\n//如果没有RDB子进程，也没有AOF重写子进程，并且AOF重写被设置为待调度执行，那么调用rewriteAppendOnlyFileBackground函数进行AOF重写\nif (server.rdb_child_pid == -1 && server.aof_child_pid == -1 &&\n        server.aof_rewrite_scheduled)\n{\n        rewriteAppendOnlyFileBackground();\n}\n\n\n事实上，这里的代码也回答了我们刚才提到的问题：待调度执行的 AOF 重写会在什么时候执行？\n\n其实，如果 AOF 重写没法立即执行的话，我们也不用担心。因为只要 aof_rewrite_scheduled 变量被设置为 1 了，那么 serverCron 函数就默认会每 100 毫秒执行并检测这个变量值。所以，如果正在执行的 RDB 子进程和 AOF 重写子进程结束了之后，被调度执行的 AOF 重写就可以很快得到执行。\n\n其次，即使 AOF 重写操作没有被设置为待调度执行，serverCron 函数也会周期性判断是否需要执行 AOF 重写。这里的判断条件主要有三个，分别是 AOF 功能已启用、AOF 文件大小比例超出阈值，以及 AOF 文件大小绝对值超出阈值。\n\n这样一来，当这三个条件都满足时，并且也没有 RDB 子进程和 AOF 子进程在运行的话，此时，serverCron 函数就会调用 rewriteAppendOnlyFileBackground 函数执行 AOF 重写。这部分的代码逻辑如下所示：\n\n//如果 AOF 功能启用、没有 RDB 子进程和 AOF 重写子进程在执行、AOF 文件大小比例设定了阈值，以及 AOF 文件大小绝对值超出了阈值，那么，进一步判断 AOF 文件大小比例是否超出阈值\n\n//如果AOF功能启用、没有RDB子进程和AOF重写子进程在执行、AOF文件大小比例设定了阈值，以及AOF文件大小绝对值超出了阈值，那么，进一步判断AOF文件大小比例是否超出阈值\nif (server.aof_state == AOF_ON && server.rdb_child_pid == -1 && server.aof_child_pid == -1 && server.aof_rewrite_perc && server.aof_current_size > server.aof_rewrite_min_size) {\n   //计算AOF文件当前大小超出基础大小的比例\n   long long base = server.aof_rewrite_base_size ? server.aof_rewrite_base_size : 1;\n   long long growth = (server.aof_current_size*100/base) - 100;\n   //如果AOF文件当前大小超出基础大小的比例已经超出预设阈值，那么执行AOF重写\n   if (growth >= server.aof_rewrite_perc) {\n      ...\n      rewriteAppendOnlyFileBackground();\n   }\n}\n\n\n那么，从这里的代码中，你会看到，为了避免 AOF 文件过大导致占用过多的磁盘空间，以及增加恢复时长，你其实可以通过设置 redis.conf 文件中的以下两个阈值，来让 Redis server 自动重写 AOF 文件。\n\n * auto-aof-rewrite-percentage：AOF 文件大小超出基础大小的比例，默认值为 100%，即超出 1 倍大小。\n * auto-aof-rewrite-min-size：AOF 文件大小绝对值的最小值，默认为 64MB。\n\n好了，到这里，我们就了解了 AOF 重写的四个触发时机，这里我也给你总结下，方便你回顾复习。\n\n * 时机一：bgrewriteaof 命令被执行。\n * 时机二：主从复制完成 RDB 文件解析和加载（无论是否成功）。\n * 时机三：AOF 重写被设置为待调度执行。\n * 时机四：AOF 被启用，同时 AOF 文件的大小比例超出阈值，以及 AOF 文件的大小绝对值超出阈值。\n\n另外，这里你还需要注意，在这四个时机下，其实都不能有正在执行的 RDB 子进程和 AOF 重写子进程，否则的话，AOF 重写就无法执行了。\n\n所以接下来，我们就来学习下 AOF 重写的基本执行过程。\n\n\n# AOF 重写的基本过程\n\n首先，我们再来看下刚才介绍的 rewriteAppendOnlyFileBackground 函数。这个函数的主体逻辑比较简单，一方面，它会通过调用 fork 函数创建一个子进程，然后在子进程中调用 rewriteAppendOnlyFile 函数进行 AOF 文件重写。\n\nrewriteAppendOnlyFile 函数是在 aof.c 文件中实现的。它主要会调用 rewriteAppendOnlyFileRio 函数（在 aof.c 文件中）来完成 AOF 日志文件的重写。具体来说，就是 rewriteAppendOnlyFileRio 函数会遍历 Redis server 的每一个数据库，把其中的每个键值对读取出来，然后记录该键值对类型对应的插入命令，以及键值对本身的内容。\n\n比如，如果读取的是一个 String 类型的键值对，那么 rewriteAppendOnlyFileRio 函数，就会记录 SET 命令和键值对本身内容；而如果读取的是 Set 类型键值对，那么它会记录 SADD 命令和键值对内容。这样一来，当需要恢复 Redis 数据库时，我们重新执行一遍 AOF 重写日志中记录的命令操作，就可以依次插入所有键值对了。\n\n另一方面，在父进程中，这个 rewriteAppendOnlyFileBackground 函数会把 aof_rewrite_scheduled 变量设置为 0，同时记录 AOF 重写开始的时间，以及记录 AOF 子进程的进程号。\n\n此外，rewriteAppendOnlyFileBackground 函数还会调用 updateDictResizePolicy 函数，禁止在 AOF 重写期间进行 rehash 操作。这是因为 rehash 操作会带来较多的数据移动操作，对于 AOF 重写子进程来说，这就意味着父进程中的内存修改会比较多。因此，AOF 重写子进程就需要执行更多的写时复制，进而完成 AOF 文件的写入，这就会给 Redis 系统的性能造成负面影响。\n\n以下代码就展示了 rewriteAppendOnlyFileBackground 函数的基本执行逻辑，你可以看下。\n\nint rewriteAppendOnlyFileBackground(void) {\n   ...\n   if ((childpid = fork()) == 0) {  //创建子进程\n      ...\n      //子进程调用rewriteAppendOnlyFile进行AOF重写\n      if (rewriteAppendOnlyFile(tmpfile) == C_OK) {\n            size_t private_dirty = zmalloc_get_private_dirty(-1);\n            ...\n            exitFromChild(0);\n        } else {\n            exitFromChild(1);\n        }\n   }\n   else{ //父进程执行的逻辑\n      ...\n      server.aof_rewrite_scheduled = 0;\n      server.aof_rewrite_time_start = time(NULL);\n      server.aof_child_pid = childpid; //记录重写子进程的进程号\n      updateDictResizePolicy(); //关闭rehash功能\n}\n\n\n而从这里，你可以看到，AOF 重写和 RDB 创建是比较类似的，它们都会创建一个子进程来遍历所有的数据库，并把数据库中的每个键值对记录到文件中。不过，AOF 重写和 RDB 文件又有两个不同的地方：\n\n * 一是，AOF 文件中是以“命令 + 键值对”的形式，来记录每个键值对的插入操作，而 RDB 文件记录的是键值对数据本身；\n * 二是，在 AOF 重写或是创建 RDB 的过程中，主进程仍然可以接收客户端写请求。不过，因为 RDB 文件只需要记录某个时刻下数据库的所有数据就行，而 AOF 重写则需要尽可能地把主进程收到的写操作，也记录到重写的日志文件中。所以，AOF 重写子进程就需要有相应的机制来和主进程进行通信，以此来接收主进程收到的写操作。\n\n下图就展示了 rewriteAppendOnlyFileBackground 函数执行的基本逻辑、主进程和 AOF 重写子进程各自执行的内容，以及主进程和子进程间的通信过程，你可以再来整体回顾下。\n\n\n\n到这里，我们就大概掌握了 AOF 重写的基本执行过程。但是在这里，你可能还会有疑问，比如说，AOF 重写的子进程和父进程，它们之间的通信过程是怎么样的呢？\n\n其实，这个通信过程是通过操作系统的管道机制（pipe）来实现的\n\n在 AOF 重写时，主进程仍然在接收客户端写操作，那么这些新写操作会记录到 AOF 重写日志中吗？如果需要记录的话，重写子进程又是通过什么方式向主进程获取这些写操作的呢？\n\necho 接下来就带你了解下 AOF 重写过程中所使用的管道机制，以及主进程和重写子进程的交互过程\n\n * 一方面，你就可以了解 AOF 重写日志包含的写操作的完整程度，当你要使用 AOF 日志恢复 Redis 数据库时，就知道 AOF 能恢复到的程度是怎样的\n * 一方面，因为 AOF 重写子进程就是通过操作系统提供的管道机制，来和 Redis 主进程交互的，所以学完这节课之后，你还可以掌握管道技术，从而用来实现进程间的通信\n\n好了，接下来，我们就先来了解下管道机制\n\n\n# 深入重写缓冲区\n\n\n# 如何使用管道进行父子进程间通信？\n\n首先我们要知道，当进程 A 通过调用 fork 函数创建一个子进程 B，然后进程 A 和 B 要进行通信时，我们通常都需要依赖操作系统提供的通信机制，而管道（pipe）就是一种用于父子进程间通信的常用机制。\n\n具体来说，管道机制在操作系统内核中创建了一块缓冲区，父进程 A 可以打开管道，并往这块缓冲区中写入数据。同时，子进程 B 也可以打开管道，从这块缓冲区中读取数据。这里，你需要注意的是，进程每次往管道中写入数据时，只能追加写到缓冲区中当前数据所在的尾部，而进程每次从管道中读取数据时，只能从缓冲区的头部读取数据。\n\n其实，管道创建的这块缓冲区就像一个先进先出的队列一样，写数据的进程写到队列尾部，而读数据的进程则从队列头读取。下图就展示了两个进程使用管道进行数据通信的过程，你可以看下。\n\n\n\n好了，了解了管道的基本功能后，我们再来看下使用管道时需要注意的一个关键点。管道中的数据在一个时刻只能向一个方向流动，这也就是说，如果父进程 A 往管道中写入了数据，那么此时子进程 B 只能从管道中读取数据。类似的，如果子进程 B 往管道中写入了数据，那么此时父进程 A 只能从管道中读取数据。而如果父子进程间需要同时进行数据传输通信，我们就需要创建两个管道了。\n\n下面，我们就来看下怎么用代码实现管道通信。这其实是和操作系统提供的管道的系统调用 pipe 有关，pipe 的函数原型如下所示：\n\nint pipe(int pipefd[2]);\n\n\n你可以看到，pipe 的参数是一个数组 pipefd，表示的是管道的文件描述符。这是因为进程在往管道中写入或读取数据时，其实是使用 write 或 read 函数的，而 write 和 read 函数需要通过文件描述符才能进行写数据和读数据操作。\n\n数组 pipefd 有两个元素 pipefd[0]和 pipefd[1]，分别对应了管道的读描述符和写描述符。这也就是说，当进程需要从管道中读数据时，就需要用到 pipefd[0]，而往管道中写入数据时，就使用 pipefd[1]。\n\n这里我写了一份示例代码，展示了父子进程如何使用管道通信，你可以看下。\n\nint main()\n{\n    int fd[2], nr = 0, nw = 0;\n    char buf[128];\n    pipe(fd);\n    pid = fork();\n\n  if(pid == 0) {\n      //子进程调用read从fd[0]描述符中读取数据\n        printf("child process wait for message\\n");\n        nr = read(fds[0], buf, sizeof(buf))\n        printf("child process receive %s\\n", buf);\n  }else{\n       //父进程调用write往fd[1]描述符中写入数据\n        printf("parent process send message\\n");\n        strcpy(buf, "Hello from parent");\n        nw = write(fd[1], buf, sizeof(buf));\n        printf("parent process send %d bytes to child.\\n", nw);\n    }\n    return 0;\n}\n\n\n从代码中，你可以看到，在父子进程进行管道通信前，我们需要在代码中定义用于保存读写描述符的数组 fd，然后调用 pipe 系统创建管道，并把数组 fd 作为参数传给 pipe 函数。紧接着，在父进程的代码中，父进程会调用 write 函数往管道文件描述符 fd[1]中写入数据，另一方面，子进程调用 read 函数从管道文件描述符 fd[0]中读取数据。\n\n这里，为了便于你理解，我也画了一张图，你可以参考。\n\n\n\n好了，现在你就了解了如何使用管道来进行父子进程的通信了。那么下面，我们就来看下在 AOF 重写过程中，重写子进程是如何用管道和主进程（也就是它的父进程）进行通信的。\n\n\n# AOF 重写子进程如何使用管道和父进程交互？\n\n我们先来看下在 AOF 重写过程中，都创建了几个管道。\n\n这实际上是 AOF 重写函数 rewriteAppendOnlyFileBackground 在执行过程中，通过调用 aofCreatePipes 函数来完成的，如下所示：\n\nint rewriteAppendOnlyFileBackground(void) {\n…\nif (aofCreatePipes() != C_OK) return C_ERR;\n…\n}\n\n\n这个 aofCreatePipes 函数是在aof.c文件中实现的，它的逻辑比较简单，可以分成三步。\n\n第一步，aofCreatePipes 函数创建了包含 6 个文件描述符元素的数组 fds。就像我刚才给你介绍的，每一个管道会对应两个文件描述符，所以，数组 fds 其实对应了 AOF 重写过程中要用到的三个管道。紧接着，aofCreatePipes 函数就调用 pipe 系统调用函数，分别创建三个管道。\n\n这部分代码如下所示，你可以看下。\n\nint aofCreatePipes(void) {\n    int fds[6] = {-1, -1, -1, -1, -1, -1};\n    int j;\n    if (pipe(fds) == -1) goto error; /* parent -> children data. */\n    if (pipe(fds+2) == -1) goto error; /* children -> parent ack. */\n  if (pipe(fds+4) == -1) goto error;\n  …}\n}\n\n\n第二步，aofCreatePipes 函数会调用 anetNonBlock 函数（在anet.c文件中），将 fds\n\n数组的第一和第二个描述符（fds[0]和 fds[1]）对应的管道设置为非阻塞。然后，aofCreatePipes 函数会调用 aeCreateFileEvent 函数，在数组 fds 的第三个描述符 (fds[2]) 上注册了读事件的监听，对应的回调函数是 aofChildPipeReadable。aofChildPipeReadable 函数也是在 aof.c 文件中实现的，我稍后会给你详细介绍它。\n\nint aofCreatePipes(void) {\n…\nif (anetNonBlock(NULL,fds[0]) != ANET_OK) goto error;\nif (anetNonBlock(NULL,fds[1]) != ANET_OK) goto error;\nif (aeCreateFileEvent(server.el, fds[2], AE_READABLE, aofChildPipeReadable, NULL) == AE_ERR) goto error;\n…\n}\n\n\n这样，在完成了管道创建、管道设置和读事件注册后，最后一步，aofCreatePipes 函数会将数组 fds 中的六个文件描述符，分别复制给 server 变量的成员变量，如下所示：\n\nint aofCreatePipes(void) {\n…\nserver.aof_pipe_write_data_to_child = fds[1];\nserver.aof_pipe_read_data_from_parent = fds[0];\nserver.aof_pipe_write_ack_to_parent = fds[3];\nserver.aof_pipe_read_ack_from_child = fds[2];\nserver.aof_pipe_write_ack_to_child = fds[5];\nserver.aof_pipe_read_ack_from_parent = fds[4];\n…\n}\n\n\n在这一步中，我们就可以从 server 变量的成员变量名中，看到 aofCreatePipes 函数创建的三个管道，以及它们各自的用途。\n\n * fds[0]和 fds[1]：对应了主进程和重写子进程间用于传递操作命令的管道，它们分别对应读描述符和写描述符。\n * fds[2]和 fds[3]：对应了重写子进程向父进程发送 ACK 信息的管道，它们分别对应读描述符和写描述符。\n * fds[4]和 fds[5]：对应了父进程向重写子进程发送 ACK 信息的管道，它们分别对应读描述符和写描述符。\n\n下图也展示了 aofCreatePipes 函数的基本执行流程，你可以再回顾下。\n\n\n\n好了，了解了 AOF 重写过程中的管道个数和用途后，下面我们再来看下这些管道具体是如何使用的。\n\n# 操作命令传输管道的使用\n\n实际上，当 AOF 重写子进程在执行时，主进程还会继续接收和处理客户端写请求。这些写操作会被主进程正常写入 AOF 日志文件，这个过程是由 feedAppendOnlyFile 函数（在 aof.c 文件中）来完成。\n\nfeedAppendOnlyFile 函数在执行的最后一步，会判断当前是否有 AOF 重写子进程在运行。如果有的话，它就会调用 aofRewriteBufferAppend 函数（在 aof.c 文件中），如下所示：\n\nif (server.aof_child_pid != -1)\n        aofRewriteBufferAppend((unsigned char*)buf,sdslen(buf));\n\n\naofRewriteBufferAppend 函数的作用是将参数 buf，追加写到全局变量 server 的 aof_rewrite_buf_blocks 这个列表中。\n\n这里，你需要注意的是，参数 buf 是一个字节数组，feedAppendOnlyFile 函数会将主进程收到的命令操作写入到 buf 中。而 aof_rewrite_buf_blocks 列表中的每个元素是 aofrwblock 结构体类型，这个结构体中包括了一个字节数组，大小是 AOF_RW_BUF_BLOCK_SIZE，默认值是 10MB。此外，aofrwblock 结构体还记录了字节数组已经使用的空间和剩余可用的空间。\n\n以下代码展示了 aofrwblock 结构体的定义，你可以看下。\n\ntypedef struct aofrwblock {\n    unsigned long used, free; //buf数组已用空间和剩余可用空间\n    char buf[AOF_RW_BUF_BLOCK_SIZE]; //宏定义AOF_RW_BUF_BLOCK_SIZE默认为10MB\n} aofrwblock;\n\n\n这样一来，aofrwblock 结构体就相当于是一个 10MB 的数据块，记录了 AOF 重写期间主进程收到的命令，而 aof_rewrite_buf_blocks 列表负责将这些数据块连接起来。当 aofRewriteBufferAppend 函数执行时，它会从 aof_rewrite_buf_blocks 列表中取出一个 aofrwblock 类型的数据块，用来记录命令操作。\n\n当然，如果当前数据块中的空间不够保存参数 buf 中记录的命令操作，那么 aofRewriteBufferAppend 函数就会再分配一个 aofrwblock 数据块。\n\n好了，当 aofRewriteBufferAppend 函数将命令操作记录到 aof_rewrite_buf_blocks 列表中之后，它还会检查 aof_pipe_write_data_to_child 管道描述符上是否注册了写事件，这个管道描述符就对应了我刚才给你介绍的 fds[1]。\n\n如果没有注册写事件，那么 aofRewriteBufferAppend 函数就会调用 aeCreateFileEvent 函数，注册一个写事件，这个写事件会监听 aof_pipe_write_data_to_child 这个管道描述符，也就是主进程和重写子进程间的操作命令传输管道。\n\n当这个管道可以写入数据时，写事件对应的回调函数 aofChildWriteDiffData（在 aof.c 文件中）就会被调用执行。这个过程你可以参考下面的代码：\n\nvoid aofRewriteBufferAppend(unsigned char *s, unsigned long len) {\n...\n//检查aof_pipe_write_data_to_child描述符上是否有事件\nif (aeGetFileEvents(server.el,server.aof_pipe_write_data_to_child) == 0) {\n     //如果没有注册事件，那么注册一个写事件，回调函数是aofChildWriteDiffData\n     aeCreateFileEvent(server.el, server.aof_pipe_write_data_to_child,\n            AE_WRITABLE, aofChildWriteDiffData, NULL);\n}\n...}\n\n\n其实，刚才我介绍的写事件回调函数 aofChildWriteDiffData，它的主要作用是从 aof_rewrite_buf_blocks 列表中逐个取出数据块，然后通过 aof_pipe_write_data_to_child 管道描述符，将数据块中的命令操作通过管道发给重写子进程，这个过程如下所示：\n\nvoid aofChildWriteDiffData(aeEventLoop *el, int fd, void *privdata, int mask) {\n...\nwhile(1) {\n   //从aof_rewrite_buf_blocks列表中取出数据块\n   ln = listFirst(server.aof_rewrite_buf_blocks);\n   block = ln ? ln->value : NULL;\n   if (block->used > 0) {\n      //调用write将数据块写入主进程和重写子进程间的管道\n      nwritten = write(server.aof_pipe_write_data_to_child,\n                             block->buf,block->used);\n      if (nwritten <= 0) return;\n            ...\n        }\n ...}}\n\n\n好了，这样一来，你就了解了主进程其实是在正常记录 AOF 日志时，将收到的命令操作写入 aof_rewrite_buf_blocks 列表中的数据块，然后再通过 aofChildWriteDiffData 函数将记录的命令操作通过主进程和重写子进程间的管道发给子进程。\n\n下图也展示了这个过程，你可以再来回顾下。\n\n\n\n然后，我们接着来看下重写子进程，是如何从管道中读取父进程发送的命令操作的。\n\n这实际上是由 aofReadDiffFromParent 函数（在 aof.c 文件中）来完成的。这个函数会使用一个 64KB 大小的缓冲区，然后调用 read 函数，读取父进程和重写子进程间的操作命令传输管道中的数据。以下代码也展示了 aofReadDiffFromParent 函数的基本执行流程，你可以看下。\n\nssize_t aofReadDiffFromParent(void) {\n    char buf[65536]; //管道默认的缓冲区大小\n    ssize_t nread, total = 0;\n    //调用read函数从aof_pipe_read_data_from_parent中读取数据\n    while ((nread =\n      read(server.aof_pipe_read_data_from_parent,buf,sizeof(buf))) > 0) {\n        server.aof_child_diff = sdscatlen(server.aof_child_diff,buf,nread);\n        total += nread;\n    }\n    return total;\n}\n\n\n那么，从代码中，你可以看到 aofReadDiffFromParent 函数会通过 aof_pipe_read_data_from_parent 描述符读取数据。然后，它会将读取的操作命令追加到全局变量 server 的 aof_child_diff 字符串中。而在 AOF 重写函数 rewriteAppendOnlyFile 的执行过程最后，aof_child_diff 字符串会被写入 AOF 重写日志文件，以便我们在使用 AOF 重写日志时，能尽可能地恢复重写期间收到的操作。\n\n这个 aof_child_diff 字符串写入重写日志文件的过程，你可以参考下面给出的代码：\n\nint rewriteAppendOnlyFile(char *filename) {\n...\n//将aof_child_diff中累积的操作命令写入AOF重写日志文件\nif (rioWrite(&aof,server.aof_child_diff,sdslen(server.aof_child_diff)) == 0)\n        goto werr;\n...\n}\n\n\n所以也就是说，aofReadDiffFromParent 函数实现了重写子进程向主进程读取操作命令。那么在这里，我们还需要搞清楚的问题是：aofReadDiffFromParent 函数会在哪里被调用，也就是重写子进程会在什么时候从管道中读取主进程收到的操作。\n\n其实，aofReadDiffFromParent 函数一共会被以下三个函数调用。\n\n * rewriteAppendOnlyFileRio 函数：这个函数是由重写子进程执行的，它负责遍历 Redis 每个数据库，生成 AOF 重写日志，在这个过程中，它会不时地调用 aofReadDiffFromParent 函数。\n * rewriteAppendOnlyFile 函数：这个函数是重写日志的主体函数，也是由重写子进程执行的，它本身会调用 rewriteAppendOnlyFileRio 函数。此外，它在调用完 rewriteAppendOnlyFileRio 函数后，还会多次调用 aofReadDiffFromParent 函数，以尽可能多地读取主进程在重写日志期间收到的操作命令。\n * rdbSaveRio 函数：这个函数是创建 RDB 文件的主体函数。当我们使用 AOF 和 RDB 混合持久化机制时，这个函数也会调用 aofReadDiffFromParent 函数。\n\n从这里，我们可以看到，Redis 源码在实现 AOF 重写过程中，其实会多次让重写子进程向主进程读取新收到的操作命令，这也是为了让重写日志尽可能多地记录最新的操作，提供更加完整的操作记录。\n\n最后，我们再来看下重写子进程和主进程间用来传递 ACK 信息的两个管道的使用。\n\n# ACK 管道的使用\n\n刚才在介绍主进程调用 aofCreatePipes 函数创建管道时，你就了解到了，主进程会在 aof_pipe_read_ack_from_child 管道描述符上注册读事件。这个描述符对应了重写子进程向主进程发送 ACK 信息的管道。同时，这个描述符是一个读描述符，表示主进程从管道中读取 ACK 信息。\n\n其实，重写子进程在执行 rewriteAppendOnlyFile 函数时，这个函数在完成日志重写，以及多次向父进程读取操作命令后，就会调用 write 函数，向 aof_pipe_write_ack_to_parent 描述符对应的管道中写入“！”，这就是重写子进程向主进程发送 ACK 信号，让主进程停止发送收到的新写操作。这个过程如下所示：\n\nint rewriteAppendOnlyFile(char *filename) {\n...\nif (write(server.aof_pipe_write_ack_to_parent,"!",1) != 1) goto werr;\n...}\n\n\n一旦重写子进程向主进程发送 ACK 信息的管道中有了数据，aof_pipe_read_ack_from_child 管道描述符上注册的读事件就会被触发，也就是说，这个管道中有数据可以读取了。那么，aof_pipe_read_ack_from_child 管道描述符上，注册的回调函数 aofChildPipeReadable（在 aof.c 文件中）就会执行。\n\n这个函数会判断从 aof_pipe_read_ack_from_child 管道描述符读取的数据是否是“！”，如果是的话，那它就会调用 write 函数，往 aof_pipe_write_ack_to_child 管道描述符上写入“！”，表示主进程已经收到重写子进程发送的 ACK 信息，同时它会给重写子进程回复一个 ACK 信息。这个过程如下所示：\n\nvoid aofChildPipeReadable(aeEventLoop *el, int fd, void *privdata, int mask) {\n...\nif (read(fd,&byte,1) == 1 && byte == \'!\') {\n   ...\n   if (write(server.aof_pipe_write_ack_to_child,"!",1) != 1) { ...}\n}\n...\n}\n\n\n好了，到这里，我们就了解了，重写子进程在完成日志重写后，是先给主进程发送 ACK 信息。然后主进程在 aof_pipe_read_ack_from_child 描述符上监听读事件发生，并调用 aofChildPipeReadable 函数向子进程发送 ACK 信息。\n\n最后，重写子进程执行的 rewriteAppendOnlyFile 函数，会调用 syncRead 函数，从 aof_pipe_read_ack_from_parent 管道描述符上，读取主进程发送给它的 ACK 信息，如下所示：\n\nint rewriteAppendOnlyFile(char *filename) {\n...\nif (syncRead(server.aof_pipe_read_ack_from_parent,&byte,1,5000) != 1  || byte != \'!\') goto werr\n...\n}\n\n\n下图也展示了 ACK 管道的使用过程，你可以再回顾下。\n\n\n\n这样一来，重写子进程和主进程之间就通过两个 ACK 管道，相互确认重写过程结束了。\n\n\n# 总结\n\n 1. AOF 重写的触发时机。这既包括了我们主动执行 bgrewriteaof 命令，也包括了 Redis server 根据 AOF 文件大小而自动触发的重写。此外，在主从复制的过程中，从节点也会启动 AOF 重写，形成一份完整的 AOF 日志，以便后续进行恢复。当然你也要知道，当要触发 AOF 重写时，Redis server 是不能运行 RDB 子进程和 AOF 重写子进程的。\n\n 2. AOF 重写的基本执行过程。AOF 重写和 RDB 创建的过程类似，它也是创建了一个子进程来完成重写工作。这是因为 AOF 重写操作，实际上需要遍历 Redis server 上的所有数据库，把每个键值对以插入操作的形式写入日志文件，而日志文件又要进行写盘操作。所以，Redis 源码使用子进程来实现 AOF 重写，这就避免了阻塞主线程，也减少了对 Redis 整体性能的影响。\n\n 3. 注管道机制的使用\n\n 4. 主进程和重写子进程使用管道通信的过程\n\n在这个过程中，AOF 重写子进程和主进程是使用了一个操作命令传输管道和两个 ACK 信息发送管道。操作命令传输管道是用于主进程写入收到的新操作命令，以及用于重写子进程读取操作命令，而 ACK 信息发送管道是在重写结束时，重写子进程和主进程用来相互确认重写过程的结束。最后，重写子进程会进一步将收到的操作命令记录到重写日志文件中。\n\n这样一来，AOF 重写过程中主进程收到的新写操作，就不会被遗漏了\n\n * 一方面，这些新写操作会被记录在正常的 AOF 日志中\n * 一方面，主进程会将新写操作缓存在 aof_rewrite_buf_blocks 数据块列表中，并通过管道发送给重写子进程。这样，就能尽可能地保证重写日志具有最新、最完整的写操作了\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. aof 重写会在什么条件下触发？这些条件如何影响 redis 性能？\n 2. 在 aof 重写时，redis 如何确保新的写操作不会丢失？\n 3. 为什么 redis 要在 aof 重写中使用管道通信？这个机制解决了哪些问题？\n 4. 即使操作系统没宕机，为什么在 aof 模式下仍可能存在数据丢失风险？\n 5. aof 重写如何确保写入日志的完整性？子进程是怎么处理新的写操作的？\n 6. 在什么情况下，aof 重写可能会导致 redis 性能变慢？\n 7. always 模式下，数据为什么还可能丢失？如何平衡 aof 的性能与安全？\n 8. 当 redis 变慢时，如何判断是否是 aof 重写导致的？\n\n\n# 前言\n\n我们知道，redis 除了使用内存快照 rdb 来保证数据可靠性之外，还可以使用 aof 日志。不过，rdb 文件是将某一时刻的内存数据保存成一个文件，而 aof 日志则会记录接收到的所有写操作。如果 redis server 的写请求很多，那么 aof 日志中记录的操作也会越来越多，进而就导致 aof 日志文件越来越大。\n\n然后，为了避免产生过大的 aof 日志文件，redis 会对 aof 文件进行重写，也就是针对当前数据库中每个键值对的最新内容，记录它的插入操作，而不再记录它的历史写操作了。这样一来，重写后的 aof 日志文件就能变小了。\n\n那么，aof 重写在哪些时候会被触发呢？以及 aof 重写需要写文件，这个过程会阻塞 redis 的主线程，进而影响 redis 的性能吗？\n\necho 接下来就给你介绍下 aof 核心流程以及重写的实现过程，通过了解它的实现，我们就可以清楚地了解到 aof 重写过程的表现，以及它对 redis server 的影响。这样，当你再遇到 redis server 性能变慢的问题时，你就可以排查是否是 aof 重写导致的了。\n\n好，接下来，我们先来看下 aof 核心流程\n\n\n# aof 核心流程\n\naof 持久化分为三个步骤\n\n * 命令追加\n * 文件写入\n * 文件同步\n\n\n# 命令追加\n\n当 aof 持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的 aof buf 缓冲区的末尾\n\nstruct redisserver{\n\t...\n\t//aof 缓冲区\n\tsds aof_buf;\n\t...\n}\n\n\n\n# 写入与同步\n\nredis 的服务器进程就是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行像 servercron 函数这样需要定时运行的函数\n\n因为服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到 aof buf 缓冲区里面，所以在服务器每次结束一个事件循环之前，它都会调用 flushappendonlyfile 函数，考虑是否需要将 aof buf 缓冲区中的内容写入和保存到 aof 文件里面，这个过程的伪代码如下\n\ndef eventloop():\n\twhile true:\n        # 在处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中\n\t\tprocessfileevents()\n\t\tpricesstimeevents()\n        # 考虑是否将 aof_buf 中的内容写入和保存到 aof 文件里\n\t\tflushappendonlyfile();\n\n\nflushappendonlyfile 的行为由在 redis.conf 中的 appendfsync选项的值来决定\n\nappendfsync 选项的值   flushappendonlyfile 函数行为\nalways             将 aof 缓冲区中的所有内容写入并同步到 aof 文件。\neverysec           将 aof 缓冲区中的所有内容写入到 aof 文件，如果上次同步 aof 文件的时间距离现在超过一秒钟，那么再次对\n                   aof 文件进行同步，并且这个同步操作是由一个线程专门负责执行的。\nno                 将 aof 缓冲区中的所有内容写入到 aof 文件，但并不对 aof 文件进行同步，何时同步由操作系统来决定。\n\n默认值是 everysec\n\n文件的写入和同步\n\n为了提高文件的写入效率，在现代操作系统中，当用户调用 write 函数，将一些数据写入到文件的时候，操作系统通常会将写入数据暂时保存在一个内存缓冲区里面等到缓冲区的空间被填满、或者超过了指定的时限之后，才真正地将缓冲区中的数据写入到磁盘里面。 这种做法虽然提高了效率，但也为写入数据带来了安全问题，因为如果计算机发生停机，那么保存在内存缓冲区里面的写入数据将会丢失。 为此，系统提供了 fsync 和 fdatasync 两个同步函数，它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里面，从而确保写入数据的安全性\n\n如果这时 flushappendonlyfile 函数被调用，假设服务器当前 appendfsyne 选项的值为 everysec，并且距离上次同步 aof 文件已经超过一秒钟，那么服务器会先将 aof buf 中的内容写人到 aof 文件中，然后再对 aof 文件进行同步。\n\n\n# 持久化的效率和安全性\n\n服务器配置 appendfsync 选项的值直接决定 aof 持久化功能的效率和安全性。\n\n * 当 appendfsync 的值为 always 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 aof 文件，并且同步 aof 文件，所以 always 的效率是 appendfsync 选项三个值当中最慢的一个，但从安全性来说，always 也是最安全的，因为即使出现故障停机，aof 持久化也只会丢失一个事件循环中所产生的命令数据\n * 当 appendfsync 的值为 everysec 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 aof 文件，并且每隔一秒就要在子线程中对 aof 文件进行一次同步。从效率上来讲，everysec 模式足够快，并且就算出现故障停机，数据库也只丢失一秒钟的命令数据。\n * 当 appendfsync 的值为 no 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 aof 文件，至于何时对 aof 文件进行同步，则由操作系统控制。因为处于 no 模式下的 flushappendonlyfile 调用无须执行同步操作，所以该模式下的 aof 文件写入速度总是最快的，不过因为这种模式会在系统缓存中积累一段时间的写入数据，所以该模式的单次同步时长通常是三种模式中时间最长的。从平摊操作的角度来看，no 模式和 everysec 模式的效率类似，当出现故障停机时，使用 no 模式的服务器将丢失上次同步 aof 文件之后的所有写命令数据\n\n# 如果 redis 宕机了，操作系统没有宕机，会有数据丢失吗？\n\n不一定 因为写入了 系统缓存 操作系统如果刷盘成功 就不会有丢失，redis 宕机不影响操作系统刷盘\n\n# 为什么在 always 下也可能会丢失一个事件循环中所产生的数据？\n\n如果客户端没有收到 ok 响应，可能会丢失一条数据\n\n但是\n\n在 appendfsync 为 always 模式下，如果客户端收到 redis 返回的 ok 响应，意味着该命令的结果已经成功写入到 aof 文件并同步到磁盘\n\n * 在 always 模式下，redis 在每个命令执行后都会立刻将该命令追加到 aof_buf 缓冲区，然后立即将缓冲区中的内容写入 aof 文件，并执行 fsync 操作（即将数据同步到磁盘）\n * 当 fsync 成功完成后，redis 才会返回 ok 响应给客户端\n\n因此，在 always 模式下，如果客户端收到 ok 响应，意味着：\n\n 1. 命令已经被执行。\n 2. 命令的结果已经成功追加到 aof 文件。\n 3. aof 文件已经通过 fsync 操作将数据同步到了磁盘。\n\n正因为 always 模式确保每条命令在返回 ok 之前都已经被同步到磁盘，所以从客户端的视角来看，只要收到了 ok，就可以认为该数据已经被持久化到磁盘，不会因为 redis 崩溃或服务器断电等原因丢失\n\n\n# aof 文件载入\n\n\n\n 1. 创建一个不带网络连接的伪客户端\n 2. 从 aof 文件中分析并读取出一条写命令\n 3. 使用伪客户端执行被读出的写命令\n 4. 重复 2 3，知道所有写命令被处理完毕为止\n\n\n# aof 重写\n\n\n# aof 重写函数与触发时机\n\n首先，实现 aof 重写的函数是 rewriteappendonlyfilebackground，它是在aof.c文件中实现的。在这个函数中，会调用 fork 函数创建一个 aof 重写子进程，来实际执行重写操作。关于这个函数的具体实现，我稍后会给你详细介绍。这里呢，我们先来看看，这个函数会被哪些函数调用，这样我们就可以了解 aof 重写的触发时机了。\n\n实际上，rewriteappendonlyfilebackground 函数一共会在三个函数中被调用。\n\n**第一个是 bgrewriteaofcommand 函数。**这个函数是在 aof.c 文件中实现的，对应了我们在 redis server 上执行 bgrewriteaof 命令，也就是说，我们手动触发了 aof rewrite 的执行。\n\n不过，即使我们手动执行了 bgrewriteaof 命令，bgrewriteaofcommand 函数也会根据以下两个条件，来判断是否实际执行 aof 重写。\n\n * **条件一：当前是否已经有 aof 重写的子进程正在执行。**如果有的话，那么 bgrewriteaofcommand 函数就不再执行 aof 重写了。\n * **条件二：当前是否有创建 rdb 的子进程正在执行。**如果有的话，bgrewriteaofcommand 函数会把全局变量 server 的 aof_rewrite_scheduled 成员变量设置为 1，这个标志表明 redis server 已经将 aof 重写设为待调度运行，等后续条件满足时，它就会实际执行 aof 重写（我们一会儿就会看到，当 aof_rewrite_scheduled 设置为 1 以后，redis server 会在哪些条件下实际执行重写操作）。\n\n所以这也就是说，只有当前既没有 aof 重写子进程也没有 rdb 子进程，bgrewriteaofcommand 函数才会立即调用 rewriteappendonlyfilebackground 函数，实际执行 aof 重写。\n\n以下代码展示了 bgrewriteaofcommand 函数的基本执行逻辑，你可以看下。\n\nvoid bgrewriteaofcommand(client *c) {\n    if (server.aof_child_pid != -1) {\n        .. //有aof重写子进程，因此不执行重写\n    } else if (server.rdb_child_pid != -1) {\n        server.aof_rewrite_scheduled = 1; //有rdb子进程，将aof重写设置为待调度运行\n        ...\n    } else if (rewriteappendonlyfilebackground() == c_ok) { //实际执行aof重写\n        ...\n    }\n    ...\n}\n\n\n**第二个是 startappendonly 函数。**这个函数也是在 aof.c 文件中实现的，它本身会被 configsetcommand 函数（在config.c文件中）和 restartaofaftersync 函数（在replication.c文件中）调用。\n\n首先，对于 configsetcommand 函数来说，它对应了我们在 redis 中执行 config 命令启用 aof 功能，如下所示：\n\nconfig set appendonly yes\n\n\n这样，一旦 aof 功能启用后，configsetcommand 函数就会调用 startappendonly 函数，执行一次 aof 重写。\n\n而对于 restartaofaftersync 函数来说，它会在主从节点的复制过程中被调用。简单来说，就是当主从节点在进行复制时，如果从节点的 aof 选项被打开，那么在加载解析 rdb 文件时，aof 选项就会被关闭。然后，无论从节点是否成功加载了 rdb 文件，restartaofaftersync 函数都会被调用，用来恢复被关闭的 aof 功能。\n\n那么在这个过程中，restartaofaftersync 函数就会调用 startappendonly 函数，并进一步调用 rewriteappendonlyfilebackground 函数，来执行一次 aof 重写。\n\n这里你要注意，和 bgrewriteaofcommand 函数类似，startappendonly 函数也会判断当前是否有 rdb 子进程在执行，如果有的话，它会将 aof 重写设置为待调度执行。除此之外，如果 startappendonly 函数检测到有 aof 重写子进程在执行，那么它就会把该子进程先 kill 掉，然后再调用 rewriteappendonlyfilebackground 函数进行 aof 重写。\n\n所以到这里，我们其实可以发现，无论是 bgrewriteaofcommand 函数还是 startappendonly 函数，当它们检测到有 rdb 子进程在执行的时候，就会把 aof_rewrite_scheduled 变量设置为 1，这表示 aof 重写操作将在条件满足时再被执行。\n\n**那么，redis server 什么时候会再检查 aof 重写操作的条件是否满足呢？**这就和 rewriteappendonlyfilebackground 函数被调用的第三个函数，servercron 函数相关了。\n\n**第三个是 servercron 函数。**在 redis server 运行时，servercron 函数是会被周期性执行的。然后它在执行的过程中，会做两次判断来决定是否执行 aof 重写。\n\n首先，servercron 函数会检测当前是否没有 rdb 子进程和 aof 重写子进程在执行，并检测是否有 aof 重写操作被设置为了待调度执行，也就是 aof_rewrite_scheduled 变量值为 1。\n\n如果这三个条件都满足，那么 servercron 函数就会调用 rewriteappendonlyfilebackground 函数来执行 aof 重写。servercron 函数里面的这部分执行逻辑如下所示：\n\n//如果没有 rdb 子进程，也没有 aof 重写子进程，并且 aof 重写被设置为待调度执行，那么调用 rewriteappendonlyfilebackground 函数进行 aof 重写\n\n//如果没有rdb子进程，也没有aof重写子进程，并且aof重写被设置为待调度执行，那么调用rewriteappendonlyfilebackground函数进行aof重写\nif (server.rdb_child_pid == -1 && server.aof_child_pid == -1 &&\n        server.aof_rewrite_scheduled)\n{\n        rewriteappendonlyfilebackground();\n}\n\n\n事实上，这里的代码也回答了我们刚才提到的问题：待调度执行的 aof 重写会在什么时候执行？\n\n其实，如果 aof 重写没法立即执行的话，我们也不用担心。因为只要 aof_rewrite_scheduled 变量被设置为 1 了，那么 servercron 函数就默认会每 100 毫秒执行并检测这个变量值。所以，如果正在执行的 rdb 子进程和 aof 重写子进程结束了之后，被调度执行的 aof 重写就可以很快得到执行。\n\n其次，即使 aof 重写操作没有被设置为待调度执行，servercron 函数也会周期性判断是否需要执行 aof 重写。这里的判断条件主要有三个，分别是 aof 功能已启用、aof 文件大小比例超出阈值，以及 aof 文件大小绝对值超出阈值。\n\n这样一来，当这三个条件都满足时，并且也没有 rdb 子进程和 aof 子进程在运行的话，此时，servercron 函数就会调用 rewriteappendonlyfilebackground 函数执行 aof 重写。这部分的代码逻辑如下所示：\n\n//如果 aof 功能启用、没有 rdb 子进程和 aof 重写子进程在执行、aof 文件大小比例设定了阈值，以及 aof 文件大小绝对值超出了阈值，那么，进一步判断 aof 文件大小比例是否超出阈值\n\n//如果aof功能启用、没有rdb子进程和aof重写子进程在执行、aof文件大小比例设定了阈值，以及aof文件大小绝对值超出了阈值，那么，进一步判断aof文件大小比例是否超出阈值\nif (server.aof_state == aof_on && server.rdb_child_pid == -1 && server.aof_child_pid == -1 && server.aof_rewrite_perc && server.aof_current_size > server.aof_rewrite_min_size) {\n   //计算aof文件当前大小超出基础大小的比例\n   long long base = server.aof_rewrite_base_size ? server.aof_rewrite_base_size : 1;\n   long long growth = (server.aof_current_size*100/base) - 100;\n   //如果aof文件当前大小超出基础大小的比例已经超出预设阈值，那么执行aof重写\n   if (growth >= server.aof_rewrite_perc) {\n      ...\n      rewriteappendonlyfilebackground();\n   }\n}\n\n\n那么，从这里的代码中，你会看到，为了避免 aof 文件过大导致占用过多的磁盘空间，以及增加恢复时长，你其实可以通过设置 redis.conf 文件中的以下两个阈值，来让 redis server 自动重写 aof 文件。\n\n * auto-aof-rewrite-percentage：aof 文件大小超出基础大小的比例，默认值为 100%，即超出 1 倍大小。\n * auto-aof-rewrite-min-size：aof 文件大小绝对值的最小值，默认为 64mb。\n\n好了，到这里，我们就了解了 aof 重写的四个触发时机，这里我也给你总结下，方便你回顾复习。\n\n * 时机一：bgrewriteaof 命令被执行。\n * 时机二：主从复制完成 rdb 文件解析和加载（无论是否成功）。\n * 时机三：aof 重写被设置为待调度执行。\n * 时机四：aof 被启用，同时 aof 文件的大小比例超出阈值，以及 aof 文件的大小绝对值超出阈值。\n\n另外，这里你还需要注意，在这四个时机下，其实都不能有正在执行的 rdb 子进程和 aof 重写子进程，否则的话，aof 重写就无法执行了。\n\n所以接下来，我们就来学习下 aof 重写的基本执行过程。\n\n\n# aof 重写的基本过程\n\n首先，我们再来看下刚才介绍的 rewriteappendonlyfilebackground 函数。这个函数的主体逻辑比较简单，一方面，它会通过调用 fork 函数创建一个子进程，然后在子进程中调用 rewriteappendonlyfile 函数进行 aof 文件重写。\n\nrewriteappendonlyfile 函数是在 aof.c 文件中实现的。它主要会调用 rewriteappendonlyfilerio 函数（在 aof.c 文件中）来完成 aof 日志文件的重写。具体来说，就是 rewriteappendonlyfilerio 函数会遍历 redis server 的每一个数据库，把其中的每个键值对读取出来，然后记录该键值对类型对应的插入命令，以及键值对本身的内容。\n\n比如，如果读取的是一个 string 类型的键值对，那么 rewriteappendonlyfilerio 函数，就会记录 set 命令和键值对本身内容；而如果读取的是 set 类型键值对，那么它会记录 sadd 命令和键值对内容。这样一来，当需要恢复 redis 数据库时，我们重新执行一遍 aof 重写日志中记录的命令操作，就可以依次插入所有键值对了。\n\n另一方面，在父进程中，这个 rewriteappendonlyfilebackground 函数会把 aof_rewrite_scheduled 变量设置为 0，同时记录 aof 重写开始的时间，以及记录 aof 子进程的进程号。\n\n此外，rewriteappendonlyfilebackground 函数还会调用 updatedictresizepolicy 函数，禁止在 aof 重写期间进行 rehash 操作。这是因为 rehash 操作会带来较多的数据移动操作，对于 aof 重写子进程来说，这就意味着父进程中的内存修改会比较多。因此，aof 重写子进程就需要执行更多的写时复制，进而完成 aof 文件的写入，这就会给 redis 系统的性能造成负面影响。\n\n以下代码就展示了 rewriteappendonlyfilebackground 函数的基本执行逻辑，你可以看下。\n\nint rewriteappendonlyfilebackground(void) {\n   ...\n   if ((childpid = fork()) == 0) {  //创建子进程\n      ...\n      //子进程调用rewriteappendonlyfile进行aof重写\n      if (rewriteappendonlyfile(tmpfile) == c_ok) {\n            size_t private_dirty = zmalloc_get_private_dirty(-1);\n            ...\n            exitfromchild(0);\n        } else {\n            exitfromchild(1);\n        }\n   }\n   else{ //父进程执行的逻辑\n      ...\n      server.aof_rewrite_scheduled = 0;\n      server.aof_rewrite_time_start = time(null);\n      server.aof_child_pid = childpid; //记录重写子进程的进程号\n      updatedictresizepolicy(); //关闭rehash功能\n}\n\n\n而从这里，你可以看到，aof 重写和 rdb 创建是比较类似的，它们都会创建一个子进程来遍历所有的数据库，并把数据库中的每个键值对记录到文件中。不过，aof 重写和 rdb 文件又有两个不同的地方：\n\n * 一是，aof 文件中是以“命令 + 键值对”的形式，来记录每个键值对的插入操作，而 rdb 文件记录的是键值对数据本身；\n * 二是，在 aof 重写或是创建 rdb 的过程中，主进程仍然可以接收客户端写请求。不过，因为 rdb 文件只需要记录某个时刻下数据库的所有数据就行，而 aof 重写则需要尽可能地把主进程收到的写操作，也记录到重写的日志文件中。所以，aof 重写子进程就需要有相应的机制来和主进程进行通信，以此来接收主进程收到的写操作。\n\n下图就展示了 rewriteappendonlyfilebackground 函数执行的基本逻辑、主进程和 aof 重写子进程各自执行的内容，以及主进程和子进程间的通信过程，你可以再来整体回顾下。\n\n\n\n到这里，我们就大概掌握了 aof 重写的基本执行过程。但是在这里，你可能还会有疑问，比如说，aof 重写的子进程和父进程，它们之间的通信过程是怎么样的呢？\n\n其实，这个通信过程是通过操作系统的管道机制（pipe）来实现的\n\n在 aof 重写时，主进程仍然在接收客户端写操作，那么这些新写操作会记录到 aof 重写日志中吗？如果需要记录的话，重写子进程又是通过什么方式向主进程获取这些写操作的呢？\n\necho 接下来就带你了解下 aof 重写过程中所使用的管道机制，以及主进程和重写子进程的交互过程\n\n * 一方面，你就可以了解 aof 重写日志包含的写操作的完整程度，当你要使用 aof 日志恢复 redis 数据库时，就知道 aof 能恢复到的程度是怎样的\n * 一方面，因为 aof 重写子进程就是通过操作系统提供的管道机制，来和 redis 主进程交互的，所以学完这节课之后，你还可以掌握管道技术，从而用来实现进程间的通信\n\n好了，接下来，我们就先来了解下管道机制\n\n\n# 深入重写缓冲区\n\n\n# 如何使用管道进行父子进程间通信？\n\n首先我们要知道，当进程 a 通过调用 fork 函数创建一个子进程 b，然后进程 a 和 b 要进行通信时，我们通常都需要依赖操作系统提供的通信机制，而管道（pipe）就是一种用于父子进程间通信的常用机制。\n\n具体来说，管道机制在操作系统内核中创建了一块缓冲区，父进程 a 可以打开管道，并往这块缓冲区中写入数据。同时，子进程 b 也可以打开管道，从这块缓冲区中读取数据。这里，你需要注意的是，进程每次往管道中写入数据时，只能追加写到缓冲区中当前数据所在的尾部，而进程每次从管道中读取数据时，只能从缓冲区的头部读取数据。\n\n其实，管道创建的这块缓冲区就像一个先进先出的队列一样，写数据的进程写到队列尾部，而读数据的进程则从队列头读取。下图就展示了两个进程使用管道进行数据通信的过程，你可以看下。\n\n\n\n好了，了解了管道的基本功能后，我们再来看下使用管道时需要注意的一个关键点。管道中的数据在一个时刻只能向一个方向流动，这也就是说，如果父进程 a 往管道中写入了数据，那么此时子进程 b 只能从管道中读取数据。类似的，如果子进程 b 往管道中写入了数据，那么此时父进程 a 只能从管道中读取数据。而如果父子进程间需要同时进行数据传输通信，我们就需要创建两个管道了。\n\n下面，我们就来看下怎么用代码实现管道通信。这其实是和操作系统提供的管道的系统调用 pipe 有关，pipe 的函数原型如下所示：\n\nint pipe(int pipefd[2]);\n\n\n你可以看到，pipe 的参数是一个数组 pipefd，表示的是管道的文件描述符。这是因为进程在往管道中写入或读取数据时，其实是使用 write 或 read 函数的，而 write 和 read 函数需要通过文件描述符才能进行写数据和读数据操作。\n\n数组 pipefd 有两个元素 pipefd[0]和 pipefd[1]，分别对应了管道的读描述符和写描述符。这也就是说，当进程需要从管道中读数据时，就需要用到 pipefd[0]，而往管道中写入数据时，就使用 pipefd[1]。\n\n这里我写了一份示例代码，展示了父子进程如何使用管道通信，你可以看下。\n\nint main()\n{\n    int fd[2], nr = 0, nw = 0;\n    char buf[128];\n    pipe(fd);\n    pid = fork();\n\n  if(pid == 0) {\n      //子进程调用read从fd[0]描述符中读取数据\n        printf("child process wait for message\\n");\n        nr = read(fds[0], buf, sizeof(buf))\n        printf("child process receive %s\\n", buf);\n  }else{\n       //父进程调用write往fd[1]描述符中写入数据\n        printf("parent process send message\\n");\n        strcpy(buf, "hello from parent");\n        nw = write(fd[1], buf, sizeof(buf));\n        printf("parent process send %d bytes to child.\\n", nw);\n    }\n    return 0;\n}\n\n\n从代码中，你可以看到，在父子进程进行管道通信前，我们需要在代码中定义用于保存读写描述符的数组 fd，然后调用 pipe 系统创建管道，并把数组 fd 作为参数传给 pipe 函数。紧接着，在父进程的代码中，父进程会调用 write 函数往管道文件描述符 fd[1]中写入数据，另一方面，子进程调用 read 函数从管道文件描述符 fd[0]中读取数据。\n\n这里，为了便于你理解，我也画了一张图，你可以参考。\n\n\n\n好了，现在你就了解了如何使用管道来进行父子进程的通信了。那么下面，我们就来看下在 aof 重写过程中，重写子进程是如何用管道和主进程（也就是它的父进程）进行通信的。\n\n\n# aof 重写子进程如何使用管道和父进程交互？\n\n我们先来看下在 aof 重写过程中，都创建了几个管道。\n\n这实际上是 aof 重写函数 rewriteappendonlyfilebackground 在执行过程中，通过调用 aofcreatepipes 函数来完成的，如下所示：\n\nint rewriteappendonlyfilebackground(void) {\n…\nif (aofcreatepipes() != c_ok) return c_err;\n…\n}\n\n\n这个 aofcreatepipes 函数是在aof.c文件中实现的，它的逻辑比较简单，可以分成三步。\n\n第一步，aofcreatepipes 函数创建了包含 6 个文件描述符元素的数组 fds。就像我刚才给你介绍的，每一个管道会对应两个文件描述符，所以，数组 fds 其实对应了 aof 重写过程中要用到的三个管道。紧接着，aofcreatepipes 函数就调用 pipe 系统调用函数，分别创建三个管道。\n\n这部分代码如下所示，你可以看下。\n\nint aofcreatepipes(void) {\n    int fds[6] = {-1, -1, -1, -1, -1, -1};\n    int j;\n    if (pipe(fds) == -1) goto error; /* parent -> children data. */\n    if (pipe(fds+2) == -1) goto error; /* children -> parent ack. */\n  if (pipe(fds+4) == -1) goto error;\n  …}\n}\n\n\n第二步，aofcreatepipes 函数会调用 anetnonblock 函数（在anet.c文件中），将 fds\n\n数组的第一和第二个描述符（fds[0]和 fds[1]）对应的管道设置为非阻塞。然后，aofcreatepipes 函数会调用 aecreatefileevent 函数，在数组 fds 的第三个描述符 (fds[2]) 上注册了读事件的监听，对应的回调函数是 aofchildpipereadable。aofchildpipereadable 函数也是在 aof.c 文件中实现的，我稍后会给你详细介绍它。\n\nint aofcreatepipes(void) {\n…\nif (anetnonblock(null,fds[0]) != anet_ok) goto error;\nif (anetnonblock(null,fds[1]) != anet_ok) goto error;\nif (aecreatefileevent(server.el, fds[2], ae_readable, aofchildpipereadable, null) == ae_err) goto error;\n…\n}\n\n\n这样，在完成了管道创建、管道设置和读事件注册后，最后一步，aofcreatepipes 函数会将数组 fds 中的六个文件描述符，分别复制给 server 变量的成员变量，如下所示：\n\nint aofcreatepipes(void) {\n…\nserver.aof_pipe_write_data_to_child = fds[1];\nserver.aof_pipe_read_data_from_parent = fds[0];\nserver.aof_pipe_write_ack_to_parent = fds[3];\nserver.aof_pipe_read_ack_from_child = fds[2];\nserver.aof_pipe_write_ack_to_child = fds[5];\nserver.aof_pipe_read_ack_from_parent = fds[4];\n…\n}\n\n\n在这一步中，我们就可以从 server 变量的成员变量名中，看到 aofcreatepipes 函数创建的三个管道，以及它们各自的用途。\n\n * fds[0]和 fds[1]：对应了主进程和重写子进程间用于传递操作命令的管道，它们分别对应读描述符和写描述符。\n * fds[2]和 fds[3]：对应了重写子进程向父进程发送 ack 信息的管道，它们分别对应读描述符和写描述符。\n * fds[4]和 fds[5]：对应了父进程向重写子进程发送 ack 信息的管道，它们分别对应读描述符和写描述符。\n\n下图也展示了 aofcreatepipes 函数的基本执行流程，你可以再回顾下。\n\n\n\n好了，了解了 aof 重写过程中的管道个数和用途后，下面我们再来看下这些管道具体是如何使用的。\n\n# 操作命令传输管道的使用\n\n实际上，当 aof 重写子进程在执行时，主进程还会继续接收和处理客户端写请求。这些写操作会被主进程正常写入 aof 日志文件，这个过程是由 feedappendonlyfile 函数（在 aof.c 文件中）来完成。\n\nfeedappendonlyfile 函数在执行的最后一步，会判断当前是否有 aof 重写子进程在运行。如果有的话，它就会调用 aofrewritebufferappend 函数（在 aof.c 文件中），如下所示：\n\nif (server.aof_child_pid != -1)\n        aofrewritebufferappend((unsigned char*)buf,sdslen(buf));\n\n\naofrewritebufferappend 函数的作用是将参数 buf，追加写到全局变量 server 的 aof_rewrite_buf_blocks 这个列表中。\n\n这里，你需要注意的是，参数 buf 是一个字节数组，feedappendonlyfile 函数会将主进程收到的命令操作写入到 buf 中。而 aof_rewrite_buf_blocks 列表中的每个元素是 aofrwblock 结构体类型，这个结构体中包括了一个字节数组，大小是 aof_rw_buf_block_size，默认值是 10mb。此外，aofrwblock 结构体还记录了字节数组已经使用的空间和剩余可用的空间。\n\n以下代码展示了 aofrwblock 结构体的定义，你可以看下。\n\ntypedef struct aofrwblock {\n    unsigned long used, free; //buf数组已用空间和剩余可用空间\n    char buf[aof_rw_buf_block_size]; //宏定义aof_rw_buf_block_size默认为10mb\n} aofrwblock;\n\n\n这样一来，aofrwblock 结构体就相当于是一个 10mb 的数据块，记录了 aof 重写期间主进程收到的命令，而 aof_rewrite_buf_blocks 列表负责将这些数据块连接起来。当 aofrewritebufferappend 函数执行时，它会从 aof_rewrite_buf_blocks 列表中取出一个 aofrwblock 类型的数据块，用来记录命令操作。\n\n当然，如果当前数据块中的空间不够保存参数 buf 中记录的命令操作，那么 aofrewritebufferappend 函数就会再分配一个 aofrwblock 数据块。\n\n好了，当 aofrewritebufferappend 函数将命令操作记录到 aof_rewrite_buf_blocks 列表中之后，它还会检查 aof_pipe_write_data_to_child 管道描述符上是否注册了写事件，这个管道描述符就对应了我刚才给你介绍的 fds[1]。\n\n如果没有注册写事件，那么 aofrewritebufferappend 函数就会调用 aecreatefileevent 函数，注册一个写事件，这个写事件会监听 aof_pipe_write_data_to_child 这个管道描述符，也就是主进程和重写子进程间的操作命令传输管道。\n\n当这个管道可以写入数据时，写事件对应的回调函数 aofchildwritediffdata（在 aof.c 文件中）就会被调用执行。这个过程你可以参考下面的代码：\n\nvoid aofrewritebufferappend(unsigned char *s, unsigned long len) {\n...\n//检查aof_pipe_write_data_to_child描述符上是否有事件\nif (aegetfileevents(server.el,server.aof_pipe_write_data_to_child) == 0) {\n     //如果没有注册事件，那么注册一个写事件，回调函数是aofchildwritediffdata\n     aecreatefileevent(server.el, server.aof_pipe_write_data_to_child,\n            ae_writable, aofchildwritediffdata, null);\n}\n...}\n\n\n其实，刚才我介绍的写事件回调函数 aofchildwritediffdata，它的主要作用是从 aof_rewrite_buf_blocks 列表中逐个取出数据块，然后通过 aof_pipe_write_data_to_child 管道描述符，将数据块中的命令操作通过管道发给重写子进程，这个过程如下所示：\n\nvoid aofchildwritediffdata(aeeventloop *el, int fd, void *privdata, int mask) {\n...\nwhile(1) {\n   //从aof_rewrite_buf_blocks列表中取出数据块\n   ln = listfirst(server.aof_rewrite_buf_blocks);\n   block = ln ? ln->value : null;\n   if (block->used > 0) {\n      //调用write将数据块写入主进程和重写子进程间的管道\n      nwritten = write(server.aof_pipe_write_data_to_child,\n                             block->buf,block->used);\n      if (nwritten <= 0) return;\n            ...\n        }\n ...}}\n\n\n好了，这样一来，你就了解了主进程其实是在正常记录 aof 日志时，将收到的命令操作写入 aof_rewrite_buf_blocks 列表中的数据块，然后再通过 aofchildwritediffdata 函数将记录的命令操作通过主进程和重写子进程间的管道发给子进程。\n\n下图也展示了这个过程，你可以再来回顾下。\n\n\n\n然后，我们接着来看下重写子进程，是如何从管道中读取父进程发送的命令操作的。\n\n这实际上是由 aofreaddifffromparent 函数（在 aof.c 文件中）来完成的。这个函数会使用一个 64kb 大小的缓冲区，然后调用 read 函数，读取父进程和重写子进程间的操作命令传输管道中的数据。以下代码也展示了 aofreaddifffromparent 函数的基本执行流程，你可以看下。\n\nssize_t aofreaddifffromparent(void) {\n    char buf[65536]; //管道默认的缓冲区大小\n    ssize_t nread, total = 0;\n    //调用read函数从aof_pipe_read_data_from_parent中读取数据\n    while ((nread =\n      read(server.aof_pipe_read_data_from_parent,buf,sizeof(buf))) > 0) {\n        server.aof_child_diff = sdscatlen(server.aof_child_diff,buf,nread);\n        total += nread;\n    }\n    return total;\n}\n\n\n那么，从代码中，你可以看到 aofreaddifffromparent 函数会通过 aof_pipe_read_data_from_parent 描述符读取数据。然后，它会将读取的操作命令追加到全局变量 server 的 aof_child_diff 字符串中。而在 aof 重写函数 rewriteappendonlyfile 的执行过程最后，aof_child_diff 字符串会被写入 aof 重写日志文件，以便我们在使用 aof 重写日志时，能尽可能地恢复重写期间收到的操作。\n\n这个 aof_child_diff 字符串写入重写日志文件的过程，你可以参考下面给出的代码：\n\nint rewriteappendonlyfile(char *filename) {\n...\n//将aof_child_diff中累积的操作命令写入aof重写日志文件\nif (riowrite(&aof,server.aof_child_diff,sdslen(server.aof_child_diff)) == 0)\n        goto werr;\n...\n}\n\n\n所以也就是说，aofreaddifffromparent 函数实现了重写子进程向主进程读取操作命令。那么在这里，我们还需要搞清楚的问题是：aofreaddifffromparent 函数会在哪里被调用，也就是重写子进程会在什么时候从管道中读取主进程收到的操作。\n\n其实，aofreaddifffromparent 函数一共会被以下三个函数调用。\n\n * rewriteappendonlyfilerio 函数：这个函数是由重写子进程执行的，它负责遍历 redis 每个数据库，生成 aof 重写日志，在这个过程中，它会不时地调用 aofreaddifffromparent 函数。\n * rewriteappendonlyfile 函数：这个函数是重写日志的主体函数，也是由重写子进程执行的，它本身会调用 rewriteappendonlyfilerio 函数。此外，它在调用完 rewriteappendonlyfilerio 函数后，还会多次调用 aofreaddifffromparent 函数，以尽可能多地读取主进程在重写日志期间收到的操作命令。\n * rdbsaverio 函数：这个函数是创建 rdb 文件的主体函数。当我们使用 aof 和 rdb 混合持久化机制时，这个函数也会调用 aofreaddifffromparent 函数。\n\n从这里，我们可以看到，redis 源码在实现 aof 重写过程中，其实会多次让重写子进程向主进程读取新收到的操作命令，这也是为了让重写日志尽可能多地记录最新的操作，提供更加完整的操作记录。\n\n最后，我们再来看下重写子进程和主进程间用来传递 ack 信息的两个管道的使用。\n\n# ack 管道的使用\n\n刚才在介绍主进程调用 aofcreatepipes 函数创建管道时，你就了解到了，主进程会在 aof_pipe_read_ack_from_child 管道描述符上注册读事件。这个描述符对应了重写子进程向主进程发送 ack 信息的管道。同时，这个描述符是一个读描述符，表示主进程从管道中读取 ack 信息。\n\n其实，重写子进程在执行 rewriteappendonlyfile 函数时，这个函数在完成日志重写，以及多次向父进程读取操作命令后，就会调用 write 函数，向 aof_pipe_write_ack_to_parent 描述符对应的管道中写入“！”，这就是重写子进程向主进程发送 ack 信号，让主进程停止发送收到的新写操作。这个过程如下所示：\n\nint rewriteappendonlyfile(char *filename) {\n...\nif (write(server.aof_pipe_write_ack_to_parent,"!",1) != 1) goto werr;\n...}\n\n\n一旦重写子进程向主进程发送 ack 信息的管道中有了数据，aof_pipe_read_ack_from_child 管道描述符上注册的读事件就会被触发，也就是说，这个管道中有数据可以读取了。那么，aof_pipe_read_ack_from_child 管道描述符上，注册的回调函数 aofchildpipereadable（在 aof.c 文件中）就会执行。\n\n这个函数会判断从 aof_pipe_read_ack_from_child 管道描述符读取的数据是否是“！”，如果是的话，那它就会调用 write 函数，往 aof_pipe_write_ack_to_child 管道描述符上写入“！”，表示主进程已经收到重写子进程发送的 ack 信息，同时它会给重写子进程回复一个 ack 信息。这个过程如下所示：\n\nvoid aofchildpipereadable(aeeventloop *el, int fd, void *privdata, int mask) {\n...\nif (read(fd,&byte,1) == 1 && byte == \'!\') {\n   ...\n   if (write(server.aof_pipe_write_ack_to_child,"!",1) != 1) { ...}\n}\n...\n}\n\n\n好了，到这里，我们就了解了，重写子进程在完成日志重写后，是先给主进程发送 ack 信息。然后主进程在 aof_pipe_read_ack_from_child 描述符上监听读事件发生，并调用 aofchildpipereadable 函数向子进程发送 ack 信息。\n\n最后，重写子进程执行的 rewriteappendonlyfile 函数，会调用 syncread 函数，从 aof_pipe_read_ack_from_parent 管道描述符上，读取主进程发送给它的 ack 信息，如下所示：\n\nint rewriteappendonlyfile(char *filename) {\n...\nif (syncread(server.aof_pipe_read_ack_from_parent,&byte,1,5000) != 1  || byte != \'!\') goto werr\n...\n}\n\n\n下图也展示了 ack 管道的使用过程，你可以再回顾下。\n\n\n\n这样一来，重写子进程和主进程之间就通过两个 ack 管道，相互确认重写过程结束了。\n\n\n# 总结\n\n 1. aof 重写的触发时机。这既包括了我们主动执行 bgrewriteaof 命令，也包括了 redis server 根据 aof 文件大小而自动触发的重写。此外，在主从复制的过程中，从节点也会启动 aof 重写，形成一份完整的 aof 日志，以便后续进行恢复。当然你也要知道，当要触发 aof 重写时，redis server 是不能运行 rdb 子进程和 aof 重写子进程的。\n\n 2. aof 重写的基本执行过程。aof 重写和 rdb 创建的过程类似，它也是创建了一个子进程来完成重写工作。这是因为 aof 重写操作，实际上需要遍历 redis server 上的所有数据库，把每个键值对以插入操作的形式写入日志文件，而日志文件又要进行写盘操作。所以，redis 源码使用子进程来实现 aof 重写，这就避免了阻塞主线程，也减少了对 redis 整体性能的影响。\n\n 3. 注管道机制的使用\n\n 4. 主进程和重写子进程使用管道通信的过程\n\n在这个过程中，aof 重写子进程和主进程是使用了一个操作命令传输管道和两个 ack 信息发送管道。操作命令传输管道是用于主进程写入收到的新操作命令，以及用于重写子进程读取操作命令，而 ack 信息发送管道是在重写结束时，重写子进程和主进程用来相互确认重写过程的结束。最后，重写子进程会进一步将收到的操作命令记录到重写日志文件中。\n\n这样一来，aof 重写过程中主进程收到的新写操作，就不会被遗漏了\n\n * 一方面，这些新写操作会被记录在正常的 aof 日志中\n * 一方面，主进程会将新写操作缓存在 aof_rewrite_buf_blocks 数据块列表中，并通过管道发送给重写子进程。这样，就能尽可能地保证重写日志具有最新、最完整的写操作了\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Redis 中的延迟监控",frontmatter:{title:"Redis 中的延迟监控",date:"2024-09-15T23:27:13.000Z",permalink:"/pages/aa75e9/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E5%9B%9B%E3%80%81%E6%94%AF%E7%BA%BF%E4%BB%BB%E5%8A%A1/20.Redis%20%E4%B8%AD%E7%9A%84%E5%BB%B6%E8%BF%9F%E7%9B%91%E6%8E%A7.html",relativePath:"Redis 系统设计/04.四、支线任务/20.Redis 中的延迟监控.md",key:"v-df3caa06",path:"/pages/aa75e9/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:405},{level:2,title:"延迟监控框架的实现",slug:"延迟监控框架的实现",normalizedTitle:"延迟监控框架的实现",charIndex:760},{level:3,title:"记录事件执行情况的数据结构",slug:"记录事件执行情况的数据结构",normalizedTitle:"记录事件执行情况的数据结构",charIndex:1236},{level:3,title:"如何实现延迟事件的采样？",slug:"如何实现延迟事件的采样",normalizedTitle:"如何实现延迟事件的采样？",charIndex:2273},{level:3,title:"延迟分析和提供应对措施建议",slug:"延迟分析和提供应对措施建议",normalizedTitle:"延迟分析和提供应对措施建议",charIndex:5478},{level:2,title:"慢命令日志的实现",slug:"慢命令日志的实现",normalizedTitle:"慢命令日志的实现",charIndex:7015},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:10252},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10666}],headersStr:"前言 延迟监控框架的实现 记录事件执行情况的数据结构 如何实现延迟事件的采样？ 延迟分析和提供应对措施建议 慢命令日志的实现 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. Redis 是以低延迟著称的数据库，那么当它的响应速度变慢时，如何快速诊断出问题所在？\n 2. 你知道 Redis 的哪些事件可能导致它运行缓慢吗？它们是如何被监控的？\n 3. 当 Redis 的某个命令执行变慢时，我们如何捕捉这些“慢命令”并进行详细分析？\n 4. 在 Redis 延迟监控中，如何通过采样机制高效记录每类事件的执行时长？\n 5. 为什么 Redis 需要为延迟事件和慢命令分别设计不同的监控和日志机制？这两者的核心区别是什么？\n 6. Redis 如何通过“时间序列”的方式保存和分析多次延迟事件的数据，以便为后续问题排查提供更多线索？\n 7. 在延迟监控中，Redis 是如何通过统计延迟事件的最大值、最小值、均值等数据来帮助用户诊断问题的？\n 8. Redis 的慢命令日志能提供哪些关键信息？如何通过这些信息定位到具体的慢命令和它的来源？\n\n\n# 前言\n\nRedis的一个显著特征就是能提供低延迟的数据访问。而如果Redis在运行过程中变慢了，我们就需要有方法能监控到哪些命令执行变慢了。更进一步的需求，就是我们需要有方法监控到，是Redis运行过程中的哪些事件导致Redis变慢了。这样一来，我们就既可以检查这些慢命令，分析它们的操作类型和访问的数据量，进而提出应对方法，也可以检查监控记录的事件，分析事件发生的原因并提出应对方法\n\n那么，为了满足这些需求，我们就需要有一套监控框架\n\n * 一方面能监测导致Redis变慢的事件\n * 另一方面，能监控并记录变慢的命令。\n\n其实，这也是我们在开发后端系统时，经常会面临的一个运维开发需求，也就是如何监控后端系统的运行状态。\n\necho 来带你了解Redis的延迟监控框架和慢命令日志的设计与实现。\n\n\n# 延迟监控框架的实现\n\n实际上，Redis 在运行过程中，以下表格中给出的几类事件都会导致 Redis 变慢，我们通常也把这些事件称为延迟事件。你可以参考表格中的这些事件类型，以及它们在源码中对应的事件名称。\n\n事件类型      源码中对应名称\n命令事件      command、fast-command\nAOF事件     aof-write-pending-fsync, aof-write-active-child,\n          aof-write-alone, aof-fstat, aof-rewrite-diff-write,\n          aof-rename\nfork事件    fork\n过期Key事件   expire-cycie\n缓存替换事件    eviction-del, eviction-cycie\n\n那么针对这些事件，Redis实现了延迟监控框架，通过采样的方式来记录它们的执行情况。当需要排查问题时，延迟监控框架提供了latency history命令，以便运维人员检查这些事件。\n\n下面，我们就来看下记录事件执行情况的数据结构。因为延迟监控框架是在latency.h和latency.c文件中实现的，你也可以在这两个文件中找到相应的数据结构和函数。\n\n\n# 记录事件执行情况的数据结构\n\n首先，Redis是使用了latencySample结构体，来记录延迟事件的采样时间和事件的实际执行时长，这个结构体的代码如下所示：\n\nstruct latencySample {\n    int32_t time;  //事件的采样时间\n    uint32_t latency;  //事件的执行时长（以毫秒为单位）\n};\n\n\n而在latencySample这个结构体基础上，Redis又设计了latencyTimeSeries结构体，这个结构体使用了一个latencySample类型的数组，记录了针对某一类事件的一系列采样结果，这样就可以为分析Redis变慢提供更多的事件信息。\n\nstruct latencyTimeSeries {\n    int idx;  //采样事件数组的写入位置\n    uint32_t max;  //当前事件的最大延迟\n    struct latencySample samples[LATENCY_TS_LEN]; //采样事件数组，记录LATENCY_TS_LEN个采样结果，LATENCY_TS_LEN默认为160\n};\n\n\n另外，也因为延迟监控框架要记录的延迟事件有很多种，所以 Redis 还进一步设计了一个哈希表latency_events，作为全局变量server的一个成员变量，用来记录不同事件的采样结果数组，如下所示：\n\nstruct redisServer {\n   …\n   dict *latency_events;\n   …\n}\n\n\n这个哈希表是在Redis server启动初始化的函数initServer中，通过调用latencyMonitorInit函数来完成创建的，如下所示：\n\nvoid initServer(void) {\n    …\n    latencyMonitorInit();\n}\n \nvoid latencyMonitorInit(void) {\n    server.latency_events = dictCreate(&latencyTimeSeriesDictType,NULL);\n}\n\n\n好，了解了记录延迟事件的数据结构和初始化操作后，我们再来看下事件采样是如何实现的。\n\n\n# 如何实现延迟事件的采样？\n\n延迟事件的采样函数是latencyAddSample，它的函数原型如下所示。它的参数中包含了要记录的事件名称，这实际是对应了latency_events哈希表中的一个哈希项。此外，它的参数中还包括该事件的执行时长。\n\nvoid latencyAddSample(char *event, mstime_t latency)\n\n\nlatencyAddSample函数的执行逻辑并不复杂，主要可以分成三步。\n\n首先，它会根据传入的事件名称，在latency_events哈希表中查找该事件。如果该事件对应的哈希项还不存在，它就会在哈希表中加入该事件，如下所示：\n\n//查找事件对应的哈希项\nstruct latencyTimeSeries *ts = dictFetchValue(server.latency_events,event);\n…\nif (ts == NULL) { //如果哈希项为空，就新建哈希项\n    ts = zmalloc(sizeof(*ts));\n    ts->idx = 0;\n    ts->max = 0;\n    memset(ts->samples,0,sizeof(ts->samples));\n    dictAdd(server.latency_events,zstrdup(event),ts); //在哈希表中插入哈希项\n}\n\n\n然后，latencyAddSample函数会根据传入的事件执行时间，更新当前记录的该类事件的最大执行时间，如下所示：\n\nif (latency > ts->max) ts->max = latency;\n\n\n最后，latencyAddSample函数会实际记录当前的采样结果。\n\n不过在这一步，如果它发现当前的采样结果，和前一个采样结果是在同一秒中获得的，并且如果当前采样结果的事件执行时长，大于前一个采样结果的话，那么latencyAddSample函数就会直接更新前一个采样结果中记录的执行时长了，而不是新插入一个采样结果。\n\n否则的话，latencyAddSample函数才会新插入一个采样结果。这样设计的目的，也是为了避免在同一秒中记录过多的采样结果。\n\n下面的代码展示了latencyAddSample函数实际记录采样结果的逻辑，你可以看下。\n\n//获得同类事件的前一个采样结果\nprev = (ts->idx + LATENCY_TS_LEN - 1) % LATENCY_TS_LEN;\n//如果当前和前一个采样结果在同一秒中\nif (ts->samples[prev].time == now) { \n    //如果当前采用结果的执行时长大于前一个采样结果\n    if (latency > ts->samples[prev].latency) \n        //直接更新前一个采样结果的执行时长\n        ts->samples[prev].latency = latency;\n    return;\n}\n//否则，新插入当前的采样结果\nts->samples[ts->idx].time = time(NULL);\nts->samples[ts->idx].latency = latency;\n\n\n而在这里，你也要注意一点，就是latencyAddSample函数在记录采样结果时，会重复使用采样结果数组latencyTimeSeries。所以，如果采样结果数量超过数组默认大小时，旧的采样结果是会被覆盖掉的。如果你要记录更多的采样结果，就需要扩大latencyTimeSeries数组的长度。\n\n那么，latencyAddSample函数是在什么时候调用进行采样的呢?\n\n其实，latencyAddSample函数是被封装在了latencyAddSampleIfNeeded函数中。在latencyAddSampleIfNeeded函数中，它只会在事件执行时长超过latency-monitor-threshold配置项的值时，才调用latencyAddSample函数记录采样结果。你可以看看下面给出的latencyAddSampleIfNeeded函数定义。\n\nlatencyAddSampleIfNeeded(event,var){\n    if (server.latency_monitor_threshold &&  (var) >= server.latency_monitor_threshold)\n          latencyAddSample((event),(var));\n}\n    \n\n\n而latencyAddSampleIfNeeded函数，实际上会在刚才介绍的延迟事件发生时被调用。这里我来给你举两个例子。\n\n比如，当Redis命令通过call函数（在server.c文件中）执行时，call函数就会调用latencyAddSampleIfNeeded函数进行采样，如下所示：\n\nif (flags & CMD_CALL_SLOWLOG && c->cmd->proc != execCommand) {\n    //根据命令数据结构中flags的CMD_FAST标记，决定当前是fast-command事件还是command事件\n    char *latency_event = (c->cmd->flags & CMD_FAST) ?\n        "fast-command" : "command";\n    latencyAddSampleIfNeeded(latency_event,duration/1000);\n    …\n}\n\n\n再比如，当Redis调用flushAppendOnlyFile函数写AOF文件时，如果AOF文件刷盘的配置项是AOF_FSYNC_ALWAYS，那么flushAppendOnlyFile函数就会调用latencyAddSampleIfNeeded函数，记录aof-fsync-always延迟事件的采样结果，如下所示：\n\nvoid flushAppendOnlyFile(int force) {\n…\nif (server.aof_fsync == AOF_FSYNC_ALWAYS) {\nlatencyStartMonitor(latency); //调用latencyStartMonitor函数开始计时\nredis_fsync(server.aof_fd); //实际将数据写入磁盘\n        latencyEndMonitor(latency); //调用latencyEndMonitor结束计时，并计算时长\n        latencyAddSampleIfNeeded("aof-fsync-always",latency);\n…}\n}\n\n\n那么在这里，你需要注意的是，Redis源码在调用latencyAddSampleIfNeeded函数记录采样结果时，经常会在延迟事件执行前，调用latencyStartMonitor函数开始计时，并在事件执行结束后，调用latencyEndMonitor函数结束计时和计算事件执行时长。\n\n此外，你也可以在阅读Redis源码的工具中，比如sublime、sourceinsight等，通过查找函数关系调用，找到latencyAddSampleIfNeeded函数被调用的其他地方。\n\n好了，到这里，Redis延迟监控框架就能通过latencyAddSampleIfNeeded函数，来记录延迟事件的采样结果了。而实际上，Redis延迟监控框架还实现了延迟分析，并能提供应对延迟变慢的建议，我们再来看下。\n\n\n# 延迟分析和提供应对措施建议\n\n首先，Redis是提供了latency doctor命令，来给出延迟分析结果和应对方法建议的。当我们执行这条命令的时候，Redis就会使用latencyCommand函数来处理。而在处理这个命令时，latencyCommand函数会调用createLatencyReport函数，来生成延迟分析报告和应对方法建议。\n\n具体来说，createLatencyReport函数会针对latency_events哈希表中记录的每一类事件，先调用analyzeLatencyForEvent函数，计算获得采样的延迟事件执行时长的均值、最大/最小值等统计结果。具体的统计计算过程，你可以仔细阅读下analyzeLatencyForEvent函数的源码。\n\n然后，createLatencyReport函数会针对这类事件，结合Redis配置项等信息给出应对措施。\n\n其实，在createLatencyReport函数中，它定义了多个int变量，当这些变量的值为1时，就表示建议Redis使用者采用一种应对高延迟的措施。我在下面的代码中展示了部分应对措施对应的变量，你可以看下。另外你也可以阅读createLatencyReport函数源码，去了解所有的措施。\n\nsds createLatencyReport(void) {\n    …\n    int advise_slowlog_enabled = 0;  //建议启用slowlog\n    int advise_slowlog_tuning = 0;   //建议重新配置slowlog阈值\n    int advise_slowlog_inspect = 0;   //建议检查slowlog结果\n    int advise_disk_contention = 0;   //建议减少磁盘竞争\n    …\n}\n\n\n我们也来简单举个例子。比如说，针对command事件，createLatencyReport函数就会根据slowlog的设置情况，给出启用slowlog、调整slowlog阈值、检查slowlog日志结果和避免使用bigkey的应对建议。这部分代码如下所示：\n\nif (!strcasecmp(event,"command")) {\n    \n   //如果没有启用slowlog，则建议启用slowlog\n   if (server.slowlog_log_slower_than < 0) {\n       advise_slowlog_enabled = 1;\n       advices++;\n\t}  \n    //如果slowlog使用的命令时长阈值太大，建议调整slowlog阈值\n\telse if (server.slowlog_log_slower_than/1000 >server.latency_monitor_threshold){\n        advise_slowlog_tuning = 1;\n        advices++;\n    }\n    //建议检查slowlog结果\n    advise_slowlog_inspect = 1; \n    //建议避免使用bigkey\n    advise_large_objects = 1; \n    advices += 2;\n}\n\n\n所以，像createLatencyReport函数这样在计算延迟统计结果的同时，也给出应对措施的设计就很不错，这也是从Redis开发者的角度给出的建议，它更具有针对性。\n\n好了，到这里，我们就了解了延迟监控框架的实现。接下来，我们再来学习下Redis中慢命令日志的实现。\n\n\n# 慢命令日志的实现\n\nRedis是使用了一个较为简单的方法来记录慢命令日志，也就是用一个列表，把执行时间超出慢命令日志执行时间阈值的命令记录下来。\n\n在Redis全局变量server对应的数据结构redisServer中，有一个list类型的成员变量slowlog，它就是用来记录慢命令日志的列表的，如下所示：\n\nstruct redisServer {\n    …\n    list *slowlog;\n    …\n}\n\n\n而实现慢命令日志记录功能的代码是在slowlog.c文件中。这里的主要函数是slowlogPushEntryIfNeeded，它的原型如下所示：\n\nvoid slowlogPushEntryIfNeeded(client *c, robj **argv, int argc, long long duration)\n\n\n从代码中你可以看到，这个函数的参数包含了当前执行命令及其参数argv，以及当前命令的执行时长duration。\n\n这个函数的逻辑也不复杂，它会判断当前命令的执行时长duration，是否大于 redis.conf 配置文件中的慢命令日志阈值 slowlog-log-slower-than。如果大于的话，它就会调用slowlogCreateEntry函数，为这条命令创建一条慢命令日志项，并调用listAddNodeHeader函数，把这条日志项加入到日志列表头，如下所示：\n\n//当前命令的执行时长是否大于配置项\nif (duration >= server.slowlog_log_slower_than)\n   listAddNodeHead(server.slowlog, slowlogCreateEntry(c,argv,argc,duration));\n\n\n当然，如果日志列表中记录了太多日志项，它消耗的内存资源也会增加。所以slowlogPushEntryIfNeeded函数在添加日志项时，会判断整个日志列表的长度是否超过配置项slowlog-max-len。一旦超过了，它就会把列表末尾的日志项删除，如下所示：\n\n//如果日志列表超过阈值长度，就删除列表末尾的日志项\nwhile (listLength(server.slowlog) > server.slowlog_max_len)\n        listDelNode(server.slowlog,listLast(server.slowlog))\n\n\n现在，我们也就了解了记录慢命令日志项的主要函数，slowlogPushEntryIfNeeded的基本逻辑了。然后我们再来看下，它在记录日志项时调用的slowlogCreateEntry函数。\n\n这个函数是用来创建一个慢命令日志项。慢命令日志项的数据结构是slowlogEntry，如下所示：\n\ntypedef struct slowlogEntry {\n    //日志项对应的命令及参数\n    robj **argv;     \n    //日志项对应的命令及参数个数\n    int argc;        \n    //日志项的唯一ID\n    long long id; \n    //日志项对应命令的执行时长（以微秒为单位）\n    long long duration;  \n    //日志项对应命令的执行时间戳\n    time_t time;        \n    //日志项对应命令的发送客户端名称\n    sds cname;      \n    //日志项对应命令的发送客户端网络地址\n    sds peerid;         \n} slowlogEntry;\n\n\n从slowLogEntry的定义中，你可以看到，它会把慢命令及其参数，以及发送命令的客户端网络地址记录下来。这样设计的好处是，当我们分析慢命令日志时，就可以直接看到慢命令本身及其参数了，而且可以知道发送命令的客户端信息。而这些信息，就有利于我们排查慢命令的起因和来源。\n\n比如说，如果我们发现日志中记录的命令参数非常多，那么它就可能是一条操作bigkey的命令。\n\n当然，考虑到内存资源有限，slowlogCreateEntry函数在创建慢命令日志项时，也会判断命令参数个数。如果命令参数个数，超出了阈值SLOWLOG_ENTRY_MAX_ARGC这个宏定义的大小（默认32）时，它就不会记录超出阈值的参数了，而是记录下剩余的参数个数。这样一来，慢命令日志项中就既记录了部分命令参数，有助于排查问题，也避免了记录过多参数，占用过多内存。\n\n下面的代码展示了slowlogCreateEntry的基本执行逻辑，你可以看下。\n\nslowlogEntry *slowlogCreateEntry(client *c, robj **argv, int argc, long long duration) {\n    //分配日志项空间\n    slowlogEntry *se = zmalloc(sizeof(*se)); \n    //待记录的参数个数，默认为当前命令的参数个数\n    int j, slargc = argc;  \n\n    //如果当前命令参数个数超出阈值，则只记录阈值个数的参数\n    if (slargc > SLOWLOG_ENTRY_MAX_ARGC) slargc = SLOWLOG_ENTRY_MAX_ARGC;\n    se->argc = slargc;\n    …\n    //逐一记录命令及参数\n    for (j = 0; j < slargc; j++) {\n        //如果命令参数个数超出阈值，使用最后一个参数记录当前命令实际剩余的参数个数\n       if (slargc != argc && j == slargc-1) {  \n          se->argv[j] = createObject(OBJ_STRING,\n                    sdscatprintf(sdsempty(),"... (%d more arguments)",\n                    argc-slargc+1));\n            } else {\n            …  //将命令参数填充到日志项中\n            }}\n    … //将命令执行时长、客户端地址等信息填充到日志项中\n}\n\n\n好了，到这里，你就了解了慢命令日志的实现。最后，你也要注意，慢命令日志只会记录超出执行时长阈值的命令信息，而不会像延迟监控框架那样记录多种事件。所以，记录日志的函数slowlogPushEntryIfNeeded，只会在命令执行函数call（在server.c文件中）中被调用，如下所示：\n\nvoid call(client *c, int flags) {\n    …\n   \t//命令执行前计时\n    start = server.ustime; \n    //命令实际执行\n    c->cmd->proc(c);  \n    //命令执行完成计算耗时\n    duration = ustime()-start; \n    …\n    if (flags & CMD_CALL_SLOWLOG && c->cmd->proc != execCommand) {\n        …\n        //调用 slowlogPushEntryIfNeeded 函数记录慢命令\n        slowlogPushEntryIfNeeded(c,c->argv,c->argc,duration);\n    }\n    …\n}\n\n\n\n# 总结\n\nRedis实现的延迟监控框架和慢命令日志。\n\n你要知道，Redis源码会针对可能导致Redis运行变慢的五类事件，在它们执行时进行采样。而一旦这些事件的执行时长超过阈值时，监控框架就会将采样结果记录下来，以便后续分析使用。这种针对延迟事件进行采样记录的监控方法，其实是很值得我们学习的。\n\n而慢命令日志的实现则较为简单，就是针对运行时长超出阈值的命令，使用一个列表把它们记录下来，这里面包括了命令及参数，以及发送命令的客户端信息，这样可以方便运维人员查看分析。\n\n当然，Redis源码中实现的延迟监控框架主要是关注导致延迟增加的事件，它记录的延迟事件，也是和Redis运行过程中可能会导致运行变慢的操作紧耦合的。此外，Redis的INFO命令也提供了Redis运行时的监控信息，不过你要知道，INFO命令的实现，主要是在全局变量server的成员变量中，用来记录Redis实例的实时运行状态或是资源使用情况的。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. redis 是以低延迟著称的数据库，那么当它的响应速度变慢时，如何快速诊断出问题所在？\n 2. 你知道 redis 的哪些事件可能导致它运行缓慢吗？它们是如何被监控的？\n 3. 当 redis 的某个命令执行变慢时，我们如何捕捉这些“慢命令”并进行详细分析？\n 4. 在 redis 延迟监控中，如何通过采样机制高效记录每类事件的执行时长？\n 5. 为什么 redis 需要为延迟事件和慢命令分别设计不同的监控和日志机制？这两者的核心区别是什么？\n 6. redis 如何通过“时间序列”的方式保存和分析多次延迟事件的数据，以便为后续问题排查提供更多线索？\n 7. 在延迟监控中，redis 是如何通过统计延迟事件的最大值、最小值、均值等数据来帮助用户诊断问题的？\n 8. redis 的慢命令日志能提供哪些关键信息？如何通过这些信息定位到具体的慢命令和它的来源？\n\n\n# 前言\n\nredis的一个显著特征就是能提供低延迟的数据访问。而如果redis在运行过程中变慢了，我们就需要有方法能监控到哪些命令执行变慢了。更进一步的需求，就是我们需要有方法监控到，是redis运行过程中的哪些事件导致redis变慢了。这样一来，我们就既可以检查这些慢命令，分析它们的操作类型和访问的数据量，进而提出应对方法，也可以检查监控记录的事件，分析事件发生的原因并提出应对方法\n\n那么，为了满足这些需求，我们就需要有一套监控框架\n\n * 一方面能监测导致redis变慢的事件\n * 另一方面，能监控并记录变慢的命令。\n\n其实，这也是我们在开发后端系统时，经常会面临的一个运维开发需求，也就是如何监控后端系统的运行状态。\n\necho 来带你了解redis的延迟监控框架和慢命令日志的设计与实现。\n\n\n# 延迟监控框架的实现\n\n实际上，redis 在运行过程中，以下表格中给出的几类事件都会导致 redis 变慢，我们通常也把这些事件称为延迟事件。你可以参考表格中的这些事件类型，以及它们在源码中对应的事件名称。\n\n事件类型      源码中对应名称\n命令事件      command、fast-command\naof事件     aof-write-pending-fsync, aof-write-active-child,\n          aof-write-alone, aof-fstat, aof-rewrite-diff-write,\n          aof-rename\nfork事件    fork\n过期key事件   expire-cycie\n缓存替换事件    eviction-del, eviction-cycie\n\n那么针对这些事件，redis实现了延迟监控框架，通过采样的方式来记录它们的执行情况。当需要排查问题时，延迟监控框架提供了latency history命令，以便运维人员检查这些事件。\n\n下面，我们就来看下记录事件执行情况的数据结构。因为延迟监控框架是在latency.h和latency.c文件中实现的，你也可以在这两个文件中找到相应的数据结构和函数。\n\n\n# 记录事件执行情况的数据结构\n\n首先，redis是使用了latencysample结构体，来记录延迟事件的采样时间和事件的实际执行时长，这个结构体的代码如下所示：\n\nstruct latencysample {\n    int32_t time;  //事件的采样时间\n    uint32_t latency;  //事件的执行时长（以毫秒为单位）\n};\n\n\n而在latencysample这个结构体基础上，redis又设计了latencytimeseries结构体，这个结构体使用了一个latencysample类型的数组，记录了针对某一类事件的一系列采样结果，这样就可以为分析redis变慢提供更多的事件信息。\n\nstruct latencytimeseries {\n    int idx;  //采样事件数组的写入位置\n    uint32_t max;  //当前事件的最大延迟\n    struct latencysample samples[latency_ts_len]; //采样事件数组，记录latency_ts_len个采样结果，latency_ts_len默认为160\n};\n\n\n另外，也因为延迟监控框架要记录的延迟事件有很多种，所以 redis 还进一步设计了一个哈希表latency_events，作为全局变量server的一个成员变量，用来记录不同事件的采样结果数组，如下所示：\n\nstruct redisserver {\n   …\n   dict *latency_events;\n   …\n}\n\n\n这个哈希表是在redis server启动初始化的函数initserver中，通过调用latencymonitorinit函数来完成创建的，如下所示：\n\nvoid initserver(void) {\n    …\n    latencymonitorinit();\n}\n \nvoid latencymonitorinit(void) {\n    server.latency_events = dictcreate(&latencytimeseriesdicttype,null);\n}\n\n\n好，了解了记录延迟事件的数据结构和初始化操作后，我们再来看下事件采样是如何实现的。\n\n\n# 如何实现延迟事件的采样？\n\n延迟事件的采样函数是latencyaddsample，它的函数原型如下所示。它的参数中包含了要记录的事件名称，这实际是对应了latency_events哈希表中的一个哈希项。此外，它的参数中还包括该事件的执行时长。\n\nvoid latencyaddsample(char *event, mstime_t latency)\n\n\nlatencyaddsample函数的执行逻辑并不复杂，主要可以分成三步。\n\n首先，它会根据传入的事件名称，在latency_events哈希表中查找该事件。如果该事件对应的哈希项还不存在，它就会在哈希表中加入该事件，如下所示：\n\n//查找事件对应的哈希项\nstruct latencytimeseries *ts = dictfetchvalue(server.latency_events,event);\n…\nif (ts == null) { //如果哈希项为空，就新建哈希项\n    ts = zmalloc(sizeof(*ts));\n    ts->idx = 0;\n    ts->max = 0;\n    memset(ts->samples,0,sizeof(ts->samples));\n    dictadd(server.latency_events,zstrdup(event),ts); //在哈希表中插入哈希项\n}\n\n\n然后，latencyaddsample函数会根据传入的事件执行时间，更新当前记录的该类事件的最大执行时间，如下所示：\n\nif (latency > ts->max) ts->max = latency;\n\n\n最后，latencyaddsample函数会实际记录当前的采样结果。\n\n不过在这一步，如果它发现当前的采样结果，和前一个采样结果是在同一秒中获得的，并且如果当前采样结果的事件执行时长，大于前一个采样结果的话，那么latencyaddsample函数就会直接更新前一个采样结果中记录的执行时长了，而不是新插入一个采样结果。\n\n否则的话，latencyaddsample函数才会新插入一个采样结果。这样设计的目的，也是为了避免在同一秒中记录过多的采样结果。\n\n下面的代码展示了latencyaddsample函数实际记录采样结果的逻辑，你可以看下。\n\n//获得同类事件的前一个采样结果\nprev = (ts->idx + latency_ts_len - 1) % latency_ts_len;\n//如果当前和前一个采样结果在同一秒中\nif (ts->samples[prev].time == now) { \n    //如果当前采用结果的执行时长大于前一个采样结果\n    if (latency > ts->samples[prev].latency) \n        //直接更新前一个采样结果的执行时长\n        ts->samples[prev].latency = latency;\n    return;\n}\n//否则，新插入当前的采样结果\nts->samples[ts->idx].time = time(null);\nts->samples[ts->idx].latency = latency;\n\n\n而在这里，你也要注意一点，就是latencyaddsample函数在记录采样结果时，会重复使用采样结果数组latencytimeseries。所以，如果采样结果数量超过数组默认大小时，旧的采样结果是会被覆盖掉的。如果你要记录更多的采样结果，就需要扩大latencytimeseries数组的长度。\n\n那么，latencyaddsample函数是在什么时候调用进行采样的呢?\n\n其实，latencyaddsample函数是被封装在了latencyaddsampleifneeded函数中。在latencyaddsampleifneeded函数中，它只会在事件执行时长超过latency-monitor-threshold配置项的值时，才调用latencyaddsample函数记录采样结果。你可以看看下面给出的latencyaddsampleifneeded函数定义。\n\nlatencyaddsampleifneeded(event,var){\n    if (server.latency_monitor_threshold &&  (var) >= server.latency_monitor_threshold)\n          latencyaddsample((event),(var));\n}\n    \n\n\n而latencyaddsampleifneeded函数，实际上会在刚才介绍的延迟事件发生时被调用。这里我来给你举两个例子。\n\n比如，当redis命令通过call函数（在server.c文件中）执行时，call函数就会调用latencyaddsampleifneeded函数进行采样，如下所示：\n\nif (flags & cmd_call_slowlog && c->cmd->proc != execcommand) {\n    //根据命令数据结构中flags的cmd_fast标记，决定当前是fast-command事件还是command事件\n    char *latency_event = (c->cmd->flags & cmd_fast) ?\n        "fast-command" : "command";\n    latencyaddsampleifneeded(latency_event,duration/1000);\n    …\n}\n\n\n再比如，当redis调用flushappendonlyfile函数写aof文件时，如果aof文件刷盘的配置项是aof_fsync_always，那么flushappendonlyfile函数就会调用latencyaddsampleifneeded函数，记录aof-fsync-always延迟事件的采样结果，如下所示：\n\nvoid flushappendonlyfile(int force) {\n…\nif (server.aof_fsync == aof_fsync_always) {\nlatencystartmonitor(latency); //调用latencystartmonitor函数开始计时\nredis_fsync(server.aof_fd); //实际将数据写入磁盘\n        latencyendmonitor(latency); //调用latencyendmonitor结束计时，并计算时长\n        latencyaddsampleifneeded("aof-fsync-always",latency);\n…}\n}\n\n\n那么在这里，你需要注意的是，redis源码在调用latencyaddsampleifneeded函数记录采样结果时，经常会在延迟事件执行前，调用latencystartmonitor函数开始计时，并在事件执行结束后，调用latencyendmonitor函数结束计时和计算事件执行时长。\n\n此外，你也可以在阅读redis源码的工具中，比如sublime、sourceinsight等，通过查找函数关系调用，找到latencyaddsampleifneeded函数被调用的其他地方。\n\n好了，到这里，redis延迟监控框架就能通过latencyaddsampleifneeded函数，来记录延迟事件的采样结果了。而实际上，redis延迟监控框架还实现了延迟分析，并能提供应对延迟变慢的建议，我们再来看下。\n\n\n# 延迟分析和提供应对措施建议\n\n首先，redis是提供了latency doctor命令，来给出延迟分析结果和应对方法建议的。当我们执行这条命令的时候，redis就会使用latencycommand函数来处理。而在处理这个命令时，latencycommand函数会调用createlatencyreport函数，来生成延迟分析报告和应对方法建议。\n\n具体来说，createlatencyreport函数会针对latency_events哈希表中记录的每一类事件，先调用analyzelatencyforevent函数，计算获得采样的延迟事件执行时长的均值、最大/最小值等统计结果。具体的统计计算过程，你可以仔细阅读下analyzelatencyforevent函数的源码。\n\n然后，createlatencyreport函数会针对这类事件，结合redis配置项等信息给出应对措施。\n\n其实，在createlatencyreport函数中，它定义了多个int变量，当这些变量的值为1时，就表示建议redis使用者采用一种应对高延迟的措施。我在下面的代码中展示了部分应对措施对应的变量，你可以看下。另外你也可以阅读createlatencyreport函数源码，去了解所有的措施。\n\nsds createlatencyreport(void) {\n    …\n    int advise_slowlog_enabled = 0;  //建议启用slowlog\n    int advise_slowlog_tuning = 0;   //建议重新配置slowlog阈值\n    int advise_slowlog_inspect = 0;   //建议检查slowlog结果\n    int advise_disk_contention = 0;   //建议减少磁盘竞争\n    …\n}\n\n\n我们也来简单举个例子。比如说，针对command事件，createlatencyreport函数就会根据slowlog的设置情况，给出启用slowlog、调整slowlog阈值、检查slowlog日志结果和避免使用bigkey的应对建议。这部分代码如下所示：\n\nif (!strcasecmp(event,"command")) {\n    \n   //如果没有启用slowlog，则建议启用slowlog\n   if (server.slowlog_log_slower_than < 0) {\n       advise_slowlog_enabled = 1;\n       advices++;\n\t}  \n    //如果slowlog使用的命令时长阈值太大，建议调整slowlog阈值\n\telse if (server.slowlog_log_slower_than/1000 >server.latency_monitor_threshold){\n        advise_slowlog_tuning = 1;\n        advices++;\n    }\n    //建议检查slowlog结果\n    advise_slowlog_inspect = 1; \n    //建议避免使用bigkey\n    advise_large_objects = 1; \n    advices += 2;\n}\n\n\n所以，像createlatencyreport函数这样在计算延迟统计结果的同时，也给出应对措施的设计就很不错，这也是从redis开发者的角度给出的建议，它更具有针对性。\n\n好了，到这里，我们就了解了延迟监控框架的实现。接下来，我们再来学习下redis中慢命令日志的实现。\n\n\n# 慢命令日志的实现\n\nredis是使用了一个较为简单的方法来记录慢命令日志，也就是用一个列表，把执行时间超出慢命令日志执行时间阈值的命令记录下来。\n\n在redis全局变量server对应的数据结构redisserver中，有一个list类型的成员变量slowlog，它就是用来记录慢命令日志的列表的，如下所示：\n\nstruct redisserver {\n    …\n    list *slowlog;\n    …\n}\n\n\n而实现慢命令日志记录功能的代码是在slowlog.c文件中。这里的主要函数是slowlogpushentryifneeded，它的原型如下所示：\n\nvoid slowlogpushentryifneeded(client *c, robj **argv, int argc, long long duration)\n\n\n从代码中你可以看到，这个函数的参数包含了当前执行命令及其参数argv，以及当前命令的执行时长duration。\n\n这个函数的逻辑也不复杂，它会判断当前命令的执行时长duration，是否大于 redis.conf 配置文件中的慢命令日志阈值 slowlog-log-slower-than。如果大于的话，它就会调用slowlogcreateentry函数，为这条命令创建一条慢命令日志项，并调用listaddnodeheader函数，把这条日志项加入到日志列表头，如下所示：\n\n//当前命令的执行时长是否大于配置项\nif (duration >= server.slowlog_log_slower_than)\n   listaddnodehead(server.slowlog, slowlogcreateentry(c,argv,argc,duration));\n\n\n当然，如果日志列表中记录了太多日志项，它消耗的内存资源也会增加。所以slowlogpushentryifneeded函数在添加日志项时，会判断整个日志列表的长度是否超过配置项slowlog-max-len。一旦超过了，它就会把列表末尾的日志项删除，如下所示：\n\n//如果日志列表超过阈值长度，就删除列表末尾的日志项\nwhile (listlength(server.slowlog) > server.slowlog_max_len)\n        listdelnode(server.slowlog,listlast(server.slowlog))\n\n\n现在，我们也就了解了记录慢命令日志项的主要函数，slowlogpushentryifneeded的基本逻辑了。然后我们再来看下，它在记录日志项时调用的slowlogcreateentry函数。\n\n这个函数是用来创建一个慢命令日志项。慢命令日志项的数据结构是slowlogentry，如下所示：\n\ntypedef struct slowlogentry {\n    //日志项对应的命令及参数\n    robj **argv;     \n    //日志项对应的命令及参数个数\n    int argc;        \n    //日志项的唯一id\n    long long id; \n    //日志项对应命令的执行时长（以微秒为单位）\n    long long duration;  \n    //日志项对应命令的执行时间戳\n    time_t time;        \n    //日志项对应命令的发送客户端名称\n    sds cname;      \n    //日志项对应命令的发送客户端网络地址\n    sds peerid;         \n} slowlogentry;\n\n\n从slowlogentry的定义中，你可以看到，它会把慢命令及其参数，以及发送命令的客户端网络地址记录下来。这样设计的好处是，当我们分析慢命令日志时，就可以直接看到慢命令本身及其参数了，而且可以知道发送命令的客户端信息。而这些信息，就有利于我们排查慢命令的起因和来源。\n\n比如说，如果我们发现日志中记录的命令参数非常多，那么它就可能是一条操作bigkey的命令。\n\n当然，考虑到内存资源有限，slowlogcreateentry函数在创建慢命令日志项时，也会判断命令参数个数。如果命令参数个数，超出了阈值slowlog_entry_max_argc这个宏定义的大小（默认32）时，它就不会记录超出阈值的参数了，而是记录下剩余的参数个数。这样一来，慢命令日志项中就既记录了部分命令参数，有助于排查问题，也避免了记录过多参数，占用过多内存。\n\n下面的代码展示了slowlogcreateentry的基本执行逻辑，你可以看下。\n\nslowlogentry *slowlogcreateentry(client *c, robj **argv, int argc, long long duration) {\n    //分配日志项空间\n    slowlogentry *se = zmalloc(sizeof(*se)); \n    //待记录的参数个数，默认为当前命令的参数个数\n    int j, slargc = argc;  \n\n    //如果当前命令参数个数超出阈值，则只记录阈值个数的参数\n    if (slargc > slowlog_entry_max_argc) slargc = slowlog_entry_max_argc;\n    se->argc = slargc;\n    …\n    //逐一记录命令及参数\n    for (j = 0; j < slargc; j++) {\n        //如果命令参数个数超出阈值，使用最后一个参数记录当前命令实际剩余的参数个数\n       if (slargc != argc && j == slargc-1) {  \n          se->argv[j] = createobject(obj_string,\n                    sdscatprintf(sdsempty(),"... (%d more arguments)",\n                    argc-slargc+1));\n            } else {\n            …  //将命令参数填充到日志项中\n            }}\n    … //将命令执行时长、客户端地址等信息填充到日志项中\n}\n\n\n好了，到这里，你就了解了慢命令日志的实现。最后，你也要注意，慢命令日志只会记录超出执行时长阈值的命令信息，而不会像延迟监控框架那样记录多种事件。所以，记录日志的函数slowlogpushentryifneeded，只会在命令执行函数call（在server.c文件中）中被调用，如下所示：\n\nvoid call(client *c, int flags) {\n    …\n   \t//命令执行前计时\n    start = server.ustime; \n    //命令实际执行\n    c->cmd->proc(c);  \n    //命令执行完成计算耗时\n    duration = ustime()-start; \n    …\n    if (flags & cmd_call_slowlog && c->cmd->proc != execcommand) {\n        …\n        //调用 slowlogpushentryifneeded 函数记录慢命令\n        slowlogpushentryifneeded(c,c->argv,c->argc,duration);\n    }\n    …\n}\n\n\n\n# 总结\n\nredis实现的延迟监控框架和慢命令日志。\n\n你要知道，redis源码会针对可能导致redis运行变慢的五类事件，在它们执行时进行采样。而一旦这些事件的执行时长超过阈值时，监控框架就会将采样结果记录下来，以便后续分析使用。这种针对延迟事件进行采样记录的监控方法，其实是很值得我们学习的。\n\n而慢命令日志的实现则较为简单，就是针对运行时长超出阈值的命令，使用一个列表把它们记录下来，这里面包括了命令及参数，以及发送命令的客户端信息，这样可以方便运维人员查看分析。\n\n当然，redis源码中实现的延迟监控框架主要是关注导致延迟增加的事件，它记录的延迟事件，也是和redis运行过程中可能会导致运行变慢的操作紧耦合的。此外，redis的info命令也提供了redis运行时的监控信息，不过你要知道，info命令的实现，主要是在全局变量server的成员变量中，用来记录redis实例的实时运行状态或是资源使用情况的。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"发布与订阅",frontmatter:{title:"发布与订阅",date:"2024-09-18T01:00:52.000Z",permalink:"/pages/61d908/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E5%9B%9B%E3%80%81%E6%94%AF%E7%BA%BF%E4%BB%BB%E5%8A%A1/25.%E5%8F%91%E5%B8%83%E4%B8%8E%E8%AE%A2%E9%98%85.html",relativePath:"Redis 系统设计/04.四、支线任务/25.发布与订阅.md",key:"v-167975b0",path:"/pages/61d908/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"主从复制",frontmatter:{title:"主从复制",date:"2024-09-16T03:24:06.000Z",permalink:"/pages/ebc8dc/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E4%BA%94%E3%80%81%E9%9B%86%E7%BE%A4/25.%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6.html",relativePath:"Redis 系统设计/05.五、集群/25.主从复制.md",key:"v-0f216ec4",path:"/pages/ebc8dc/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:214},{level:2,title:"旧版复制",slug:"旧版复制",normalizedTitle:"旧版复制",charIndex:359},{level:3,title:"第一步：同步",slug:"第一步-同步",normalizedTitle:"第一步：同步",charIndex:459},{level:3,title:"第二步：命令传播",slug:"第二步-命令传播",normalizedTitle:"第二步：命令传播",charIndex:728},{level:3,title:"旧版的缺陷",slug:"旧版的缺陷",normalizedTitle:"旧版的缺陷",charIndex:797},{level:2,title:"新版复制",slug:"新版复制",normalizedTitle:"新版复制",charIndex:1199},{level:3,title:"部分重同步的实现",slug:"部分重同步的实现",normalizedTitle:"部分重同步的实现",charIndex:1482},{level:4,title:"复制偏移量",slug:"复制偏移量",normalizedTitle:"复制偏移量",charIndex:1545},{level:4,title:"复制积压缓冲区",slug:"复制积压缓冲区",normalizedTitle:"复制积压缓冲区",charIndex:1580},{level:4,title:"服务器运行 ID",slug:"服务器运行-id",normalizedTitle:"服务器运行 id",charIndex:2380},{level:3,title:"深入了解 PSYNC 命令",slug:"深入了解-psync-命令",normalizedTitle:"深入了解 psync 命令",charIndex:2773},{level:2,title:"复制流程详解",slug:"复制流程详解",normalizedTitle:"复制流程详解",charIndex:3613},{level:4,title:"步骤 1：设置主服务器的地址和端口",slug:"步骤-1-设置主服务器的地址和端口",normalizedTitle:"步骤 1：设置主服务器的地址和端口",charIndex:3665},{level:4,title:"步骤 2：建立套接字连接",slug:"步骤-2-建立套接字连接",normalizedTitle:"步骤 2：建立套接字连接",charIndex:4008},{level:4,title:"步骤 3：发送 PING 命令",slug:"步骤-3-发送-ping-命令",normalizedTitle:"步骤 3：发送 ping 命令",charIndex:4321},{level:4,title:"步骤 4：身份验证",slug:"步骤-4-身份验证",normalizedTitle:"步骤 4：身份验证",charIndex:4349},{level:4,title:"步骤 5：发送端口信息",slug:"步骤-5-发送端口信息",normalizedTitle:"步骤 5：发送端口信息",charIndex:4561},{level:4,title:"步骤 6：同步",slug:"步骤-6-同步",normalizedTitle:"步骤 6：同步",charIndex:4904},{level:4,title:"步骤 7：命令传播",slug:"步骤-7-命令传播",normalizedTitle:"步骤 7：命令传播",charIndex:5318},{level:2,title:"心跳检测",slug:"心跳检测",normalizedTitle:"心跳检测",charIndex:5418},{level:2,title:"主从复制的触发时机",slug:"主从复制的触发时机",normalizedTitle:"主从复制的触发时机",charIndex:6467},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6998},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:7266}],headersStr:"前言 旧版复制 第一步：同步 第二步：命令传播 旧版的缺陷 新版复制 部分重同步的实现 复制偏移量 复制积压缓冲区 服务器运行 ID 深入了解 PSYNC 命令 复制流程详解 步骤 1：设置主服务器的地址和端口 步骤 2：建立套接字连接 步骤 3：发送 PING 命令 步骤 4：身份验证 步骤 5：发送端口信息 步骤 6：同步 步骤 7：命令传播 心跳检测 主从复制的触发时机 总结 参考文献",content:'提出问题是一切智慧的开端\n\n 1. 为什么 Redis 早期的 SYNC 操作在断线重连时效率低下？\n 2. 如何通过 replication offset 和 replication backlog 实现高效的部分重同步？\n 3. 运行ID在主从断线重连时如何帮助确定同步方式？\n 4. PSYNC 如何优化断线后重连时的主从复制效率？\n 5. 为什么即使有 TCP 传输，Redis 仍需通过心跳机制确保数据一致？\n\n\n# 前言\n\n\n\n在 Redis 中，用户可以通过 SLAVEOF 命令或设置 slaveof 选项，让一个 Redis 服务器复制另一个 Redis 服务器。我们称它们分别为从服务器（slave）和主服务器（master）。\n\n在开始之前，我假设你已经了解主从复制功能的基本使用方法。\n\n\n# 旧版复制\n\nRedis 的复制功能分为两个阶段：\n\n * 同步：用于将从服务器的数据库状态更新至主服务器的当前状态。\n * 命令传播：用于在主服务器状态被修改时，实时将修改命令告知从服务器。\n\n\n# 第一步：同步\n\n当从服务器首次发送 SLAVEOF 命令来复制主服务器时，必须执行同步（SYNC）操作。\n\n\n\n执行步骤如下：\n\n 1. 从服务器向主服务器发送 SYNC 命令。\n 2. 主服务器收到 SYNC 命令后，执行 BGSAVE 命令，并开始记录从当前时刻起的所有写命令到缓冲区中。\n 3. BGSAVE 执行完毕后，主服务器将生成的 RDB 文件发送给从服务器。\n 4. 从服务器接收并载入该 RDB 文件。\n 5. 主服务器将缓冲区中所有的写命令发送给从服务器，从服务器执行这些命令后，状态与主服务器保持一致。\n\n\n\n\n# 第二步：命令传播\n\n同步完成后，主服务器的状态如果发生变化，它会实时向从服务器发送写命令，确保从服务器的状态与主服务器保持一致。\n\n\n# 旧版的缺陷\n\n在同步过程中，可能会遇到以下两种情况：\n\n 1. 初次复制：从服务器首次复制某个主服务器的全部数据。\n 2. 断线后重复制：处于命令传播阶段的从服务器与主服务器断开连接后，再次自动连接上主服务器并继续复制。\n\n对于断线后的重复制来说，没必要再次同步整个 RDB 文件，只需同步断线期间缺失的命令即可。\n\nSYNC 命令是一个非常耗费资源的操作\n\n 1. 主服务器需要执行 BGSAVE 命令来生成 RDB 文件，这会消耗大量的 CPU、内存和磁盘 I/O 资源。\n 2. 主服务器需要将生成的 RDB 文件发送给从服务器，传输过程会占用大量网络资源，并影响主服务器响应其他命令请求的时间。\n 3. 从服务器在接收到 RDB 文件后，需要加载该文件，在加载期间从服务器将无法处理任何命令请求。\n\n由于 SYNC 操作的高资源消耗，Redis 需要确保只在必要时执行 SYNC 操作。\n\n\n# 新版复制\n\n新版复制引入的目的是解决旧版复制在断线后重新同步时的效率低下问题。\n\n自 Redis 2.8 起，使用 PSYNC 命令取代 SYNC 命令来执行复制操作。\n\nPSYNC 具有两种模式：\n\n * 完整重同步：处理初次复制的情况，其执行步骤与 SYNC 命令类似。\n * 部分重同步：用于处理断线后重复制的情况。当从服务器在断线后重新连接到主服务器时，如果条件允许，主服务器可以仅将断线期间的写命令发送给从服务器，从服务器执行这些命令后即可与主服务器保持一致。\n\nPSYNC 的部分重同步模式有效解决了旧版复制在处理断线后重复制时的低效问题。\n\n\n\n\n# 部分重同步的实现\n\n部分重同步的实现依赖以下三个关键组件：\n\n * replication offset：主服务器和从服务器的复制偏移量。\n * replication backlog：主服务器的复制积压缓冲区。\n * run ID：主服务器的运行 ID。\n\n# 复制偏移量\n\n复制的双方都会维护各自的「复制偏移量」：\n\n * 主服务器每次向从服务器传播 N 字节的数据时，会将自身的「复制偏移量」加上 N。\n * 从服务器每次收到 N 字节的数据时，也会将自身的「复制偏移量」加上 N。\n\n\n\n通过比较主从服务器的复制偏移量，可以轻松判断两者是否一致：\n\n * 如果两者的复制偏移量相同，则主从服务器处于一致状态。\n * 如果复制偏移量不同，则主从服务器处于不一致状态。\n\n# 复制积压缓冲区\n\n「复制积压缓冲区」是由主服务器维护的一个固定大小的环形缓冲区，默认大小为 1MB。缓冲区使用先进先出（FIFO）策略，当数据超过缓冲区大小时，最旧的数据会被覆盖。\n\n主服务器在进行命令传播时，除了将写命令发送给所有从服务器，还会将命令存入复制积压缓冲区。\n\n\n\n缓冲区会保存最近传播的写命令，并为每个字节记录对应的复制偏移量。\n\n\n\n当从服务器重新连接主服务器时，会通过 PSYNC 命令将自身的复制偏移量发送给主服务器，主服务器根据偏移量决定同步方式：\n\n * 如果偏移量之后的数据仍然存在于「复制积压缓冲区」中，则主服务器执行部分重同步操作。\n * 如果偏移量之后的数据已不在缓冲区中，则主服务器执行完整重同步操作。\n\n回到断线重连的例子：\n\n 1. 当从服务器 A 断线后重新连接主服务器时，发送 PSYNC 命令并报告自身的复制偏移量为 10086。\n 2. 主服务器检查「复制积压缓冲区」中的数据，确认偏移量 10086 之后的数据仍然存在，因此向从服务器返回 +CONTINUE，表示可以进行部分重同步\n 3. 主服务器将偏移量 10086 之后的数据（偏移量 10087 至 10119）发送给从服务器\n 4. 从服务器接收这 33 字节的数据后，与主服务器的状态保持一致\n\n\n\n# 服务器运行 ID\n\n除复制偏移量和复制积压缓冲区外，部分重同步还依赖于「服务器运行 ID」\n\n * 每个 Redis 服务器（无论是主服务器还是从服务器）都有一个唯一的运行 ID，由 40 个随机的十六进制字符组成，例如：53b9b28df8042fdc9ab5e3fcbbbabff1d5dce2b3\n\n运行 ID 的使用流程如下：\n\n 1. 当从服务器首次复制主服务器时，主服务器会将自己的运行 ID 发送给从服务器，从服务器保存该运行 ID\n 2. 当从服务器断线并重新连接到主服务器时，会发送之前保存的运行 ID 给当前连接的主服务器：\n    * 如果两者的运行 ID 相同，说明从服务器之前复制的就是该主服务器，主服务器可以执行部分重同步操作\n    * 如果运行 ID 不同，说明从服务器之前复制的主服务器与当前连接的主服务器不同，主服务器需要执行完整重同步操作\n\n\n# 深入了解 PSYNC 命令\n\n我们已经学习了 replication offset、replication backlog 和 run ID。接下来，我们将深入探讨 PSYNC 命令的完整细节。\n\nPSYNC 命令的调用方法有两种：\n\n 1. 从服务器的初次复制：如果从服务器以前没有复制过任何主服务器，或者之前执行过 SLAVEOF no one 命令，那么从服务器在开始新的复制时将向主服务器发送 PSYNC ? -1 命令，主动请求主服务器进行完整重同步（因为此时不可能执行部分重同步）。\n 2. 从服务器的再次复制：如果从服务器已经复制过某个主服务器，那么从服务器在开始新的复制时将向主服务器发送 PSYNC <runid> <offset> 命令，其中 runid 是上一次复制的主服务器的运行 ID，而 offset 则是从服务器当前的复制偏移量。接收到这个命令的主服务器会通过这两个参数判断应该对从服务器执行哪种同步操作。\n\n根据情况，接收到 PSYNC 命令的主服务器会向从服务器返回以下三种回复之一：\n\n 1. 如果主服务器返回 +FULLRESYNC <runid> <offset> 回复，则表示主服务器将与从服务器执行完整重同步操作。runid 是该主服务器的运行 ID，从服务器会将该 ID 保存起来，以便在下一次发送 PSYNC 命令时使用；offset 是主服务器当前的复制偏移量，从服务器会将该值作为自己的初始化偏移量。\n 2. 如果主服务器返回 +CONTINUE 回复，则表示主服务器将与从服务器执行部分重同步操作。从服务器只需等待主服务器将缺少的数据发送过来即可。\n 3. 如果主服务器返回 -ERR 回复，则表示主服务器的版本低于 Redis 2.8，无法识别 PSYNC 命令。在这种情况下，从服务器将退回到发送 SYNC 命令，并与主服务器执行完整同步操作。这种降级兼容性处理确保了主从复制在旧版本环境下仍然可以正常进行。\n\n\n\n\n# 复制流程详解\n\n我们之前讲述了新旧版本的复制细节，现在将整个复制流程串联到 Redis 主线中。\n\n# 步骤 1：设置主服务器的地址和端口\n\n当客户端向从服务器发送 SLAVEOF 127.0.0.1 6379 命令时：\n\nstruct redisServer {\n    ...\n    // 主服务器的地址\n    char *masterhost;\n    // 主服务器的端口\n    int masterport;\n    ...\n};\n\n\n从服务器在 redisServer 结构体中设置主服务器的地址和端口。\n\n注意\n\nSLAVEOF 命令是一个异步命令。在完成 masterhost 属性和 masterport 属性的设置工作后，从服务器将向发送 SLAVEOF 命令的客户端返回 OK，表示复制指令已经被接收，而实际的复制工作将在 OK 返回之后才真正开始执行。\n\n# 步骤 2：建立套接字连接\n\n在 SLAVEOF 命令执行之后，从服务器将根据命令所设置的 IP 地址和端口，创建与主服务器的套接字连接。\n\n\n\n如果从服务器创建的套接字成功连接到主服务器，则从服务器将为该套接字关联一个专门用于处理复制工作的文件事件处理器，该处理器负责执行后续的复制工作，比如接收 RDB 文件，以及接收主服务器传播来的写命令等。\n\n主服务器在接受从服务器的套接字连接后，将为该套接字创建相应的客户端状态，并将从服务器视作一个连接到主服务器的客户端。这时，从服务器同时具有服务器（server）和客户端（client）两个身份：从服务器可以向主服务器发送命令请求，而主服务器则会向从服务器返回命令回复。\n\n# 步骤 3：发送 PING 命令\n\n一图胜千言\n\n\n\n# 步骤 4：身份验证\n\n从服务器在收到主服务器返回的 "PONG" 回复之后，下一步是决定是否进行身份验证：\n\n * 如果从服务器设置了 masterauth 选项，则进行身份验证。\n * 如果从服务器没有设置 masterauth 选项，则不进行身份验证。\n\n在需要进行身份验证的情况下，从服务器主动向主服务器发送 AUTH 命令，命令的参数为从服务器 masterauth 选项的值。\n\n\n\n整个流程如下所示：\n\n\n\n# 步骤 5：发送端口信息\n\n在身份验证步骤之后，从服务器将执行命令 REPLCONF listening-port <port-number>，向主服务器发送从服务器的监听端口号。\n\n\n\n主服务器在接收到该命令后，会将端口号记录在从服务器所对应的客户端状态的 slave listening port 属性中：\n\ntypedef struct redisClient {\n    ...\n    // 从服务器的监听端口号\n    int slave_listening_port;\n    ...\n} redisClient;\n\n\n> slave_listening_port 属性目前唯一的作用是在主服务器执行 INFO replication 命令时打印出从服务器的端口号。\n\n# 步骤 6：同步\n\n在这一步，从服务器将向主服务器发送 PSYNC 命令，执行同步操作，并将自己的数据库更新至主服务器数据库当前所处的状态。值得一提的是，在同步操作执行之前，只有从服务器是主服务器的客户端；但在执行同步操作之后，主服务器也会成为从服务器的客户端。\n\n * 如果 PSYNC 命令执行的是完整重同步操作，那么主服务器需要成为从服务器的客户端，才能将保存在缓冲区中的写命令发送给从服务器执行。\n * 如果 PSYNC 命令执行的是部分重同步操作，那么主服务器需要成为从服务器的客户端，才能向从服务器发送保存在复制积压缓冲区中的写命令。\n\n因此，在同步操作执行之后，主从服务器双方都是对方的客户端，它们可以互相发送命令请求或返回命令回复。\n\n正因为主服务器成为了从服务器的客户端，主服务器才能通过发送写命令来改变从服务器的数据库状态。这不仅在同步操作中需要用到，也是主服务器对从服务器执行命令传播的基础操作。\n\n\n\n# 步骤 7：命令传播\n\n完成同步后，主从服务器将进入命令传播阶段。此时，主服务器将持续将其执行的写命令发送给从服务器，而从服务器则持续接收并执行主服务器发来的写命令，以保持主从服务器的一致性。\n\n\n# 心跳检测\n\n在命令传播阶段，从服务器默认以每秒一次的频率，向主服务器发送 REPLCONF ACK <replication offset> 命令，其中 replication offset 是从服务器当前的复制偏移量。\n\n发送 REPLCONF ACK 命令对于主从服务器有三个作用：\n\n 1. 检测主从服务器的网络连接状态：主从服务器通过发送和接收 REPLCONF ACK 命令检查网络连接是否正常，同时监测网络延迟。主服务器通过监测从服务器的复制偏移量，可以快速感知网络抖动和数据同步状态，从而及时识别和处理可能的数据丢失或延迟问题。心跳机制的及时响应有助于主服务器快速感知与从服务器的连接状态，当心跳丢失时，主服务器可以立即采取措施，保证数据的一致性和可靠性。\n\n 2. 辅助实现 min-slaves 配置选项：Redis 的 min-slaves-to-write 和 min-slaves-max-lag 两个选项可以防止主服务器在不安全的情况下执行写命令。例如，如果设置了以下选项：\n    \n    * min-slaves-to-write 3\n    * min-slaves-max-lag 10\n    \n    那么在从服务器数量少于 3 个，或者三个从服务器的延迟值都大于或等于 10 秒时，主服务器将拒绝执行写命令。\n\n 3. 检测命令丢失：如果因为网络故障，主服务器传播给从服务器的写命令在途中丢失，那么从服务器需要通过不断增加的偏移量来告知主服务器，主服务器将检查其复制积压缓冲区，补发丢失的写命令。\n\n已经使用 TCP 了，为什么还要保证消息可靠传达?\n\n\n\n注意\n\n * 在传输层，TCP的三次握手保证了双方通讯的可靠性，稳定性。简而言之，用户发送的消息， 在忽视应用层的情况下，无论如何都会从自身主机的 “发送缓冲区” 抵达对方主机的 “接收缓冲区”\n * 在应用层，数据包有可能因为用户突然的切后台或者是弱网状态导致没法从操作系统内核抵达应用层，反之也是如此, 为此,我们需要在应用层做好可靠传输协议的保证，防止数据丢失的情况\n\n所以，尽管 TCP 能保证传输层的数据可靠性，但在应用层，数据包有可能因为网络不稳定等因素导致丢失。因此，Redis 使用 REPLCONF ACK 命令来在应用层增加传输可靠性。REPLCONF ACK 命令不仅可以检测网络延迟，还能通过主服务器监控从服务器的复制偏移量，及时重传丢失的数据，确保数据的一致性。\n\n\n# 主从复制的触发时机\n\n 1. 从节点首次启动或重新连接时：从节点连接到主节点后，触发全量或增量同步。\n 2. 主节点有数据更新时：主节点每次执行写操作后，增量数据会实时同步给从节点。\n 3. 主节点的手动或自动备份操作：执行 BGSAVE、BGREWRITEAOF 等命令时，可能触发主从数据同步。\n 4. 主从配置变更或故障转移时：如通过 Sentinel 或集群模式进行故障转移，或者手动执行 SLAVEOF 命令时，从节点会重新与新的主节点同步。\n 5. 从节点主动请求同步：从节点落后于主节点的数据或出现不一致时，会主动请求主节点重新同步。\n 6. 复制积压缓冲区溢出：如果从节点长时间未同步，主节点的缓冲区溢出，触发全量同步。\n 7. 集群模式下的主从复制：在 Redis 集群模式中，当集群分片中的某个主节点故障时，集群会通过选举机制选择一个新的主节点。被选中的从节点会与其他从节点进行主从复制，保持数据一致性。\n 8. Sentinel 模式的主从复制：在 Sentinel 模式下，Sentinel 负责监控主从节点的健康状态。当主节点故障时，Sentinel 会自动将某个从节点提升为新的主节点，并通知其他从节点与新的主节点进行同步。\n\n\n# 总结\n\n * Redis 2.8 以前的复制功能不能高效地处理断线后重复制情况，但Redis 2.8新添加的部分重同步功能可以解决这个问题。\n * 部分重同步通过复制偏移量、复制积压缓冲区、服务器运行ID三个部分来实现\n * 在复制操作刚开始的时候，从服务器会成为主服务器的客户端，并通过向主服务器发送命令请求来执行复制步骤，而在复制操作的后期，主从服务器会互相成为对方的客户端。\n * 主服务器通过向从服务器传播命令来更新从服务器的状态，保持主从服务器一致而从服务器则通过向主服务器发送命令来进行心跳检测，以及命令丢失检测。\n\n\n# 参考文献\n\n * 彻底搞懂Redis主从复制原理及实战 - cooffeeli - 博客园 (cnblogs.com)\n\n * 深入学习Redis（3）：主从复制 - 编程迷思 - 博客园 (cnblogs.com)\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 为什么 redis 早期的 sync 操作在断线重连时效率低下？\n 2. 如何通过 replication offset 和 replication backlog 实现高效的部分重同步？\n 3. 运行id在主从断线重连时如何帮助确定同步方式？\n 4. psync 如何优化断线后重连时的主从复制效率？\n 5. 为什么即使有 tcp 传输，redis 仍需通过心跳机制确保数据一致？\n\n\n# 前言\n\n\n\n在 redis 中，用户可以通过 slaveof 命令或设置 slaveof 选项，让一个 redis 服务器复制另一个 redis 服务器。我们称它们分别为从服务器（slave）和主服务器（master）。\n\n在开始之前，我假设你已经了解主从复制功能的基本使用方法。\n\n\n# 旧版复制\n\nredis 的复制功能分为两个阶段：\n\n * 同步：用于将从服务器的数据库状态更新至主服务器的当前状态。\n * 命令传播：用于在主服务器状态被修改时，实时将修改命令告知从服务器。\n\n\n# 第一步：同步\n\n当从服务器首次发送 slaveof 命令来复制主服务器时，必须执行同步（sync）操作。\n\n\n\n执行步骤如下：\n\n 1. 从服务器向主服务器发送 sync 命令。\n 2. 主服务器收到 sync 命令后，执行 bgsave 命令，并开始记录从当前时刻起的所有写命令到缓冲区中。\n 3. bgsave 执行完毕后，主服务器将生成的 rdb 文件发送给从服务器。\n 4. 从服务器接收并载入该 rdb 文件。\n 5. 主服务器将缓冲区中所有的写命令发送给从服务器，从服务器执行这些命令后，状态与主服务器保持一致。\n\n\n\n\n# 第二步：命令传播\n\n同步完成后，主服务器的状态如果发生变化，它会实时向从服务器发送写命令，确保从服务器的状态与主服务器保持一致。\n\n\n# 旧版的缺陷\n\n在同步过程中，可能会遇到以下两种情况：\n\n 1. 初次复制：从服务器首次复制某个主服务器的全部数据。\n 2. 断线后重复制：处于命令传播阶段的从服务器与主服务器断开连接后，再次自动连接上主服务器并继续复制。\n\n对于断线后的重复制来说，没必要再次同步整个 rdb 文件，只需同步断线期间缺失的命令即可。\n\nsync 命令是一个非常耗费资源的操作\n\n 1. 主服务器需要执行 bgsave 命令来生成 rdb 文件，这会消耗大量的 cpu、内存和磁盘 i/o 资源。\n 2. 主服务器需要将生成的 rdb 文件发送给从服务器，传输过程会占用大量网络资源，并影响主服务器响应其他命令请求的时间。\n 3. 从服务器在接收到 rdb 文件后，需要加载该文件，在加载期间从服务器将无法处理任何命令请求。\n\n由于 sync 操作的高资源消耗，redis 需要确保只在必要时执行 sync 操作。\n\n\n# 新版复制\n\n新版复制引入的目的是解决旧版复制在断线后重新同步时的效率低下问题。\n\n自 redis 2.8 起，使用 psync 命令取代 sync 命令来执行复制操作。\n\npsync 具有两种模式：\n\n * 完整重同步：处理初次复制的情况，其执行步骤与 sync 命令类似。\n * 部分重同步：用于处理断线后重复制的情况。当从服务器在断线后重新连接到主服务器时，如果条件允许，主服务器可以仅将断线期间的写命令发送给从服务器，从服务器执行这些命令后即可与主服务器保持一致。\n\npsync 的部分重同步模式有效解决了旧版复制在处理断线后重复制时的低效问题。\n\n\n\n\n# 部分重同步的实现\n\n部分重同步的实现依赖以下三个关键组件：\n\n * replication offset：主服务器和从服务器的复制偏移量。\n * replication backlog：主服务器的复制积压缓冲区。\n * run id：主服务器的运行 id。\n\n# 复制偏移量\n\n复制的双方都会维护各自的「复制偏移量」：\n\n * 主服务器每次向从服务器传播 n 字节的数据时，会将自身的「复制偏移量」加上 n。\n * 从服务器每次收到 n 字节的数据时，也会将自身的「复制偏移量」加上 n。\n\n\n\n通过比较主从服务器的复制偏移量，可以轻松判断两者是否一致：\n\n * 如果两者的复制偏移量相同，则主从服务器处于一致状态。\n * 如果复制偏移量不同，则主从服务器处于不一致状态。\n\n# 复制积压缓冲区\n\n「复制积压缓冲区」是由主服务器维护的一个固定大小的环形缓冲区，默认大小为 1mb。缓冲区使用先进先出（fifo）策略，当数据超过缓冲区大小时，最旧的数据会被覆盖。\n\n主服务器在进行命令传播时，除了将写命令发送给所有从服务器，还会将命令存入复制积压缓冲区。\n\n\n\n缓冲区会保存最近传播的写命令，并为每个字节记录对应的复制偏移量。\n\n\n\n当从服务器重新连接主服务器时，会通过 psync 命令将自身的复制偏移量发送给主服务器，主服务器根据偏移量决定同步方式：\n\n * 如果偏移量之后的数据仍然存在于「复制积压缓冲区」中，则主服务器执行部分重同步操作。\n * 如果偏移量之后的数据已不在缓冲区中，则主服务器执行完整重同步操作。\n\n回到断线重连的例子：\n\n 1. 当从服务器 a 断线后重新连接主服务器时，发送 psync 命令并报告自身的复制偏移量为 10086。\n 2. 主服务器检查「复制积压缓冲区」中的数据，确认偏移量 10086 之后的数据仍然存在，因此向从服务器返回 +continue，表示可以进行部分重同步\n 3. 主服务器将偏移量 10086 之后的数据（偏移量 10087 至 10119）发送给从服务器\n 4. 从服务器接收这 33 字节的数据后，与主服务器的状态保持一致\n\n\n\n# 服务器运行 id\n\n除复制偏移量和复制积压缓冲区外，部分重同步还依赖于「服务器运行 id」\n\n * 每个 redis 服务器（无论是主服务器还是从服务器）都有一个唯一的运行 id，由 40 个随机的十六进制字符组成，例如：53b9b28df8042fdc9ab5e3fcbbbabff1d5dce2b3\n\n运行 id 的使用流程如下：\n\n 1. 当从服务器首次复制主服务器时，主服务器会将自己的运行 id 发送给从服务器，从服务器保存该运行 id\n 2. 当从服务器断线并重新连接到主服务器时，会发送之前保存的运行 id 给当前连接的主服务器：\n    * 如果两者的运行 id 相同，说明从服务器之前复制的就是该主服务器，主服务器可以执行部分重同步操作\n    * 如果运行 id 不同，说明从服务器之前复制的主服务器与当前连接的主服务器不同，主服务器需要执行完整重同步操作\n\n\n# 深入了解 psync 命令\n\n我们已经学习了 replication offset、replication backlog 和 run id。接下来，我们将深入探讨 psync 命令的完整细节。\n\npsync 命令的调用方法有两种：\n\n 1. 从服务器的初次复制：如果从服务器以前没有复制过任何主服务器，或者之前执行过 slaveof no one 命令，那么从服务器在开始新的复制时将向主服务器发送 psync ? -1 命令，主动请求主服务器进行完整重同步（因为此时不可能执行部分重同步）。\n 2. 从服务器的再次复制：如果从服务器已经复制过某个主服务器，那么从服务器在开始新的复制时将向主服务器发送 psync <runid> <offset> 命令，其中 runid 是上一次复制的主服务器的运行 id，而 offset 则是从服务器当前的复制偏移量。接收到这个命令的主服务器会通过这两个参数判断应该对从服务器执行哪种同步操作。\n\n根据情况，接收到 psync 命令的主服务器会向从服务器返回以下三种回复之一：\n\n 1. 如果主服务器返回 +fullresync <runid> <offset> 回复，则表示主服务器将与从服务器执行完整重同步操作。runid 是该主服务器的运行 id，从服务器会将该 id 保存起来，以便在下一次发送 psync 命令时使用；offset 是主服务器当前的复制偏移量，从服务器会将该值作为自己的初始化偏移量。\n 2. 如果主服务器返回 +continue 回复，则表示主服务器将与从服务器执行部分重同步操作。从服务器只需等待主服务器将缺少的数据发送过来即可。\n 3. 如果主服务器返回 -err 回复，则表示主服务器的版本低于 redis 2.8，无法识别 psync 命令。在这种情况下，从服务器将退回到发送 sync 命令，并与主服务器执行完整同步操作。这种降级兼容性处理确保了主从复制在旧版本环境下仍然可以正常进行。\n\n\n\n\n# 复制流程详解\n\n我们之前讲述了新旧版本的复制细节，现在将整个复制流程串联到 redis 主线中。\n\n# 步骤 1：设置主服务器的地址和端口\n\n当客户端向从服务器发送 slaveof 127.0.0.1 6379 命令时：\n\nstruct redisserver {\n    ...\n    // 主服务器的地址\n    char *masterhost;\n    // 主服务器的端口\n    int masterport;\n    ...\n};\n\n\n从服务器在 redisserver 结构体中设置主服务器的地址和端口。\n\n注意\n\nslaveof 命令是一个异步命令。在完成 masterhost 属性和 masterport 属性的设置工作后，从服务器将向发送 slaveof 命令的客户端返回 ok，表示复制指令已经被接收，而实际的复制工作将在 ok 返回之后才真正开始执行。\n\n# 步骤 2：建立套接字连接\n\n在 slaveof 命令执行之后，从服务器将根据命令所设置的 ip 地址和端口，创建与主服务器的套接字连接。\n\n\n\n如果从服务器创建的套接字成功连接到主服务器，则从服务器将为该套接字关联一个专门用于处理复制工作的文件事件处理器，该处理器负责执行后续的复制工作，比如接收 rdb 文件，以及接收主服务器传播来的写命令等。\n\n主服务器在接受从服务器的套接字连接后，将为该套接字创建相应的客户端状态，并将从服务器视作一个连接到主服务器的客户端。这时，从服务器同时具有服务器（server）和客户端（client）两个身份：从服务器可以向主服务器发送命令请求，而主服务器则会向从服务器返回命令回复。\n\n# 步骤 3：发送 ping 命令\n\n一图胜千言\n\n\n\n# 步骤 4：身份验证\n\n从服务器在收到主服务器返回的 "pong" 回复之后，下一步是决定是否进行身份验证：\n\n * 如果从服务器设置了 masterauth 选项，则进行身份验证。\n * 如果从服务器没有设置 masterauth 选项，则不进行身份验证。\n\n在需要进行身份验证的情况下，从服务器主动向主服务器发送 auth 命令，命令的参数为从服务器 masterauth 选项的值。\n\n\n\n整个流程如下所示：\n\n\n\n# 步骤 5：发送端口信息\n\n在身份验证步骤之后，从服务器将执行命令 replconf listening-port <port-number>，向主服务器发送从服务器的监听端口号。\n\n\n\n主服务器在接收到该命令后，会将端口号记录在从服务器所对应的客户端状态的 slave listening port 属性中：\n\ntypedef struct redisclient {\n    ...\n    // 从服务器的监听端口号\n    int slave_listening_port;\n    ...\n} redisclient;\n\n\n> slave_listening_port 属性目前唯一的作用是在主服务器执行 info replication 命令时打印出从服务器的端口号。\n\n# 步骤 6：同步\n\n在这一步，从服务器将向主服务器发送 psync 命令，执行同步操作，并将自己的数据库更新至主服务器数据库当前所处的状态。值得一提的是，在同步操作执行之前，只有从服务器是主服务器的客户端；但在执行同步操作之后，主服务器也会成为从服务器的客户端。\n\n * 如果 psync 命令执行的是完整重同步操作，那么主服务器需要成为从服务器的客户端，才能将保存在缓冲区中的写命令发送给从服务器执行。\n * 如果 psync 命令执行的是部分重同步操作，那么主服务器需要成为从服务器的客户端，才能向从服务器发送保存在复制积压缓冲区中的写命令。\n\n因此，在同步操作执行之后，主从服务器双方都是对方的客户端，它们可以互相发送命令请求或返回命令回复。\n\n正因为主服务器成为了从服务器的客户端，主服务器才能通过发送写命令来改变从服务器的数据库状态。这不仅在同步操作中需要用到，也是主服务器对从服务器执行命令传播的基础操作。\n\n\n\n# 步骤 7：命令传播\n\n完成同步后，主从服务器将进入命令传播阶段。此时，主服务器将持续将其执行的写命令发送给从服务器，而从服务器则持续接收并执行主服务器发来的写命令，以保持主从服务器的一致性。\n\n\n# 心跳检测\n\n在命令传播阶段，从服务器默认以每秒一次的频率，向主服务器发送 replconf ack <replication offset> 命令，其中 replication offset 是从服务器当前的复制偏移量。\n\n发送 replconf ack 命令对于主从服务器有三个作用：\n\n 1. 检测主从服务器的网络连接状态：主从服务器通过发送和接收 replconf ack 命令检查网络连接是否正常，同时监测网络延迟。主服务器通过监测从服务器的复制偏移量，可以快速感知网络抖动和数据同步状态，从而及时识别和处理可能的数据丢失或延迟问题。心跳机制的及时响应有助于主服务器快速感知与从服务器的连接状态，当心跳丢失时，主服务器可以立即采取措施，保证数据的一致性和可靠性。\n\n 2. 辅助实现 min-slaves 配置选项：redis 的 min-slaves-to-write 和 min-slaves-max-lag 两个选项可以防止主服务器在不安全的情况下执行写命令。例如，如果设置了以下选项：\n    \n    * min-slaves-to-write 3\n    * min-slaves-max-lag 10\n    \n    那么在从服务器数量少于 3 个，或者三个从服务器的延迟值都大于或等于 10 秒时，主服务器将拒绝执行写命令。\n\n 3. 检测命令丢失：如果因为网络故障，主服务器传播给从服务器的写命令在途中丢失，那么从服务器需要通过不断增加的偏移量来告知主服务器，主服务器将检查其复制积压缓冲区，补发丢失的写命令。\n\n已经使用 tcp 了，为什么还要保证消息可靠传达?\n\n\n\n注意\n\n * 在传输层，tcp的三次握手保证了双方通讯的可靠性，稳定性。简而言之，用户发送的消息， 在忽视应用层的情况下，无论如何都会从自身主机的 “发送缓冲区” 抵达对方主机的 “接收缓冲区”\n * 在应用层，数据包有可能因为用户突然的切后台或者是弱网状态导致没法从操作系统内核抵达应用层，反之也是如此, 为此,我们需要在应用层做好可靠传输协议的保证，防止数据丢失的情况\n\n所以，尽管 tcp 能保证传输层的数据可靠性，但在应用层，数据包有可能因为网络不稳定等因素导致丢失。因此，redis 使用 replconf ack 命令来在应用层增加传输可靠性。replconf ack 命令不仅可以检测网络延迟，还能通过主服务器监控从服务器的复制偏移量，及时重传丢失的数据，确保数据的一致性。\n\n\n# 主从复制的触发时机\n\n 1. 从节点首次启动或重新连接时：从节点连接到主节点后，触发全量或增量同步。\n 2. 主节点有数据更新时：主节点每次执行写操作后，增量数据会实时同步给从节点。\n 3. 主节点的手动或自动备份操作：执行 bgsave、bgrewriteaof 等命令时，可能触发主从数据同步。\n 4. 主从配置变更或故障转移时：如通过 sentinel 或集群模式进行故障转移，或者手动执行 slaveof 命令时，从节点会重新与新的主节点同步。\n 5. 从节点主动请求同步：从节点落后于主节点的数据或出现不一致时，会主动请求主节点重新同步。\n 6. 复制积压缓冲区溢出：如果从节点长时间未同步，主节点的缓冲区溢出，触发全量同步。\n 7. 集群模式下的主从复制：在 redis 集群模式中，当集群分片中的某个主节点故障时，集群会通过选举机制选择一个新的主节点。被选中的从节点会与其他从节点进行主从复制，保持数据一致性。\n 8. sentinel 模式的主从复制：在 sentinel 模式下，sentinel 负责监控主从节点的健康状态。当主节点故障时，sentinel 会自动将某个从节点提升为新的主节点，并通知其他从节点与新的主节点进行同步。\n\n\n# 总结\n\n * redis 2.8 以前的复制功能不能高效地处理断线后重复制情况，但redis 2.8新添加的部分重同步功能可以解决这个问题。\n * 部分重同步通过复制偏移量、复制积压缓冲区、服务器运行id三个部分来实现\n * 在复制操作刚开始的时候，从服务器会成为主服务器的客户端，并通过向主服务器发送命令请求来执行复制步骤，而在复制操作的后期，主从服务器会互相成为对方的客户端。\n * 主服务器通过向从服务器传播命令来更新从服务器的状态，保持主从服务器一致而从服务器则通过向主服务器发送命令来进行心跳检测，以及命令丢失检测。\n\n\n# 参考文献\n\n * 彻底搞懂redis主从复制原理及实战 - cooffeeli - 博客园 (cnblogs.com)\n\n * 深入学习redis（3）：主从复制 - 编程迷思 - 博客园 (cnblogs.com)\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"cluster",frontmatter:{title:"cluster",date:"2024-09-16T03:24:30.000Z",permalink:"/pages/040403/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E4%BA%94%E3%80%81%E9%9B%86%E7%BE%A4/35.cluster.html",relativePath:"Redis 系统设计/05.五、集群/35.cluster.md",key:"v-01f7e9de",path:"/pages/040403/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"节点",slug:"节点",normalizedTitle:"节点",charIndex:80},{level:3,title:"启动节点",slug:"启动节点",normalizedTitle:"启动节点",charIndex:1581},{level:3,title:"集群数据结构",slug:"集群数据结构",normalizedTitle:"集群数据结构",charIndex:2426},{level:3,title:"CLUSTER MEET 命令的实现",slug:"cluster-meet-命令的实现",normalizedTitle:"cluster meet 命令的实现",charIndex:1605},{level:2,title:"槽指派",slug:"槽指派",normalizedTitle:"槽指派",charIndex:83},{level:3,title:"记录节点的槽指派信息",slug:"记录节点的槽指派信息",normalizedTitle:"记录节点的槽指派信息",charIndex:7698},{level:3,title:"传播节点的槽指派信息",slug:"传播节点的槽指派信息",normalizedTitle:"传播节点的槽指派信息",charIndex:8508},{level:3,title:"记录集群所有槽的指派信息",slug:"记录集群所有槽的指派信息",normalizedTitle:"记录集群所有槽的指派信息",charIndex:9243},{level:3,title:"CLUSTER ADDSLOTS 命令的实现",slug:"cluster-addslots-命令的实现",normalizedTitle:"cluster addslots 命令的实现",charIndex:10876},{level:2,title:"在集群中执行命令",slug:"在集群中执行命令",normalizedTitle:"在集群中执行命令",charIndex:12087},{level:3,title:"计算键属于哪个槽",slug:"计算键属于哪个槽",normalizedTitle:"计算键属于哪个槽",charIndex:13006},{level:3,title:"判断槽是否由当前节点负责处理",slug:"判断槽是否由当前节点负责处理",normalizedTitle:"判断槽是否由当前节点负责处理",charIndex:13589},{level:3,title:"MOVED 错误",slug:"moved-错误",normalizedTitle:"moved 错误",charIndex:14442},{level:3,title:"节点数据库的实现",slug:"节点数据库的实现",normalizedTitle:"节点数据库的实现",charIndex:15837},{level:2,title:"重新分片",slug:"重新分片",normalizedTitle:"重新分片",charIndex:92},{level:2,title:"ASK 错误",slug:"ask-错误",normalizedTitle:"ask 错误",charIndex:18918},{level:3,title:"CLUSTER SETSLOT IMPORTING命令的实现",slug:"cluster-setslot-importing命令的实现",normalizedTitle:"cluster setslot importing命令的实现",charIndex:20035},{level:3,title:"CLUSTER SETSLOT MIGRATING 命令的实现",slug:"cluster-setslot-migrating-命令的实现",normalizedTitle:"cluster setslot migrating 命令的实现",charIndex:20702},{level:3,title:"ASK 错误",slug:"ask-错误-2",normalizedTitle:"ask 错误",charIndex:18918},{level:3,title:"ASKING命令",slug:"asking命令",normalizedTitle:"asking命令",charIndex:22001},{level:3,title:"ASK错误和MOVED错误的区别",slug:"ask错误和moved错误的区别",normalizedTitle:"ask错误和moved错误的区别",charIndex:20013},{level:2,title:"复制与故障转移",slug:"复制与故障转移",normalizedTitle:"复制与故障转移",charIndex:23853},{level:3,title:"设置从节点",slug:"设置从节点",normalizedTitle:"设置从节点",charIndex:24703},{level:3,title:"故障检测",slug:"故障检测",normalizedTitle:"故障检测",charIndex:26137},{level:3,title:"故障转移",slug:"故障转移",normalizedTitle:"故障转移",charIndex:64},{level:3,title:"选举新的主节点",slug:"选举新的主节点",normalizedTitle:"选举新的主节点",charIndex:27922},{level:2,title:"消息",slug:"消息",normalizedTitle:"消息",charIndex:105},{level:3,title:"消息头",slug:"消息头",normalizedTitle:"消息头",charIndex:29584},{level:3,title:"MEET、PING、PONG消息的实现",slug:"meet、ping、pong消息的实现",normalizedTitle:"meet、ping、pong消息的实现",charIndex:31234},{level:3,title:"FAIL消息的实现",slug:"fail消息的实现",normalizedTitle:"fail消息的实现",charIndex:32699},{level:3,title:"PUBLISH消息的实现",slug:"publish消息的实现",normalizedTitle:"publish消息的实现",charIndex:33619},{level:2,title:"重点回顾",slug:"重点回顾",normalizedTitle:"重点回顾",charIndex:34943},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:35516}],headersStr:"前言 节点 启动节点 集群数据结构 CLUSTER MEET 命令的实现 槽指派 记录节点的槽指派信息 传播节点的槽指派信息 记录集群所有槽的指派信息 CLUSTER ADDSLOTS 命令的实现 在集群中执行命令 计算键属于哪个槽 判断槽是否由当前节点负责处理 MOVED 错误 节点数据库的实现 重新分片 ASK 错误 CLUSTER SETSLOT IMPORTING命令的实现 CLUSTER SETSLOT MIGRATING 命令的实现 ASK 错误 ASKING命令 ASK错误和MOVED错误的区别 复制与故障转移 设置从节点 故障检测 故障转移 选举新的主节点 消息 消息头 MEET、PING、PONG消息的实现 FAIL消息的实现 PUBLISH消息的实现 重点回顾 参考文献",content:'# 前言\n\nRedis 集群是 Redis 提供的分布式数据库方案，集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能。\n\n本节将对集群的节点、槽指派、命令执行、重新分片、转向、故障转移、消息等各个方面进行介绍\n\n\n# 节点\n\n一个 Redis 集群通常由多个节点（node）组成， 在刚开始的时候， 每个节点都是相互独立的， 它们都处于一个只包含自己的集群当中， 要组建一个真正可工作的集群， 我们必须将各个独立的节点连接起来， 构成一个包含多个节点的集群。\n\n连接各个节点的工作可以使用 CLUSTER MEET 命令来完成， 该命令的格式如下：\n\nCLUSTER MEET <ip> <port>\n\n\n向一个节点 node 发送 CLUSTER MEET 命令， 可以让 node 节点与 ip 和 port 所指定的节点进行握手（handshake）， 当握手成功时， node 节点就会将 ip 和 port 所指定的节点添加到 node 节点当前所在的集群中。\n\n举个例子， 假设现在有三个独立的节点 127.0.0.1:7000 、 127.0.0.1:7001 、 127.0.0.1:7002 （下文省略 IP 地址，直接使用端口号来区分各个节点）， 我们首先使用客户端连上节点 7000 ， 通过发送 CLUSTER NODE 命令可以看到， 集群目前只包含 7000 自己一个节点：\n\n$ redis-cli -c -p 7000\n127.0.0.1:7000> CLUSTER NODES\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n通过向节点 7000 发送以下命令， 我们可以将节点 7001 添加到节点 7000 所在的集群里面：\n\n127.0.0.1:7000> CLUSTER MEET 127.0.0.1 7001\nOK\n\n127.0.0.1:7000> CLUSTER NODES\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388204746210 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n继续向节点 7000 发送以下命令， 我们可以将节点 7002 也添加到节点 7000 和节点 7001 所在的集群里面：\n\n127.0.0.1:7000> CLUSTER MEET 127.0.0.1 7002\nOK\n\n127.0.0.1:7000> CLUSTER NODES\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388204848376 0 connected\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388204847977 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n现在， 这个集群里面包含了 7000 、 7001 和 7002 三个节点， 图 IMAGE_CONNECT_NODES_1 至 IMAGE_CONNECT_NODES_5 展示了这三个节点进行握手的整个过程。\n\n\n\n\n\n\n\n\n\n\n\n本节接下来的内容将介绍启动节点的方法， 和集群有关的数据结构， 以及 CLUSTER MEET 命令的实现原理。\n\n\n# 启动节点\n\n一个节点就是一个运行在集群模式下的 Redis 服务器， Redis 服务器在启动时会根据 cluster-enabled 配置选项的是否为 yes 来决定是否开启服务器的集群模式， 如图 IMAGE_NODE_OR_SERVER 所示。\n\n\n\n节点（运行在集群模式下的 Redis 服务器）会继续使用所有在单机模式中使用的服务器组件， 比如说：\n\n * 节点会继续使用文件事件处理器来处理命令请求和返回命令回复。\n * 节点会继续使用时间事件处理器来执行 serverCron 函数， 而 serverCron 函数又会调用集群模式特有的 clusterCron 函数： clusterCron 函数负责执行在集群模式下需要执行的常规操作， 比如向集群中的其他节点发送 Gossip 消息， 检查节点是否断线； 又或者检查是否需要对下线节点进行自动故障转移， 等等。\n * 节点会继续使用数据库来保存键值对数据，键值对依然会是各种不同类型的对象。\n * 节点会继续使用 RDB 持久化模块和 AOF 持久化模块来执行持久化工作。\n * 节点会继续使用发布与订阅模块来执行 PUBLISH 、 SUBSCRIBE 等命令。\n * 节点会继续使用复制模块来进行节点的复制工作。\n * 节点会继续使用 Lua 脚本环境来执行客户端输入的 Lua 脚本。\n\n诸如此类。\n\n除此之外， 节点会继续使用 redisServer 结构来保存服务器的状态， 使用 redisClient 结构来保存客户端的状态， 至于那些只有在集群模式下才会用到的数据， 节点将它们保存到了 cluster.h/clusterNode 结构， cluster.h/clusterLink 结构， 以及 cluster.h/clusterState 结构里面， 接下来的一节将对这三种数据结构进行介绍\n\n\n# 集群数据结构\n\nclusterNode 结构保存了一个节点的当前状态， 比如节点的创建时间， 节点的名字， 节点当前的配置纪元， 节点的 IP 和地址， 等等。\n\n每个节点都会使用一个 clusterNode 结构来记录自己的状态， 并为集群中的所有其他节点（包括主节点和从节点）都创建一个相应的 clusterNode 结构， 以此来记录其他节点的状态：\n\nstruct clusterNode {\n\n    // 创建节点的时间\n    mstime_t ctime;\n\n    // 节点的名字，由 40 个十六进制字符组成\n    // 例如 68eef66df23420a5862208ef5b1a7005b806f2ff\n    char name[REDIS_CLUSTER_NAMELEN];\n\n    // 节点标识\n    // 使用各种不同的标识值记录节点的角色（比如主节点或者从节点），\n    // 以及节点目前所处的状态（比如在线或者下线）。\n    int flags;\n\n    // 节点当前的配置纪元，用于实现故障转移\n    uint64_t configEpoch;\n\n    // 节点的 IP 地址\n    char ip[REDIS_IP_STR_LEN];\n\n    // 节点的端口号\n    int port;\n\n    // 保存连接节点所需的有关信息\n    clusterLink *link;\n\n};\n\n\nclusterNode 结构的 link 属性是一个 clusterLink 结构， 该结构保存了连接节点所需的有关信息， 比如套接字描述符， 输入缓冲区和输出缓冲区：\n\ntypedef struct clusterLink {\n\n    // 连接的创建时间\n    mstime_t ctime;\n\n    // TCP 套接字描述符\n    int fd;\n\n    // 输出缓冲区，保存着等待发送给其他节点的消息（message）。\n    sds sndbuf;\n\n    // 输入缓冲区，保存着从其他节点接收到的消息。\n    sds rcvbuf;\n\n    // 与这个连接相关联的节点，如果没有的话就为 NULL\n    struct clusterNode *node;\n\n} clusterLink;\n\n\nredisClient 结构和 clusterLink 结构的相同和不同之处\n\nredisClient 结构和 clusterLink 结构都有自己的套接字描述符和输入、输出缓冲区， 这两个结构的区别在于， redisClient 结构中的套接字和缓冲区是用于连接客户端的， 而 clusterLink 结构中的套接字和缓冲区则是用于连接节点的。\n\n最后， 每个节点都保存着一个 clusterState 结构， 这个结构记录了在当前节点的视角下， 集群目前所处的状态 —— 比如集群是在线还是下线， 集群包含多少个节点， 集群当前的配置纪元， 诸如此类：\n\ntypedef struct clusterState {\n\n    // 指向当前节点的指针\n    clusterNode *myself;\n\n    // 集群当前的配置纪元，用于实现故障转移\n    uint64_t currentEpoch;\n\n    // 集群当前的状态：是在线还是下线\n    int state;\n\n    // 集群中至少处理着一个槽的节点的数量\n    int size;\n\n    // 集群节点名单（包括 myself 节点）\n    // 字典的键为节点的名字，字典的值为节点对应的 clusterNode 结构\n    dict *nodes;\n\n} clusterState;\n\n\n以前面介绍的 7000 、 7001 、 7002 三个节点为例， 图 IMAGE_CLUSTER_STATE_OF_7000 展示了节点 7000 创建的 clusterState 结构， 这个结构从节点 7000 的角度记录了集群、以及集群包含的三个节点的当前状态 （为了空间考虑，图中省略了 clusterNode 结构的一部分属性）：\n\n * 结构的 currentEpoch 属性的值为 0 ， 表示集群当前的配置纪元为 0 。\n * 结构的 size 属性的值为 0 ， 表示集群目前没有任何节点在处理槽： 因此结构的 state 属性的值为 REDIS_CLUSTER_FAIL —— 这表示集群目前处于下线状态。\n * 结构的 nodes 字典记录了集群目前包含的三个节点， 这三个节点分别由三个 clusterNode 结构表示： 其中 myself 指针指向代表节点 7000 的 clusterNode 结构， 而字典中的另外两个指针则分别指向代表节点 7001 和代表节点 7002 的 clusterNode 结构， 这两个节点是节点 7000 已知的在集群中的其他节点。\n * 三个节点的 clusterNode 结构的 flags 属性都是 REDIS_NODE_MASTER ，说明三个节点都是主节点。\n\n节点 7001 和节点 7002 也会创建类似的 clusterState 结构：\n\n * 不过在节点 7001 创建的 clusterState 结构中， myself 指针将指向代表节点 7001 的 clusterNode 结构， 而节点 7000 和节点 7002 则是集群中的其他节点。\n * 而在节点 7002 创建的 clusterState 结构中， myself 指针将指向代表节点 7002 的 clusterNode 结构， 而节点 7000 和节点 7001 则是集群中的其他节点。\n\n\n\n\n# CLUSTER MEET 命令的实现\n\n通过向节点 A 发送 CLUSTER MEET 命令， 客户端可以让接收命令的节点 A 将另一个节点 B 添加到节点 A 当前所在的集群里面：\n\nCLUSTER MEET <ip> <port>\n\n\n收到命令的节点 A 将与节点 B 进行握手（handshake）， 以此来确认彼此的存在， 并为将来的进一步通信打好基础：\n\n 1. 节点 A 会为节点 B 创建一个 clusterNode 结构， 并将该结构添加到自己的 clusterState.nodes 字典里面。\n 2. 之后， 节点 A 将根据 CLUSTER MEET 命令给定的 IP 地址和端口号， 向节点 B 发送一条 MEET 消息（message）。\n 3. 如果一切顺利， 节点 B 将接收到节点 A 发送的 MEET 消息， 节点 B 会为节点 A 创建一个 clusterNode 结构， 并将该结构添加到自己的 clusterState.nodes 字典里面。\n 4. 之后， 节点 B 将向节点 A 返回一条 PONG 消息。\n 5. 如果一切顺利， 节点 A 将接收到节点 B 返回的 PONG 消息， 通过这条 PONG 消息节点 A 可以知道节点 B 已经成功地接收到了自己发送的 MEET 消息。\n 6. 之后， 节点 A 将向节点 B 返回一条 PING 消息。\n 7. 如果一切顺利， 节点 B 将接收到节点 A 返回的 PING 消息， 通过这条 PING 消息节点 B 可以知道节点 A 已经成功地接收到了自己返回的 PONG 消息， 握手完成。\n\n图 IMAGE_HANDSHAKE 展示了以上步骤描述的握手过程。\n\n\n\n之后， 节点 A 会将节点 B 的信息通过 Gossip 协议传播给集群中的其他节点， 让其他节点也与节点 B 进行握手， 最终， 经过一段时间之后， 节点 B 会被集群中的所有节点认识\n\n\n# 槽指派\n\nRedis 集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为 16384 个槽（slot），数据库中的每个键都属于这 16384个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。\n\n * 当数据库中的 16384 个槽都有节点在处理时，集群处于上线状态（ok）\n * 如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态（fail）\n\n在上一节，我们使用 CLUSTER MEET 命令将 7000、7001、7002 三个节点连接到了同一个集群里面，不过这个集群目前仍然处于下线状态，因为集群中的三个节点都没有在处理任何槽：\n\n127.0.0.1:7000> CLUSTER INFO\ncluster_state:fail\ncluster_slots_assigned:0\ncluster_slots_ok:0\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:3\ncluster_size:0\ncluster_current_epoch:0\ncluster_stats_messages_sent:110\ncluster_stats_messages_received:28\n\n\n通过向节点发送 CLUSTER ADDSLOTS 命令，我们可以将一个或多个槽指派（assign）给节点负责：\n\nCLUSTER ADDSLOTS <slot> [slot]\n\n\n举个例子，执行以下命令可以将槽0至槽5000指派给节点7000负责：\n\n127.0.0.1:7000> CLUSTER ADDSLOTS 0 1 2 3 4  5000\nOK\n127.0.0.1:7000> CLUSTER NODES\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388316664849 0 connected\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388316665850 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n\n\n为了让 7000、7001、7002 三个节点所在的集群进入上线状态，我们继续执行以下命令，将槽 5001 至槽 10000 指派给节点 7001 负责：\n\n127.0.0.1:7001> CLUSTER ADDSLOTS 5001 5002 5003 5004 10000\nOK\n\n\n然后将槽 10001 至槽 16383 指派给7002负责：\n\n127.0.0.1:7002> CLUSTER ADDSLOTS 10001 10002 10003 10004 16383\nOK\n\n\n当以上三个CLUSTER ADDSLOTS命令都执行完毕之后，数据库中的16384个槽都已经被指派给了相应的节点，集群进入上线状态：\n\n127.0.0.1:7000> CLUSTER INFO\ncluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:3\ncluster_size:3\ncluster_current_epoch:0\ncluster_stats_messages_sent:2699\ncluster_stats_messages_received:2617\n127.0.0.1:7000> CLUSTER NODES\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388317426165 0 connected 10001-16383\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388317427167 0 connected 5001-10000\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n\n\n本节接下来的内容将首先介绍节点保存槽指派信息的方法，以及节点之间传播槽指派信息的方法，之后再介绍CLUSTER ADDSLOTS命令的实现。\n\n\n# 记录节点的槽指派信息\n\nclusterNode结构的slots属性和numslot属性记录了节点负责处理哪些槽：\n\nstruct clusterNode {\n\n  unsigned char slots[16384/8];\n  int numslots;\n \n};\n\n\nslots属性是一个二进制位数组（bit array），这个数组的长度为16384/8=2048个字节，共包含16384个二进制位。\n\nRedis以0为起始索引，16383为终止索引，对slots数组中的16384个二进制位进行编号，并根据索引i上的二进制位的值来判断节点是否负责处理槽i：\n\n * 如果slots数组在索引i上的二进制位的值为1，那么表示节点负责处理槽i。\n\n * 如果slots数组在索引i上的二进制位的值为0，那么表示节点不负责处理槽i。\n\n图17-9展示了一个slots数组示例：这个数组索引0至索引7上的二进制位的值都为1，其余所有二进制位的值都为0，这表示节点负责处理槽0至槽7。\n\n\n\n图17-9　一个slots数组示例\n\n图17-10 展示了另一个slots数组示例：这个数组索引1、3、5、8、9、10上的二进制位的值都为1，而其余所有二进制位的值都为0，这表示节点负责处理槽1、3、5、8、9、10。\n\n\n\n图17-10　另一个slots数组示例\n\n因为取出和设置slots数组中的任意一个二进制位的值的复杂度仅为O（1），所以对于一个给定节点的slots数组来说，程序检查节点是否负责处理某个槽，又或者将某个槽指派给节点负责，这两个动作的复杂度都是O（1）。\n\n至于numslots属性则记录节点负责处理的槽的数量，也即是slots数组中值为1的二进制位的数量。\n\n比如说，对于图17-9所示的slots数组来说，节点处理的槽数量为8，而对于图17-10所示的slots数组来说，节点处理的槽数量为6。\n\n\n# 传播节点的槽指派信息\n\n一个节点除了会将自己负责处理的槽记录在 clusterNode 结构的 slots 属性和 numslots 属性之外，它还会将自己的 slots 数组通过消息发送给集群中的其他节点，以此来告知其他节点自己目前负责处理哪些槽。\n\n举个例子，对于前面展示的包含7000、7001、7002三个节点的集群来说：\n\n * 节点7000会通过消息向节点7001和节点7002发送自己的slots数组，以此来告知这两个节点，自己负责处理槽0至槽5000，如图17-11所示。\n\n * 节点7001会通过消息向节点7000和节点7002发送自己的slots数组，以此来告知这两个节点，自己负责处理槽5001至槽10000，如图17-12所示。\n\n * 节点7002会通过消息向节点7000和节点7001发送自己的slots数组，以此来告知这两个节点，自己负责处理槽10001至槽16383，如图17-13所示。\n\n\n\n图17-11　7000告知7001和7002自己负责处理的槽\n\n\n\n\n\n图17-13　7002告知7000和7001自己负责处理的槽\n\n当节点A通过消息从节点B那里接收到节点B的slots数组时，节点A会在自己的 clusterState.nodes 字典中查找节点B对应的 clusterNode 结构，并对结构中的 slots 数组进行保存或者更新。\n\n因为集群中的每个节点都会将自己的 slots 数组通过消息发送给集群中的其他节点，并且每个接收到 slots 数组的节点都会将数组保存到相应节点的 clusterNode 结构里面，因此，集群中的每个节点都会知道数据库中的 16384 个槽分别被指派给了集群中的哪些节点。\n\n\n# 记录集群所有槽的指派信息\n\nclusterState 结构中的 slots 数组记录了集群中所有 16384 个槽的指派信息：\n\ntypedef struct clusterState {\n \n  clusterNode *slots[16384];\n  \n} clusterState;\n\n\nslots数组包含16384个项，每个数组项都是一个指向clusterNode结构的指针：\n\n * 如果slots[i]指针指向NULL，那么表示槽i尚未指派给任何节点。\n\n * 如果slots[i]指针指向一个clusterNode结构，那么表示槽i已经指派给了clusterNode结构所代表的节点。\n\n举个例子，对于7000、7001、7002三个节点来说，它们的clusterState结构的slots数组将会是图17-14所示的样子：\n\n * 数组项slots[0]至slots[5000]的指针都指向代表节点7000的clusterNode结构，表示槽0至5000都指派给了节点7000。\n\n * 数组项slots[5001]至slots[10000]的指针都指向代表节点7001的clusterNode结构，表示槽5001至10000都指派给了节点7001。\n\n * 数组项slots[10001]至slots[16383]的指针都指向代表节点7002的clusterNode结构，表示槽10001至16383都指派给了节点7002。\n\n如果只将槽指派信息保存在各个节点的clusterNode.slots数组里，会出现一些无法高效地解决的问题，而clusterState.slots数组的存在解决了这些问题：\n\n * 如果节点只使用clusterNode.slots数组来记录槽的指派信息，那么为了知道槽i是否已经被指派，或者槽i被指派给了哪个节点，程序需要遍历clusterState.nodes字典中的所有clusterNode结构，检查这些结构的slots数组，直到找到负责处理槽i的节点为止，这个过程的复杂度为O（N），其中N为clusterState.nodes字典保存的clusterNode结构的数量。\n\n * 而通过将所有槽的指派信息保存在clusterState.slots数组里面，程序要检查槽i是否已经被指派，又或者取得负责处理槽i的节点，只需要访问clusterState.slots[i]的值即可，这个操作的复杂度仅为O(1)。\n\n举个例子，对于图17-14所示的slots数组来说，如果程序需要知道槽10002被指派给了哪个节点，那么只要访问数组项slots[10002]，就可以马上知道槽10002被指派给了节点7002，如图17-15所示。\n\n\n\n图17-15　访问slots[10002]的值\n\n要说明的一点是，虽然clusterState.slots数组记录了集群中所有槽的指派信息，但使用clusterNode结构的slots数组来记录单个节点的槽指派信息仍然是有必要的：\n\n * 因为当程序需要将某个节点的槽指派信息通过消息发送给其他节点时，程序只需要将相应节点的clusterNode.slots数组整个发送出去就可以了。\n\n * 另一方面，如果Redis不使用clusterNode.slots数组，而单独使用clusterState.slots数组的话，那么每次要将节点A的槽指派信息传播给其他节点时，程序必须先遍历整个clusterState.slots数组，记录节点A负责处理哪些槽，然后才能发送节点A的槽指派信息，这比直接发送clusterNode.slots数组要麻烦和低效得多。\n\nclusterState.slots数组记录了集群中所有槽的指派信息，而clusterNode.slots数组只记录了clusterNode结构所代表的节点的槽指派信息，这是两个slots数组的关键区别所在。\n\n\n# CLUSTER ADDSLOTS 命令的实现\n\nCLUSTER ADDSLOTS 命令接受一个或多个槽作为参数，并将所有输入的槽指派给接收该命令的节点负责：\n\nCLUSTER ADDSLOTS <slot> [slot]\n\n\nCLUSTER ADDSLOTS 命令的实现可以用以下伪代码来表示：\n\ndef CLUSTER_ADDSLOTS(*all_input_slots):\n    # 遍历所有输入槽，检查它们是否都是未指派槽\n    for i in all_input_slots:\n        # 如果有哪怕一个槽已经被指派给了某个节点\n        # 那么向客户端返回错误，并终止命令执行\n        if clusterState.slots[i] != NULL:\n            reply_error()\n            return\n    # 如果所有输入槽都是未指派槽\n    # 那么再次遍历所有输入槽，将这些槽指派给当前节点\n    for i in all_input_slots:\n        # 设置clusterState结构的slots数组\n        # 将slots[i]的指针指向代表当前节点的clusterNode结构\n        clusterState.slots[i] = clusterState.myself\n        # 访问代表当前节点的clusterNode结构的slots数组\n        # 将数组在索引i上的二进制位设置为1\n        setSlotBit(clusterState.myself.slots, i)\n\n\n举个例子，图17-16展示了一个节点的clusterState结构，clusterState.slots数组中的所有指针都指向NULL，并且clusterNode.slots数组中的所有二进制位的值都是0，这说明当前节点没有被指派任何槽，并且集群中的所有槽都是未指派的。\n\n\n\n图17-16　节点的clusterState结构\n\n当客户端对17-16所示的节点执行命令：\n\nCLUSTER ADDSLOTS 1 2\n\n\n将槽1和槽2指派给节点之后，节点的clusterState结构将被更新成图17-17所示的样子：\n\n * clusterState.slots数组在索引1和索引2上的指针指向了代表当前节点的clusterNode结构。\n\n * 并且clusterNode.slots数组在索引1和索引2上的位被设置成了1。\n\n\n\n图17-17　执行 CLUSTER ADDSLOTS 命令之后的 clusterState 结构\n\n最后，在 CLUSTER ADDSLOTS 命令执行完毕之后，节点会通过发送消息告知集群中的其他节点，自己目前正在负责处理哪些槽。\n\n\n# 在集群中执行命令\n\n在对数据库中的16384个槽都进行了指派之后，集群就会进入上线状态，这时客户端就可以向集群中的节点发送数据命令了。\n\n当客户端向节点发送与数据库键有关的命令时，接收命令的节点会计算出命令要处理的数据库键属于哪个槽，并检查这个槽是否指派给了自己：\n\n * 如果键所在的槽正好就指派给了当前节点，那么节点直接执行这个命令。\n\n * 如果键所在的槽并没有指派给当前节点，那么节点会向客户端返回一个MOVED错误，指引客户端转向（redirect）至正确的节点，并再次发送之前想要执行的命令。\n\n图17-18展示了这两种情况的判断流程。\n\n\n\n图17-18　判断客户端是否需要转向的流程\n\n举个例子，如果我们在之前提到的，由7000、7001、7002三个节点组成的集群中，用客户端连上节点7000，并发送以下命令，那么命令会直接被节点7000执行：\n\n127.0.0.1:7000> SET date "2013-12-31"\nOK\n\n\n因为键date所在的槽2022正是由节点7000负责处理的。\n\n但是，如果我们执行以下命令，那么客户端会先被转向至节点7001，然后再执行命令：\n\n127.0.0.1:7000> SET msg "happy new year!"\n-> Redirected to slot [6257] located at 127.0.0.1:7001\nOK\n127.0.0.1:7001> GET msg\n"happy new year!"\n\n\n这是因为键msg所在的槽6257是由节点7001负责处理的，而不是由最初接收命令的节点7000负责处理：\n\n * 当客户端第一次向节点7000发送SET命令的时候，节点7000会向客户端返回MOVED错误，指引客户端转向至节点7001。\n\n * 当客户端转向到节点7001之后，客户端重新向节点7001发送SET命令，这个命令会被节点7001成功执行。\n\n本节接下来的内容将介绍计算键所属槽的方法，节点判断某个槽是否由自己负责的方法，以及MOVED错误的实现方法，最后，本节还会介绍节点和单机Redis服务器保存键值对数据的相同和不同之处。\n\n\n# 计算键属于哪个槽\n\n节点使用以下算法来计算给定键key属于哪个槽：\n\ndef slot_number(key):\n    return CRC16(key) & 16383\n\n\n其中CRC16（key）语句用于计算键key的CRC-16校验和，而&16383语句则用于计算出一个介于0至16383之间的整数作为键key的槽号。\n\n使用CLUSTER KEYSLOT命令可以查看一个给定键属于哪个槽：\n\n127.0.0.1:7000> CLUSTER KEYSLOT "date"\n(integer) 2022\n127.0.0.1:7000> CLUSTER KEYSLOT "msg"\n(integer) 6257\n127.0.0.1:7000> CLUSTER KEYSLOT "name"\n(integer) 5798\n127.0.0.1:7000> CLUSTER KEYSLOT "fruits"\n(integer) 14943\n\n\nCLUSTER KEYSLOT命令就是通过调用上面给出的槽分配算法来实现的，以下是该命令的伪代码实现：\n\ndef CLUSTER_KEYSLOT(key):\n    # 计算槽号\n    slot = slot_number(key)\n    # 将槽号返回给客户端\n    reply_client(slot)\n\n\n\n# 判断槽是否由当前节点负责处理\n\n当节点计算出键所属的槽i之后，节点就会检查自己在clusterState.slots数组中的项i，判断键所在的槽是否由自己负责：\n\n1）如果clusterState.slots[i]等于clusterState.myself，那么说明槽i由当前节点负责，节点可以执行客户端发送的命令。\n\n2）如果clusterState.slots[i]不等于clusterState.myself，那么说明槽i并非由当前节点负责，节点会根据clusterState.slots[i]指向的clusterNode结构所记录的节点IP和端口号，向客户端返回MOVED错误，指引客户端转向至正在处理槽i的节点。\n\n举个例子，假设图17-19为节点7000的clusterState结构：\n\n * 当客户端向节点7000发送命令SET date"2013-12-31"的时候，节点首先计算出键date属于槽2022，然后检查得出clusterState.slots[2022]等于clusterState.myself，这说明槽2022正是由节点7000负责，于是节点7000直接执行这个SET命令，并将结果返回给发送命令的客户端。\n\n * 当客户端向节点7000发送命令SET msg"happy new year！"的时候，节点首先计算出键msg属于槽6257，然后检查clusterState.slots[6257]是否等于clusterState.myself，结果发现两者并不相等：这说明槽6257并非由节点7000负责处理，于是节点7000访问clusterState.slots[6257]所指向的clusterNode结构，并根据结构中记录的IP地址127.0.0.1和端口号7001，向客户端返回错误MOVED 6257 127.0.0.1:7001，指引节点转向至正在负责处理槽6257的节点7001。\n\n\n\n图17-19　节点7000的clusterState结构\n\n\n# MOVED 错误\n\n当节点发现键所在的槽并非由自己负责处理的时候，节点就会向客户端返回一个 MOVED 错误，指引客户端转向至正在负责槽的节点。\n\nMOVED错误的格式为：\n\nMOVED <slot> <ip>:<port>\n\n\n其中slot为键所在的槽，而ip和port则是负责处理槽slot的节点的IP地址和端口号。例如错误：\n\nOVED 10086 127.0.0.1:7002\n\n\n表示槽10086正由IP地址为127.0.0.1，端口号为7002的节点负责。\n\n又例如错误：\n\nMOVED 789 127.0.0.1:7000\n\n\n表示槽789正由IP地址为127.0.0.1，端口号为7000的节点负责。\n\n当客户端接收到节点返回的MOVED错误时，客户端会根据MOVED错误中提供的IP地址和端口号，转向至负责处理槽slot的节点，并向该节点重新发送之前想要执行的命令。以前面的客户端从节点7000转向至7001的情况作为例子：\n\n127.0.0.1:7000> SET msg "happy new year!"\n-> Redirected to slot [6257] located at 127.0.0.1:7001\nOK\n127.0.0.1:7001>\n\n\n图17-20展示了客户端向节点7000发送SET命令，并获得MOVED错误的过程。\n\n\n\n图17-20　节点7000向客户端返回MOVED错误\n\n而图17-21则展示了客户端根据MOVED错误，转向至节点7001，并重新发送SET命令的过程。\n\n\n\n图17-21　客户端根据MOVED错误的指示转向至节点7001\n\n一个集群客户端通常会与集群中的多个节点创建套接字连接，而所谓的节点转向实际上就是换一个套接字来发送命令。\n\n如果客户端尚未与想要转向的节点创建套接字连接，那么客户端会先根据MOVED错误提供的IP地址和端口号来连接节点，然后再进行转向。\n\n> 被隐藏的MOVED错误\n> \n> 集群模式的redis-cli客户端在接收到MOVED错误时，并不会打印出MOVED错误，而是根据MOVED错误自动进行节点转向，并打印出转向信息，所以我们是看不见节点返回的MOVED错误的：\n> \n> $ redis-cli -c -p 7000 # \n> 集群模式\n> 127.0.0.1:7000> SET msg "happy new year!"\n> -> Redirected to slot [6257] located at 127.0.0.1:7001\n> OK\n> 127.0.0.1:7001>\n> \n> \n> 但是，如果我们使用单机（stand alone）模式的redis-cli客户端，再次向节点7000发送相同的命令，那么MOVED错误就会被客户端打印出来：\n> \n> $ redis-cli -p 7000 # \n> 单机模式\n> 127.0.0.1:7000> SET msg "happy new year!"\n> (error) MOVED 6257 127.0.0.1:7001\n> 127.0.0.1:7000>\n> \n> \n> 这是因为单机模式的redis-cli客户端不清楚MOVED错误的作用，所以它只会直接将MOVED错误直接打印出来，而不会进行自动转向。\n\n\n# 节点数据库的实现\n\n集群节点保存键值对以及键值对过期时间的方式，与第9章里面介绍的单机Redis服务器保存键值对以及键值对过期时间的方式完全相同。\n\n节点和单机服务器在数据库方面的一个区别是，节点只能使用0号数据库，而单机Redis服务器则没有这一限制。\n\n举个例子，图17-22展示了节点7000的数据库状态，数据库中包含列表键"lst"，哈希键"book"，以及字符串键"date"，其中键"lst"和键"book"带有过期时间。\n\n另外，除了将键值对保存在数据库里面之外，节点还会用clusterState结构中的slots_to_keys跳跃表来保存槽和键之间的关系：\n\ntypedef struct clusterState {\n\n  zskiplist *slots_to_keys;\n \n} clusterState;\n\n\n\n\n图17-22　节点7000的数据库\n\nslots_to_keys跳跃表每个节点的分值（score）都是一个槽号，而每个节点的成员（member）都是一个数据库键：\n\n * 每当节点往数据库中添加一个新的键值对时，节点就会将这个键以及键的槽号关联到slots_to_keys跳跃表。\n\n * 当节点删除数据库中的某个键值对时，节点就会在slots_to_keys跳跃表解除被删除键与槽号的关联。\n\n举个例子，对于图17-22所示的数据库，节点7000将创建类似图17-23所示的slots_to_keys跳跃表：\n\n * 键"book"所在跳跃表节点的分值为1337.0，这表示键"book"所在的槽为1337。\n\n * 键"date"所在跳跃表节点的分值为2022.0，这表示键"date"所在的槽为2022。\n\n * 键"lst"所在跳跃表节点的分值为3347.0，这表示键"lst"所在的槽为3347。\n\n通过在slots_to_keys跳跃表中记录各个数据库键所属的槽，节点可以很方便地对属于某个或某些槽的所有数据库键进行批量操作，例如命令CLUSTER GETKEYSINSLOT命令可以返回最多count个属于槽slot的数据库键，而这个命令就是通过遍历slots_to_keys跳跃表来实现的。\n\n\n\n图17-23　节点7000的slots_to_keys跳跃表\n\n\n# 重新分片\n\nRedis集群的重新分片操作可以将任意数量已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽所属的键值对也会从源节点被移动到目标节点。\n\n重新分片操作可以在线（online）进行，在重新分片的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。\n\n举个例子，对于之前提到的，包含7000、7001、7002三个节点的集群来说，我们可以向这个集群添加一个IP为127.0.0.1，端口号为7003的节点（后面简称节点7003）：\n\n$ redis-cli -c -p 7000\n127.0.0.1:7000> CLUSTER MEET 127.0.0.1 7003\nOK\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388635782831 0 connected 5001-10000\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388635782831 0 connected 10001-16383\n04579925484ce537d3410d7ce97bd2e260c459a2 127.0.0.1:7003 master - 0 1388635782330 0 connected\n\n\n然后通过重新分片操作，将原本指派给节点7002的槽15001至16383改为指派给节点7003。\n\n以下是重新分片操作执行之后，节点的槽分配状态：\n\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master -0 0 0 connected 0-5000\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master -0 1388635782831 0 connected 5001-10000\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master -0 1388635782831 0 connected 10001-15000\n04579925484ce537d3410d7ce97bd2e260c459a2 127.0.0.1:7003 master -0 1388635782330 0 connected 15001-16383\n\n\n重新分片的实现原理\n\nRedis集群的重新分片操作是由Redis的集群管理软件redis-trib负责执行的，Redis提供了进行重新分片所需的所有命令，而redis-trib则通过向源节点和目标节点发送命令来进行重新分片操作。\n\nredis-trib对集群的单个槽slot进行重新分片的步骤如下：\n\n1）redis-trib对目标节点发送CLUSTER SETSLOT <slot> IMPORTING <source_id> 命令，让目标节点准备好从源节点导入（import）属于槽slot的键值对。\n\n2）redis-trib对源节点发送CLUSTER SETSLOT <slot> MIGRATING <target_id>命令，让源节点准备好将属于槽slot的键值对迁移（migrate）至目标节点。\n\n3）redis-trib向源节点发送CLUSTER GETKEYSINSLOT <slot> <count> 命令，获得最多count个属于槽slot的键值对的键名（key name）。\n\n4）对于步骤3获得的每个键名，redis-trib都向源节点发送一个MIGRATE <target_ip ><target_port> <key_name> 0 <timeout>命令，将被选中的键原子地从源节点迁移至目标节点。\n\n5）重复执行步骤3和步骤4，直到源节点保存的所有属于槽slot的键值对都被迁移至目标节点为止。每次迁移键的过程如图17-24所示。\n\n6）redis-trib向集群中的任意一个节点发送CLUSTER SETSLOT <slot> NODE <target_id>命令，将槽slot指派给目标节点，这一指派信息会通过消息发送至整个集群，最终集群中的所有节点都会知道槽slot已经指派给了目标节点。\n\n\n\n图17-24　迁移键的过程\n\n图17-25 展示了对槽slot进行重新分片的整个过程。\n\n如果重新分片涉及多个槽，那么redis-trib将对每个给定的槽分别执行上面给出的步骤。\n\n\n\n图17-25　对槽slot进行重新分片的过程\n\n\n# ASK 错误\n\n在进行重新分片期间，源节点向目标节点迁移一个槽的过程中，可能会出现这样一种情况：属于被迁移槽的一部分键值对保存在源节点里面，而另一部分键值对则保存在目标节点里面。\n\n当客户端向源节点发送一个与数据库键有关的命令，并且命令要处理的数据库键恰好就属于正在被迁移的槽时：\n\n * 源节点会先在自己的数据库里面查找指定的键，如果找到的话，就直接执行客户端发送的命令。\n * 相反地，如果源节点没能在自己的数据库里面找到指定的键，那么这个键有可能已经被迁移到了目标节点，源节点将向客户端返回一个ASK错误，指引客户端转向正在导入槽的目标节点，并再次发送之前想要执行的命令。\n\n图17-26展示了源节点判断是否需要向客户端发送ASK错误的整个过程。\n\n\n\n图17-26　判断是否发送ASK错误的过程\n\n举个例子，假设节点7002正在向节点7003迁移槽16198，这个槽包含"is"和"love"两个键，其中键"is"还留在节点7002，而键"love"已经被迁移到了节点7003。\n\n如果我们向节点7002发送关于键"is"的命令，那么这个命令会直接被节点7002执行：\n\n127.0.0.1:7002> GET "is"\n"you get the key \'is\'"\n\n\n而如果我们向节点7002发送关于键"love"的命令，那么客户端会先被转向至节点7003，然后再次执行命令：\n\n127.0.0.1:7002> GET "love"\n-> Redirected to slot [16198] located at 127.0.0.1:7003\n"you get the key \'love\'"\n127.0.0.1:7003>\n\n\n> 被隐藏的ASK错误\n> \n> 和接到MOVED错误时的情况类似，集群模式的redis-cli在接到ASK错误时也不会打印错误，而是自动根据错误提供的IP地址和端口进行转向动作。如果想看到节点发送的ASK错误的话，可以使用单机模式的redis-cli客户端：\n> \n> $ redis-cli -p 7002\n> 127.0.0.1:7002> GET "love"\n> (error) ASK 16198 127.0.0.1:7003\n\n注意\n\n在写这篇文章的时候，集群模式的redis-cli并未支持ASK自动转向，上面展示的ASK自动转向行为实际上是根据MOVED自动转向行为虚构出来的。因此，当集群模式的redis-cli真正支持ASK自动转向时，它的行为和上面展示的行为可能会有所不同。\n\n本节将对ASK错误的实现原理进行说明，并对比ASK错误和MOVED错误的区别。\n\n\n# CLUSTER SETSLOT IMPORTING命令的实现\n\nclusterState结构的importing_slots_from数组记录了当前节点正在从其他节点导入的槽：\n\ntypedef struct clusterState {\n\n  clusterNode *importing_slots_from[16384];\n \n} clusterState;\n\n\n如果importing_slots_from[i]的值不为NULL，而是指向一个clusterNode结构，那么表示当前节点正在从clusterNode所代表的节点导入槽i。\n\n在对集群进行重新分片的时候，向目标节点发送命令：\n\nCLUSTER SETSLOT <i> IMPORTING <source_id>\n\n\n可以将目标节点clusterState.importing_slots_from[i]的值设置为source_id所代表节点的clusterNode结构。\n\n举个例子，如果客户端向节点7003发送以下命令：\n\n# 9dfb 是节点7002 的ID \n127.0.0.1:7003> CLUSTER SETSLOT 16198 IMPORTING 9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26\nOK\n\n\n那么节点7003的clusterState.importing_slots_from数组将变成图17-27所示的样子。\n\n\n\n图17-27　节点7003的importing_slots_from数组\n\n\n# CLUSTER SETSLOT MIGRATING 命令的实现\n\nclusterState结构的migrating_slots_to数组记录了当前节点正在迁移至其他节点的槽：\n\ntypedef struct clusterState {\n   \n   clusterNode *migrating_slots_to[16384];\n   \n} clusterState;\n\n\n如果migrating_slots_to[i]的值不为NULL，而是指向一个 clusterNode 结构，那么表示当前节点正在将槽i迁移至clusterNode所代表的节点。\n\n在对集群进行重新分片的时候，向源节点发送命令：\n\nCLUSTER SETSLOT <i> MIGRATING <target_id>\n\n\n可以将源节点clusterState.migrating_slots_to[i]的值设置为target_id所代表节点的clusterNode结构。\n\n举个例子，如果客户端向节点7002发送以下命令：\n\n# 0457\n是节点7003 \n的ID \n127.0.0.1:7002> CLUSTER SETSLOT 16198 MIGRATING 04579925484ce537d3410d7ce97bd2e260c459a2\nOK\n\n\n那么节点7002的clusterState.migrating_slots_to数组将变成图17-28所示的样子。\n\n图17-28　节点7002的migrating_slots_to数组\n\n\n# ASK 错误\n\n如果节点收到一个关于键key的命令请求，并且键key所属的槽i正好就指派给了这个节点，那么节点会尝试在自己的数据库里查找键key，如果找到了的话，节点就直接执行客户端发送的命令。\n\n与此相反，如果节点没有在自己的数据库里找到键key，那么节点会检查自己的clusterState.migrating_slots_to[i]，看键key所属的槽i是否正在进行迁移，如果槽i的确在进行迁移的话，那么节点会向客户端发送一个ASK错误，引导客户端到正在导入槽i的节点去查找键key。\n\n举个例子，假设在节点7002向节点7003迁移槽16198期间，有一个客户端向节点7002发送命令：\n\nGET \n“love”\n\n\n因为键"love"正好属于槽16198，所以节点7002会首先在自己的数据库中查找键"love"，但并没有找到，通过检查自己的clusterState.migrating_slots_to[16198]，节点7002发现自己正在将槽16198迁移至节点7003，于是它向客户端返回错误：\n\nASK 16198 127.0.0.1:7003\n\n\n这个错误表示客户端可以尝试到IP为127.0.0.1，端口号为7003的节点去执行和槽16198有关的操作，如图17-29所示。\n\n\n\n图17-29　客户端接收到节点7002返回的ASK错误\n\n接到ASK错误的客户端会根据错误提供的IP地址和端口号，转向至正在导入槽的目标节点，然后首先向目标节点发送一个ASKING命令，之后再重新发送原本想要执行的命令。\n\n以前面的例子来说，当客户端接收到节点7002返回的以下错误时：\n\nASK 16198 127.0.0.1:7003\n\n\n客户端会转向至节点7003，首先发送命令：\n\nASKING\n\n\n然后再次发送命令：\n\nGET "love"\n\n\n并获得回复：\n\n"you get the key \'love\'"\n\n\n整个过程如图17-30所示。\n\n\n\n图17-30　客户端转向至节点7003\n\n\n# ASKING命令\n\nASKING命令唯一要做的就是打开发送该命令的客户端的REDIS_ASKING标识，以下是该命令的伪代码实现：\n\ndef ASKING():\n    # \n打开标识\n    client.flags |= REDIS_ASKING\n    # \n向客户端返回OK \n回复\n    reply("OK")\n\n\n在一般情况下，如果客户端向节点发送一个关于槽i的命令，而槽i又没有指派给这个节点的话，那么节点将向客户端返回一个MOVED错误；但是，如果节点的clusterState.importing_slots_from[i]显示节点正在导入槽i，并且发送命令的客户端带有REDIS_ASKING标识，那么节点将破例执行这个关于槽i的命令一次，图17-31展示了这个判断过程。\n\n\n\n图17-31　节点判断是否执行客户端命令的过程\n\n当客户端接收到ASK错误并转向至正在导入槽的节点时，客户端会先向节点发送一个ASKING命令，然后才重新发送想要执行的命令，这是因为如果客户端不发送ASKING命令，而直接发送想要执行的命令的话，那么客户端发送的命令将被节点拒绝执行，并返回MOVED错误。\n\n举个例子，我们可以使用普通模式的redis-cli客户端，向正在导入槽16198的节点7003发送以下命令：\n\n$ ./redis-cli -p 7003\n127.0.0.1:7003> GET "love"\n(error) MOVED 16198 127.0.0.1:7002\n\n\n虽然节点7003正在导入槽16198，但槽16198目前仍然是指派给了节点7002，所以节点7003会向客户端返回MOVED错误，指引客户端转向至节点7002。\n\n但是，如果我们在发送GET命令之前，先向节点发送一个ASKING命令，那么这个GET命令就会被节点7003执行：\n\n127.0.0.1:7003> ASKING\nOK\n127.0.0.1:7003> GET "love"\n"you get the key \'love\'"\n\n\n另外要注意的是，客户端的REDIS_ASKING标识是一个一次性标识，当节点执行了一个带有REDIS_ASKING标识的客户端发送的命令之后，客户端的REDIS_ASKING标识就会被移除。\n\n举个例子，如果我们在成功执行GET命令之后，再次向节点7003发送GET命令，那么第二次发送的GET命令将执行失败，因为这时客户端的REDIS_ASKING标识已经被移除：\n\n127.0.0.1:7003> ASKING                 #\n打开REDIS_ASKING\n标识\nOK\n127.0.0.1:7003> GET "love"   #\n移除REDIS_ASKING\n标识\n"you get the key \'love\'"\n127.0.0.1:7003> GET "love"   # REDIS_ASKING\n标识未打开，执行失败\n(error) MOVED 16198 127.0.0.1:7002\n\n\n\n# ASK错误和MOVED错误的区别\n\nASK错误和MOVED错误都会导致客户端转向，它们的区别在于：\n\n * MOVED错误代表槽的负责权已经从一个节点转移到了另一个节点：在客户端收到关于槽i的MOVED错误之后，客户端每次遇到关于槽i的命令请求时，都可以直接将命令请求发送至MOVED错误所指向的节点，因为该节点就是目前负责槽i的节点。\n\n * 与此相反，ASK错误只是两个节点在迁移槽的过程中使用的一种临时措施：在客户端收到关于槽i的ASK错误之后，客户端只会在接下来的一次命令请求中将关于槽i的命令请求发送至ASK错误所指示的节点，但这种转向不会对客户端今后发送关于槽i的命令请求产生任何影响，客户端仍然会将关于槽i的命令请求发送至目前负责处理槽i的节点，除非ASK错误再次出现。\n\n\n# 复制与故障转移\n\nRedis集群中的节点分为主节点（master）和从节点（slave），其中主节点用于处理槽，而从节点则用于复制某个主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求。\n\n举个例子，对于包含7000、7001、7002、7003四个主节点的集群来说，我们可以将7004、7005两个节点添加到集群里面，并将这两个节点设定为节点7000的从节点，如图17-32所示（图中以双圆形表示主节点，单圆形表示从节点）。\n\n\n\n图17-32　设置节点7004和节点7005成为节点7000的从节点\n\n表17-1记录了集群各个节点的当前状态，以及它们正在做的工作。\n\n表17-1　集群各个节点的当前状态\n\n\n\n如果这时，节点7000进入下线状态，那么集群中仍在正常运作的几个主节点将在节点7000的两个从节点——节点7004和节点7005中选出一个节点作为新的主节点，这个新的主节点将接管原来节点7000负责处理的槽，并继续处理客户端发送的命令请求。\n\n例如，如果节点7004被选中为新的主节点，那么节点7004将接管原来由节点7000负责处理的槽0至槽5000，节点7005也会从原来的复制节点7000，改为复制节点7004，如图17-33所示（图中用虚线包围的节点为已下线节点）。\n\n\n\n图17-33　节点7004成为新的主节点\n\n表17-2记录了在对节点7000进行故障转移之后，集群各个节点的当前状态，以及它们正在做的工作。\n\n表17-2　集群各个节点的当前状态\n\n\n\n如果在故障转移完成之后，下线的节点7000重新上线，那么它将成为节点7004的从节点，如图17-34所示。\n\n\n\n图17-34　重新上线的节点7000成为节点7004的从节点\n\n表17-3展示了节点7000复制节点7004之后，集群中各个节点的状态。\n\n表17-3　集群各个节点的当前状态\n\n\n\n本节接下来的内容将介绍节点的复制方法，检测节点是否下线的方法，以及对下线主节点进行故障转移的方法。\n\n\n# 设置从节点\n\n向一个节点发送命令：\n\nCLUSTER REPLICATE <node_id>\n\n\n可以让接收命令的节点成为node_id所指定节点的从节点，并开始对主节点进行复制：\n\n * 接收到该命令的节点首先会在自己的clusterState.nodes字典中找到node_id所对应节点的clusterNode结构，并将自己的clusterState.myself.slaveof指针指向这个结构，以此来记录这个节点正在复制的主节点：\n   \n   struct clusterNode {\n   \n        //如果这是一个从节点，那么指向主节点\n        struct clusterNode *slaveof;\n       \n      };\n   \n\n * 然后节点会修改自己在clusterState.myself.flags中的属性，关闭原本的REDIS_NODE_MASTER标识，打开REDIS_NODE_SLAVE标识，表示这个节点已经由原来的主节点变成了从节点。\n\n * 最后，节点会调用复制代码，并根据clusterState.myself.slaveof指向的clusterNode结构所保存的IP地址和端口号，对主节点进行复制。因为节点的复制功能和单机Redis服务器的复制功能使用了相同的代码，所以让从节点复制主节点相当于向从节点发送命令SLAVEOF。\n\n图17-35展示了节点7004在复制节点7000时的clusterState结构：\n\n * clusterState.myself.flags属性的值为REDIS_NODE_SLAVE，表示节点7004是一个从节点。\n\n * clusterState.myself.slaveof指针指向代表节点7000的结构，表示节点7004正在复制的主节点为节点7000。\n\n\n\n图17-35　节点7004的clusterState结构\n\n一个节点成为从节点，并开始复制某个主节点这一信息会通过消息发送给集群中的其他节点，最终集群中的所有节点都会知道某个从节点正在复制某个主节点。\n\n集群中的所有节点都会在代表主节点的clusterNode结构的slaves属性和numslaves属性中记录正在复制这个主节点的从节点名单：\n\nstruct clusterNode {\n    // \n    // \n正在复制这个主节点的从节点数量\n    int numslaves;\n    // \n一个数组\n    // \n每个数组项指向一个正在复制这个主节点的从节点的clusterNode\n结构\n    struct clusterNode **slaves;\n    // \n};\n\n\n举个例子，图17-36记录了节点7004和节点7005成为节点7000的从节点之后，集群中的各个节点为节点7000创建的clusterNode结构的样子：\n\n * 代表节点7000的clusterNode结构的numslaves属性的值为2，这说明有两个从节点正在复制节点7000。\n\n * 代表节点7000的clusterNode结构的slaves数组的两个项分别指向代表节点7004和代表节点7005的clusterNode结构，这说明节点7000的两个从节点分别是节点7004和节点7005。\n\n\n\n图17-36　集群中的各个节点为节点7000创建的clusterNode结构\n\n\n# 故障检测\n\n集群中的每个节点都会定期地向集群中的其他节点发送PING消息，以此来检测对方是否在线，如果接收PING消息的节点没有在规定的时间内，向发送PING消息的节点返回PONG消息，那么发送PING消息的节点就会将接收PING消息的节点标记为疑似下线（probable fail，PFAIL）。\n\n举个例子，如果节点7001向节点7000发送了一条PING消息，但是节点7000没有在规定的时间内，向节点7001返回一条PONG消息，那么节点7001就会在自己的clusterState.nodes字典中找到节点7000所对应的clusterNode结构，并在结构的flags属性中打开REDIS_NODE_PFAIL标识，以此表示节点7000进入了疑似下线状态，如图17-37所示。\n\n\n\n图17-37　代表节点7000的clusterNode结构\n\n集群中的各个节点会通过互相发送消息的方式来交换集群中各个节点的状态信息，例如某个节点是处于在线状态、疑似下线状态（PFAIL），还是已下线状态（FAIL）。\n\n当一个主节点A通过消息得知主节点B认为主节点C进入了疑似下线状态时，主节点A会在自己的clusterState.nodes字典中找到主节点C所对应的clusterNode结构，并将主节点B的下线报告（failure report）添加到clusterNode结构的fail_reports链表里面：\n\nstruct clusterNode {\n  // \n  // \n一个链表，记录了所有其他节点对该节点的下线报告\n  list *fail_reports;\n  // \n};\n\n\n每个下线报告由一个clusterNodeFailReport结构表示：\n\nstruct clusterNodeFailReport {\n  // \n报告目标节点已经下线的节点\n  struct clusterNode *node;\n  // \n最后一次从node\n节点收到下线报告的时间\n  // \n程序使用这个时间戳来检查下线报告是否过期\n  // \n（与当前时间相差太久的下线报告会被删除）\n  mstime_t time;\n} typedef clusterNodeFailReport;\n\n\n举个例子，如果主节点7001在收到主节点7002、主节点7003发送的消息后得知，主节点7002和主节点7003都认为主节点7000进入了疑似下线状态，那么主节点7001将为主节点7000创建图17-38所示的下线报告。\n\n\n\n图17-38　节点7000的下线报告\n\n如果在一个集群里面，半数以上负责处理槽的主节点都将某个主节点x报告为疑似下线，那么这个主节点x将被标记为已下线（FAIL），将主节点x标记为已下线的节点会向集群广播一条关于主节点x的FAIL消息，所有收到这条FAIL消息的节点都会立即将主节点x标记为已下线。\n\n举个例子，对于图17-38所示的下线报告来说，主节点7002和主节点7003都认为主节点7000进入了下线状态，并且主节点7001也认为主节点7000进入了疑似下线状态（代表主节点7000的结构打开了REDIS_NODE_PFAIL标识），综合起来，在集群四个负责处理槽的主节点里面，有三个都将主节点7000标记为下线，数量已经超过了半数，所以主节点7001会将主节点7000标记为已下线，并向集群广播一条关于主节点7000的FAIL消息，如图17-39所示。\n\n\n\n图17-39　节点7001向集群广播FAIL消息\n\n\n# 故障转移\n\n当一个从节点发现自己正在复制的主节点进入了已下线状态时，从节点将开始对下线主节点进行故障转移，以下是故障转移的执行步骤：\n\n1）复制下线主节点的所有从节点里面，会有一个从节点被选中。\n\n2）被选中的从节点会执行SLAVEOF no one命令，成为新的主节点。\n\n3）新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。\n\n4）新的主节点向集群广播一条PONG消息，这条PONG消息可以让集群中的其他节点立即知道这个节点已经由从节点变成了主节点，并且这个主节点已经接管了原本由已下线节点负责处理的槽。\n\n5）新的主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成。\n\n\n# 选举新的主节点\n\n新的主节点是通过选举产生的。\n\n以下是集群选举新的主节点的方法：\n\n1）集群的配置纪元是一个自增计数器，它的初始值为0。\n\n2）当集群里的某个节点开始一次故障转移操作时，集群配置纪元的值会被增一。\n\n3）对于每个配置纪元，集群里每个负责处理槽的主节点都有一次投票的机会，而第一个向主节点要求投票的从节点将获得主节点的投票。\n\n4）当从节点发现自己正在复制的主节点进入已下线状态时，从节点会向集群广播一条CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST消息，要求所有收到这条消息、并且具有投票权的主节点向这个从节点投票。\n\n5）如果一个主节点具有投票权（它正在负责处理槽），并且这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，表示这个主节点支持从节点成为新的主节点。\n\n6）每个参与选举的从节点都会接收CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，并根据自己收到了多少条这种消息来统计自己获得了多少主节点的支持。\n\n7）如果集群里有N个具有投票权的主节点，那么当一个从节点收集到大于等于N/2+1张支持票时，这个从节点就会当选为新的主节点。\n\n8）因为在每一个配置纪元里面，每个具有投票权的主节点只能投一次票，所以如果有N个主节点进行投票，那么具有大于等于N/2+1张支持票的从节点只会有一个，这确保了新的主节点只会有一个。\n\n9）如果在一个配置纪元里面没有从节点能收集到足够多的支持票，那么集群进入一个新的配置纪元，并再次进行选举，直到选出新的主节点为止。\n\n这个选举新主节点的方法和第16章介绍的选举领头Sentinel的方法非常相似，因为两者都是基于Raft算法的领头选举（leader election）方法来实现的。\n\n\n# 消息\n\n集群中的各个节点通过发送和接收消息（message）来进行通信，我们称发送消息的节点为发送者（sender），接收消息的节点为接收者（receiver），如图17-40所示。\n\n\n\n图17-40　发送者和接收者\n\n节点发送的消息主要有以下五种：\n\n * MEET消息：当发送者接到客户端发送的CLUSTER MEET命令时，发送者会向接收者发送MEET消息，请求接收者加入到发送者当前所处的集群里面。\n\n * PING消息：集群里的每个节点默认每隔一秒钟就会从已知节点列表中随机选出五个节点，然后对这五个节点中最长时间没有发送过PING消息的节点发送PING消息，以此来检测被选中的节点是否在线。除此之外，如果节点A最后一次收到节点B发送的PONG消息的时间，距离当前时间已经超过了节点A的cluster-node-timeout选项设置时长的一半，那么节点A也会向节点B发送PING消息，这可以防止节点A因为长时间没有随机选中节点B作为PING消息的发送对象而导致对节点B的信息更新滞后。\n\n * PONG消息：当接收者收到发送者发来的MEET消息或者PING消息时，为了向发送者确认这条MEET消息或者PING消息已到达，接收者会向发送者返回一条PONG消息。另外，一个节点也可以通过向集群广播自己的PONG消息来让集群中的其他节点立即刷新关于这个节点的认识，例如当一次故障转移操作成功执行之后，新的主节点会向集群广播一条PONG消息，以此来让集群中的其他节点立即知道这个节点已经变成了主节点，并且接管了已下线节点负责的槽。\n\n * FAIL消息：当一个主节点A判断另一个主节点B已经进入FAIL状态时，节点A会向集群广播一条关于节点B的FAIL消息，所有收到这条消息的节点都会立即将节点B标记为已下线。\n\n * PUBLISH消息：当节点接收到一个PUBLISH命令时，节点会执行这个命令，并向集群广播一条PUBLISH消息，所有接收到这条PUBLISH消息的节点都会执行相同的PUBLISH命令。\n\n一条消息由消息头（header）和消息正文（data）组成，接下来的内容将首先介绍消息头，然后再分别介绍上面提到的五种不同类型的消息正文。\n\n\n# 消息头\n\n节点发送的所有消息都由一个消息头包裹，消息头除了包含消息正文之外，还记录了消息发送者自身的一些信息，因为这些信息也会被消息接收者用到，所以严格来讲，我们可以认为消息头本身也是消息的一部分。\n\n每个消息头都由一个cluster.h/clusterMsg结构表示：\n\ntypedef struct {\n  // 消息的长度（包括这个消息头的长度和消息正文的长度）\n  uint32_t totlen;\n  // 消息的类型\n  uint16_t type;\n  // 消息正文包含的节点信息数量\n  // 只在发送MEET、PING、PONG这三种Gossip协议消息时使用\n  uint16_t count;\n  // 发送者所处的配置纪元\n  uint64_t currentEpoch;\n // 如果发送者是一个主节点，那么这里记录的是发送者的配置纪元\n  // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的配置纪元\n  uint64_t configEpoch;\n  // 发送者的名字（ID\n） \n  char sender[REDIS_CLUSTER_NAMELEN];\n  // 发送者目前的槽指派信息\n  unsigned char myslots[REDIS_CLUSTER_SLOTS/8];\n  // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的名字\n  // 如果发送者是一个主节点，那么这里记录的是REDIS_NODE_NULL_NAME\n  // （一个40字节长，值全为0的字节数组）\n  char slaveof[REDIS_CLUSTER_NAMELEN];\n  // 发送者的端口号\n  uint16_t port;\n  // 发送者的标识值\n  uint16_t flags;\n  // 发送者所处集群的状态\n  unsigned char state;\n  // 消息的正文（或者说，内容）\n  union clusterMsgData data;\n} clusterMsg;\n\n\nclusterMsg.data属性指向联合cluster.h/clusterMsgData，这个联合就是消息的正文：\n\nunion clusterMsgData {\n  // MEET、PING、PONG消息的正文\n  struct {\n    // 每条MEET、PING、PONG消息都包含两个\n    // clusterMsgDataGossip结构\n    clusterMsgDataGossip gossip[1];\n  } ping;\n  // FAIL消息的正文\n  struct {\n    clusterMsgDataFail about;\n  } fail;\n  // PUBLISH消息的正文\n  struct {\n    clusterMsgDataPublish msg;\n  } publish;\n  // 其他消息的正文\n};\n\n\nclusterMsg结构的currentEpoch、sender、myslots等属性记录了发送者自身的节点信息，接收者会根据这些信息，在自己的clusterState.nodes字典里找到发送者对应的clusterNode结构，并对结构进行更新。\n\n举个例子，通过对比接收者为发送者记录的槽指派信息，以及发送者在消息头的myslots属性记录的槽指派信息，接收者可以知道发送者的槽指派信息是否发生了变化。\n\n又或者说，通过对比接收者为发送者记录的标识值，以及发送者在消息头的flags属性记录的标识值，接收者可以知道发送者的状态和角色是否发生了变化，例如节点状态由原来的在线变成了下线，或者由主节点变成了从节点等等。\n\n\n# MEET、PING、PONG消息的实现\n\nRedis集群中的各个节点通过Gossip协议来交换各自关于不同节点的状态信息，其中Gossip协议由MEET、PING、PONG三种消息实现，这三种消息的正文都由两个cluster.h/clusterMsgDataGossip结构组成：\n\nunion clusterMsgData {\n  // \n  // MEET、PING和PONG消息的正文\n  struct {\n    // 每条MEET、PING、PONG消息都包含两个\n    // clusterMsgDataGossip结构\n    clusterMsgDataGossip gossip[1];\n  } ping;\n  // 其他消息的正文 \n};\n\n\n\n因为MEET、PING、PONG三种消息都使用相同的消息正文，所以节点通过消息头的type属性来判断一条消息是MEET消息、PING消息还是PONG消息。\n\n每次发送MEET、PING、PONG消息时，发送者都从自己的已知节点列表中随机选出两个节点（可以是主节点或者从节点），并将这两个被选中节点的信息分别保存到两个clusterMsgDataGossip结构里面。\n\nclusterMsgDataGossip结构记录了被选中节点的名字，发送者与被选中节点最后一次发送和接收PING消息和PONG消息的时间戳，被选中节点的IP地址和端口号，以及被选中节点的标识值：\n\ntypedef struct {\n  // 节点的名字\n  char nodename[REDIS_CLUSTER_NAMELEN];\n  // 最后一次向该节点发送PING消息的时间戳\n  uint32_t ping_sent;\n  // 最后一次从该节点接收到PONG消息的时间戳\n  uint32_t pong_received;\n  // 节点的IP地址\n  char ip[16];\n  // 节点的端口号\n  uint16_t port;\n  // 节点的标识值\n  uint16_t flags;\n} clusterMsgDataGossip;\n\n\n当接收者收到MEET、PING、PONG消息时，接收者会访问消息正文中的两个clusterMsgDataGossip结构，并根据自己是否认识clusterMsgDataGossip结构中记录的被选中节点来选择进行哪种操作：\n\n * 如果被选中节点不存在于接收者的已知节点列表，那么说明接收者是第一次接触到被选中节点，接收者将根据结构中记录的IP地址和端口号等信息，与被选中节点进行握手。\n\n * 如果被选中节点已经存在于接收者的已知节点列表，那么说明接收者之前已经与被选中节点进行过接触，接收者将根据clusterMsgDataGossip结构记录的信息，对被选中节点所对应的clusterNode结构进行更新。\n\n举个发送PING消息和返回PONG消息的例子，假设在一个包含A、B、C、D、E、F六个节点的集群里：\n\n * 节点A向节点D发送PING消息，并且消息里面包含了节点B和节点C的信息，当节点D收到这条PING消息时，它将更新自己对节点B和节点C的认识。\n\n * 之后，节点D将向节点A返回一条PONG消息，并且消息里面包含了节点E和节点F的消息，当节点A收到这条PONG消息时，它将更新自己对节点E和节点F的认识。\n\n整个通信过程如图17-41所示。\n\n\n\n图17-41　一个PING-PONG消息通信示例\n\n\n# FAIL消息的实现\n\n当集群里的主节点A将主节点B标记为已下线（FAIL）时，主节点A将向集群广播一条关于主节点B的FAIL消息，所有接收到这条FAIL消息的节点都会将主节点B标记为已下线。\n\n在集群的节点数量比较大的情况下，单纯使用Gossip协议来传播节点的已下线信息会给节点的信息更新带来一定延迟，因为Gossip协议消息通常需要一段时间才能传播至整个集群，而发送FAIL消息可以让集群里的所有节点立即知道某个主节点已下线，从而尽快判断是否需要将集群标记为下线，又或者对下线主节点进行故障转移。\n\nFAIL消息的正文由cluster.h/clusterMsgDataFail结构表示，这个结构只包含一个nodename属性，该属性记录了已下线节点的名字：\n\ntypedef struct {\n    char nodename[REDIS_CLUSTER_NAMELEN];\n} clusterMsgDataFail;\n\n\n因为集群里的所有节点都有一个独一无二的名字，所以FAIL消息里面只需要保存下线节点的名字，接收到消息的节点就可以根据这个名字来判断是哪个节点下线了。\n\n举个例子，对于包含7000、7001、7002、7003四个主节点的集群来说：\n\n * 如果主节点7001发现主节点7000已下线，那么主节点7001将向主节点7002和主节点7003发送FAIL消息，其中FAIL消息中包含的节点名字为主节点7000的名字，以此来表示主节点7000已下线。\n\n * 当主节点7002和主节点7003都接收到主节点7001发送的FAIL消息时，它们也会将主节点7000标记为已下线。\n\n * 因为这时集群已经有超过一半的主节点认为主节点7000已下线，所以集群剩下的几个主节点可以判断是否需要将集群标记为下线，又或者开始对主节点7000进行故障转移。\n\n图17-42至图17-44展示了节点发送和接收FAIL消息的整个过程。\n\n\n\n图17-42　节点7001将节点7000标记为已下线\n\n\n\n图17-43　节点7001向集群广播FAIL消息\n\n\n\n图17-44　节点7002和节点7003也将节点7000标记为已下线\n\n\n# PUBLISH消息的实现\n\n当客户端向集群中的某个节点发送命令\n\nPUBLISH <channel> <message>\n\n\n的时候，接收到PUBLISH命令的节点不仅会向channel频道发送消息message，它还会向集群广播一条PUBLISH消息，所有接收到这条PUBLISH消息的节点都会向channel频道发送message消息。\n\n换句话说，向集群中的某个节点发送命令：\n\nPUBLISH <channel> <message>\n\n\n将导致集群中的所有节点都向channel频道发送message消息。\n\n举个例子，对于包含7000、7001、7002、7003四个节点的集群来说，如果节点7000收到了客户端发送的PUBLISH命令，那么节点7000将向7001、7002、7003三个节点发送PUBLISH消息，如图17-45所示。\n\n\n\n图17-45　接收到PUBLISH命令的节点7000向集群广播PUBLISH消息\n\nPUBLISH消息的正文由cluster.h/clusterMsgDataPublish结构表示：\n\ntypedef struct {\n  uint32_t channel_len;\n  uint32_t message_len;\n  // 定义为8 字节只是为了对齐其他消息结构\n  // 实际的长度由保存的内容决定\n  unsigned char bulk_data[8];\n} clusterMsgDataPublish;\n\n\nclusterMsgDataPublish结构的bulk_data属性是一个字节数组，这个字节数组保存了客户端通过PUBLISH命令发送给节点的channel参数和message参数，而结构的channel_len和message_len则分别保存了channel参数的长度和message参数的长度：\n\n * 其中bulk_data的0字节至channel_len-1字节保存的是channel参数。\n\n * 而bulk_data的channel_len字节至channel_len+message_len-1字节保存的则是message参数。\n\n举个例子，如果节点收到的PUBLISH命令为：\n\nPUBLISH "news.it" "hello"\n\n\n那么节点发送的PUBLISH消息的clusterMsgDataPublish结构将如图17-46所示：其中bulk_data数组的前七个字节保存了channel参数的值"news.it"，而bulk_data数组的后五个字节则保存了message参数的值"hello"。\n\n\n\n图17-46　clusterMsgDataPublish结构示例\n\n> 为什么不直接向节点广播PUBLISH命令\n> \n> 实际上，要让集群的所有节点都执行相同的PUBLISH命令，最简单的方法就是向所有节点广播相同的PUBLISH命令，这也是Redis在复制PUBLISH命令时所使用的方法，不过因为这种做法并不符合Redis集群的“各个节点通过发送和接收消息来进行通信”这一规则，所以节点没有采取广播PUBLISH命令的做法。\n\n\n# 重点回顾\n\n * 节点通过握手来将其他节点添加到自己所处的集群当中。\n * 集群中的 16384 个槽可以分别指派给集群中的各个节点， 每个节点都会记录哪些槽指派给了自己， 而哪些槽又被指派给了其他节点。\n * 节点在接到一个命令请求时， 会先检查这个命令请求要处理的键所在的槽是否由自己负责， 如果不是的话， 节点将向客户端返回一个 MOVED 错误， MOVED 错误携带的信息可以指引客户端转向至正在负责相关槽的节点。\n * 对 Redis 集群的重新分片工作是由客户端执行的， 重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点。\n * 如果节点 A 正在迁移槽 i 至节点 B ， 那么当节点 A 没能在自己的数据库中找到命令指定的数据库键时， 节点 A 会向客户端返回一个 ASK 错误， 指引客户端到节点 B 继续查找指定的数据库键。\n * MOVED 错误表示槽的负责权已经从一个节点转移到了另一个节点， 而 ASK 错误只是两个节点在迁移槽的过程中使用的一种临时措施。\n * 集群里的从节点用于复制主节点， 并在主节点下线时， 代替主节点继续处理命令请求。\n * 集群中的节点通过发送和接收消息来进行通讯， 常见的消息包括 MEET 、 PING 、 PONG 、 PUBLISH 、 FAIL 五种。\n\n\n# 参考文献\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'# 前言\n\nredis 集群是 redis 提供的分布式数据库方案，集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能。\n\n本节将对集群的节点、槽指派、命令执行、重新分片、转向、故障转移、消息等各个方面进行介绍\n\n\n# 节点\n\n一个 redis 集群通常由多个节点（node）组成， 在刚开始的时候， 每个节点都是相互独立的， 它们都处于一个只包含自己的集群当中， 要组建一个真正可工作的集群， 我们必须将各个独立的节点连接起来， 构成一个包含多个节点的集群。\n\n连接各个节点的工作可以使用 cluster meet 命令来完成， 该命令的格式如下：\n\ncluster meet <ip> <port>\n\n\n向一个节点 node 发送 cluster meet 命令， 可以让 node 节点与 ip 和 port 所指定的节点进行握手（handshake）， 当握手成功时， node 节点就会将 ip 和 port 所指定的节点添加到 node 节点当前所在的集群中。\n\n举个例子， 假设现在有三个独立的节点 127.0.0.1:7000 、 127.0.0.1:7001 、 127.0.0.1:7002 （下文省略 ip 地址，直接使用端口号来区分各个节点）， 我们首先使用客户端连上节点 7000 ， 通过发送 cluster node 命令可以看到， 集群目前只包含 7000 自己一个节点：\n\n$ redis-cli -c -p 7000\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n通过向节点 7000 发送以下命令， 我们可以将节点 7001 添加到节点 7000 所在的集群里面：\n\n127.0.0.1:7000> cluster meet 127.0.0.1 7001\nok\n\n127.0.0.1:7000> cluster nodes\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388204746210 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n继续向节点 7000 发送以下命令， 我们可以将节点 7002 也添加到节点 7000 和节点 7001 所在的集群里面：\n\n127.0.0.1:7000> cluster meet 127.0.0.1 7002\nok\n\n127.0.0.1:7000> cluster nodes\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388204848376 0 connected\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388204847977 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n现在， 这个集群里面包含了 7000 、 7001 和 7002 三个节点， 图 image_connect_nodes_1 至 image_connect_nodes_5 展示了这三个节点进行握手的整个过程。\n\n\n\n\n\n\n\n\n\n\n\n本节接下来的内容将介绍启动节点的方法， 和集群有关的数据结构， 以及 cluster meet 命令的实现原理。\n\n\n# 启动节点\n\n一个节点就是一个运行在集群模式下的 redis 服务器， redis 服务器在启动时会根据 cluster-enabled 配置选项的是否为 yes 来决定是否开启服务器的集群模式， 如图 image_node_or_server 所示。\n\n\n\n节点（运行在集群模式下的 redis 服务器）会继续使用所有在单机模式中使用的服务器组件， 比如说：\n\n * 节点会继续使用文件事件处理器来处理命令请求和返回命令回复。\n * 节点会继续使用时间事件处理器来执行 servercron 函数， 而 servercron 函数又会调用集群模式特有的 clustercron 函数： clustercron 函数负责执行在集群模式下需要执行的常规操作， 比如向集群中的其他节点发送 gossip 消息， 检查节点是否断线； 又或者检查是否需要对下线节点进行自动故障转移， 等等。\n * 节点会继续使用数据库来保存键值对数据，键值对依然会是各种不同类型的对象。\n * 节点会继续使用 rdb 持久化模块和 aof 持久化模块来执行持久化工作。\n * 节点会继续使用发布与订阅模块来执行 publish 、 subscribe 等命令。\n * 节点会继续使用复制模块来进行节点的复制工作。\n * 节点会继续使用 lua 脚本环境来执行客户端输入的 lua 脚本。\n\n诸如此类。\n\n除此之外， 节点会继续使用 redisserver 结构来保存服务器的状态， 使用 redisclient 结构来保存客户端的状态， 至于那些只有在集群模式下才会用到的数据， 节点将它们保存到了 cluster.h/clusternode 结构， cluster.h/clusterlink 结构， 以及 cluster.h/clusterstate 结构里面， 接下来的一节将对这三种数据结构进行介绍\n\n\n# 集群数据结构\n\nclusternode 结构保存了一个节点的当前状态， 比如节点的创建时间， 节点的名字， 节点当前的配置纪元， 节点的 ip 和地址， 等等。\n\n每个节点都会使用一个 clusternode 结构来记录自己的状态， 并为集群中的所有其他节点（包括主节点和从节点）都创建一个相应的 clusternode 结构， 以此来记录其他节点的状态：\n\nstruct clusternode {\n\n    // 创建节点的时间\n    mstime_t ctime;\n\n    // 节点的名字，由 40 个十六进制字符组成\n    // 例如 68eef66df23420a5862208ef5b1a7005b806f2ff\n    char name[redis_cluster_namelen];\n\n    // 节点标识\n    // 使用各种不同的标识值记录节点的角色（比如主节点或者从节点），\n    // 以及节点目前所处的状态（比如在线或者下线）。\n    int flags;\n\n    // 节点当前的配置纪元，用于实现故障转移\n    uint64_t configepoch;\n\n    // 节点的 ip 地址\n    char ip[redis_ip_str_len];\n\n    // 节点的端口号\n    int port;\n\n    // 保存连接节点所需的有关信息\n    clusterlink *link;\n\n};\n\n\nclusternode 结构的 link 属性是一个 clusterlink 结构， 该结构保存了连接节点所需的有关信息， 比如套接字描述符， 输入缓冲区和输出缓冲区：\n\ntypedef struct clusterlink {\n\n    // 连接的创建时间\n    mstime_t ctime;\n\n    // tcp 套接字描述符\n    int fd;\n\n    // 输出缓冲区，保存着等待发送给其他节点的消息（message）。\n    sds sndbuf;\n\n    // 输入缓冲区，保存着从其他节点接收到的消息。\n    sds rcvbuf;\n\n    // 与这个连接相关联的节点，如果没有的话就为 null\n    struct clusternode *node;\n\n} clusterlink;\n\n\nredisclient 结构和 clusterlink 结构的相同和不同之处\n\nredisclient 结构和 clusterlink 结构都有自己的套接字描述符和输入、输出缓冲区， 这两个结构的区别在于， redisclient 结构中的套接字和缓冲区是用于连接客户端的， 而 clusterlink 结构中的套接字和缓冲区则是用于连接节点的。\n\n最后， 每个节点都保存着一个 clusterstate 结构， 这个结构记录了在当前节点的视角下， 集群目前所处的状态 —— 比如集群是在线还是下线， 集群包含多少个节点， 集群当前的配置纪元， 诸如此类：\n\ntypedef struct clusterstate {\n\n    // 指向当前节点的指针\n    clusternode *myself;\n\n    // 集群当前的配置纪元，用于实现故障转移\n    uint64_t currentepoch;\n\n    // 集群当前的状态：是在线还是下线\n    int state;\n\n    // 集群中至少处理着一个槽的节点的数量\n    int size;\n\n    // 集群节点名单（包括 myself 节点）\n    // 字典的键为节点的名字，字典的值为节点对应的 clusternode 结构\n    dict *nodes;\n\n} clusterstate;\n\n\n以前面介绍的 7000 、 7001 、 7002 三个节点为例， 图 image_cluster_state_of_7000 展示了节点 7000 创建的 clusterstate 结构， 这个结构从节点 7000 的角度记录了集群、以及集群包含的三个节点的当前状态 （为了空间考虑，图中省略了 clusternode 结构的一部分属性）：\n\n * 结构的 currentepoch 属性的值为 0 ， 表示集群当前的配置纪元为 0 。\n * 结构的 size 属性的值为 0 ， 表示集群目前没有任何节点在处理槽： 因此结构的 state 属性的值为 redis_cluster_fail —— 这表示集群目前处于下线状态。\n * 结构的 nodes 字典记录了集群目前包含的三个节点， 这三个节点分别由三个 clusternode 结构表示： 其中 myself 指针指向代表节点 7000 的 clusternode 结构， 而字典中的另外两个指针则分别指向代表节点 7001 和代表节点 7002 的 clusternode 结构， 这两个节点是节点 7000 已知的在集群中的其他节点。\n * 三个节点的 clusternode 结构的 flags 属性都是 redis_node_master ，说明三个节点都是主节点。\n\n节点 7001 和节点 7002 也会创建类似的 clusterstate 结构：\n\n * 不过在节点 7001 创建的 clusterstate 结构中， myself 指针将指向代表节点 7001 的 clusternode 结构， 而节点 7000 和节点 7002 则是集群中的其他节点。\n * 而在节点 7002 创建的 clusterstate 结构中， myself 指针将指向代表节点 7002 的 clusternode 结构， 而节点 7000 和节点 7001 则是集群中的其他节点。\n\n\n\n\n# cluster meet 命令的实现\n\n通过向节点 a 发送 cluster meet 命令， 客户端可以让接收命令的节点 a 将另一个节点 b 添加到节点 a 当前所在的集群里面：\n\ncluster meet <ip> <port>\n\n\n收到命令的节点 a 将与节点 b 进行握手（handshake）， 以此来确认彼此的存在， 并为将来的进一步通信打好基础：\n\n 1. 节点 a 会为节点 b 创建一个 clusternode 结构， 并将该结构添加到自己的 clusterstate.nodes 字典里面。\n 2. 之后， 节点 a 将根据 cluster meet 命令给定的 ip 地址和端口号， 向节点 b 发送一条 meet 消息（message）。\n 3. 如果一切顺利， 节点 b 将接收到节点 a 发送的 meet 消息， 节点 b 会为节点 a 创建一个 clusternode 结构， 并将该结构添加到自己的 clusterstate.nodes 字典里面。\n 4. 之后， 节点 b 将向节点 a 返回一条 pong 消息。\n 5. 如果一切顺利， 节点 a 将接收到节点 b 返回的 pong 消息， 通过这条 pong 消息节点 a 可以知道节点 b 已经成功地接收到了自己发送的 meet 消息。\n 6. 之后， 节点 a 将向节点 b 返回一条 ping 消息。\n 7. 如果一切顺利， 节点 b 将接收到节点 a 返回的 ping 消息， 通过这条 ping 消息节点 b 可以知道节点 a 已经成功地接收到了自己返回的 pong 消息， 握手完成。\n\n图 image_handshake 展示了以上步骤描述的握手过程。\n\n\n\n之后， 节点 a 会将节点 b 的信息通过 gossip 协议传播给集群中的其他节点， 让其他节点也与节点 b 进行握手， 最终， 经过一段时间之后， 节点 b 会被集群中的所有节点认识\n\n\n# 槽指派\n\nredis 集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为 16384 个槽（slot），数据库中的每个键都属于这 16384个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。\n\n * 当数据库中的 16384 个槽都有节点在处理时，集群处于上线状态（ok）\n * 如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态（fail）\n\n在上一节，我们使用 cluster meet 命令将 7000、7001、7002 三个节点连接到了同一个集群里面，不过这个集群目前仍然处于下线状态，因为集群中的三个节点都没有在处理任何槽：\n\n127.0.0.1:7000> cluster info\ncluster_state:fail\ncluster_slots_assigned:0\ncluster_slots_ok:0\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:3\ncluster_size:0\ncluster_current_epoch:0\ncluster_stats_messages_sent:110\ncluster_stats_messages_received:28\n\n\n通过向节点发送 cluster addslots 命令，我们可以将一个或多个槽指派（assign）给节点负责：\n\ncluster addslots <slot> [slot]\n\n\n举个例子，执行以下命令可以将槽0至槽5000指派给节点7000负责：\n\n127.0.0.1:7000> cluster addslots 0 1 2 3 4  5000\nok\n127.0.0.1:7000> cluster nodes\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388316664849 0 connected\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388316665850 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n\n\n为了让 7000、7001、7002 三个节点所在的集群进入上线状态，我们继续执行以下命令，将槽 5001 至槽 10000 指派给节点 7001 负责：\n\n127.0.0.1:7001> cluster addslots 5001 5002 5003 5004 10000\nok\n\n\n然后将槽 10001 至槽 16383 指派给7002负责：\n\n127.0.0.1:7002> cluster addslots 10001 10002 10003 10004 16383\nok\n\n\n当以上三个cluster addslots命令都执行完毕之后，数据库中的16384个槽都已经被指派给了相应的节点，集群进入上线状态：\n\n127.0.0.1:7000> cluster info\ncluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:3\ncluster_size:3\ncluster_current_epoch:0\ncluster_stats_messages_sent:2699\ncluster_stats_messages_received:2617\n127.0.0.1:7000> cluster nodes\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388317426165 0 connected 10001-16383\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388317427167 0 connected 5001-10000\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n\n\n本节接下来的内容将首先介绍节点保存槽指派信息的方法，以及节点之间传播槽指派信息的方法，之后再介绍cluster addslots命令的实现。\n\n\n# 记录节点的槽指派信息\n\nclusternode结构的slots属性和numslot属性记录了节点负责处理哪些槽：\n\nstruct clusternode {\n\n  unsigned char slots[16384/8];\n  int numslots;\n \n};\n\n\nslots属性是一个二进制位数组（bit array），这个数组的长度为16384/8=2048个字节，共包含16384个二进制位。\n\nredis以0为起始索引，16383为终止索引，对slots数组中的16384个二进制位进行编号，并根据索引i上的二进制位的值来判断节点是否负责处理槽i：\n\n * 如果slots数组在索引i上的二进制位的值为1，那么表示节点负责处理槽i。\n\n * 如果slots数组在索引i上的二进制位的值为0，那么表示节点不负责处理槽i。\n\n图17-9展示了一个slots数组示例：这个数组索引0至索引7上的二进制位的值都为1，其余所有二进制位的值都为0，这表示节点负责处理槽0至槽7。\n\n\n\n图17-9　一个slots数组示例\n\n图17-10 展示了另一个slots数组示例：这个数组索引1、3、5、8、9、10上的二进制位的值都为1，而其余所有二进制位的值都为0，这表示节点负责处理槽1、3、5、8、9、10。\n\n\n\n图17-10　另一个slots数组示例\n\n因为取出和设置slots数组中的任意一个二进制位的值的复杂度仅为o（1），所以对于一个给定节点的slots数组来说，程序检查节点是否负责处理某个槽，又或者将某个槽指派给节点负责，这两个动作的复杂度都是o（1）。\n\n至于numslots属性则记录节点负责处理的槽的数量，也即是slots数组中值为1的二进制位的数量。\n\n比如说，对于图17-9所示的slots数组来说，节点处理的槽数量为8，而对于图17-10所示的slots数组来说，节点处理的槽数量为6。\n\n\n# 传播节点的槽指派信息\n\n一个节点除了会将自己负责处理的槽记录在 clusternode 结构的 slots 属性和 numslots 属性之外，它还会将自己的 slots 数组通过消息发送给集群中的其他节点，以此来告知其他节点自己目前负责处理哪些槽。\n\n举个例子，对于前面展示的包含7000、7001、7002三个节点的集群来说：\n\n * 节点7000会通过消息向节点7001和节点7002发送自己的slots数组，以此来告知这两个节点，自己负责处理槽0至槽5000，如图17-11所示。\n\n * 节点7001会通过消息向节点7000和节点7002发送自己的slots数组，以此来告知这两个节点，自己负责处理槽5001至槽10000，如图17-12所示。\n\n * 节点7002会通过消息向节点7000和节点7001发送自己的slots数组，以此来告知这两个节点，自己负责处理槽10001至槽16383，如图17-13所示。\n\n\n\n图17-11　7000告知7001和7002自己负责处理的槽\n\n\n\n\n\n图17-13　7002告知7000和7001自己负责处理的槽\n\n当节点a通过消息从节点b那里接收到节点b的slots数组时，节点a会在自己的 clusterstate.nodes 字典中查找节点b对应的 clusternode 结构，并对结构中的 slots 数组进行保存或者更新。\n\n因为集群中的每个节点都会将自己的 slots 数组通过消息发送给集群中的其他节点，并且每个接收到 slots 数组的节点都会将数组保存到相应节点的 clusternode 结构里面，因此，集群中的每个节点都会知道数据库中的 16384 个槽分别被指派给了集群中的哪些节点。\n\n\n# 记录集群所有槽的指派信息\n\nclusterstate 结构中的 slots 数组记录了集群中所有 16384 个槽的指派信息：\n\ntypedef struct clusterstate {\n \n  clusternode *slots[16384];\n  \n} clusterstate;\n\n\nslots数组包含16384个项，每个数组项都是一个指向clusternode结构的指针：\n\n * 如果slots[i]指针指向null，那么表示槽i尚未指派给任何节点。\n\n * 如果slots[i]指针指向一个clusternode结构，那么表示槽i已经指派给了clusternode结构所代表的节点。\n\n举个例子，对于7000、7001、7002三个节点来说，它们的clusterstate结构的slots数组将会是图17-14所示的样子：\n\n * 数组项slots[0]至slots[5000]的指针都指向代表节点7000的clusternode结构，表示槽0至5000都指派给了节点7000。\n\n * 数组项slots[5001]至slots[10000]的指针都指向代表节点7001的clusternode结构，表示槽5001至10000都指派给了节点7001。\n\n * 数组项slots[10001]至slots[16383]的指针都指向代表节点7002的clusternode结构，表示槽10001至16383都指派给了节点7002。\n\n如果只将槽指派信息保存在各个节点的clusternode.slots数组里，会出现一些无法高效地解决的问题，而clusterstate.slots数组的存在解决了这些问题：\n\n * 如果节点只使用clusternode.slots数组来记录槽的指派信息，那么为了知道槽i是否已经被指派，或者槽i被指派给了哪个节点，程序需要遍历clusterstate.nodes字典中的所有clusternode结构，检查这些结构的slots数组，直到找到负责处理槽i的节点为止，这个过程的复杂度为o（n），其中n为clusterstate.nodes字典保存的clusternode结构的数量。\n\n * 而通过将所有槽的指派信息保存在clusterstate.slots数组里面，程序要检查槽i是否已经被指派，又或者取得负责处理槽i的节点，只需要访问clusterstate.slots[i]的值即可，这个操作的复杂度仅为o(1)。\n\n举个例子，对于图17-14所示的slots数组来说，如果程序需要知道槽10002被指派给了哪个节点，那么只要访问数组项slots[10002]，就可以马上知道槽10002被指派给了节点7002，如图17-15所示。\n\n\n\n图17-15　访问slots[10002]的值\n\n要说明的一点是，虽然clusterstate.slots数组记录了集群中所有槽的指派信息，但使用clusternode结构的slots数组来记录单个节点的槽指派信息仍然是有必要的：\n\n * 因为当程序需要将某个节点的槽指派信息通过消息发送给其他节点时，程序只需要将相应节点的clusternode.slots数组整个发送出去就可以了。\n\n * 另一方面，如果redis不使用clusternode.slots数组，而单独使用clusterstate.slots数组的话，那么每次要将节点a的槽指派信息传播给其他节点时，程序必须先遍历整个clusterstate.slots数组，记录节点a负责处理哪些槽，然后才能发送节点a的槽指派信息，这比直接发送clusternode.slots数组要麻烦和低效得多。\n\nclusterstate.slots数组记录了集群中所有槽的指派信息，而clusternode.slots数组只记录了clusternode结构所代表的节点的槽指派信息，这是两个slots数组的关键区别所在。\n\n\n# cluster addslots 命令的实现\n\ncluster addslots 命令接受一个或多个槽作为参数，并将所有输入的槽指派给接收该命令的节点负责：\n\ncluster addslots <slot> [slot]\n\n\ncluster addslots 命令的实现可以用以下伪代码来表示：\n\ndef cluster_addslots(*all_input_slots):\n    # 遍历所有输入槽，检查它们是否都是未指派槽\n    for i in all_input_slots:\n        # 如果有哪怕一个槽已经被指派给了某个节点\n        # 那么向客户端返回错误，并终止命令执行\n        if clusterstate.slots[i] != null:\n            reply_error()\n            return\n    # 如果所有输入槽都是未指派槽\n    # 那么再次遍历所有输入槽，将这些槽指派给当前节点\n    for i in all_input_slots:\n        # 设置clusterstate结构的slots数组\n        # 将slots[i]的指针指向代表当前节点的clusternode结构\n        clusterstate.slots[i] = clusterstate.myself\n        # 访问代表当前节点的clusternode结构的slots数组\n        # 将数组在索引i上的二进制位设置为1\n        setslotbit(clusterstate.myself.slots, i)\n\n\n举个例子，图17-16展示了一个节点的clusterstate结构，clusterstate.slots数组中的所有指针都指向null，并且clusternode.slots数组中的所有二进制位的值都是0，这说明当前节点没有被指派任何槽，并且集群中的所有槽都是未指派的。\n\n\n\n图17-16　节点的clusterstate结构\n\n当客户端对17-16所示的节点执行命令：\n\ncluster addslots 1 2\n\n\n将槽1和槽2指派给节点之后，节点的clusterstate结构将被更新成图17-17所示的样子：\n\n * clusterstate.slots数组在索引1和索引2上的指针指向了代表当前节点的clusternode结构。\n\n * 并且clusternode.slots数组在索引1和索引2上的位被设置成了1。\n\n\n\n图17-17　执行 cluster addslots 命令之后的 clusterstate 结构\n\n最后，在 cluster addslots 命令执行完毕之后，节点会通过发送消息告知集群中的其他节点，自己目前正在负责处理哪些槽。\n\n\n# 在集群中执行命令\n\n在对数据库中的16384个槽都进行了指派之后，集群就会进入上线状态，这时客户端就可以向集群中的节点发送数据命令了。\n\n当客户端向节点发送与数据库键有关的命令时，接收命令的节点会计算出命令要处理的数据库键属于哪个槽，并检查这个槽是否指派给了自己：\n\n * 如果键所在的槽正好就指派给了当前节点，那么节点直接执行这个命令。\n\n * 如果键所在的槽并没有指派给当前节点，那么节点会向客户端返回一个moved错误，指引客户端转向（redirect）至正确的节点，并再次发送之前想要执行的命令。\n\n图17-18展示了这两种情况的判断流程。\n\n\n\n图17-18　判断客户端是否需要转向的流程\n\n举个例子，如果我们在之前提到的，由7000、7001、7002三个节点组成的集群中，用客户端连上节点7000，并发送以下命令，那么命令会直接被节点7000执行：\n\n127.0.0.1:7000> set date "2013-12-31"\nok\n\n\n因为键date所在的槽2022正是由节点7000负责处理的。\n\n但是，如果我们执行以下命令，那么客户端会先被转向至节点7001，然后再执行命令：\n\n127.0.0.1:7000> set msg "happy new year!"\n-> redirected to slot [6257] located at 127.0.0.1:7001\nok\n127.0.0.1:7001> get msg\n"happy new year!"\n\n\n这是因为键msg所在的槽6257是由节点7001负责处理的，而不是由最初接收命令的节点7000负责处理：\n\n * 当客户端第一次向节点7000发送set命令的时候，节点7000会向客户端返回moved错误，指引客户端转向至节点7001。\n\n * 当客户端转向到节点7001之后，客户端重新向节点7001发送set命令，这个命令会被节点7001成功执行。\n\n本节接下来的内容将介绍计算键所属槽的方法，节点判断某个槽是否由自己负责的方法，以及moved错误的实现方法，最后，本节还会介绍节点和单机redis服务器保存键值对数据的相同和不同之处。\n\n\n# 计算键属于哪个槽\n\n节点使用以下算法来计算给定键key属于哪个槽：\n\ndef slot_number(key):\n    return crc16(key) & 16383\n\n\n其中crc16（key）语句用于计算键key的crc-16校验和，而&16383语句则用于计算出一个介于0至16383之间的整数作为键key的槽号。\n\n使用cluster keyslot命令可以查看一个给定键属于哪个槽：\n\n127.0.0.1:7000> cluster keyslot "date"\n(integer) 2022\n127.0.0.1:7000> cluster keyslot "msg"\n(integer) 6257\n127.0.0.1:7000> cluster keyslot "name"\n(integer) 5798\n127.0.0.1:7000> cluster keyslot "fruits"\n(integer) 14943\n\n\ncluster keyslot命令就是通过调用上面给出的槽分配算法来实现的，以下是该命令的伪代码实现：\n\ndef cluster_keyslot(key):\n    # 计算槽号\n    slot = slot_number(key)\n    # 将槽号返回给客户端\n    reply_client(slot)\n\n\n\n# 判断槽是否由当前节点负责处理\n\n当节点计算出键所属的槽i之后，节点就会检查自己在clusterstate.slots数组中的项i，判断键所在的槽是否由自己负责：\n\n1）如果clusterstate.slots[i]等于clusterstate.myself，那么说明槽i由当前节点负责，节点可以执行客户端发送的命令。\n\n2）如果clusterstate.slots[i]不等于clusterstate.myself，那么说明槽i并非由当前节点负责，节点会根据clusterstate.slots[i]指向的clusternode结构所记录的节点ip和端口号，向客户端返回moved错误，指引客户端转向至正在处理槽i的节点。\n\n举个例子，假设图17-19为节点7000的clusterstate结构：\n\n * 当客户端向节点7000发送命令set date"2013-12-31"的时候，节点首先计算出键date属于槽2022，然后检查得出clusterstate.slots[2022]等于clusterstate.myself，这说明槽2022正是由节点7000负责，于是节点7000直接执行这个set命令，并将结果返回给发送命令的客户端。\n\n * 当客户端向节点7000发送命令set msg"happy new year！"的时候，节点首先计算出键msg属于槽6257，然后检查clusterstate.slots[6257]是否等于clusterstate.myself，结果发现两者并不相等：这说明槽6257并非由节点7000负责处理，于是节点7000访问clusterstate.slots[6257]所指向的clusternode结构，并根据结构中记录的ip地址127.0.0.1和端口号7001，向客户端返回错误moved 6257 127.0.0.1:7001，指引节点转向至正在负责处理槽6257的节点7001。\n\n\n\n图17-19　节点7000的clusterstate结构\n\n\n# moved 错误\n\n当节点发现键所在的槽并非由自己负责处理的时候，节点就会向客户端返回一个 moved 错误，指引客户端转向至正在负责槽的节点。\n\nmoved错误的格式为：\n\nmoved <slot> <ip>:<port>\n\n\n其中slot为键所在的槽，而ip和port则是负责处理槽slot的节点的ip地址和端口号。例如错误：\n\noved 10086 127.0.0.1:7002\n\n\n表示槽10086正由ip地址为127.0.0.1，端口号为7002的节点负责。\n\n又例如错误：\n\nmoved 789 127.0.0.1:7000\n\n\n表示槽789正由ip地址为127.0.0.1，端口号为7000的节点负责。\n\n当客户端接收到节点返回的moved错误时，客户端会根据moved错误中提供的ip地址和端口号，转向至负责处理槽slot的节点，并向该节点重新发送之前想要执行的命令。以前面的客户端从节点7000转向至7001的情况作为例子：\n\n127.0.0.1:7000> set msg "happy new year!"\n-> redirected to slot [6257] located at 127.0.0.1:7001\nok\n127.0.0.1:7001>\n\n\n图17-20展示了客户端向节点7000发送set命令，并获得moved错误的过程。\n\n\n\n图17-20　节点7000向客户端返回moved错误\n\n而图17-21则展示了客户端根据moved错误，转向至节点7001，并重新发送set命令的过程。\n\n\n\n图17-21　客户端根据moved错误的指示转向至节点7001\n\n一个集群客户端通常会与集群中的多个节点创建套接字连接，而所谓的节点转向实际上就是换一个套接字来发送命令。\n\n如果客户端尚未与想要转向的节点创建套接字连接，那么客户端会先根据moved错误提供的ip地址和端口号来连接节点，然后再进行转向。\n\n> 被隐藏的moved错误\n> \n> 集群模式的redis-cli客户端在接收到moved错误时，并不会打印出moved错误，而是根据moved错误自动进行节点转向，并打印出转向信息，所以我们是看不见节点返回的moved错误的：\n> \n> $ redis-cli -c -p 7000 # \n> 集群模式\n> 127.0.0.1:7000> set msg "happy new year!"\n> -> redirected to slot [6257] located at 127.0.0.1:7001\n> ok\n> 127.0.0.1:7001>\n> \n> \n> 但是，如果我们使用单机（stand alone）模式的redis-cli客户端，再次向节点7000发送相同的命令，那么moved错误就会被客户端打印出来：\n> \n> $ redis-cli -p 7000 # \n> 单机模式\n> 127.0.0.1:7000> set msg "happy new year!"\n> (error) moved 6257 127.0.0.1:7001\n> 127.0.0.1:7000>\n> \n> \n> 这是因为单机模式的redis-cli客户端不清楚moved错误的作用，所以它只会直接将moved错误直接打印出来，而不会进行自动转向。\n\n\n# 节点数据库的实现\n\n集群节点保存键值对以及键值对过期时间的方式，与第9章里面介绍的单机redis服务器保存键值对以及键值对过期时间的方式完全相同。\n\n节点和单机服务器在数据库方面的一个区别是，节点只能使用0号数据库，而单机redis服务器则没有这一限制。\n\n举个例子，图17-22展示了节点7000的数据库状态，数据库中包含列表键"lst"，哈希键"book"，以及字符串键"date"，其中键"lst"和键"book"带有过期时间。\n\n另外，除了将键值对保存在数据库里面之外，节点还会用clusterstate结构中的slots_to_keys跳跃表来保存槽和键之间的关系：\n\ntypedef struct clusterstate {\n\n  zskiplist *slots_to_keys;\n \n} clusterstate;\n\n\n\n\n图17-22　节点7000的数据库\n\nslots_to_keys跳跃表每个节点的分值（score）都是一个槽号，而每个节点的成员（member）都是一个数据库键：\n\n * 每当节点往数据库中添加一个新的键值对时，节点就会将这个键以及键的槽号关联到slots_to_keys跳跃表。\n\n * 当节点删除数据库中的某个键值对时，节点就会在slots_to_keys跳跃表解除被删除键与槽号的关联。\n\n举个例子，对于图17-22所示的数据库，节点7000将创建类似图17-23所示的slots_to_keys跳跃表：\n\n * 键"book"所在跳跃表节点的分值为1337.0，这表示键"book"所在的槽为1337。\n\n * 键"date"所在跳跃表节点的分值为2022.0，这表示键"date"所在的槽为2022。\n\n * 键"lst"所在跳跃表节点的分值为3347.0，这表示键"lst"所在的槽为3347。\n\n通过在slots_to_keys跳跃表中记录各个数据库键所属的槽，节点可以很方便地对属于某个或某些槽的所有数据库键进行批量操作，例如命令cluster getkeysinslot命令可以返回最多count个属于槽slot的数据库键，而这个命令就是通过遍历slots_to_keys跳跃表来实现的。\n\n\n\n图17-23　节点7000的slots_to_keys跳跃表\n\n\n# 重新分片\n\nredis集群的重新分片操作可以将任意数量已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽所属的键值对也会从源节点被移动到目标节点。\n\n重新分片操作可以在线（online）进行，在重新分片的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。\n\n举个例子，对于之前提到的，包含7000、7001、7002三个节点的集群来说，我们可以向这个集群添加一个ip为127.0.0.1，端口号为7003的节点（后面简称节点7003）：\n\n$ redis-cli -c -p 7000\n127.0.0.1:7000> cluster meet 127.0.0.1 7003\nok\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388635782831 0 connected 5001-10000\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388635782831 0 connected 10001-16383\n04579925484ce537d3410d7ce97bd2e260c459a2 127.0.0.1:7003 master - 0 1388635782330 0 connected\n\n\n然后通过重新分片操作，将原本指派给节点7002的槽15001至16383改为指派给节点7003。\n\n以下是重新分片操作执行之后，节点的槽分配状态：\n\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master -0 0 0 connected 0-5000\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master -0 1388635782831 0 connected 5001-10000\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master -0 1388635782831 0 connected 10001-15000\n04579925484ce537d3410d7ce97bd2e260c459a2 127.0.0.1:7003 master -0 1388635782330 0 connected 15001-16383\n\n\n重新分片的实现原理\n\nredis集群的重新分片操作是由redis的集群管理软件redis-trib负责执行的，redis提供了进行重新分片所需的所有命令，而redis-trib则通过向源节点和目标节点发送命令来进行重新分片操作。\n\nredis-trib对集群的单个槽slot进行重新分片的步骤如下：\n\n1）redis-trib对目标节点发送cluster setslot <slot> importing <source_id> 命令，让目标节点准备好从源节点导入（import）属于槽slot的键值对。\n\n2）redis-trib对源节点发送cluster setslot <slot> migrating <target_id>命令，让源节点准备好将属于槽slot的键值对迁移（migrate）至目标节点。\n\n3）redis-trib向源节点发送cluster getkeysinslot <slot> <count> 命令，获得最多count个属于槽slot的键值对的键名（key name）。\n\n4）对于步骤3获得的每个键名，redis-trib都向源节点发送一个migrate <target_ip ><target_port> <key_name> 0 <timeout>命令，将被选中的键原子地从源节点迁移至目标节点。\n\n5）重复执行步骤3和步骤4，直到源节点保存的所有属于槽slot的键值对都被迁移至目标节点为止。每次迁移键的过程如图17-24所示。\n\n6）redis-trib向集群中的任意一个节点发送cluster setslot <slot> node <target_id>命令，将槽slot指派给目标节点，这一指派信息会通过消息发送至整个集群，最终集群中的所有节点都会知道槽slot已经指派给了目标节点。\n\n\n\n图17-24　迁移键的过程\n\n图17-25 展示了对槽slot进行重新分片的整个过程。\n\n如果重新分片涉及多个槽，那么redis-trib将对每个给定的槽分别执行上面给出的步骤。\n\n\n\n图17-25　对槽slot进行重新分片的过程\n\n\n# ask 错误\n\n在进行重新分片期间，源节点向目标节点迁移一个槽的过程中，可能会出现这样一种情况：属于被迁移槽的一部分键值对保存在源节点里面，而另一部分键值对则保存在目标节点里面。\n\n当客户端向源节点发送一个与数据库键有关的命令，并且命令要处理的数据库键恰好就属于正在被迁移的槽时：\n\n * 源节点会先在自己的数据库里面查找指定的键，如果找到的话，就直接执行客户端发送的命令。\n * 相反地，如果源节点没能在自己的数据库里面找到指定的键，那么这个键有可能已经被迁移到了目标节点，源节点将向客户端返回一个ask错误，指引客户端转向正在导入槽的目标节点，并再次发送之前想要执行的命令。\n\n图17-26展示了源节点判断是否需要向客户端发送ask错误的整个过程。\n\n\n\n图17-26　判断是否发送ask错误的过程\n\n举个例子，假设节点7002正在向节点7003迁移槽16198，这个槽包含"is"和"love"两个键，其中键"is"还留在节点7002，而键"love"已经被迁移到了节点7003。\n\n如果我们向节点7002发送关于键"is"的命令，那么这个命令会直接被节点7002执行：\n\n127.0.0.1:7002> get "is"\n"you get the key \'is\'"\n\n\n而如果我们向节点7002发送关于键"love"的命令，那么客户端会先被转向至节点7003，然后再次执行命令：\n\n127.0.0.1:7002> get "love"\n-> redirected to slot [16198] located at 127.0.0.1:7003\n"you get the key \'love\'"\n127.0.0.1:7003>\n\n\n> 被隐藏的ask错误\n> \n> 和接到moved错误时的情况类似，集群模式的redis-cli在接到ask错误时也不会打印错误，而是自动根据错误提供的ip地址和端口进行转向动作。如果想看到节点发送的ask错误的话，可以使用单机模式的redis-cli客户端：\n> \n> $ redis-cli -p 7002\n> 127.0.0.1:7002> get "love"\n> (error) ask 16198 127.0.0.1:7003\n\n注意\n\n在写这篇文章的时候，集群模式的redis-cli并未支持ask自动转向，上面展示的ask自动转向行为实际上是根据moved自动转向行为虚构出来的。因此，当集群模式的redis-cli真正支持ask自动转向时，它的行为和上面展示的行为可能会有所不同。\n\n本节将对ask错误的实现原理进行说明，并对比ask错误和moved错误的区别。\n\n\n# cluster setslot importing命令的实现\n\nclusterstate结构的importing_slots_from数组记录了当前节点正在从其他节点导入的槽：\n\ntypedef struct clusterstate {\n\n  clusternode *importing_slots_from[16384];\n \n} clusterstate;\n\n\n如果importing_slots_from[i]的值不为null，而是指向一个clusternode结构，那么表示当前节点正在从clusternode所代表的节点导入槽i。\n\n在对集群进行重新分片的时候，向目标节点发送命令：\n\ncluster setslot <i> importing <source_id>\n\n\n可以将目标节点clusterstate.importing_slots_from[i]的值设置为source_id所代表节点的clusternode结构。\n\n举个例子，如果客户端向节点7003发送以下命令：\n\n# 9dfb 是节点7002 的id \n127.0.0.1:7003> cluster setslot 16198 importing 9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26\nok\n\n\n那么节点7003的clusterstate.importing_slots_from数组将变成图17-27所示的样子。\n\n\n\n图17-27　节点7003的importing_slots_from数组\n\n\n# cluster setslot migrating 命令的实现\n\nclusterstate结构的migrating_slots_to数组记录了当前节点正在迁移至其他节点的槽：\n\ntypedef struct clusterstate {\n   \n   clusternode *migrating_slots_to[16384];\n   \n} clusterstate;\n\n\n如果migrating_slots_to[i]的值不为null，而是指向一个 clusternode 结构，那么表示当前节点正在将槽i迁移至clusternode所代表的节点。\n\n在对集群进行重新分片的时候，向源节点发送命令：\n\ncluster setslot <i> migrating <target_id>\n\n\n可以将源节点clusterstate.migrating_slots_to[i]的值设置为target_id所代表节点的clusternode结构。\n\n举个例子，如果客户端向节点7002发送以下命令：\n\n# 0457\n是节点7003 \n的id \n127.0.0.1:7002> cluster setslot 16198 migrating 04579925484ce537d3410d7ce97bd2e260c459a2\nok\n\n\n那么节点7002的clusterstate.migrating_slots_to数组将变成图17-28所示的样子。\n\n图17-28　节点7002的migrating_slots_to数组\n\n\n# ask 错误\n\n如果节点收到一个关于键key的命令请求，并且键key所属的槽i正好就指派给了这个节点，那么节点会尝试在自己的数据库里查找键key，如果找到了的话，节点就直接执行客户端发送的命令。\n\n与此相反，如果节点没有在自己的数据库里找到键key，那么节点会检查自己的clusterstate.migrating_slots_to[i]，看键key所属的槽i是否正在进行迁移，如果槽i的确在进行迁移的话，那么节点会向客户端发送一个ask错误，引导客户端到正在导入槽i的节点去查找键key。\n\n举个例子，假设在节点7002向节点7003迁移槽16198期间，有一个客户端向节点7002发送命令：\n\nget \n“love”\n\n\n因为键"love"正好属于槽16198，所以节点7002会首先在自己的数据库中查找键"love"，但并没有找到，通过检查自己的clusterstate.migrating_slots_to[16198]，节点7002发现自己正在将槽16198迁移至节点7003，于是它向客户端返回错误：\n\nask 16198 127.0.0.1:7003\n\n\n这个错误表示客户端可以尝试到ip为127.0.0.1，端口号为7003的节点去执行和槽16198有关的操作，如图17-29所示。\n\n\n\n图17-29　客户端接收到节点7002返回的ask错误\n\n接到ask错误的客户端会根据错误提供的ip地址和端口号，转向至正在导入槽的目标节点，然后首先向目标节点发送一个asking命令，之后再重新发送原本想要执行的命令。\n\n以前面的例子来说，当客户端接收到节点7002返回的以下错误时：\n\nask 16198 127.0.0.1:7003\n\n\n客户端会转向至节点7003，首先发送命令：\n\nasking\n\n\n然后再次发送命令：\n\nget "love"\n\n\n并获得回复：\n\n"you get the key \'love\'"\n\n\n整个过程如图17-30所示。\n\n\n\n图17-30　客户端转向至节点7003\n\n\n# asking命令\n\nasking命令唯一要做的就是打开发送该命令的客户端的redis_asking标识，以下是该命令的伪代码实现：\n\ndef asking():\n    # \n打开标识\n    client.flags |= redis_asking\n    # \n向客户端返回ok \n回复\n    reply("ok")\n\n\n在一般情况下，如果客户端向节点发送一个关于槽i的命令，而槽i又没有指派给这个节点的话，那么节点将向客户端返回一个moved错误；但是，如果节点的clusterstate.importing_slots_from[i]显示节点正在导入槽i，并且发送命令的客户端带有redis_asking标识，那么节点将破例执行这个关于槽i的命令一次，图17-31展示了这个判断过程。\n\n\n\n图17-31　节点判断是否执行客户端命令的过程\n\n当客户端接收到ask错误并转向至正在导入槽的节点时，客户端会先向节点发送一个asking命令，然后才重新发送想要执行的命令，这是因为如果客户端不发送asking命令，而直接发送想要执行的命令的话，那么客户端发送的命令将被节点拒绝执行，并返回moved错误。\n\n举个例子，我们可以使用普通模式的redis-cli客户端，向正在导入槽16198的节点7003发送以下命令：\n\n$ ./redis-cli -p 7003\n127.0.0.1:7003> get "love"\n(error) moved 16198 127.0.0.1:7002\n\n\n虽然节点7003正在导入槽16198，但槽16198目前仍然是指派给了节点7002，所以节点7003会向客户端返回moved错误，指引客户端转向至节点7002。\n\n但是，如果我们在发送get命令之前，先向节点发送一个asking命令，那么这个get命令就会被节点7003执行：\n\n127.0.0.1:7003> asking\nok\n127.0.0.1:7003> get "love"\n"you get the key \'love\'"\n\n\n另外要注意的是，客户端的redis_asking标识是一个一次性标识，当节点执行了一个带有redis_asking标识的客户端发送的命令之后，客户端的redis_asking标识就会被移除。\n\n举个例子，如果我们在成功执行get命令之后，再次向节点7003发送get命令，那么第二次发送的get命令将执行失败，因为这时客户端的redis_asking标识已经被移除：\n\n127.0.0.1:7003> asking                 #\n打开redis_asking\n标识\nok\n127.0.0.1:7003> get "love"   #\n移除redis_asking\n标识\n"you get the key \'love\'"\n127.0.0.1:7003> get "love"   # redis_asking\n标识未打开，执行失败\n(error) moved 16198 127.0.0.1:7002\n\n\n\n# ask错误和moved错误的区别\n\nask错误和moved错误都会导致客户端转向，它们的区别在于：\n\n * moved错误代表槽的负责权已经从一个节点转移到了另一个节点：在客户端收到关于槽i的moved错误之后，客户端每次遇到关于槽i的命令请求时，都可以直接将命令请求发送至moved错误所指向的节点，因为该节点就是目前负责槽i的节点。\n\n * 与此相反，ask错误只是两个节点在迁移槽的过程中使用的一种临时措施：在客户端收到关于槽i的ask错误之后，客户端只会在接下来的一次命令请求中将关于槽i的命令请求发送至ask错误所指示的节点，但这种转向不会对客户端今后发送关于槽i的命令请求产生任何影响，客户端仍然会将关于槽i的命令请求发送至目前负责处理槽i的节点，除非ask错误再次出现。\n\n\n# 复制与故障转移\n\nredis集群中的节点分为主节点（master）和从节点（slave），其中主节点用于处理槽，而从节点则用于复制某个主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求。\n\n举个例子，对于包含7000、7001、7002、7003四个主节点的集群来说，我们可以将7004、7005两个节点添加到集群里面，并将这两个节点设定为节点7000的从节点，如图17-32所示（图中以双圆形表示主节点，单圆形表示从节点）。\n\n\n\n图17-32　设置节点7004和节点7005成为节点7000的从节点\n\n表17-1记录了集群各个节点的当前状态，以及它们正在做的工作。\n\n表17-1　集群各个节点的当前状态\n\n\n\n如果这时，节点7000进入下线状态，那么集群中仍在正常运作的几个主节点将在节点7000的两个从节点——节点7004和节点7005中选出一个节点作为新的主节点，这个新的主节点将接管原来节点7000负责处理的槽，并继续处理客户端发送的命令请求。\n\n例如，如果节点7004被选中为新的主节点，那么节点7004将接管原来由节点7000负责处理的槽0至槽5000，节点7005也会从原来的复制节点7000，改为复制节点7004，如图17-33所示（图中用虚线包围的节点为已下线节点）。\n\n\n\n图17-33　节点7004成为新的主节点\n\n表17-2记录了在对节点7000进行故障转移之后，集群各个节点的当前状态，以及它们正在做的工作。\n\n表17-2　集群各个节点的当前状态\n\n\n\n如果在故障转移完成之后，下线的节点7000重新上线，那么它将成为节点7004的从节点，如图17-34所示。\n\n\n\n图17-34　重新上线的节点7000成为节点7004的从节点\n\n表17-3展示了节点7000复制节点7004之后，集群中各个节点的状态。\n\n表17-3　集群各个节点的当前状态\n\n\n\n本节接下来的内容将介绍节点的复制方法，检测节点是否下线的方法，以及对下线主节点进行故障转移的方法。\n\n\n# 设置从节点\n\n向一个节点发送命令：\n\ncluster replicate <node_id>\n\n\n可以让接收命令的节点成为node_id所指定节点的从节点，并开始对主节点进行复制：\n\n * 接收到该命令的节点首先会在自己的clusterstate.nodes字典中找到node_id所对应节点的clusternode结构，并将自己的clusterstate.myself.slaveof指针指向这个结构，以此来记录这个节点正在复制的主节点：\n   \n   struct clusternode {\n   \n        //如果这是一个从节点，那么指向主节点\n        struct clusternode *slaveof;\n       \n      };\n   \n\n * 然后节点会修改自己在clusterstate.myself.flags中的属性，关闭原本的redis_node_master标识，打开redis_node_slave标识，表示这个节点已经由原来的主节点变成了从节点。\n\n * 最后，节点会调用复制代码，并根据clusterstate.myself.slaveof指向的clusternode结构所保存的ip地址和端口号，对主节点进行复制。因为节点的复制功能和单机redis服务器的复制功能使用了相同的代码，所以让从节点复制主节点相当于向从节点发送命令slaveof。\n\n图17-35展示了节点7004在复制节点7000时的clusterstate结构：\n\n * clusterstate.myself.flags属性的值为redis_node_slave，表示节点7004是一个从节点。\n\n * clusterstate.myself.slaveof指针指向代表节点7000的结构，表示节点7004正在复制的主节点为节点7000。\n\n\n\n图17-35　节点7004的clusterstate结构\n\n一个节点成为从节点，并开始复制某个主节点这一信息会通过消息发送给集群中的其他节点，最终集群中的所有节点都会知道某个从节点正在复制某个主节点。\n\n集群中的所有节点都会在代表主节点的clusternode结构的slaves属性和numslaves属性中记录正在复制这个主节点的从节点名单：\n\nstruct clusternode {\n    // \n    // \n正在复制这个主节点的从节点数量\n    int numslaves;\n    // \n一个数组\n    // \n每个数组项指向一个正在复制这个主节点的从节点的clusternode\n结构\n    struct clusternode **slaves;\n    // \n};\n\n\n举个例子，图17-36记录了节点7004和节点7005成为节点7000的从节点之后，集群中的各个节点为节点7000创建的clusternode结构的样子：\n\n * 代表节点7000的clusternode结构的numslaves属性的值为2，这说明有两个从节点正在复制节点7000。\n\n * 代表节点7000的clusternode结构的slaves数组的两个项分别指向代表节点7004和代表节点7005的clusternode结构，这说明节点7000的两个从节点分别是节点7004和节点7005。\n\n\n\n图17-36　集群中的各个节点为节点7000创建的clusternode结构\n\n\n# 故障检测\n\n集群中的每个节点都会定期地向集群中的其他节点发送ping消息，以此来检测对方是否在线，如果接收ping消息的节点没有在规定的时间内，向发送ping消息的节点返回pong消息，那么发送ping消息的节点就会将接收ping消息的节点标记为疑似下线（probable fail，pfail）。\n\n举个例子，如果节点7001向节点7000发送了一条ping消息，但是节点7000没有在规定的时间内，向节点7001返回一条pong消息，那么节点7001就会在自己的clusterstate.nodes字典中找到节点7000所对应的clusternode结构，并在结构的flags属性中打开redis_node_pfail标识，以此表示节点7000进入了疑似下线状态，如图17-37所示。\n\n\n\n图17-37　代表节点7000的clusternode结构\n\n集群中的各个节点会通过互相发送消息的方式来交换集群中各个节点的状态信息，例如某个节点是处于在线状态、疑似下线状态（pfail），还是已下线状态（fail）。\n\n当一个主节点a通过消息得知主节点b认为主节点c进入了疑似下线状态时，主节点a会在自己的clusterstate.nodes字典中找到主节点c所对应的clusternode结构，并将主节点b的下线报告（failure report）添加到clusternode结构的fail_reports链表里面：\n\nstruct clusternode {\n  // \n  // \n一个链表，记录了所有其他节点对该节点的下线报告\n  list *fail_reports;\n  // \n};\n\n\n每个下线报告由一个clusternodefailreport结构表示：\n\nstruct clusternodefailreport {\n  // \n报告目标节点已经下线的节点\n  struct clusternode *node;\n  // \n最后一次从node\n节点收到下线报告的时间\n  // \n程序使用这个时间戳来检查下线报告是否过期\n  // \n（与当前时间相差太久的下线报告会被删除）\n  mstime_t time;\n} typedef clusternodefailreport;\n\n\n举个例子，如果主节点7001在收到主节点7002、主节点7003发送的消息后得知，主节点7002和主节点7003都认为主节点7000进入了疑似下线状态，那么主节点7001将为主节点7000创建图17-38所示的下线报告。\n\n\n\n图17-38　节点7000的下线报告\n\n如果在一个集群里面，半数以上负责处理槽的主节点都将某个主节点x报告为疑似下线，那么这个主节点x将被标记为已下线（fail），将主节点x标记为已下线的节点会向集群广播一条关于主节点x的fail消息，所有收到这条fail消息的节点都会立即将主节点x标记为已下线。\n\n举个例子，对于图17-38所示的下线报告来说，主节点7002和主节点7003都认为主节点7000进入了下线状态，并且主节点7001也认为主节点7000进入了疑似下线状态（代表主节点7000的结构打开了redis_node_pfail标识），综合起来，在集群四个负责处理槽的主节点里面，有三个都将主节点7000标记为下线，数量已经超过了半数，所以主节点7001会将主节点7000标记为已下线，并向集群广播一条关于主节点7000的fail消息，如图17-39所示。\n\n\n\n图17-39　节点7001向集群广播fail消息\n\n\n# 故障转移\n\n当一个从节点发现自己正在复制的主节点进入了已下线状态时，从节点将开始对下线主节点进行故障转移，以下是故障转移的执行步骤：\n\n1）复制下线主节点的所有从节点里面，会有一个从节点被选中。\n\n2）被选中的从节点会执行slaveof no one命令，成为新的主节点。\n\n3）新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。\n\n4）新的主节点向集群广播一条pong消息，这条pong消息可以让集群中的其他节点立即知道这个节点已经由从节点变成了主节点，并且这个主节点已经接管了原本由已下线节点负责处理的槽。\n\n5）新的主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成。\n\n\n# 选举新的主节点\n\n新的主节点是通过选举产生的。\n\n以下是集群选举新的主节点的方法：\n\n1）集群的配置纪元是一个自增计数器，它的初始值为0。\n\n2）当集群里的某个节点开始一次故障转移操作时，集群配置纪元的值会被增一。\n\n3）对于每个配置纪元，集群里每个负责处理槽的主节点都有一次投票的机会，而第一个向主节点要求投票的从节点将获得主节点的投票。\n\n4）当从节点发现自己正在复制的主节点进入已下线状态时，从节点会向集群广播一条clustermsg_type_failover_auth_request消息，要求所有收到这条消息、并且具有投票权的主节点向这个从节点投票。\n\n5）如果一个主节点具有投票权（它正在负责处理槽），并且这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条clustermsg_type_failover_auth_ack消息，表示这个主节点支持从节点成为新的主节点。\n\n6）每个参与选举的从节点都会接收clustermsg_type_failover_auth_ack消息，并根据自己收到了多少条这种消息来统计自己获得了多少主节点的支持。\n\n7）如果集群里有n个具有投票权的主节点，那么当一个从节点收集到大于等于n/2+1张支持票时，这个从节点就会当选为新的主节点。\n\n8）因为在每一个配置纪元里面，每个具有投票权的主节点只能投一次票，所以如果有n个主节点进行投票，那么具有大于等于n/2+1张支持票的从节点只会有一个，这确保了新的主节点只会有一个。\n\n9）如果在一个配置纪元里面没有从节点能收集到足够多的支持票，那么集群进入一个新的配置纪元，并再次进行选举，直到选出新的主节点为止。\n\n这个选举新主节点的方法和第16章介绍的选举领头sentinel的方法非常相似，因为两者都是基于raft算法的领头选举（leader election）方法来实现的。\n\n\n# 消息\n\n集群中的各个节点通过发送和接收消息（message）来进行通信，我们称发送消息的节点为发送者（sender），接收消息的节点为接收者（receiver），如图17-40所示。\n\n\n\n图17-40　发送者和接收者\n\n节点发送的消息主要有以下五种：\n\n * meet消息：当发送者接到客户端发送的cluster meet命令时，发送者会向接收者发送meet消息，请求接收者加入到发送者当前所处的集群里面。\n\n * ping消息：集群里的每个节点默认每隔一秒钟就会从已知节点列表中随机选出五个节点，然后对这五个节点中最长时间没有发送过ping消息的节点发送ping消息，以此来检测被选中的节点是否在线。除此之外，如果节点a最后一次收到节点b发送的pong消息的时间，距离当前时间已经超过了节点a的cluster-node-timeout选项设置时长的一半，那么节点a也会向节点b发送ping消息，这可以防止节点a因为长时间没有随机选中节点b作为ping消息的发送对象而导致对节点b的信息更新滞后。\n\n * pong消息：当接收者收到发送者发来的meet消息或者ping消息时，为了向发送者确认这条meet消息或者ping消息已到达，接收者会向发送者返回一条pong消息。另外，一个节点也可以通过向集群广播自己的pong消息来让集群中的其他节点立即刷新关于这个节点的认识，例如当一次故障转移操作成功执行之后，新的主节点会向集群广播一条pong消息，以此来让集群中的其他节点立即知道这个节点已经变成了主节点，并且接管了已下线节点负责的槽。\n\n * fail消息：当一个主节点a判断另一个主节点b已经进入fail状态时，节点a会向集群广播一条关于节点b的fail消息，所有收到这条消息的节点都会立即将节点b标记为已下线。\n\n * publish消息：当节点接收到一个publish命令时，节点会执行这个命令，并向集群广播一条publish消息，所有接收到这条publish消息的节点都会执行相同的publish命令。\n\n一条消息由消息头（header）和消息正文（data）组成，接下来的内容将首先介绍消息头，然后再分别介绍上面提到的五种不同类型的消息正文。\n\n\n# 消息头\n\n节点发送的所有消息都由一个消息头包裹，消息头除了包含消息正文之外，还记录了消息发送者自身的一些信息，因为这些信息也会被消息接收者用到，所以严格来讲，我们可以认为消息头本身也是消息的一部分。\n\n每个消息头都由一个cluster.h/clustermsg结构表示：\n\ntypedef struct {\n  // 消息的长度（包括这个消息头的长度和消息正文的长度）\n  uint32_t totlen;\n  // 消息的类型\n  uint16_t type;\n  // 消息正文包含的节点信息数量\n  // 只在发送meet、ping、pong这三种gossip协议消息时使用\n  uint16_t count;\n  // 发送者所处的配置纪元\n  uint64_t currentepoch;\n // 如果发送者是一个主节点，那么这里记录的是发送者的配置纪元\n  // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的配置纪元\n  uint64_t configepoch;\n  // 发送者的名字（id\n） \n  char sender[redis_cluster_namelen];\n  // 发送者目前的槽指派信息\n  unsigned char myslots[redis_cluster_slots/8];\n  // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的名字\n  // 如果发送者是一个主节点，那么这里记录的是redis_node_null_name\n  // （一个40字节长，值全为0的字节数组）\n  char slaveof[redis_cluster_namelen];\n  // 发送者的端口号\n  uint16_t port;\n  // 发送者的标识值\n  uint16_t flags;\n  // 发送者所处集群的状态\n  unsigned char state;\n  // 消息的正文（或者说，内容）\n  union clustermsgdata data;\n} clustermsg;\n\n\nclustermsg.data属性指向联合cluster.h/clustermsgdata，这个联合就是消息的正文：\n\nunion clustermsgdata {\n  // meet、ping、pong消息的正文\n  struct {\n    // 每条meet、ping、pong消息都包含两个\n    // clustermsgdatagossip结构\n    clustermsgdatagossip gossip[1];\n  } ping;\n  // fail消息的正文\n  struct {\n    clustermsgdatafail about;\n  } fail;\n  // publish消息的正文\n  struct {\n    clustermsgdatapublish msg;\n  } publish;\n  // 其他消息的正文\n};\n\n\nclustermsg结构的currentepoch、sender、myslots等属性记录了发送者自身的节点信息，接收者会根据这些信息，在自己的clusterstate.nodes字典里找到发送者对应的clusternode结构，并对结构进行更新。\n\n举个例子，通过对比接收者为发送者记录的槽指派信息，以及发送者在消息头的myslots属性记录的槽指派信息，接收者可以知道发送者的槽指派信息是否发生了变化。\n\n又或者说，通过对比接收者为发送者记录的标识值，以及发送者在消息头的flags属性记录的标识值，接收者可以知道发送者的状态和角色是否发生了变化，例如节点状态由原来的在线变成了下线，或者由主节点变成了从节点等等。\n\n\n# meet、ping、pong消息的实现\n\nredis集群中的各个节点通过gossip协议来交换各自关于不同节点的状态信息，其中gossip协议由meet、ping、pong三种消息实现，这三种消息的正文都由两个cluster.h/clustermsgdatagossip结构组成：\n\nunion clustermsgdata {\n  // \n  // meet、ping和pong消息的正文\n  struct {\n    // 每条meet、ping、pong消息都包含两个\n    // clustermsgdatagossip结构\n    clustermsgdatagossip gossip[1];\n  } ping;\n  // 其他消息的正文 \n};\n\n\n\n因为meet、ping、pong三种消息都使用相同的消息正文，所以节点通过消息头的type属性来判断一条消息是meet消息、ping消息还是pong消息。\n\n每次发送meet、ping、pong消息时，发送者都从自己的已知节点列表中随机选出两个节点（可以是主节点或者从节点），并将这两个被选中节点的信息分别保存到两个clustermsgdatagossip结构里面。\n\nclustermsgdatagossip结构记录了被选中节点的名字，发送者与被选中节点最后一次发送和接收ping消息和pong消息的时间戳，被选中节点的ip地址和端口号，以及被选中节点的标识值：\n\ntypedef struct {\n  // 节点的名字\n  char nodename[redis_cluster_namelen];\n  // 最后一次向该节点发送ping消息的时间戳\n  uint32_t ping_sent;\n  // 最后一次从该节点接收到pong消息的时间戳\n  uint32_t pong_received;\n  // 节点的ip地址\n  char ip[16];\n  // 节点的端口号\n  uint16_t port;\n  // 节点的标识值\n  uint16_t flags;\n} clustermsgdatagossip;\n\n\n当接收者收到meet、ping、pong消息时，接收者会访问消息正文中的两个clustermsgdatagossip结构，并根据自己是否认识clustermsgdatagossip结构中记录的被选中节点来选择进行哪种操作：\n\n * 如果被选中节点不存在于接收者的已知节点列表，那么说明接收者是第一次接触到被选中节点，接收者将根据结构中记录的ip地址和端口号等信息，与被选中节点进行握手。\n\n * 如果被选中节点已经存在于接收者的已知节点列表，那么说明接收者之前已经与被选中节点进行过接触，接收者将根据clustermsgdatagossip结构记录的信息，对被选中节点所对应的clusternode结构进行更新。\n\n举个发送ping消息和返回pong消息的例子，假设在一个包含a、b、c、d、e、f六个节点的集群里：\n\n * 节点a向节点d发送ping消息，并且消息里面包含了节点b和节点c的信息，当节点d收到这条ping消息时，它将更新自己对节点b和节点c的认识。\n\n * 之后，节点d将向节点a返回一条pong消息，并且消息里面包含了节点e和节点f的消息，当节点a收到这条pong消息时，它将更新自己对节点e和节点f的认识。\n\n整个通信过程如图17-41所示。\n\n\n\n图17-41　一个ping-pong消息通信示例\n\n\n# fail消息的实现\n\n当集群里的主节点a将主节点b标记为已下线（fail）时，主节点a将向集群广播一条关于主节点b的fail消息，所有接收到这条fail消息的节点都会将主节点b标记为已下线。\n\n在集群的节点数量比较大的情况下，单纯使用gossip协议来传播节点的已下线信息会给节点的信息更新带来一定延迟，因为gossip协议消息通常需要一段时间才能传播至整个集群，而发送fail消息可以让集群里的所有节点立即知道某个主节点已下线，从而尽快判断是否需要将集群标记为下线，又或者对下线主节点进行故障转移。\n\nfail消息的正文由cluster.h/clustermsgdatafail结构表示，这个结构只包含一个nodename属性，该属性记录了已下线节点的名字：\n\ntypedef struct {\n    char nodename[redis_cluster_namelen];\n} clustermsgdatafail;\n\n\n因为集群里的所有节点都有一个独一无二的名字，所以fail消息里面只需要保存下线节点的名字，接收到消息的节点就可以根据这个名字来判断是哪个节点下线了。\n\n举个例子，对于包含7000、7001、7002、7003四个主节点的集群来说：\n\n * 如果主节点7001发现主节点7000已下线，那么主节点7001将向主节点7002和主节点7003发送fail消息，其中fail消息中包含的节点名字为主节点7000的名字，以此来表示主节点7000已下线。\n\n * 当主节点7002和主节点7003都接收到主节点7001发送的fail消息时，它们也会将主节点7000标记为已下线。\n\n * 因为这时集群已经有超过一半的主节点认为主节点7000已下线，所以集群剩下的几个主节点可以判断是否需要将集群标记为下线，又或者开始对主节点7000进行故障转移。\n\n图17-42至图17-44展示了节点发送和接收fail消息的整个过程。\n\n\n\n图17-42　节点7001将节点7000标记为已下线\n\n\n\n图17-43　节点7001向集群广播fail消息\n\n\n\n图17-44　节点7002和节点7003也将节点7000标记为已下线\n\n\n# publish消息的实现\n\n当客户端向集群中的某个节点发送命令\n\npublish <channel> <message>\n\n\n的时候，接收到publish命令的节点不仅会向channel频道发送消息message，它还会向集群广播一条publish消息，所有接收到这条publish消息的节点都会向channel频道发送message消息。\n\n换句话说，向集群中的某个节点发送命令：\n\npublish <channel> <message>\n\n\n将导致集群中的所有节点都向channel频道发送message消息。\n\n举个例子，对于包含7000、7001、7002、7003四个节点的集群来说，如果节点7000收到了客户端发送的publish命令，那么节点7000将向7001、7002、7003三个节点发送publish消息，如图17-45所示。\n\n\n\n图17-45　接收到publish命令的节点7000向集群广播publish消息\n\npublish消息的正文由cluster.h/clustermsgdatapublish结构表示：\n\ntypedef struct {\n  uint32_t channel_len;\n  uint32_t message_len;\n  // 定义为8 字节只是为了对齐其他消息结构\n  // 实际的长度由保存的内容决定\n  unsigned char bulk_data[8];\n} clustermsgdatapublish;\n\n\nclustermsgdatapublish结构的bulk_data属性是一个字节数组，这个字节数组保存了客户端通过publish命令发送给节点的channel参数和message参数，而结构的channel_len和message_len则分别保存了channel参数的长度和message参数的长度：\n\n * 其中bulk_data的0字节至channel_len-1字节保存的是channel参数。\n\n * 而bulk_data的channel_len字节至channel_len+message_len-1字节保存的则是message参数。\n\n举个例子，如果节点收到的publish命令为：\n\npublish "news.it" "hello"\n\n\n那么节点发送的publish消息的clustermsgdatapublish结构将如图17-46所示：其中bulk_data数组的前七个字节保存了channel参数的值"news.it"，而bulk_data数组的后五个字节则保存了message参数的值"hello"。\n\n\n\n图17-46　clustermsgdatapublish结构示例\n\n> 为什么不直接向节点广播publish命令\n> \n> 实际上，要让集群的所有节点都执行相同的publish命令，最简单的方法就是向所有节点广播相同的publish命令，这也是redis在复制publish命令时所使用的方法，不过因为这种做法并不符合redis集群的“各个节点通过发送和接收消息来进行通信”这一规则，所以节点没有采取广播publish命令的做法。\n\n\n# 重点回顾\n\n * 节点通过握手来将其他节点添加到自己所处的集群当中。\n * 集群中的 16384 个槽可以分别指派给集群中的各个节点， 每个节点都会记录哪些槽指派给了自己， 而哪些槽又被指派给了其他节点。\n * 节点在接到一个命令请求时， 会先检查这个命令请求要处理的键所在的槽是否由自己负责， 如果不是的话， 节点将向客户端返回一个 moved 错误， moved 错误携带的信息可以指引客户端转向至正在负责相关槽的节点。\n * 对 redis 集群的重新分片工作是由客户端执行的， 重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点。\n * 如果节点 a 正在迁移槽 i 至节点 b ， 那么当节点 a 没能在自己的数据库中找到命令指定的数据库键时， 节点 a 会向客户端返回一个 ask 错误， 指引客户端到节点 b 继续查找指定的数据库键。\n * moved 错误表示槽的负责权已经从一个节点转移到了另一个节点， 而 ask 错误只是两个节点在迁移槽的过程中使用的一种临时措施。\n * 集群里的从节点用于复制主节点， 并在主节点下线时， 代替主节点继续处理命令请求。\n * 集群中的节点通过发送和接收消息来进行通讯， 常见的消息包括 meet 、 ping 、 pong 、 publish 、 fail 五种。\n\n\n# 参考文献\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"Echo 系统设计",frontmatter:{home:!0,title:"Echo 系统设计",heroImage:"https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409152149836.png",heroText:"EchoDesign",tagline:"🚀系统设计之「讲解 + 面试指南」，水滴石穿，设计无银弹！",actionText:"开始使用 →",actionLink:"/pages/fccd91/",bannerBg:"none",postList:"none"},regularPath:"/",relativePath:"index.md",key:"v-a11d1cc4",path:"/",headersStr:null,content:'# 📖 必看系列\n\n * 👉《热门系统设计算法》：重点突击系统设计中常见算法\n * 👉《Redis 系统设计》：手把手教你学会 Redis 系统设计，贯穿 90% 知识点\n * 👉《Kafka 系统设计》：手把手教你学会 Kafka 系统设计，贯穿 90% 知识点\n * 👉《Netty 系统设计》：手把手教你学会 Netty 系统设计，贯穿 90% 知识点\n * 👉《实战系统设计》：手把手带你体验实战系统设计\n\n\n# ⚡ 反馈与交流\n\n在使用过程中有任何问题和想法，请给我提 Issue。 你也可以在 Issue 查看别人提的问题和给出解决方案。\n\n或者加入我们的交流群：参与贡献可以榜上留名 💯\n\n「EchoDesign」微信群(添加我微信备注"进群系统设计")',normalizedContent:'# 📖 必看系列\n\n * 👉《热门系统设计算法》：重点突击系统设计中常见算法\n * 👉《redis 系统设计》：手把手教你学会 redis 系统设计，贯穿 90% 知识点\n * 👉《kafka 系统设计》：手把手教你学会 kafka 系统设计，贯穿 90% 知识点\n * 👉《netty 系统设计》：手把手教你学会 netty 系统设计，贯穿 90% 知识点\n * 👉《实战系统设计》：手把手带你体验实战系统设计\n\n\n# ⚡ 反馈与交流\n\n在使用过程中有任何问题和想法，请给我提 issue。 你也可以在 issue 查看别人提的问题和给出解决方案。\n\n或者加入我们的交流群：参与贡献可以榜上留名 💯\n\n「echodesign」微信群(添加我微信备注"进群系统设计")',charsets:{cjk:!0},lastUpdated:"2024/09/19, 03:26:41",lastUpdatedTimestamp:1726716401e3},{title:"分布式缓存",frontmatter:{title:"分布式缓存",date:"2024-09-14T02:09:39.000Z",permalink:"/pages/84cb49/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/01.%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98.html",relativePath:"实战系统设计/01.设计基础设施/01.分布式缓存.md",key:"v-38079923",path:"/pages/84cb49/",headers:[{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:2},{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:124},{level:2,title:"渐进式设计",slug:"渐进式设计",normalizedTitle:"渐进式设计",charIndex:251},{level:3,title:"本地缓存",slug:"本地缓存",normalizedTitle:"本地缓存",charIndex:261},{level:3,title:"分布式缓存",slug:"分布式缓存",normalizedTitle:"分布式缓存",charIndex:272},{level:4,title:"认识分布式缓存",slug:"认识分布式缓存",normalizedTitle:"认识分布式缓存",charIndex:281},{level:4,title:"如何选择缓存节点？",slug:"如何选择缓存节点",normalizedTitle:"如何选择缓存节点？",charIndex:294},{level:4,title:"缓存客户端是什么？",slug:"缓存客户端是什么",normalizedTitle:"缓存客户端是什么？",charIndex:311},{level:4,title:"获取缓存节点列表",slug:"获取缓存节点列表",normalizedTitle:"获取缓存节点列表",charIndex:332},{level:2,title:"非功能设计",slug:"非功能设计",normalizedTitle:"非功能设计",charIndex:383},{level:3,title:"高可用",slug:"高可用",normalizedTitle:"高可用",charIndex:74},{level:3,title:"高可靠",slug:"高可靠",normalizedTitle:"高可靠",charIndex:403},{level:2,title:"还有什么重要的",slug:"还有什么重要的",normalizedTitle:"还有什么重要的",charIndex:482},{level:3,title:"一致性",slug:"一致性",normalizedTitle:"一致性",charIndex:149},{level:3,title:"数据过期",slug:"数据过期",normalizedTitle:"数据过期",charIndex:221},{level:3,title:"数据淘汰策略",slug:"数据淘汰策略",normalizedTitle:"数据淘汰策略",charIndex:533},{level:3,title:"本地缓存+远程缓存",slug:"本地缓存-远程缓存",normalizedTitle:"本地缓存+远程缓存",charIndex:544},{level:3,title:"安全",slug:"安全",normalizedTitle:"安全",charIndex:234},{level:3,title:"监控和日志",slug:"监控和日志",normalizedTitle:"监控和日志",charIndex:238},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:575}],headersStr:"需求 前置知识 渐进式设计 本地缓存 分布式缓存 认识分布式缓存 如何选择缓存节点？ 缓存客户端是什么？ 获取缓存节点列表 非功能设计 高可用 高可靠 还有什么重要的 一致性 数据过期 数据淘汰策略 本地缓存+远程缓存 安全 监控和日志 总结",content:"# 需求\n\n功能性\n\n * put(key,value)\n * get(key)\n\n非功能性\n\n * 高扩展（随着数据和请求的增加轻松扩展）\n * 高可用（硬件/网络故障下仍然可用）\n * 高性能（put和get的高性能！）\n * 持久化\n\n\n# 前置知识\n\n * LRU 算法\n\n * 哈希取模与一致性哈希算法\n\n * 专用缓存集群与共存缓存\n\n * 缓存客户端\n\n * 静态与动态缓存服务器列表配置\n\n * 主从复制\n\n * 缓存一致性、数据过期、本地和远程缓存、安全性、监控和日志记录。\n\n\n# 渐进式设计\n\n\n# 本地缓存\n\n\n\n\n# 分布式缓存\n\n# 认识分布式缓存\n\n\n\n# 如何选择缓存节点？\n\n\n\n\n\n# 缓存客户端是什么？\n\n\n\n * 客户端如何获取缓存节点列表？\n * 当扩容或缩容时，缓存节点列表如何更新？\n\n# 获取缓存节点列表\n\n\n\n\n# 非功能设计\n\n\n# 高可用\n\n\n\n\n# 高可靠\n\n * 数据异步复制到replica\n * 数据同步复制到replica\n * 数据同步复制到所有replica然后返回？\n\n我们需要权衡！\n\n\n# 还有什么重要的\n\n\n# 一致性\n\n引入同步复制，确保所有缓存节点的视图一致\n\n\n# 数据过期\n\n\n# 数据淘汰策略\n\n\n# 本地缓存+远程缓存\n\n\n# 安全\n\n\n# 监控和日志\n\n\n# 总结\n\n",normalizedContent:"# 需求\n\n功能性\n\n * put(key,value)\n * get(key)\n\n非功能性\n\n * 高扩展（随着数据和请求的增加轻松扩展）\n * 高可用（硬件/网络故障下仍然可用）\n * 高性能（put和get的高性能！）\n * 持久化\n\n\n# 前置知识\n\n * lru 算法\n\n * 哈希取模与一致性哈希算法\n\n * 专用缓存集群与共存缓存\n\n * 缓存客户端\n\n * 静态与动态缓存服务器列表配置\n\n * 主从复制\n\n * 缓存一致性、数据过期、本地和远程缓存、安全性、监控和日志记录。\n\n\n# 渐进式设计\n\n\n# 本地缓存\n\n\n\n\n# 分布式缓存\n\n# 认识分布式缓存\n\n\n\n# 如何选择缓存节点？\n\n\n\n\n\n# 缓存客户端是什么？\n\n\n\n * 客户端如何获取缓存节点列表？\n * 当扩容或缩容时，缓存节点列表如何更新？\n\n# 获取缓存节点列表\n\n\n\n\n# 非功能设计\n\n\n# 高可用\n\n\n\n\n# 高可靠\n\n * 数据异步复制到replica\n * 数据同步复制到replica\n * 数据同步复制到所有replica然后返回？\n\n我们需要权衡！\n\n\n# 还有什么重要的\n\n\n# 一致性\n\n引入同步复制，确保所有缓存节点的视图一致\n\n\n# 数据过期\n\n\n# 数据淘汰策略\n\n\n# 本地缓存+远程缓存\n\n\n# 安全\n\n\n# 监控和日志\n\n\n# 总结\n\n",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"限流器",frontmatter:{title:"限流器",date:"2024-09-14T02:09:39.000Z",permalink:"/pages/57d5a5/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/02.%E9%99%90%E6%B5%81%E5%99%A8.html",relativePath:"实战系统设计/01.设计基础设施/02.限流器.md",key:"v-0fe45e7f",path:"/pages/57d5a5/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:190},{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:299},{level:2,title:"组件架构",slug:"组件架构",normalizedTitle:"组件架构",charIndex:438},{level:2,title:"接口和类",slug:"接口和类",normalizedTitle:"接口和类",charIndex:449},{level:2,title:"消息广播",slug:"消息广播",normalizedTitle:"消息广播",charIndex:355},{level:2,title:"如何集成",slug:"如何集成",normalizedTitle:"如何集成",charIndex:471},{level:2,title:"答疑解惑",slug:"答疑解惑",normalizedTitle:"答疑解惑",charIndex:482},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:753}],headersStr:"概述 需求 前置知识 组件架构 接口和类 消息广播 如何集成 答疑解惑 总结",content:"# 概述\n\n\n\n * 我们需要编写一个可扩展以处理高负载的软件吗？\n\n * 负载均衡器中已经实现了最大连接数和服务端点上的最大线程数呢。我们还需要节流吗？\n\n * * 这种机制是不加区别的，有些操作快，有些操作慢，负载均衡器不了解每个操作的成本，如果我们想对某个操作实现特定的限流，负载均衡器就不好使了\n\n * 我们如何单独控制每个主机呢？\n\n * * 主机间相互通信！\n\n\n# 需求\n\n功能性\n\n * allowRequest(request)\n\n非功能性\n\n * 低延迟（尽快做出决定）\n * 准确性（尽可能准确）\n * 可扩展（支持集群中任意数量的主机）\n * 高可用\n * 高可靠\n\n\n# 前置知识\n\n * 令牌桶算法\n * 速率限制解决方案的面向对象设计\n * 负载均衡器最大连接数，自动扩展\n * 消息广播：全网状网络拓扑、gossip 通信、分布式缓存、协调服务\n * 通信协议：TCP、UDP\n * 嵌入式速率限制器与守护进程\n * 存储桶管理、同步\n\n\n# 组件架构\n\n\n\n\n# 接口和类\n\n\n\n\n# 消息广播\n\n\n\n\n# 如何集成\n\n\n\n\n# 答疑解惑\n\n * 我的服务非常受欢迎，有数百万用户。这是否意味着内存中存储了数百万个桶？\n\n * * 可以瞬间增加很多桶，但是当桶没用的时候，可以删除桶\n\n * 有哪些失败的场景？\n\n * * 守护线程失败\n   * 网络分区，无法传播消息，每个主机将允许更多的请求\n\n * 我们是否需要一个自动配置管理服务？\n\n * 我有点担心同步问题。这不是瓶颈吗？\n\n * * 原则上来说 我们要重视并发安全\n   * 但是我们没有必要实现，会对性能造成影响\n\n * 当客户的请求被拒绝时，他们应该做什么？\n\n * * 重试（指数退避、抖动）\n\n\n# 总结\n\n",normalizedContent:"# 概述\n\n\n\n * 我们需要编写一个可扩展以处理高负载的软件吗？\n\n * 负载均衡器中已经实现了最大连接数和服务端点上的最大线程数呢。我们还需要节流吗？\n\n * * 这种机制是不加区别的，有些操作快，有些操作慢，负载均衡器不了解每个操作的成本，如果我们想对某个操作实现特定的限流，负载均衡器就不好使了\n\n * 我们如何单独控制每个主机呢？\n\n * * 主机间相互通信！\n\n\n# 需求\n\n功能性\n\n * allowrequest(request)\n\n非功能性\n\n * 低延迟（尽快做出决定）\n * 准确性（尽可能准确）\n * 可扩展（支持集群中任意数量的主机）\n * 高可用\n * 高可靠\n\n\n# 前置知识\n\n * 令牌桶算法\n * 速率限制解决方案的面向对象设计\n * 负载均衡器最大连接数，自动扩展\n * 消息广播：全网状网络拓扑、gossip 通信、分布式缓存、协调服务\n * 通信协议：tcp、udp\n * 嵌入式速率限制器与守护进程\n * 存储桶管理、同步\n\n\n# 组件架构\n\n\n\n\n# 接口和类\n\n\n\n\n# 消息广播\n\n\n\n\n# 如何集成\n\n\n\n\n# 答疑解惑\n\n * 我的服务非常受欢迎，有数百万用户。这是否意味着内存中存储了数百万个桶？\n\n * * 可以瞬间增加很多桶，但是当桶没用的时候，可以删除桶\n\n * 有哪些失败的场景？\n\n * * 守护线程失败\n   * 网络分区，无法传播消息，每个主机将允许更多的请求\n\n * 我们是否需要一个自动配置管理服务？\n\n * 我有点担心同步问题。这不是瓶颈吗？\n\n * * 原则上来说 我们要重视并发安全\n   * 但是我们没有必要实现，会对性能造成影响\n\n * 当客户的请求被拒绝时，他们应该做什么？\n\n * * 重试（指数退避、抖动）\n\n\n# 总结\n\n",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"热点探查（Top k）",frontmatter:{title:"热点探查（Top k）",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/5dcb6b/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/03.%E7%83%AD%E7%82%B9%E6%8E%A2%E6%9F%A5%EF%BC%88Top%20k%EF%BC%89.html",relativePath:"实战系统设计/01.设计基础设施/03.热点探查（Top k）.md",key:"v-1fc37214",path:"/pages/5dcb6b/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:313},{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:373},{level:2,title:"渐进设计",slug:"渐进设计",normalizedTitle:"渐进设计",charIndex:600},{level:3,title:"哈希表+单主机",slug:"哈希表-单主机",normalizedTitle:"哈希表+单主机",charIndex:609},{level:3,title:"哈希表+多主机",slug:"哈希表-多主机",normalizedTitle:"哈希表+多主机",charIndex:739},{level:3,title:"哈希表+多主机+分区",slug:"哈希表-多主机-分区",normalizedTitle:"哈希表+多主机+分区",charIndex:830},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:959},{level:2,title:"高层架构",slug:"高层架构",normalizedTitle:"高层架构",charIndex:1630},{level:2,title:"客户端检索数据",slug:"客户端检索数据",normalizedTitle:"客户端检索数据",charIndex:2400},{level:2,title:"解答疑惑",slug:"解答疑惑",normalizedTitle:"解答疑惑",charIndex:2485},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:5033}],headersStr:"概述 前置知识 需求 渐进设计 哈希表+单主机 哈希表+多主机 哈希表+多主机+分区 总结 高层架构 客户端检索数据 解答疑惑 参考资料",content:"# 概述\n\nTop k 问题解决方案的各种应用（Google/Twitter/YouTube 趋势、热门产品、波动性股票、DDoS 攻击预防）。\n\n在这样的规模下，数据库或分布式缓存不是一个选项。我们可能正在处理 1M RPS。如果我们使用 DB 来跟踪视图计数，首先写入/更新会非常慢，然后找到前 K 项需要扫描整个数据集。\n\n也许 MapReduce 可以提供帮助。但这还不够。我们需要尽可能接近实时地返回重磅统计数据。\n\n例如：\n\n * Calculate top 100 list for last\n * 1 min, 5 min, 15 mins, 60 mins etc.\n\n这使得这个问题成为流处理问题\n\n\n# 前置知识\n\nMapReduce\n\nCount-min sketch\n\n数据聚合原理\n\n合并 N 个排序列表问题\n\n\n# 需求\n\n功能\n\n * topK(k,startTime,endTime)\n\n非功能\n\n * 高扩展（随着数据量的增加而扩展：视频、推文、帖子等）\n * 高可用性（在硬件/网络故障中幸存，没有 SPOF）\n * 高性能（返回前100名列表需要几十毫秒，考虑到性能要求，这暗示了最终列表应该预先计算，我们应该避免在调用 top K API 时进行繁重的计算。）\n * 准确性（例如，通过使用数据采样，我们可能不会计算每个元素，而只计算一小部分事件）\n\n\n# 渐进设计\n\n\n# 哈希表+单主机\n\n * 在 hashmap 中保留传入事件列表的计数\n\n * * 按频率对 hashmap 中的条目列表进行排序，并返回前 K 个元素。时间 O(nLogN)\n   * 将元素放在大小为 K 的堆上。时间复杂度O(nLogK)\n\n\n\n\n\n\n# 哈希表+多主机\n\n * 如果您将所有 youtube 视频 ID 存储在您作为主机的内存中，内存将是一个问题\n\n\n\n虽然增加了吞吐量，但是有内存瓶颈，hash 表会越来越大\n\n\n# 哈希表+多主机+分区\n\n * 我们不会将所有哈希表数据从所有主机发送到存储主机。相反，我们会在每个主机上单独计算 topK 列表，最后我们需要在存储主机上合并这些排序的列表。\n\n\n\n数据分区可以不将所有数据存储到一个主机上，减少了每个主机内存的压力\n\n\n# 总结\n\n问题\n\n * 我们认为数据集是无界的，这就是为什么我们能够考虑将其划分为多个块。但流数据没有界限。它不断涌现。在这种情况下，处理器主机只能在一段时间内继续积累数据，在此之前它将耗尽内存。比如 1 分钟。\n\n * * 我们将 1 min 数据刷新到存储主机\n   * 存储主机存储每分钟的热点列表\n   * 我们故意丢失了有关非 topK 元素的所有信息。我们负担不起在内存中存储每个视频的信息。\n   * 但是，如果我们想找到过去 1 小时或过去 1 天的 topK，我们如何使用 60 个 1 分钟 list 来构建它呢。鉴于当前的方法，没有解决此问题的正确方法。要找到当天的 topK，我们需要全天的完整数据集。\n   * 需求冲突，保留完整的 1 天数据（以满足需求）或丢失它以负担存储。让我们把所有数据存储在磁盘上，因为它无法放入内存，并使用批处理框架来做 topK 列表。Map Reduce 架构将发挥作用\n\n * 每次引入数据分区时，我们都必须考虑数据复制，以便将每个分区的副本存储在多个节点上。我们需要考虑在集群中添加/删除新节点时重新平衡。我们需要处理热分区。\n\n解决方案\n\n在进入上面讨论的方法之前，让我们想一想是否有一个简单的解决方案来解决 topK 问题？\n\n权衡利弊权衡利弊，所有的选择，不过是权衡利弊罢了\n\n我们需要在此过程中做出牺牲。准确性就是牺牲\n\n我们要合理利用数据结构，这将帮助我们使用固定大小的内存计算 topK，但结果可能不是 100% 准确\n\nCount-Min Sketch！\n\n\n\n\n# 高层架构\n\n\n\n * API Gateway：连接到视频内容交付系统，该系统将提供视频请求\n\n * * 对于我们的用例，我们对 API 网关的 1 个功能感兴趣，即日志生成，其中记录对 API 的每次调用。通常这些日志用于监控、日志记录和审计。我们将使用这些日志来计算每个视频的观看次数。我们可能有一个后台进程，它从日志中读取数据，进行一些初始聚合并发送数据进行进一步处理。\n   * 为 API 网关服务上的缓冲区分配内存，读取日志行，并构建频率计数哈希表。此缓冲区的大小应有限，当缓冲区已满时，将刷新数据。如果缓冲区在某个时间段内未满，我们可以根据时间段进行 flush。\n   * 其他选项可以是动态聚合数据，而不写入日志文件。或者完全跳过 API 网关端的数据聚合，并将有关每个事件（正在查看的视频）的信息进一步发送以进行处理。评估每个选项的优缺点。\n   * 我们可以通过以紧凑的二进制格式（例如 Apache Avro）序列化数据来节省网络 IO 利用率，并让 CPU 付出代价。所有这些注意事项都取决于 API 网关主机上可用的资源，即内存、CPU、网络和磁盘 IO。\n\n * distributed messaging system：初始聚合数据将发送到分布式消息传递系统，如 Apache Kafka。\n\n * Fast path and Slow path\n\n * * 在 Fast path 中，我们将大致计算 topK hitter 的结果。结果将在几秒钟内提供。\n   * 在 slow path 中，我们将精确计算 topK hitter 的结果。几分钟/几小时内即可获得结果。\n   * 根据系统对系统时序的限制（是否需要近实时结果，或者延迟是否可以接受以实现精度），您应该选择任一路径。\n\n\n\n\n\n\n\n\n# 客户端检索数据\n\n\n\n合并 2 个不同的结果集来回答 API 调用并不准确，但这是一种权衡。您无法在任何时间聚合数据。您必须了解确切需要的东西，并据此进行构建。\n\n\n# 解答疑惑\n\n * 我们是否可以使用哈希映射，但每隔几秒钟将其内容（转换为堆后）刷新到存储中，而不是使用 CMS？\n\n * * 对于小规模，使用哈希映射是完全可以的。当规模增长时，哈希映射可能会变得太大（使用大量内存）。为了防止这种情况，我们可以对数据进行分区，以便只有所有数据的子集进入 Fast Processor 服务主机。但它使架构复杂化。CMS 的美妙之处在于它消耗有限的（定义的）内存，并且无需对数据进行分区。CMS 的缺点是它大约计算数字。权衡，权衡......\n\n * 我们如何将 count-min sketch 和 heap 存储到数据库中？如何设计 table 架构？\n\n * * Heap 只是一个一维数组。CMS 是一个二维数组。这意味着两者都可以很容易地序列化为字节数组。使用语言原生序列化 API 或备受推崇的序列化框架（Protobufs、Thrift、Avro）。我们可以将它们以这种形式存储在数据库中。\n\n * 虽然 CMS 是为了节省内存，但我们还有 n log k 时间来获得前 k，对吧？\n\n * * 是的。它是 O(nlogk)（用于堆）+ O(klogk)（用于对最终列表进行排序）。N 通常比 k 大得多。所以，O(nlogk) 是主导的。\n\n * 如果 CMS 只用于 1 min 计数，为什么我们不直接使用哈希表来计数呢？毕竟，数据集的大小不会无限增长。\n\n * * 对于中小规模，哈希表解决方案可能效果很好。但请记住，如果我们尝试创建一个需要为许多不同场景查找前 K 个列表的服务，则可能会有很多这样的哈希表，并且它不会很好地扩展。例如，最常喜欢/不喜欢的视频、观看次数最多（基于时间）的视频、评论次数最多的视频、视频打开期间异常数量最多的前 K 名等。类似的统计数据可以按渠道级别、每个国家/地区等进行计算。长话短说，我们可能需要使用我们的服务来计算许多不同的前 K 名列表\n\n * 如何合并两个 1 小时的 top k 列表，以获得 2 小时的 top k？\n\n * * 我们需要对相同标识符的值求和。换句话说，我们会汇总两个列表中相同视频的观看次数。并获取合并列表的前 K （通过排序或使用 Heap）。 [不过，这不一定是 100% 准确的结果]\n\n * 当存在您提到的不同情况时，CMS 如何工作......最赞/最不喜欢的视频。我们需要构建多个 CMS 吗？我们是否需要为每个类别指定哈希值？无论哪种方式，它们都需要更多的内存，就像哈希表一样。\n\n * * 正确。我们需要特定的 CMS 来计算不同的事件类型：视频观看次数、喜欢、不喜欢、提交评论等。\n\n * 关于慢路径，我对数据分区器感到困惑。我们是否可以删除第一个 Distribute Messaging System 和 data partitioner？API 网关将根据其分区直接向第二个 Distribute Messaging System 发送消息。例如，API 网关会将所有 B 消息发送到分区 1，将所有 A 消息发送到分区 2，将所有 C 消息发送到分区 3。为什么我们需要第一个 Distribute Messaging System 和data partitioner？如果我们使用 Kalfa 作为 Distribute Messaging System，我们可以只为一组消息类型创建一个主题。\n\n * * 在大规模（例如 YouTube 规模）的情况下，API Gateway 集群将处理大量请求。我假设这些是数千甚至数万台 CPU 密集型计算机。主要目标是提供视频内容并尽可能少地做 “其他” 事情。在这样的机器上，我们通常希望避免任何繁重的聚合或逻辑。我们能做的最简单的事情是将每个视频观看请求批处理在一起。我的意思是根本不做任何聚合。创建包含如下内容的单个消息：{A = 1， B = 1， C = 1}，并将其发送以进行进一步处理。在您提到的选项中，我们仍然需要在 API Gateway 端进行聚合。由于规模很大，我们无法承受每个视频观看请求向第二个 DMS 发送一条消息的后果。我的意思是我们不能有三条消息，比如：{A = 1}、{B = 1}、{C = 1}。如视频中所述，我们希望在每个下一个阶段降低请求率。\n\n * 我有一个关于快速路径的问题，似乎您将聚合后的 CMS 存储在存储系统中，但这足以计算前 k 个吗？我觉得我们需要有一个网站列表，并在某个地方维护一个大小为 k 的堆，以找出前 k 个。\n\n * * 你是对的。我们始终保留两种数据结构：一个 count-min sketch 和一个 Fast Processor 中的堆。我们使用 count-min sketch 进行计数，而 heap 存储前 k 个列表。在 Storage 服务中，我们也可以同时保留两者或仅保留堆。但是 heap 始终存在。\n\n * 所以总的来说，我们仍然需要存储 keys。。。Count-min Sketch 无需单独维护 Key 的计数，从而有助于节省成本...当必须找到前 k 个元素时，必须遍历每个键并使用 count-min sketch 来找到前 k 个元素......这种理解准确吗？\n\n * * 我们需要存储 keys ，但只需要存储其中的 K（或更多）。并非全部。\n   * 当每个 key 到来时，我们执行以下操作：\n * * * 将其添加到 count-min 草图中。\n     * 从 count-min 草图中获取密钥计数。\n     * 检查当前 key 是否在堆中。如果它出现在堆中，我们在那里更新它的 count 值。如果它不存在于堆中，我们检查堆是否已满。如果未满，我们将此键添加到堆中。如果 heap 已满，则检查最小 heap 元素并将其值与当前 key count 值进行比较。此时，我们可以删除最小元素并添加当前键（如果当前键计数 > 最小元素值）。\n * * 这样我们只保留预定义数量的 key。这保证了我们永远不会超过内存，因为 count-min sketch 和堆的大小都是有限的\n\n\n# 参考资料\n\nhttps://www.youtube.com/watch?v=kx-XDoPjoHw",normalizedContent:"# 概述\n\ntop k 问题解决方案的各种应用（google/twitter/youtube 趋势、热门产品、波动性股票、ddos 攻击预防）。\n\n在这样的规模下，数据库或分布式缓存不是一个选项。我们可能正在处理 1m rps。如果我们使用 db 来跟踪视图计数，首先写入/更新会非常慢，然后找到前 k 项需要扫描整个数据集。\n\n也许 mapreduce 可以提供帮助。但这还不够。我们需要尽可能接近实时地返回重磅统计数据。\n\n例如：\n\n * calculate top 100 list for last\n * 1 min, 5 min, 15 mins, 60 mins etc.\n\n这使得这个问题成为流处理问题\n\n\n# 前置知识\n\nmapreduce\n\ncount-min sketch\n\n数据聚合原理\n\n合并 n 个排序列表问题\n\n\n# 需求\n\n功能\n\n * topk(k,starttime,endtime)\n\n非功能\n\n * 高扩展（随着数据量的增加而扩展：视频、推文、帖子等）\n * 高可用性（在硬件/网络故障中幸存，没有 spof）\n * 高性能（返回前100名列表需要几十毫秒，考虑到性能要求，这暗示了最终列表应该预先计算，我们应该避免在调用 top k api 时进行繁重的计算。）\n * 准确性（例如，通过使用数据采样，我们可能不会计算每个元素，而只计算一小部分事件）\n\n\n# 渐进设计\n\n\n# 哈希表+单主机\n\n * 在 hashmap 中保留传入事件列表的计数\n\n * * 按频率对 hashmap 中的条目列表进行排序，并返回前 k 个元素。时间 o(nlogn)\n   * 将元素放在大小为 k 的堆上。时间复杂度o(nlogk)\n\n\n\n\n\n\n# 哈希表+多主机\n\n * 如果您将所有 youtube 视频 id 存储在您作为主机的内存中，内存将是一个问题\n\n\n\n虽然增加了吞吐量，但是有内存瓶颈，hash 表会越来越大\n\n\n# 哈希表+多主机+分区\n\n * 我们不会将所有哈希表数据从所有主机发送到存储主机。相反，我们会在每个主机上单独计算 topk 列表，最后我们需要在存储主机上合并这些排序的列表。\n\n\n\n数据分区可以不将所有数据存储到一个主机上，减少了每个主机内存的压力\n\n\n# 总结\n\n问题\n\n * 我们认为数据集是无界的，这就是为什么我们能够考虑将其划分为多个块。但流数据没有界限。它不断涌现。在这种情况下，处理器主机只能在一段时间内继续积累数据，在此之前它将耗尽内存。比如 1 分钟。\n\n * * 我们将 1 min 数据刷新到存储主机\n   * 存储主机存储每分钟的热点列表\n   * 我们故意丢失了有关非 topk 元素的所有信息。我们负担不起在内存中存储每个视频的信息。\n   * 但是，如果我们想找到过去 1 小时或过去 1 天的 topk，我们如何使用 60 个 1 分钟 list 来构建它呢。鉴于当前的方法，没有解决此问题的正确方法。要找到当天的 topk，我们需要全天的完整数据集。\n   * 需求冲突，保留完整的 1 天数据（以满足需求）或丢失它以负担存储。让我们把所有数据存储在磁盘上，因为它无法放入内存，并使用批处理框架来做 topk 列表。map reduce 架构将发挥作用\n\n * 每次引入数据分区时，我们都必须考虑数据复制，以便将每个分区的副本存储在多个节点上。我们需要考虑在集群中添加/删除新节点时重新平衡。我们需要处理热分区。\n\n解决方案\n\n在进入上面讨论的方法之前，让我们想一想是否有一个简单的解决方案来解决 topk 问题？\n\n权衡利弊权衡利弊，所有的选择，不过是权衡利弊罢了\n\n我们需要在此过程中做出牺牲。准确性就是牺牲\n\n我们要合理利用数据结构，这将帮助我们使用固定大小的内存计算 topk，但结果可能不是 100% 准确\n\ncount-min sketch！\n\n\n\n\n# 高层架构\n\n\n\n * api gateway：连接到视频内容交付系统，该系统将提供视频请求\n\n * * 对于我们的用例，我们对 api 网关的 1 个功能感兴趣，即日志生成，其中记录对 api 的每次调用。通常这些日志用于监控、日志记录和审计。我们将使用这些日志来计算每个视频的观看次数。我们可能有一个后台进程，它从日志中读取数据，进行一些初始聚合并发送数据进行进一步处理。\n   * 为 api 网关服务上的缓冲区分配内存，读取日志行，并构建频率计数哈希表。此缓冲区的大小应有限，当缓冲区已满时，将刷新数据。如果缓冲区在某个时间段内未满，我们可以根据时间段进行 flush。\n   * 其他选项可以是动态聚合数据，而不写入日志文件。或者完全跳过 api 网关端的数据聚合，并将有关每个事件（正在查看的视频）的信息进一步发送以进行处理。评估每个选项的优缺点。\n   * 我们可以通过以紧凑的二进制格式（例如 apache avro）序列化数据来节省网络 io 利用率，并让 cpu 付出代价。所有这些注意事项都取决于 api 网关主机上可用的资源，即内存、cpu、网络和磁盘 io。\n\n * distributed messaging system：初始聚合数据将发送到分布式消息传递系统，如 apache kafka。\n\n * fast path and slow path\n\n * * 在 fast path 中，我们将大致计算 topk hitter 的结果。结果将在几秒钟内提供。\n   * 在 slow path 中，我们将精确计算 topk hitter 的结果。几分钟/几小时内即可获得结果。\n   * 根据系统对系统时序的限制（是否需要近实时结果，或者延迟是否可以接受以实现精度），您应该选择任一路径。\n\n\n\n\n\n\n\n\n# 客户端检索数据\n\n\n\n合并 2 个不同的结果集来回答 api 调用并不准确，但这是一种权衡。您无法在任何时间聚合数据。您必须了解确切需要的东西，并据此进行构建。\n\n\n# 解答疑惑\n\n * 我们是否可以使用哈希映射，但每隔几秒钟将其内容（转换为堆后）刷新到存储中，而不是使用 cms？\n\n * * 对于小规模，使用哈希映射是完全可以的。当规模增长时，哈希映射可能会变得太大（使用大量内存）。为了防止这种情况，我们可以对数据进行分区，以便只有所有数据的子集进入 fast processor 服务主机。但它使架构复杂化。cms 的美妙之处在于它消耗有限的（定义的）内存，并且无需对数据进行分区。cms 的缺点是它大约计算数字。权衡，权衡......\n\n * 我们如何将 count-min sketch 和 heap 存储到数据库中？如何设计 table 架构？\n\n * * heap 只是一个一维数组。cms 是一个二维数组。这意味着两者都可以很容易地序列化为字节数组。使用语言原生序列化 api 或备受推崇的序列化框架（protobufs、thrift、avro）。我们可以将它们以这种形式存储在数据库中。\n\n * 虽然 cms 是为了节省内存，但我们还有 n log k 时间来获得前 k，对吧？\n\n * * 是的。它是 o(nlogk)（用于堆）+ o(klogk)（用于对最终列表进行排序）。n 通常比 k 大得多。所以，o(nlogk) 是主导的。\n\n * 如果 cms 只用于 1 min 计数，为什么我们不直接使用哈希表来计数呢？毕竟，数据集的大小不会无限增长。\n\n * * 对于中小规模，哈希表解决方案可能效果很好。但请记住，如果我们尝试创建一个需要为许多不同场景查找前 k 个列表的服务，则可能会有很多这样的哈希表，并且它不会很好地扩展。例如，最常喜欢/不喜欢的视频、观看次数最多（基于时间）的视频、评论次数最多的视频、视频打开期间异常数量最多的前 k 名等。类似的统计数据可以按渠道级别、每个国家/地区等进行计算。长话短说，我们可能需要使用我们的服务来计算许多不同的前 k 名列表\n\n * 如何合并两个 1 小时的 top k 列表，以获得 2 小时的 top k？\n\n * * 我们需要对相同标识符的值求和。换句话说，我们会汇总两个列表中相同视频的观看次数。并获取合并列表的前 k （通过排序或使用 heap）。 [不过，这不一定是 100% 准确的结果]\n\n * 当存在您提到的不同情况时，cms 如何工作......最赞/最不喜欢的视频。我们需要构建多个 cms 吗？我们是否需要为每个类别指定哈希值？无论哪种方式，它们都需要更多的内存，就像哈希表一样。\n\n * * 正确。我们需要特定的 cms 来计算不同的事件类型：视频观看次数、喜欢、不喜欢、提交评论等。\n\n * 关于慢路径，我对数据分区器感到困惑。我们是否可以删除第一个 distribute messaging system 和 data partitioner？api 网关将根据其分区直接向第二个 distribute messaging system 发送消息。例如，api 网关会将所有 b 消息发送到分区 1，将所有 a 消息发送到分区 2，将所有 c 消息发送到分区 3。为什么我们需要第一个 distribute messaging system 和data partitioner？如果我们使用 kalfa 作为 distribute messaging system，我们可以只为一组消息类型创建一个主题。\n\n * * 在大规模（例如 youtube 规模）的情况下，api gateway 集群将处理大量请求。我假设这些是数千甚至数万台 cpu 密集型计算机。主要目标是提供视频内容并尽可能少地做 “其他” 事情。在这样的机器上，我们通常希望避免任何繁重的聚合或逻辑。我们能做的最简单的事情是将每个视频观看请求批处理在一起。我的意思是根本不做任何聚合。创建包含如下内容的单个消息：{a = 1， b = 1， c = 1}，并将其发送以进行进一步处理。在您提到的选项中，我们仍然需要在 api gateway 端进行聚合。由于规模很大，我们无法承受每个视频观看请求向第二个 dms 发送一条消息的后果。我的意思是我们不能有三条消息，比如：{a = 1}、{b = 1}、{c = 1}。如视频中所述，我们希望在每个下一个阶段降低请求率。\n\n * 我有一个关于快速路径的问题，似乎您将聚合后的 cms 存储在存储系统中，但这足以计算前 k 个吗？我觉得我们需要有一个网站列表，并在某个地方维护一个大小为 k 的堆，以找出前 k 个。\n\n * * 你是对的。我们始终保留两种数据结构：一个 count-min sketch 和一个 fast processor 中的堆。我们使用 count-min sketch 进行计数，而 heap 存储前 k 个列表。在 storage 服务中，我们也可以同时保留两者或仅保留堆。但是 heap 始终存在。\n\n * 所以总的来说，我们仍然需要存储 keys。。。count-min sketch 无需单独维护 key 的计数，从而有助于节省成本...当必须找到前 k 个元素时，必须遍历每个键并使用 count-min sketch 来找到前 k 个元素......这种理解准确吗？\n\n * * 我们需要存储 keys ，但只需要存储其中的 k（或更多）。并非全部。\n   * 当每个 key 到来时，我们执行以下操作：\n * * * 将其添加到 count-min 草图中。\n     * 从 count-min 草图中获取密钥计数。\n     * 检查当前 key 是否在堆中。如果它出现在堆中，我们在那里更新它的 count 值。如果它不存在于堆中，我们检查堆是否已满。如果未满，我们将此键添加到堆中。如果 heap 已满，则检查最小 heap 元素并将其值与当前 key count 值进行比较。此时，我们可以删除最小元素并添加当前键（如果当前键计数 > 最小元素值）。\n * * 这样我们只保留预定义数量的 key。这保证了我们永远不会超过内存，因为 count-min sketch 和堆的大小都是有限的\n\n\n# 参考资料\n\nhttps://www.youtube.com/watch?v=kx-xdopjohw",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"消息队列",frontmatter:{title:"消息队列",date:"2024-09-14T16:43:00.000Z",permalink:"/pages/567090/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/04.%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97.html",relativePath:"实战系统设计/01.设计基础设施/04.消息队列.md",key:"v-6ee174df",path:"/pages/567090/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:11},{level:2,title:"高层抽象",slug:"高层抽象",normalizedTitle:"高层抽象",charIndex:78},{level:2,title:"虚拟IP和负载均衡",slug:"虚拟ip和负载均衡",normalizedTitle:"虚拟ip和负载均衡",charIndex:89},{level:2,title:"前端服务",slug:"前端服务",normalizedTitle:"前端服务",charIndex:105},{level:2,title:"元数据服务",slug:"元数据服务",normalizedTitle:"元数据服务",charIndex:116},{level:2,title:"后端服务",slug:"后端服务",normalizedTitle:"后端服务",charIndex:128},{level:2,title:"还有什么重要的？",slug:"还有什么重要的",normalizedTitle:"还有什么重要的？",charIndex:145},{level:3,title:"队列的创建与删除",slug:"队列的创建与删除",normalizedTitle:"队列的创建与删除",charIndex:160},{level:3,title:"消息的删除",slug:"消息的删除",normalizedTitle:"消息的删除",charIndex:173},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:183}],headersStr:"概述 需求 高层抽象 虚拟IP和负载均衡 前端服务 元数据服务 后端服务 还有什么重要的？ 队列的创建与删除 消息的删除 总结",content:"# 概述\n\n\n\n\n# 需求\n\n功能性\n\n * sendMessage(messageBody)\n * receiveMessage()\n\n非功能性\n\n\n# 高层抽象\n\n\n\n\n# 虚拟IP和负载均衡\n\n\n\n\n# 前端服务\n\n\n\n\n# 元数据服务\n\n\n\n\n# 后端服务\n\n\n\n\n\n\n\n\n\n\n# 还有什么重要的？\n\n\n\n\n# 队列的创建与删除\n\n\n# 消息的删除\n\n\n# 总结\n\n",normalizedContent:"# 概述\n\n\n\n\n# 需求\n\n功能性\n\n * sendmessage(messagebody)\n * receivemessage()\n\n非功能性\n\n\n# 高层抽象\n\n\n\n\n# 虚拟ip和负载均衡\n\n\n\n\n# 前端服务\n\n\n\n\n# 元数据服务\n\n\n\n\n# 后端服务\n\n\n\n\n\n\n\n\n\n\n# 还有什么重要的？\n\n\n\n\n# 队列的创建与删除\n\n\n# 消息的删除\n\n\n# 总结\n\n",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"服务通知",frontmatter:{title:"服务通知",date:"2024-09-14T16:43:28.000Z",permalink:"/pages/8416e6/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/05.%E8%AE%A2%E9%98%85%E5%8F%91%E5%B8%83.html",relativePath:"实战系统设计/01.设计基础设施/05.订阅发布.md",key:"v-6d73d8ca",path:"/pages/8416e6/",headers:[{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:2},{level:2,title:"高层架构",slug:"高层架构",normalizedTitle:"高层架构",charIndex:142},{level:2,title:"客户端主机",slug:"客户端主机",normalizedTitle:"客户端主机",charIndex:153}],headersStr:"需求 高层架构 客户端主机",content:"# 需求\n\n功能性\n\n * createTopic(topicname)\n * publish(topicName,message)\n * subscribe(topicName,endpoint)\n\n非功能性\n\n * 高可扩展性\n * 高可用性\n * 高性能\n * 持久性\n\n\n# 高层架构\n\n\n\n\n# 客户端主机",normalizedContent:"# 需求\n\n功能性\n\n * createtopic(topicname)\n * publish(topicname,message)\n * subscribe(topicname,endpoint)\n\n非功能性\n\n * 高可扩展性\n * 高可用性\n * 高性能\n * 持久性\n\n\n# 高层架构\n\n\n\n\n# 客户端主机",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"动态线程池",frontmatter:{title:"动态线程池",date:"2024-09-14T23:28:32.000Z",permalink:"/pages/d81a42/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/06.%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0.html",relativePath:"实战系统设计/01.设计基础设施/06.动态线程池.md",key:"v-216630e2",path:"/pages/d81a42/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"设计 微信",frontmatter:{title:"设计 微信",date:"2024-09-14T02:08:51.000Z",permalink:"/pages/a95d7d/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E8%AE%BE%E8%AE%A1%E7%83%AD%E9%97%A8%E5%BA%94%E7%94%A8/01.%E8%AE%BE%E8%AE%A1%20%E5%BE%AE%E4%BF%A1.html",relativePath:"实战系统设计/05.设计热门应用/01.设计 微信.md",key:"v-2ce111a7",path:"/pages/a95d7d/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"设计Twitter",frontmatter:{title:"设计Twitter",date:"2024-09-14T02:08:51.000Z",permalink:"/pages/90ad66/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E8%AE%BE%E8%AE%A1%E7%83%AD%E9%97%A8%E5%BA%94%E7%94%A8/02.%E8%AE%BE%E8%AE%A1Twitter.html",relativePath:"实战系统设计/05.设计热门应用/02.设计Twitter.md",key:"v-7923cebe",path:"/pages/90ad66/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"双写一致性",frontmatter:{title:"双写一致性",date:"2024-09-14T16:50:17.000Z",permalink:"/pages/def08a/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.html",relativePath:"实战系统设计/07.经典场景设计/01.双写一致性.md",key:"v-36c2e4ce",path:"/pages/def08a/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"引入缓存提高性能",slug:"引入缓存提高性能",normalizedTitle:"引入缓存提高性能",charIndex:136},{level:2,title:"缓存利用率和一致性问题",slug:"缓存利用率和一致性问题",normalizedTitle:"缓存利用率和一致性问题",charIndex:841},{level:3,title:"先来看第一个问题，如何提高缓存利用率？",slug:"先来看第一个问题-如何提高缓存利用率",normalizedTitle:"先来看第一个问题，如何提高缓存利用率？",charIndex:857},{level:2,title:"并发引发的一致性问题",slug:"并发引发的一致性问题",normalizedTitle:"并发引发的一致性问题",charIndex:1822},{level:2,title:"删除缓存可以保证一致性吗？",slug:"删除缓存可以保证一致性吗",normalizedTitle:"删除缓存可以保证一致性吗？",charIndex:2434},{level:2,title:"如何保证两步都执行成功？",slug:"如何保证两步都执行成功",normalizedTitle:"如何保证两步都执行成功？",charIndex:3376},{level:2,title:"主从库延迟和延迟双删问题",slug:"主从库延迟和延迟双删问题",normalizedTitle:"主从库延迟和延迟双删问题",charIndex:4973},{level:2,title:"可以做到强一致吗？",slug:"可以做到强一致吗",normalizedTitle:"可以做到强一致吗？",charIndex:6103},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6604},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:7140}],headersStr:"前言 引入缓存提高性能 缓存利用率和一致性问题 先来看第一个问题，如何提高缓存利用率？ 并发引发的一致性问题 删除缓存可以保证一致性吗？ 如何保证两步都执行成功？ 主从库延迟和延迟双删问题 可以做到强一致吗？ 总结 参考文献",content:"# 前言\n\n感觉这是一个很宏大的命题\n\n我个人觉得引入缓存之后，如果为了短时间的不一致性问题，选择让系统设计变得更加复杂的话，完全没必要\n\n可以将请求分流，要保证强一致的请求走数据库，能忍受不一致的请求走缓存\n\necho 带着你沿着场景渐进式的了解双写一致性问题\n\n\n# 引入缓存提高性能\n\n我们从最简单的场景开始讲起。\n\n如果你的业务处于起步阶段，流量非常小，那无论是读请求还是写请求，直接操作数据库即可，这时你的架构模型是这样的：\n\n\n\n但随着业务量的增长，你的项目请求量越来越大，这时如果每次都从数据库中读数据，那肯定会有性能问题。\n\n这个阶段通常的做法是，引入「缓存」来提高读性能，架构模型就变成了这样：\n\n\n\n当下优秀的缓存中间件，当属 Redis 莫属，它不仅性能非常高，还提供了很多友好的数据类型，可以很好地满足我们的业务需求。\n\n但引入缓存之后，你就会面临一个问题：之前数据只存在数据库中，现在要放到缓存中读取，具体要怎么存呢？\n\n有三种方案：\n\n * Cache Aside Pattern（旁路缓存模式）\n * Read/Write Through Pattern（读写穿透）\n * Write Behind Pattern（异步缓存写入）\n\n最简单直接的方案是「全量数据刷到缓存中」：\n\n * 数据库的数据，全量刷入缓存（不设置失效时间）\n * 写请求只更新数据库，不更新缓存\n * 启动一个定时任务，定时把数据库的数据，更新到缓存中\n\n\n\n这个方案的优点是，所有读请求都可以直接「命中」缓存，不需要再查数据库，性能非常高。\n\n但缺点也很明显，有 2 个问题：\n\n 1. 缓存利用率低：不经常访问的数据，还一直留在缓存中\n 2. 数据不一致：因为是「定时」刷新缓存，缓存和数据库存在不一致（取决于定时任务的执行频率）\n\n所以，这种方案一般更适合业务「体量小」，且对数据一致性要求不高的业务场景。\n\n那如果我们的业务体量很大，怎么解决这 2 个问题呢？\n\n\n# 缓存利用率和一致性问题\n\n\n# 先来看第一个问题，如何提高缓存利用率？\n\n想要缓存利用率「最大化」，我们很容易想到的方案是，缓存中只保留最近访问的「热数据」。但具体要怎么做呢？\n\n我们可以这样优化：\n\n * 写请求依旧只写数据库\n * 读请求先读缓存，如果缓存不存在，则从数据库读取，并重建缓存\n * 同时，写入缓存中的数据，都设置失效时间\n\n\n\n这样一来，缓存中不经常访问的数据，随着时间的推移，都会逐渐「过期」淘汰掉，最终缓存中保留的，都是经常被访问的「热数据」，缓存利用率得以最大化。\n\n再来看数据一致性问题。\n\n要想保证缓存和数据库「实时」一致，那就不能再用定时任务刷新缓存了。\n\n所以，当数据发生更新时，我们不仅要操作数据库，还要一并操作缓存。具体操作就是，修改一条数据时，不仅要更新数据库，也要连带缓存一起更新。\n\n但数据库和缓存都更新，又存在先后问题，那对应的方案就有 2 个：\n\n 1. 先更新缓存，后更新数据库\n 2. 先更新数据库，后更新缓存\n\n哪个方案更好呢？\n\n先不考虑并发问题，正常情况下，无论谁先谁后，都可以让两者保持一致，但现在我们需要重点考虑「异常」情况。\n\n因为操作分为两步，那么就很有可能存在「第一步成功、第二步失败」的情况发生。\n\n这 2 种方案我们一个个来分析。\n\n1) 先更新缓存，后更新数据库\n\n如果缓存更新成功了，但数据库更新失败，那么此时缓存中是最新值，但数据库中是「旧值」。\n\n虽然此时读请求可以命中缓存，拿到正确的值，但是，一旦缓存「失效」，就会从数据库中读取到「旧值」，重建缓存也是这个旧值。\n\n这时用户会发现自己之前修改的数据又「变回去」了，对业务造成影响。\n\n2) 先更新数据库，后更新缓存\n\n如果数据库更新成功了，但缓存更新失败，那么此时数据库中是最新值，缓存中是「旧值」。\n\n之后的读请求读到的都是旧数据，只有当缓存「失效」后，才能从数据库中得到正确的值。\n\n这时用户会发现，自己刚刚修改了数据，但却看不到变更，一段时间过后，数据才变更过来，对业务也会有影响。\n\n可见，无论谁先谁后，但凡后者发生异常，就会对业务造成影响。那怎么解决这个问题呢？\n\n别急，后面我会详细给出对应的解决方案。\n\n我们继续分析，除了操作失败问题，还有什么场景会影响数据一致性？\n\n这里我们还需要重点关注：并发问题。\n\n\n# 并发引发的一致性问题\n\n假设我们采用**「先更新数据库，再更新缓存」**的方案，并且两步都可以「成功执行」的前提下，如果存在并发，情况会是怎样的呢？\n\n有线程 A 和线程 B 两个线程，需要更新「同一条」数据，会发生这样的场景：\n\n 1. 线程 A 更新数据库（X = 1）\n 2. 线程 B 更新数据库（X = 2）\n 3. 线程 B 更新缓存（X = 2）\n 4. 线程 A 更新缓存（X = 1）\n\n最终 X 的值在缓存中是 1，在数据库中是 2，发生不一致。\n\n也就是说，A 虽然先于 B 发生，但 B 操作数据库和缓存的时间，却要比 A 的时间短，执行时序发生「错乱」，最终这条数据结果是不符合预期的。\n\n> 同样地，采用「先更新缓存，再更新数据库」的方案，也会有类似问题，这里不再详述。\n\n除此之外，我们从「缓存利用率」的角度来评估这个方案，也是不太推荐的。\n\n这是因为每次数据发生变更，都「无脑」更新缓存，但是缓存中的数据不一定会被「马上读取」，这就会导致缓存中可能存放了很多不常访问的数据，浪费缓存资源。\n\n而且很多情况下，写到缓存中的值，并不是与数据库中的值一一对应的，很有可能是先查询数据库，再经过一系列「计算」得出一个值，才把这个值才写到缓存中。\n\n由此可见，这种「更新数据库 + 更新缓存」的方案，不仅缓存利用率不高，还会造成机器性能的浪费。\n\n所以此时我们需要考虑另外一种方案：删除缓存。\n\n\n# 删除缓存可以保证一致性吗？\n\n删除缓存对应的方案也有 2 种：\n\n 1. 先删除缓存，后更新数据库\n 2. 先更新数据库，后删除缓存\n\n经过前面的分析我们已经得知，但凡「第二步」操作失败，都会导致数据不一致。\n\n这里我不再详述具体场景，你可以按照前面的思路推演一下，就可以看到依旧存在数据不一致的情况。\n\n这里我们重点来看「并发」问题。\n\n1) 先删除缓存，后更新数据库\n\n如果有 2 个线程要并发「读写」数据，可能会发生以下场景：\n\n 1. 线程 A 要更新 X = 2（原值 X = 1）\n 2. 线程 A 先删除缓存\n 3. 线程 B 读缓存，发现不存在，从数据库中读取到旧值（X = 1）\n 4. 线程 A 将新值写入数据库（X = 2）\n 5. 线程 B 将旧值写入缓存（X = 1）\n\n最终 X 的值在缓存中是 1（旧值），在数据库中是 2（新值），发生不一致。\n\n可见，先删除缓存，后更新数据库，当发生「读+写」并发时，还是存在数据不一致的情况。\n\n2) 先更新数据库，后删除缓存\n\n依旧是 2 个线程并发「读写」数据：\n\n 1. 缓存中 X 不存在（数据库 X = 1）\n 2. 线程 A 读取数据库，得到旧值（X = 1）\n 3. 线程 B 更新数据库（X = 2)\n 4. 线程 B 删除缓存\n 5. 线程 A 将旧值写入缓存（X = 1）\n\n最终 X 的值在缓存中是 1（旧值），在数据库中是 2（新值），也发生不一致。\n\n这种情况「理论」来说是可能发生的，但实际真的有可能发生吗？\n\n其实概率「很低」，这是因为它必须满足 3 个条件：\n\n 1. 缓存刚好已失效\n 2. 读请求 + 写请求并发\n 3. 更新数据库 + 删除缓存的时间（步骤 3-4），要比读数据库 + 写缓存时间短（步骤 2 和 5）\n\n仔细想一下，条件 3 发生的概率其实是非常低的。\n\n因为写数据库一般会先「加锁」，所以写数据库，通常是要比读数据库的时间更长的。\n\n这么来看，「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的。\n\n所以，我们应该采用这种方案，来操作数据库和缓存。\n\n好，解决了并发问题，我们继续来看前面遗留的，第二步执行「失败」导致数据不一致的问题。\n\n\n# 如何保证两步都执行成功？\n\n前面我们分析到，无论是更新缓存还是删除缓存，只要第二步发生失败，那么就会导致数据库和缓存不一致。\n\n前面我们分析到，无论是更新缓存还是删除缓存，只要第二步发生失败，那么就会导致数据库和缓存不一致。\n\n保证第二步成功执行，就是解决问题的关键。\n\n想一下，程序在执行过程中发生异常，最简单的解决办法是什么？\n\n答案是：重试。\n\n是的，其实这里我们也可以这样做。\n\n无论是先操作缓存，还是先操作数据库，但凡后者执行失败了，我们就可以发起重试，尽可能地去做「补偿」。\n\n那这是不是意味着，只要执行失败，我们「无脑重试」就可以了呢？\n\n答案是否定的。现实情况往往没有想的这么简单，失败后立即重试的问题在于：\n\n * 立即重试很大概率「还会失败」\n * 「重试次数」设置多少才合理？\n * 重试会一直「占用」这个线程资源，无法服务其它客户端请求\n\n看到了么，虽然我们想通过重试的方式解决问题，但这种「同步」重试的方案依旧不严谨。\n\n那更好的方案应该怎么做？\n\n答案是：异步重试。什么是异步重试？\n\n其实就是把重试请求写到「消息队列」中，然后由专门的消费者来重试，直到成功。\n\n或者更直接的做法，为了避免第二步执行失败，我们可以把操作缓存这一步，直接放到消息队列中，由消费者来操作缓存。\n\n到这里你可能会问，写消息队列也有可能会失败啊？而且，引入消息队列，这又增加了更多的维护成本，这样做值得吗？\n\n这个问题很好，但我们思考这样一个问题：如果在执行失败的线程中一直重试，还没等执行成功，此时如果项目「重启」了，那这次重试请求也就「丢失」了，那这条数据就一直不一致了。\n\n所以，这里我们必须把重试或第二步操作放到另一个「服务」中，这个服务用「消息队列」最为合适。这是因为消息队列的特性，正好符合我们的需求：\n\n * 消息队列保证可靠性：写到队列中的消息，成功消费之前不会丢失（重启项目也不担心）\n * 消息队列保证消息成功投递：下游从队列拉取消息，成功消费后才会删除消息，否则还会继续投递消息给消费者（符合我们重试的场景）\n\n至于写队列失败和消息队列的维护成本问题：\n\n * 写队列失败：操作缓存和写消息队列，「同时失败」的概率其实是很小的\n * 维护成本：我们项目中一般都会用到消息队列，维护成本并没有新增很多\n\n所以，引入消息队列来解决这个问题，是比较合适的。这时架构模型就变成了这样：\n\n那如果你确实不想在应用中去写消息队列，是否有更简单的方案，同时又可以保证一致性呢？\n\n方案还是有的，这就是近几年比较流行的解决方案：订阅数据库变更日志，再操作缓存。\n\n具体来讲就是，我们的业务应用在修改数据时，「只需」修改数据库，无需操作缓存。\n\n那什么时候操作缓存呢？这就和数据库的「变更日志」有关了。\n\n拿 MySQL 举例，当一条数据发生修改时，MySQL 就会产生一条变更日志（Binlog），我们可以订阅这个日志，拿到具体操作的数据，然后再根据这条数据，去删除对应的缓存。\n\n订阅变更日志，目前也有了比较成熟的开源中间件，例如阿里的 canal，使用这种方案的优点在于：\n\n * 无需考虑写消息队列失败情况：只要写 MySQL 成功，Binlog 肯定会有\n * 自动投递到下游队列：canal 自动把数据库变更日志「投递」给下游的消息队列\n\n当然，与此同时，我们需要投入精力去维护 canal 的高可用和稳定性。\n\n> 如果你有留意观察很多数据库的特性，就会发现其实很多数据库都逐渐开始提供「订阅变更日志」的功能了，相信不远的将来，我们就不用通过中间件来拉取日志，自己写程序就可以订阅变更日志了，这样可以进一步简化流程。\n\n至此，我们可以得出结论，想要保证数据库和缓存一致性，推荐采用「先更新数据库，再删除缓存」方案，并配合「消息队列」或「订阅变更日志」的方式来做。\n\n\n# 主从库延迟和延迟双删问题\n\n到这里，还有 2 个问题，是我们没有重点分析过的。\n\n第一个问题，还记得前面讲到的「先删除缓存，再更新数据库」方案，导致不一致的场景么？\n\n这里我再把例子拿过来让你复习一下：\n\n2 个线程要并发「读写」数据，可能会发生以下场景：\n\n 1. 线程 A 要更新 X = 2（原值 X = 1）\n 2. 线程 A 先删除缓存\n 3. 线程 B 读缓存，发现不存在，从数据库中读取到旧值（X = 1）\n 4. 线程 A 将新值写入数据库（X = 2）\n 5. 线程 B 将旧值写入缓存（X = 1）\n\n最终 X 的值在缓存中是 1（旧值），在数据库中是 2（新值），发生不一致。\n\n第二个问题：是关于「读写分离 + 主从复制延迟」情况下，缓存和数据库一致性的问题。\n\n在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」其实也会导致不一致：\n\n 1. 线程 A 更新主库 X = 2（原值 X = 1）\n 2. 线程 A 删除缓存\n 3. 线程 B 查询缓存，没有命中，查询「从库」得到旧值（从库 X = 1）\n 4. 从库「同步」完成（主从库 X = 2）\n 5. 线程 B 将「旧值」写入缓存（X = 1）\n\n最终 X 的值在缓存中是 1（旧值），在主从库中是 2（新值），也发生不一致。\n\n看到了么？这 2 个问题的核心在于：缓存都被回种了「旧值」。\n\n那怎么解决这类问题呢？\n\n最有效的办法就是，把缓存删掉。\n\n但是，不能立即删，而是需要「延迟删」，这就是业界给出的方案：缓存延迟双删策略。\n\n按照延时双删策略，这 2 个问题的解决方案是这样的：\n\n解决第一个问题：在线程 A 删除缓存、更新完数据库之后，先「休眠一会」，再「删除」一次缓存。\n\n解决第二个问题：线程 A 可以生成一条「延时消息」，写到消息队列中，消费者延时「删除」缓存。\n\n这两个方案的目的，都是为了把缓存清掉，这样一来，下次就可以从数据库读取到最新值，写入缓存。\n\n但问题来了，这个「延迟删除」缓存，延迟时间到底设置要多久呢？\n\n * 问题1：延迟时间要大于「主从复制」的延迟时间\n * 问题2：延迟时间要大于线程 B 读取数据库 + 写入缓存的时间\n\n但是，这个时间在分布式和高并发场景下，其实是很难评估的\n\n很多时候，我们都是凭借经验大致估算这个延迟时间，例如延迟 1-5s，只能尽可能地降低不一致的概率。\n\n所以你看，采用这种方案，也只是尽可能保证一致性而已，极端情况下，还是有可能发生不一致。\n\n所以实际使用中，我还是建议你采用「先更新数据库，再删除缓存」的方案，同时，要尽可能地保证「主从复制」不要有太大延迟，降低出问题的概率。\n\n\n# 可以做到强一致吗？\n\n看到这里你可能会想，这些方案还是不够完美，我就想让缓存和数据库「强一致」，到底能不能做到呢？\n\n其实很难。\n\n要想做到强一致，最常见的方案是 2PC、3PC、Paxos、Raft 这类一致性协议，但它们的性能往往比较差，而且这些方案也比较复杂，还要考虑各种容错问题。\n\n相反，这时我们换个角度思考一下，我们引入缓存的目的是什么？\n\n没错，性能。\n\n一旦我们决定使用缓存，那必然要面临一致性问题。性能和一致性就像天平的两端，无法做到都满足要求。\n\n而且，就拿我们前面讲到的方案来说，当操作数据库和缓存完成之前，只要有其它请求可以进来，都有可能查到「中间状态」的数据。\n\n所以如果非要追求强一致，那必须要求所有更新操作完成之前期间，不能有「任何请求」进来。\n\n虽然我们可以通过加「分布锁」的方式来实现，但我们要付出的代价，很可能会超过引入缓存带来的性能提升。\n\n所以，既然决定使用缓存，就必须容忍「一致性」问题，我们只能尽可能地去降低问题出现的概率。\n\n同时我们也要知道，缓存都是有「失效时间」的，就算在这期间存在短期不一致，我们依旧有失效时间来兜底，这样也能达到最终一致。\n\n\n# 总结\n\n好了，总结一下这篇文章的重点。\n\n 1. 想要提高应用的性能，可以引入「缓存」来解决\n 2. 引入缓存后，需要考虑缓存和数据库一致性问题，可选的方案有：「更新数据库 + 更新缓存」、「更新数据库 + 删除缓存」\n 3. 更新数据库 + 更新缓存方案，在「并发」场景下无法保证缓存和数据一致性，且存在「缓存资源浪费」和「机器性能浪费」的情况发生\n 4. 在更新数据库 + 删除缓存的方案中，「先删除缓存，再更新数据库」在「并发」场景下依旧有数据不一致问题，解决方案是「延迟双删」，但这个延迟时间很难评估，所以推荐用「先更新数据库，再删除缓存」的方案\n 5. 在「先更新数据库，再删除缓存」方案下，为了保证两步都成功执行，需配合「消息队列」或「订阅变更日志」的方案来做，本质是通过「重试」的方式保证数据一致性\n 6. 在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」也会导致缓存和数据库不一致，缓解此问题的方案是「延迟双删」，凭借经验发送「延迟消息」到队列中，延迟删除缓存，同时也要控制主从库延迟，尽可能降低不一致发生的概率\n 7. 如果要实现强一致性可以采用的方案是 2PC、3PC、Paxos、Raft 这类一致性协议，或者使用分布式锁\n\n\n# 参考文献\n\n缓存和数据库一致性问题，看这篇就够了 - 水滴与银弹",normalizedContent:"# 前言\n\n感觉这是一个很宏大的命题\n\n我个人觉得引入缓存之后，如果为了短时间的不一致性问题，选择让系统设计变得更加复杂的话，完全没必要\n\n可以将请求分流，要保证强一致的请求走数据库，能忍受不一致的请求走缓存\n\necho 带着你沿着场景渐进式的了解双写一致性问题\n\n\n# 引入缓存提高性能\n\n我们从最简单的场景开始讲起。\n\n如果你的业务处于起步阶段，流量非常小，那无论是读请求还是写请求，直接操作数据库即可，这时你的架构模型是这样的：\n\n\n\n但随着业务量的增长，你的项目请求量越来越大，这时如果每次都从数据库中读数据，那肯定会有性能问题。\n\n这个阶段通常的做法是，引入「缓存」来提高读性能，架构模型就变成了这样：\n\n\n\n当下优秀的缓存中间件，当属 redis 莫属，它不仅性能非常高，还提供了很多友好的数据类型，可以很好地满足我们的业务需求。\n\n但引入缓存之后，你就会面临一个问题：之前数据只存在数据库中，现在要放到缓存中读取，具体要怎么存呢？\n\n有三种方案：\n\n * cache aside pattern（旁路缓存模式）\n * read/write through pattern（读写穿透）\n * write behind pattern（异步缓存写入）\n\n最简单直接的方案是「全量数据刷到缓存中」：\n\n * 数据库的数据，全量刷入缓存（不设置失效时间）\n * 写请求只更新数据库，不更新缓存\n * 启动一个定时任务，定时把数据库的数据，更新到缓存中\n\n\n\n这个方案的优点是，所有读请求都可以直接「命中」缓存，不需要再查数据库，性能非常高。\n\n但缺点也很明显，有 2 个问题：\n\n 1. 缓存利用率低：不经常访问的数据，还一直留在缓存中\n 2. 数据不一致：因为是「定时」刷新缓存，缓存和数据库存在不一致（取决于定时任务的执行频率）\n\n所以，这种方案一般更适合业务「体量小」，且对数据一致性要求不高的业务场景。\n\n那如果我们的业务体量很大，怎么解决这 2 个问题呢？\n\n\n# 缓存利用率和一致性问题\n\n\n# 先来看第一个问题，如何提高缓存利用率？\n\n想要缓存利用率「最大化」，我们很容易想到的方案是，缓存中只保留最近访问的「热数据」。但具体要怎么做呢？\n\n我们可以这样优化：\n\n * 写请求依旧只写数据库\n * 读请求先读缓存，如果缓存不存在，则从数据库读取，并重建缓存\n * 同时，写入缓存中的数据，都设置失效时间\n\n\n\n这样一来，缓存中不经常访问的数据，随着时间的推移，都会逐渐「过期」淘汰掉，最终缓存中保留的，都是经常被访问的「热数据」，缓存利用率得以最大化。\n\n再来看数据一致性问题。\n\n要想保证缓存和数据库「实时」一致，那就不能再用定时任务刷新缓存了。\n\n所以，当数据发生更新时，我们不仅要操作数据库，还要一并操作缓存。具体操作就是，修改一条数据时，不仅要更新数据库，也要连带缓存一起更新。\n\n但数据库和缓存都更新，又存在先后问题，那对应的方案就有 2 个：\n\n 1. 先更新缓存，后更新数据库\n 2. 先更新数据库，后更新缓存\n\n哪个方案更好呢？\n\n先不考虑并发问题，正常情况下，无论谁先谁后，都可以让两者保持一致，但现在我们需要重点考虑「异常」情况。\n\n因为操作分为两步，那么就很有可能存在「第一步成功、第二步失败」的情况发生。\n\n这 2 种方案我们一个个来分析。\n\n1) 先更新缓存，后更新数据库\n\n如果缓存更新成功了，但数据库更新失败，那么此时缓存中是最新值，但数据库中是「旧值」。\n\n虽然此时读请求可以命中缓存，拿到正确的值，但是，一旦缓存「失效」，就会从数据库中读取到「旧值」，重建缓存也是这个旧值。\n\n这时用户会发现自己之前修改的数据又「变回去」了，对业务造成影响。\n\n2) 先更新数据库，后更新缓存\n\n如果数据库更新成功了，但缓存更新失败，那么此时数据库中是最新值，缓存中是「旧值」。\n\n之后的读请求读到的都是旧数据，只有当缓存「失效」后，才能从数据库中得到正确的值。\n\n这时用户会发现，自己刚刚修改了数据，但却看不到变更，一段时间过后，数据才变更过来，对业务也会有影响。\n\n可见，无论谁先谁后，但凡后者发生异常，就会对业务造成影响。那怎么解决这个问题呢？\n\n别急，后面我会详细给出对应的解决方案。\n\n我们继续分析，除了操作失败问题，还有什么场景会影响数据一致性？\n\n这里我们还需要重点关注：并发问题。\n\n\n# 并发引发的一致性问题\n\n假设我们采用**「先更新数据库，再更新缓存」**的方案，并且两步都可以「成功执行」的前提下，如果存在并发，情况会是怎样的呢？\n\n有线程 a 和线程 b 两个线程，需要更新「同一条」数据，会发生这样的场景：\n\n 1. 线程 a 更新数据库（x = 1）\n 2. 线程 b 更新数据库（x = 2）\n 3. 线程 b 更新缓存（x = 2）\n 4. 线程 a 更新缓存（x = 1）\n\n最终 x 的值在缓存中是 1，在数据库中是 2，发生不一致。\n\n也就是说，a 虽然先于 b 发生，但 b 操作数据库和缓存的时间，却要比 a 的时间短，执行时序发生「错乱」，最终这条数据结果是不符合预期的。\n\n> 同样地，采用「先更新缓存，再更新数据库」的方案，也会有类似问题，这里不再详述。\n\n除此之外，我们从「缓存利用率」的角度来评估这个方案，也是不太推荐的。\n\n这是因为每次数据发生变更，都「无脑」更新缓存，但是缓存中的数据不一定会被「马上读取」，这就会导致缓存中可能存放了很多不常访问的数据，浪费缓存资源。\n\n而且很多情况下，写到缓存中的值，并不是与数据库中的值一一对应的，很有可能是先查询数据库，再经过一系列「计算」得出一个值，才把这个值才写到缓存中。\n\n由此可见，这种「更新数据库 + 更新缓存」的方案，不仅缓存利用率不高，还会造成机器性能的浪费。\n\n所以此时我们需要考虑另外一种方案：删除缓存。\n\n\n# 删除缓存可以保证一致性吗？\n\n删除缓存对应的方案也有 2 种：\n\n 1. 先删除缓存，后更新数据库\n 2. 先更新数据库，后删除缓存\n\n经过前面的分析我们已经得知，但凡「第二步」操作失败，都会导致数据不一致。\n\n这里我不再详述具体场景，你可以按照前面的思路推演一下，就可以看到依旧存在数据不一致的情况。\n\n这里我们重点来看「并发」问题。\n\n1) 先删除缓存，后更新数据库\n\n如果有 2 个线程要并发「读写」数据，可能会发生以下场景：\n\n 1. 线程 a 要更新 x = 2（原值 x = 1）\n 2. 线程 a 先删除缓存\n 3. 线程 b 读缓存，发现不存在，从数据库中读取到旧值（x = 1）\n 4. 线程 a 将新值写入数据库（x = 2）\n 5. 线程 b 将旧值写入缓存（x = 1）\n\n最终 x 的值在缓存中是 1（旧值），在数据库中是 2（新值），发生不一致。\n\n可见，先删除缓存，后更新数据库，当发生「读+写」并发时，还是存在数据不一致的情况。\n\n2) 先更新数据库，后删除缓存\n\n依旧是 2 个线程并发「读写」数据：\n\n 1. 缓存中 x 不存在（数据库 x = 1）\n 2. 线程 a 读取数据库，得到旧值（x = 1）\n 3. 线程 b 更新数据库（x = 2)\n 4. 线程 b 删除缓存\n 5. 线程 a 将旧值写入缓存（x = 1）\n\n最终 x 的值在缓存中是 1（旧值），在数据库中是 2（新值），也发生不一致。\n\n这种情况「理论」来说是可能发生的，但实际真的有可能发生吗？\n\n其实概率「很低」，这是因为它必须满足 3 个条件：\n\n 1. 缓存刚好已失效\n 2. 读请求 + 写请求并发\n 3. 更新数据库 + 删除缓存的时间（步骤 3-4），要比读数据库 + 写缓存时间短（步骤 2 和 5）\n\n仔细想一下，条件 3 发生的概率其实是非常低的。\n\n因为写数据库一般会先「加锁」，所以写数据库，通常是要比读数据库的时间更长的。\n\n这么来看，「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的。\n\n所以，我们应该采用这种方案，来操作数据库和缓存。\n\n好，解决了并发问题，我们继续来看前面遗留的，第二步执行「失败」导致数据不一致的问题。\n\n\n# 如何保证两步都执行成功？\n\n前面我们分析到，无论是更新缓存还是删除缓存，只要第二步发生失败，那么就会导致数据库和缓存不一致。\n\n前面我们分析到，无论是更新缓存还是删除缓存，只要第二步发生失败，那么就会导致数据库和缓存不一致。\n\n保证第二步成功执行，就是解决问题的关键。\n\n想一下，程序在执行过程中发生异常，最简单的解决办法是什么？\n\n答案是：重试。\n\n是的，其实这里我们也可以这样做。\n\n无论是先操作缓存，还是先操作数据库，但凡后者执行失败了，我们就可以发起重试，尽可能地去做「补偿」。\n\n那这是不是意味着，只要执行失败，我们「无脑重试」就可以了呢？\n\n答案是否定的。现实情况往往没有想的这么简单，失败后立即重试的问题在于：\n\n * 立即重试很大概率「还会失败」\n * 「重试次数」设置多少才合理？\n * 重试会一直「占用」这个线程资源，无法服务其它客户端请求\n\n看到了么，虽然我们想通过重试的方式解决问题，但这种「同步」重试的方案依旧不严谨。\n\n那更好的方案应该怎么做？\n\n答案是：异步重试。什么是异步重试？\n\n其实就是把重试请求写到「消息队列」中，然后由专门的消费者来重试，直到成功。\n\n或者更直接的做法，为了避免第二步执行失败，我们可以把操作缓存这一步，直接放到消息队列中，由消费者来操作缓存。\n\n到这里你可能会问，写消息队列也有可能会失败啊？而且，引入消息队列，这又增加了更多的维护成本，这样做值得吗？\n\n这个问题很好，但我们思考这样一个问题：如果在执行失败的线程中一直重试，还没等执行成功，此时如果项目「重启」了，那这次重试请求也就「丢失」了，那这条数据就一直不一致了。\n\n所以，这里我们必须把重试或第二步操作放到另一个「服务」中，这个服务用「消息队列」最为合适。这是因为消息队列的特性，正好符合我们的需求：\n\n * 消息队列保证可靠性：写到队列中的消息，成功消费之前不会丢失（重启项目也不担心）\n * 消息队列保证消息成功投递：下游从队列拉取消息，成功消费后才会删除消息，否则还会继续投递消息给消费者（符合我们重试的场景）\n\n至于写队列失败和消息队列的维护成本问题：\n\n * 写队列失败：操作缓存和写消息队列，「同时失败」的概率其实是很小的\n * 维护成本：我们项目中一般都会用到消息队列，维护成本并没有新增很多\n\n所以，引入消息队列来解决这个问题，是比较合适的。这时架构模型就变成了这样：\n\n那如果你确实不想在应用中去写消息队列，是否有更简单的方案，同时又可以保证一致性呢？\n\n方案还是有的，这就是近几年比较流行的解决方案：订阅数据库变更日志，再操作缓存。\n\n具体来讲就是，我们的业务应用在修改数据时，「只需」修改数据库，无需操作缓存。\n\n那什么时候操作缓存呢？这就和数据库的「变更日志」有关了。\n\n拿 mysql 举例，当一条数据发生修改时，mysql 就会产生一条变更日志（binlog），我们可以订阅这个日志，拿到具体操作的数据，然后再根据这条数据，去删除对应的缓存。\n\n订阅变更日志，目前也有了比较成熟的开源中间件，例如阿里的 canal，使用这种方案的优点在于：\n\n * 无需考虑写消息队列失败情况：只要写 mysql 成功，binlog 肯定会有\n * 自动投递到下游队列：canal 自动把数据库变更日志「投递」给下游的消息队列\n\n当然，与此同时，我们需要投入精力去维护 canal 的高可用和稳定性。\n\n> 如果你有留意观察很多数据库的特性，就会发现其实很多数据库都逐渐开始提供「订阅变更日志」的功能了，相信不远的将来，我们就不用通过中间件来拉取日志，自己写程序就可以订阅变更日志了，这样可以进一步简化流程。\n\n至此，我们可以得出结论，想要保证数据库和缓存一致性，推荐采用「先更新数据库，再删除缓存」方案，并配合「消息队列」或「订阅变更日志」的方式来做。\n\n\n# 主从库延迟和延迟双删问题\n\n到这里，还有 2 个问题，是我们没有重点分析过的。\n\n第一个问题，还记得前面讲到的「先删除缓存，再更新数据库」方案，导致不一致的场景么？\n\n这里我再把例子拿过来让你复习一下：\n\n2 个线程要并发「读写」数据，可能会发生以下场景：\n\n 1. 线程 a 要更新 x = 2（原值 x = 1）\n 2. 线程 a 先删除缓存\n 3. 线程 b 读缓存，发现不存在，从数据库中读取到旧值（x = 1）\n 4. 线程 a 将新值写入数据库（x = 2）\n 5. 线程 b 将旧值写入缓存（x = 1）\n\n最终 x 的值在缓存中是 1（旧值），在数据库中是 2（新值），发生不一致。\n\n第二个问题：是关于「读写分离 + 主从复制延迟」情况下，缓存和数据库一致性的问题。\n\n在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」其实也会导致不一致：\n\n 1. 线程 a 更新主库 x = 2（原值 x = 1）\n 2. 线程 a 删除缓存\n 3. 线程 b 查询缓存，没有命中，查询「从库」得到旧值（从库 x = 1）\n 4. 从库「同步」完成（主从库 x = 2）\n 5. 线程 b 将「旧值」写入缓存（x = 1）\n\n最终 x 的值在缓存中是 1（旧值），在主从库中是 2（新值），也发生不一致。\n\n看到了么？这 2 个问题的核心在于：缓存都被回种了「旧值」。\n\n那怎么解决这类问题呢？\n\n最有效的办法就是，把缓存删掉。\n\n但是，不能立即删，而是需要「延迟删」，这就是业界给出的方案：缓存延迟双删策略。\n\n按照延时双删策略，这 2 个问题的解决方案是这样的：\n\n解决第一个问题：在线程 a 删除缓存、更新完数据库之后，先「休眠一会」，再「删除」一次缓存。\n\n解决第二个问题：线程 a 可以生成一条「延时消息」，写到消息队列中，消费者延时「删除」缓存。\n\n这两个方案的目的，都是为了把缓存清掉，这样一来，下次就可以从数据库读取到最新值，写入缓存。\n\n但问题来了，这个「延迟删除」缓存，延迟时间到底设置要多久呢？\n\n * 问题1：延迟时间要大于「主从复制」的延迟时间\n * 问题2：延迟时间要大于线程 b 读取数据库 + 写入缓存的时间\n\n但是，这个时间在分布式和高并发场景下，其实是很难评估的\n\n很多时候，我们都是凭借经验大致估算这个延迟时间，例如延迟 1-5s，只能尽可能地降低不一致的概率。\n\n所以你看，采用这种方案，也只是尽可能保证一致性而已，极端情况下，还是有可能发生不一致。\n\n所以实际使用中，我还是建议你采用「先更新数据库，再删除缓存」的方案，同时，要尽可能地保证「主从复制」不要有太大延迟，降低出问题的概率。\n\n\n# 可以做到强一致吗？\n\n看到这里你可能会想，这些方案还是不够完美，我就想让缓存和数据库「强一致」，到底能不能做到呢？\n\n其实很难。\n\n要想做到强一致，最常见的方案是 2pc、3pc、paxos、raft 这类一致性协议，但它们的性能往往比较差，而且这些方案也比较复杂，还要考虑各种容错问题。\n\n相反，这时我们换个角度思考一下，我们引入缓存的目的是什么？\n\n没错，性能。\n\n一旦我们决定使用缓存，那必然要面临一致性问题。性能和一致性就像天平的两端，无法做到都满足要求。\n\n而且，就拿我们前面讲到的方案来说，当操作数据库和缓存完成之前，只要有其它请求可以进来，都有可能查到「中间状态」的数据。\n\n所以如果非要追求强一致，那必须要求所有更新操作完成之前期间，不能有「任何请求」进来。\n\n虽然我们可以通过加「分布锁」的方式来实现，但我们要付出的代价，很可能会超过引入缓存带来的性能提升。\n\n所以，既然决定使用缓存，就必须容忍「一致性」问题，我们只能尽可能地去降低问题出现的概率。\n\n同时我们也要知道，缓存都是有「失效时间」的，就算在这期间存在短期不一致，我们依旧有失效时间来兜底，这样也能达到最终一致。\n\n\n# 总结\n\n好了，总结一下这篇文章的重点。\n\n 1. 想要提高应用的性能，可以引入「缓存」来解决\n 2. 引入缓存后，需要考虑缓存和数据库一致性问题，可选的方案有：「更新数据库 + 更新缓存」、「更新数据库 + 删除缓存」\n 3. 更新数据库 + 更新缓存方案，在「并发」场景下无法保证缓存和数据一致性，且存在「缓存资源浪费」和「机器性能浪费」的情况发生\n 4. 在更新数据库 + 删除缓存的方案中，「先删除缓存，再更新数据库」在「并发」场景下依旧有数据不一致问题，解决方案是「延迟双删」，但这个延迟时间很难评估，所以推荐用「先更新数据库，再删除缓存」的方案\n 5. 在「先更新数据库，再删除缓存」方案下，为了保证两步都成功执行，需配合「消息队列」或「订阅变更日志」的方案来做，本质是通过「重试」的方式保证数据一致性\n 6. 在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」也会导致缓存和数据库不一致，缓解此问题的方案是「延迟双删」，凭借经验发送「延迟消息」到队列中，延迟删除缓存，同时也要控制主从库延迟，尽可能降低不一致发生的概率\n 7. 如果要实现强一致性可以采用的方案是 2pc、3pc、paxos、raft 这类一致性协议，或者使用分布式锁\n\n\n# 参考文献\n\n缓存和数据库一致性问题，看这篇就够了 - 水滴与银弹",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"哨兵",frontmatter:{title:"哨兵",date:"2024-09-16T03:24:20.000Z",permalink:"/pages/af8752/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E4%BA%94%E3%80%81%E9%9B%86%E7%BE%A4/30.%E5%93%A8%E5%85%B5.html",relativePath:"Redis 系统设计/05.五、集群/30.哨兵.md",key:"v-b090e128",path:"/pages/af8752/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 11:29:27",lastUpdatedTimestamp:1726658967e3},{title:"缓存穿透",frontmatter:{title:"缓存穿透",date:"2024-09-14T16:50:37.000Z",permalink:"/pages/1e9e8e/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/02.%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F.html",relativePath:"实战系统设计/07.经典场景设计/02.缓存穿透.md",key:"v-5ef7030e",path:"/pages/1e9e8e/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"什么是缓存穿透",slug:"什么是缓存穿透",normalizedTitle:"什么是缓存穿透",charIndex:149},{level:2,title:"缓存穿透如何解决",slug:"缓存穿透如何解决",normalizedTitle:"缓存穿透如何解决",charIndex:606},{level:3,title:"设置空值",slug:"设置空值",normalizedTitle:"设置空值",charIndex:908},{level:3,title:"布隆过滤器",slug:"布隆过滤器",normalizedTitle:"布隆过滤器",charIndex:96},{level:3,title:"接口限流",slug:"接口限流",normalizedTitle:"接口限流",charIndex:1390}],headersStr:"前言 什么是缓存穿透 缓存穿透如何解决 设置空值 布隆过滤器 接口限流",content:"# 前言\n\n先来一个问题：如何解决复杂 where 下的缓存穿透？\n\n * 分为正常请求和非法请求\n * 正常请求：给 where 进行 hash，然后存 redis，合理设置过期时间，并利用布隆过滤器过滤\n * 非法请求：记录查询 null 的次数，如果太大，进行封禁，采用指数退避算法\n\n\n# 什么是缓存穿透\n\n缓存穿透最直白的意思就是，我们的业务系统在接收到请求时在缓存中并没有查到数据，从而穿透到了后端数据库里面查数据的过程。\n\n当然，既然使用了缓存，肯定会难免有穿透的发生，正常的少量穿透是对我们业务来说是不会造成任何影响的，因为:\n\n * 毕竟我们的缓存容量有限，不可能去缓存所有数据，当面临较大请求时，查询到未被缓存的数据时，就会发生穿透。\n * 互联网业务的数据访问模型一般是遵循“二八”原则的，即 20% 的数据为热点数据，80% 的数据是非热点不被常访问的数据。\n\n现在既然我们的缓存容量有限，然后 20% 的数据为热点数据，也就是说，我们可以利用有限的容量去缓存那 20% 的数据，其实就是可以保护我们的后端系统的，至于80%非热点不常用的数据发生穿透了，是我们能够接受的。\n\n那究竟什么的缓存穿透会影响到我们的系统呢？是大量的穿透请求超过了我们后端系统的承受范围，比如恶意的穿透攻击，这样的穿透就很有可能把我们的系统给干崩溃。接下来，我们就来基于相关应用场景来解决这种缓存穿透。\n\n\n# 缓存穿透如何解决\n\n在我们APP的在线搜索相关系统里面，有个产品product 1 并没有在数据库中进行存储，现在通过cache aside pattern 策略（缓存读写策略，开发必备）查这个product 1 。\n\n那查询一个数据库中本身就没有的数据后面会怎样呢？依照cache aside 策略，读取时，首先会读取缓存，没读到数据就会穿透到读数据库，现在数据库也没有，也就没有数据写回缓存。那么，再来个请求依然如此，更多的请求来还是一样，这样的缓存就没意义了。\n\n通过上面场景我们可以看到，这样的系统面临非正常的穿透是会崩溃掉的，那我们该怎么去解决呢？一般我们对此有两种方案，都是有用的：\n\n * 设置空值\n * 布隆过滤器\n\n\n# 设置空值\n\n通过上面场景我们知道，当有大量恶意的穿透请求到数据库，就会给我们系统带来灾难。\n\n所以，当我们请求数据中没有数据或者因为代码bug带来的异常造成的数据为空，这个时候我们就可以回写一个空值null到缓存中。同时，我们还要给这个null值设置过期时间，因为这个空值不具有实际业务性，而且还占用空间。\n\n可见设置空值是可以阻挡大量穿透请求的，但是如果有大量的获取并不存在数据的穿透请求的话例如恶意攻击，则会浪费缓存空间，如果这种null值过量的话，还会淘汰掉本身缓存存在的数据，这就会使我们的缓存命中率下降。\n\n**生产建议，**在使用设置空值方案时，我们要做好监控，预防缓存空间被过多null值占领造成的缓存空间浪费，如果这种数据量太大，就不再建议使用，那就使用另一种方案，即布隆过滤器。\n\n\n# 布隆过滤器\n\n生产建议：\n\n * 采用多个hash 算法计算hash 值，这样可以减少误判的几率。\n * 布隆过滤器会消耗一定内存空间，根据业务场景进行评估需要多大内存，最后依据公司资源以及成本，看是否能够接受。\n\n\n# 接口限流\n\n根据用户或者 IP 对接口进行限流，对于异常频繁的访问行为，还可以采取黑名单机制，例如将异常 IP 列入黑名单。\n\n后面提到的缓存击穿和雪崩都可以配合接口限流来解决，毕竟这些问题的关键都是有很多请求落到了数据库上造成数据库压力过大。",normalizedContent:"# 前言\n\n先来一个问题：如何解决复杂 where 下的缓存穿透？\n\n * 分为正常请求和非法请求\n * 正常请求：给 where 进行 hash，然后存 redis，合理设置过期时间，并利用布隆过滤器过滤\n * 非法请求：记录查询 null 的次数，如果太大，进行封禁，采用指数退避算法\n\n\n# 什么是缓存穿透\n\n缓存穿透最直白的意思就是，我们的业务系统在接收到请求时在缓存中并没有查到数据，从而穿透到了后端数据库里面查数据的过程。\n\n当然，既然使用了缓存，肯定会难免有穿透的发生，正常的少量穿透是对我们业务来说是不会造成任何影响的，因为:\n\n * 毕竟我们的缓存容量有限，不可能去缓存所有数据，当面临较大请求时，查询到未被缓存的数据时，就会发生穿透。\n * 互联网业务的数据访问模型一般是遵循“二八”原则的，即 20% 的数据为热点数据，80% 的数据是非热点不被常访问的数据。\n\n现在既然我们的缓存容量有限，然后 20% 的数据为热点数据，也就是说，我们可以利用有限的容量去缓存那 20% 的数据，其实就是可以保护我们的后端系统的，至于80%非热点不常用的数据发生穿透了，是我们能够接受的。\n\n那究竟什么的缓存穿透会影响到我们的系统呢？是大量的穿透请求超过了我们后端系统的承受范围，比如恶意的穿透攻击，这样的穿透就很有可能把我们的系统给干崩溃。接下来，我们就来基于相关应用场景来解决这种缓存穿透。\n\n\n# 缓存穿透如何解决\n\n在我们app的在线搜索相关系统里面，有个产品product 1 并没有在数据库中进行存储，现在通过cache aside pattern 策略（缓存读写策略，开发必备）查这个product 1 。\n\n那查询一个数据库中本身就没有的数据后面会怎样呢？依照cache aside 策略，读取时，首先会读取缓存，没读到数据就会穿透到读数据库，现在数据库也没有，也就没有数据写回缓存。那么，再来个请求依然如此，更多的请求来还是一样，这样的缓存就没意义了。\n\n通过上面场景我们可以看到，这样的系统面临非正常的穿透是会崩溃掉的，那我们该怎么去解决呢？一般我们对此有两种方案，都是有用的：\n\n * 设置空值\n * 布隆过滤器\n\n\n# 设置空值\n\n通过上面场景我们知道，当有大量恶意的穿透请求到数据库，就会给我们系统带来灾难。\n\n所以，当我们请求数据中没有数据或者因为代码bug带来的异常造成的数据为空，这个时候我们就可以回写一个空值null到缓存中。同时，我们还要给这个null值设置过期时间，因为这个空值不具有实际业务性，而且还占用空间。\n\n可见设置空值是可以阻挡大量穿透请求的，但是如果有大量的获取并不存在数据的穿透请求的话例如恶意攻击，则会浪费缓存空间，如果这种null值过量的话，还会淘汰掉本身缓存存在的数据，这就会使我们的缓存命中率下降。\n\n**生产建议，**在使用设置空值方案时，我们要做好监控，预防缓存空间被过多null值占领造成的缓存空间浪费，如果这种数据量太大，就不再建议使用，那就使用另一种方案，即布隆过滤器。\n\n\n# 布隆过滤器\n\n生产建议：\n\n * 采用多个hash 算法计算hash 值，这样可以减少误判的几率。\n * 布隆过滤器会消耗一定内存空间，根据业务场景进行评估需要多大内存，最后依据公司资源以及成本，看是否能够接受。\n\n\n# 接口限流\n\n根据用户或者 ip 对接口进行限流，对于异常频繁的访问行为，还可以采取黑名单机制，例如将异常 ip 列入黑名单。\n\n后面提到的缓存击穿和雪崩都可以配合接口限流来解决，毕竟这些问题的关键都是有很多请求落到了数据库上造成数据库压力过大。",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"缓存击穿",frontmatter:{title:"缓存击穿",date:"2024-09-14T16:50:56.000Z",permalink:"/pages/1d96b2/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/03.%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF.html",relativePath:"实战系统设计/07.经典场景设计/03.缓存击穿.md",key:"v-bba36134",path:"/pages/1d96b2/",headers:[{level:2,title:"前言：什么是缓存击穿",slug:"前言-什么是缓存击穿",normalizedTitle:"前言：什么是缓存击穿",charIndex:2},{level:2,title:"有哪些解决办法",slug:"有哪些解决办法",normalizedTitle:"有哪些解决办法",charIndex:209},{level:2,title:"双检锁解决缓存击穿",slug:"双检锁解决缓存击穿",normalizedTitle:"双检锁解决缓存击穿",charIndex:367}],headersStr:"前言：什么是缓存击穿 有哪些解决办法 双检锁解决缓存击穿",content:'# 前言：什么是缓存击穿\n\n缓存击穿中，请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 。这就可能会导致瞬时大量的请求直接打到了数据库上，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了。\n\n\n\n举个例子：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力\n\n\n# 有哪些解决办法\n\n 1. 永不过期（不推荐）：设置热点数据永不过期或者过期时间比较长。\n 2. 提前预热（推荐）：针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期。\n 3. 加锁（看情况）：在缓存失效后，通过设置互斥锁确保只有一个请求去查询数据库并更新缓存。\n\n\n# 双检锁解决缓存击穿\n\n> 单例模式的一种实现，双重检测，其中的一层检测是为了提高效率。由于项目中采用了多线程，所以在第一个线程没有从缓存中获取到数据之后，有可能其他线程已经完成了读取数据库写入缓存的操作，也就是说，第一个线程再次得到时间片的时候，就没有必要访问数据库获取数据了。第二层检测是为了避免额外的访库操作。\n\npublic  Student getStudentById(Integer id) {\n    redisTemplate.setKeySerializer(new StringRedisSerializer());\n    //查询缓存\n    Student student = (Student) redisTemplate.opsForValue().get("studentKey");\n    //判断缓存是否为空\n    if (null == student) {\n\n        //双重检测锁实现\n        synchronized (this) {\n\n            student = (Student) redisTemplate.opsForValue().get("studentKey");\n\n            if (null == student) {\n                System.out.println("查询了数据库......");\n                //查询数据库\n                student = studentMapper.selectByPrimaryKey(id);\n                //放入缓存\n                redisTemplate.opsForValue().set("studentKey", student);\n            }\n        }\n    } else {\n        System.out.println("查询了缓存......");\n    }\n    return student;\n}\n',normalizedContent:'# 前言：什么是缓存击穿\n\n缓存击穿中，请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 。这就可能会导致瞬时大量的请求直接打到了数据库上，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了。\n\n\n\n举个例子：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力\n\n\n# 有哪些解决办法\n\n 1. 永不过期（不推荐）：设置热点数据永不过期或者过期时间比较长。\n 2. 提前预热（推荐）：针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期。\n 3. 加锁（看情况）：在缓存失效后，通过设置互斥锁确保只有一个请求去查询数据库并更新缓存。\n\n\n# 双检锁解决缓存击穿\n\n> 单例模式的一种实现，双重检测，其中的一层检测是为了提高效率。由于项目中采用了多线程，所以在第一个线程没有从缓存中获取到数据之后，有可能其他线程已经完成了读取数据库写入缓存的操作，也就是说，第一个线程再次得到时间片的时候，就没有必要访问数据库获取数据了。第二层检测是为了避免额外的访库操作。\n\npublic  student getstudentbyid(integer id) {\n    redistemplate.setkeyserializer(new stringredisserializer());\n    //查询缓存\n    student student = (student) redistemplate.opsforvalue().get("studentkey");\n    //判断缓存是否为空\n    if (null == student) {\n\n        //双重检测锁实现\n        synchronized (this) {\n\n            student = (student) redistemplate.opsforvalue().get("studentkey");\n\n            if (null == student) {\n                system.out.println("查询了数据库......");\n                //查询数据库\n                student = studentmapper.selectbyprimarykey(id);\n                //放入缓存\n                redistemplate.opsforvalue().set("studentkey", student);\n            }\n        }\n    } else {\n        system.out.println("查询了缓存......");\n    }\n    return student;\n}\n',charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"任务补偿",frontmatter:{title:"任务补偿",date:"2024-09-14T16:51:13.000Z",permalink:"/pages/24abe0/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/04.%E4%BB%BB%E5%8A%A1%E8%A1%A5%E5%81%BF.html",relativePath:"实战系统设计/07.经典场景设计/04.任务补偿.md",key:"v-6f56ad33",path:"/pages/24abe0/",headers:[{level:2,title:"前言：补偿机制的意义？",slug:"前言-补偿机制的意义",normalizedTitle:"前言：补偿机制的意义？",charIndex:2},{level:2,title:"补偿 该怎么做？",slug:"补偿-该怎么做",normalizedTitle:"补偿 该怎么做？",charIndex:726},{level:3,title:"回滚",slug:"回滚",normalizedTitle:"回滚",charIndex:771},{level:3,title:"重试",slug:"重试",normalizedTitle:"重试",charIndex:318},{level:2,title:"重试 的最佳实践",slug:"重试-的最佳实践",normalizedTitle:"重试 的最佳实践",charIndex:3585},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4108},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:4244}],headersStr:"前言：补偿机制的意义？ 补偿 该怎么做？ 回滚 重试 重试 的最佳实践 总结 参考文献",content:"# 前言：补偿机制的意义？\n\n以电商的购物场景为例：\n\n客户端 ----\x3e购物车微服务 ----\x3e订单微服务 ----\x3e 支付微服务。\n\n这种调用链非常普遍。\n\n那么为什么需要考虑补偿机制呢？\n\n正如之前几篇文章所说，一次跨机器的通信可能会经过DNS 服务，网卡、交换机、路由器、负载均衡等设备，这些设备都不一定是一直稳定的，在数据传输的整个过程中，只要任意一个环节出错，都会导致问题的产生。\n\n而在分布式场景中，一个完整的业务又是由多次跨机器通信组成的，所以产生问题的概率成倍数增加。\n\n但是，这些问题并不完全代表真正的系统无法处理请求，所以我们应当尽可能的自动消化掉这些异常。\n\n可能你会问，之前也看到过「补偿」和「事务补偿」或者「重试」，它们之间的关系是什么？\n\n你其实可以不用太纠结这些名字，从目的来说都是一样的。就是一旦某个操作发生了异常，如何通过内部机制将这个异常产生的「不一致」状态消除掉。\n\n> 在 echo 看来，不管用什么方式，只要通过额外的方式解决了问题都可以理解为是「补偿」，所以「事务补偿」和「重试」都是「补偿」的子集。前者是一个逆向操作，而后者则是一个正向操作。\n\n只是从结果来看，两者的意义不同。「事务补偿」意味着“放弃”，当前操作必然会失败。\n\n事务补偿\n\n\n\n「重试」则还有处理成功的机会。这两种方式分别适用于不同的场景。\n\n重试\n\n\n\n因为「补偿」已经是一个额外流程了，既然能够走这个额外流程，说明时效性并不是第一考虑的因素，所以做补偿的核心要点是：宁可慢，不可错。\n\n因此，不要草率的就确定了补偿的实施方案，需要谨慎的评估。虽说错误无法100%避免，但是抱着这样的一个心态或多或少可以减少一些错误的发生。\n\n\n# 补偿 该怎么做？\n\n做「补偿」的主流方式就前面提到的「事务补偿」和「重试」，以下会被称作「回滚」和「重试」。\n\n我们先来聊聊「回滚」。相比「重试」，它逻辑上更简单一些。\n\n\n# 回滚\n\necho 将回滚分为2种模式，一种叫「显式回滚」（调用逆向接口），一种叫「隐式回滚」（无需调用逆向接口）。\n\n最常见的就是「显式回滚」。这个方案无非就是做2个事情：\n\n首先要确定失败的步骤和状态，从而确定需要回滚的范围。一个业务的流程，往往在设计之初就制定好了，所以确定回滚的范围比较容易。但这里唯一需要注意的一点就是：如果在一个业务处理中涉及到的服务并不是都提供了「回滚接口」，那么在编排服务时应该把提供「回滚接口」的服务放在前面，这样当后面的工作服务错误时还有机会「回滚」。\n\n其次要能提供「回滚」操作使用到的业务数据。「回滚」时提供的数据越多，越有益于程序的健壮性。因为程序可以在收到「回滚」操作的时候可以做业务的检查，比如检查账户是否相等，金额是否一致等等。\n\n由于这个中间状态的数据结构和数据大小并不固定，所以echo建议你在实现这点的时候可以将相关的数据序列化成一个json，然后存放到一个nosql类型的存储中。\n\n「隐式回滚」相对来说运用场景比较少。它意味着这个回滚动作你不需要进行额外处理，下游服务内部有类似“预占”并且“超时失效”的机制的。例如：\n\n电商场景中，会将订单中的商品先预占库存，等待用户在 15 分钟内支付。如果没有收到用户的支付，则释放库存。\n\n下面聊聊可以有很多玩法，也更容易陷入坑里的「重试」。\n\n\n# 重试\n\n「重试」最大的好处在于，业务系统可以不需要提供「逆向接口」，这是一个对长期开发成本特别大的利好，毕竟业务是天天在变的。所以，在可能的情况下，应该优先考虑使用「重试」。\n\n不过，相比「回滚」来说「重试」的适用场景更少一些，所以我们第一步首先要判断，当前场景是否适合「重试」。比如：\n\n * 下游系统返回「请求超时」、「被限流中」等临时状态的时候，我们可以考虑重试\n * 而如果是返回“余额不足”、“无权限”等明确无法继续的业务性错误的时候就不需要重试了\n * 一些中间件或者rpc框架中返回Http503、404等没有何时恢复的预期的时候，也不需要重试\n\n如果确定要进行「重试」，我们还需要选定一个合适的「重试策略」。主流的「重试策略」主要是以下几种。\n\n策略1.立即重试。有时故障是候暂时性，可能是因网络数据包冲突或硬件组件流量高峰等事件造成的。在此情况下，适合立即重试操作。不过，立即重试次数不应超过一次，如果立即重试失败，应改用其它的策略。\n\n策略2.固定间隔。应用程序每次尝试的间隔时间相同。 这个好理解，例如，固定每 3 秒重试操作。（以下所有示例代码中的具体的数字仅供参考。）\n\n策略1和策略2多用于前端系统的交互式操作中。\n\n策略3.增量间隔。每一次的重试间隔时间增量递增。比如，第一次0秒、第二次3秒、第三次6秒，9、12、15这样。\n\nreturn (retryCount - 1) * incrementInterval;\n\n\n使得失败次数越多的重试请求优先级排到越后面，给新进入的重试请求让道。\n\n策略4.指数间隔。每一次的重试间隔呈指数级增加。和增量间隔“殊途同归”，都是想让失败次数越多的重试请求优先级排到越后面，只不过这个方案的增长幅度更大一些。\n\nreturn 2 ^ retryCount;\n\n\n策略5.全抖动。在递增的基础上，增加随机性（可以把其中的指数增长部分替换成增量增长。）。适用于将某一时刻集中产生的大量重试请求进行压力分散的场景。\n\nreturn random(0 , 2 ^ retryCount);\n\n\n策略6.等抖动。在「指数间隔」和「全抖动」之间寻求一个中庸的方案，降低随机性的作用。适用场景和「全抖动」一样。\n\nvar baseNum = 2 ^ retryCount;return baseNum + random(0 , baseNum);\n\n\n3、4、5、6策略的表现情况大致是这样。(x轴为重试次数)\n\n\n\n为什么说「重试」有坑呢？\n\n正如前面聊到的那样，出于对开发成本考虑，你在做「重试」的时候可能是复用的常规调用的接口。那么此时就不得不提一个「幂等性」问题。\n\n如果实现「重试」选用的技术方案不能100%确保不会重复发起重试，那么「幂等性」问题是一个必须要考虑的问题。哪怕技术方案可以确保100%不会重复发起重试，出于对意外情况的考量，尽量也考虑一下「幂等性」问题。\n\n**幂等性：**不管对程序发起几次重复调用，程序表现的状态（所有相关的数据变化）与调用一次的结果是一致的话，就是保证了幂等性。\n\n这意味着可以根据需要重复或重试操作，而不会导致意外的影响。对于非幂等操作，算法可能必须跟踪操作是否已经执行。\n\n所以，一旦某个功能支持「重试」，那么整个链路上的接口都需要考虑幂等性问题，不能因为服务的多次调用而导致业务数据的累计增加或减少。\n\n满足「幂等性」其实就是需要想办法识别重复的请求，并且将其过滤掉。思路就是：\n\n 1. 给每个请求定义一个唯一标识。\n 2. 在进行「重试」的时候判断这个请求是否已经被执行或者正在被执行，如果是则抛弃该请求。\n\n**第1点，**我们可以使用一个全局唯一id生成器或者生成服务（可以扩展阅读，分布式系统中的必备良药 —— 全局唯一单据号生成）。 或者简单粗暴一些，使用官方类库自带的Guid、uuid之类的也行。\n\n然后通过rpc框架在发起调用的客户端中，对每个请求增加一个唯一标识的字段进行赋值。\n\n**第2点，**我们可以在服务端通过Aop的方式切入到实际的处理逻辑代码之前和之后，一起配合做验证。\n\n\n\n大致的代码思路如下。\n\n【方法执行前】if(isExistLog(requestId)){  //1.判断请求是否已被接收过。  对应序号3\n    var lastResult = getLastResult();  //2.获取用于判断之前的请求是否已经处理完成。  对应序号4\n    if(lastResult == null){  \n        var result = waitResult();  //挂起等待处理完成\n        return result;\n    }\n    else{\n        return lastResult;\n    }  \n}\nelse{\n    log(requestId);  //3.记录该请求已接收\n}\n\n//do something..【方法执行后】\n\nlogResult(requestId, result);  //4.将结果也更新一下。\n\n\n如果「补偿」这个工作是通过MQ来进行的话，这事就可以直接在对接MQ所封装的SDK中做。在生产端赋值全局唯一标识，在消费端通过唯一标识消重。\n\n\n# 重试 的最佳实践\n\n再聊一些 echo 积累的最佳实践吧，都是针对「重试」的，的确这也是工作中最常用的方案。\n\n「重试」特别适合在高负载情况下被「降级」，当然也应当受到「限流」和「熔断」机制的影响。当「重试」的“矛”与「限流」和「熔断」的“盾”搭配使用，效果才是最好。\n\n需要衡量增加补偿机制的投入产出比。一些不是很重要的问题时，应该「快速失败」而不是「重试」。\n\n过度积极的重试策略（例如间隔太短或重试次数过多）会对下游服务造成不利影响，这点一定要注意。\n\n一定要给「重试」制定一个终止策略。\n\n当回滚的过程很困难或代价很大的情况下，可以接受很长的间隔及大量的重试次数，DDD中经常被提到的「saga」模式其实也是这样的思路。不过，前提是不会因为保留或锁定稀缺资源而阻止其他操作（比如1、2、3、4、5几个串行操作。由于2一直没处理完成导致3、4、5没法继续进行）。\n\n可以离线的事务一致性维护机制\n\n 1. 第一步：在线业务生成可疑记录\n 2. 第二步：离线服务诊断可疑记录，生成故障记录\n 3. 第三步：离线服务尝试对故障记录进行智能修复（补偿或重试）\n 4. 第四步：对于无法修复，或者修复过程失败的记录发出告警，交由人工处理。\n\n\n# 总结\n\n这篇我们先聊了下做「补偿」的意义，以及做补偿的2个方式「回滚」和「重试」的实现思路。\n\n然后，提醒你要注意「重试」的时候需要考虑幂等性问题，并且z哥也给出了一个解决思路。\n\n最后，分享了几个 echo 总结的针对「重试」的最佳实践。\n\n希望对你有所帮助。\n\n\n# 参考文献\n\n99%的人都能看懂的分布式系统「补偿」机制 - 知乎 (zhihu.com)",normalizedContent:"# 前言：补偿机制的意义？\n\n以电商的购物场景为例：\n\n客户端 ----\x3e购物车微服务 ----\x3e订单微服务 ----\x3e 支付微服务。\n\n这种调用链非常普遍。\n\n那么为什么需要考虑补偿机制呢？\n\n正如之前几篇文章所说，一次跨机器的通信可能会经过dns 服务，网卡、交换机、路由器、负载均衡等设备，这些设备都不一定是一直稳定的，在数据传输的整个过程中，只要任意一个环节出错，都会导致问题的产生。\n\n而在分布式场景中，一个完整的业务又是由多次跨机器通信组成的，所以产生问题的概率成倍数增加。\n\n但是，这些问题并不完全代表真正的系统无法处理请求，所以我们应当尽可能的自动消化掉这些异常。\n\n可能你会问，之前也看到过「补偿」和「事务补偿」或者「重试」，它们之间的关系是什么？\n\n你其实可以不用太纠结这些名字，从目的来说都是一样的。就是一旦某个操作发生了异常，如何通过内部机制将这个异常产生的「不一致」状态消除掉。\n\n> 在 echo 看来，不管用什么方式，只要通过额外的方式解决了问题都可以理解为是「补偿」，所以「事务补偿」和「重试」都是「补偿」的子集。前者是一个逆向操作，而后者则是一个正向操作。\n\n只是从结果来看，两者的意义不同。「事务补偿」意味着“放弃”，当前操作必然会失败。\n\n事务补偿\n\n\n\n「重试」则还有处理成功的机会。这两种方式分别适用于不同的场景。\n\n重试\n\n\n\n因为「补偿」已经是一个额外流程了，既然能够走这个额外流程，说明时效性并不是第一考虑的因素，所以做补偿的核心要点是：宁可慢，不可错。\n\n因此，不要草率的就确定了补偿的实施方案，需要谨慎的评估。虽说错误无法100%避免，但是抱着这样的一个心态或多或少可以减少一些错误的发生。\n\n\n# 补偿 该怎么做？\n\n做「补偿」的主流方式就前面提到的「事务补偿」和「重试」，以下会被称作「回滚」和「重试」。\n\n我们先来聊聊「回滚」。相比「重试」，它逻辑上更简单一些。\n\n\n# 回滚\n\necho 将回滚分为2种模式，一种叫「显式回滚」（调用逆向接口），一种叫「隐式回滚」（无需调用逆向接口）。\n\n最常见的就是「显式回滚」。这个方案无非就是做2个事情：\n\n首先要确定失败的步骤和状态，从而确定需要回滚的范围。一个业务的流程，往往在设计之初就制定好了，所以确定回滚的范围比较容易。但这里唯一需要注意的一点就是：如果在一个业务处理中涉及到的服务并不是都提供了「回滚接口」，那么在编排服务时应该把提供「回滚接口」的服务放在前面，这样当后面的工作服务错误时还有机会「回滚」。\n\n其次要能提供「回滚」操作使用到的业务数据。「回滚」时提供的数据越多，越有益于程序的健壮性。因为程序可以在收到「回滚」操作的时候可以做业务的检查，比如检查账户是否相等，金额是否一致等等。\n\n由于这个中间状态的数据结构和数据大小并不固定，所以echo建议你在实现这点的时候可以将相关的数据序列化成一个json，然后存放到一个nosql类型的存储中。\n\n「隐式回滚」相对来说运用场景比较少。它意味着这个回滚动作你不需要进行额外处理，下游服务内部有类似“预占”并且“超时失效”的机制的。例如：\n\n电商场景中，会将订单中的商品先预占库存，等待用户在 15 分钟内支付。如果没有收到用户的支付，则释放库存。\n\n下面聊聊可以有很多玩法，也更容易陷入坑里的「重试」。\n\n\n# 重试\n\n「重试」最大的好处在于，业务系统可以不需要提供「逆向接口」，这是一个对长期开发成本特别大的利好，毕竟业务是天天在变的。所以，在可能的情况下，应该优先考虑使用「重试」。\n\n不过，相比「回滚」来说「重试」的适用场景更少一些，所以我们第一步首先要判断，当前场景是否适合「重试」。比如：\n\n * 下游系统返回「请求超时」、「被限流中」等临时状态的时候，我们可以考虑重试\n * 而如果是返回“余额不足”、“无权限”等明确无法继续的业务性错误的时候就不需要重试了\n * 一些中间件或者rpc框架中返回http503、404等没有何时恢复的预期的时候，也不需要重试\n\n如果确定要进行「重试」，我们还需要选定一个合适的「重试策略」。主流的「重试策略」主要是以下几种。\n\n策略1.立即重试。有时故障是候暂时性，可能是因网络数据包冲突或硬件组件流量高峰等事件造成的。在此情况下，适合立即重试操作。不过，立即重试次数不应超过一次，如果立即重试失败，应改用其它的策略。\n\n策略2.固定间隔。应用程序每次尝试的间隔时间相同。 这个好理解，例如，固定每 3 秒重试操作。（以下所有示例代码中的具体的数字仅供参考。）\n\n策略1和策略2多用于前端系统的交互式操作中。\n\n策略3.增量间隔。每一次的重试间隔时间增量递增。比如，第一次0秒、第二次3秒、第三次6秒，9、12、15这样。\n\nreturn (retrycount - 1) * incrementinterval;\n\n\n使得失败次数越多的重试请求优先级排到越后面，给新进入的重试请求让道。\n\n策略4.指数间隔。每一次的重试间隔呈指数级增加。和增量间隔“殊途同归”，都是想让失败次数越多的重试请求优先级排到越后面，只不过这个方案的增长幅度更大一些。\n\nreturn 2 ^ retrycount;\n\n\n策略5.全抖动。在递增的基础上，增加随机性（可以把其中的指数增长部分替换成增量增长。）。适用于将某一时刻集中产生的大量重试请求进行压力分散的场景。\n\nreturn random(0 , 2 ^ retrycount);\n\n\n策略6.等抖动。在「指数间隔」和「全抖动」之间寻求一个中庸的方案，降低随机性的作用。适用场景和「全抖动」一样。\n\nvar basenum = 2 ^ retrycount;return basenum + random(0 , basenum);\n\n\n3、4、5、6策略的表现情况大致是这样。(x轴为重试次数)\n\n\n\n为什么说「重试」有坑呢？\n\n正如前面聊到的那样，出于对开发成本考虑，你在做「重试」的时候可能是复用的常规调用的接口。那么此时就不得不提一个「幂等性」问题。\n\n如果实现「重试」选用的技术方案不能100%确保不会重复发起重试，那么「幂等性」问题是一个必须要考虑的问题。哪怕技术方案可以确保100%不会重复发起重试，出于对意外情况的考量，尽量也考虑一下「幂等性」问题。\n\n**幂等性：**不管对程序发起几次重复调用，程序表现的状态（所有相关的数据变化）与调用一次的结果是一致的话，就是保证了幂等性。\n\n这意味着可以根据需要重复或重试操作，而不会导致意外的影响。对于非幂等操作，算法可能必须跟踪操作是否已经执行。\n\n所以，一旦某个功能支持「重试」，那么整个链路上的接口都需要考虑幂等性问题，不能因为服务的多次调用而导致业务数据的累计增加或减少。\n\n满足「幂等性」其实就是需要想办法识别重复的请求，并且将其过滤掉。思路就是：\n\n 1. 给每个请求定义一个唯一标识。\n 2. 在进行「重试」的时候判断这个请求是否已经被执行或者正在被执行，如果是则抛弃该请求。\n\n**第1点，**我们可以使用一个全局唯一id生成器或者生成服务（可以扩展阅读，分布式系统中的必备良药 —— 全局唯一单据号生成）。 或者简单粗暴一些，使用官方类库自带的guid、uuid之类的也行。\n\n然后通过rpc框架在发起调用的客户端中，对每个请求增加一个唯一标识的字段进行赋值。\n\n**第2点，**我们可以在服务端通过aop的方式切入到实际的处理逻辑代码之前和之后，一起配合做验证。\n\n\n\n大致的代码思路如下。\n\n【方法执行前】if(isexistlog(requestid)){  //1.判断请求是否已被接收过。  对应序号3\n    var lastresult = getlastresult();  //2.获取用于判断之前的请求是否已经处理完成。  对应序号4\n    if(lastresult == null){  \n        var result = waitresult();  //挂起等待处理完成\n        return result;\n    }\n    else{\n        return lastresult;\n    }  \n}\nelse{\n    log(requestid);  //3.记录该请求已接收\n}\n\n//do something..【方法执行后】\n\nlogresult(requestid, result);  //4.将结果也更新一下。\n\n\n如果「补偿」这个工作是通过mq来进行的话，这事就可以直接在对接mq所封装的sdk中做。在生产端赋值全局唯一标识，在消费端通过唯一标识消重。\n\n\n# 重试 的最佳实践\n\n再聊一些 echo 积累的最佳实践吧，都是针对「重试」的，的确这也是工作中最常用的方案。\n\n「重试」特别适合在高负载情况下被「降级」，当然也应当受到「限流」和「熔断」机制的影响。当「重试」的“矛”与「限流」和「熔断」的“盾”搭配使用，效果才是最好。\n\n需要衡量增加补偿机制的投入产出比。一些不是很重要的问题时，应该「快速失败」而不是「重试」。\n\n过度积极的重试策略（例如间隔太短或重试次数过多）会对下游服务造成不利影响，这点一定要注意。\n\n一定要给「重试」制定一个终止策略。\n\n当回滚的过程很困难或代价很大的情况下，可以接受很长的间隔及大量的重试次数，ddd中经常被提到的「saga」模式其实也是这样的思路。不过，前提是不会因为保留或锁定稀缺资源而阻止其他操作（比如1、2、3、4、5几个串行操作。由于2一直没处理完成导致3、4、5没法继续进行）。\n\n可以离线的事务一致性维护机制\n\n 1. 第一步：在线业务生成可疑记录\n 2. 第二步：离线服务诊断可疑记录，生成故障记录\n 3. 第三步：离线服务尝试对故障记录进行智能修复（补偿或重试）\n 4. 第四步：对于无法修复，或者修复过程失败的记录发出告警，交由人工处理。\n\n\n# 总结\n\n这篇我们先聊了下做「补偿」的意义，以及做补偿的2个方式「回滚」和「重试」的实现思路。\n\n然后，提醒你要注意「重试」的时候需要考虑幂等性问题，并且z哥也给出了一个解决思路。\n\n最后，分享了几个 echo 总结的针对「重试」的最佳实践。\n\n希望对你有所帮助。\n\n\n# 参考文献\n\n99%的人都能看懂的分布式系统「补偿」机制 - 知乎 (zhihu.com)",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"秒杀",frontmatter:{title:"秒杀",date:"2024-09-14T16:51:28.000Z",permalink:"/pages/a72629/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/05.%E7%A7%92%E6%9D%80.html",relativePath:"实战系统设计/07.经典场景设计/05.秒杀.md",key:"v-fa43c8ec",path:"/pages/a72629/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"秒杀会有哪些问题",slug:"秒杀会有哪些问题",normalizedTitle:"秒杀会有哪些问题",charIndex:21},{level:2,title:"解决方案",slug:"解决方案",normalizedTitle:"解决方案",charIndex:1438},{level:3,title:"前端",slug:"前端",normalizedTitle:"前端",charIndex:1447},{level:3,title:"Nginx",slug:"nginx",normalizedTitle:"nginx",charIndex:2626},{level:3,title:"风控",slug:"风控",normalizedTitle:"风控",charIndex:2613},{level:3,title:"后端",slug:"后端",normalizedTitle:"后端",charIndex:1535},{level:3,title:"数据库",slug:"数据库",normalizedTitle:"数据库",charIndex:1180},{level:3,title:"分布式事务",slug:"分布式事务",normalizedTitle:"分布式事务",charIndex:4905},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:5198},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:5347}],headersStr:"前言 秒杀会有哪些问题 解决方案 前端 Nginx 风控 后端 数据库 分布式事务 总结 参考文献",content:"# 前言\n\n秒杀系统的架构图\n\n\n\n\n# 秒杀会有哪些问题\n\n高并发\n\n是的高并发这个是我们想都不用想的一个点，一瞬间这么多人进来这不是高并发什么时候是呢？\n\n是吧，秒杀的特点就是这样时间极短、 瞬间用户量大。\n\n正常的店铺营销都是用极低的价格配合上短信、APP的精准推送，吸引特别多的用户来参与这场秒杀，爽了商家苦了开发呀。\n\n秒杀大家都知道如果真的营销到位，价格诱人，几十万的流量我觉得完全不是问题，那单机的Redis我感觉3-4W的QPS还是能顶得住的，但是再高了就没办法了，那这个数据随便搞个热销商品的秒杀可能都不止了。\n\n大量的请求进来，我们需要考虑的点就很多了，缓存雪崩，缓存击穿，缓存穿透这些我之前提到的点都是有可能发生的，出现问题打挂DB那就很难受了，活动失败用户体验差，活动人气没了，最后背锅的还是开发\n\n超卖\n\n但凡是个秒杀，都怕超卖，如果卖的是尿不湿还好，要是换成100个MacBook Pro，商家的预算经费卖100个可以赚点还可以造势，结果你写错程序多卖出去200个，你不发货用户投诉你，平台封你店，你发货就血亏，你怎么办？\n\n那最后只能杀个开发祭天解气了，秒杀的价格本来就低了，基本上都是不怎么赚钱的，超卖了就恐怖了呀，所以超卖也是很关键的一个点。\n\n恶意请求\n\n你这么低的价格，假如我抢到了，我转手卖掉我不是血赚？就算我不卖我也不亏啊，那用户知道，你知道，别的别有用心的人（黑客、黄牛…）肯定也知道的。\n\n那简单啊，我知道你什么时候抢，我搞个几十台机器搞点脚本，我也模拟出来十几万个人左右的请求，那我是不是意味着我基本上有80%的成功率了。\n\n真实情况可能远远不止，因为机器请求的速度比人的手速往往快太多了，在贵州的敖丙我每年回家抢高铁票都是秒光的，我也不知道有没有黄牛的功劳，我要Diss你，黄牛。杰伦演唱会门票抢不到，我也Diss你。\n\nTip：科普下，小道消息了解到的，黄牛的抢票系统，比国内很多小公司的系统还吊很多，架构设计都是顶级的，我用顶配的服务加上顶配的架构设计，你还想看演唱会？还想回家？\n\n不过不用黄牛我回家都难，我们云贵川跟我一样要回家过年的仔太多了555！\n\n链接暴露\n\n前面几个问题大家可能都很好理解，一看到这个有的小伙伴可能会比较疑惑，啥是链接暴露呀？\n\n相信是个开发同学都对这个画面一点都不陌生吧，懂点行的仔都可以打开谷歌的开发者模式，然后看看你的网页代码，有的就有URL，但是我写VUE的时候是事件触发然后去调用文件里面的接口看源码看不到，但是我可以点击一下查看你的请求地址啊，不过你好像可以对按钮在秒杀前置灰。\n\n不管怎么样子都有危险，撇开外面的所有的东西你都挡住了，你卖这个东西实在便宜得过分，有诱惑力，你能保证开发不动心？开发知道地址，在秒杀的时候自己提前请求。。。（开发：怎么TM又是我）\n\n数据库\n\n每秒上万甚至十几万的QPS（每秒请求数）直接打到数据库，基本上都要把库打挂掉，而且你服务不单单是做秒杀的还涉及其他的业务，你没做降级、限流、熔断啥的，别的一起挂，小公司的话可能全站崩溃404。\n\n反正不管你秒杀怎么挂，你别把别的搞挂了对吧，搞挂了就不是杀一个程序员能搞定的。\n\n程序员：我TM好难啊！\n\n问题都列出来了，那怎么设计，怎么解决这些问题就是接下去要考虑的了，我们对症下药。\n\n我会从我设计的秒杀系统从上到下去给大家介绍我们正常电商秒杀系统在每一层做了些什么，每一层存在的问题，难点等。\n\n\n# 解决方案\n\n\n# 前端\n\n秒杀系统普遍都是商城网页、H5、APP、小程序这几项。\n\n在前端这一层其实我们可以做的事情有很多，如果用node去做，甚至能直接处理掉整个秒杀，但是node其实应该属于后端，所以我不讨论node Service了。\n\n资源静态化\n\n秒杀一般都是特定的商品还有页面模板，现在一般都是前后端分离的，页面一般都是不会经过后端的，但是前端也要自己的服务器啊，那就把能提前放入cdn服务器的东西都放进去，反正把所有能提升效率的步骤都做一下，减少真正秒杀时候服务器的压力。\n\n秒杀链接加盐\n\n我们上面说了链接要是提前暴露出去可能有人直接访问url就提前秒杀了，那又有小伙伴要说了我做个时间的校验就好了呀，那我告诉你，知道链接的地址比起页面人工点击的还是有很大优势。\n\n我知道url了，那我通过程序不断获取最新的北京时间，可以达到毫秒级别的，我就在00毫秒的时候请求，我敢说绝对比你人工点的成功率大太多了，而且我可以一毫秒发送N次请求，搞不好你卖100个产品我全拿了。\n\n那这种情况怎么避免？把URL动态化，就连写代码的人都不知道，你就通过MD5之类的摘要算法加密随机的字符串去做url，然后通过前端代码获取url后台校验才能通过。\n\n这个只能防止一部分没耐心继续破解下去的黑客，有耐心的人研究出来还是能破解，在电商场景存在很多这样的羊毛党，那怎么做呢？后面我会说。\n\n限流\n\n限流这里我觉得应该分为前端限流和后端限流。\n\n物理控制\n\n大家有没有发现没到秒杀前，一般按钮都是置灰的，只有时间到了，才能点击。\n\n这是因为怕大家在时间快到的最后几秒秒疯狂请求服务器，然后还没到秒杀的时候基本上服务器就挂了。\n\n这个时候就需要前端的配合，定时去请求你的后端服务器，获取最新的北京时间，到时间点再给按钮可用状态。\n\n按钮可以点击之后也得给他置灰几秒，不然他一样在开始之后一直点的。\n\n你敢说你们秒杀的时候不是这样的？\n\n前端限流：这个很简单，一般秒杀不会让你一直点的，一般都是点击一下或者两下然后几秒之后才可以继续点击，这也是保护服务器的一种手段。\n\n后端限流：秒杀的时候肯定是涉及到后续的订单生成和支付等操作，但是都只是成功的幸运儿才会走到那一步，那一旦100个产品卖光了，return了一个false，前端直接秒杀结束，然后你后端也关闭后续无效请求的介入了。\n\nTip：真正的限流还会有限流组件的加入例如：阿里的Sentinel、Hystrix等。我这里就不展开了，就说一下物理的限流。\n\n我们卖1000件商品，请求有10W，我们不需要把十万都放进来，你可以放1W请求进来，然后再进行操作，因为秒杀对于用户本身就是黑盒的，所以你怎么做的他们是没感知的，至于为啥放1W进来，而不是刚好1000，是因为会丢掉一些薅羊毛的用户，至于怎么判断，后面的风控阶段我会说。\n\n\n# Nginx\n\nNginx大家想必都不陌生了吧，这玩意是高性能的web服务器，并发也随便顶几万不是梦，但是我们的Tomcat只能顶几百的并发呀，那简单呀负载均衡嘛，一台服务几百，那就多搞点，在秒杀的时候多租点流量机。\n\nTip：据我所知国内某大厂就是在去年春节活动期间租光了亚洲所有的服务器，小公司也很喜欢在双十一期间买流量机来顶住压力。\n\n恶意请求拦截也需要用到它，一般单个用户请求次数太夸张，不像人为的请求在网关那一层就得拦截掉了，不然请求多了他抢不抢得到是一回事，服务器压力上去了，可能占用网络带宽或者把服务器打崩、缓存击穿等等\n\n\n# 风控\n\n我可以明确的告诉大家，前面的所有措施还是拦不住很多羊毛党，因为他们是专业的团队，他们可以注册很多账号来薅你的羊毛，而且不用机器请求，就用群控，操作几乎跟真实用户一模一样。\n\n那怎么办，是不是无解了？\n\n这个时候就需要风控同学的介入了，在请求到达后端之前，风控可以根据账号行为分析出这个账号机器人的概率大不大，我现在负责公司的某些特殊系统，每个用户的行为都是会送到我们大数据团队进行分析处理，给你打上对应标签的。\n\n那黑客其实也有办法：养号\n\n他们去黑市买真实用户有过很多记录的账号，买到了还不闲着，帮他们去购物啥的，让系统无法识别他们是黑号还是真实用户的号。\n\n怎么办？\n\n通杀！是的没有办法，只能通杀了，通杀的意思就是，我们通过风管分析出来这个用户是真实用户的概率没有其他用户概率大，那就认为他是机器了，丢弃他的请求。\n\n之前的限流我们放进来10000个请求，但是我们真正的库存只有1000个，那我们就算出最有可能是真实用户的1000人进行秒杀，丢弃其他请求，因为秒杀本来就是黑盒操作的，用户层面是无感知的，这样设计能让真实的用户买到东西，还可以减少自己被薅羊毛的概率。\n\n风控可以说是流量进入的最后一道门槛了，所以很多公司的风控是很强的，蚂蚁金服的风控大家如果了解过就知道了，你的资金在支付宝被盗了，他们是能做到全款补偿是有原因的。\n\n\n# 后端\n\n服务单一职责\n\n设计个能抗住高并发的系统，我觉得还是得单一职责。\n\n什么意思呢，大家都知道现在设计都是微服务的设计思想，然后再用分布式的部署方式。\n\n也就是我们下单是有个订单服务，用户登录管理等有个用户服务等等，那为啥我们不给秒杀也开个服务，我们把秒杀的代码业务逻辑放一起。\n\n单一职责的好处就是就算秒杀没抗住，秒杀库崩了，服务挂了，也不会影响到其他的服务。（高可用）\n\nRedis 集群\n\n之前不是说单机的Redis顶不住嘛，那简单多找几个兄弟啊，秒杀本来就是读多写少，那你们是不是瞬间想起来我之前跟你们提到过的，Redis集群，主从同步、读写分离，我们还搞点哨兵，开启持久化直接无敌高可用！\n\n库存预热\n\n秒杀的本质，就是对库存的抢夺，每个秒杀的用户来你都去数据库查询库存校验库存，然后扣减库存，撇开性能因数，你不觉得这样好繁琐，对业务开发人员都不友好，而且数据库顶不住啊。\n\n我们都知道数据库顶不住但是他的兄弟非关系型的数据库Redis能顶啊！\n\n那不简单了，我们要开始秒杀前你通过定时任务或者运维同学提前把商品的库存加载到Redis中去，让整个流程都在Redis里面去做，然后等秒杀介绍了，再异步的去修改库存就好了。\n\n但是用了Redis就有一个问题了，我们上面说了我们采用主从，就是我们会去读取库存然后再判断然后有库存才去减库存，正常情况没问题，但是高并发的情况问题就很大了。\n\n**多品几遍！！！**就比如现在库存只剩下1个了，我们高并发嘛，4个服务器一起查询了发现都是还有1个，那大家都觉得是自己抢到了，就都去扣库存，那结果就变成了-3，是的只有一个是真的抢到了，别的都是超卖的。咋办？\n\n事务\n\nRedis本身是支持事务的，而且他有很多原子命令的，大家也可以用LUA，还可以用他的管道，乐观锁他也知支持。\n\n限流&降级&熔断&隔离\n\n这个为啥要做呢，不怕一万就怕万一，万一你真的顶不住了，限流，顶不住就挡一部分出去但是不能说不行，降级，降级了还是被打挂了，熔断，至少不要影响别的系统，隔离，你本身就独立的，但是你会调用其他的系统嘛，你快不行了你别拖累兄弟们啊。\n\n消息队列（削峰填谷）\n\n一说到这个名词，很多小伙伴就知道了，对的MQ，你买东西少了你直接100个请求改库我觉得没问题，但是万一秒杀一万个，10万个呢？服务器挂了，程序员又要背锅的。\n\n秒杀就是这种瞬间流量很高，但是平时又没有流量的场景，那消息队列完全契合这样的场景了呀，削峰填谷。\n\n\n\nTip：可能小伙伴说我们业务达不到这个量级，没必要。但是我想说我们写代码，就不应该写出有逻辑漏洞的代码，至少以后公司体量上去了，别人一看居然不用改代码，一看代码作者是xxx？有点东西！\n\n你可以把它放消息队列，然后一点点消费去改库存就好了嘛，不过单个商品其实一次修改就够了，我这里说的是某个点多个商品一起秒杀的场景，像极了双十一零点。\n\n\n# 数据库\n\n数据库用MySQL只要连接池设置合理一般问题是不大的，不过一般大公司不缺钱而且秒杀这样的活动十分频繁，我之前所在的公司就是这样秒杀特卖这样的场景一直都是不间断的。\n\n单独给秒杀建立一个数据库，为秒杀服务，表的设计也是竟可能的简单点，现在的互联网架构部署都是分库的。\n\n至于表就看大家怎么设计了，该设置索引的地方还是要设置索引的，建完后记得用explain看看SQL的执行计划。（不了解的小伙伴也没事，MySQL章节去康康）\n\n\n# 分布式事务\n\n这为啥我不放在后端而放到最后来讲呢？\n\n因为上面的任何一步都是可能出错的，而且我们是在不同的服务里面出错的，那就涉及分布式事务了，但是分布式事务大家想的是一定要成功什么的那就不对了，还是那句话，几个请求丢了就丢了，要保证时效和服务的可用可靠。\n\n所以TCC和最终一致性其实不是很适合，TCC开发成本很大，所有接口都要写三次，因为涉及TCC的三个阶段。\n\n最终一致性基本上都是靠轮训的操作去保证一个操作一定成功，那时效性就大打折扣了。\n\n大家觉得不那么可靠的**两段式（2PC）和三段式（3PC）**就派上用场了，他们不一定能保证数据最终一致，但是效率上还算ok。\n\n\n# 总结\n\n到这里我想我已经基本上把该考虑的点还有对应的解决方案也都说了一下，不知道还有没有没考虑到的，但是就算没考虑到我想我这个设计，应该也能撑住一个完整的秒杀流程。\n\n最后大家再看看这个秒杀系统或许会有新的感悟，是不是一个系统真的没有大家想的那么简单，而且我还是有漏掉的细节，这是一定的。\n\n\n# 参考文献\n\n面试了十个应届生九个都是秒杀系统，你确定你们那是秒杀？_小公司的qps只有几十-CSDN博客",normalizedContent:"# 前言\n\n秒杀系统的架构图\n\n\n\n\n# 秒杀会有哪些问题\n\n高并发\n\n是的高并发这个是我们想都不用想的一个点，一瞬间这么多人进来这不是高并发什么时候是呢？\n\n是吧，秒杀的特点就是这样时间极短、 瞬间用户量大。\n\n正常的店铺营销都是用极低的价格配合上短信、app的精准推送，吸引特别多的用户来参与这场秒杀，爽了商家苦了开发呀。\n\n秒杀大家都知道如果真的营销到位，价格诱人，几十万的流量我觉得完全不是问题，那单机的redis我感觉3-4w的qps还是能顶得住的，但是再高了就没办法了，那这个数据随便搞个热销商品的秒杀可能都不止了。\n\n大量的请求进来，我们需要考虑的点就很多了，缓存雪崩，缓存击穿，缓存穿透这些我之前提到的点都是有可能发生的，出现问题打挂db那就很难受了，活动失败用户体验差，活动人气没了，最后背锅的还是开发\n\n超卖\n\n但凡是个秒杀，都怕超卖，如果卖的是尿不湿还好，要是换成100个macbook pro，商家的预算经费卖100个可以赚点还可以造势，结果你写错程序多卖出去200个，你不发货用户投诉你，平台封你店，你发货就血亏，你怎么办？\n\n那最后只能杀个开发祭天解气了，秒杀的价格本来就低了，基本上都是不怎么赚钱的，超卖了就恐怖了呀，所以超卖也是很关键的一个点。\n\n恶意请求\n\n你这么低的价格，假如我抢到了，我转手卖掉我不是血赚？就算我不卖我也不亏啊，那用户知道，你知道，别的别有用心的人（黑客、黄牛…）肯定也知道的。\n\n那简单啊，我知道你什么时候抢，我搞个几十台机器搞点脚本，我也模拟出来十几万个人左右的请求，那我是不是意味着我基本上有80%的成功率了。\n\n真实情况可能远远不止，因为机器请求的速度比人的手速往往快太多了，在贵州的敖丙我每年回家抢高铁票都是秒光的，我也不知道有没有黄牛的功劳，我要diss你，黄牛。杰伦演唱会门票抢不到，我也diss你。\n\ntip：科普下，小道消息了解到的，黄牛的抢票系统，比国内很多小公司的系统还吊很多，架构设计都是顶级的，我用顶配的服务加上顶配的架构设计，你还想看演唱会？还想回家？\n\n不过不用黄牛我回家都难，我们云贵川跟我一样要回家过年的仔太多了555！\n\n链接暴露\n\n前面几个问题大家可能都很好理解，一看到这个有的小伙伴可能会比较疑惑，啥是链接暴露呀？\n\n相信是个开发同学都对这个画面一点都不陌生吧，懂点行的仔都可以打开谷歌的开发者模式，然后看看你的网页代码，有的就有url，但是我写vue的时候是事件触发然后去调用文件里面的接口看源码看不到，但是我可以点击一下查看你的请求地址啊，不过你好像可以对按钮在秒杀前置灰。\n\n不管怎么样子都有危险，撇开外面的所有的东西你都挡住了，你卖这个东西实在便宜得过分，有诱惑力，你能保证开发不动心？开发知道地址，在秒杀的时候自己提前请求。。。（开发：怎么tm又是我）\n\n数据库\n\n每秒上万甚至十几万的qps（每秒请求数）直接打到数据库，基本上都要把库打挂掉，而且你服务不单单是做秒杀的还涉及其他的业务，你没做降级、限流、熔断啥的，别的一起挂，小公司的话可能全站崩溃404。\n\n反正不管你秒杀怎么挂，你别把别的搞挂了对吧，搞挂了就不是杀一个程序员能搞定的。\n\n程序员：我tm好难啊！\n\n问题都列出来了，那怎么设计，怎么解决这些问题就是接下去要考虑的了，我们对症下药。\n\n我会从我设计的秒杀系统从上到下去给大家介绍我们正常电商秒杀系统在每一层做了些什么，每一层存在的问题，难点等。\n\n\n# 解决方案\n\n\n# 前端\n\n秒杀系统普遍都是商城网页、h5、app、小程序这几项。\n\n在前端这一层其实我们可以做的事情有很多，如果用node去做，甚至能直接处理掉整个秒杀，但是node其实应该属于后端，所以我不讨论node service了。\n\n资源静态化\n\n秒杀一般都是特定的商品还有页面模板，现在一般都是前后端分离的，页面一般都是不会经过后端的，但是前端也要自己的服务器啊，那就把能提前放入cdn服务器的东西都放进去，反正把所有能提升效率的步骤都做一下，减少真正秒杀时候服务器的压力。\n\n秒杀链接加盐\n\n我们上面说了链接要是提前暴露出去可能有人直接访问url就提前秒杀了，那又有小伙伴要说了我做个时间的校验就好了呀，那我告诉你，知道链接的地址比起页面人工点击的还是有很大优势。\n\n我知道url了，那我通过程序不断获取最新的北京时间，可以达到毫秒级别的，我就在00毫秒的时候请求，我敢说绝对比你人工点的成功率大太多了，而且我可以一毫秒发送n次请求，搞不好你卖100个产品我全拿了。\n\n那这种情况怎么避免？把url动态化，就连写代码的人都不知道，你就通过md5之类的摘要算法加密随机的字符串去做url，然后通过前端代码获取url后台校验才能通过。\n\n这个只能防止一部分没耐心继续破解下去的黑客，有耐心的人研究出来还是能破解，在电商场景存在很多这样的羊毛党，那怎么做呢？后面我会说。\n\n限流\n\n限流这里我觉得应该分为前端限流和后端限流。\n\n物理控制\n\n大家有没有发现没到秒杀前，一般按钮都是置灰的，只有时间到了，才能点击。\n\n这是因为怕大家在时间快到的最后几秒秒疯狂请求服务器，然后还没到秒杀的时候基本上服务器就挂了。\n\n这个时候就需要前端的配合，定时去请求你的后端服务器，获取最新的北京时间，到时间点再给按钮可用状态。\n\n按钮可以点击之后也得给他置灰几秒，不然他一样在开始之后一直点的。\n\n你敢说你们秒杀的时候不是这样的？\n\n前端限流：这个很简单，一般秒杀不会让你一直点的，一般都是点击一下或者两下然后几秒之后才可以继续点击，这也是保护服务器的一种手段。\n\n后端限流：秒杀的时候肯定是涉及到后续的订单生成和支付等操作，但是都只是成功的幸运儿才会走到那一步，那一旦100个产品卖光了，return了一个false，前端直接秒杀结束，然后你后端也关闭后续无效请求的介入了。\n\ntip：真正的限流还会有限流组件的加入例如：阿里的sentinel、hystrix等。我这里就不展开了，就说一下物理的限流。\n\n我们卖1000件商品，请求有10w，我们不需要把十万都放进来，你可以放1w请求进来，然后再进行操作，因为秒杀对于用户本身就是黑盒的，所以你怎么做的他们是没感知的，至于为啥放1w进来，而不是刚好1000，是因为会丢掉一些薅羊毛的用户，至于怎么判断，后面的风控阶段我会说。\n\n\n# nginx\n\nnginx大家想必都不陌生了吧，这玩意是高性能的web服务器，并发也随便顶几万不是梦，但是我们的tomcat只能顶几百的并发呀，那简单呀负载均衡嘛，一台服务几百，那就多搞点，在秒杀的时候多租点流量机。\n\ntip：据我所知国内某大厂就是在去年春节活动期间租光了亚洲所有的服务器，小公司也很喜欢在双十一期间买流量机来顶住压力。\n\n恶意请求拦截也需要用到它，一般单个用户请求次数太夸张，不像人为的请求在网关那一层就得拦截掉了，不然请求多了他抢不抢得到是一回事，服务器压力上去了，可能占用网络带宽或者把服务器打崩、缓存击穿等等\n\n\n# 风控\n\n我可以明确的告诉大家，前面的所有措施还是拦不住很多羊毛党，因为他们是专业的团队，他们可以注册很多账号来薅你的羊毛，而且不用机器请求，就用群控，操作几乎跟真实用户一模一样。\n\n那怎么办，是不是无解了？\n\n这个时候就需要风控同学的介入了，在请求到达后端之前，风控可以根据账号行为分析出这个账号机器人的概率大不大，我现在负责公司的某些特殊系统，每个用户的行为都是会送到我们大数据团队进行分析处理，给你打上对应标签的。\n\n那黑客其实也有办法：养号\n\n他们去黑市买真实用户有过很多记录的账号，买到了还不闲着，帮他们去购物啥的，让系统无法识别他们是黑号还是真实用户的号。\n\n怎么办？\n\n通杀！是的没有办法，只能通杀了，通杀的意思就是，我们通过风管分析出来这个用户是真实用户的概率没有其他用户概率大，那就认为他是机器了，丢弃他的请求。\n\n之前的限流我们放进来10000个请求，但是我们真正的库存只有1000个，那我们就算出最有可能是真实用户的1000人进行秒杀，丢弃其他请求，因为秒杀本来就是黑盒操作的，用户层面是无感知的，这样设计能让真实的用户买到东西，还可以减少自己被薅羊毛的概率。\n\n风控可以说是流量进入的最后一道门槛了，所以很多公司的风控是很强的，蚂蚁金服的风控大家如果了解过就知道了，你的资金在支付宝被盗了，他们是能做到全款补偿是有原因的。\n\n\n# 后端\n\n服务单一职责\n\n设计个能抗住高并发的系统，我觉得还是得单一职责。\n\n什么意思呢，大家都知道现在设计都是微服务的设计思想，然后再用分布式的部署方式。\n\n也就是我们下单是有个订单服务，用户登录管理等有个用户服务等等，那为啥我们不给秒杀也开个服务，我们把秒杀的代码业务逻辑放一起。\n\n单一职责的好处就是就算秒杀没抗住，秒杀库崩了，服务挂了，也不会影响到其他的服务。（高可用）\n\nredis 集群\n\n之前不是说单机的redis顶不住嘛，那简单多找几个兄弟啊，秒杀本来就是读多写少，那你们是不是瞬间想起来我之前跟你们提到过的，redis集群，主从同步、读写分离，我们还搞点哨兵，开启持久化直接无敌高可用！\n\n库存预热\n\n秒杀的本质，就是对库存的抢夺，每个秒杀的用户来你都去数据库查询库存校验库存，然后扣减库存，撇开性能因数，你不觉得这样好繁琐，对业务开发人员都不友好，而且数据库顶不住啊。\n\n我们都知道数据库顶不住但是他的兄弟非关系型的数据库redis能顶啊！\n\n那不简单了，我们要开始秒杀前你通过定时任务或者运维同学提前把商品的库存加载到redis中去，让整个流程都在redis里面去做，然后等秒杀介绍了，再异步的去修改库存就好了。\n\n但是用了redis就有一个问题了，我们上面说了我们采用主从，就是我们会去读取库存然后再判断然后有库存才去减库存，正常情况没问题，但是高并发的情况问题就很大了。\n\n**多品几遍！！！**就比如现在库存只剩下1个了，我们高并发嘛，4个服务器一起查询了发现都是还有1个，那大家都觉得是自己抢到了，就都去扣库存，那结果就变成了-3，是的只有一个是真的抢到了，别的都是超卖的。咋办？\n\n事务\n\nredis本身是支持事务的，而且他有很多原子命令的，大家也可以用lua，还可以用他的管道，乐观锁他也知支持。\n\n限流&降级&熔断&隔离\n\n这个为啥要做呢，不怕一万就怕万一，万一你真的顶不住了，限流，顶不住就挡一部分出去但是不能说不行，降级，降级了还是被打挂了，熔断，至少不要影响别的系统，隔离，你本身就独立的，但是你会调用其他的系统嘛，你快不行了你别拖累兄弟们啊。\n\n消息队列（削峰填谷）\n\n一说到这个名词，很多小伙伴就知道了，对的mq，你买东西少了你直接100个请求改库我觉得没问题，但是万一秒杀一万个，10万个呢？服务器挂了，程序员又要背锅的。\n\n秒杀就是这种瞬间流量很高，但是平时又没有流量的场景，那消息队列完全契合这样的场景了呀，削峰填谷。\n\n\n\ntip：可能小伙伴说我们业务达不到这个量级，没必要。但是我想说我们写代码，就不应该写出有逻辑漏洞的代码，至少以后公司体量上去了，别人一看居然不用改代码，一看代码作者是xxx？有点东西！\n\n你可以把它放消息队列，然后一点点消费去改库存就好了嘛，不过单个商品其实一次修改就够了，我这里说的是某个点多个商品一起秒杀的场景，像极了双十一零点。\n\n\n# 数据库\n\n数据库用mysql只要连接池设置合理一般问题是不大的，不过一般大公司不缺钱而且秒杀这样的活动十分频繁，我之前所在的公司就是这样秒杀特卖这样的场景一直都是不间断的。\n\n单独给秒杀建立一个数据库，为秒杀服务，表的设计也是竟可能的简单点，现在的互联网架构部署都是分库的。\n\n至于表就看大家怎么设计了，该设置索引的地方还是要设置索引的，建完后记得用explain看看sql的执行计划。（不了解的小伙伴也没事，mysql章节去康康）\n\n\n# 分布式事务\n\n这为啥我不放在后端而放到最后来讲呢？\n\n因为上面的任何一步都是可能出错的，而且我们是在不同的服务里面出错的，那就涉及分布式事务了，但是分布式事务大家想的是一定要成功什么的那就不对了，还是那句话，几个请求丢了就丢了，要保证时效和服务的可用可靠。\n\n所以tcc和最终一致性其实不是很适合，tcc开发成本很大，所有接口都要写三次，因为涉及tcc的三个阶段。\n\n最终一致性基本上都是靠轮训的操作去保证一个操作一定成功，那时效性就大打折扣了。\n\n大家觉得不那么可靠的**两段式（2pc）和三段式（3pc）**就派上用场了，他们不一定能保证数据最终一致，但是效率上还算ok。\n\n\n# 总结\n\n到这里我想我已经基本上把该考虑的点还有对应的解决方案也都说了一下，不知道还有没有没考虑到的，但是就算没考虑到我想我这个设计，应该也能撑住一个完整的秒杀流程。\n\n最后大家再看看这个秒杀系统或许会有新的感悟，是不是一个系统真的没有大家想的那么简单，而且我还是有漏掉的细节，这是一定的。\n\n\n# 参考文献\n\n面试了十个应届生九个都是秒杀系统，你确定你们那是秒杀？_小公司的qps只有几十-csdn博客",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"超卖",frontmatter:{title:"超卖",date:"2024-09-14T22:14:25.000Z",permalink:"/pages/8a57f2/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/06.%E8%B6%85%E5%8D%96.html",relativePath:"实战系统设计/07.经典场景设计/06.超卖.md",key:"v-a0c228a6",path:"/pages/8a57f2/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"实现方案",slug:"实现方案",normalizedTitle:"实现方案",charIndex:96},{level:3,title:"数据库扣减库存",slug:"数据库扣减库存",normalizedTitle:"数据库扣减库存",charIndex:105},{level:3,title:"redis扣减库存",slug:"redis扣减库存",normalizedTitle:"redis扣减库存",charIndex:764},{level:3,title:"lua脚本扣减库存",slug:"lua脚本扣减库存",normalizedTitle:"lua脚本扣减库存",charIndex:2141},{level:3,title:"redis decr + 分布式锁",slug:"redis-decr-分布式锁",normalizedTitle:"redis decr + 分布式锁",charIndex:2879},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:3513}],headersStr:"前言 实现方案 数据库扣减库存 redis扣减库存 lua脚本扣减库存 redis decr + 分布式锁 参考文献",content:'# 前言\n\n超卖问题，简单来说就是卖出的商品数量超出库存数\n\n一般是难点在于秒杀场景下的超卖\n\necho 觉得 在秒杀场景下这个库存是可以不用回退的，因为本就是瞬时的，回退也没有意义\n\n\n# 实现方案\n\n\n# 数据库扣减库存\n\nupdate product set stock=stock-1 where id=123;\n\n\n这种写法对于扣减库存是没有问题的，但如何控制库存不足的情况下，不让用户操作呢？\n\n这就需要在update之前，先查一下库存是否足够了。\n\n伪代码如下：\n\nint stock = mapper.getStockById(123);\nif(stock > 0) {\n  int count = mapper.updateStock(123);\n  if(count > 0) {\n    addOrder(123);\n  }\n}\n\n\n大家有没有发现这段代码的问题？\n\n没错，查询操作和更新操作不是原子性的，会导致在并发的场景下，出现库存超卖的情况。\n\n有人可能会说，这样好办，加把锁，不就搞定了，比如使用synchronized关键字。\n\n确实，可以，但是性能不够好。\n\n还有更优雅的处理方案，即基于数据库的乐观锁，这样会少一次数据库查询，而且能够天然的保证数据操作的原子性。\n\n只需将上面的sql稍微调整一下：\n\nupdate product set stock=stock-1 where id=product and stock > 0;\n\n\n在sql最后加上：stock > 0，就能保证不会出现超卖的情况。\n\n但需要频繁访问数据库，我们都知道数据库连接是非常昂贵的资源。在高并发的场景下，可能会造成系统雪崩。而且，容易出现多个请求，同时竞争行锁的情况，造成相互等待，从而出现死锁的问题。\n\n\n# redis扣减库存\n\nredis的incr方法是原子性的，可以用该方法扣减库存。\n\nboolean exist = redisClient.query(productId,userId);\nif(exist) {\n\treturn -1;\n}\nint stock = redisClient.queryStock(productId);\nif(stock <=0) {\n\treturn 0;\n}\nredisClient.incrby(productId, -1);\nredisClient.add(productId,userId);\nreturn 1;\n\n\n代码流程如下：\n\n 1. 先判断该用户有没有秒杀过该商品，如果已经秒杀过，则直接返回-1。\n 2. 查询库存，如果库存小于等于0，则直接返回0，表示库存不足。\n 3. 如果库存充足，则扣减库存，然后将本次秒杀记录保存起来。然后返回1，表示成功。\n\n估计很多小伙伴，一开始都会按这样的思路写代码。但如果仔细想想会发现，这段代码有问题。\n\n有什么问题呢？\n\n如果在高并发下，有多个请求同时查询库存，当时都大于0。由于查询库存和更新库存非原则操作，则会出现库存为负数的情况，即库存超卖。\n\n当然有人可能会说，加个synchronized不就解决问题？\n\n调整后代码如下：\n\nboolean exist = redisClient.query(productId,userId);\nif(exist) {\n\treturn -1;\n}\nsynchronized(this) {\nint stock = redisClient.queryStock(productId);\nif(stock <=0) {\n\treturn 0;\n}\nredisClient.incrby(productId, -1);\nredisClient.add(productId,userId);\n}\n\nreturn 1;\n\n\n加synchronized确实能解决库存为负数问题，但是这样会导致接口性能急剧下降，每次查询都需要竞争同一把锁，显然不太合理。\n\n为了解决上面的问题，代码优化如下：\n\nboolean exist = redisClient.query(productId,userId);\nif(exist) {\n  return -1;\n}\nif(redisClient.incrby(productId, -1)<0) {\n  return 0;\n}\nredisClient.add(productId,userId);\nreturn 1;\n\n\n该代码主要流程如下：\n\n 1. 先判断该用户有没有秒杀过该商品，如果已经秒杀过，则直接返回-1。\n 2. 扣减库存，判断返回值是否小于0，如果小于0，则直接返回0，表示库存不足。\n 3. 如果扣减库存后，返回值大于或等于0，则将本次秒杀记录保存起来。然后返回1，表示成功。\n\n该方案咋一看，好像没问题。\n\n但如果在高并发场景中，有多个请求同时扣减库存，大多数请求的incrby操作之后，结果都会小于0。\n\n虽说，库存出现负数，不会出现超卖的问题。但由于这里是预减库存，如果负数值负的太多的话，后面万一要回退库存时，就会导致库存不准。\n\n那么，有没有更好的方案呢？\n\n\n# lua脚本扣减库存\n\n我们都知道lua脚本，是能够保证原子性的，它跟redis一起配合使用，能够完美解决上面的问题。\n\nlua脚本有段非常经典的代码：\n\n  StringBuilder lua = new StringBuilder();\n  lua.append("if (redis.call(\'exists\', KEYS[1]) == 1) then");\n  lua.append("    local stock = tonumber(redis.call(\'get\', KEYS[1]));");\n  lua.append("    if (stock == -1) then");\n  lua.append("        return 1;");\n  lua.append("    end;");\n  lua.append("    if (stock > 0) then");\n  lua.append("        redis.call(\'incrby\', KEYS[1], -1);");\n  lua.append("        return stock;");\n  lua.append("    end;");\n  lua.append("    return 0;");\n  lua.append("end;");\n  lua.append("return -1;");\n\n\n该代码的主要流程如下：\n\n 1. 先判断商品id是否存在，如果不存在则直接返回。\n 2. 获取该商品id的库存，判断库存如果是-1，则直接返回，表示不限制库存。\n 3. 如果库存大于0，则扣减库存。\n 4. 如果库存等于0，是直接返回，表示库存不足。\n\n\n# redis decr + 分布式锁\n\n\n\n先用 decr 去扣减库存，然后用分布式锁，锁住当前库存，防止库存回退现象的发生\n\n 1. 在 redis 集群模式下【以我们的场景为例】，incr 请求操作也可能在请求时发生网络抖动超时返回。这个时候incr有可能成功，也有可能失败。可能是请求超时，也可能是请求完的应答超时。那么incr 的值可能就不准。【实际使用中10万次，可能会有10万零1和不足10万】，那么为了这样一个临界状态的可靠性，所以添加 setNx 加锁只有成功和失败。\n 2. setNx 因为是非独占锁，所以key不存在释放。setNx 的key 可以过期时间可以优化为活动的有效期时间为结束。—— 而独占锁，其实你永远也不好把握释放时间，因为秒杀都是瞬态的，释放的晚了活动用户都走了，释放的早了，流程可能还没处理完。\n 3. 对于 setNx 可能还有些时候，集群主从切换，或者活动出问题的时候恢复。如果恢复的 incr 值多了，那么有 setNx 锁拦截后，会更加可靠。\n 4. 关于库存恢复，一般这类抽奖都是瞬态的，且redis集群非常稳定。所以很少有需要恢复库存，如果需要恢复库存，那么是把失败的秒杀incr对应的值的key，加入到待消费队列中。等整体库存消耗后，开始消耗队列库存。\n 5. 这里的锁的颗粒度在于一个用户一个锁的key，所以没有个人释放再需要被让别人抢占的需要，因为这不是独占锁。所以锁的key可以设置活动结束后释放。\n\n\n# 参考文献\n\n面试必考：秒杀系统如何设计？-腾讯云开发者社区-腾讯云 (tencent.com)',normalizedContent:'# 前言\n\n超卖问题，简单来说就是卖出的商品数量超出库存数\n\n一般是难点在于秒杀场景下的超卖\n\necho 觉得 在秒杀场景下这个库存是可以不用回退的，因为本就是瞬时的，回退也没有意义\n\n\n# 实现方案\n\n\n# 数据库扣减库存\n\nupdate product set stock=stock-1 where id=123;\n\n\n这种写法对于扣减库存是没有问题的，但如何控制库存不足的情况下，不让用户操作呢？\n\n这就需要在update之前，先查一下库存是否足够了。\n\n伪代码如下：\n\nint stock = mapper.getstockbyid(123);\nif(stock > 0) {\n  int count = mapper.updatestock(123);\n  if(count > 0) {\n    addorder(123);\n  }\n}\n\n\n大家有没有发现这段代码的问题？\n\n没错，查询操作和更新操作不是原子性的，会导致在并发的场景下，出现库存超卖的情况。\n\n有人可能会说，这样好办，加把锁，不就搞定了，比如使用synchronized关键字。\n\n确实，可以，但是性能不够好。\n\n还有更优雅的处理方案，即基于数据库的乐观锁，这样会少一次数据库查询，而且能够天然的保证数据操作的原子性。\n\n只需将上面的sql稍微调整一下：\n\nupdate product set stock=stock-1 where id=product and stock > 0;\n\n\n在sql最后加上：stock > 0，就能保证不会出现超卖的情况。\n\n但需要频繁访问数据库，我们都知道数据库连接是非常昂贵的资源。在高并发的场景下，可能会造成系统雪崩。而且，容易出现多个请求，同时竞争行锁的情况，造成相互等待，从而出现死锁的问题。\n\n\n# redis扣减库存\n\nredis的incr方法是原子性的，可以用该方法扣减库存。\n\nboolean exist = redisclient.query(productid,userid);\nif(exist) {\n\treturn -1;\n}\nint stock = redisclient.querystock(productid);\nif(stock <=0) {\n\treturn 0;\n}\nredisclient.incrby(productid, -1);\nredisclient.add(productid,userid);\nreturn 1;\n\n\n代码流程如下：\n\n 1. 先判断该用户有没有秒杀过该商品，如果已经秒杀过，则直接返回-1。\n 2. 查询库存，如果库存小于等于0，则直接返回0，表示库存不足。\n 3. 如果库存充足，则扣减库存，然后将本次秒杀记录保存起来。然后返回1，表示成功。\n\n估计很多小伙伴，一开始都会按这样的思路写代码。但如果仔细想想会发现，这段代码有问题。\n\n有什么问题呢？\n\n如果在高并发下，有多个请求同时查询库存，当时都大于0。由于查询库存和更新库存非原则操作，则会出现库存为负数的情况，即库存超卖。\n\n当然有人可能会说，加个synchronized不就解决问题？\n\n调整后代码如下：\n\nboolean exist = redisclient.query(productid,userid);\nif(exist) {\n\treturn -1;\n}\nsynchronized(this) {\nint stock = redisclient.querystock(productid);\nif(stock <=0) {\n\treturn 0;\n}\nredisclient.incrby(productid, -1);\nredisclient.add(productid,userid);\n}\n\nreturn 1;\n\n\n加synchronized确实能解决库存为负数问题，但是这样会导致接口性能急剧下降，每次查询都需要竞争同一把锁，显然不太合理。\n\n为了解决上面的问题，代码优化如下：\n\nboolean exist = redisclient.query(productid,userid);\nif(exist) {\n  return -1;\n}\nif(redisclient.incrby(productid, -1)<0) {\n  return 0;\n}\nredisclient.add(productid,userid);\nreturn 1;\n\n\n该代码主要流程如下：\n\n 1. 先判断该用户有没有秒杀过该商品，如果已经秒杀过，则直接返回-1。\n 2. 扣减库存，判断返回值是否小于0，如果小于0，则直接返回0，表示库存不足。\n 3. 如果扣减库存后，返回值大于或等于0，则将本次秒杀记录保存起来。然后返回1，表示成功。\n\n该方案咋一看，好像没问题。\n\n但如果在高并发场景中，有多个请求同时扣减库存，大多数请求的incrby操作之后，结果都会小于0。\n\n虽说，库存出现负数，不会出现超卖的问题。但由于这里是预减库存，如果负数值负的太多的话，后面万一要回退库存时，就会导致库存不准。\n\n那么，有没有更好的方案呢？\n\n\n# lua脚本扣减库存\n\n我们都知道lua脚本，是能够保证原子性的，它跟redis一起配合使用，能够完美解决上面的问题。\n\nlua脚本有段非常经典的代码：\n\n  stringbuilder lua = new stringbuilder();\n  lua.append("if (redis.call(\'exists\', keys[1]) == 1) then");\n  lua.append("    local stock = tonumber(redis.call(\'get\', keys[1]));");\n  lua.append("    if (stock == -1) then");\n  lua.append("        return 1;");\n  lua.append("    end;");\n  lua.append("    if (stock > 0) then");\n  lua.append("        redis.call(\'incrby\', keys[1], -1);");\n  lua.append("        return stock;");\n  lua.append("    end;");\n  lua.append("    return 0;");\n  lua.append("end;");\n  lua.append("return -1;");\n\n\n该代码的主要流程如下：\n\n 1. 先判断商品id是否存在，如果不存在则直接返回。\n 2. 获取该商品id的库存，判断库存如果是-1，则直接返回，表示不限制库存。\n 3. 如果库存大于0，则扣减库存。\n 4. 如果库存等于0，是直接返回，表示库存不足。\n\n\n# redis decr + 分布式锁\n\n\n\n先用 decr 去扣减库存，然后用分布式锁，锁住当前库存，防止库存回退现象的发生\n\n 1. 在 redis 集群模式下【以我们的场景为例】，incr 请求操作也可能在请求时发生网络抖动超时返回。这个时候incr有可能成功，也有可能失败。可能是请求超时，也可能是请求完的应答超时。那么incr 的值可能就不准。【实际使用中10万次，可能会有10万零1和不足10万】，那么为了这样一个临界状态的可靠性，所以添加 setnx 加锁只有成功和失败。\n 2. setnx 因为是非独占锁，所以key不存在释放。setnx 的key 可以过期时间可以优化为活动的有效期时间为结束。—— 而独占锁，其实你永远也不好把握释放时间，因为秒杀都是瞬态的，释放的晚了活动用户都走了，释放的早了，流程可能还没处理完。\n 3. 对于 setnx 可能还有些时候，集群主从切换，或者活动出问题的时候恢复。如果恢复的 incr 值多了，那么有 setnx 锁拦截后，会更加可靠。\n 4. 关于库存恢复，一般这类抽奖都是瞬态的，且redis集群非常稳定。所以很少有需要恢复库存，如果需要恢复库存，那么是把失败的秒杀incr对应的值的key，加入到待消费队列中。等整体库存消耗后，开始消耗队列库存。\n 5. 这里的锁的颗粒度在于一个用户一个锁的key，所以没有个人释放再需要被让别人抢占的需要，因为这不是独占锁。所以锁的key可以设置活动结束后释放。\n\n\n# 参考文献\n\n面试必考：秒杀系统如何设计？-腾讯云开发者社区-腾讯云 (tencent.com)',charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"多级缓存",frontmatter:{title:"多级缓存",date:"2024-09-14T16:52:24.000Z",permalink:"/pages/51aa8b/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/07.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98.html",relativePath:"实战系统设计/07.经典场景设计/07.多级缓存.md",key:"v-330e9b4e",path:"/pages/51aa8b/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"幂等&防重",frontmatter:{title:"幂等&防重",date:"2024-09-14T16:52:57.000Z",permalink:"/pages/4fc8cb/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/09.%E5%B9%82%E7%AD%89&%E9%98%B2%E9%87%8D.html",relativePath:"实战系统设计/07.经典场景设计/09.幂等&防重.md",key:"v-5be28fe2",path:"/pages/4fc8cb/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"1. insert 前先 select",slug:"_1-insert-前先-select",normalizedTitle:"1. insert 前先 select",charIndex:653},{level:2,title:"2. 加悲观锁",slug:"_2-加悲观锁",normalizedTitle:"2. 加悲观锁",charIndex:881},{level:2,title:"3. 加乐观锁",slug:"_3-加乐观锁",normalizedTitle:"3. 加乐观锁",charIndex:1715},{level:2,title:"4. 加唯一索引",slug:"_4-加唯一索引",normalizedTitle:"4. 加唯一索引",charIndex:2528},{level:2,title:"5. 建防重表",slug:"_5-建防重表",normalizedTitle:"5. 建防重表",charIndex:3040},{level:2,title:"6. 根据状态机",slug:"_6-根据状态机",normalizedTitle:"6. 根据状态机",charIndex:3397},{level:2,title:"7. 加分布式锁",slug:"_7-加分布式锁",normalizedTitle:"7. 加分布式锁",charIndex:3982},{level:2,title:"8. 获取token",slug:"_8-获取token",normalizedTitle:"8. 获取token",charIndex:4516},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4950},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:5048}],headersStr:"前言 1. insert 前先 select 2. 加悲观锁 3. 加乐观锁 4. 加唯一索引 5. 建防重表 6. 根据状态机 7. 加分布式锁 8. 获取token 总结 参考文献",content:"# 前言\n\n接口幂等性问题，对于开发人员来说，是一个跟语言无关的公共问题。本文分享了一些解决这类问题非常实用的办法，绝大部分内容我在项目中实践过的，给有需要的小伙伴一个参考。\n\n不知道你有没有遇到过这些场景：\n\n 1. 有时我们在填写某些form表单时，保存按钮不小心快速点了两次，表中竟然产生了两条重复的数据，只是id不一样。\n 2. 我们在项目中为了解决接口超时问题，通常会引入了重试机制。第一次请求接口超时了，请求方没能及时获取返回结果（此时有可能已经成功了），为了避免返回错误的结果（这种情况不可能直接返回失败吧？），于是会对该请求重试几次，这样也会产生重复的数据。\n 3. mq消费者在读取消息时，有时候会读取到重复消息（至于什么原因这里先不说，有兴趣的小伙伴，可以找我私聊），如果处理不好，也会产生重复的数据。\n\n没错，这些都是幂等性问题。\n\n接口幂等性是指用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。\n\n这类问题多发于接口的：\n\n * insert操作，这种情况下多次请求，可能会产生重复数据。\n * update操作，如果只是单纯的更新数据，比如：update user set status=1 where id=1，是没有问题的。如果还有计算，比如：update user set status=status+1 where id=1，这种情况下多次请求，可能会导致数据错误。\n\n那么我们要如何保证接口幂等性？本文将会告诉你答案。\n\n\n# 1. insert 前先 select\n\n通常情况下，在保存数据的接口中，我们为了防止产生重复数据，一般会在insert前，先根据name或code字段select一下数据。如果该数据已存在，则执行update操作，如果不存在，才执行 insert操作。\n\n\n\n该方案可能是我们平时在防止产生重复数据时，使用最多的方案。但是该方案不适用于并发场景，在并发场景中，要配合其他方案一起使用，否则同样会产生重复数据。我在这里提一下，是为了避免大家踩坑。\n\n\n# 2. 加悲观锁\n\n在支付场景中，用户A的账号余额有150元，想转出100元，正常情况下用户A的余额只剩50元。一般情况下，sql是这样的：\n\nupdate user amount = amount-100 where id=123;\n\n\n如果出现多次相同的请求，可能会导致用户A的余额变成负数。这种情况，用户A来可能要哭了。于此同时，系统开发人员可能也要哭了，因为这是很严重的系统bug。\n\n为了解决这个问题，可以加悲观锁，将用户A的那行数据锁住，在同一时刻只允许一个请求获得锁，更新数据，其他的请求则等待。\n\n通常情况下通过如下sql锁住单行数据：\n\nselect * from user id=123 for update;\n\n\n具体流程如下：\n\n\n\n具体步骤：\n\n 1. 多个请求同时根据id查询用户信息。\n 2. 判断余额是否不足100，如果余额不足，则直接返回余额不足。\n 3. 如果余额充足，则通过for update再次查询用户信息，并且尝试获取锁。\n 4. 只有第一个请求能获取到行锁，其余没有获取锁的请求，则等待下一次获取锁的机会。\n 5. 第一个请求获取到锁之后，判断余额是否不足100，如果余额足够，则进行update操作。\n 6. 如果余额不足，说明是重复请求，则直接返回成功。\n\n> 需要特别注意的是：如果使用的是mysql数据库，存储引擎必须用innodb，因为它才支持事务。此外，这里id字段一定要是主键或者唯一索引，不然会锁住整张表。\n\n悲观锁需要在同一个事务操作过程中锁住一行数据，如果事务耗时比较长，会造成大量的请求等待，影响接口性能。此外，每次请求接口很难保证都有相同的返回值，所以不适合幂等性设计场景，但是在防重场景中是可以的使用的。在这里顺便说一下，防重设计 和 幂等设计，其实是有区别的。防重设计主要为了避免产生重复数据，对接口返回没有太多要求。而幂等设计除了避免产生重复数据之外，还要求每次请求都返回一样的结果。\n\n\n# 3. 加乐观锁\n\n既然悲观锁有性能问题，为了提升接口性能，我们可以使用乐观锁。需要在表中增加一个timestamp或者version字段，这里以version字段为例。\n\n在更新数据之前先查询一下数据：\n\nselect id,amount,version from user id=123;\n\n\n如果数据存在，假设查到的version等于1，再使用id和version字段作为查询条件更新数据：\n\nupdate user set amount=amount+100,version=version+1\nwhere id=123 and version=1;\n\n\n更新数据的同时version+1，然后判断本次update操作的影响行数，如果大于0，则说明本次更新成功，如果等于0，则说明本次更新没有让数据变更。\n\n由于第一次请求version等于1是可以成功的，操作成功后version变成2了。这时如果并发的请求过来，再执行相同的sql：\n\nupdate user set amount=amount+100,version=version+1\nwhere id=123 and version=1;\n\n\n该update操作不会真正更新数据，最终sql的执行结果影响行数是0，因为version已经变成2了，where中的version=1肯定无法满足条件。但为了保证接口幂等性，接口可以直接返回成功，因为version值已经修改了，那么前面必定已经成功过一次，后面都是重复的请求。\n\n具体流程如下：\n\n\n\n具体步骤：\n\n 1. 先根据id查询用户信息，包含version字段\n 2. 根据id和version字段值作为where条件的参数，更新用户信息，同时version+1\n 3. 判断操作影响行数，如果影响1行，则说明是一次请求，可以做其他数据操作。\n 4. 如果影响0行，说明是重复请求，则直接返回成功。\n\n\n# 4. 加唯一索引\n\n绝大数情况下，为了防止重复数据的产生，我们都会在表中加唯一索引，这是一个非常简单，并且有效的方案。\n\nalter table `order` add UNIQUE KEY `un_code` (`code`);\n\n\n加了唯一索引之后，第一次请求数据可以插入成功。但后面的相同请求，插入数据时会报Duplicate entry '002' for key 'order.un_code异常，表示唯一索引有冲突。\n\n虽说抛异常对数据来说没有影响，不会造成错误数据。但是为了保证接口幂等性，我们需要对该异常进行捕获，然后返回成功。\n\n如果是java程序需要捕获：DuplicateKeyException异常，如果使用了spring框架还需要捕获：MySQLIntegrityConstraintViolationException异常。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 将该数据插入mysql\n 3. 判断是否执行成功，如果成功，则操作其他数据（可能还有其他的业务逻辑）。\n 4. 如果执行失败，捕获唯一索引冲突异常，直接返回成功。\n\n\n# 5. 建防重表\n\n有时候表中并非所有的场景都不允许产生重复的数据，只有某些特定场景才不允许。这时候，直接在表中加唯一索引，显然是不太合适的。\n\n针对这种情况，我们可以通过建防重表来解决问题。\n\n该表可以只包含两个字段：id 和 唯一索引，唯一索引可以是多个字段比如：name、code等组合起来的唯一标识，例如：susan_0001。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 将该数据插入mysql防重表\n 3. 判断是否执行成功，如果成功，则做mysql其他的数据操作（可能还有其他的业务逻辑）。\n 4. 如果执行失败，捕获唯一索引冲突异常，直接返回成功。\n\n> 需要特别注意的是：防重表和业务表必须在同一个数据库中，并且操作要在同一个事务中。\n\n\n# 6. 根据状态机\n\n很多时候业务表是有状态的，比如订单表中有：1-下单、2-已支付、3-完成、4-撤销等状态。如果这些状态的值是有规律的，按照业务节点正好是从小到大，我们就能通过它来保证接口的幂等性。\n\n假如id=123的订单状态是已支付，现在要变成完成状态。\n\nupdate `order` set status=3 where id=123 and status=2;\n\n\n第一次请求时，该订单的状态是已支付，值是2，所以该update语句可以正常更新数据，sql执行结果的影响行数是1，订单状态变成了3。\n\n后面有相同的请求过来，再执行相同的sql时，由于订单状态变成了3，再用status=2作为条件，无法查询出需要更新的数据，所以最终sql执行结果的影响行数是0，即不会真正的更新数据。但为了保证接口幂等性，影响行数是0时，接口也可以直接返回成功。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 根据id和当前状态作为条件，更新成下一个状态\n 3. 判断操作影响行数，如果影响了1行，说明当前操作成功，可以进行其他数据操作。\n 4. 如果影响了0行，说明是重复请求，直接返回成功。\n\n> 主要特别注意的是，该方案仅限于要更新的表有状态字段，并且刚好要更新状态字段的这种特殊情况，并非所有场景都适用。\n\n\n# 7. 加分布式锁\n\n其实前面介绍过的加唯一索引或者加防重表，本质是使用了数据库的分布式锁，也属于分布式锁的一种。但由于数据库分布式锁的性能不太好，我们可以改用：redis或zookeeper。\n\n鉴于现在很多公司分布式配置中心改用apollo或nacos，已经很少用zookeeper了，我们以redis为例介绍分布式锁。\n\n目前主要有三种方式实现redis的分布式锁：\n\n 1. setNx命令\n 2. set命令\n 3. Redission框架\n\n每种方案各有利弊，具体实现细节我就不说了，有兴趣的朋友可以加我微信找我私聊。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端会收集数据，并且生成订单号code作为唯一业务字段。\n 2. 使用redis的set命令，将该订单code设置到redis中，同时设置超时时间。\n 3. 判断是否设置成功，如果设置成功，说明是第一次请求，则进行数据操作。\n 4. 如果设置失败，说明是重复请求，则直接返回成功。\n\n> 需要特别注意的是：分布式锁一定要设置一个合理的过期时间，如果设置过短，无法有效的防止重复请求。如果设置过长，可能会浪费redis的存储空间，需要根据实际业务情况而定。\n\n\n# 8. 获取token\n\n除了上述方案之外，还有最后一种使用token的方案。该方案跟之前的所有方案都有点不一样，需要两次请求才能完成一次业务操作。\n\n 1. 第一次请求获取token\n 2. 第二次请求带着这个token，完成业务操作。\n\n具体流程图如下：\n\n第一步，先获取token。\n\n\n\n第二步，做具体业务操作。\n\n\n\n具体步骤：\n\n 1. 用户访问页面时，浏览器自动发起获取token请求。\n 2. 服务端生成token，保存到redis中，然后返回给浏览器。\n 3. 用户通过浏览器发起请求时，携带该token。\n 4. 在redis中查询该token是否存在，如果不存在，说明是第一次请求，做则后续的数据操作。\n 5. 如果存在，说明是重复请求，则直接返回成功。\n 6. 在redis中token会在过期时间之后，被自动删除。\n\n以上方案是针对幂等设计的。\n\n如果是防重设计，流程图要改改：\n\n\n\n> 需要特别注意的是：token必须是全局唯一的\n\n\n# 总结\n\n 1. insert 前先 select\n 2. 加悲观锁\n 3. 加乐观锁\n 4. 加唯一索引\n 5. 建防重表\n 6. 根据状态机\n 7. 加分布式锁\n 8. 获取token\n\n\n# 参考文献\n\n高并发下如何保证接口的幂等性？ - 苏三说技术 - 博客园 (cnblogs.com)",normalizedContent:"# 前言\n\n接口幂等性问题，对于开发人员来说，是一个跟语言无关的公共问题。本文分享了一些解决这类问题非常实用的办法，绝大部分内容我在项目中实践过的，给有需要的小伙伴一个参考。\n\n不知道你有没有遇到过这些场景：\n\n 1. 有时我们在填写某些form表单时，保存按钮不小心快速点了两次，表中竟然产生了两条重复的数据，只是id不一样。\n 2. 我们在项目中为了解决接口超时问题，通常会引入了重试机制。第一次请求接口超时了，请求方没能及时获取返回结果（此时有可能已经成功了），为了避免返回错误的结果（这种情况不可能直接返回失败吧？），于是会对该请求重试几次，这样也会产生重复的数据。\n 3. mq消费者在读取消息时，有时候会读取到重复消息（至于什么原因这里先不说，有兴趣的小伙伴，可以找我私聊），如果处理不好，也会产生重复的数据。\n\n没错，这些都是幂等性问题。\n\n接口幂等性是指用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。\n\n这类问题多发于接口的：\n\n * insert操作，这种情况下多次请求，可能会产生重复数据。\n * update操作，如果只是单纯的更新数据，比如：update user set status=1 where id=1，是没有问题的。如果还有计算，比如：update user set status=status+1 where id=1，这种情况下多次请求，可能会导致数据错误。\n\n那么我们要如何保证接口幂等性？本文将会告诉你答案。\n\n\n# 1. insert 前先 select\n\n通常情况下，在保存数据的接口中，我们为了防止产生重复数据，一般会在insert前，先根据name或code字段select一下数据。如果该数据已存在，则执行update操作，如果不存在，才执行 insert操作。\n\n\n\n该方案可能是我们平时在防止产生重复数据时，使用最多的方案。但是该方案不适用于并发场景，在并发场景中，要配合其他方案一起使用，否则同样会产生重复数据。我在这里提一下，是为了避免大家踩坑。\n\n\n# 2. 加悲观锁\n\n在支付场景中，用户a的账号余额有150元，想转出100元，正常情况下用户a的余额只剩50元。一般情况下，sql是这样的：\n\nupdate user amount = amount-100 where id=123;\n\n\n如果出现多次相同的请求，可能会导致用户a的余额变成负数。这种情况，用户a来可能要哭了。于此同时，系统开发人员可能也要哭了，因为这是很严重的系统bug。\n\n为了解决这个问题，可以加悲观锁，将用户a的那行数据锁住，在同一时刻只允许一个请求获得锁，更新数据，其他的请求则等待。\n\n通常情况下通过如下sql锁住单行数据：\n\nselect * from user id=123 for update;\n\n\n具体流程如下：\n\n\n\n具体步骤：\n\n 1. 多个请求同时根据id查询用户信息。\n 2. 判断余额是否不足100，如果余额不足，则直接返回余额不足。\n 3. 如果余额充足，则通过for update再次查询用户信息，并且尝试获取锁。\n 4. 只有第一个请求能获取到行锁，其余没有获取锁的请求，则等待下一次获取锁的机会。\n 5. 第一个请求获取到锁之后，判断余额是否不足100，如果余额足够，则进行update操作。\n 6. 如果余额不足，说明是重复请求，则直接返回成功。\n\n> 需要特别注意的是：如果使用的是mysql数据库，存储引擎必须用innodb，因为它才支持事务。此外，这里id字段一定要是主键或者唯一索引，不然会锁住整张表。\n\n悲观锁需要在同一个事务操作过程中锁住一行数据，如果事务耗时比较长，会造成大量的请求等待，影响接口性能。此外，每次请求接口很难保证都有相同的返回值，所以不适合幂等性设计场景，但是在防重场景中是可以的使用的。在这里顺便说一下，防重设计 和 幂等设计，其实是有区别的。防重设计主要为了避免产生重复数据，对接口返回没有太多要求。而幂等设计除了避免产生重复数据之外，还要求每次请求都返回一样的结果。\n\n\n# 3. 加乐观锁\n\n既然悲观锁有性能问题，为了提升接口性能，我们可以使用乐观锁。需要在表中增加一个timestamp或者version字段，这里以version字段为例。\n\n在更新数据之前先查询一下数据：\n\nselect id,amount,version from user id=123;\n\n\n如果数据存在，假设查到的version等于1，再使用id和version字段作为查询条件更新数据：\n\nupdate user set amount=amount+100,version=version+1\nwhere id=123 and version=1;\n\n\n更新数据的同时version+1，然后判断本次update操作的影响行数，如果大于0，则说明本次更新成功，如果等于0，则说明本次更新没有让数据变更。\n\n由于第一次请求version等于1是可以成功的，操作成功后version变成2了。这时如果并发的请求过来，再执行相同的sql：\n\nupdate user set amount=amount+100,version=version+1\nwhere id=123 and version=1;\n\n\n该update操作不会真正更新数据，最终sql的执行结果影响行数是0，因为version已经变成2了，where中的version=1肯定无法满足条件。但为了保证接口幂等性，接口可以直接返回成功，因为version值已经修改了，那么前面必定已经成功过一次，后面都是重复的请求。\n\n具体流程如下：\n\n\n\n具体步骤：\n\n 1. 先根据id查询用户信息，包含version字段\n 2. 根据id和version字段值作为where条件的参数，更新用户信息，同时version+1\n 3. 判断操作影响行数，如果影响1行，则说明是一次请求，可以做其他数据操作。\n 4. 如果影响0行，说明是重复请求，则直接返回成功。\n\n\n# 4. 加唯一索引\n\n绝大数情况下，为了防止重复数据的产生，我们都会在表中加唯一索引，这是一个非常简单，并且有效的方案。\n\nalter table `order` add unique key `un_code` (`code`);\n\n\n加了唯一索引之后，第一次请求数据可以插入成功。但后面的相同请求，插入数据时会报duplicate entry '002' for key 'order.un_code异常，表示唯一索引有冲突。\n\n虽说抛异常对数据来说没有影响，不会造成错误数据。但是为了保证接口幂等性，我们需要对该异常进行捕获，然后返回成功。\n\n如果是java程序需要捕获：duplicatekeyexception异常，如果使用了spring框架还需要捕获：mysqlintegrityconstraintviolationexception异常。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 将该数据插入mysql\n 3. 判断是否执行成功，如果成功，则操作其他数据（可能还有其他的业务逻辑）。\n 4. 如果执行失败，捕获唯一索引冲突异常，直接返回成功。\n\n\n# 5. 建防重表\n\n有时候表中并非所有的场景都不允许产生重复的数据，只有某些特定场景才不允许。这时候，直接在表中加唯一索引，显然是不太合适的。\n\n针对这种情况，我们可以通过建防重表来解决问题。\n\n该表可以只包含两个字段：id 和 唯一索引，唯一索引可以是多个字段比如：name、code等组合起来的唯一标识，例如：susan_0001。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 将该数据插入mysql防重表\n 3. 判断是否执行成功，如果成功，则做mysql其他的数据操作（可能还有其他的业务逻辑）。\n 4. 如果执行失败，捕获唯一索引冲突异常，直接返回成功。\n\n> 需要特别注意的是：防重表和业务表必须在同一个数据库中，并且操作要在同一个事务中。\n\n\n# 6. 根据状态机\n\n很多时候业务表是有状态的，比如订单表中有：1-下单、2-已支付、3-完成、4-撤销等状态。如果这些状态的值是有规律的，按照业务节点正好是从小到大，我们就能通过它来保证接口的幂等性。\n\n假如id=123的订单状态是已支付，现在要变成完成状态。\n\nupdate `order` set status=3 where id=123 and status=2;\n\n\n第一次请求时，该订单的状态是已支付，值是2，所以该update语句可以正常更新数据，sql执行结果的影响行数是1，订单状态变成了3。\n\n后面有相同的请求过来，再执行相同的sql时，由于订单状态变成了3，再用status=2作为条件，无法查询出需要更新的数据，所以最终sql执行结果的影响行数是0，即不会真正的更新数据。但为了保证接口幂等性，影响行数是0时，接口也可以直接返回成功。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 根据id和当前状态作为条件，更新成下一个状态\n 3. 判断操作影响行数，如果影响了1行，说明当前操作成功，可以进行其他数据操作。\n 4. 如果影响了0行，说明是重复请求，直接返回成功。\n\n> 主要特别注意的是，该方案仅限于要更新的表有状态字段，并且刚好要更新状态字段的这种特殊情况，并非所有场景都适用。\n\n\n# 7. 加分布式锁\n\n其实前面介绍过的加唯一索引或者加防重表，本质是使用了数据库的分布式锁，也属于分布式锁的一种。但由于数据库分布式锁的性能不太好，我们可以改用：redis或zookeeper。\n\n鉴于现在很多公司分布式配置中心改用apollo或nacos，已经很少用zookeeper了，我们以redis为例介绍分布式锁。\n\n目前主要有三种方式实现redis的分布式锁：\n\n 1. setnx命令\n 2. set命令\n 3. redission框架\n\n每种方案各有利弊，具体实现细节我就不说了，有兴趣的朋友可以加我微信找我私聊。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端会收集数据，并且生成订单号code作为唯一业务字段。\n 2. 使用redis的set命令，将该订单code设置到redis中，同时设置超时时间。\n 3. 判断是否设置成功，如果设置成功，说明是第一次请求，则进行数据操作。\n 4. 如果设置失败，说明是重复请求，则直接返回成功。\n\n> 需要特别注意的是：分布式锁一定要设置一个合理的过期时间，如果设置过短，无法有效的防止重复请求。如果设置过长，可能会浪费redis的存储空间，需要根据实际业务情况而定。\n\n\n# 8. 获取token\n\n除了上述方案之外，还有最后一种使用token的方案。该方案跟之前的所有方案都有点不一样，需要两次请求才能完成一次业务操作。\n\n 1. 第一次请求获取token\n 2. 第二次请求带着这个token，完成业务操作。\n\n具体流程图如下：\n\n第一步，先获取token。\n\n\n\n第二步，做具体业务操作。\n\n\n\n具体步骤：\n\n 1. 用户访问页面时，浏览器自动发起获取token请求。\n 2. 服务端生成token，保存到redis中，然后返回给浏览器。\n 3. 用户通过浏览器发起请求时，携带该token。\n 4. 在redis中查询该token是否存在，如果不存在，说明是第一次请求，做则后续的数据操作。\n 5. 如果存在，说明是重复请求，则直接返回成功。\n 6. 在redis中token会在过期时间之后，被自动删除。\n\n以上方案是针对幂等设计的。\n\n如果是防重设计，流程图要改改：\n\n\n\n> 需要特别注意的是：token必须是全局唯一的\n\n\n# 总结\n\n 1. insert 前先 select\n 2. 加悲观锁\n 3. 加乐观锁\n 4. 加唯一索引\n 5. 建防重表\n 6. 根据状态机\n 7. 加分布式锁\n 8. 获取token\n\n\n# 参考文献\n\n高并发下如何保证接口的幂等性？ - 苏三说技术 - 博客园 (cnblogs.com)",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"超时&重试",frontmatter:{title:"超时&重试",date:"2024-09-14T16:52:35.000Z",permalink:"/pages/0dfb49/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/08.%E8%B6%85%E6%97%B6&%E9%87%8D%E8%AF%95.html",relativePath:"实战系统设计/07.经典场景设计/08.超时&重试.md",key:"v-4971f274",path:"/pages/0dfb49/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:35},{level:3,title:"重试的风险",slug:"重试的风险",normalizedTitle:"重试的风险",charIndex:147},{level:3,title:"重试的使用成本",slug:"重试的使用成本",normalizedTitle:"重试的使用成本",charIndex:754},{level:2,title:"重试治理",slug:"重试治理",normalizedTitle:"重试治理",charIndex:1089},{level:3,title:"动态配置",slug:"动态配置",normalizedTitle:"动态配置",charIndex:1191},{level:3,title:"退避策略",slug:"退避策略",normalizedTitle:"退避策略",charIndex:1835},{level:3,title:"防止 retry storm",slug:"防止-retry-storm",normalizedTitle:"防止 retry storm",charIndex:2122},{level:4,title:"限制单点重试",slug:"限制单点重试",normalizedTitle:"限制单点重试",charIndex:2176},{level:4,title:"限制链路重试",slug:"限制链路重试",normalizedTitle:"限制链路重试",charIndex:2562},{level:4,title:"超时处理",slug:"超时处理",normalizedTitle:"超时处理",charIndex:3355},{level:4,title:"超时场景优化",slug:"超时场景优化",normalizedTitle:"超时场景优化",charIndex:4118},{level:4,title:"结合 DDL",slug:"结合-ddl",normalizedTitle:"结合 ddl",charIndex:5186},{level:4,title:"实际的链路放大效应",slug:"实际的链路放大效应",normalizedTitle:"实际的链路放大效应",charIndex:5645},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6011},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:6196}],headersStr:"前言 重试的风险 重试的使用成本 重试治理 动态配置 退避策略 防止 retry storm 限制单点重试 限制链路重试 超时处理 超时场景优化 结合 DDL 实际的链路放大效应 总结 参考文献",content:"下述是引用字节某大佬的一篇文章，讲的是如何在微服务中进行重试\n\n\n# 前言\n\n在微服务架构中，一个大系统被拆分成多个小服务，小服务之间大量 RPC 调用，经常可能因为网络抖动等原因导致 RPC 调用失败，这时候使用重试机制可以提高请求的最终成功率，减少故障影响，让系统运行更稳定。\n\n\n\n\n# 重试的风险\n\n重试能够提高服务稳定性，但是一般情况下大家都不会轻易去重试，或者说不敢重试，主要是因为重试有放大故障的风险。\n\n首先，重试会加大直接下游的负载。如下图，假设 A 服务调用 B 服务，重试次数设置为 r（包括首次请求），当 B 高负载时很可能调用不成功，这时 A 调用失败重试 B ，B 服务的被调用量快速增大，最坏情况下可能放大到 r 倍，不仅不能请求成功，还可能导致 B 的负载继续升高，甚至直接打挂。\n\n\n\n更可怕的是，重试还会存在链路放大的效应，结合下图说明一下：\n\n\n\n假设现在场景是 Backend A 调用 Backend B，Backend B 调用 DB Frontend，均设置重试次数为 3 。如果 Backend B 调用 DB Frontend，请求 3 次都失败了，这时 Backend B 会给 Backend A 返回失败。但是 Backend A 也有重试的逻辑，Backend A 重试 Backend B 三次，每一次 Backend B 都会请求 DB Frontend 3 次，这样算起来，DB Frontend 就会被请求了 9 次，实际是指数级扩大。假设正常访问量是 n，链路一共有 m 层，每层重试次数为 r，则最后一层受到的访问量最大，为 n * r ^ (m - 1) 。这种指数放大的效应很可怕，可能导致链路上多层都被打挂，整个系统雪崩。\n\n\n# 重试的使用成本\n\n另外使用重试的成本也比较高。之前在字节跳动的内部框架和服务治理平台中都没有支持重试，在一些很需要重试的业务场景下（比如调用一些第三方业务经常失败），业务方可能用简单 for 循环来实现，基本不会考虑重试的放大效应，这样很不安全，公司内部出现过多次因为重试而导致的事故，且出事故的时候还需要修改代码上线才能关闭重试，导致事故恢复也不迅速。\n\n另外也有一些业务使用开源的重试组件，这些组件通常会考虑对直接下游的保护，但不会考虑链路级别的重试放大，另外需要业务方修改 RPC 调用代码才能使用，对业务代码入侵较多，而且也是静态配置，需要修改配置时都必须重新上线。\n\n基于以上的背景，为了让业务方能够灵活安全的使用重试，我们字节跳动直播中台团队设计和实现了一个重试治理组件，具有以下优点：\n\n 1. 能够在链路级别防重试风暴。\n\n 2. 保证易用性，业务接入成本小。\n\n 3. 具有灵活性，能够动态调整配置。\n\n下面介绍具体的实现方案。\n\n\n# 重试治理\n\n\n# 动态配置\n\n如何让业务方简单接入是首先要解决的问题。如果还是普通组件库的方式，依旧免不了要大量入侵用户代码，且很难动态调整。\n\n字节跳动的 Golang 开发框架支持中间件 (Milddleware) 模式，可以注册多个自定义 Middleware 并依次递归调用，通常是用于完成打印日志、上报监控等非业务逻辑，能够有效将业务和非业务代码功能进行解耦。因此我们决定使用 Middleware 的方式来实现重试功能，定义一个 Middleware 并在内部实现对 RPC 的重复调用，把重试的配置信息用字节跳动的分布式配置存储中心存储，这样 Middleware 中能够读取配置中心的配置并进行重试，对用户来说不需要修改调用 RPC 的代码，而只需要在服务中引入一个全局的 Middleware 即可。\n\n如下面的整体架构图所示，我们提供配置的网页和后台，用户能够在专门进行服务治理的页面上很方便的对 RPC 进行配置修改并自动生效，内部的实现逻辑对用户透明，对业务代码无入侵。\n\n\n\n配置的维度按照字节跳动的 RPC 调用特点，选定 [调用方服务，调用方集群，被调用服务， 被调用方法] 为一个元组，按照元组来进行配置。Middleware 中封装了读取配置的方法，在 RPC 调用的时候会自动读取并生效。\n\n这种 Middleware 的方式能够让业务方很容易接入，相对于之前普通组件库的方式要方便很多，并且一次接入以后就具有动态配置的能力，可能很方便地调整或者关闭重试配置。\n\n\n# 退避策略\n\n确定了接入方式以后就可以开始实现重试组件的具体功能，一个重试组件所包含的基本功能中，除了重试次数和总延时这样的基础配置外，还需要有退避策略。\n\n对于一些暂时性的错误，如网络抖动等，可能立即重试还是会失败，通常等待一小会儿再重试的话成功率会较高，并且也可能打散上游重试的时间，较少因为同时都重试而导致的下游瞬间流量高峰。决定等待多久之后再重试的方法叫做退避策略，我们实现了常见的退避策略，如：\n\n * 线性退避：每次等待固定时间后重试。\n\n * 随机退避：在一定范围内随机等待一个时间后重试。\n\n * 指数退避：连续重试时，每次等待时间都是前一次的倍数。\n\n\n# 防止 retry storm\n\n如何安全重试，防止 retry storm 是我们面临的最大的难题。\n\n# 限制单点重试\n\n首先要在单点进行限制，一个服务不能不受限制的重试下游，很容易造成下游被打挂。除了限制用户设定的重试次数上限外，更重要的是限制重试请求的成功率。\n\n实现的方案很简单，基于断路器的思想，限制 请求失败/请求成功 的比率，给重试增加熔断功能。我们采用了常见的滑动窗口的方法来实现，如下图，内存中为每一类 RPC 调用维护一个滑动窗口，比如窗口分 10 个 bucket ，每个 bucket 里面记录了 1s 内 RPC 的请求结果数据（成功、失败）。新的一秒到来时，生成新的 bucket ，并淘汰最早的一个 bucket ，只维持 10s 的数据。在新请求这个 RPC 失败时，根据前 10s 内的 失败/成功 是否超过阈值来判断是否可以重试。默认阈值是 0.1 ，即下游最多承受 1.1 倍的 QPS ，用户可以根据需要自行调整熔断开关和阈值。\n\n\n\n# 限制链路重试\n\n前面说过在多级链路中如果每层都配置重试可能导致调用量指数级扩大，虽然有了重试熔断之后，重试不再是指数增长(每一单节点重试扩大限制了 1.1 倍)，但还是会随着链路的级数增长而扩大调用次数，因此还是需要从链路层面来考虑重试的安全性。\n\n链路层面的防重试风暴的核心是限制每层都发生重试，理想情况下只有最下一层发生重试。Google SRE 中指出了 Google 内部使用特殊错误码的方式来实现：\n\n * 统一约定一个特殊的 status code ，它表示：调用失败，但别重试。\n\n * 任何一级重试失败后，生成该 status code 并返回给上层。\n\n * 上层收到该 status code 后停止对这个下游的重试，并将错误码再传给自己的上层。\n\n这种方式理想情况下只有最下一层发生重试，它的上游收到错误码后都不会重试，链路整体放大倍数也就是 r 倍(单层的重试次数)。但是这种策略依赖于业务方传递错误码，对业务代码有一定入侵，而且通常业务方的代码差异很大，调用 RPC 的方式和场景也各不相同，需要业务方配合进行大量改造，很可能因为漏改等原因导致没有把从下游拿到的错误码传递给上游。\n\n好在字节跳动内部用的 RPC 协议中有扩展字段，我们在 Middleware 中做了很多尝试，封装了错误码处理和传递的逻辑，在 RPC 的 Response 扩展字段中传递错误码标识 nomore_retry ，它告诉上游不要再重试了。Middleware 完成错误码的生成、识别、传递等整个生命周期的管理，不需要业务方修改本身的 RPC 逻辑，错误码的方案对业务来说是透明的。\n\n\n\n在链路中，推进每层都接入重试组件，这样每一层都可以通过识别这个标志位来停止重试，并逐层往上传递，上层也都停止重试，做到链路层面的防护，达到“只有最靠近错误发生的那一层才重试”的效果。\n\n# 超时处理\n\n在测试错误码上传的方案时，我们发现超时的情况可能导致传递错误码的方案失效。\n\n对于 A -> B -> C 的场景，假设 B -> C 超时，B 重试请求 C ，这时候很可能 A -> B 也超时了，所以 A 没有拿到 B 返回的错误码，而是也会重试 B , 这个时候虽然 B 重试 C 且生成了重试失败的错误码，但是却不能再传递给 A 。这种情况下，A 还是会重试 B ，如果链路中每一层都超时，那么还是会出现链路指数扩大的效应。\n\n因此为了处理这种情况，除了下游传递重试错误标志以外，我们还实现了“对重试请求不重试”的方案。\n\n对于重试的请求，我们在 Request 中打上一个特殊的 retry flag ，在上面 A -> B -> C 的链路，当 B 收到 A 的请求时会先读取这个 flag 判断这个请求是不是重试请求，如果是，那它调用 C 即使失败也不会重试；否则调用 C 失败后会重试 C 。同时 B 也会把这个 retry flag 下传，它发出的请求也会有这个标志，它的下游也不会再对这个请求重试。\n\n\n\n这样即使 A 因为超时而拿不到 B 的返回，对 B 发出重试请求后，B 能感知到并且不会对 C 重试，这样 A 最多请求 r 次，B 最多请求 r + r - 1，如果后面还有更下层次的话，C 最多请求 r + r + r - 2 次， 第 i 层最多请求 i * r - (i-1) 次，最坏情况下是倍数增长，不是指数增长了。加上实际还有重试熔断的限制，增长的幅度要小很多。\n\n通过重试熔断来限制单点的放大倍数，通过重试错误标志链路回传的方式来保证只有最下层发生重试，又通过重试请求 flag 链路下传的方式来保证对重试请求不重试，多种控制策略结合，可以有效地较少重试放大效应。\n\n# 超时场景优化\n\n分布式系统中，RPC 请求的结果有三种状态：成功、失败、超时，其中最难处理的就是超时的情况。但是超时往往又是最经常发生的那一个，我们统计了字节跳动直播业务线上一些重要服务的 RPC 错误分布，发现占比最高的就是超时错误，怕什么偏来什么。\n\n在超时重试的场景中，虽然给重试请求添加 retry flag 能防止指数扩大，但是却不能提高请求成功率。如下图，假如 A 和 B 的超时时间都是 1000ms ，当 C 负载很高导致 B 访问 C 超时，这时 B 会重试 C ，但是时间已经超过了 1000ms ，时间 A 这里也超时了并且断开了和 B 的连接，所以 B 这次重试 C 不管是否成功都是无用功，从 A 的视角看，本次请求已经失败了。\n\n\n\n这种情况的本质原因是因为链路上的超时时间设置得不合理，上游和下游的超时时间设置的一样，甚至上游的超时时间比下游还要短。在实际情况中业务一般都没有专门配置过 RPC 的超时时间，所以可能上下游都是默认的超时，时长是一样的。为了应对这种情况，我们需要有一个机制来优化超时情况下的稳定性，并减少无用的重试。\n\n如下图，正常重试的场景是等拿到 Resp1 (或者拿到超时结果) 后再发起第二次请求，整体耗时是 t1 + t2 。我们分析下，service A 在发出去 Req1 之后可能等待很长的时间，比如 1s ，但是这个请求的 pct99 或者 pct999 可能通常只有 100ms 以内，如果超过了 100ms ，有很大概率是这次访问最终会超时，能不能不要傻等，而是提前重试呢？\n\n\n\n基于这种思想，我们引入并实现了 Backup Requests 的方案。如下图，我们预先设定一个阈值 t3（比超时时间小，通常建议是 RPC 请求延时的 pct99 ），当 Req1 发出去后超过 t3 时间都没有返回，那我们直接发起重试请求 Req2 ，这样相当于同时有两个请求运行。然后等待请求返回，只要 Resp1 或者 Resp2 任意一个返回成功的结果，就可以立即结束这次请求，这样整体的耗时就是 t4 ，它表示从第一个请求发出到第一个成功结果返回之间的时间，相比于等待超时后再发出请求，这种机制能大大减少整体延时。\n\n\n\n实际上 Backup Requests 是一种用访问量来换成功率 (或者说低延时) 的思想，当然我们会控制它的访问量增大比率，在发起重试之前，会为第一次的请求记录一次失败，并检查当前失败率是否超过了熔断阈值，这样整体的访问比率还是会在控制之内。\n\n# 结合 DDL\n\nBackup Requests 的思路能在缩短整体请求延时的同时减少一部分的无效请求，但不是所有业务场景下都适合配置 Backup Requests ，因此我们又结合了 DDL 来控制无效重试。\n\nDDL 是“ Deadline Request 调用链超时”的简称，我们知道 TCP/IP 协议中的 TTL 用于判断数据包在网络中的时间是否太长而应被丢弃，DDL 与之类似，它是一种全链路式的调用超时，可以用来判断当前的 RPC 请求是否还需要继续下去。如下图，字节跳动的基础团队已经实现了 DDL 功能，在 RPC 请求调用链中会带上超时时间，并且每经过一层就减去该层处理的时间，如果剩下的时间已经小于等于 0 ，则可以不需要再请求下游，直接返回失败即可。\n\n\n\nDDL 的方式能有效减少对下游的无效调用，我们在重试治理中也结合了 DDL 的数据，在每一次发起重试前都会判断 DDL 的剩余值是否还大于 0 ，如果已经不满足条件了，那也就没必要对下游重试，这样能做到最大限度的减少无用的重试。\n\n# 实际的链路放大效应\n\n之前说的链路指数放大是理想情况下的分析，实际的情况要复杂很多，因为有很多影响因素：\n\n策略         说明\n重试熔断       请求失败 / 成功 > 0.1 时停止重试\n链路上传错误标志   下层重试失败后上传错误标志，上层不再重试\n链路下传重试标志   重试请求特殊标记，下层对重试请求不会重试\nDDL        当剩余时间不够时不再发起重试请求\n框架熔断       微服务框架本身熔断、过载保护等机制也会影响重试效果\n\n各种因素综合下来，最终实际方法情况不是一个简单的计算公式能说明，我们构造了多层调用链路，在线上实际测试和记录了在不同错误类型、不同错误率的情况下使用重试治理组件的效果，发现接入重试治理组件后能够在链路层面有效的控制重试放大倍数，大幅减少重试导致系统雪崩的概率。\n\n\n# 总结\n\n如上所述，基于服务治理的思想我们开发了重试治理的功能，支持动态配置，接入方式基本无需入侵业务代码，并使用多种策略结合的方式在链路层面控制重试放大效应，兼顾易用性、灵活性、安全性，在字节跳动内部已经有包括直播在内的很多服务接入使用并上线验证，对提高服务本身稳定性有良好的效果。目前方案已经被验证并在字节跳动直播等业务推广，后续将为更多的字节跳动业务服务。\n\n\n# 参考文献\n\n如何优雅地重试 (qq.com)",normalizedContent:"下述是引用字节某大佬的一篇文章，讲的是如何在微服务中进行重试\n\n\n# 前言\n\n在微服务架构中，一个大系统被拆分成多个小服务，小服务之间大量 rpc 调用，经常可能因为网络抖动等原因导致 rpc 调用失败，这时候使用重试机制可以提高请求的最终成功率，减少故障影响，让系统运行更稳定。\n\n\n\n\n# 重试的风险\n\n重试能够提高服务稳定性，但是一般情况下大家都不会轻易去重试，或者说不敢重试，主要是因为重试有放大故障的风险。\n\n首先，重试会加大直接下游的负载。如下图，假设 a 服务调用 b 服务，重试次数设置为 r（包括首次请求），当 b 高负载时很可能调用不成功，这时 a 调用失败重试 b ，b 服务的被调用量快速增大，最坏情况下可能放大到 r 倍，不仅不能请求成功，还可能导致 b 的负载继续升高，甚至直接打挂。\n\n\n\n更可怕的是，重试还会存在链路放大的效应，结合下图说明一下：\n\n\n\n假设现在场景是 backend a 调用 backend b，backend b 调用 db frontend，均设置重试次数为 3 。如果 backend b 调用 db frontend，请求 3 次都失败了，这时 backend b 会给 backend a 返回失败。但是 backend a 也有重试的逻辑，backend a 重试 backend b 三次，每一次 backend b 都会请求 db frontend 3 次，这样算起来，db frontend 就会被请求了 9 次，实际是指数级扩大。假设正常访问量是 n，链路一共有 m 层，每层重试次数为 r，则最后一层受到的访问量最大，为 n * r ^ (m - 1) 。这种指数放大的效应很可怕，可能导致链路上多层都被打挂，整个系统雪崩。\n\n\n# 重试的使用成本\n\n另外使用重试的成本也比较高。之前在字节跳动的内部框架和服务治理平台中都没有支持重试，在一些很需要重试的业务场景下（比如调用一些第三方业务经常失败），业务方可能用简单 for 循环来实现，基本不会考虑重试的放大效应，这样很不安全，公司内部出现过多次因为重试而导致的事故，且出事故的时候还需要修改代码上线才能关闭重试，导致事故恢复也不迅速。\n\n另外也有一些业务使用开源的重试组件，这些组件通常会考虑对直接下游的保护，但不会考虑链路级别的重试放大，另外需要业务方修改 rpc 调用代码才能使用，对业务代码入侵较多，而且也是静态配置，需要修改配置时都必须重新上线。\n\n基于以上的背景，为了让业务方能够灵活安全的使用重试，我们字节跳动直播中台团队设计和实现了一个重试治理组件，具有以下优点：\n\n 1. 能够在链路级别防重试风暴。\n\n 2. 保证易用性，业务接入成本小。\n\n 3. 具有灵活性，能够动态调整配置。\n\n下面介绍具体的实现方案。\n\n\n# 重试治理\n\n\n# 动态配置\n\n如何让业务方简单接入是首先要解决的问题。如果还是普通组件库的方式，依旧免不了要大量入侵用户代码，且很难动态调整。\n\n字节跳动的 golang 开发框架支持中间件 (milddleware) 模式，可以注册多个自定义 middleware 并依次递归调用，通常是用于完成打印日志、上报监控等非业务逻辑，能够有效将业务和非业务代码功能进行解耦。因此我们决定使用 middleware 的方式来实现重试功能，定义一个 middleware 并在内部实现对 rpc 的重复调用，把重试的配置信息用字节跳动的分布式配置存储中心存储，这样 middleware 中能够读取配置中心的配置并进行重试，对用户来说不需要修改调用 rpc 的代码，而只需要在服务中引入一个全局的 middleware 即可。\n\n如下面的整体架构图所示，我们提供配置的网页和后台，用户能够在专门进行服务治理的页面上很方便的对 rpc 进行配置修改并自动生效，内部的实现逻辑对用户透明，对业务代码无入侵。\n\n\n\n配置的维度按照字节跳动的 rpc 调用特点，选定 [调用方服务，调用方集群，被调用服务， 被调用方法] 为一个元组，按照元组来进行配置。middleware 中封装了读取配置的方法，在 rpc 调用的时候会自动读取并生效。\n\n这种 middleware 的方式能够让业务方很容易接入，相对于之前普通组件库的方式要方便很多，并且一次接入以后就具有动态配置的能力，可能很方便地调整或者关闭重试配置。\n\n\n# 退避策略\n\n确定了接入方式以后就可以开始实现重试组件的具体功能，一个重试组件所包含的基本功能中，除了重试次数和总延时这样的基础配置外，还需要有退避策略。\n\n对于一些暂时性的错误，如网络抖动等，可能立即重试还是会失败，通常等待一小会儿再重试的话成功率会较高，并且也可能打散上游重试的时间，较少因为同时都重试而导致的下游瞬间流量高峰。决定等待多久之后再重试的方法叫做退避策略，我们实现了常见的退避策略，如：\n\n * 线性退避：每次等待固定时间后重试。\n\n * 随机退避：在一定范围内随机等待一个时间后重试。\n\n * 指数退避：连续重试时，每次等待时间都是前一次的倍数。\n\n\n# 防止 retry storm\n\n如何安全重试，防止 retry storm 是我们面临的最大的难题。\n\n# 限制单点重试\n\n首先要在单点进行限制，一个服务不能不受限制的重试下游，很容易造成下游被打挂。除了限制用户设定的重试次数上限外，更重要的是限制重试请求的成功率。\n\n实现的方案很简单，基于断路器的思想，限制 请求失败/请求成功 的比率，给重试增加熔断功能。我们采用了常见的滑动窗口的方法来实现，如下图，内存中为每一类 rpc 调用维护一个滑动窗口，比如窗口分 10 个 bucket ，每个 bucket 里面记录了 1s 内 rpc 的请求结果数据（成功、失败）。新的一秒到来时，生成新的 bucket ，并淘汰最早的一个 bucket ，只维持 10s 的数据。在新请求这个 rpc 失败时，根据前 10s 内的 失败/成功 是否超过阈值来判断是否可以重试。默认阈值是 0.1 ，即下游最多承受 1.1 倍的 qps ，用户可以根据需要自行调整熔断开关和阈值。\n\n\n\n# 限制链路重试\n\n前面说过在多级链路中如果每层都配置重试可能导致调用量指数级扩大，虽然有了重试熔断之后，重试不再是指数增长(每一单节点重试扩大限制了 1.1 倍)，但还是会随着链路的级数增长而扩大调用次数，因此还是需要从链路层面来考虑重试的安全性。\n\n链路层面的防重试风暴的核心是限制每层都发生重试，理想情况下只有最下一层发生重试。google sre 中指出了 google 内部使用特殊错误码的方式来实现：\n\n * 统一约定一个特殊的 status code ，它表示：调用失败，但别重试。\n\n * 任何一级重试失败后，生成该 status code 并返回给上层。\n\n * 上层收到该 status code 后停止对这个下游的重试，并将错误码再传给自己的上层。\n\n这种方式理想情况下只有最下一层发生重试，它的上游收到错误码后都不会重试，链路整体放大倍数也就是 r 倍(单层的重试次数)。但是这种策略依赖于业务方传递错误码，对业务代码有一定入侵，而且通常业务方的代码差异很大，调用 rpc 的方式和场景也各不相同，需要业务方配合进行大量改造，很可能因为漏改等原因导致没有把从下游拿到的错误码传递给上游。\n\n好在字节跳动内部用的 rpc 协议中有扩展字段，我们在 middleware 中做了很多尝试，封装了错误码处理和传递的逻辑，在 rpc 的 response 扩展字段中传递错误码标识 nomore_retry ，它告诉上游不要再重试了。middleware 完成错误码的生成、识别、传递等整个生命周期的管理，不需要业务方修改本身的 rpc 逻辑，错误码的方案对业务来说是透明的。\n\n\n\n在链路中，推进每层都接入重试组件，这样每一层都可以通过识别这个标志位来停止重试，并逐层往上传递，上层也都停止重试，做到链路层面的防护，达到“只有最靠近错误发生的那一层才重试”的效果。\n\n# 超时处理\n\n在测试错误码上传的方案时，我们发现超时的情况可能导致传递错误码的方案失效。\n\n对于 a -> b -> c 的场景，假设 b -> c 超时，b 重试请求 c ，这时候很可能 a -> b 也超时了，所以 a 没有拿到 b 返回的错误码，而是也会重试 b , 这个时候虽然 b 重试 c 且生成了重试失败的错误码，但是却不能再传递给 a 。这种情况下，a 还是会重试 b ，如果链路中每一层都超时，那么还是会出现链路指数扩大的效应。\n\n因此为了处理这种情况，除了下游传递重试错误标志以外，我们还实现了“对重试请求不重试”的方案。\n\n对于重试的请求，我们在 request 中打上一个特殊的 retry flag ，在上面 a -> b -> c 的链路，当 b 收到 a 的请求时会先读取这个 flag 判断这个请求是不是重试请求，如果是，那它调用 c 即使失败也不会重试；否则调用 c 失败后会重试 c 。同时 b 也会把这个 retry flag 下传，它发出的请求也会有这个标志，它的下游也不会再对这个请求重试。\n\n\n\n这样即使 a 因为超时而拿不到 b 的返回，对 b 发出重试请求后，b 能感知到并且不会对 c 重试，这样 a 最多请求 r 次，b 最多请求 r + r - 1，如果后面还有更下层次的话，c 最多请求 r + r + r - 2 次， 第 i 层最多请求 i * r - (i-1) 次，最坏情况下是倍数增长，不是指数增长了。加上实际还有重试熔断的限制，增长的幅度要小很多。\n\n通过重试熔断来限制单点的放大倍数，通过重试错误标志链路回传的方式来保证只有最下层发生重试，又通过重试请求 flag 链路下传的方式来保证对重试请求不重试，多种控制策略结合，可以有效地较少重试放大效应。\n\n# 超时场景优化\n\n分布式系统中，rpc 请求的结果有三种状态：成功、失败、超时，其中最难处理的就是超时的情况。但是超时往往又是最经常发生的那一个，我们统计了字节跳动直播业务线上一些重要服务的 rpc 错误分布，发现占比最高的就是超时错误，怕什么偏来什么。\n\n在超时重试的场景中，虽然给重试请求添加 retry flag 能防止指数扩大，但是却不能提高请求成功率。如下图，假如 a 和 b 的超时时间都是 1000ms ，当 c 负载很高导致 b 访问 c 超时，这时 b 会重试 c ，但是时间已经超过了 1000ms ，时间 a 这里也超时了并且断开了和 b 的连接，所以 b 这次重试 c 不管是否成功都是无用功，从 a 的视角看，本次请求已经失败了。\n\n\n\n这种情况的本质原因是因为链路上的超时时间设置得不合理，上游和下游的超时时间设置的一样，甚至上游的超时时间比下游还要短。在实际情况中业务一般都没有专门配置过 rpc 的超时时间，所以可能上下游都是默认的超时，时长是一样的。为了应对这种情况，我们需要有一个机制来优化超时情况下的稳定性，并减少无用的重试。\n\n如下图，正常重试的场景是等拿到 resp1 (或者拿到超时结果) 后再发起第二次请求，整体耗时是 t1 + t2 。我们分析下，service a 在发出去 req1 之后可能等待很长的时间，比如 1s ，但是这个请求的 pct99 或者 pct999 可能通常只有 100ms 以内，如果超过了 100ms ，有很大概率是这次访问最终会超时，能不能不要傻等，而是提前重试呢？\n\n\n\n基于这种思想，我们引入并实现了 backup requests 的方案。如下图，我们预先设定一个阈值 t3（比超时时间小，通常建议是 rpc 请求延时的 pct99 ），当 req1 发出去后超过 t3 时间都没有返回，那我们直接发起重试请求 req2 ，这样相当于同时有两个请求运行。然后等待请求返回，只要 resp1 或者 resp2 任意一个返回成功的结果，就可以立即结束这次请求，这样整体的耗时就是 t4 ，它表示从第一个请求发出到第一个成功结果返回之间的时间，相比于等待超时后再发出请求，这种机制能大大减少整体延时。\n\n\n\n实际上 backup requests 是一种用访问量来换成功率 (或者说低延时) 的思想，当然我们会控制它的访问量增大比率，在发起重试之前，会为第一次的请求记录一次失败，并检查当前失败率是否超过了熔断阈值，这样整体的访问比率还是会在控制之内。\n\n# 结合 ddl\n\nbackup requests 的思路能在缩短整体请求延时的同时减少一部分的无效请求，但不是所有业务场景下都适合配置 backup requests ，因此我们又结合了 ddl 来控制无效重试。\n\nddl 是“ deadline request 调用链超时”的简称，我们知道 tcp/ip 协议中的 ttl 用于判断数据包在网络中的时间是否太长而应被丢弃，ddl 与之类似，它是一种全链路式的调用超时，可以用来判断当前的 rpc 请求是否还需要继续下去。如下图，字节跳动的基础团队已经实现了 ddl 功能，在 rpc 请求调用链中会带上超时时间，并且每经过一层就减去该层处理的时间，如果剩下的时间已经小于等于 0 ，则可以不需要再请求下游，直接返回失败即可。\n\n\n\nddl 的方式能有效减少对下游的无效调用，我们在重试治理中也结合了 ddl 的数据，在每一次发起重试前都会判断 ddl 的剩余值是否还大于 0 ，如果已经不满足条件了，那也就没必要对下游重试，这样能做到最大限度的减少无用的重试。\n\n# 实际的链路放大效应\n\n之前说的链路指数放大是理想情况下的分析，实际的情况要复杂很多，因为有很多影响因素：\n\n策略         说明\n重试熔断       请求失败 / 成功 > 0.1 时停止重试\n链路上传错误标志   下层重试失败后上传错误标志，上层不再重试\n链路下传重试标志   重试请求特殊标记，下层对重试请求不会重试\nddl        当剩余时间不够时不再发起重试请求\n框架熔断       微服务框架本身熔断、过载保护等机制也会影响重试效果\n\n各种因素综合下来，最终实际方法情况不是一个简单的计算公式能说明，我们构造了多层调用链路，在线上实际测试和记录了在不同错误类型、不同错误率的情况下使用重试治理组件的效果，发现接入重试治理组件后能够在链路层面有效的控制重试放大倍数，大幅减少重试导致系统雪崩的概率。\n\n\n# 总结\n\n如上所述，基于服务治理的思想我们开发了重试治理的功能，支持动态配置，接入方式基本无需入侵业务代码，并使用多种策略结合的方式在链路层面控制重试放大效应，兼顾易用性、灵活性、安全性，在字节跳动内部已经有包括直播在内的很多服务接入使用并上线验证，对提高服务本身稳定性有良好的效果。目前方案已经被验证并在字节跳动直播等业务推广，后续将为更多的字节跳动业务服务。\n\n\n# 参考文献\n\n如何优雅地重试 (qq.com)",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"海量数据计数",frontmatter:{title:"海量数据计数",date:"2024-09-14T16:52:01.000Z",permalink:"/pages/f3295f/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/10.%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E8%AE%A1%E6%95%B0.html",relativePath:"实战系统设计/07.经典场景设计/10.海量数据计数.md",key:"v-a4942194",path:"/pages/f3295f/",headers:[{level:2,title:"引子",slug:"引子",normalizedTitle:"引子",charIndex:2},{level:2,title:"计数在业务上的特点",slug:"计数在业务上的特点",normalizedTitle:"计数在业务上的特点",charIndex:410},{level:2,title:"支撑高并发的计数系统要如何设计",slug:"支撑高并发的计数系统要如何设计",normalizedTitle:"支撑高并发的计数系统要如何设计",charIndex:808},{level:2,title:"如何降低计数系统的存储成本",slug:"如何降低计数系统的存储成本",normalizedTitle:"如何降低计数系统的存储成本",charIndex:2153},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4116}],headersStr:"引子 计数在业务上的特点 支撑高并发的计数系统要如何设计 如何降低计数系统的存储成本 总结",content:"# 引子\n\n在地铁上，你也许会经常刷微博、点赞热搜，如果有抽奖活动，再转 发一波，而这些与微博息息相关的数据，其实就是微博场景下的计数数据，细说起来，它主要有几类\n\n * 微博的评论数、点赞数、转发数、浏览数、表态数等等；\n * 用户的粉丝数、关注数、发布微博数、私信数等等。\n\n微博维度的计数代表了这条微博受欢迎的程度，用户维度的数据（尤其是粉丝数），代表了这个用户的影响力，因此大家会普遍看重这些计数信息。并且在很多场景下，我们都需要查询计数数据（比如首页信息流页面、个人主页面），计数数据访问量巨大，所以需要设计计数系统维护它\n\n但在设计计数系统时，不少人会出现性能不高、存储成本很大的问题，比如，把计数与微博数据存储在一起，这样每次更新计数的时候都需要锁住这一行记录，降低了写入的并发。在我看来，之所以出现这些问题，还是因为你对计数系统的设计和优化不甚了解，所以要想解决痛点，你有必要形成完备的设计方案\n\n\n# 计数在业务上的特点\n\n * 数据量巨大，微博系统中微博条目的数量早已经超过了千亿级别，仅仅计算 微博的转发、评论、点赞、浏览等核心计数，其数据量级就已经在几千亿的级别。更何 况微博条目的数量还在不断高速地增长，并且随着微博业务越来越复杂，微博维度的计数种类也可能会持续扩展（比如说增加了表态数），因此，仅仅是微博维度上的计数量 级就已经过了万亿级别。除此之外，微博的用户量级已经超过了 10 亿，用户维度的计数 量级相比微博维度来说虽然相差很大，但是也达到了百亿级别。那么如何存储这些过万 亿级别的数字，对我们来说就是一大挑战\n * 访问量大，对于性能的要求高。微博的日活用户超过 2 亿，月活用户接近 5 亿，核心服 务（比如首页信息流）访问量级到达每秒几十万次，计数系统的访问量级也超过了每秒 百万级别，而且在性能方面，它要求要毫秒级别返回结果\n * 对于可用性、数字的准确性要求高\n\n\n# 支撑高并发的计数系统要如何设计\n\n刚开始设计计数系统的时候，微博的流量还没有现在这么夸张，我们本着 KISS（Keep It Simple and Stupid）原则，尽量将系统设计的简单易维护，所以，我们使用 MySQL 存储计数的数据，因为它是我们最熟悉的，团队在运维上经验也会比较丰富。举个具体的例子。\n\nselect repost_count, comment_count, praise_count, view_count from t_weibo_count\n\n\n随着微博的不断壮大，之前的计数系统面临了很多的问题和挑战。\n\n比如微博用户量和发布的微博量增加迅猛，计数存储数据量级也飞速增长，而 MySQL 数据库单表的存储量级达到几千万的时候，性能上就会有损耗。所以我们考虑使用分库分表的 方式分散数据量，提升读取计数的性能。\n\n我们用“weibo_id”作为分区键，在选择分库分表的方式时，考虑了下面两种\n\n * 一种方式是选择一种哈希算法对 weibo_id 计算哈希值，然后依据这个哈希值计算出需 要存储到哪一个库哪一张表中\n * 另一种方式是按照 weibo_id 生成的时间来做分库分表，我们在第 10 讲谈到发号器的时候曾经提到，ID 的生成最好带有业务意义的字段，比如生成 ID 的时间戳\n\n分表的时候，可以先依据发号器的算法反解出时间戳，然后按照时间戳来做分库分表， 比如，一天一张表或者一个月一张表等等。\n\n因为越是最近发布的微博，计数数据的访问量就越大，所以虽然我考虑了两种方案，但是按照时间来分库分表会造成数据访问的不均匀，最后用了哈希的方式来做分库分表。\n\n\n\n与此同时，计数的访问量级也有质的飞越。在微博最初的版本中，首页信息流里面是不展示 计数数据的，那么使用 MySQL 也可以承受当时读取计数的访问量。但是后来在首页信息流中也要展示转发、评论和点赞等计数数据了。而信息流的访问量巨大，仅仅靠数据库已经 完全不能承担如此高的并发量了。于是我们考虑使用 Redis 来加速读请求，通过部署多个从节点来提升可用性和性能，并且通过 Hash 的方式对数据做分片，也基本上可以保证计数的读取性能。然而，这种数据库 + 缓存的方式有一个弊端：无法保证数据的一致性，比如，如果数据库写入成功而缓存更新失败，就会导致数据的不一致，影响计数的准确性。所以，我们完全抛弃了 MySQL，全面使用 Redis 来作为计数的存储组\n\n\n\n除了考虑计数的读取性能之外，由于热门微博的计数变化频率相当快，也需要考虑如何提升 计数的写入性能。比如，每次在转发一条微博的时候，都需要增加这条微博的转发数，那么 如果明星发布结婚、离婚的微博，瞬时就可能会产生几万甚至几十万的转发。如果是你的话，要如何降低写压力呢？\n\n你可能已经想到用消息队列来削峰填谷了，也就是说，我们在转发微博的时候向消息队列写 入一条消息，然后在消息处理程序中给这条微博的转发计数加 1。这里需要注意的一点， 我们可以通过批量处理消息的方式进一步减小 Redis 的写压力，比如像下面这样连续更改 三次转发数（我用 SQL 来表示来方便你理解）：\n\n\n\n这个时候，你可以把它们合并成一次更新：\n\n\n\n\n# 如何降低计数系统的存储成本\n\n讲到这里，我其实已经告诉你一个支撑高并发查询请求的计数系统是如何实现的了。但是在微博的场景下，计数的量级是万亿的级别，这也给我们提了更高的要求，就是如何在有限的存储成本下实现对于全量计数数据的存取。\n\n你知道，Redis 是使用内存来存储信息，相比于使用磁盘存储数据的 MySQL 来说，存储的成本不可同日而语，比如一台服务器磁盘可以挂载到 2 个 T，但是内存可能只有 128G，这样磁盘的存储空间就是内存的 16 倍。而 Redis 基于通用性的考虑，对于内存的使用比较粗 放，存在大量的指针以及额外数据结构的开销，如果要存储一个 KV 类型的计数信息，Key 是 8 字节 Long 类型的 weibo_id，Value 是 4 字节 int 类型的转发数，存储在 Redis 中之后会占用超过 70 个字节的空间，空间的浪费是巨大的。如果你面临这个问题，要如何优化呢\n\n我建议你先对原生 Redis 做一些改造，采用新的数据结构和数据类型来存储计数数据。我 在改造时，主要涉及了两点\n\n * 一是原生的 Redis 在存储 Key 时是按照字符串类型来存储的，比如一个 8 字节的 Long 类型的数据，需要 8（sdshdr 数据结构长度）+ 19（8 字节数字的长度）+1（’\\0’） =28 个字节，如果我们使用 Long 类型来存储就只需要 8 个字节，会节省 20 个字节的 空间；\n * 二是去除了原生 Redis 中多余的指针，如果要存储一个 KV 信息就只需要 8（weibo_id）+4（转发数）=12 个字节，相比之前有很大的改进。\n\n同时，我们也会使用一个大的数组来存储计数信息，存储的位置是基于 weibo_id 的哈希值来计算出来的，具体的算法像下面展示的这样：\n\n插入时:\nh1 = hash1(weibo_id) // 根据微博 ID 计算 Hash\nh2 = hash2(weibo_id) // 根据微博 ID 计算另一个 Hash，用以解决前一个 Hash 算法带来的冲突\nfor s in 0,1000\n\tpos = (h1 + h2*s) % tsize // 如果发生冲突，就多算几次 Hash2\n\t\tif(isempty(pos) || isdelete(pos))\n\t\t\tt[ pos ] = item  // 写入数组\n查询时:\nfor s in 0,1000\n\tpos = (h1 + h2*s) % tsize  // 依照插入数据时候的逻辑，计算出存储在数组中的位置\n\t\tif(!isempty(pos) && t[pos]==weibo_id)\n\t\t\treturn t[pos]\nreturn 0 \n删除时:\ninsert(FFFF) // 插入一个特殊的标\n\n\n在对原生的 Redis 做了改造之后，你还需要进一步考虑如何节省内存的使用。比如，微博的计数有转发数、评论数、浏览数、点赞数等等，如果每一个计数都需要存储 weibo_id， 那么总共就需要 8（weibo_id）*4（4 个微博 ID）+4（转发数） + 4（评论数） + 4（点 赞数） + 4（浏览数）= 48 字节。但是我们可以把相同微博 ID 的计数存储在一起，这样就只需要记录一个微博 ID，省掉了多余的三个微博 ID 的存储开销，存储空间就进一步减少 了。\n\n不过，即使经过上面的优化，由于计数的量级实在是太过巨大，并且还在以极快的速度增 长，所以如果我们以全内存的方式来存储计数信息，就需要使用非常多的机器来支撑。\n\n冷热分离\n\n然而微博计数的数据具有明显的热点属性：越是最近的微博越是会被访问到，时间上久远的微博被访问的几率很小。所以为了尽量减少服务器的使用，我们考虑给计数服务增加 SSD 磁盘，然后将时间上比较久远的数据 dump 到磁盘上，内存中只保留最近的数据。当我们要读取冷数据的时候，使用单独的 I/O 线程异步地将冷数据从 SSD 磁盘中加载到一块儿单 独的 Cold Cache 中\n\n\n\n在经过了上面这些优化之后，我们的计数服务就可以支撑高并发大数据量的考验，无论是在 性能上、成本上和可用性上都能够达到业务的需求了。\n\n总的来说，我用微博设计计数系统的例子，并不是仅仅告诉你计数系统是如何做的，而是想 告诉你在做系统设计的时候需要了解自己系统目前的痛点是什么，然后再针对痛点来做细致 的优化。比如，微博计数系统的痛点是存储的成本，那么我们后期做的事情很多都是围绕着 如何使用有限的服务器存储全量的计数数据，即使是对开源组件（Redis）做深度的定制会 带来很大的运维成本，也只能被认为是为了实现计数系统而必须要做的权衡。\n\n\n# 总结\n\n 1. 数据库 + 缓存的方案是计数系统的初级阶段，完全可以支撑中小访问量和存储量的存储 服务。如果你的项目还处在初级阶段，量级还不是很大，那么你一开始可以考虑使用这 种方案。\n 2. 通过对原生 Redis 组件的改造，我们可以极大地减小存储数据的内存开销。\n 3. 使用 SSD+ 内存的方案可以最终解决存储计数数据的成本问题。这个方式适用于冷热数 据明显的场景，你在使用时需要考虑如何将内存中的数据做换入换出",normalizedContent:"# 引子\n\n在地铁上，你也许会经常刷微博、点赞热搜，如果有抽奖活动，再转 发一波，而这些与微博息息相关的数据，其实就是微博场景下的计数数据，细说起来，它主要有几类\n\n * 微博的评论数、点赞数、转发数、浏览数、表态数等等；\n * 用户的粉丝数、关注数、发布微博数、私信数等等。\n\n微博维度的计数代表了这条微博受欢迎的程度，用户维度的数据（尤其是粉丝数），代表了这个用户的影响力，因此大家会普遍看重这些计数信息。并且在很多场景下，我们都需要查询计数数据（比如首页信息流页面、个人主页面），计数数据访问量巨大，所以需要设计计数系统维护它\n\n但在设计计数系统时，不少人会出现性能不高、存储成本很大的问题，比如，把计数与微博数据存储在一起，这样每次更新计数的时候都需要锁住这一行记录，降低了写入的并发。在我看来，之所以出现这些问题，还是因为你对计数系统的设计和优化不甚了解，所以要想解决痛点，你有必要形成完备的设计方案\n\n\n# 计数在业务上的特点\n\n * 数据量巨大，微博系统中微博条目的数量早已经超过了千亿级别，仅仅计算 微博的转发、评论、点赞、浏览等核心计数，其数据量级就已经在几千亿的级别。更何 况微博条目的数量还在不断高速地增长，并且随着微博业务越来越复杂，微博维度的计数种类也可能会持续扩展（比如说增加了表态数），因此，仅仅是微博维度上的计数量 级就已经过了万亿级别。除此之外，微博的用户量级已经超过了 10 亿，用户维度的计数 量级相比微博维度来说虽然相差很大，但是也达到了百亿级别。那么如何存储这些过万 亿级别的数字，对我们来说就是一大挑战\n * 访问量大，对于性能的要求高。微博的日活用户超过 2 亿，月活用户接近 5 亿，核心服 务（比如首页信息流）访问量级到达每秒几十万次，计数系统的访问量级也超过了每秒 百万级别，而且在性能方面，它要求要毫秒级别返回结果\n * 对于可用性、数字的准确性要求高\n\n\n# 支撑高并发的计数系统要如何设计\n\n刚开始设计计数系统的时候，微博的流量还没有现在这么夸张，我们本着 kiss（keep it simple and stupid）原则，尽量将系统设计的简单易维护，所以，我们使用 mysql 存储计数的数据，因为它是我们最熟悉的，团队在运维上经验也会比较丰富。举个具体的例子。\n\nselect repost_count, comment_count, praise_count, view_count from t_weibo_count\n\n\n随着微博的不断壮大，之前的计数系统面临了很多的问题和挑战。\n\n比如微博用户量和发布的微博量增加迅猛，计数存储数据量级也飞速增长，而 mysql 数据库单表的存储量级达到几千万的时候，性能上就会有损耗。所以我们考虑使用分库分表的 方式分散数据量，提升读取计数的性能。\n\n我们用“weibo_id”作为分区键，在选择分库分表的方式时，考虑了下面两种\n\n * 一种方式是选择一种哈希算法对 weibo_id 计算哈希值，然后依据这个哈希值计算出需 要存储到哪一个库哪一张表中\n * 另一种方式是按照 weibo_id 生成的时间来做分库分表，我们在第 10 讲谈到发号器的时候曾经提到，id 的生成最好带有业务意义的字段，比如生成 id 的时间戳\n\n分表的时候，可以先依据发号器的算法反解出时间戳，然后按照时间戳来做分库分表， 比如，一天一张表或者一个月一张表等等。\n\n因为越是最近发布的微博，计数数据的访问量就越大，所以虽然我考虑了两种方案，但是按照时间来分库分表会造成数据访问的不均匀，最后用了哈希的方式来做分库分表。\n\n\n\n与此同时，计数的访问量级也有质的飞越。在微博最初的版本中，首页信息流里面是不展示 计数数据的，那么使用 mysql 也可以承受当时读取计数的访问量。但是后来在首页信息流中也要展示转发、评论和点赞等计数数据了。而信息流的访问量巨大，仅仅靠数据库已经 完全不能承担如此高的并发量了。于是我们考虑使用 redis 来加速读请求，通过部署多个从节点来提升可用性和性能，并且通过 hash 的方式对数据做分片，也基本上可以保证计数的读取性能。然而，这种数据库 + 缓存的方式有一个弊端：无法保证数据的一致性，比如，如果数据库写入成功而缓存更新失败，就会导致数据的不一致，影响计数的准确性。所以，我们完全抛弃了 mysql，全面使用 redis 来作为计数的存储组\n\n\n\n除了考虑计数的读取性能之外，由于热门微博的计数变化频率相当快，也需要考虑如何提升 计数的写入性能。比如，每次在转发一条微博的时候，都需要增加这条微博的转发数，那么 如果明星发布结婚、离婚的微博，瞬时就可能会产生几万甚至几十万的转发。如果是你的话，要如何降低写压力呢？\n\n你可能已经想到用消息队列来削峰填谷了，也就是说，我们在转发微博的时候向消息队列写 入一条消息，然后在消息处理程序中给这条微博的转发计数加 1。这里需要注意的一点， 我们可以通过批量处理消息的方式进一步减小 redis 的写压力，比如像下面这样连续更改 三次转发数（我用 sql 来表示来方便你理解）：\n\n\n\n这个时候，你可以把它们合并成一次更新：\n\n\n\n\n# 如何降低计数系统的存储成本\n\n讲到这里，我其实已经告诉你一个支撑高并发查询请求的计数系统是如何实现的了。但是在微博的场景下，计数的量级是万亿的级别，这也给我们提了更高的要求，就是如何在有限的存储成本下实现对于全量计数数据的存取。\n\n你知道，redis 是使用内存来存储信息，相比于使用磁盘存储数据的 mysql 来说，存储的成本不可同日而语，比如一台服务器磁盘可以挂载到 2 个 t，但是内存可能只有 128g，这样磁盘的存储空间就是内存的 16 倍。而 redis 基于通用性的考虑，对于内存的使用比较粗 放，存在大量的指针以及额外数据结构的开销，如果要存储一个 kv 类型的计数信息，key 是 8 字节 long 类型的 weibo_id，value 是 4 字节 int 类型的转发数，存储在 redis 中之后会占用超过 70 个字节的空间，空间的浪费是巨大的。如果你面临这个问题，要如何优化呢\n\n我建议你先对原生 redis 做一些改造，采用新的数据结构和数据类型来存储计数数据。我 在改造时，主要涉及了两点\n\n * 一是原生的 redis 在存储 key 时是按照字符串类型来存储的，比如一个 8 字节的 long 类型的数据，需要 8（sdshdr 数据结构长度）+ 19（8 字节数字的长度）+1（’\\0’） =28 个字节，如果我们使用 long 类型来存储就只需要 8 个字节，会节省 20 个字节的 空间；\n * 二是去除了原生 redis 中多余的指针，如果要存储一个 kv 信息就只需要 8（weibo_id）+4（转发数）=12 个字节，相比之前有很大的改进。\n\n同时，我们也会使用一个大的数组来存储计数信息，存储的位置是基于 weibo_id 的哈希值来计算出来的，具体的算法像下面展示的这样：\n\n插入时:\nh1 = hash1(weibo_id) // 根据微博 id 计算 hash\nh2 = hash2(weibo_id) // 根据微博 id 计算另一个 hash，用以解决前一个 hash 算法带来的冲突\nfor s in 0,1000\n\tpos = (h1 + h2*s) % tsize // 如果发生冲突，就多算几次 hash2\n\t\tif(isempty(pos) || isdelete(pos))\n\t\t\tt[ pos ] = item  // 写入数组\n查询时:\nfor s in 0,1000\n\tpos = (h1 + h2*s) % tsize  // 依照插入数据时候的逻辑，计算出存储在数组中的位置\n\t\tif(!isempty(pos) && t[pos]==weibo_id)\n\t\t\treturn t[pos]\nreturn 0 \n删除时:\ninsert(ffff) // 插入一个特殊的标\n\n\n在对原生的 redis 做了改造之后，你还需要进一步考虑如何节省内存的使用。比如，微博的计数有转发数、评论数、浏览数、点赞数等等，如果每一个计数都需要存储 weibo_id， 那么总共就需要 8（weibo_id）*4（4 个微博 id）+4（转发数） + 4（评论数） + 4（点 赞数） + 4（浏览数）= 48 字节。但是我们可以把相同微博 id 的计数存储在一起，这样就只需要记录一个微博 id，省掉了多余的三个微博 id 的存储开销，存储空间就进一步减少 了。\n\n不过，即使经过上面的优化，由于计数的量级实在是太过巨大，并且还在以极快的速度增 长，所以如果我们以全内存的方式来存储计数信息，就需要使用非常多的机器来支撑。\n\n冷热分离\n\n然而微博计数的数据具有明显的热点属性：越是最近的微博越是会被访问到，时间上久远的微博被访问的几率很小。所以为了尽量减少服务器的使用，我们考虑给计数服务增加 ssd 磁盘，然后将时间上比较久远的数据 dump 到磁盘上，内存中只保留最近的数据。当我们要读取冷数据的时候，使用单独的 i/o 线程异步地将冷数据从 ssd 磁盘中加载到一块儿单 独的 cold cache 中\n\n\n\n在经过了上面这些优化之后，我们的计数服务就可以支撑高并发大数据量的考验，无论是在 性能上、成本上和可用性上都能够达到业务的需求了。\n\n总的来说，我用微博设计计数系统的例子，并不是仅仅告诉你计数系统是如何做的，而是想 告诉你在做系统设计的时候需要了解自己系统目前的痛点是什么，然后再针对痛点来做细致 的优化。比如，微博计数系统的痛点是存储的成本，那么我们后期做的事情很多都是围绕着 如何使用有限的服务器存储全量的计数数据，即使是对开源组件（redis）做深度的定制会 带来很大的运维成本，也只能被认为是为了实现计数系统而必须要做的权衡。\n\n\n# 总结\n\n 1. 数据库 + 缓存的方案是计数系统的初级阶段，完全可以支撑中小访问量和存储量的存储 服务。如果你的项目还处在初级阶段，量级还不是很大，那么你一开始可以考虑使用这 种方案。\n 2. 通过对原生 redis 组件的改造，我们可以极大地减小存储数据的内存开销。\n 3. 使用 ssd+ 内存的方案可以最终解决存储计数数据的成本问题。这个方式适用于冷热数 据明显的场景，你在使用时需要考虑如何将内存中的数据做换入换出",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"消息未读数系统",frontmatter:{title:"消息未读数系统",date:"2024-09-14T23:57:17.000Z",permalink:"/pages/6b9d68/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/07.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/11.%E6%B6%88%E6%81%AF%E6%9C%AA%E8%AF%BB%E6%95%B0%E7%B3%BB%E7%BB%9F.html",relativePath:"实战系统设计/07.经典场景设计/11.消息未读数系统.md",key:"v-5a65c5ca",path:"/pages/6b9d68/",headers:[{level:2,title:"引子",slug:"引子",normalizedTitle:"引子",charIndex:2},{level:2,title:"系统通知的未读数要如何设计",slug:"系统通知的未读数要如何设计",normalizedTitle:"系统通知的未读数要如何设计",charIndex:242},{level:2,title:"如何为信息流的未读数设计方案",slug:"如何为信息流的未读数设计方案",normalizedTitle:"如何为信息流的未读数设计方案",charIndex:2043},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:3457}],headersStr:"引子 系统通知的未读数要如何设计 如何为信息流的未读数设计方案 总结",content:"# 引子\n\n未读数也是系统中一个常见的模块，以微博系统为例，你可看到有多个未读计数的场景，\n\n比如：\n\n * 当有人 @你、评论你、给你的博文点赞或者给你发送私信的时候，你会收到相应的未读提醒；\n * 在早期的微博版本中有系统通知的功能，也就是系统会给全部用户发送消息，通知用户有新的版本或者有一些好玩的运营活动，如果用户没有看，系统就会给他展示有多少条未读的提醒。\n * 我们在浏览信息流的时候，如果长时间没有刷新页面，那么信息流上方就会提示你在这段时间有多少条信息没有看\n\n\n# 系统通知的未读数要如何设计\n\n来看具体的例子。假如你的系统中只有 A、B、C 三个用户，那么你可以在通用计数系统中 增加一块儿内存区域，并且以用户 ID 为 Key 来存储这三个用户的未读通知数据，当系统发 送一个新的通知时，我们会循环给每一个用户的未读数加 1，这个处理逻辑的伪代码就像下 面这样\n\nList<Long> userIds = getAllUserIds();\n    for(Long id : userIds) {\n    incrUnreadCount(id);\n}\n\n\n但随着系统中的用户越来越多，这个方案存在两个致命的问题\n\n * 首先，获取全量用户就是一个比较耗时的操作，相当于对用户库做一次全表的扫描，这不仅会对数据库造成很大的压力，而且查询全量用户数据的响应时间是很长的，对于在线业务来说是难以接受的。如果你的用户库已经做了分库分表，那么就要扫描所有的库表，响应时间就更长了。不过有一个折中的方法， 那就是在发送系统通知之前，先从线下的数据仓库中 获取全量的用户 ID，并且存储在一个本地的文件中，然后再轮询所有的用户 ID，给这些用 户增加未读计数。\n   * 这似乎是一个可行的技术方案，然而它给所有人增加未读计数，会消耗非常长的时间。你计 算一下，假如你的系统中有一个亿的用户，给一个用户增加未读数需要消耗 1ms，那么给 所有人都增加未读计数就需要 100000000 * 1 /1000 = 100000 秒，也就是超过一天的时 间；即使你启动 100 个线程并发的设置，也需要十几分钟的时间才能完成，而用户很难接 受这么长的延迟时间。\n * 另外，使用这种方式需要给系统中的每一个用户都记一个未读数的值，而在系统中，活跃用 户只是很少的一部分，大部分的用户是不活跃的，甚至从来没有打开过系统通知，为这些用 户记录未读数显然是一种浪费。\n\n通过上面的内容，你可以知道为什么我们不能用通用计数系统实现系统通知未读数了吧？那正确的做法是什么呢\n\n要知道，系统通知实际上是存储在一个大的列表中的，这个列表对所有用户共享，也就是所有人看到的都是同一份系统通知的数据。不过不同的人最近看到的消息不同，所以每个人会有不同的未读数。因此，你可以记录一下在这个列表中每个人看过最后一条消息的 ID，然 后统计这个 ID 之后有多少条消息，这就是未读数了\n\n\n\n上述就是 timeline 模型\n\n这个方案在实现时有这样几个关键点：\n\n * 用户访问系统通知页面需要设置未读数为 0，我们需要将用户最近看过的通知 ID 设置为最新的一条系统通知 ID\n * 如果最近看过的通知 ID 为空，则认为是一个新的用户，返回未读数为 0；\n * 对于非活跃用户，比如最近一个月都没有登录和使用过系统的用户，可以把用户最近看过的通知 ID 清空，节省内存空间。\n\n这是一种比较通用的方案，即节省内存，又能尽量减少获取未读数的延迟。\n\n这个方案适用 的另一个业务场景是全量用户打点的场景，比如像下面这张微博截图中的红点\n\n\n\n这个红点和系统通知类似，也是一种通知全量用户的手段，如果逐个通知用户，延迟也是无法接受的。因此你可以采用和系统通知类似的方案。\n\n首先，我们为每一个用户存储一个时间戳，代表最近点过这个红点的时间，用户点了红点， 就把这个时间戳设置为当前时间；然后，我们也记录一个全局的时间戳，这个时间戳标识最新的一次打点时间，如果你在后台操作给全体用户打点，就更新这个时间戳为当前时间。而 我们在判断是否需要展示红点时，只需要判断用户的时间戳和全局时间戳的大小，如果用户时间戳小于全局时间戳，代表在用户最后一次点击红点之后又有新的红点推送，那么就要展 示红点，反之，就不展示红点了。\n\n\n\n这两个场景的共性是全部用户共享一份有限的存储数据，每个人只记录自己在这份存储中的 偏移量，就可以得到未读数了。\n\n你可以看到，系统消息未读的实现方案不是很复杂，它通过设计避免了操作全量数据未读数，如果你的系统中有这种打红点的需求，那我建议你可以结合实际工作灵活使用上述方案。\n\n最后一个需求关注的是微博信息流的未读数，在现在的社交系统中，关注关系已经成为标配的功能，而基于关注关系的信息流也是一种非常重要的信息聚合方式，因此，如何设计信息流的未读数系统就成了你必须面对的一个问题。\n\n\n# 如何为信息流的未读数设计方案\n\n信息流的未读数之所以复杂主要有这样几点原因\n\n首先，微博的信息流是基于关注关系的，未读数也是基于关注关系的，就是说，你关注的人发布了新的微博，那么你作为粉丝未读数就要增加 1。\n\n如果微博用户都是像我这样只有几百粉丝的“小透明”就简单了，你发微博的时候系统给你粉丝的未读数增加 1 不 是什么难事儿。但是对于一些动辄几千万甚至上亿粉丝的微博大 V 就麻烦了，增加未读 数可能需要几个小时。假设你是杨幂的粉丝，想了解她实时发布的博文，那么如果当她 发布博文几个小时之后，你才收到提醒，这显然是不能接受的。所以未读数的延迟是你在设计方案时首先要考虑的内容。\n\n其次，信息流未读数请求量极大、并发极高，这是因为接口是客户端轮询请求的，不是用户触发的。\n\n也就是说，用户即使打开微博客户端什么都不做，这个接口也会被请求到。在几年前，请求未读数接口的量级就已经接近每秒 50 万次，这几年随着微博量级的增长，请求量也变得更高。而作为微博的非核心接口，我们不太可能使用大量的机器来抗未读数请求，因此，如何使用有限的资源来支撑如此高的流量是这个方案的难点。 最后，它不像系统通知那样有共享的存储，因为每个人关注的人不同，信息流的列表也就不同，所以也就没办法采用系统通知未读数的方案。\n\n那要如何设计能够承接每秒几十万次请求的信息流未读数系统呢？你可以这样做：\n\n首先，在通用计数器中记录每一个用户发布的博文数； 然后在 Redis 或者 Memcached 中记录一个人所有关注人的博文数快照，当用户点击未读消息重置未读数为 0 时，将他关注所有人的博文数刷新到快照中； 这样，他关注所有人的博文总数减去快照中的博文总数就是他的信息流未读数。\n\n\n\n假如用户 A，像上图这样关注了用户 B、C、D，其中 B 发布的博文数是 10，C 发布的博 文数是 8，D 发布的博文数是 14，而在用户 A 最近一次查看未读消息时，记录在快照中的 这三个用户的博文数分别是 6、7、12，因此用户 A 的未读数就是（10-6）+（8-7）+（14-12）=7。\n\n这个方案设计简单，并且是全内存操作，性能足够好，能够支撑比较高的并发，事实上微博团队仅仅用 16 台普通的服务器就支撑了每秒接近 50 万次的请求，这就足以证明这个方案的性能有多出色，因此，它完全能够满足信息流未读数的需求。\n\n当然了这个方案也有一些缺陷，比如说快照中需要存储关注关系，如果关注关系变更的时候 更新不及时，那么就会造成未读数不准确；快照采用的是全缓存存储，如果缓存满了就会剔 除一些数据，那么被剔除用户的未读数就变为 0 了。但是好在用户对于未读数的准确度要求不高（未读 10 条还是 11 条，其实用户有时候看不出来），因此，这些缺陷也是可以接受的\n\n通过分享未读数系统设计这个案例，我想给你一些建议：\n\n 1. 缓存是提升系统性能和抵抗大并发量的神器，像是微博信息流未读数这么大的量级我们 仅仅使用十几台服务器就可以支撑，这全都是缓存的功劳；\n 2. 要围绕系统设计的关键困难点想解决办法，就像我们解决系统通知未读数的延迟问题一 样；\n 3. 合理分析业务场景，明确哪些是可以权衡的，哪些是不行的，会对你的系统设计增益良多，比如对于长久不登录用户，我们就会记录未读数为 0，通过这样的权衡，可以极大 地减少内存的占用，减少成本。\n\n\n# 总结\n\n 1. 评论未读、@未读、赞未读等一对一关系的未读数可以使用上节课讲到的通用计数方案来解决；\n 2. 在系统通知未读、全量用户打点等存在有限的共享存储的场景下，可以通过记录用户上次操作的时间或者偏移量，来实现未读方案；\n 3. 最后，信息流未读方案最为复杂，采用的是记录用户博文数快照的方式",normalizedContent:"# 引子\n\n未读数也是系统中一个常见的模块，以微博系统为例，你可看到有多个未读计数的场景，\n\n比如：\n\n * 当有人 @你、评论你、给你的博文点赞或者给你发送私信的时候，你会收到相应的未读提醒；\n * 在早期的微博版本中有系统通知的功能，也就是系统会给全部用户发送消息，通知用户有新的版本或者有一些好玩的运营活动，如果用户没有看，系统就会给他展示有多少条未读的提醒。\n * 我们在浏览信息流的时候，如果长时间没有刷新页面，那么信息流上方就会提示你在这段时间有多少条信息没有看\n\n\n# 系统通知的未读数要如何设计\n\n来看具体的例子。假如你的系统中只有 a、b、c 三个用户，那么你可以在通用计数系统中 增加一块儿内存区域，并且以用户 id 为 key 来存储这三个用户的未读通知数据，当系统发 送一个新的通知时，我们会循环给每一个用户的未读数加 1，这个处理逻辑的伪代码就像下 面这样\n\nlist<long> userids = getalluserids();\n    for(long id : userids) {\n    incrunreadcount(id);\n}\n\n\n但随着系统中的用户越来越多，这个方案存在两个致命的问题\n\n * 首先，获取全量用户就是一个比较耗时的操作，相当于对用户库做一次全表的扫描，这不仅会对数据库造成很大的压力，而且查询全量用户数据的响应时间是很长的，对于在线业务来说是难以接受的。如果你的用户库已经做了分库分表，那么就要扫描所有的库表，响应时间就更长了。不过有一个折中的方法， 那就是在发送系统通知之前，先从线下的数据仓库中 获取全量的用户 id，并且存储在一个本地的文件中，然后再轮询所有的用户 id，给这些用 户增加未读计数。\n   * 这似乎是一个可行的技术方案，然而它给所有人增加未读计数，会消耗非常长的时间。你计 算一下，假如你的系统中有一个亿的用户，给一个用户增加未读数需要消耗 1ms，那么给 所有人都增加未读计数就需要 100000000 * 1 /1000 = 100000 秒，也就是超过一天的时 间；即使你启动 100 个线程并发的设置，也需要十几分钟的时间才能完成，而用户很难接 受这么长的延迟时间。\n * 另外，使用这种方式需要给系统中的每一个用户都记一个未读数的值，而在系统中，活跃用 户只是很少的一部分，大部分的用户是不活跃的，甚至从来没有打开过系统通知，为这些用 户记录未读数显然是一种浪费。\n\n通过上面的内容，你可以知道为什么我们不能用通用计数系统实现系统通知未读数了吧？那正确的做法是什么呢\n\n要知道，系统通知实际上是存储在一个大的列表中的，这个列表对所有用户共享，也就是所有人看到的都是同一份系统通知的数据。不过不同的人最近看到的消息不同，所以每个人会有不同的未读数。因此，你可以记录一下在这个列表中每个人看过最后一条消息的 id，然 后统计这个 id 之后有多少条消息，这就是未读数了\n\n\n\n上述就是 timeline 模型\n\n这个方案在实现时有这样几个关键点：\n\n * 用户访问系统通知页面需要设置未读数为 0，我们需要将用户最近看过的通知 id 设置为最新的一条系统通知 id\n * 如果最近看过的通知 id 为空，则认为是一个新的用户，返回未读数为 0；\n * 对于非活跃用户，比如最近一个月都没有登录和使用过系统的用户，可以把用户最近看过的通知 id 清空，节省内存空间。\n\n这是一种比较通用的方案，即节省内存，又能尽量减少获取未读数的延迟。\n\n这个方案适用 的另一个业务场景是全量用户打点的场景，比如像下面这张微博截图中的红点\n\n\n\n这个红点和系统通知类似，也是一种通知全量用户的手段，如果逐个通知用户，延迟也是无法接受的。因此你可以采用和系统通知类似的方案。\n\n首先，我们为每一个用户存储一个时间戳，代表最近点过这个红点的时间，用户点了红点， 就把这个时间戳设置为当前时间；然后，我们也记录一个全局的时间戳，这个时间戳标识最新的一次打点时间，如果你在后台操作给全体用户打点，就更新这个时间戳为当前时间。而 我们在判断是否需要展示红点时，只需要判断用户的时间戳和全局时间戳的大小，如果用户时间戳小于全局时间戳，代表在用户最后一次点击红点之后又有新的红点推送，那么就要展 示红点，反之，就不展示红点了。\n\n\n\n这两个场景的共性是全部用户共享一份有限的存储数据，每个人只记录自己在这份存储中的 偏移量，就可以得到未读数了。\n\n你可以看到，系统消息未读的实现方案不是很复杂，它通过设计避免了操作全量数据未读数，如果你的系统中有这种打红点的需求，那我建议你可以结合实际工作灵活使用上述方案。\n\n最后一个需求关注的是微博信息流的未读数，在现在的社交系统中，关注关系已经成为标配的功能，而基于关注关系的信息流也是一种非常重要的信息聚合方式，因此，如何设计信息流的未读数系统就成了你必须面对的一个问题。\n\n\n# 如何为信息流的未读数设计方案\n\n信息流的未读数之所以复杂主要有这样几点原因\n\n首先，微博的信息流是基于关注关系的，未读数也是基于关注关系的，就是说，你关注的人发布了新的微博，那么你作为粉丝未读数就要增加 1。\n\n如果微博用户都是像我这样只有几百粉丝的“小透明”就简单了，你发微博的时候系统给你粉丝的未读数增加 1 不 是什么难事儿。但是对于一些动辄几千万甚至上亿粉丝的微博大 v 就麻烦了，增加未读 数可能需要几个小时。假设你是杨幂的粉丝，想了解她实时发布的博文，那么如果当她 发布博文几个小时之后，你才收到提醒，这显然是不能接受的。所以未读数的延迟是你在设计方案时首先要考虑的内容。\n\n其次，信息流未读数请求量极大、并发极高，这是因为接口是客户端轮询请求的，不是用户触发的。\n\n也就是说，用户即使打开微博客户端什么都不做，这个接口也会被请求到。在几年前，请求未读数接口的量级就已经接近每秒 50 万次，这几年随着微博量级的增长，请求量也变得更高。而作为微博的非核心接口，我们不太可能使用大量的机器来抗未读数请求，因此，如何使用有限的资源来支撑如此高的流量是这个方案的难点。 最后，它不像系统通知那样有共享的存储，因为每个人关注的人不同，信息流的列表也就不同，所以也就没办法采用系统通知未读数的方案。\n\n那要如何设计能够承接每秒几十万次请求的信息流未读数系统呢？你可以这样做：\n\n首先，在通用计数器中记录每一个用户发布的博文数； 然后在 redis 或者 memcached 中记录一个人所有关注人的博文数快照，当用户点击未读消息重置未读数为 0 时，将他关注所有人的博文数刷新到快照中； 这样，他关注所有人的博文总数减去快照中的博文总数就是他的信息流未读数。\n\n\n\n假如用户 a，像上图这样关注了用户 b、c、d，其中 b 发布的博文数是 10，c 发布的博 文数是 8，d 发布的博文数是 14，而在用户 a 最近一次查看未读消息时，记录在快照中的 这三个用户的博文数分别是 6、7、12，因此用户 a 的未读数就是（10-6）+（8-7）+（14-12）=7。\n\n这个方案设计简单，并且是全内存操作，性能足够好，能够支撑比较高的并发，事实上微博团队仅仅用 16 台普通的服务器就支撑了每秒接近 50 万次的请求，这就足以证明这个方案的性能有多出色，因此，它完全能够满足信息流未读数的需求。\n\n当然了这个方案也有一些缺陷，比如说快照中需要存储关注关系，如果关注关系变更的时候 更新不及时，那么就会造成未读数不准确；快照采用的是全缓存存储，如果缓存满了就会剔 除一些数据，那么被剔除用户的未读数就变为 0 了。但是好在用户对于未读数的准确度要求不高（未读 10 条还是 11 条，其实用户有时候看不出来），因此，这些缺陷也是可以接受的\n\n通过分享未读数系统设计这个案例，我想给你一些建议：\n\n 1. 缓存是提升系统性能和抵抗大并发量的神器，像是微博信息流未读数这么大的量级我们 仅仅使用十几台服务器就可以支撑，这全都是缓存的功劳；\n 2. 要围绕系统设计的关键困难点想解决办法，就像我们解决系统通知未读数的延迟问题一 样；\n 3. 合理分析业务场景，明确哪些是可以权衡的，哪些是不行的，会对你的系统设计增益良多，比如对于长久不登录用户，我们就会记录未读数为 0，通过这样的权衡，可以极大 地减少内存的占用，减少成本。\n\n\n# 总结\n\n 1. 评论未读、@未读、赞未读等一对一关系的未读数可以使用上节课讲到的通用计数方案来解决；\n 2. 在系统通知未读、全量用户打点等存在有限的共享存储的场景下，可以通过记录用户上次操作的时间或者偏移量，来实现未读方案；\n 3. 最后，信息流未读方案最为复杂，采用的是记录用户博文数快照的方式",charsets:{cjk:!0},lastUpdated:"2024/09/18, 03:50:07",lastUpdatedTimestamp:1726631407e3},{title:"高性能排行榜系统",frontmatter:{title:"高性能排行榜系统",date:"2024-09-18T11:47:06.000Z",permalink:"/pages/9e787a/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/10.%E8%AE%BE%E8%AE%A1%E7%BB%84%E4%BB%B6/01.%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%92%E8%A1%8C%E6%A6%9C%E7%B3%BB%E7%BB%9F.html",relativePath:"实战系统设计/10.设计组件/01.高性能排行榜系统.md",key:"v-4b29b46c",path:"/pages/9e787a/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 10:50:56",lastUpdatedTimestamp:1726656656e3},{title:"基于 Redis 实现延时消息",frontmatter:{title:"基于 Redis 实现延时消息",date:"2024-09-18T11:47:31.000Z",permalink:"/pages/bbce3f/"},regularPath:"/%E5%AE%9E%E6%88%98%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/10.%E8%AE%BE%E8%AE%A1%E7%BB%84%E4%BB%B6/05.%E5%9F%BA%E4%BA%8E%20Redis%20%E5%AE%9E%E7%8E%B0%E5%BB%B6%E6%97%B6%E6%B6%88%E6%81%AF.html",relativePath:"实战系统设计/10.设计组件/05.基于 Redis 实现延时消息.md",key:"v-b9edac60",path:"/pages/bbce3f/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/18, 10:50:56",lastUpdatedTimestamp:1726656656e3}],themeConfig:{blogInfo:{blogCreate:"2024-09-10",indexView:!1,pageView:!1,readingTime:!0,eachFileWords:[{name:"Bloom Filter",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/01.Bloom Filter.md",wordsCount:"1.7k",readingTime:"6.2m",title:"Bloom Filter",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/fccd91/"},{name:"Consistent Hashing",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/02.Consistent Hashing.md",wordsCount:"2.8k",readingTime:"9.9m",title:"Consistent Hashing",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/1e28a2/"},{name:"Count-Min Sketch",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/03.Count-Min Sketch.md",wordsCount:"2.4k",readingTime:"9.3m",title:"Count-Min Sketch",date:"2024-09-14T13:30:19.000Z",permalink:"/pages/8624c5"},{name:"LRU",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/04.LRU.md",wordsCount:"1.5k",readingTime:"5.8m",title:"LRU",date:"2024-09-14T16:39:57.000Z",permalink:"/pages/87589a"},{name:"LFU",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/05.LFU.md",wordsCount:24,readingTime:"1",title:"LFU",date:"2024-09-14T18:14:42.000Z",permalink:"/pages/7d22be/"},{name:"hash & rehash",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/06.hash & rehash.md",wordsCount:19,readingTime:"1",title:"hash & rehash",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d43d1/"},{name:"Timing Wheels",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/10.Timing Wheels.md",wordsCount:"6.9k",readingTime:"25.8m",title:"Timing Wheels",date:"2024-09-15T02:25:42.000Z",permalink:"/pages/44dcc2/"},{name:"介绍",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/01.一、前言/01.介绍.md",wordsCount:13,readingTime:"1",title:"介绍",date:"2024-09-15T21:10:46.000Z",permalink:"/pages/b9733b/"},{name:"指南",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/01.一、前言/01.指南.md",wordsCount:247,readingTime:"1m",title:"指南",date:"2024-09-17T16:51:40.000Z",permalink:"/pages/e21d7f/"},{name:"Kafka 整体架构",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/03.Kafka 整体架构.md",wordsCount:16,readingTime:"1",title:"Kafka 整体架构",date:"2024-09-18T17:19:19.000Z",permalink:"/pages/46a58a/"},{name:"生产者",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/05.生产者.md",wordsCount:"7.1k",readingTime:"32.6m",title:"生产者",date:"2024-09-18T12:41:49.000Z",permalink:"/pages/bb1005/"},{name:"Broker",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/07.Broker.md",wordsCount:24,readingTime:"1",title:"Broker",date:"2024-09-18T12:58:30.000Z",permalink:"/pages/aa0392/"},{name:"消费者",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/10.消费者.md",wordsCount:"1.2k",readingTime:"4.7m",title:"消费者",date:"2024-09-18T12:42:00.000Z",permalink:"/pages/ca141b/"},{name:"Kafka Producer 设计分析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/10.Kafka Producer 设计分析.md",wordsCount:"1.3k",readingTime:"4.6m",title:"Kafka Producer 设计分析",date:"2024-09-18T15:32:05.000Z",permalink:"/pages/19bf78/"},{name:"Kafka Producer 源码分析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/15.Kafka Producer 源码分析.md",wordsCount:"2.2k",readingTime:"8.5m",title:"Kafka Producer 源码分析",date:"2024-09-18T15:32:32.000Z",permalink:"/pages/32a8ff/"},{name:"RecordAccumulator 源码分析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/20.RecordAccumulator 源码分析.md",wordsCount:"3.3k",readingTime:"13.2m",title:"RecordAccumulator 源码分析",date:"2024-09-18T15:30:43.000Z",permalink:"/pages/b77953/"},{name:"Sender 类代码分析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/40.Sender 类代码分析.md",wordsCount:"2.5k",readingTime:"11.8m",title:"Sender 类代码分析",date:"2024-09-18T15:33:30.000Z",permalink:"/pages/bf1d55/"},{name:"NetworkClient 类代码分析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/500.「生产者」源码分析/50.NetworkClient 类代码分析.md",wordsCount:"1.3k",readingTime:"5.6m",title:"NetworkClient 类代码分析",date:"2024-09-18T15:34:03.000Z",permalink:"/pages/b2caf1/"},{name:"KafkaConsumer 类代码分析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/600.「消费者」源码分析/05.KafkaConsumer 类代码分析.md",wordsCount:"2.5k",readingTime:"10m",title:"KafkaConsumer 类代码分析",date:"2024-09-18T17:25:41.000Z",permalink:"/pages/6c0fd7/"},{name:"控制器",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/05.控制器.md",wordsCount:"2.9k",readingTime:"10.4m",title:"控制器",date:"2024-09-18T17:40:39.000Z",permalink:"/pages/d9df7d/"},{name:"协调器",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/10.协调器.md",wordsCount:"3.2k",readingTime:"11.2m",title:"Untitled",date:"2024-09-18T18:05:57.000Z",permalink:"/pages/b60a98/"},{name:"日志管理器",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/15.日志管理器.md",wordsCount:"1.1k",readingTime:"4.2m",title:"日志管理器",date:"2024-09-18T18:09:47.000Z",permalink:"/pages/b6cf91/"},{name:"副本管理器",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/06.三、主线任务/700.「Broker」原理分析/20.副本管理器.md",wordsCount:545,readingTime:"2m",title:"副本管理器",date:"2024-09-18T18:10:37.000Z",permalink:"/pages/f57330/"},{name:"高可用探究",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/08.四、设计目标/20.高可用探究.md",wordsCount:"3.6k",readingTime:"13.5m",title:"高可用探究",date:"2024-09-18T13:29:28.000Z",permalink:"/pages/2dec11/"},{name:"高扩展探究",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/08.四、设计目标/25.高扩展探究.md",wordsCount:16,readingTime:"1",title:"高扩展探究",date:"2024-09-18T13:29:40.000Z",permalink:"/pages/168766/"},{name:"高并发探究",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/08.四、设计目标/35.高并发探究.md",wordsCount:"2.7k",readingTime:"10m",title:"高并发探究",date:"2024-09-18T13:30:04.000Z",permalink:"/pages/dd50fc/"},{name:"介绍",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/03.Nginx 系统设计/01.Nginx 系统设计/01.介绍.md",wordsCount:13,readingTime:"1",title:"介绍",date:"2024-09-15T21:10:47.000Z",permalink:"/pages/4601ca/"},{name:"碎碎念",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/06.动态/01.碎碎念.md",wordsCount:353,readingTime:"1.3m",title:"碎碎念",date:"2024-09-15T01:18:31.000Z",permalink:"/pages/52ebd8/"},{name:"指南",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/05.一、前言/05.指南.md",wordsCount:367,readingTime:"1.6m",title:"指南",date:"2024-09-18T19:53:25.000Z",permalink:"/pages/bfab10/"},{name:"IO 多路复用详解",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/10.二、基础知识/07.IO 多路复用详解.md",wordsCount:"9.8k",readingTime:"36.3m",title:"IO 多路复用详解",date:"2024-09-19T09:17:34.000Z",permalink:"/pages/c0367e/"},{name:"零拷贝详解",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/10.二、基础知识/15.零拷贝详解.md",wordsCount:16,readingTime:"1",title:"零拷贝详解",date:"2024-09-18T21:08:24.000Z",permalink:"/pages/3f7882/"},{name:"Future 和 Promise",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/10.二、基础知识/20.Future 和 Promise.md",wordsCount:14,readingTime:"1",title:"Future 和 Promise",date:"2024-09-18T21:09:42.000Z",permalink:"/pages/cb89e4/"},{name:"TCP 拆包与粘包",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/10.二、基础知识/30.TCP 拆包与粘包.md",wordsCount:17,readingTime:"1",title:"TCP 拆包与粘包",date:"2024-09-18T21:15:00.000Z",permalink:"/pages/9b88cb/"},{name:"心跳机制详解",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/10.二、基础知识/40.心跳机制详解.md",wordsCount:17,readingTime:"1",title:"心跳机制详解",date:"2024-09-18T21:15:29.000Z",permalink:"/pages/cab36c/"},{name:"Netty 框架概述",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/20.三、主线任务/01.Netty 框架概述.md",wordsCount:883,readingTime:"3.4m",title:"Netty 框架概述",date:"2024-09-18T21:01:51.000Z",permalink:"/pages/b0bc66/"},{name:"Bootstrap（client）源码解析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/20.三、主线任务/02.Bootstrap（client）源码解析.md",wordsCount:"5.8k",readingTime:"23.9m",title:"Bootstrap（client）源码解析",date:"2024-09-18T21:10:11.000Z",permalink:"/pages/9582d0/"},{name:"Bootstrap（server）源码解析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/20.三、主线任务/03.Bootstrap（server）源码解析.md",wordsCount:"3.7k",readingTime:"15m",title:"Bootstrap（server）源码解析",date:"2024-09-18T21:59:55.000Z",permalink:"/pages/b2a14a/"},{name:"Channel 源码解析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/20.三、主线任务/05.Channel 源码解析.md",wordsCount:"5.8k",readingTime:"24.3m",title:"Channel 源码解析",date:"2024-09-18T21:11:11.000Z",permalink:"/pages/8d1ba9/Channel"},{name:"EventLoop 源码解析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/20.三、主线任务/10.EventLoop 源码解析.md",wordsCount:"8.8k",readingTime:"35.9m",title:"EventLoop 源码解析",date:"2024-09-18T21:10:31.000Z",permalink:"/pages/397456/"},{name:"ChannelHandler 源码解析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/20.三、主线任务/20.ChannelHandler 源码解析.md",wordsCount:16,readingTime:"1",title:"ChannelHandler 源码解析",date:"2024-09-18T21:11:52.000Z",permalink:"/pages/ce1f78/"},{name:"ChannelPipeline 源码解析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/20.三、主线任务/30.ChannelPipeline 源码解析.md",wordsCount:"6.4k",readingTime:"26.7m",title:"ChannelPipeline 源码解析",date:"2024-09-18T21:12:21.000Z",permalink:"/pages/4234c0/"},{name:"Bytebuf 源码解析",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/20.三、主线任务/40.Bytebuf 源码解析.md",wordsCount:16,readingTime:"1",title:"ByteBuf 源码解析",date:"2024-09-18T21:12:36.000Z",permalink:"/pages/43eb30/"},{name:"第一课：透过 Netty 看 IO 模型",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/25.四、深入 Netty 核心/05.第一课：透过 Netty 看 IO 模型.md",wordsCount:"11.7k",readingTime:"41.7m",title:"第一课：透过 Netty 看 IO 模型",date:"2024-09-18T21:07:35.000Z",permalink:"/pages/65674f/"},{name:"第二课：Reactor在Netty中的实现（创建篇）",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/25.四、深入 Netty 核心/10.第二课：Reactor在Netty中的实现（创建篇）.md",wordsCount:"8.5k",readingTime:"35m",title:"第二课：Reactor在 Netty 中的实现（创建篇）",date:"2024-09-19T09:55:47.000Z",permalink:"/pages/440035/"},{name:"第三课：Netty 核心引擎 Reactor 的运转架构",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/25.四、深入 Netty 核心/12.第三课：Netty 核心引擎 Reactor 的运转架构.md",wordsCount:"16.5k",readingTime:"1h4m",title:"第三课：Netty 核心引擎 Reactor 的运转架构",date:"2024-09-19T10:57:06.000Z",permalink:"/pages/bb668a/"},{name:"第四课：Netty 如何高效接收网络连接",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/25.四、深入 Netty 核心/15.第四课：Netty 如何高效接收网络连接.md",wordsCount:"12.4k",readingTime:"52.1m",title:"第四课：Netty 如何高效接收网络连接",date:"2024-09-19T11:10:03.000Z",permalink:"/pages/be98dc/"},{name:"第五课：Netty 如何高效接收网络数据",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/25.四、深入 Netty 核心/18.第五课：Netty 如何高效接收网络数据.md",wordsCount:"11.3k",readingTime:"46.2m",title:"第五课：Netty 如何高效接收网络数据",date:"2024-09-19T11:11:04.000Z",permalink:"/pages/0938c1/"},{name:"第六课：Netty 如何高效发送网络数据",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/25.四、深入 Netty 核心/20.第六课：Netty 如何高效发送网络数据.md",wordsCount:"23.2k",readingTime:"1h35m",title:"第六课：Netty 如何高效发送网络数据",date:"2024-09-19T11:12:27.000Z",permalink:"/pages/a73206/"},{name:"第七课：Netty 中IO事件的触发时机和传播流程",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/25.四、深入 Netty 核心/25.第七课：Netty 中IO事件的触发时机和传播流程.md",wordsCount:"22.9k",readingTime:"1h40m",title:"第七课：Netty 中IO事件的触发时机和传播流程",date:"2024-09-19T11:14:00.000Z",permalink:"/pages/a1b0fe/"},{name:"第一课：ByteBuf 体系的设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Netty 系统设计/30.五、深入 Netty 内存管理/05.第一课：ByteBuf 体系的设计与实现.md",wordsCount:"39.3k",readingTime:"2h47m",title:"第一课：ByteBuf 体系的设计与实现",date:"2024-09-19T11:20:49.000Z",permalink:"/pages/8be98a/"},{name:"指南",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/01.一、前言/01.指南.md",wordsCount:"1.1k",readingTime:"4.4m",title:"指南",date:"2024-09-15T17:31:05.000Z",permalink:"/pages/252196/"},{name:"Redis 伪码蓝图【必看】",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/01.一、前言/05.Redis 伪码蓝图【必看】.md",wordsCount:"1.1k",readingTime:"4.5m",title:"Redis 伪码蓝图【必看】",date:"2024-09-16T01:33:42.000Z",permalink:"/pages/69fbd7/"},{name:"String 设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/02.二、基础知识/01.String 设计与实现.md",wordsCount:"5.2k",readingTime:"19.2m",title:"String 设计与实现",date:"2024-09-16T02:46:18.000Z",permalink:"/pages/bdae41/"},{name:"List 设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/02.二、基础知识/02.List 设计与实现.md",wordsCount:"9.5k",readingTime:"35.9m",title:"List 设计与实现",date:"2024-09-16T02:46:18.000Z",permalink:"/pages/bd1e41/"},{name:"Hash 设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/02.二、基础知识/05.Hash 设计与实现.md",wordsCount:"4.7k",readingTime:"18.9m",title:"Hash 设计与实现",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d4311/"},{name:"ZSet 设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/02.二、基础知识/10.ZSet 设计与实现.md",wordsCount:"5.6k",readingTime:"20.8m",title:"ZSet 设计与实现",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d4312/"},{name:"Linux 中的 IO 多路复用",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.三、主线任务/01.Linux 中的 IO 多路复用.md",wordsCount:"5.8k",readingTime:"21.1m",title:"Linux 中的 IO 多路复用",date:"2024-09-15T17:16:08.000Z",permalink:"/pages/34fa27"},{name:"Redis Server 初始化",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.三、主线任务/03.Redis Server 初始化.md",wordsCount:"5.3k",readingTime:"19.9m",title:"Redis Server 初始化",date:"2024-09-16T03:04:10.000Z",permalink:"/pages/d4ecb9/"},{name:"Redis 的 Reactor 模型",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.三、主线任务/05.Redis 的 Reactor 模型.md",wordsCount:"4.9k",readingTime:"18.1m",title:"Redis 的 Reactor 模型",date:"2024-09-15T17:26:35.000Z",permalink:"/pages/d6b00d"},{name:"深入 Redis 事件驱动框架",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.三、主线任务/08.深入 Redis 事件驱动框架.md",wordsCount:"9.3k",readingTime:"34.6m",title:"深入 Redis 事件驱动框架",date:"2024-09-15T01:36:53.000Z",permalink:"/pages/264b06/"},{name:"Redis 的执行模式",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.三、主线任务/09.Redis 的执行模式.md",wordsCount:"4k",readingTime:"15.1m",title:"Redis 的执行模式",date:"2024-09-15T20:23:53.000Z",permalink:"/pages/e6d8ef/"},{name:"Redis 多IO线程",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.三、主线任务/16.Redis 多IO线程.md",wordsCount:"9.1k",readingTime:"34.4m",title:"Redis 多IO线程",date:"2024-09-17T20:15:03.000Z",permalink:"/pages/0850b6/"},{name:"LRU 策略",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.四、支线任务/05.LRU 策略.md",wordsCount:"6.5k",readingTime:"25.9m",title:"LRU 策略",date:"2024-09-15T23:59:53.000Z",permalink:"/pages/b43a19/"},{name:"LFU 策略",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.四、支线任务/10.LFU 策略.md",wordsCount:"5.3k",readingTime:"20.3m",title:"LFU 策略",date:"2024-09-15T23:59:53.000Z",permalink:"/pages/b43a89/"},{name:"Redis 过期策略",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.四、支线任务/13.Redis 过期策略.md",wordsCount:"2.1k",readingTime:"8.6m",title:"Redis 过期策略",date:"2024-09-16T03:23:25.000Z",permalink:"/pages/f44fbe/"},{name:"RDB 持久化",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.四、支线任务/15.RDB 持久化.md",wordsCount:"6.2k",readingTime:"23.8m",title:"RDB 持久化",date:"2024-09-16T03:23:41.000Z",permalink:"/pages/9b17a6/"},{name:"AOF 持久化",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.四、支线任务/17.AOF 持久化.md",wordsCount:"10.3k",readingTime:"38.3m",title:"AOF 持久化",date:"2024-09-16T03:23:41.000Z",permalink:"/pages/9b17a7/"},{name:"Redis 中的延迟监控",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.四、支线任务/20.Redis 中的延迟监控.md",wordsCount:"4.8k",readingTime:"17.7m",title:"Redis 中的延迟监控",date:"2024-09-15T23:27:13.000Z",permalink:"/pages/aa75e9/"},{name:"发布与订阅",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.四、支线任务/25.发布与订阅.md",wordsCount:16,readingTime:"1",title:"发布与订阅",date:"2024-09-18T01:00:52.000Z",permalink:"/pages/61d908/"},{name:"主从复制",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/05.五、集群/25.主从复制.md",wordsCount:"5.3k",readingTime:"19.3m",title:"主从复制",date:"2024-09-16T03:24:06.000Z",permalink:"/pages/ebc8dc/"},{name:"哨兵",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/05.五、集群/30.哨兵.md",wordsCount:13,readingTime:"1",title:"哨兵",date:"2024-09-16T03:24:20.000Z",permalink:"/pages/af8752/"},{name:"cluster",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/05.五、集群/35.cluster.md",wordsCount:"18.5k",readingTime:"1h10m",title:"cluster",date:"2024-09-16T03:24:30.000Z",permalink:"/pages/040403/"},{name:"分布式缓存",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/01.设计基础设施/01.分布式缓存.md",wordsCount:418,readingTime:"1.7m",title:"分布式缓存",date:"2024-09-14T02:09:39.000Z",permalink:"/pages/84cb49/"},{name:"限流器",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/01.设计基础设施/02.限流器.md",wordsCount:559,readingTime:"2.1m",title:"限流器",date:"2024-09-14T02:09:39.000Z",permalink:"/pages/57d5a5/"},{name:"热点探查（Top k）",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/01.设计基础设施/03.热点探查（Top k）.md",wordsCount:"3.4k",readingTime:"12.3m",title:"热点探查（Top k）",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/5dcb6b/"},{name:"消息队列",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/01.设计基础设施/04.消息队列.md",wordsCount:197,readingTime:"1m",title:"消息队列",date:"2024-09-14T16:43:00.000Z",permalink:"/pages/567090/"},{name:"订阅发布",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/01.设计基础设施/05.订阅发布.md",wordsCount:67,readingTime:"1",title:"服务通知",date:"2024-09-14T16:43:28.000Z",permalink:"/pages/8416e6/"},{name:"动态线程池",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/01.设计基础设施/06.动态线程池.md",wordsCount:16,readingTime:"1",title:"动态线程池",date:"2024-09-14T23:28:32.000Z",permalink:"/pages/d81a42/"},{name:"设计 微信",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/05.设计热门应用/01.设计 微信.md",wordsCount:15,readingTime:"1",title:"设计 微信",date:"2024-09-14T02:08:51.000Z",permalink:"/pages/a95d7d/"},{name:"设计Twitter",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/05.设计热门应用/02.设计Twitter.md",wordsCount:14,readingTime:"1",title:"设计Twitter",date:"2024-09-14T02:08:51.000Z",permalink:"/pages/90ad66/"},{name:"双写一致性",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/01.双写一致性.md",wordsCount:"5.6k",readingTime:"19.3m",title:"双写一致性",date:"2024-09-14T16:50:17.000Z",permalink:"/pages/def08a/"},{name:"缓存穿透",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/02.缓存穿透.md",wordsCount:"1.2k",readingTime:"4.2m",title:"缓存穿透",date:"2024-09-14T16:50:37.000Z",permalink:"/pages/1e9e8e/"},{name:"缓存击穿",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/03.缓存击穿.md",wordsCount:560,readingTime:"2m",title:"缓存击穿",date:"2024-09-14T16:50:56.000Z",permalink:"/pages/1d96b2/"},{name:"任务补偿",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/04.任务补偿.md",wordsCount:"3.3k",readingTime:"11.8m",title:"任务补偿",date:"2024-09-14T16:51:13.000Z",permalink:"/pages/24abe0/"},{name:"秒杀",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/05.秒杀.md",wordsCount:"4.6k",readingTime:"15.5m",title:"秒杀",date:"2024-09-14T16:51:28.000Z",permalink:"/pages/a72629/"},{name:"超卖",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/06.超卖.md",wordsCount:"1.8k",readingTime:"7m",title:"超卖",date:"2024-09-14T22:14:25.000Z",permalink:"/pages/8a57f2/"},{name:"多级缓存",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/07.多级缓存.md",wordsCount:15,readingTime:"1",title:"多级缓存",date:"2024-09-14T16:52:24.000Z",permalink:"/pages/51aa8b/"},{name:"超时&重试",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/08.超时&重试.md",wordsCount:"4.8k",readingTime:"16.9m",title:"超时&重试",date:"2024-09-14T16:52:35.000Z",permalink:"/pages/0dfb49/"},{name:"幂等&防重",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/09.幂等&防重.md",wordsCount:"3.5k",readingTime:"13m",title:"幂等&防重",date:"2024-09-14T16:52:57.000Z",permalink:"/pages/4fc8cb/"},{name:"海量数据计数",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/10.海量数据计数.md",wordsCount:"3.3k",readingTime:"11.5m",title:"海量数据计数",date:"2024-09-14T16:52:01.000Z",permalink:"/pages/f3295f/"},{name:"消息未读数系统",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/07.经典场景设计/11.消息未读数系统.md",wordsCount:"3k",readingTime:"10.5m",title:"消息未读数系统",date:"2024-09-14T23:57:17.000Z",permalink:"/pages/6b9d68/"},{name:"高性能排行榜系统",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/10.设计组件/01.高性能排行榜系统.md",wordsCount:19,readingTime:"1",title:"高性能排行榜系统",date:"2024-09-18T11:47:06.000Z",permalink:"/pages/9e787a/"},{name:"基于 Redis 实现延时消息",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/实战系统设计/10.设计组件/05.基于 Redis 实现延时消息.md",wordsCount:20,readingTime:"1",title:"基于 Redis 实现延时消息",date:"2024-09-18T11:47:31.000Z",permalink:"/pages/bbce3f/"}],mdFileCountType:"archives",totalWords:"archives"},pageButton:!1,nav:[{text:"🏠首页",link:"/"},{text:"✒️热门算法",link:"/pages/fccd91/"},{text:"🎖️赏析经典设计",items:[{text:"Redis 深度剖析",link:"/pages/252196/"},{text:"Kafka 深度剖析",link:"/pages/b9733b/"},{text:"Netty 深度剖析",link:"/pages/bfab10/"},{text:"Nginx 深度剖析",link:"/pages/4601ca/"}]},{text:"🧑‍💻实战系统设计",link:"/pages/84cb49/"},{text:"❓问答",link:"/pages/92b2ee/"},{text:"👀动态",link:"/pages/52ebd8/"}],logo:"/img/logo.png",repo:"echo-lxy/echo-system-design",searchMaxSuggestions:10,lastUpdated:"上次更新于",docsDir:"docs",editLinks:!0,editLinkText:"编辑",sidebar:{"/01.热门算法/":[{title:"热门算法",collapsable:!0,children:[["01.热门算法/01.Bloom Filter.md","Bloom Filter","/pages/fccd91/"],["01.热门算法/02.Consistent Hashing.md","Consistent Hashing","/pages/1e28a2/"],["01.热门算法/03.Count-Min Sketch.md","Count-Min Sketch","/pages/8624c5"],["01.热门算法/04.LRU.md","LRU","/pages/87589a"],["01.热门算法/05.LFU.md","LFU","/pages/7d22be/"],["01.热门算法/06.hash & rehash.md","hash & rehash","/pages/2d43d1/"],["01.热门算法/10.Timing Wheels.md","Timing Wheels","/pages/44dcc2/"]]}],catalogue:{},"/02.Kafka  系统设计/":[{title:"一、前言",collapsable:!0,children:[["01.一、前言/01.指南.md","指南","/pages/e21d7f/"]]},{title:"三、主线任务",collapsable:!0,children:[["06.三、主线任务/03.Kafka 整体架构.md","Kafka 整体架构","/pages/46a58a/"],["06.三、主线任务/05.生产者.md","生产者","/pages/bb1005/"],["06.三、主线任务/07.Broker.md","Broker","/pages/aa0392/"],["06.三、主线任务/10.消费者.md","消费者","/pages/ca141b/"],{title:"「生产者」源码分析",collapsable:!0,children:[["06.三、主线任务/500.「生产者」源码分析/10.Kafka Producer 设计分析.md","Kafka Producer 设计分析","/pages/19bf78/"],["06.三、主线任务/500.「生产者」源码分析/15.Kafka Producer 源码分析.md","Kafka Producer 源码分析","/pages/32a8ff/"],["06.三、主线任务/500.「生产者」源码分析/20.RecordAccumulator 源码分析.md","RecordAccumulator 源码分析","/pages/b77953/"],["06.三、主线任务/500.「生产者」源码分析/40.Sender 类代码分析.md","Sender 类代码分析","/pages/bf1d55/"],["06.三、主线任务/500.「生产者」源码分析/50.NetworkClient 类代码分析.md","NetworkClient 类代码分析","/pages/b2caf1/"]]},{title:"「消费者」源码分析",collapsable:!0,children:[["06.三、主线任务/600.「消费者」源码分析/05.KafkaConsumer 类代码分析.md","KafkaConsumer 类代码分析","/pages/6c0fd7/"]]},{title:"「Broker」原理分析",collapsable:!0,children:[["06.三、主线任务/700.「Broker」原理分析/05.控制器.md","控制器","/pages/d9df7d/"],["06.三、主线任务/700.「Broker」原理分析/10.协调器.md","Untitled","/pages/b60a98/"],["06.三、主线任务/700.「Broker」原理分析/15.日志管理器.md","日志管理器","/pages/b6cf91/"],["06.三、主线任务/700.「Broker」原理分析/20.副本管理器.md","副本管理器","/pages/f57330/"]]}]},{title:"四、设计目标",collapsable:!0,children:[["08.四、设计目标/20.高可用探究.md","高可用探究","/pages/2dec11/"],["08.四、设计目标/25.高扩展探究.md","高扩展探究","/pages/168766/"],["08.四、设计目标/35.高并发探究.md","高并发探究","/pages/dd50fc/"]]}],"/03.Nginx 系统设计/":[{title:"Nginx 系统设计",collapsable:!0,children:[["01.Nginx 系统设计/01.介绍.md","介绍","/pages/4601ca/"]]}],"/06.动态/":[["01.碎碎念.md","碎碎念","/pages/52ebd8/"]],"/Netty 系统设计/":[{title:"一、前言",collapsable:!0,children:[["05.一、前言/05.指南.md","指南","/pages/bfab10/"]]},{title:"二、基础知识",collapsable:!0,children:[["10.二、基础知识/07.IO 多路复用详解.md","IO 多路复用详解","/pages/c0367e/"],["10.二、基础知识/15.零拷贝详解.md","零拷贝详解","/pages/3f7882/"],["10.二、基础知识/20.Future 和 Promise.md","Future 和 Promise","/pages/cb89e4/"],["10.二、基础知识/30.TCP 拆包与粘包.md","TCP 拆包与粘包","/pages/9b88cb/"],["10.二、基础知识/40.心跳机制详解.md","心跳机制详解","/pages/cab36c/"]]},{title:"三、主线任务",collapsable:!0,children:[["20.三、主线任务/01.Netty 框架概述.md","Netty 框架概述","/pages/b0bc66/"],["20.三、主线任务/02.Bootstrap（client）源码解析.md","Bootstrap（client）源码解析","/pages/9582d0/"],["20.三、主线任务/03.Bootstrap（server）源码解析.md","Bootstrap（server）源码解析","/pages/b2a14a/"],["20.三、主线任务/05.Channel 源码解析.md","Channel 源码解析","/pages/8d1ba9/Channel"],["20.三、主线任务/10.EventLoop 源码解析.md","EventLoop 源码解析","/pages/397456/"],["20.三、主线任务/20.ChannelHandler 源码解析.md","ChannelHandler 源码解析","/pages/ce1f78/"],["20.三、主线任务/30.ChannelPipeline 源码解析.md","ChannelPipeline 源码解析","/pages/4234c0/"],["20.三、主线任务/40.Bytebuf 源码解析.md","ByteBuf 源码解析","/pages/43eb30/"]]},{title:"四、深入 Netty 核心",collapsable:!0,children:[["25.四、深入 Netty 核心/05.第一课：透过 Netty 看 IO 模型.md","第一课：透过 Netty 看 IO 模型","/pages/65674f/"],["25.四、深入 Netty 核心/10.第二课：Reactor在Netty中的实现（创建篇）.md","第二课：Reactor在 Netty 中的实现（创建篇）","/pages/440035/"],["25.四、深入 Netty 核心/12.第三课：Netty 核心引擎 Reactor 的运转架构.md","第三课：Netty 核心引擎 Reactor 的运转架构","/pages/bb668a/"],["25.四、深入 Netty 核心/15.第四课：Netty 如何高效接收网络连接.md","第四课：Netty 如何高效接收网络连接","/pages/be98dc/"],["25.四、深入 Netty 核心/18.第五课：Netty 如何高效接收网络数据.md","第五课：Netty 如何高效接收网络数据","/pages/0938c1/"],["25.四、深入 Netty 核心/20.第六课：Netty 如何高效发送网络数据.md","第六课：Netty 如何高效发送网络数据","/pages/a73206/"],["25.四、深入 Netty 核心/25.第七课：Netty 中IO事件的触发时机和传播流程.md","第七课：Netty 中IO事件的触发时机和传播流程","/pages/a1b0fe/"]]},{title:"五、深入 Netty 内存管理",collapsable:!0,children:[["30.五、深入 Netty 内存管理/05.第一课：ByteBuf 体系的设计与实现.md","第一课：ByteBuf 体系的设计与实现","/pages/8be98a/"]]}],"/Redis 系统设计/":[{title:"一、前言",collapsable:!0,children:[["01.一、前言/01.指南.md","指南","/pages/252196/"],["01.一、前言/05.Redis 伪码蓝图【必看】.md","Redis 伪码蓝图【必看】","/pages/69fbd7/"]]},{title:"二、基础知识",collapsable:!0,children:[["02.二、基础知识/01.String 设计与实现.md","String 设计与实现","/pages/bdae41/"],["02.二、基础知识/02.List 设计与实现.md","List 设计与实现","/pages/bd1e41/"],["02.二、基础知识/05.Hash 设计与实现.md","Hash 设计与实现","/pages/2d4311/"],["02.二、基础知识/10.ZSet 设计与实现.md","ZSet 设计与实现","/pages/2d4312/"]]},{title:"三、主线任务",collapsable:!0,children:[["03.三、主线任务/01.Linux 中的 IO 多路复用.md","Linux 中的 IO 多路复用","/pages/34fa27"],["03.三、主线任务/03.Redis Server 初始化.md","Redis Server 初始化","/pages/d4ecb9/"],["03.三、主线任务/05.Redis 的 Reactor 模型.md","Redis 的 Reactor 模型","/pages/d6b00d"],["03.三、主线任务/08.深入 Redis 事件驱动框架.md","深入 Redis 事件驱动框架","/pages/264b06/"],["03.三、主线任务/09.Redis 的执行模式.md","Redis 的执行模式","/pages/e6d8ef/"],["03.三、主线任务/16.Redis 多IO线程.md","Redis 多IO线程","/pages/0850b6/"]]},{title:"四、支线任务",collapsable:!0,children:[["04.四、支线任务/05.LRU 策略.md","LRU 策略","/pages/b43a19/"],["04.四、支线任务/10.LFU 策略.md","LFU 策略","/pages/b43a89/"],["04.四、支线任务/13.Redis 过期策略.md","Redis 过期策略","/pages/f44fbe/"],["04.四、支线任务/15.RDB 持久化.md","RDB 持久化","/pages/9b17a6/"],["04.四、支线任务/17.AOF 持久化.md","AOF 持久化","/pages/9b17a7/"],["04.四、支线任务/20.Redis 中的延迟监控.md","Redis 中的延迟监控","/pages/aa75e9/"],["04.四、支线任务/25.发布与订阅.md","发布与订阅","/pages/61d908/"]]},{title:"五、集群",collapsable:!0,children:[["05.五、集群/25.主从复制.md","主从复制","/pages/ebc8dc/"],["05.五、集群/30.哨兵.md","哨兵","/pages/af8752/"],["05.五、集群/35.cluster.md","cluster","/pages/040403/"]]}],"/实战系统设计/":[{title:"设计基础设施",collapsable:!0,children:[["01.设计基础设施/01.分布式缓存.md","分布式缓存","/pages/84cb49/"],["01.设计基础设施/02.限流器.md","限流器","/pages/57d5a5/"],["01.设计基础设施/03.热点探查（Top k）.md","热点探查（Top k）","/pages/5dcb6b/"],["01.设计基础设施/04.消息队列.md","消息队列","/pages/567090/"],["01.设计基础设施/05.订阅发布.md","服务通知","/pages/8416e6/"],["01.设计基础设施/06.动态线程池.md","动态线程池","/pages/d81a42/"]]},{title:"设计热门应用",collapsable:!0,children:[["05.设计热门应用/01.设计 微信.md","设计 微信","/pages/a95d7d/"],["05.设计热门应用/02.设计Twitter.md","设计Twitter","/pages/90ad66/"]]},{title:"经典场景设计",collapsable:!0,children:[["07.经典场景设计/01.双写一致性.md","双写一致性","/pages/def08a/"],["07.经典场景设计/02.缓存穿透.md","缓存穿透","/pages/1e9e8e/"],["07.经典场景设计/03.缓存击穿.md","缓存击穿","/pages/1d96b2/"],["07.经典场景设计/04.任务补偿.md","任务补偿","/pages/24abe0/"],["07.经典场景设计/05.秒杀.md","秒杀","/pages/a72629/"],["07.经典场景设计/06.超卖.md","超卖","/pages/8a57f2/"],["07.经典场景设计/07.多级缓存.md","多级缓存","/pages/51aa8b/"],["07.经典场景设计/08.超时&重试.md","超时&重试","/pages/0dfb49/"],["07.经典场景设计/09.幂等&防重.md","幂等&防重","/pages/4fc8cb/"],["07.经典场景设计/10.海量数据计数.md","海量数据计数","/pages/f3295f/"],["07.经典场景设计/11.消息未读数系统.md","消息未读数系统","/pages/6b9d68/"]]},{title:"设计组件",collapsable:!0,children:[["10.设计组件/01.高性能排行榜系统.md","高性能排行榜系统","/pages/9e787a/"],["10.设计组件/05.基于 Redis 实现延时消息.md","基于 Redis 实现延时消息","/pages/bbce3f/"]]}]},updateBar:{showToArticle:!1},pageStyle:"line",category:!1,tag:!1,author:{name:"echo",href:"https://gitee.com/brother-one"},social:{icons:[{iconClass:"icon-youjian",title:"发邮件",link:"mailto:lixinyang2002@163.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/echo-lxy"},{iconClass:"icon-erji",title:"听音乐",link:"https://music.163.com/#/playlist?id=755597173"}]},footer:{createYear:2024,copyrightInfo:"Xinyang Li | MIT License"},htmlModules:{pageT:'\n    <div class="wwads-cn wwads-horizontal page-wwads" data-id="136"></div>\n    <style>\n      .page-wwads{\n        width:100%!important;\n        min-height: 0;\n        margin: 0;\n      }\n      .page-wwads .wwads-img img{\n        width:80px!important;\n      }\n      .page-wwads .wwads-poweredby{\n        width: 40px;\n        position: absolute;\n        right: 25px;\n        bottom: 3px;\n      }\n      .wwads-content .wwads-text, .page-wwads .wwads-text{\n        height: 100%;\n        padding-top: 5px;\n        display: block;\n      }\n  </style>\n  '}}};var Rs=t(95),Is=t(96),Os=t(11);var As={computed:{$filterPosts(){return this.$site.pages.filter(e=>{const{frontmatter:{pageComponent:n,article:t,home:r}}=e;return!(n||!1===t||!0===r)})},$sortPosts(){return(e=this.$filterPosts).sort((e,n)=>{const t=e.frontmatter.sticky,r=n.frontmatter.sticky;return t&&r?t==r?Object(Os.a)(e,n):t-r:t&&!r?-1:!t&&r?1:Object(Os.a)(e,n)}),e;var e},$sortPostsByDate(){return(e=this.$filterPosts).sort((e,n)=>Object(Os.a)(e,n)),e;var e},$groupPosts(){return function(e){const n={},t={};for(let r=0,a=e.length;r<a;r++){const{frontmatter:{categories:a,tags:i}}=e[r];"array"===Object(Os.n)(a)&&a.forEach(t=>{t&&(n[t]||(n[t]=[]),n[t].push(e[r]))}),"array"===Object(Os.n)(i)&&i.forEach(n=>{n&&(t[n]||(t[n]=[]),t[n].push(e[r]))})}return{categories:n,tags:t}}(this.$sortPosts)},$categoriesAndTags(){return function(e){const n=[],t=[];for(let t in e.categories)n.push({key:t,length:e.categories[t].length});for(let n in e.tags)t.push({key:n,length:e.tags[n].length});return{categories:n,tags:t}}(this.$groupPosts)}}};Jt.component(Rs.default),Jt.component(Is.default);function Ts(e){return e.toString().padStart(2,"0")}t(245);Jt.component("PageInfo",()=>t.e(7).then(t.bind(null,344))),Jt.component("WebInfo",()=>Promise.all([t.e(0),t.e(4)]).then(t.bind(null,342))),Jt.component("Badge",()=>Promise.all([t.e(0),t.e(5)]).then(t.bind(null,440))),Jt.component("CodeBlock",()=>Promise.resolve().then(t.bind(null,95))),Jt.component("CodeGroup",()=>Promise.resolve().then(t.bind(null,96)));t(246);var Ns=[({Vue:e,options:n,router:t,siteData:r,isServer:a})=>{a||t.afterEach(()=>{var e;e=function(){setTimeout((function(){void 0===window._AdBlockInit&&function(){const e=document.getElementsByClassName("wwads-cn"),n=document.querySelector(".wwads-content");e[0]&&!n&&(e[0].innerHTML=h)}()}),3e3)},"complete"===document.readyState||"interactive"===document.readyState?setTimeout(e,1):document.addEventListener("DOMContentLoaded",e),setTimeout(()=>{const e=document.querySelector(".page-wwads");if(!e)return;const n=e.querySelector(".wwads-hide");n&&(n.onclick=()=>{e.style.display="none"}),"none"===e.style.display&&(e.style.display="flex")},900)})},({Vue:e,options:n,router:t,siteData:r})=>{r.pages.map(e=>{const{frontmatter:{date:n,author:t}}=e;"string"==typeof n&&"Z"===n.charAt(n.length-1)&&(e.frontmatter.date=function(e){e instanceof Date||(e=new Date(e));return`${e.getUTCFullYear()}-${Ts(e.getUTCMonth()+1)}-${Ts(e.getUTCDate())} ${Ts(e.getUTCHours())}:${Ts(e.getUTCMinutes())}:${Ts(e.getUTCSeconds())}`}(n)),t?e.author=t:r.themeConfig.author&&(e.author=r.themeConfig.author)}),e.mixin(As)},{},({Vue:e})=>{e.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},({router:e})=>{"undefined"!=typeof window&&(window._hmt=window._hmt||[],function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?01293bffa6c3962016c08ba685c79d78";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n)}(),e.afterEach((function(e){_hmt.push(["_trackPageview",e.fullPath])})))}],zs=["PageInfo"];class Ls extends class{constructor(){this.store=new Jt({data:{state:{}}})}$get(e){return this.store.state[e]}$set(e,n){Jt.set(this.store.state,e,n)}$emit(...e){this.store.$emit(...e)}$on(...e){this.store.$on(...e)}}{}Object.assign(Ls.prototype,{getPageAsyncComponent:sl,getLayoutAsyncComponent:cl,getAsyncComponent:dl,getVueComponent:ul});var Ps={install(e){const n=new Ls;e.$vuepress=n,e.prototype.$vuepress=n}};function Ds(e,n){const t=n.toLowerCase();return e.options.routes.some(e=>e.path.toLowerCase()===t)}var Fs={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(e){const n=this.pageKey||this.$parent.$page.key;return hl("pageKey",n),Jt.component(n)||Jt.component(n,sl(n)),Jt.component(n)?e(n):e("")}},Us={functional:!0,props:{slotKey:String,required:!0},render:(e,{props:n,slots:t})=>e("div",{class:["content__"+n.slotKey]},t()[n.slotKey])},Hs={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},js=(t(247),t(248),Object(Al.a)(Hs,(function(){var e=this._self._c;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),Ms={functional:!0,render(e,{parent:n,children:t}){if(n._isMounted)return t;n.$once("hook:mounted",()=>{n.$forceUpdate()})}};Jt.config.productionTip=!1,Jt.use(Wo),Jt.use(Ps),Jt.mixin(function(e,n,t=Jt){!function(e){e.locales&&Object.keys(e.locales).forEach(n=>{e.locales[n].path=n});Object.freeze(e)}(n),t.$vuepress.$set("siteData",n);const r=new(e(t.$vuepress.$get("siteData"))),a=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(r)),i={};return Object.keys(a).reduce((e,n)=>(n.startsWith("$")&&(e[n]=a[n].get),e),i),{computed:i}}(e=>class{setPage(e){this.__page=e}get $site(){return e}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:e={}}=this.$site;let n,t;for(const r in e)"/"===r?t=e[r]:0===this.$page.path.indexOf(r)&&(n=e[r]);return n||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:e}=this.$page.frontmatter;return"string"==typeof e&&e}get $title(){const e=this.$page,{metaTitle:n}=this.$page.frontmatter;if("string"==typeof n)return n;const t=this.$siteTitle,r=e.frontmatter.home?null:e.frontmatter.title||e.title;return t?r?r+" | "+t:t:r||"VuePress"}get $description(){const e=function(e){if(e){const n=e.filter(e=>"description"===e.name)[0];if(n)return n.content}}(this.$page.frontmatter.meta);return e||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(e,n){for(let t=0;t<e.length;t++){const r=e[t];if(r.path.toLowerCase()===n.toLowerCase())return r}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},Ss)),Jt.component("Content",Fs),Jt.component("ContentSlotsDistributor",Us),Jt.component("OutboundLink",js),Jt.component("ClientOnly",Ms),Jt.component("Layout",cl("Layout")),Jt.component("NotFound",cl("NotFound")),Jt.prototype.$withBase=function(e){const n=this.$site.base;return"/"===e.charAt(0)?n+e.slice(1):e},window.__VUEPRESS__={version:"1.9.9",hash:"b99ba83"},async function(e){const n="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:Ss.routerBase||Ss.base,t=new Wo({base:n,mode:"history",fallback:!1,routes:Bs,scrollBehavior:(e,n,t)=>t||(e.hash?!Jt.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(e.hash)}:{x:0,y:0})});!function(e){e.beforeEach((n,t,r)=>{if(Ds(e,n.path))r();else if(/(\/|\.html)$/.test(n.path))if(/\/$/.test(n.path)){const t=n.path.replace(/\/$/,"")+".html";Ds(e,t)?r(t):r()}else r();else{const t=n.path+"/",a=n.path+".html";Ds(e,a)?r(a):Ds(e,t)?r(t):r()}})}(t);const r={};try{await Promise.all(Ns.filter(e=>"function"==typeof e).map(n=>n({Vue:Jt,options:r,router:t,siteData:Ss,isServer:e})))}catch(e){console.error(e)}return{app:new Jt(Object.assign(r,{router:t,render:e=>e("div",{attrs:{id:"app"}},[e("RouterView",{ref:"layout"}),e("div",{class:"global-ui"},zs.map(n=>e(n)))])})),router:t}}(!1).then(({app:e,router:n})=>{n.onReady(()=>{e.$mount("#app")})})}]);