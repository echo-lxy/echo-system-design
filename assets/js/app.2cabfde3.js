(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var r,a,o=e[0],l=e[1],d=e[2],u=0,p=[];u<o.length;u++)a=o[u],Object.prototype.hasOwnProperty.call(i,a)&&i[a]&&p.push(i[a][0]),i[a]=0;for(r in l)Object.prototype.hasOwnProperty.call(l,r)&&(n[r]=l[r]);for(c&&c(e);p.length;)p.shift()();return s.push.apply(s,d||[]),t()}function t(){for(var n,e=0;e<s.length;e++){for(var t=s[e],r=!0,o=1;o<t.length;o++){var l=t[o];0!==i[l]&&(r=!1)}r&&(s.splice(e--,1),n=a(a.s=t[0]))}return n}var r={},i={1:0},s=[];function a(e){if(r[e])return r[e].exports;var t=r[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,a),t.l=!0,t.exports}a.e=function(n){var e=[],t=i[n];if(0!==t)if(t)e.push(t[2]);else{var r=new Promise((function(e,r){t=i[n]=[e,r]}));e.push(t[2]=r);var s,o=document.createElement("script");o.charset="utf-8",o.timeout=120,a.nc&&o.setAttribute("nonce",a.nc),o.src=function(n){return a.p+"assets/js/"+({}[n]||n)+"."+{2:"933750c6",3:"a408e1cc",4:"73518a0f",5:"39da0049",6:"0b558b79",7:"7eb70372",8:"0ec7f6e8",9:"6e274fca",10:"2cc95be8",11:"c7521a10",12:"38d1989d",13:"850793b0",14:"1141ec27",15:"13af8159",16:"ad3bea63",17:"5a68d813",18:"65fbb5d7",19:"d3a922ae",20:"955beaf3",21:"a4ec6789",22:"464f596d",23:"4a651ae2",24:"3a2179f2",25:"baaceee4",26:"ef7c5123",27:"1f8b0123",28:"0f20c8df",29:"c1bc7826",30:"f0a56d8e",31:"15ca8a18",32:"341f6cc8",33:"666ec040",34:"92c43a51",35:"9443fc09",36:"cc54b233",37:"6e218346",38:"7ef093e6",39:"10c8db43",40:"9297d7be",41:"e3eb01b6",42:"ad9e260f",43:"abf2240d",44:"883ddc3a",45:"b88c01aa",46:"30cb3360",47:"1356924b",48:"f93e48db",49:"90bb7d7a",50:"f367f440",51:"ce476bce",52:"d6296891",53:"15aa3cce",54:"8077b7be",55:"8ecb06cd",56:"7046b51c",57:"b19aebc0",58:"e03a4aa6",59:"66f0b66e",60:"60d6167b"}[n]+".js"}(n);var l=new Error;s=function(e){o.onerror=o.onload=null,clearTimeout(d);var t=i[n];if(0!==t){if(t){var r=e&&("load"===e.type?"missing":e.type),s=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+r+": "+s+")",l.name="ChunkLoadError",l.type=r,l.request=s,t[1](l)}i[n]=void 0}};var d=setTimeout((function(){s({type:"timeout",target:o})}),12e4);o.onerror=o.onload=s,document.head.appendChild(o)}return Promise.all(e)},a.m=n,a.c=r,a.d=function(n,e,t){a.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},a.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},a.t=function(n,e){if(1&e&&(n=a(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(a.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var r in n)a.d(t,r,function(e){return n[e]}.bind(null,r));return t},a.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return a.d(e,"a",e),e},a.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},a.p="/",a.oe=function(n){throw console.error(n),n};var o=window.webpackJsonp=window.webpackJsonp||[],l=o.push.bind(o);o.push=e,o=o.slice();for(var d=0;d<o.length;d++)e(o[d]);var c=l;s.push([107,0]),t()}([function(n,e,t){"use strict";var r=function(n){return n&&n.Math===Math&&n};n.exports=r("object"==typeof globalThis&&globalThis)||r("object"==typeof window&&window)||r("object"==typeof self&&self)||r("object"==typeof global&&global)||r("object"==typeof this&&this)||function(){return this}()||Function("return this")()},function(n,e,t){"use strict";var r="object"==typeof document&&document.all;n.exports=void 0===r&&void 0!==r?function(n){return"function"==typeof n||n===r}:function(n){return"function"==typeof n}},function(n,e,t){"use strict";var r=t(26),i=Function.prototype,s=i.call,a=r&&i.bind.bind(s,s);n.exports=r?a:function(n){return function(){return s.apply(n,arguments)}}},function(n,e,t){"use strict";n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){"use strict";function r(n,e,t,r,i,s,a,o){var l,d="function"==typeof n?n.options:n;if(e&&(d.render=e,d.staticRenderFns=t,d._compiled=!0),r&&(d.functional=!0),s&&(d._scopeId="data-v-"+s),a?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),i&&i.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(a)},d._ssrRegister=l):i&&(l=o?function(){i.call(this,(d.functional?this.parent:this).$root.$options.shadowRoot)}:i),l)if(d.functional){d._injectStyles=l;var c=d.render;d.render=function(n,e){return l.call(e),c(n,e)}}else{var u=d.beforeCreate;d.beforeCreate=u?[].concat(u,l):[l]}return{exports:n,options:d}}t.d(e,"a",(function(){return r}))},function(n,e,t){"use strict";var r=t(3);n.exports=!r((function(){return 7!==Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){"use strict";var r=t(1);n.exports=function(n){return"object"==typeof n?null!==n:r(n)}},function(n,e,t){var r=t(69),i="object"==typeof self&&self&&self.Object===Object&&self,s=r||i||Function("return this")();n.exports=s},function(n,e,t){"use strict";var r=t(2),i=t(31),s=r({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return s(i(n),e)}},function(n,e,t){var r=t(165),i=t(168);n.exports=function(n,e){var t=i(n,e);return r(t)?t:void 0}},function(n,e,t){"use strict";t.d(e,"e",(function(){return r})),t.d(e,"b",(function(){return s})),t.d(e,"j",(function(){return a})),t.d(e,"g",(function(){return l})),t.d(e,"h",(function(){return d})),t.d(e,"i",(function(){return c})),t.d(e,"c",(function(){return u})),t.d(e,"f",(function(){return p})),t.d(e,"l",(function(){return f})),t.d(e,"m",(function(){return h})),t.d(e,"d",(function(){return v})),t.d(e,"k",(function(){return _})),t.d(e,"n",(function(){return g})),t.d(e,"a",(function(){return y}));t(16);const r=/#.*$/,i=/\.(md|html)$/,s=/\/$/,a=/^[a-z]+:/i;function o(n){return decodeURI(n).replace(r,"").replace(i,"")}function l(n){return a.test(n)}function d(n){return/^mailto:/.test(n)}function c(n){return/^tel:/.test(n)}function u(n){if(l(n))return n;if(!n)return"404";const e=n.match(r),t=e?e[0]:"",i=o(n);return s.test(i)?n:i+".html"+t}function p(n,e){const t=n.hash,i=function(n){const e=n&&n.match(r);if(e)return e[0]}(e);if(i&&t!==i)return!1;return o(n.path)===o(e)}function f(n,e,t){if(l(e))return{type:"external",path:e};t&&(e=function(n,e,t){const r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;const i=e.split("/");t&&i[i.length-1]||i.pop();const s=n.replace(/^\//,"").split("/");for(let n=0;n<s.length;n++){const e=s[n];".."===e?i.pop():"."!==e&&i.push(e)}""!==i[0]&&i.unshift("");return i.join("/")}(e,t));const r=o(e);for(let e=0;e<n.length;e++)if(o(n[e].regularPath)===r)return Object.assign({},n[e],{type:"page",path:u(n[e].path)});return console.error(`[vuepress] No matching page found for sidebar item "${e}"`),{}}function h(n,e,t,r){const{pages:i,themeConfig:s}=t,a=r&&s.locales&&s.locales[r]||s;if("auto"===(n.frontmatter.sidebar||a.sidebar||s.sidebar))return m(n);const o=a.sidebar||s.sidebar;if(o){const{base:t,config:r}=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(const r in e)if(0===(t=n,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(r)))return{base:r,config:e[r]};var t;return{}}(e,o);return"auto"===r?m(n):r?r.map(n=>function n(e,t,r,i=1){if("string"==typeof e)return f(t,e,r);if(Array.isArray(e))return Object.assign(f(t,e[0],r),{title:e[1]});{i>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const s=e.children||[];return 0===s.length&&e.path?Object.assign(f(t,e.path,r),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:s.map(e=>n(e,t,r,i+1)),collapsable:!1!==e.collapsable}}}(n,i,t)):[]}return[]}function m(n){const e=v(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map(e=>({type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}))}]}function v(n){let e;return(n=n.map(n=>Object.assign({},n))).forEach(n=>{2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)}),n.filter(n=>2===n.level)}function _(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function g(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function b(n){let e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function y(n,e){return b(e)-b(n)}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){var r=t(15),i=t(150),s=t(151),a=r?r.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":a&&a in Object(n)?i(n):s(n)}},function(n,e,t){"use strict";var r=t(5),i=t(17),s=t(34);n.exports=r?function(n,e,t){return i.f(n,e,s(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){var r=t(8).Symbol;n.exports=r},function(n,e,t){"use strict";var r=t(25),i=t(31),s=t(32),a=t(144),o=t(146);r({target:"Array",proto:!0,arity:1,forced:t(3)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(n){return n instanceof TypeError}}()},{push:function(n){var e=i(this),t=s(e),r=arguments.length;o(t+r);for(var l=0;l<r;l++)e[t]=arguments[l],t++;return a(e,t),t}})},function(n,e,t){"use strict";var r=t(5),i=t(64),s=t(101),a=t(47),o=t(54),l=TypeError,d=Object.defineProperty,c=Object.getOwnPropertyDescriptor;e.f=r?s?function(n,e,t){if(a(n),e=o(e),a(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var r=c(n,e);r&&r.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:r.configurable,enumerable:"enumerable"in t?t.enumerable:r.enumerable,writable:!1})}return d(n,e,t)}:d:function(n,e,t){if(a(n),e=o(e),a(t),i)try{return d(n,e,t)}catch(n){}if("get"in t||"set"in t)throw new l("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){"use strict";var r=t(2),i=r({}.toString),s=r("".slice);n.exports=function(n){return s(i(n),8,-1)}},function(n,e,t){var r=t(155),i=t(156),s=t(157),a=t(158),o=t(159);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=i,l.prototype.get=s,l.prototype.has=a,l.prototype.set=o,n.exports=l},function(n,e,t){var r=t(71);n.exports=function(n,e){for(var t=n.length;t--;)if(r(n[t][0],e))return t;return-1}},function(n,e,t){var r=t(10)(Object,"create");n.exports=r},function(n,e,t){var r=t(177);n.exports=function(n,e){var t=n.__data__;return r(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var r=t(45);n.exports=function(n){if("string"==typeof n||r(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var r,i;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(i="function"==typeof(r=function(){var n,e,t={version:"0.2.0"},r=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function i(n,e,t){return n<e?e:n>t?t:n}function s(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(r[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=i(n,r.minimum,1),t.status=1===n?null:n;var l=t.render(!e),d=l.querySelector(r.barSelector),c=r.speed,u=r.easing;return l.offsetWidth,a((function(e){""===r.positionUsing&&(r.positionUsing=t.getPositioningCSS()),o(d,function(n,e,t){var i;return(i="translate3d"===r.positionUsing?{transform:"translate3d("+s(n)+"%,0,0)"}:"translate"===r.positionUsing?{transform:"translate("+s(n)+"%,0)"}:{"margin-left":s(n)+"%"}).transition="all "+e+"ms "+t,i}(n,c,u)),1===n?(o(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){o(l,{transition:"all "+c+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),c)}),c)):setTimeout(e,c)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),r.trickleSpeed)};return r.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*i(Math.random()*e,.1,.95)),e=i(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*r.trickleRate)},n=0,e=0,t.promise=function(r){return r&&"resolved"!==r.state()?(0===e&&t.start(),n++,e++,r.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");d(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=r.template;var i,a=e.querySelector(r.barSelector),l=n?"-100":s(t.status||0),c=document.querySelector(r.parent);return o(a,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),r.showSpinner||(i=e.querySelector(r.spinnerSelector))&&p(i),c!=document.body&&d(c,"nprogress-custom-parent"),c.appendChild(e),e},t.remove=function(){c(document.documentElement,"nprogress-busy"),c(document.querySelector(r.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&p(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var a=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),o=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var r,i=n.length,s=e.charAt(0).toUpperCase()+e.slice(1);i--;)if((r=n[i]+s)in t)return r;return e}(t))}function r(n,e,r){e=t(e),n.style[e]=r}return function(n,e){var t,i,s=arguments;if(2==s.length)for(t in e)void 0!==(i=e[t])&&e.hasOwnProperty(t)&&r(n,t,i);else r(n,s[1],s[2])}}();function l(n,e){return("string"==typeof n?n:u(n)).indexOf(" "+e+" ")>=0}function d(n,e){var t=u(n),r=t+e;l(t,e)||(n.className=r.substring(1))}function c(n,e){var t,r=u(n);l(n,e)&&(t=r.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function u(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function p(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?r.call(e,t,e,n):r)||(n.exports=i)},function(n,e,t){"use strict";var r=t(0),i=t(52).f,s=t(14),a=t(97),o=t(37),l=t(65),d=t(125);n.exports=function(n,e){var t,c,u,p,f,h=n.target,m=n.global,v=n.stat;if(t=m?r:v?r[h]||o(h,{}):r[h]&&r[h].prototype)for(c in e){if(p=e[c],u=n.dontCallGetSet?(f=i(t,c))&&f.value:t[c],!d(m?c:h+(v?".":"#")+c,n.forced)&&void 0!==u){if(typeof p==typeof u)continue;l(p,u)}(n.sham||u&&u.sham)&&s(p,"sham",!0),a(t,c,p,n)}}},function(n,e,t){"use strict";var r=t(3);n.exports=!r((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e,t){"use strict";var r=t(48),i=t(35);n.exports=function(n){return r(i(n))}},function(n,e,t){"use strict";var r=t(0),i=t(1),s=function(n){return i(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?s(r[n]):r[n]&&r[n][e]}},function(n,e,t){"use strict";var r=t(1),i=t(112),s=TypeError;n.exports=function(n){if(r(n))return n;throw new s(i(n)+" is not a function")}},function(n,e,t){"use strict";var r=t(0),i=t(61),s=t(9),a=t(63),o=t(58),l=t(57),d=r.Symbol,c=i("wks"),u=l?d.for||d:d&&d.withoutSetter||a;n.exports=function(n){return s(c,n)||(c[n]=o&&s(d,n)?d[n]:u("Symbol."+n)),c[n]}},function(n,e,t){"use strict";var r=t(35),i=Object;n.exports=function(n){return i(r(n))}},function(n,e,t){"use strict";var r=t(123);n.exports=function(n){return r(n.length)}},function(n,e,t){"use strict";var r=t(26),i=Function.prototype.call;n.exports=r?i.bind(i):function(){return i.apply(i,arguments)}},function(n,e,t){"use strict";n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){"use strict";var r=t(53),i=TypeError;n.exports=function(n){if(r(n))throw new i("Can't call method on "+n);return n}},function(n,e,t){"use strict";var r=t(62),i=t(0),s=t(37),a=n.exports=i["__core-js_shared__"]||s("__core-js_shared__",{});(a.versions||(a.versions=[])).push({version:"3.38.1",mode:r?"pure":"global",copyright:"© 2014-2024 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.38.1/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e,t){"use strict";var r=t(0),i=Object.defineProperty;n.exports=function(n,e){try{i(r,n,{value:e,configurable:!0,writable:!0})}catch(t){r[n]=e}return e}},function(n,e,t){var r=t(149),i=t(12),s=Object.prototype,a=s.hasOwnProperty,o=s.propertyIsEnumerable,l=r(function(){return arguments}())?r:function(n){return i(n)&&a.call(n,"callee")&&!o.call(n,"callee")};n.exports=l},function(n,e,t){var r=t(10)(t(8),"Map");n.exports=r},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var r=t(169),i=t(176),s=t(178),a=t(179),o=t(180);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=i,l.prototype.get=s,l.prototype.has=a,l.prototype.set=o,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var r=t(6),i=t(45),s=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,a=/^\w*$/;n.exports=function(n,e){if(r(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!i(n))||(a.test(n)||!s.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var r=t(13),i=t(12);n.exports=function(n){return"symbol"==typeof n||i(n)&&"[object Symbol]"==r(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){"use strict";var r=t(7),i=String,s=TypeError;n.exports=function(n){if(r(n))return n;throw new s(i(n)+" is not an object")}},function(n,e,t){"use strict";var r=t(2),i=t(3),s=t(18),a=Object,o=r("".split);n.exports=i((function(){return!a("z").propertyIsEnumerable(0)}))?function(n){return"String"===s(n)?o(n,""):a(n)}:a},function(n,e,t){"use strict";n.exports={}},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e){var t=/^\s+|\s+$/g,r=/^[-+]0x[0-9a-f]+$/i,i=/^0b[01]+$/i,s=/^0o[0-7]+$/i,a=parseInt,o="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,d=o||l||Function("return this")(),c=Object.prototype.toString,u=Math.max,p=Math.min,f=function(){return d.Date.now()};function h(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function m(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==c.call(n)}(n))return NaN;if(h(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=h(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var o=i.test(n);return o||s.test(n)?a(n.slice(2),o?2:8):r.test(n)?NaN:+n}n.exports=function(n,e,t){var r,i,s,a,o,l,d=0,c=!1,v=!1,_=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function g(e){var t=r,s=i;return r=i=void 0,d=e,a=n.apply(s,t)}function b(n){return d=n,o=setTimeout(k,e),c?g(n):a}function y(n){var t=n-l;return void 0===l||t>=e||t<0||v&&n-d>=s}function k(){var n=f();if(y(n))return E(n);o=setTimeout(k,function(n){var t=e-(n-l);return v?p(t,s-(n-d)):t}(n))}function E(n){return o=void 0,_&&r?g(n):(r=i=void 0,a)}function R(){var n=f(),t=y(n);if(r=arguments,i=this,l=n,t){if(void 0===o)return b(l);if(v)return o=setTimeout(k,e),g(l)}return void 0===o&&(o=setTimeout(k,e)),a}return e=m(e)||0,h(t)&&(c=!!t.leading,s=(v="maxWait"in t)?u(m(t.maxWait)||0,e):s,_="trailing"in t?!!t.trailing:_),R.cancel=function(){void 0!==o&&clearTimeout(o),d=0,r=l=i=o=void 0},R.flush=function(){return void 0===o?a:E(f())},R}},function(n,e,t){"use strict";var r=t(5),i=t(33),s=t(109),a=t(34),o=t(27),l=t(54),d=t(9),c=t(64),u=Object.getOwnPropertyDescriptor;e.f=r?u:function(n,e){if(n=o(n),e=l(e),c)try{return u(n,e)}catch(n){}if(d(n,e))return a(!i(s.f,n,e),n[e])}},function(n,e,t){"use strict";n.exports=function(n){return null==n}},function(n,e,t){"use strict";var r=t(110),i=t(55);n.exports=function(n){var e=r(n,"string");return i(e)?e:e+""}},function(n,e,t){"use strict";var r=t(28),i=t(1),s=t(56),a=t(57),o=Object;n.exports=a?function(n){return"symbol"==typeof n}:function(n){var e=r("Symbol");return i(e)&&s(e.prototype,o(n))}},function(n,e,t){"use strict";var r=t(2);n.exports=r({}.isPrototypeOf)},function(n,e,t){"use strict";var r=t(58);n.exports=r&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){"use strict";var r=t(59),i=t(3),s=t(0).String;n.exports=!!Object.getOwnPropertySymbols&&!i((function(){var n=Symbol("symbol detection");return!s(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&r&&r<41}))},function(n,e,t){"use strict";var r,i,s=t(0),a=t(60),o=s.process,l=s.Deno,d=o&&o.versions||l&&l.version,c=d&&d.v8;c&&(i=(r=c.split("."))[0]>0&&r[0]<4?1:+(r[0]+r[1])),!i&&a&&(!(r=a.match(/Edge\/(\d+)/))||r[1]>=74)&&(r=a.match(/Chrome\/(\d+)/))&&(i=+r[1]),n.exports=i},function(n,e,t){"use strict";var r=t(0).navigator,i=r&&r.userAgent;n.exports=i?String(i):""},function(n,e,t){"use strict";var r=t(36);n.exports=function(n,e){return r[n]||(r[n]=e||{})}},function(n,e,t){"use strict";n.exports=!1},function(n,e,t){"use strict";var r=t(2),i=0,s=Math.random(),a=r(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+a(++i+s,36)}},function(n,e,t){"use strict";var r=t(5),i=t(3),s=t(100);n.exports=!r&&!i((function(){return 7!==Object.defineProperty(s("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){"use strict";var r=t(9),i=t(118),s=t(52),a=t(17);n.exports=function(n,e,t){for(var o=i(e),l=a.f,d=s.f,c=0;c<o.length;c++){var u=o[c];r(n,u)||t&&r(t,u)||l(n,u,d(e,u))}}},function(n,e,t){"use strict";var r=t(122);n.exports=function(n){var e=+n;return e!=e||0===e?0:r(e)}},function(n,e,t){"use strict";var r=t(132),i=t(7),s=t(35),a=t(133);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=r(Object.prototype,"__proto__","set"))(t,[]),e=t instanceof Array}catch(n){}return function(t,r){return s(t),a(r),i(t)?(e?n(t,r):t.__proto__=r,t):t}}():void 0)},function(n,e){n.exports=function(n,e){for(var t=-1,r=e.length,i=n.length;++t<r;)n[i+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var r=t(19),i=t(160),s=t(161),a=t(162),o=t(163),l=t(164);function d(n){var e=this.__data__=new r(n);this.size=e.size}d.prototype.clear=i,d.prototype.delete=s,d.prototype.get=a,d.prototype.has=o,d.prototype.set=l,n.exports=d},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var r=t(13),i=t(40);n.exports=function(n){if(!i(n))return!1;var e=r(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var r=t(181),i=t(12);n.exports=function n(e,t,s,a,o){return e===t||(null==e||null==t||!i(e)&&!i(t)?e!=e&&t!=t:r(e,t,s,a,n,o))}},function(n,e,t){var r=t(76),i=t(184),s=t(77);n.exports=function(n,e,t,a,o,l){var d=1&t,c=n.length,u=e.length;if(c!=u&&!(d&&u>c))return!1;var p=l.get(n),f=l.get(e);if(p&&f)return p==e&&f==n;var h=-1,m=!0,v=2&t?new r:void 0;for(l.set(n,e),l.set(e,n);++h<c;){var _=n[h],g=e[h];if(a)var b=d?a(g,_,h,e,n,l):a(_,g,h,n,e,l);if(void 0!==b){if(b)continue;m=!1;break}if(v){if(!i(e,(function(n,e){if(!s(v,e)&&(_===n||o(_,n,t,a,l)))return v.push(e)}))){m=!1;break}}else if(_!==g&&!o(_,g,t,a,l)){m=!1;break}}return l.delete(n),l.delete(e),m}},function(n,e,t){var r=t(41),i=t(182),s=t(183);function a(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new r;++e<t;)this.add(n[e])}a.prototype.add=a.prototype.push=i,a.prototype.has=s,n.exports=a},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var r=t(194),i=t(200),s=t(82);n.exports=function(n){return s(n)?r(n):i(n)}},function(n,e,t){(function(n){var r=t(8),i=t(196),s=e&&!e.nodeType&&e,a=s&&"object"==typeof n&&n&&!n.nodeType&&n,o=a&&a.exports===s?r.Buffer:void 0,l=(o?o.isBuffer:void 0)||i;n.exports=l}).call(this,t(50)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var r=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==r||"symbol"!=r&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var r=t(197),i=t(198),s=t(199),a=s&&s.isTypedArray,o=a?i(a):r;n.exports=o},function(n,e,t){var r=t(72),i=t(43);n.exports=function(n){return null!=n&&i(n.length)&&!r(n)}},function(n,e,t){var r=t(10)(t(8),"Set");n.exports=r},function(n,e,t){var r=t(40);n.exports=function(n){return n==n&&!r(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var r=t(87),i=t(23);n.exports=function(n,e){for(var t=0,s=(e=r(e,n)).length;null!=n&&t<s;)n=n[i(e[t++])];return t&&t==s?n:void 0}},function(n,e,t){var r=t(6),i=t(44),s=t(211),a=t(214);n.exports=function(n,e){return r(n)?n:i(n,e)?[n]:s(a(n))}},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(147),i=t(152),s=t(223),a=t(231),o=t(240),l=t(106),d=s((function(n){var e=l(n);return o(e)&&(e=void 0),a(r(n,1,o,!0),i(e,2))}));n.exports=d},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var r=/["'&<>]/;n.exports=function(n){var e,t=""+n,i=r.exec(t);if(!i)return t;var s="",a=0,o=0;for(a=i.index;a<t.length;a++){switch(t.charCodeAt(a)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}o!==a&&(s+=t.substring(o,a)),o=a+1,s+=e}return o!==a?s+t.substring(o,a):s}},function(n){n.exports=JSON.parse('{"en-US":{"author":"author","beforeAuthor":"Copyright © ","afterAuthor":"\\nLink: "},"zh-CN":{"author":"作者","beforeAuthor":"著作权归","afterAuthor":"所有。\\n链接："}}')},function(n,e,t){"use strict";t.r(e);var r={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},i=(t(243),t(4)),s=Object(i.a)(r,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=s.exports},function(n,e,t){"use strict";t.r(e);var r={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(n){this.codeTabs.forEach(n=>{n.elm.classList.remove("theme-code-block__active")}),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(n=>Boolean(n.componentOptions)).map((n,e)=>(""===n.componentOptions.propsData.active&&(this.activeCodeTabIndex=e),{title:n.componentOptions.propsData.title,elm:n.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(n){this.activeCodeTabIndex=n}}},i=(t(244),t(4)),s=Object(i.a)(r,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"theme-code-group"},[e("div",{staticClass:"theme-code-group__nav"},[e("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(t,r){return e("li",{key:t.title,staticClass:"theme-code-group__li"},[e("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":r===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(r)}}},[n._v("\n            "+n._s(t.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?e("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=s.exports},function(n,e,t){"use strict";var r=t(1),i=t(17),s=t(102),a=t(37);n.exports=function(n,e,t,o){o||(o={});var l=o.enumerable,d=void 0!==o.name?o.name:e;if(r(t)&&s(t,d,o),o.global)l?n[e]=t:a(e,t);else{try{o.unsafe?n[e]&&(l=!0):delete n[e]}catch(n){}l?n[e]=t:i.f(n,e,{value:t,enumerable:!1,configurable:!o.nonConfigurable,writable:!o.nonWritable})}return n}},function(n,e,t){"use strict";n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){"use strict";var r=t(138),i=String;n.exports=function(n){if("Symbol"===r(n))throw new TypeError("Cannot convert a Symbol value to a string");return i(n)}},function(n,e,t){"use strict";var r=t(0),i=t(7),s=r.document,a=i(s)&&i(s.createElement);n.exports=function(n){return a?s.createElement(n):{}}},function(n,e,t){"use strict";var r=t(5),i=t(3);n.exports=r&&i((function(){return 42!==Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){"use strict";var r=t(2),i=t(3),s=t(1),a=t(9),o=t(5),l=t(114).CONFIGURABLE,d=t(115),c=t(116),u=c.enforce,p=c.get,f=String,h=Object.defineProperty,m=r("".slice),v=r("".replace),_=r([].join),g=o&&!i((function(){return 8!==h((function(){}),"length",{value:8}).length})),b=String(String).split("String"),y=n.exports=function(n,e,t){"Symbol("===m(f(e),0,7)&&(e="["+v(f(e),/^Symbol\(([^)]*)\).*$/,"$1")+"]"),t&&t.getter&&(e="get "+e),t&&t.setter&&(e="set "+e),(!a(n,"name")||l&&n.name!==e)&&(o?h(n,"name",{value:e,configurable:!0}):n.name=e),g&&t&&a(t,"arity")&&n.length!==t.arity&&h(n,"length",{value:t.arity});try{t&&a(t,"constructor")&&t.constructor?o&&h(n,"prototype",{writable:!1}):n.prototype&&(n.prototype=void 0)}catch(n){}var r=u(n);return a(r,"source")||(r.source=_(b,"string"==typeof e?e:"")),n};Function.prototype.toString=y((function(){return s(this)&&p(this).source||d(this)}),"toString")},function(n,e,t){"use strict";var r=t(61),i=t(63),s=r("keys");n.exports=function(n){return s[n]||(s[n]=i(n))}},function(n,e,t){"use strict";var r=t(2),i=t(9),s=t(27),a=t(120).indexOf,o=t(49),l=r([].push);n.exports=function(n,e){var t,r=s(n),d=0,c=[];for(t in r)!i(o,t)&&i(r,t)&&l(c,t);for(;e.length>d;)i(r,t=e[d++])&&(~a(c,t)||l(c,t));return c}},function(n,e,t){"use strict";var r=t(25),i=t(0),s=t(130),a=t(131),o=i.WebAssembly,l=7!==new Error("e",{cause:7}).cause,d=function(n,e){var t={};t[n]=a(n,e,l),r({global:!0,constructor:!0,arity:1,forced:l},t)},c=function(n,e){if(o&&o[n]){var t={};t[n]=a("WebAssembly."+n,e,l),r({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:l},t)}};d("Error",(function(n){return function(e){return s(n,this,arguments)}})),d("EvalError",(function(n){return function(e){return s(n,this,arguments)}})),d("RangeError",(function(n){return function(e){return s(n,this,arguments)}})),d("ReferenceError",(function(n){return function(e){return s(n,this,arguments)}})),d("SyntaxError",(function(n){return function(e){return s(n,this,arguments)}})),d("TypeError",(function(n){return function(e){return s(n,this,arguments)}})),d("URIError",(function(n){return function(e){return s(n,this,arguments)}})),c("CompileError",(function(n){return function(e){return s(n,this,arguments)}})),c("LinkError",(function(n){return function(e){return s(n,this,arguments)}})),c("RuntimeError",(function(n){return function(e){return s(n,this,arguments)}}))},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){n.exports=t(249)},function(n,e,t){"use strict";var r=t(25),i=t(126).left,s=t(127),a=t(59);r({target:"Array",proto:!0,forced:!t(128)&&a>79&&a<83||!s("reduce")},{reduce:function(n){var e=arguments.length;return i(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var r={}.propertyIsEnumerable,i=Object.getOwnPropertyDescriptor,s=i&&!r.call({1:2},1);e.f=s?function(n){var e=i(this,n);return!!e&&e.enumerable}:r},function(n,e,t){"use strict";var r=t(33),i=t(7),s=t(55),a=t(111),o=t(113),l=t(30),d=TypeError,c=l("toPrimitive");n.exports=function(n,e){if(!i(n)||s(n))return n;var t,l=a(n,c);if(l){if(void 0===e&&(e="default"),t=r(l,n,e),!i(t)||s(t))return t;throw new d("Can't convert object to primitive value")}return void 0===e&&(e="number"),o(n,e)}},function(n,e,t){"use strict";var r=t(29),i=t(53);n.exports=function(n,e){var t=n[e];return i(t)?void 0:r(t)}},function(n,e,t){"use strict";var r=String;n.exports=function(n){try{return r(n)}catch(n){return"Object"}}},function(n,e,t){"use strict";var r=t(33),i=t(1),s=t(7),a=TypeError;n.exports=function(n,e){var t,o;if("string"===e&&i(t=n.toString)&&!s(o=r(t,n)))return o;if(i(t=n.valueOf)&&!s(o=r(t,n)))return o;if("string"!==e&&i(t=n.toString)&&!s(o=r(t,n)))return o;throw new a("Can't convert object to primitive value")}},function(n,e,t){"use strict";var r=t(5),i=t(9),s=Function.prototype,a=r&&Object.getOwnPropertyDescriptor,o=i(s,"name"),l=o&&"something"===function(){}.name,d=o&&(!r||r&&a(s,"name").configurable);n.exports={EXISTS:o,PROPER:l,CONFIGURABLE:d}},function(n,e,t){"use strict";var r=t(2),i=t(1),s=t(36),a=r(Function.toString);i(s.inspectSource)||(s.inspectSource=function(n){return a(n)}),n.exports=s.inspectSource},function(n,e,t){"use strict";var r,i,s,a=t(117),o=t(0),l=t(7),d=t(14),c=t(9),u=t(36),p=t(103),f=t(49),h=o.TypeError,m=o.WeakMap;if(a||u.state){var v=u.state||(u.state=new m);v.get=v.get,v.has=v.has,v.set=v.set,r=function(n,e){if(v.has(n))throw new h("Object already initialized");return e.facade=n,v.set(n,e),e},i=function(n){return v.get(n)||{}},s=function(n){return v.has(n)}}else{var _=p("state");f[_]=!0,r=function(n,e){if(c(n,_))throw new h("Object already initialized");return e.facade=n,d(n,_,e),e},i=function(n){return c(n,_)?n[_]:{}},s=function(n){return c(n,_)}}n.exports={set:r,get:i,has:s,enforce:function(n){return s(n)?i(n):r(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=i(e)).type!==n)throw new h("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){"use strict";var r=t(0),i=t(1),s=r.WeakMap;n.exports=i(s)&&/native code/.test(String(s))},function(n,e,t){"use strict";var r=t(28),i=t(2),s=t(119),a=t(124),o=t(47),l=i([].concat);n.exports=r("Reflect","ownKeys")||function(n){var e=s.f(o(n)),t=a.f;return t?l(e,t(n)):e}},function(n,e,t){"use strict";var r=t(104),i=t(98).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return r(n,i)}},function(n,e,t){"use strict";var r=t(27),i=t(121),s=t(32),a=function(n){return function(e,t,a){var o=r(e),l=s(o);if(0===l)return!n&&-1;var d,c=i(a,l);if(n&&t!=t){for(;l>c;)if((d=o[c++])!=d)return!0}else for(;l>c;c++)if((n||c in o)&&o[c]===t)return n||c||0;return!n&&-1}};n.exports={includes:a(!0),indexOf:a(!1)}},function(n,e,t){"use strict";var r=t(66),i=Math.max,s=Math.min;n.exports=function(n,e){var t=r(n);return t<0?i(t+e,0):s(t,e)}},function(n,e,t){"use strict";var r=Math.ceil,i=Math.floor;n.exports=Math.trunc||function(n){var e=+n;return(e>0?i:r)(e)}},function(n,e,t){"use strict";var r=t(66),i=Math.min;n.exports=function(n){var e=r(n);return e>0?i(e,9007199254740991):0}},function(n,e,t){"use strict";e.f=Object.getOwnPropertySymbols},function(n,e,t){"use strict";var r=t(3),i=t(1),s=/#|\.prototype\./,a=function(n,e){var t=l[o(n)];return t===c||t!==d&&(i(e)?r(e):!!e)},o=a.normalize=function(n){return String(n).replace(s,".").toLowerCase()},l=a.data={},d=a.NATIVE="N",c=a.POLYFILL="P";n.exports=a},function(n,e,t){"use strict";var r=t(29),i=t(31),s=t(48),a=t(32),o=TypeError,l="Reduce of empty array with no initial value",d=function(n){return function(e,t,d,c){var u=i(e),p=s(u),f=a(u);if(r(t),0===f&&d<2)throw new o(l);var h=n?f-1:0,m=n?-1:1;if(d<2)for(;;){if(h in p){c=p[h],h+=m;break}if(h+=m,n?h<0:f<=h)throw new o(l)}for(;n?h>=0:f>h;h+=m)h in p&&(c=t(c,p[h],h,u));return c}};n.exports={left:d(!1),right:d(!0)}},function(n,e,t){"use strict";var r=t(3);n.exports=function(n,e){var t=[][n];return!!t&&r((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){"use strict";var r=t(129);n.exports="NODE"===r},function(n,e,t){"use strict";var r=t(0),i=t(60),s=t(18),a=function(n){return i.slice(0,n.length)===n};n.exports=a("Bun/")?"BUN":a("Cloudflare-Workers")?"CLOUDFLARE":a("Deno/")?"DENO":a("Node.js/")?"NODE":r.Bun&&"string"==typeof Bun.version?"BUN":r.Deno&&"object"==typeof Deno.version?"DENO":"process"===s(r.process)?"NODE":r.window&&r.document?"BROWSER":"REST"},function(n,e,t){"use strict";var r=t(26),i=Function.prototype,s=i.apply,a=i.call;n.exports="object"==typeof Reflect&&Reflect.apply||(r?a.bind(s):function(){return a.apply(s,arguments)})},function(n,e,t){"use strict";var r=t(28),i=t(9),s=t(14),a=t(56),o=t(67),l=t(65),d=t(135),c=t(136),u=t(137),p=t(140),f=t(141),h=t(5),m=t(62);n.exports=function(n,e,t,v){var _=v?2:1,g=n.split("."),b=g[g.length-1],y=r.apply(null,g);if(y){var k=y.prototype;if(!m&&i(k,"cause")&&delete k.cause,!t)return y;var E=r("Error"),R=e((function(n,e){var t=u(v?e:n,void 0),r=v?new y(n):new y;return void 0!==t&&s(r,"message",t),f(r,R,r.stack,2),this&&a(k,this)&&c(r,this,R),arguments.length>_&&p(r,arguments[_]),r}));if(R.prototype=k,"Error"!==b?o?o(R,E):l(R,E,{name:!0}):h&&"stackTraceLimit"in y&&(d(R,y,"stackTraceLimit"),d(R,y,"prepareStackTrace")),l(R,y),!m)try{k.name!==b&&s(k,"name",b),k.constructor=R}catch(n){}return R}}},function(n,e,t){"use strict";var r=t(2),i=t(29);n.exports=function(n,e,t){try{return r(i(Object.getOwnPropertyDescriptor(n,e)[t]))}catch(n){}}},function(n,e,t){"use strict";var r=t(134),i=String,s=TypeError;n.exports=function(n){if(r(n))return n;throw new s("Can't set "+i(n)+" as a prototype")}},function(n,e,t){"use strict";var r=t(7);n.exports=function(n){return r(n)||null===n}},function(n,e,t){"use strict";var r=t(17).f;n.exports=function(n,e,t){t in n||r(n,t,{configurable:!0,get:function(){return e[t]},set:function(n){e[t]=n}})}},function(n,e,t){"use strict";var r=t(1),i=t(7),s=t(67);n.exports=function(n,e,t){var a,o;return s&&r(a=e.constructor)&&a!==t&&i(o=a.prototype)&&o!==t.prototype&&s(n,o),n}},function(n,e,t){"use strict";var r=t(99);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:r(n)}},function(n,e,t){"use strict";var r=t(139),i=t(1),s=t(18),a=t(30)("toStringTag"),o=Object,l="Arguments"===s(function(){return arguments}());n.exports=r?s:function(n){var e,t,r;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=o(n),a))?t:l?s(e):"Object"===(r=s(e))&&i(e.callee)?"Arguments":r}},function(n,e,t){"use strict";var r={};r[t(30)("toStringTag")]="z",n.exports="[object z]"===String(r)},function(n,e,t){"use strict";var r=t(7),i=t(14);n.exports=function(n,e){r(e)&&"cause"in e&&i(n,"cause",e.cause)}},function(n,e,t){"use strict";var r=t(14),i=t(142),s=t(143),a=Error.captureStackTrace;n.exports=function(n,e,t,o){s&&(a?a(n,e):r(n,"stack",i(t,o)))}},function(n,e,t){"use strict";var r=t(2),i=Error,s=r("".replace),a=String(new i("zxcasd").stack),o=/\n\s*at [^:]*:[^\n]*/,l=o.test(a);n.exports=function(n,e){if(l&&"string"==typeof n&&!i.prepareStackTrace)for(;e--;)n=s(n,o,"");return n}},function(n,e,t){"use strict";var r=t(3),i=t(34);n.exports=!r((function(){var n=new Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",i(1,7)),7!==n.stack)}))},function(n,e,t){"use strict";var r=t(5),i=t(145),s=TypeError,a=Object.getOwnPropertyDescriptor,o=r&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(n){return n instanceof TypeError}}();n.exports=o?function(n,e){if(i(n)&&!a(n,"length").writable)throw new s("Cannot set read only .length");return n.length=e}:function(n,e){return n.length=e}},function(n,e,t){"use strict";var r=t(18);n.exports=Array.isArray||function(n){return"Array"===r(n)}},function(n,e,t){"use strict";var r=TypeError;n.exports=function(n){if(n>9007199254740991)throw r("Maximum allowed index exceeded");return n}},function(n,e,t){var r=t(68),i=t(148);n.exports=function n(e,t,s,a,o){var l=-1,d=e.length;for(s||(s=i),o||(o=[]);++l<d;){var c=e[l];t>0&&s(c)?t>1?n(c,t-1,s,a,o):r(o,c):a||(o[o.length]=c)}return o}},function(n,e,t){var r=t(15),i=t(38),s=t(6),a=r?r.isConcatSpreadable:void 0;n.exports=function(n){return s(n)||i(n)||!!(a&&n&&n[a])}},function(n,e,t){var r=t(13),i=t(12);n.exports=function(n){return i(n)&&"[object Arguments]"==r(n)}},function(n,e,t){var r=t(15),i=Object.prototype,s=i.hasOwnProperty,a=i.toString,o=r?r.toStringTag:void 0;n.exports=function(n){var e=s.call(n,o),t=n[o];try{n[o]=void 0;var r=!0}catch(n){}var i=a.call(n);return r&&(e?n[o]=t:delete n[o]),i}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var r=t(153),i=t(209),s=t(46),a=t(6),o=t(220);n.exports=function(n){return"function"==typeof n?n:null==n?s:"object"==typeof n?a(n)?i(n[0],n[1]):r(n):o(n)}},function(n,e,t){var r=t(154),i=t(208),s=t(85);n.exports=function(n){var e=i(n);return 1==e.length&&e[0][2]?s(e[0][0],e[0][1]):function(t){return t===n||r(t,n,e)}}},function(n,e,t){var r=t(70),i=t(74);n.exports=function(n,e,t,s){var a=t.length,o=a,l=!s;if(null==n)return!o;for(n=Object(n);a--;){var d=t[a];if(l&&d[2]?d[1]!==n[d[0]]:!(d[0]in n))return!1}for(;++a<o;){var c=(d=t[a])[0],u=n[c],p=d[1];if(l&&d[2]){if(void 0===u&&!(c in n))return!1}else{var f=new r;if(s)var h=s(u,p,c,n,e,f);if(!(void 0===h?i(p,u,3,s,f):h))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var r=t(20),i=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=r(e,n);return!(t<0)&&(t==e.length-1?e.pop():i.call(e,t,1),--this.size,!0)}},function(n,e,t){var r=t(20);n.exports=function(n){var e=this.__data__,t=r(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var r=t(20);n.exports=function(n){return r(this.__data__,n)>-1}},function(n,e,t){var r=t(20);n.exports=function(n,e){var t=this.__data__,i=r(t,n);return i<0?(++this.size,t.push([n,e])):t[i][1]=e,this}},function(n,e,t){var r=t(19);n.exports=function(){this.__data__=new r,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var r=t(19),i=t(39),s=t(41);n.exports=function(n,e){var t=this.__data__;if(t instanceof r){var a=t.__data__;if(!i||a.length<199)return a.push([n,e]),this.size=++t.size,this;t=this.__data__=new s(a)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var r=t(72),i=t(166),s=t(40),a=t(73),o=/^\[object .+?Constructor\]$/,l=Function.prototype,d=Object.prototype,c=l.toString,u=d.hasOwnProperty,p=RegExp("^"+c.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!s(n)||i(n))&&(r(n)?p:o).test(a(n))}},function(n,e,t){var r,i=t(167),s=(r=/[^.]+$/.exec(i&&i.keys&&i.keys.IE_PROTO||""))?"Symbol(src)_1."+r:"";n.exports=function(n){return!!s&&s in n}},function(n,e,t){var r=t(8)["__core-js_shared__"];n.exports=r},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var r=t(170),i=t(19),s=t(39);n.exports=function(){this.size=0,this.__data__={hash:new r,map:new(s||i),string:new r}}},function(n,e,t){var r=t(171),i=t(172),s=t(173),a=t(174),o=t(175);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=i,l.prototype.get=s,l.prototype.has=a,l.prototype.set=o,n.exports=l},function(n,e,t){var r=t(21);n.exports=function(){this.__data__=r?r(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var r=t(21),i=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(r){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return i.call(e,n)?e[n]:void 0}},function(n,e,t){var r=t(21),i=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return r?void 0!==e[n]:i.call(e,n)}},function(n,e,t){var r=t(21);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=r&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var r=t(22);n.exports=function(n){var e=r(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var r=t(22);n.exports=function(n){return r(this,n).get(n)}},function(n,e,t){var r=t(22);n.exports=function(n){return r(this,n).has(n)}},function(n,e,t){var r=t(22);n.exports=function(n,e){var t=r(this,n),i=t.size;return t.set(n,e),this.size+=t.size==i?0:1,this}},function(n,e,t){var r=t(70),i=t(75),s=t(185),a=t(188),o=t(204),l=t(6),d=t(79),c=t(81),u="[object Object]",p=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,f,h,m){var v=l(n),_=l(e),g=v?"[object Array]":o(n),b=_?"[object Array]":o(e),y=(g="[object Arguments]"==g?u:g)==u,k=(b="[object Arguments]"==b?u:b)==u,E=g==b;if(E&&d(n)){if(!d(e))return!1;v=!0,y=!1}if(E&&!y)return m||(m=new r),v||c(n)?i(n,e,t,f,h,m):s(n,e,g,t,f,h,m);if(!(1&t)){var R=y&&p.call(n,"__wrapped__"),w=k&&p.call(e,"__wrapped__");if(R||w){var x=R?n.value():n,S=w?e.value():e;return m||(m=new r),h(x,S,t,f,m)}}return!!E&&(m||(m=new r),a(n,e,t,f,h,m))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length;++t<r;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var r=t(15),i=t(186),s=t(71),a=t(75),o=t(187),l=t(42),d=r?r.prototype:void 0,c=d?d.valueOf:void 0;n.exports=function(n,e,t,r,d,u,p){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!u(new i(n),new i(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return s(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var f=o;case"[object Set]":var h=1&r;if(f||(f=l),n.size!=e.size&&!h)return!1;var m=p.get(n);if(m)return m==e;r|=2,p.set(n,e);var v=a(f(n),f(e),r,d,u,p);return p.delete(n),v;case"[object Symbol]":if(c)return c.call(n)==c.call(e)}return!1}},function(n,e,t){var r=t(8).Uint8Array;n.exports=r},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,r){t[++e]=[r,n]})),t}},function(n,e,t){var r=t(189),i=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,s,a,o){var l=1&t,d=r(n),c=d.length;if(c!=r(e).length&&!l)return!1;for(var u=c;u--;){var p=d[u];if(!(l?p in e:i.call(e,p)))return!1}var f=o.get(n),h=o.get(e);if(f&&h)return f==e&&h==n;var m=!0;o.set(n,e),o.set(e,n);for(var v=l;++u<c;){var _=n[p=d[u]],g=e[p];if(s)var b=l?s(g,_,p,e,n,o):s(_,g,p,n,e,o);if(!(void 0===b?_===g||a(_,g,t,s,o):b)){m=!1;break}v||(v="constructor"==p)}if(m&&!v){var y=n.constructor,k=e.constructor;y==k||!("constructor"in n)||!("constructor"in e)||"function"==typeof y&&y instanceof y&&"function"==typeof k&&k instanceof k||(m=!1)}return o.delete(n),o.delete(e),m}},function(n,e,t){var r=t(190),i=t(191),s=t(78);n.exports=function(n){return r(n,s,i)}},function(n,e,t){var r=t(68),i=t(6);n.exports=function(n,e,t){var s=e(n);return i(n)?s:r(s,t(n))}},function(n,e,t){var r=t(192),i=t(193),s=Object.prototype.propertyIsEnumerable,a=Object.getOwnPropertySymbols,o=a?function(n){return null==n?[]:(n=Object(n),r(a(n),(function(e){return s.call(n,e)})))}:i;n.exports=o},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,i=0,s=[];++t<r;){var a=n[t];e(a,t,n)&&(s[i++]=a)}return s}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var r=t(195),i=t(38),s=t(6),a=t(79),o=t(80),l=t(81),d=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=s(n),c=!t&&i(n),u=!t&&!c&&a(n),p=!t&&!c&&!u&&l(n),f=t||c||u||p,h=f?r(n.length,String):[],m=h.length;for(var v in n)!e&&!d.call(n,v)||f&&("length"==v||u&&("offset"==v||"parent"==v)||p&&("buffer"==v||"byteLength"==v||"byteOffset"==v)||o(v,m))||h.push(v);return h}},function(n,e){n.exports=function(n,e){for(var t=-1,r=Array(n);++t<n;)r[t]=e(t);return r}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var r=t(13),i=t(43),s=t(12),a={};a["[object Float32Array]"]=a["[object Float64Array]"]=a["[object Int8Array]"]=a["[object Int16Array]"]=a["[object Int32Array]"]=a["[object Uint8Array]"]=a["[object Uint8ClampedArray]"]=a["[object Uint16Array]"]=a["[object Uint32Array]"]=!0,a["[object Arguments]"]=a["[object Array]"]=a["[object ArrayBuffer]"]=a["[object Boolean]"]=a["[object DataView]"]=a["[object Date]"]=a["[object Error]"]=a["[object Function]"]=a["[object Map]"]=a["[object Number]"]=a["[object Object]"]=a["[object RegExp]"]=a["[object Set]"]=a["[object String]"]=a["[object WeakMap]"]=!1,n.exports=function(n){return s(n)&&i(n.length)&&!!a[r(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var r=t(69),i=e&&!e.nodeType&&e,s=i&&"object"==typeof n&&n&&!n.nodeType&&n,a=s&&s.exports===i&&r.process,o=function(){try{var n=s&&s.require&&s.require("util").types;return n||a&&a.binding&&a.binding("util")}catch(n){}}();n.exports=o}).call(this,t(50)(n))},function(n,e,t){var r=t(201),i=t(202),s=Object.prototype.hasOwnProperty;n.exports=function(n){if(!r(n))return i(n);var e=[];for(var t in Object(n))s.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var r=t(203)(Object.keys,Object);n.exports=r},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var r=t(205),i=t(39),s=t(206),a=t(83),o=t(207),l=t(13),d=t(73),c=d(r),u=d(i),p=d(s),f=d(a),h=d(o),m=l;(r&&"[object DataView]"!=m(new r(new ArrayBuffer(1)))||i&&"[object Map]"!=m(new i)||s&&"[object Promise]"!=m(s.resolve())||a&&"[object Set]"!=m(new a)||o&&"[object WeakMap]"!=m(new o))&&(m=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,r=t?d(t):"";if(r)switch(r){case c:return"[object DataView]";case u:return"[object Map]";case p:return"[object Promise]";case f:return"[object Set]";case h:return"[object WeakMap]"}return e}),n.exports=m},function(n,e,t){var r=t(10)(t(8),"DataView");n.exports=r},function(n,e,t){var r=t(10)(t(8),"Promise");n.exports=r},function(n,e,t){var r=t(10)(t(8),"WeakMap");n.exports=r},function(n,e,t){var r=t(84),i=t(78);n.exports=function(n){for(var e=i(n),t=e.length;t--;){var s=e[t],a=n[s];e[t]=[s,a,r(a)]}return e}},function(n,e,t){var r=t(74),i=t(210),s=t(217),a=t(44),o=t(84),l=t(85),d=t(23);n.exports=function(n,e){return a(n)&&o(e)?l(d(n),e):function(t){var a=i(t,n);return void 0===a&&a===e?s(t,n):r(e,a,3)}}},function(n,e,t){var r=t(86);n.exports=function(n,e,t){var i=null==n?void 0:r(n,e);return void 0===i?t:i}},function(n,e,t){var r=t(212),i=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,s=/\\(\\)?/g,a=r((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(i,(function(n,t,r,i){e.push(r?i.replace(s,"$1"):t||n)})),e}));n.exports=a},function(n,e,t){var r=t(213);n.exports=function(n){var e=r(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var r=t(41);function i(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var r=arguments,i=e?e.apply(this,r):r[0],s=t.cache;if(s.has(i))return s.get(i);var a=n.apply(this,r);return t.cache=s.set(i,a)||s,a};return t.cache=new(i.Cache||r),t}i.Cache=r,n.exports=i},function(n,e,t){var r=t(215);n.exports=function(n){return null==n?"":r(n)}},function(n,e,t){var r=t(15),i=t(216),s=t(6),a=t(45),o=r?r.prototype:void 0,l=o?o.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(s(e))return i(e,n)+"";if(a(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,i=Array(r);++t<r;)i[t]=e(n[t],t,n);return i}},function(n,e,t){var r=t(218),i=t(219);n.exports=function(n,e){return null!=n&&i(n,e,r)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var r=t(87),i=t(38),s=t(6),a=t(80),o=t(43),l=t(23);n.exports=function(n,e,t){for(var d=-1,c=(e=r(e,n)).length,u=!1;++d<c;){var p=l(e[d]);if(!(u=null!=n&&t(n,p)))break;n=n[p]}return u||++d!=c?u:!!(c=null==n?0:n.length)&&o(c)&&a(p,c)&&(s(n)||i(n))}},function(n,e,t){var r=t(221),i=t(222),s=t(44),a=t(23);n.exports=function(n){return s(n)?r(a(n)):i(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var r=t(86);n.exports=function(n){return function(e){return r(e,n)}}},function(n,e,t){var r=t(46),i=t(224),s=t(226);n.exports=function(n,e){return s(i(n,e,r),n+"")}},function(n,e,t){var r=t(225),i=Math.max;n.exports=function(n,e,t){return e=i(void 0===e?n.length-1:e,0),function(){for(var s=arguments,a=-1,o=i(s.length-e,0),l=Array(o);++a<o;)l[a]=s[e+a];a=-1;for(var d=Array(e+1);++a<e;)d[a]=s[a];return d[e]=t(l),r(n,this,d)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var r=t(227),i=t(230)(r);n.exports=i},function(n,e,t){var r=t(228),i=t(229),s=t(46),a=i?function(n,e){return i(n,"toString",{configurable:!0,enumerable:!1,value:r(e),writable:!0})}:s;n.exports=a},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var r=t(10),i=function(){try{var n=r(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=i},function(n,e){var t=Date.now;n.exports=function(n){var e=0,r=0;return function(){var i=t(),s=16-(i-r);if(r=i,s>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var r=t(76),i=t(232),s=t(237),a=t(77),o=t(238),l=t(42);n.exports=function(n,e,t){var d=-1,c=i,u=n.length,p=!0,f=[],h=f;if(t)p=!1,c=s;else if(u>=200){var m=e?null:o(n);if(m)return l(m);p=!1,c=a,h=new r}else h=e?[]:f;n:for(;++d<u;){var v=n[d],_=e?e(v):v;if(v=t||0!==v?v:0,p&&_==_){for(var g=h.length;g--;)if(h[g]===_)continue n;e&&h.push(_),f.push(v)}else c(h,_,t)||(h!==f&&h.push(_),f.push(v))}return f}},function(n,e,t){var r=t(233);n.exports=function(n,e){return!!(null==n?0:n.length)&&r(n,e,0)>-1}},function(n,e,t){var r=t(234),i=t(235),s=t(236);n.exports=function(n,e,t){return e==e?s(n,e,t):r(n,i,t)}},function(n,e){n.exports=function(n,e,t,r){for(var i=n.length,s=t+(r?1:-1);r?s--:++s<i;)if(e(n[s],s,n))return s;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var r=t-1,i=n.length;++r<i;)if(n[r]===e)return r;return-1}},function(n,e){n.exports=function(n,e,t){for(var r=-1,i=null==n?0:n.length;++r<i;)if(t(e,n[r]))return!0;return!1}},function(n,e,t){var r=t(83),i=t(239),s=t(42),a=r&&1/s(new r([,-0]))[1]==1/0?function(n){return new r(n)}:i;n.exports=a},function(n,e){n.exports=function(){}},function(n,e,t){var r=t(82),i=t(12);n.exports=function(n){return i(n)&&r(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";t(88)},function(n,e,t){"use strict";t(89)},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";t(90)},function(n,e,t){"use strict";t(91)},function(n,e,t){"use strict";t.r(e);
/*!
 * Vue.js v2.7.16
 * (c) 2014-2023 Evan You
 * Released under the MIT License.
 */
var r=Object.freeze({}),i=Array.isArray;function s(n){return null==n}function a(n){return null!=n}function o(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function d(n){return"function"==typeof n}function c(n){return null!==n&&"object"==typeof n}var u=Object.prototype.toString;function p(n){return"[object Object]"===u.call(n)}function f(n){return"[object RegExp]"===u.call(n)}function m(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function v(n){return a(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function _(n){return null==n?"":Array.isArray(n)||p(n)&&n.toString===u?JSON.stringify(n,g,2):String(n)}function g(n,e){return e&&e.__v_isRef?e.value:e}function b(n){var e=parseFloat(n);return isNaN(e)?n:e}function y(n,e){for(var t=Object.create(null),r=n.split(","),i=0;i<r.length;i++)t[r[i]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}y("slot,component",!0);var k=y("key,ref,slot,slot-scope,is");function E(n,e){var t=n.length;if(t){if(e===n[t-1])return void(n.length=t-1);var r=n.indexOf(e);if(r>-1)return n.splice(r,1)}}var R=Object.prototype.hasOwnProperty;function w(n,e){return R.call(n,e)}function x(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var S=/-(\w)/g,I=x((function(n){return n.replace(S,(function(n,e){return e?e.toUpperCase():""}))})),A=x((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),T=/\B([A-Z])/g,O=x((function(n){return n.replace(T,"-$1").toLowerCase()}));var z=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var r=arguments.length;return r?r>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function L(n,e){e=e||0;for(var t=n.length-e,r=new Array(t);t--;)r[t]=n[t+e];return r}function C(n,e){for(var t in e)n[t]=e[t];return n}function B(n){for(var e={},t=0;t<n.length;t++)n[t]&&C(e,n[t]);return e}function P(n,e,t){}var D=function(n,e,t){return!1},j=function(n){return n};function N(n,e){if(n===e)return!0;var t=c(n),r=c(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var i=Array.isArray(n),s=Array.isArray(e);if(i&&s)return n.length===e.length&&n.every((function(n,t){return N(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(i||s)return!1;var a=Object.keys(n),o=Object.keys(e);return a.length===o.length&&a.every((function(t){return N(n[t],e[t])}))}catch(n){return!1}}function F(n,e){for(var t=0;t<n.length;t++)if(N(n[t],e))return t;return-1}function U(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}function M(n,e){return n===e?0===n&&1/n!=1/e:n==n||e==e}var q=["component","directive","filter"],$=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],K={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:D,isReservedAttr:D,isUnknownElement:D,getTagNamespace:P,parsePlatformTagName:j,mustUseProp:D,async:!0,_lifecycleHooks:$},G=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function H(n){var e=(n+"").charCodeAt(0);return 36===e||95===e}function W(n,e,t,r){Object.defineProperty(n,e,{value:t,enumerable:!!r,writable:!0,configurable:!0})}var V=new RegExp("[^".concat(G.source,".$_\\d]"));var Z="__proto__"in{},Y="undefined"!=typeof window,X=Y&&window.navigator.userAgent.toLowerCase(),J=X&&/msie|trident/.test(X),Q=X&&X.indexOf("msie 9.0")>0,nn=X&&X.indexOf("edge/")>0;X&&X.indexOf("android");var en=X&&/iphone|ipad|ipod|ios/.test(X);X&&/chrome\/\d+/.test(X),X&&/phantomjs/.test(X);var tn,rn=X&&X.match(/firefox\/(\d+)/),sn={}.watch,an=!1;if(Y)try{var on={};Object.defineProperty(on,"passive",{get:function(){an=!0}}),window.addEventListener("test-passive",null,on)}catch(n){}var ln=function(){return void 0===tn&&(tn=!Y&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),tn},dn=Y&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function cn(n){return"function"==typeof n&&/native code/.test(n.toString())}var un,pn="undefined"!=typeof Symbol&&cn(Symbol)&&"undefined"!=typeof Reflect&&cn(Reflect.ownKeys);un="undefined"!=typeof Set&&cn(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var fn=null;function hn(n){void 0===n&&(n=null),n||fn&&fn._scope.off(),fn=n,n&&n._scope.on()}var mn=function(){function n(n,e,t,r,i,s,a,o){this.tag=n,this.data=e,this.children=t,this.text=r,this.elm=i,this.ns=void 0,this.context=s,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=a,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=o,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(n.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),n}(),vn=function(n){void 0===n&&(n="");var e=new mn;return e.text=n,e.isComment=!0,e};function _n(n){return new mn(void 0,void 0,void 0,String(n))}function gn(n){var e=new mn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}"function"==typeof SuppressedError&&SuppressedError;var bn=0,yn=[],kn=function(){function n(){this._pending=!1,this.id=bn++,this.subs=[]}return n.prototype.addSub=function(n){this.subs.push(n)},n.prototype.removeSub=function(n){this.subs[this.subs.indexOf(n)]=null,this._pending||(this._pending=!0,yn.push(this))},n.prototype.depend=function(e){n.target&&n.target.addDep(this)},n.prototype.notify=function(n){var e=this.subs.filter((function(n){return n}));for(var t=0,r=e.length;t<r;t++){0,e[t].update()}},n}();kn.target=null;var En=[];function Rn(n){En.push(n),kn.target=n}function wn(){En.pop(),kn.target=En[En.length-1]}var xn=Array.prototype,Sn=Object.create(xn);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=xn[n];W(Sn,n,(function(){for(var t=[],r=0;r<arguments.length;r++)t[r]=arguments[r];var i,s=e.apply(this,t),a=this.__ob__;switch(n){case"push":case"unshift":i=t;break;case"splice":i=t.slice(2)}return i&&a.observeArray(i),a.dep.notify(),s}))}));var In=Object.getOwnPropertyNames(Sn),An={},Tn=!0;function On(n){Tn=n}var zn={notify:P,depend:P,addSub:P,removeSub:P},Ln=function(){function n(n,e,t){if(void 0===e&&(e=!1),void 0===t&&(t=!1),this.value=n,this.shallow=e,this.mock=t,this.dep=t?zn:new kn,this.vmCount=0,W(n,"__ob__",this),i(n)){if(!t)if(Z)n.__proto__=Sn;else for(var r=0,s=In.length;r<s;r++){W(n,o=In[r],Sn[o])}e||this.observeArray(n)}else{var a=Object.keys(n);for(r=0;r<a.length;r++){var o;Bn(n,o=a[r],An,void 0,e,t)}}}return n.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)Cn(n[e],!1,this.mock)},n}();function Cn(n,e,t){return n&&w(n,"__ob__")&&n.__ob__ instanceof Ln?n.__ob__:!Tn||!t&&ln()||!i(n)&&!p(n)||!Object.isExtensible(n)||n.__v_skip||Mn(n)||n instanceof mn?void 0:new Ln(n,e,t)}function Bn(n,e,t,r,s,a,o){void 0===o&&(o=!1);var l=new kn,d=Object.getOwnPropertyDescriptor(n,e);if(!d||!1!==d.configurable){var c=d&&d.get,u=d&&d.set;c&&!u||t!==An&&2!==arguments.length||(t=n[e]);var p=s?t&&t.__ob__:Cn(t,!1,a);return Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=c?c.call(n):t;return kn.target&&(l.depend(),p&&(p.dep.depend(),i(e)&&jn(e))),Mn(e)&&!s?e.value:e},set:function(e){var r=c?c.call(n):t;if(M(r,e)){if(u)u.call(n,e);else{if(c)return;if(!s&&Mn(r)&&!Mn(e))return void(r.value=e);t=e}p=s?e&&e.__ob__:Cn(e,!1,a),l.notify()}}}),l}}function Pn(n,e,t){if(!Un(n)){var r=n.__ob__;return i(n)&&m(e)?(n.length=Math.max(n.length,e),n.splice(e,1,t),r&&!r.shallow&&r.mock&&Cn(t,!1,!0),t):e in n&&!(e in Object.prototype)?(n[e]=t,t):n._isVue||r&&r.vmCount?t:r?(Bn(r.value,e,t,void 0,r.shallow,r.mock),r.dep.notify(),t):(n[e]=t,t)}}function Dn(n,e){if(i(n)&&m(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||Un(n)||w(n,e)&&(delete n[e],t&&t.dep.notify())}}function jn(n){for(var e=void 0,t=0,r=n.length;t<r;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),i(e)&&jn(e)}function Nn(n){return Fn(n,!0),W(n,"__v_isShallow",!0),n}function Fn(n,e){if(!Un(n)){Cn(n,e,ln());0}}function Un(n){return!(!n||!n.__v_isReadonly)}function Mn(n){return!(!n||!0!==n.__v_isRef)}function qn(n,e,t){Object.defineProperty(n,t,{enumerable:!0,configurable:!0,get:function(){var n=e[t];if(Mn(n))return n.value;var r=n&&n.__ob__;return r&&r.dep.depend(),n},set:function(n){var r=e[t];Mn(r)&&!Mn(n)?r.value=n:e[t]=n}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var $n;var Kn=function(){function n(n){void 0===n&&(n=!1),this.detached=n,this.active=!0,this.effects=[],this.cleanups=[],this.parent=$n,!n&&$n&&(this.index=($n.scopes||($n.scopes=[])).push(this)-1)}return n.prototype.run=function(n){if(this.active){var e=$n;try{return $n=this,n()}finally{$n=e}}else 0},n.prototype.on=function(){$n=this},n.prototype.off=function(){$n=this.parent},n.prototype.stop=function(n){if(this.active){var e=void 0,t=void 0;for(e=0,t=this.effects.length;e<t;e++)this.effects[e].teardown();for(e=0,t=this.cleanups.length;e<t;e++)this.cleanups[e]();if(this.scopes)for(e=0,t=this.scopes.length;e<t;e++)this.scopes[e].stop(!0);if(!this.detached&&this.parent&&!n){var r=this.parent.scopes.pop();r&&r!==this&&(this.parent.scopes[this.index]=r,r.index=this.index)}this.parent=void 0,this.active=!1}},n}();function Gn(n){var e=n._provided,t=n.$parent&&n.$parent._provided;return t===e?n._provided=Object.create(t):e}var Hn=x((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),r="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=r?n.slice(1):n,once:t,capture:r,passive:e}}));function Wn(n,e){function t(){var n=t.fns;if(!i(n))return Te(n,null,arguments,e,"v-on handler");for(var r=n.slice(),s=0;s<r.length;s++)Te(r[s],null,arguments,e,"v-on handler")}return t.fns=n,t}function Vn(n,e,t,r,i,a){var l,d,c,u;for(l in n)d=n[l],c=e[l],u=Hn(l),s(d)||(s(c)?(s(d.fns)&&(d=n[l]=Wn(d,a)),o(u.once)&&(d=n[l]=i(u.name,d,u.capture)),t(u.name,d,u.capture,u.passive,u.params)):d!==c&&(c.fns=d,n[l]=c));for(l in e)s(n[l])&&r((u=Hn(l)).name,e[l],u.capture)}function Zn(n,e,t){var r;n instanceof mn&&(n=n.data.hook||(n.data.hook={}));var i=n[e];function l(){t.apply(this,arguments),E(r.fns,l)}s(i)?r=Wn([l]):a(i.fns)&&o(i.merged)?(r=i).fns.push(l):r=Wn([i,l]),r.merged=!0,n[e]=r}function Yn(n,e,t,r,i){if(a(e)){if(w(e,t))return n[t]=e[t],i||delete e[t],!0;if(w(e,r))return n[t]=e[r],i||delete e[r],!0}return!1}function Xn(n){return l(n)?[_n(n)]:i(n)?function n(e,t){var r,d,c,u,p=[];for(r=0;r<e.length;r++)s(d=e[r])||"boolean"==typeof d||(c=p.length-1,u=p[c],i(d)?d.length>0&&(Jn((d=n(d,"".concat(t||"","_").concat(r)))[0])&&Jn(u)&&(p[c]=_n(u.text+d[0].text),d.shift()),p.push.apply(p,d)):l(d)?Jn(u)?p[c]=_n(u.text+d):""!==d&&p.push(_n(d)):Jn(d)&&Jn(u)?p[c]=_n(u.text+d.text):(o(e._isVList)&&a(d.tag)&&s(d.key)&&a(t)&&(d.key="__vlist".concat(t,"_").concat(r,"__")),p.push(d)));return p}(n):void 0}function Jn(n){return a(n)&&a(n.text)&&!1===n.isComment}function Qn(n,e){var t,r,s,o,l=null;if(i(n)||"string"==typeof n)for(l=new Array(n.length),t=0,r=n.length;t<r;t++)l[t]=e(n[t],t);else if("number"==typeof n)for(l=new Array(n),t=0;t<n;t++)l[t]=e(t+1,t);else if(c(n))if(pn&&n[Symbol.iterator]){l=[];for(var d=n[Symbol.iterator](),u=d.next();!u.done;)l.push(e(u.value,l.length)),u=d.next()}else for(s=Object.keys(n),l=new Array(s.length),t=0,r=s.length;t<r;t++)o=s[t],l[t]=e(n[o],o,t);return a(l)||(l=[]),l._isVList=!0,l}function ne(n,e,t,r){var i,s=this.$scopedSlots[n];s?(t=t||{},r&&(t=C(C({},r),t)),i=s(t)||(d(e)?e():e)):i=this.$slots[n]||(d(e)?e():e);var a=t&&t.slot;return a?this.$createElement("template",{slot:a},i):i}function ee(n){return Lt(this.$options,"filters",n,!0)||j}function te(n,e){return i(n)?-1===n.indexOf(e):n!==e}function re(n,e,t,r,i){var s=K.keyCodes[e]||t;return i&&r&&!K.keyCodes[e]?te(i,r):s?te(s,n):r?O(r)!==e:void 0===n}function ie(n,e,t,r,s){if(t)if(c(t)){i(t)&&(t=B(t));var a=void 0,o=function(i){if("class"===i||"style"===i||k(i))a=n;else{var o=n.attrs&&n.attrs.type;a=r||K.mustUseProp(e,o,i)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=I(i),d=O(i);l in a||d in a||(a[i]=t[i],s&&((n.on||(n.on={}))["update:".concat(i)]=function(n){t[i]=n}))};for(var l in t)o(l)}else;return n}function se(n,e){var t=this._staticTrees||(this._staticTrees=[]),r=t[n];return r&&!e||oe(r=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,this._c,this),"__static__".concat(n),!1),r}function ae(n,e,t){return oe(n,"__once__".concat(e).concat(t?"_".concat(t):""),!0),n}function oe(n,e,t){if(i(n))for(var r=0;r<n.length;r++)n[r]&&"string"!=typeof n[r]&&le(n[r],"".concat(e,"_").concat(r),t);else le(n,e,t)}function le(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function de(n,e){if(e)if(p(e)){var t=n.on=n.on?C({},n.on):{};for(var r in e){var i=t[r],s=e[r];t[r]=i?[].concat(i,s):s}}else;return n}function ce(n,e,t,r){e=e||{$stable:!t};for(var s=0;s<n.length;s++){var a=n[s];i(a)?ce(a,e,t):a&&(a.proxy&&(a.fn.proxy=!0),e[a.key]=a.fn)}return r&&(e.$key=r),e}function ue(n,e){for(var t=0;t<e.length;t+=2){var r=e[t];"string"==typeof r&&r&&(n[e[t]]=e[t+1])}return n}function pe(n,e){return"string"==typeof n?e+n:n}function fe(n){n._o=ae,n._n=b,n._s=_,n._l=Qn,n._t=ne,n._q=N,n._i=F,n._m=se,n._f=ee,n._k=re,n._b=ie,n._v=_n,n._e=vn,n._u=ce,n._g=de,n._d=ue,n._p=pe}function he(n,e){if(!n||!n.length)return{};for(var t={},r=0,i=n.length;r<i;r++){var s=n[r],a=s.data;if(a&&a.attrs&&a.attrs.slot&&delete a.attrs.slot,s.context!==e&&s.fnContext!==e||!a||null==a.slot)(t.default||(t.default=[])).push(s);else{var o=a.slot,l=t[o]||(t[o]=[]);"template"===s.tag?l.push.apply(l,s.children||[]):l.push(s)}}for(var d in t)t[d].every(me)&&delete t[d];return t}function me(n){return n.isComment&&!n.asyncFactory||" "===n.text}function ve(n){return n.isComment&&n.asyncFactory}function _e(n,e,t,i){var s,a=Object.keys(t).length>0,o=e?!!e.$stable:!a,l=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(o&&i&&i!==r&&l===i.$key&&!a&&!i.$hasNormal)return i;for(var d in s={},e)e[d]&&"$"!==d[0]&&(s[d]=ge(n,t,d,e[d]))}else s={};for(var c in t)c in s||(s[c]=be(t,c));return e&&Object.isExtensible(e)&&(e._normalized=s),W(s,"$stable",o),W(s,"$key",l),W(s,"$hasNormal",a),s}function ge(n,e,t,r){var s=function(){var e=fn;hn(n);var t=arguments.length?r.apply(null,arguments):r({}),s=(t=t&&"object"==typeof t&&!i(t)?[t]:Xn(t))&&t[0];return hn(e),t&&(!s||1===t.length&&s.isComment&&!ve(s))?void 0:t};return r.proxy&&Object.defineProperty(e,t,{get:s,enumerable:!0,configurable:!0}),s}function be(n,e){return function(){return n[e]}}function ye(n){return{get attrs(){if(!n._attrsProxy){var e=n._attrsProxy={};W(e,"_v_attr_proxy",!0),ke(e,n.$attrs,r,n,"$attrs")}return n._attrsProxy},get listeners(){n._listenersProxy||ke(n._listenersProxy={},n.$listeners,r,n,"$listeners");return n._listenersProxy},get slots(){return function(n){n._slotsProxy||Re(n._slotsProxy={},n.$scopedSlots);return n._slotsProxy}(n)},emit:z(n.$emit,n),expose:function(e){e&&Object.keys(e).forEach((function(t){return qn(n,e,t)}))}}}function ke(n,e,t,r,i){var s=!1;for(var a in e)a in n?e[a]!==t[a]&&(s=!0):(s=!0,Ee(n,a,r,i));for(var a in n)a in e||(s=!0,delete n[a]);return s}function Ee(n,e,t,r){Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){return t[r][e]}})}function Re(n,e){for(var t in e)n[t]=e[t];for(var t in n)t in e||delete n[t]}var we=null;function xe(n,e){return(n.__esModule||pn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),c(n)?e.extend(n):n}function Se(n){if(i(n))for(var e=0;e<n.length;e++){var t=n[e];if(a(t)&&(a(t.componentOptions)||ve(t)))return t}}function Ie(n,e,t,r,u,p){return(i(t)||l(t))&&(u=r,r=t,t=void 0),o(p)&&(u=2),function(n,e,t,r,l){if(a(t)&&a(t.__ob__))return vn();a(t)&&a(t.is)&&(e=t.is);if(!e)return vn();0;i(r)&&d(r[0])&&((t=t||{}).scopedSlots={default:r[0]},r.length=0);2===l?r=Xn(r):1===l&&(r=function(n){for(var e=0;e<n.length;e++)if(i(n[e]))return Array.prototype.concat.apply([],n);return n}(r));var u,p;if("string"==typeof e){var f=void 0;p=n.$vnode&&n.$vnode.ns||K.getTagNamespace(e),u=K.isReservedTag(e)?new mn(K.parsePlatformTagName(e),t,r,void 0,void 0,n):t&&t.pre||!a(f=Lt(n.$options,"components",e))?new mn(e,t,r,void 0,void 0,n):Et(f,t,n,r,e)}else u=Et(e,t,n,r);return i(u)?u:a(u)?(a(p)&&function n(e,t,r){e.ns=t,"foreignObject"===e.tag&&(t=void 0,r=!0);if(a(e.children))for(var i=0,l=e.children.length;i<l;i++){var d=e.children[i];a(d.tag)&&(s(d.ns)||o(r)&&"svg"!==d.tag)&&n(d,t,r)}}(u,p),a(t)&&function(n){c(n.style)&&Ke(n.style);c(n.class)&&Ke(n.class)}(t),u):vn()}(n,e,t,r,u)}function Ae(n,e,t){Rn();try{if(e)for(var r=e;r=r.$parent;){var i=r.$options.errorCaptured;if(i)for(var s=0;s<i.length;s++)try{if(!1===i[s].call(r,n,e,t))return}catch(n){Oe(n,r,"errorCaptured hook")}}Oe(n,e,t)}finally{wn()}}function Te(n,e,t,r,i){var s;try{(s=t?n.apply(e,t):n.call(e))&&!s._isVue&&v(s)&&!s._handled&&(s.catch((function(n){return Ae(n,r,i+" (Promise/async)")})),s._handled=!0)}catch(n){Ae(n,r,i)}return s}function Oe(n,e,t){if(K.errorHandler)try{return K.errorHandler.call(null,n,e,t)}catch(e){e!==n&&ze(e,null,"config.errorHandler")}ze(n,e,t)}function ze(n,e,t){if(!Y||"undefined"==typeof console)throw n;console.error(n)}var Le,Ce=!1,Be=[],Pe=!1;function De(){Pe=!1;var n=Be.slice(0);Be.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&cn(Promise)){var je=Promise.resolve();Le=function(){je.then(De),en&&setTimeout(P)},Ce=!0}else if(J||"undefined"==typeof MutationObserver||!cn(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Le="undefined"!=typeof setImmediate&&cn(setImmediate)?function(){setImmediate(De)}:function(){setTimeout(De,0)};else{var Ne=1,Fe=new MutationObserver(De),Ue=document.createTextNode(String(Ne));Fe.observe(Ue,{characterData:!0}),Le=function(){Ne=(Ne+1)%2,Ue.data=String(Ne)},Ce=!0}function Me(n,e){var t;if(Be.push((function(){if(n)try{n.call(e)}catch(n){Ae(n,e,"nextTick")}else t&&t(e)})),Pe||(Pe=!0,Le()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}function qe(n){return function(e,t){if(void 0===t&&(t=fn),t)return function(n,e,t){var r=n.$options;r[e]=At(r[e],t)}(t,n,e)}}qe("beforeMount"),qe("mounted"),qe("beforeUpdate"),qe("updated"),qe("beforeDestroy"),qe("destroyed"),qe("activated"),qe("deactivated"),qe("serverPrefetch"),qe("renderTracked"),qe("renderTriggered"),qe("errorCaptured");var $e=new un;function Ke(n){return function n(e,t){var r,s,a=i(e);if(!a&&!c(e)||e.__v_skip||Object.isFrozen(e)||e instanceof mn)return;if(e.__ob__){var o=e.__ob__.dep.id;if(t.has(o))return;t.add(o)}if(a)for(r=e.length;r--;)n(e[r],t);else if(Mn(e))n(e.value,t);else for(s=Object.keys(e),r=s.length;r--;)n(e[s[r]],t)}(n,$e),$e.clear(),n}var Ge,He=0,We=function(){function n(n,e,t,r,i){var s,a;s=this,void 0===(a=$n&&!$n._vm?$n:n?n._scope:void 0)&&(a=$n),a&&a.active&&a.effects.push(s),(this.vm=n)&&i&&(n._watcher=this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++He,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new un,this.newDepIds=new un,this.expression="",d(e)?this.getter=e:(this.getter=function(n){if(!V.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=P)),this.value=this.lazy?void 0:this.get()}return n.prototype.get=function(){var n;Rn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Ae(n,e,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Ke(n),wn(),this.cleanupDeps()}return n},n.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},n.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},n.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():ht(this)},n.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||c(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'.concat(this.expression,'"');Te(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},n.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},n.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},n.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&E(this.vm._scope.effects,this),this.active){for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},n}();function Ve(n,e){Ge.$on(n,e)}function Ze(n,e){Ge.$off(n,e)}function Ye(n,e){var t=Ge;return function r(){var i=e.apply(null,arguments);null!==i&&t.$off(n,r)}}function Xe(n,e,t){Ge=n,Vn(e,t||{},Ve,Ze,Ye,n),Ge=void 0}var Je=null;function Qe(n){var e=Je;return Je=n,function(){Je=e}}function nt(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function et(n,e){if(e){if(n._directInactive=!1,nt(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)et(n.$children[t]);tt(n,"activated")}}function tt(n,e,t,r){void 0===r&&(r=!0),Rn();var i=fn,s=$n;r&&hn(n);var a=n.$options[e],o="".concat(e," hook");if(a)for(var l=0,d=a.length;l<d;l++)Te(a[l],n,t||null,n,o);n._hasHookEvent&&n.$emit("hook:"+e),r&&(hn(i),s&&s.on()),wn()}var rt=[],it=[],st={},at=!1,ot=!1,lt=0;var dt=0,ct=Date.now;if(Y&&!J){var ut=window.performance;ut&&"function"==typeof ut.now&&ct()>document.createEvent("Event").timeStamp&&(ct=function(){return ut.now()})}var pt=function(n,e){if(n.post){if(!e.post)return 1}else if(e.post)return-1;return n.id-e.id};function ft(){var n,e;for(dt=ct(),ot=!0,rt.sort(pt),lt=0;lt<rt.length;lt++)(n=rt[lt]).before&&n.before(),e=n.id,st[e]=null,n.run();var t=it.slice(),r=rt.slice();lt=rt.length=it.length=0,st={},at=ot=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,et(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],r=t.vm;r&&r._watcher===t&&r._isMounted&&!r._isDestroyed&&tt(r,"updated")}}(r),function(){for(var n=0;n<yn.length;n++){var e=yn[n];e.subs=e.subs.filter((function(n){return n})),e._pending=!1}yn.length=0}(),dn&&K.devtools&&dn.emit("flush")}function ht(n){var e=n.id;if(null==st[e]&&(n!==kn.target||!n.noRecurse)){if(st[e]=!0,ot){for(var t=rt.length-1;t>lt&&rt[t].id>n.id;)t--;rt.splice(t+1,0,n)}else rt.push(n);at||(at=!0,Me(ft))}}function mt(n,e){if(n){for(var t=Object.create(null),r=pn?Reflect.ownKeys(n):Object.keys(n),i=0;i<r.length;i++){var s=r[i];if("__ob__"!==s){var a=n[s].from;if(a in e._provided)t[s]=e._provided[a];else if("default"in n[s]){var o=n[s].default;t[s]=d(o)?o.call(e):o}else 0}}return t}}function vt(n,e,t,s,a){var l,d=this,c=a.options;w(s,"_uid")?(l=Object.create(s))._original=s:(l=s,s=s._original);var u=o(c._compiled),p=!u;this.data=n,this.props=e,this.children=t,this.parent=s,this.listeners=n.on||r,this.injections=mt(c.inject,s),this.slots=function(){return d.$slots||_e(s,n.scopedSlots,d.$slots=he(t,s)),d.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return _e(s,n.scopedSlots,this.slots())}}),u&&(this.$options=c,this.$slots=this.slots(),this.$scopedSlots=_e(s,n.scopedSlots,this.$slots)),c._scopeId?this._c=function(n,e,t,r){var a=Ie(l,n,e,t,r,p);return a&&!i(a)&&(a.fnScopeId=c._scopeId,a.fnContext=s),a}:this._c=function(n,e,t,r){return Ie(l,n,e,t,r,p)}}function _t(n,e,t,r,i){var s=gn(n);return s.fnContext=t,s.fnOptions=r,e.slot&&((s.data||(s.data={})).slot=e.slot),s}function gt(n,e){for(var t in e)n[I(t)]=e[t]}function bt(n){return n.name||n.__name||n._componentTag}fe(vt.prototype);var yt={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;yt.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},r=n.data.inlineTemplate;a(r)&&(t.render=r.render,t.staticRenderFns=r.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Je)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,i,s){var a=i.data.scopedSlots,o=n.$scopedSlots,l=!!(a&&!a.$stable||o!==r&&!o.$stable||a&&n.$scopedSlots.$key!==a.$key||!a&&n.$scopedSlots.$key),d=!!(s||n.$options._renderChildren||l),c=n.$vnode;n.$options._parentVnode=i,n.$vnode=i,n._vnode&&(n._vnode.parent=i),n.$options._renderChildren=s;var u=i.data.attrs||r;n._attrsProxy&&ke(n._attrsProxy,u,c.data&&c.data.attrs||r,n,"$attrs")&&(d=!0),n.$attrs=u,t=t||r;var p=n.$options._parentListeners;if(n._listenersProxy&&ke(n._listenersProxy,t,p||r,n,"$listeners"),n.$listeners=n.$options._parentListeners=t,Xe(n,t,p),e&&n.$options.props){On(!1);for(var f=n._props,h=n.$options._propKeys||[],m=0;m<h.length;m++){var v=h[m],_=n.$options.props;f[v]=Ct(v,_,e,n)}On(!0),n.$options.propsData=e}d&&(n.$slots=he(s,i.context),n.$forceUpdate())}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,r=n.componentInstance;r._isMounted||(r._isMounted=!0,tt(r,"mounted")),n.data.keepAlive&&(t._isMounted?((e=r)._inactive=!1,it.push(e)):et(r,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(!(t&&(e._directInactive=!0,nt(e))||e._inactive)){e._inactive=!0;for(var r=0;r<e.$children.length;r++)n(e.$children[r]);tt(e,"deactivated")}}(e,!0):e.$destroy())}},kt=Object.keys(yt);function Et(n,e,t,l,d){if(!s(n)){var u=t.$options._base;if(c(n)&&(n=u.extend(n)),"function"==typeof n){var p;if(s(n.cid)&&void 0===(n=function(n,e){if(o(n.error)&&a(n.errorComp))return n.errorComp;if(a(n.resolved))return n.resolved;var t=we;if(t&&a(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t),o(n.loading)&&a(n.loadingComp))return n.loadingComp;if(t&&!a(n.owners)){var r=n.owners=[t],i=!0,l=null,d=null;t.$on("hook:destroyed",(function(){return E(r,t)}));var u=function(n){for(var e=0,t=r.length;e<t;e++)r[e].$forceUpdate();n&&(r.length=0,null!==l&&(clearTimeout(l),l=null),null!==d&&(clearTimeout(d),d=null))},p=U((function(t){n.resolved=xe(t,e),i?r.length=0:u(!0)})),f=U((function(e){a(n.errorComp)&&(n.error=!0,u(!0))})),h=n(p,f);return c(h)&&(v(h)?s(n.resolved)&&h.then(p,f):v(h.component)&&(h.component.then(p,f),a(h.error)&&(n.errorComp=xe(h.error,e)),a(h.loading)&&(n.loadingComp=xe(h.loading,e),0===h.delay?n.loading=!0:l=setTimeout((function(){l=null,s(n.resolved)&&s(n.error)&&(n.loading=!0,u(!1))}),h.delay||200)),a(h.timeout)&&(d=setTimeout((function(){d=null,s(n.resolved)&&f(null)}),h.timeout)))),i=!1,n.loading?n.loadingComp:n.resolved}}(p=n,u)))return function(n,e,t,r,i){var s=vn();return s.asyncFactory=n,s.asyncMeta={data:e,context:t,children:r,tag:i},s}(p,e,t,l,d);e=e||{},Wt(n),a(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",r=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var s=e.on||(e.on={}),o=s[r],l=e.model.callback;a(o)?(i(o)?-1===o.indexOf(l):o!==l)&&(s[r]=[l].concat(o)):s[r]=l}(n.options,e);var f=function(n,e,t){var r=e.options.props;if(!s(r)){var i={},o=n.attrs,l=n.props;if(a(o)||a(l))for(var d in r){var c=O(d);Yn(i,l,d,c,!0)||Yn(i,o,d,c,!1)}return i}}(e,n);if(o(n.options.functional))return function(n,e,t,s,o){var l=n.options,d={},c=l.props;if(a(c))for(var u in c)d[u]=Ct(u,c,e||r);else a(t.attrs)&&gt(d,t.attrs),a(t.props)&&gt(d,t.props);var p=new vt(t,d,o,s,n),f=l.render.call(null,p._c,p);if(f instanceof mn)return _t(f,t,p.parent,l,p);if(i(f)){for(var h=Xn(f)||[],m=new Array(h.length),v=0;v<h.length;v++)m[v]=_t(h[v],t,p.parent,l,p);return m}}(n,f,e,t,l);var h=e.on;if(e.on=e.nativeOn,o(n.options.abstract)){var m=e.slot;e={},m&&(e.slot=m)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<kt.length;t++){var r=kt[t],i=e[r],s=yt[r];i===s||i&&i._merged||(e[r]=i?Rt(s,i):s)}}(e);var _=bt(n.options)||d;return new mn("vue-component-".concat(n.cid).concat(_?"-".concat(_):""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:f,listeners:h,tag:d,children:l},p)}}}function Rt(n,e){var t=function(t,r){n(t,r),e(t,r)};return t._merged=!0,t}var wt=P,xt=K.optionMergeStrategies;function St(n,e,t){if(void 0===t&&(t=!0),!e)return n;for(var r,i,s,a=pn?Reflect.ownKeys(e):Object.keys(e),o=0;o<a.length;o++)"__ob__"!==(r=a[o])&&(i=n[r],s=e[r],t&&w(n,r)?i!==s&&p(i)&&p(s)&&St(i,s):Pn(n,r,s));return n}function It(n,e,t){return t?function(){var r=d(e)?e.call(t,t):e,i=d(n)?n.call(t,t):n;return r?St(r,i):i}:e?n?function(){return St(d(e)?e.call(this,this):e,d(n)?n.call(this,this):n)}:e:n}function At(n,e){var t=e?n?n.concat(e):i(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function Tt(n,e,t,r){var i=Object.create(n||null);return e?C(i,e):i}xt.data=function(n,e,t){return t?It(n,e,t):e&&"function"!=typeof e?n:It(n,e)},$.forEach((function(n){xt[n]=At})),q.forEach((function(n){xt[n+"s"]=Tt})),xt.watch=function(n,e,t,r){if(n===sn&&(n=void 0),e===sn&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var s={};for(var a in C(s,n),e){var o=s[a],l=e[a];o&&!i(o)&&(o=[o]),s[a]=o?o.concat(l):i(l)?l:[l]}return s},xt.props=xt.methods=xt.inject=xt.computed=function(n,e,t,r){if(!n)return e;var i=Object.create(null);return C(i,n),e&&C(i,e),i},xt.provide=function(n,e){return n?function(){var t=Object.create(null);return St(t,d(n)?n.call(this):n),e&&St(t,d(e)?e.call(this):e,!1),t}:e};var Ot=function(n,e){return void 0===e?n:e};function zt(n,e,t){if(d(e)&&(e=e.options),function(n,e){var t=n.props;if(t){var r,s,a={};if(i(t))for(r=t.length;r--;)"string"==typeof(s=t[r])&&(a[I(s)]={type:null});else if(p(t))for(var o in t)s=t[o],a[I(o)]=p(s)?s:{type:s};else 0;n.props=a}}(e),function(n,e){var t=n.inject;if(t){var r=n.inject={};if(i(t))for(var s=0;s<t.length;s++)r[t[s]]={from:t[s]};else if(p(t))for(var a in t){var o=t[a];r[a]=p(o)?C({from:a},o):{from:o}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var r=e[t];d(r)&&(e[t]={bind:r,update:r})}}(e),!e._base&&(e.extends&&(n=zt(n,e.extends,t)),e.mixins))for(var r=0,s=e.mixins.length;r<s;r++)n=zt(n,e.mixins[r],t);var a,o={};for(a in n)l(a);for(a in e)w(n,a)||l(a);function l(r){var i=xt[r]||Ot;o[r]=i(n[r],e[r],t,r)}return o}function Lt(n,e,t,r){if("string"==typeof t){var i=n[e];if(w(i,t))return i[t];var s=I(t);if(w(i,s))return i[s];var a=A(s);return w(i,a)?i[a]:i[t]||i[s]||i[a]}}function Ct(n,e,t,r){var i=e[n],s=!w(t,n),a=t[n],o=jt(Boolean,i.type);if(o>-1)if(s&&!w(i,"default"))a=!1;else if(""===a||a===O(n)){var l=jt(String,i.type);(l<0||o<l)&&(a=!0)}if(void 0===a){a=function(n,e,t){if(!w(e,"default"))return;var r=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return d(r)&&"Function"!==Pt(e.type)?r.call(n):r}(r,i,n);var c=Tn;On(!0),Cn(a),On(c)}return a}var Bt=/^\s*function (\w+)/;function Pt(n){var e=n&&n.toString().match(Bt);return e?e[1]:""}function Dt(n,e){return Pt(n)===Pt(e)}function jt(n,e){if(!i(e))return Dt(e,n)?0:-1;for(var t=0,r=e.length;t<r;t++)if(Dt(e[t],n))return t;return-1}var Nt={enumerable:!0,configurable:!0,get:P,set:P};function Ft(n,e,t){Nt.get=function(){return this[e][t]},Nt.set=function(n){this[e][t]=n},Object.defineProperty(n,t,Nt)}function Ut(n){var e=n.$options;if(e.props&&function(n,e){var t=n.$options.propsData||{},r=n._props=Nn({}),i=n.$options._propKeys=[];n.$parent&&On(!1);var s=function(s){i.push(s);var a=Ct(s,e,t,n);Bn(r,s,a,void 0,!0),s in n||Ft(n,"_props",s)};for(var a in e)s(a);On(!0)}(n,e.props),function(n){var e=n.$options,t=e.setup;if(t){var r=n._setupContext=ye(n);hn(n),Rn();var i=Te(t,null,[n._props||Nn({}),r],n,"setup");if(wn(),hn(),d(i))e.render=i;else if(c(i))if(n._setupState=i,i.__sfc){var s=n._setupProxy={};for(var a in i)"__sfc"!==a&&qn(s,i,a)}else for(var a in i)H(a)||qn(n,i,a);else 0}}(n),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?P:z(e[t],n)}(n,e.methods),e.data)!function(n){var e=n.$options.data;p(e=n._data=d(e)?function(n,e){Rn();try{return n.call(e,e)}catch(n){return Ae(n,e,"data()"),{}}finally{wn()}}(e,n):e||{})||(e={});var t=Object.keys(e),r=n.$options.props,i=(n.$options.methods,t.length);for(;i--;){var s=t[i];0,r&&w(r,s)||H(s)||Ft(n,"_data",s)}var a=Cn(e);a&&a.vmCount++}(n);else{var t=Cn(n._data={});t&&t.vmCount++}e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),r=ln();for(var i in e){var s=e[i],a=d(s)?s:s.get;0,r||(t[i]=new We(n,a||P,P,Mt)),i in n||qt(n,i,s)}}(n,e.computed),e.watch&&e.watch!==sn&&function(n,e){for(var t in e){var r=e[t];if(i(r))for(var s=0;s<r.length;s++)Gt(n,t,r[s]);else Gt(n,t,r)}}(n,e.watch)}var Mt={lazy:!0};function qt(n,e,t){var r=!ln();d(t)?(Nt.get=r?$t(e):Kt(t),Nt.set=P):(Nt.get=t.get?r&&!1!==t.cache?$t(e):Kt(t.get):P,Nt.set=t.set||P),Object.defineProperty(n,e,Nt)}function $t(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),kn.target&&e.depend(),e.value}}function Kt(n){return function(){return n.call(this,this)}}function Gt(n,e,t,r){return p(t)&&(r=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,r)}var Ht=0;function Wt(n){var e=n.options;if(n.super){var t=Wt(n.super);if(t!==n.superOptions){n.superOptions=t;var r=function(n){var e,t=n.options,r=n.sealedOptions;for(var i in t)t[i]!==r[i]&&(e||(e={}),e[i]=t[i]);return e}(n);r&&C(n.extendOptions,r),(e=n.options=zt(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function Vt(n){this._init(n)}function Zt(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,r=t.cid,i=n._Ctor||(n._Ctor={});if(i[r])return i[r];var s=bt(n)||bt(t.options);var a=function(n){this._init(n)};return(a.prototype=Object.create(t.prototype)).constructor=a,a.cid=e++,a.options=zt(t.options,n),a.super=t,a.options.props&&function(n){var e=n.options.props;for(var t in e)Ft(n.prototype,"_props",t)}(a),a.options.computed&&function(n){var e=n.options.computed;for(var t in e)qt(n.prototype,t,e[t])}(a),a.extend=t.extend,a.mixin=t.mixin,a.use=t.use,q.forEach((function(n){a[n]=t[n]})),s&&(a.options.components[s]=a),a.superOptions=t.options,a.extendOptions=n,a.sealedOptions=C({},a.options),i[r]=a,a}}function Yt(n){return n&&(bt(n.Ctor.options)||n.tag)}function Xt(n,e){return i(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!f(n)&&n.test(e)}function Jt(n,e){var t=n.cache,r=n.keys,i=n._vnode,s=n.$vnode;for(var a in t){var o=t[a];if(o){var l=o.name;l&&!e(l)&&Qt(t,a,r,i)}}s.componentOptions.children=void 0}function Qt(n,e,t,r){var i=n[e];!i||r&&i.tag===r.tag||i.componentInstance.$destroy(),n[e]=null,E(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=Ht++,e._isVue=!0,e.__v_skip=!0,e._scope=new Kn(!0),e._scope.parent=void 0,e._scope._vm=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),r=e._parentVnode;t.parent=e.parent,t._parentVnode=r;var i=r.componentOptions;t.propsData=i.propsData,t._parentListeners=i.listeners,t._renderChildren=i.children,t._componentTag=i.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=zt(Wt(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._provided=t?t._provided:Object.create(null),n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&Xe(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,i=t&&t.context;n.$slots=he(e._renderChildren,i),n.$scopedSlots=t?_e(n.$parent,t.data.scopedSlots,n.$slots):r,n._c=function(e,t,r,i){return Ie(n,e,t,r,i,!1)},n.$createElement=function(e,t,r,i){return Ie(n,e,t,r,i,!0)};var s=t&&t.data;Bn(n,"$attrs",s&&s.attrs||r,null,!0),Bn(n,"$listeners",e._parentListeners||r,null,!0)}(e),tt(e,"beforeCreate",void 0,!1),function(n){var e=mt(n.$options.inject,n);e&&(On(!1),Object.keys(e).forEach((function(t){Bn(n,t,e[t])})),On(!0))}(e),Ut(e),function(n){var e=n.$options.provide;if(e){var t=d(e)?e.call(n):e;if(!c(t))return;for(var r=Gn(n),i=pn?Reflect.ownKeys(t):Object.keys(t),s=0;s<i.length;s++){var a=i[s];Object.defineProperty(r,a,Object.getOwnPropertyDescriptor(t,a))}}}(e),tt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(Vt),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=Pn,n.prototype.$delete=Dn,n.prototype.$watch=function(n,e,t){if(p(e))return Gt(this,n,e,t);(t=t||{}).user=!0;var r=new We(this,n,e,t);if(t.immediate){var i='callback for immediate watcher "'.concat(r.expression,'"');Rn(),Te(e,this,[r.value],this,i),wn()}return function(){r.teardown()}}}(Vt),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var r=this;if(i(n))for(var s=0,a=n.length;s<a;s++)r.$on(n[s],t);else(r._events[n]||(r._events[n]=[])).push(t),e.test(n)&&(r._hasHookEvent=!0);return r},n.prototype.$once=function(n,e){var t=this;function r(){t.$off(n,r),e.apply(t,arguments)}return r.fn=e,t.$on(n,r),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(i(n)){for(var r=0,s=n.length;r<s;r++)t.$off(n[r],e);return t}var a,o=t._events[n];if(!o)return t;if(!e)return t._events[n]=null,t;for(var l=o.length;l--;)if((a=o[l])===e||a.fn===e){o.splice(l,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?L(t):t;for(var r=L(arguments,1),i='event handler for "'.concat(n,'"'),s=0,a=t.length;s<a;s++)Te(t[s],e,r,e,i)}return e}}(Vt),function(n){n.prototype._update=function(n,e){var t=this,r=t.$el,i=t._vnode,s=Qe(t);t._vnode=n,t.$el=i?t.__patch__(i,n):t.__patch__(t.$el,n,e,!1),s(),r&&(r.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var a=t;a&&a.$vnode&&a.$parent&&a.$vnode===a.$parent._vnode;)a.$parent.$el=a.$el,a=a.$parent},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){tt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||E(e.$children,n),n._scope.stop(),n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),tt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(Vt),function(n){fe(n.prototype),n.prototype.$nextTick=function(n){return Me(n,this)},n.prototype._render=function(){var n=this,e=n.$options,t=e.render,r=e._parentVnode;r&&n._isMounted&&(n.$scopedSlots=_e(n.$parent,r.data.scopedSlots,n.$slots,n.$scopedSlots),n._slotsProxy&&Re(n._slotsProxy,n.$scopedSlots)),n.$vnode=r;var s,a=fn,o=we;try{hn(n),we=n,s=t.call(n._renderProxy,n.$createElement)}catch(e){Ae(e,n,"render"),s=n._vnode}finally{we=o,hn(a)}return i(s)&&1===s.length&&(s=s[0]),s instanceof mn||(s=vn()),s.parent=r,s}}(Vt);var nr=[String,RegExp,Array],er={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:nr,exclude:nr,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,r=this.keyToCache;if(t){var i=t.tag,s=t.componentInstance,a=t.componentOptions;n[r]={name:Yt(a),tag:i,componentInstance:s},e.push(r),this.max&&e.length>parseInt(this.max)&&Qt(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Qt(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){Jt(n,(function(n){return Xt(e,n)}))})),this.$watch("exclude",(function(e){Jt(n,(function(n){return!Xt(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=Se(n),t=e&&e.componentOptions;if(t){var r=Yt(t),i=this.include,s=this.exclude;if(i&&(!r||!Xt(i,r))||s&&r&&Xt(s,r))return e;var a=this.cache,o=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):e.key;a[l]?(e.componentInstance=a[l].componentInstance,E(o,l),o.push(l)):(this.vnodeToCache=e,this.keyToCache=l),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return K}};Object.defineProperty(n,"config",e),n.util={warn:wt,extend:C,mergeOptions:zt,defineReactive:Bn},n.set=Pn,n.delete=Dn,n.nextTick=Me,n.observable=function(n){return Cn(n),n},n.options=Object.create(null),q.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,C(n.options.components,er),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=L(arguments,1);return t.unshift(this),d(n.install)?n.install.apply(n,t):d(n)&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=zt(this.options,n),this}}(n),Zt(n),function(n){q.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&p(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&d(t)&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(Vt),Object.defineProperty(Vt.prototype,"$isServer",{get:ln}),Object.defineProperty(Vt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Vt,"FunctionalRenderContext",{value:vt}),Vt.version="2.7.16";var tr=y("style,class"),rr=y("input,textarea,option,select,progress"),ir=y("contenteditable,draggable,spellcheck"),sr=y("events,caret,typing,plaintext-only"),ar=y("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),or="http://www.w3.org/1999/xlink",lr=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},dr=function(n){return lr(n)?n.slice(6,n.length):""},cr=function(n){return null==n||!1===n};function ur(n){for(var e=n.data,t=n,r=n;a(r.componentInstance);)(r=r.componentInstance._vnode)&&r.data&&(e=pr(r.data,e));for(;a(t=t.parent);)t&&t.data&&(e=pr(e,t.data));return function(n,e){if(a(n)||a(e))return fr(n,hr(e));return""}(e.staticClass,e.class)}function pr(n,e){return{staticClass:fr(n.staticClass,e.staticClass),class:a(n.class)?[n.class,e.class]:e.class}}function fr(n,e){return n?e?n+" "+e:n:e||""}function hr(n){return Array.isArray(n)?function(n){for(var e,t="",r=0,i=n.length;r<i;r++)a(e=hr(n[r]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):c(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var mr={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},vr=y("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),_r=y("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),gr=function(n){return vr(n)||_r(n)};var br=Object.create(null);var yr=y("text,number,password,search,email,tel,url");var kr=Object.freeze({__proto__:null,createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(mr[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Er={create:function(n,e){Rr(e)},update:function(n,e){n.data.ref!==e.data.ref&&(Rr(n,!0),Rr(e))},destroy:function(n){Rr(n,!0)}};function Rr(n,e){var t=n.data.ref;if(a(t)){var r=n.context,s=n.componentInstance||n.elm,o=e?null:s,l=e?void 0:s;if(d(t))Te(t,r,[o],r,"template ref function");else{var c=n.data.refInFor,u="string"==typeof t||"number"==typeof t,p=Mn(t),f=r.$refs;if(u||p)if(c){var h=u?f[t]:t.value;e?i(h)&&E(h,s):i(h)?h.includes(s)||h.push(s):u?(f[t]=[s],wr(r,t,f[t])):t.value=[s]}else if(u){if(e&&f[t]!==s)return;f[t]=l,wr(r,t,o)}else if(p){if(e&&t.value!==s)return;t.value=o}else 0}}}function wr(n,e,t){var r=n._setupState;r&&w(r,e)&&(Mn(r[e])?r[e].value=t:r[e]=t)}var xr=new mn("",{},[]),Sr=["create","activate","update","remove","destroy"];function Ir(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&a(n.data)===a(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,r=a(t=n.data)&&a(t=t.attrs)&&t.type,i=a(t=e.data)&&a(t=t.attrs)&&t.type;return r===i||yr(r)&&yr(i)}(n,e)||o(n.isAsyncPlaceholder)&&s(e.asyncFactory.error))}function Ar(n,e,t){var r,i,s={};for(r=e;r<=t;++r)a(i=n[r].key)&&(s[i]=r);return s}var Tr={create:Or,update:Or,destroy:function(n){Or(n,xr)}};function Or(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,r,i,s=n===xr,a=e===xr,o=Lr(n.data.directives,n.context),l=Lr(e.data.directives,e.context),d=[],c=[];for(t in l)r=o[t],i=l[t],r?(i.oldValue=r.value,i.oldArg=r.arg,Br(i,"update",e,n),i.def&&i.def.componentUpdated&&c.push(i)):(Br(i,"bind",e,n),i.def&&i.def.inserted&&d.push(i));if(d.length){var u=function(){for(var t=0;t<d.length;t++)Br(d[t],"inserted",e,n)};s?Zn(e,"insert",u):u()}c.length&&Zn(e,"postpatch",(function(){for(var t=0;t<c.length;t++)Br(c[t],"componentUpdated",e,n)}));if(!s)for(t in o)l[t]||Br(o[t],"unbind",n,n,a)}(n,e)}var zr=Object.create(null);function Lr(n,e){var t,r,i=Object.create(null);if(!n)return i;for(t=0;t<n.length;t++){if((r=n[t]).modifiers||(r.modifiers=zr),i[Cr(r)]=r,e._setupState&&e._setupState.__sfc){var s=r.def||Lt(e,"_setupState","v-"+r.name);r.def="function"==typeof s?{bind:s,update:s}:s}r.def=r.def||Lt(e.$options,"directives",r.name)}return i}function Cr(n){return n.rawName||"".concat(n.name,".").concat(Object.keys(n.modifiers||{}).join("."))}function Br(n,e,t,r,i){var s=n.def&&n.def[e];if(s)try{s(t.elm,n,t,r,i)}catch(r){Ae(r,t.context,"directive ".concat(n.name," ").concat(e," hook"))}}var Pr=[Er,Tr];function Dr(n,e){var t=e.componentOptions;if(!(a(t)&&!1===t.Ctor.options.inheritAttrs||s(n.data.attrs)&&s(e.data.attrs))){var r,i,l=e.elm,d=n.data.attrs||{},c=e.data.attrs||{};for(r in(a(c.__ob__)||o(c._v_attr_proxy))&&(c=e.data.attrs=C({},c)),c)i=c[r],d[r]!==i&&jr(l,r,i,e.data.pre);for(r in(J||nn)&&c.value!==d.value&&jr(l,"value",c.value),d)s(c[r])&&(lr(r)?l.removeAttributeNS(or,dr(r)):ir(r)||l.removeAttribute(r))}}function jr(n,e,t,r){r||n.tagName.indexOf("-")>-1?Nr(n,e,t):ar(e)?cr(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):ir(e)?n.setAttribute(e,function(n,e){return cr(e)||"false"===e?"false":"contenteditable"===n&&sr(e)?e:"true"}(e,t)):lr(e)?cr(t)?n.removeAttributeNS(or,dr(e)):n.setAttributeNS(or,e,t):Nr(n,e,t)}function Nr(n,e,t){if(cr(t))n.removeAttribute(e);else{if(J&&!Q&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var r=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",r)};n.addEventListener("input",r),n.__ieph=!0}n.setAttribute(e,t)}}var Fr={create:Dr,update:Dr};function Ur(n,e){var t=e.elm,r=e.data,i=n.data;if(!(s(r.staticClass)&&s(r.class)&&(s(i)||s(i.staticClass)&&s(i.class)))){var o=ur(e),l=t._transitionClasses;a(l)&&(o=fr(o,hr(l))),o!==t._prevClass&&(t.setAttribute("class",o),t._prevClass=o)}}var Mr,qr={create:Ur,update:Ur};function $r(n,e,t){var r=Mr;return function i(){var s=e.apply(null,arguments);null!==s&&Hr(n,i,t,r)}}var Kr=Ce&&!(rn&&Number(rn[1])<=53);function Gr(n,e,t,r){if(Kr){var i=dt,s=e;e=s._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=i||n.timeStamp<=0||n.target.ownerDocument!==document)return s.apply(this,arguments)}}Mr.addEventListener(n,e,an?{capture:t,passive:r}:t)}function Hr(n,e,t,r){(r||Mr).removeEventListener(n,e._wrapper||e,t)}function Wr(n,e){if(!s(n.data.on)||!s(e.data.on)){var t=e.data.on||{},r=n.data.on||{};Mr=e.elm||n.elm,function(n){if(a(n.__r)){var e=J?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}a(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),Vn(t,r,Gr,Hr,$r,e.context),Mr=void 0}}var Vr,Zr={create:Wr,update:Wr,destroy:function(n){return Wr(n,xr)}};function Yr(n,e){if(!s(n.data.domProps)||!s(e.data.domProps)){var t,r,i=e.elm,l=n.data.domProps||{},d=e.data.domProps||{};for(t in(a(d.__ob__)||o(d._v_attr_proxy))&&(d=e.data.domProps=C({},d)),l)t in d||(i[t]="");for(t in d){if(r=d[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),r===l[t])continue;1===i.childNodes.length&&i.removeChild(i.childNodes[0])}if("value"===t&&"PROGRESS"!==i.tagName){i._value=r;var c=s(r)?"":String(r);Xr(i,c)&&(i.value=c)}else if("innerHTML"===t&&_r(i.tagName)&&s(i.innerHTML)){(Vr=Vr||document.createElement("div")).innerHTML="<svg>".concat(r,"</svg>");for(var u=Vr.firstChild;i.firstChild;)i.removeChild(i.firstChild);for(;u.firstChild;)i.appendChild(u.firstChild)}else if(r!==l[t])try{i[t]=r}catch(n){}}}}function Xr(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,r=n._vModifiers;if(a(r)){if(r.number)return b(t)!==b(e);if(r.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var Jr={create:Yr,update:Yr},Qr=x((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var r=n.split(t);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function ni(n){var e=ei(n.style);return n.staticStyle?C(n.staticStyle,e):e}function ei(n){return Array.isArray(n)?B(n):"string"==typeof n?Qr(n):n}var ti,ri=/^--/,ii=/\s*!important$/,si=function(n,e,t){if(ri.test(e))n.style.setProperty(e,t);else if(ii.test(t))n.style.setProperty(O(e),t.replace(ii,""),"important");else{var r=oi(e);if(Array.isArray(t))for(var i=0,s=t.length;i<s;i++)n.style[r]=t[i];else n.style[r]=t}},ai=["Webkit","Moz","ms"],oi=x((function(n){if(ti=ti||document.createElement("div").style,"filter"!==(n=I(n))&&n in ti)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<ai.length;t++){var r=ai[t]+e;if(r in ti)return r}}));function li(n,e){var t=e.data,r=n.data;if(!(s(t.staticStyle)&&s(t.style)&&s(r.staticStyle)&&s(r.style))){var i,o,l=e.elm,d=r.staticStyle,c=r.normalizedStyle||r.style||{},u=d||c,p=ei(e.data.style)||{};e.data.normalizedStyle=a(p.__ob__)?C({},p):p;var f=function(n,e){var t,r={};if(e)for(var i=n;i.componentInstance;)(i=i.componentInstance._vnode)&&i.data&&(t=ni(i.data))&&C(r,t);(t=ni(n.data))&&C(r,t);for(var s=n;s=s.parent;)s.data&&(t=ni(s.data))&&C(r,t);return r}(e,!0);for(o in u)s(f[o])&&si(l,o,"");for(o in f)i=f[o],si(l,o,null==i?"":i)}}var di={create:li,update:li},ci=/\s+/;function ui(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ci).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" ".concat(n.getAttribute("class")||""," ");t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function pi(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ci).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" ".concat(n.getAttribute("class")||""," "),r=" "+e+" ";t.indexOf(r)>=0;)t=t.replace(r," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function fi(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&C(e,hi(n.name||"v")),C(e,n),e}return"string"==typeof n?hi(n):void 0}}var hi=x((function(n){return{enterClass:"".concat(n,"-enter"),enterToClass:"".concat(n,"-enter-to"),enterActiveClass:"".concat(n,"-enter-active"),leaveClass:"".concat(n,"-leave"),leaveToClass:"".concat(n,"-leave-to"),leaveActiveClass:"".concat(n,"-leave-active")}})),mi=Y&&!Q,vi="transition",_i="transitionend",gi="animation",bi="animationend";mi&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(vi="WebkitTransition",_i="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(gi="WebkitAnimation",bi="webkitAnimationEnd"));var yi=Y?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function ki(n){yi((function(){yi(n)}))}function Ei(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),ui(n,e))}function Ri(n,e){n._transitionClasses&&E(n._transitionClasses,e),pi(n,e)}function wi(n,e,t){var r=Si(n,e),i=r.type,s=r.timeout,a=r.propCount;if(!i)return t();var o="transition"===i?_i:bi,l=0,d=function(){n.removeEventListener(o,c),t()},c=function(e){e.target===n&&++l>=a&&d()};setTimeout((function(){l<a&&d()}),s+1),n.addEventListener(o,c)}var xi=/\b(transform|all)(,|$)/;function Si(n,e){var t,r=window.getComputedStyle(n),i=(r[vi+"Delay"]||"").split(", "),s=(r[vi+"Duration"]||"").split(", "),a=Ii(i,s),o=(r[gi+"Delay"]||"").split(", "),l=(r[gi+"Duration"]||"").split(", "),d=Ii(o,l),c=0,u=0;return"transition"===e?a>0&&(t="transition",c=a,u=s.length):"animation"===e?d>0&&(t="animation",c=d,u=l.length):u=(t=(c=Math.max(a,d))>0?a>d?"transition":"animation":null)?"transition"===t?s.length:l.length:0,{type:t,timeout:c,propCount:u,hasTransform:"transition"===t&&xi.test(r[vi+"Property"])}}function Ii(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return Ai(e)+Ai(n[t])})))}function Ai(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function Ti(n,e){var t=n.elm;a(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var r=fi(n.data.transition);if(!s(r)&&!a(t._enterCb)&&1===t.nodeType){for(var i=r.css,o=r.type,l=r.enterClass,u=r.enterToClass,p=r.enterActiveClass,f=r.appearClass,h=r.appearToClass,m=r.appearActiveClass,v=r.beforeEnter,_=r.enter,g=r.afterEnter,y=r.enterCancelled,k=r.beforeAppear,E=r.appear,R=r.afterAppear,w=r.appearCancelled,x=r.duration,S=Je,I=Je.$vnode;I&&I.parent;)S=I.context,I=I.parent;var A=!S._isMounted||!n.isRootInsert;if(!A||E||""===E){var T=A&&f?f:l,O=A&&m?m:p,z=A&&h?h:u,L=A&&k||v,C=A&&d(E)?E:_,B=A&&R||g,P=A&&w||y,D=b(c(x)?x.enter:x);0;var j=!1!==i&&!Q,N=Li(C),F=t._enterCb=U((function(){j&&(Ri(t,z),Ri(t,O)),F.cancelled?(j&&Ri(t,T),P&&P(t)):B&&B(t),t._enterCb=null}));n.data.show||Zn(n,"insert",(function(){var e=t.parentNode,r=e&&e._pending&&e._pending[n.key];r&&r.tag===n.tag&&r.elm._leaveCb&&r.elm._leaveCb(),C&&C(t,F)})),L&&L(t),j&&(Ei(t,T),Ei(t,O),ki((function(){Ri(t,T),F.cancelled||(Ei(t,z),N||(zi(D)?setTimeout(F,D):wi(t,o,F)))}))),n.data.show&&(e&&e(),C&&C(t,F)),j||N||F()}}}function Oi(n,e){var t=n.elm;a(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var r=fi(n.data.transition);if(s(r)||1!==t.nodeType)return e();if(!a(t._leaveCb)){var i=r.css,o=r.type,l=r.leaveClass,d=r.leaveToClass,u=r.leaveActiveClass,p=r.beforeLeave,f=r.leave,h=r.afterLeave,m=r.leaveCancelled,v=r.delayLeave,_=r.duration,g=!1!==i&&!Q,y=Li(f),k=b(c(_)?_.leave:_);0;var E=t._leaveCb=U((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),g&&(Ri(t,d),Ri(t,u)),E.cancelled?(g&&Ri(t,l),m&&m(t)):(e(),h&&h(t)),t._leaveCb=null}));v?v(R):R()}function R(){E.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),p&&p(t),g&&(Ei(t,l),Ei(t,u),ki((function(){Ri(t,l),E.cancelled||(Ei(t,d),y||(zi(k)?setTimeout(E,k):wi(t,o,E)))}))),f&&f(t,E),g||y||E())}}function zi(n){return"number"==typeof n&&!isNaN(n)}function Li(n){if(s(n))return!1;var e=n.fns;return a(e)?Li(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function Ci(n,e){!0!==e.data.show&&Ti(e)}var Bi=function(n){var e,t,r={},d=n.modules,c=n.nodeOps;for(e=0;e<Sr.length;++e)for(r[Sr[e]]=[],t=0;t<d.length;++t)a(d[t][Sr[e]])&&r[Sr[e]].push(d[t][Sr[e]]);function u(n){var e=c.parentNode(n);a(e)&&c.removeChild(e,n)}function p(n,e,t,i,s,l,d){if(a(n.elm)&&a(l)&&(n=l[d]=gn(n)),n.isRootInsert=!s,!function(n,e,t,i){var s=n.data;if(a(s)){var l=a(n.componentInstance)&&s.keepAlive;if(a(s=s.hook)&&a(s=s.init)&&s(n,!1),a(n.componentInstance))return f(n,e),h(t,n.elm,i),o(l)&&function(n,e,t,i){var s,o=n;for(;o.componentInstance;)if(o=o.componentInstance._vnode,a(s=o.data)&&a(s=s.transition)){for(s=0;s<r.activate.length;++s)r.activate[s](xr,o);e.push(o);break}h(t,n.elm,i)}(n,e,t,i),!0}}(n,e,t,i)){var u=n.data,p=n.children,v=n.tag;a(v)?(n.elm=n.ns?c.createElementNS(n.ns,v):c.createElement(v,n),g(n),m(n,p,e),a(u)&&_(n,e),h(t,n.elm,i)):o(n.isComment)?(n.elm=c.createComment(n.text),h(t,n.elm,i)):(n.elm=c.createTextNode(n.text),h(t,n.elm,i))}}function f(n,e){a(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,v(n)?(_(n,e),g(n)):(Rr(n),e.push(n))}function h(n,e,t){a(n)&&(a(t)?c.parentNode(t)===n&&c.insertBefore(n,e,t):c.appendChild(n,e))}function m(n,e,t){if(i(e)){0;for(var r=0;r<e.length;++r)p(e[r],t,n.elm,null,!0,e,r)}else l(n.text)&&c.appendChild(n.elm,c.createTextNode(String(n.text)))}function v(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return a(n.tag)}function _(n,t){for(var i=0;i<r.create.length;++i)r.create[i](xr,n);a(e=n.data.hook)&&(a(e.create)&&e.create(xr,n),a(e.insert)&&t.push(n))}function g(n){var e;if(a(e=n.fnScopeId))c.setStyleScope(n.elm,e);else for(var t=n;t;)a(e=t.context)&&a(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e),t=t.parent;a(e=Je)&&e!==n.context&&e!==n.fnContext&&a(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e)}function b(n,e,t,r,i,s){for(;r<=i;++r)p(t[r],s,n,e,!1,t,r)}function k(n){var e,t,i=n.data;if(a(i))for(a(e=i.hook)&&a(e=e.destroy)&&e(n),e=0;e<r.destroy.length;++e)r.destroy[e](n);if(a(e=n.children))for(t=0;t<n.children.length;++t)k(n.children[t])}function E(n,e,t){for(;e<=t;++e){var r=n[e];a(r)&&(a(r.tag)?(R(r),k(r)):u(r.elm))}}function R(n,e){if(a(e)||a(n.data)){var t,i=r.remove.length+1;for(a(e)?e.listeners+=i:e=function(n,e){function t(){0==--t.listeners&&u(n)}return t.listeners=e,t}(n.elm,i),a(t=n.componentInstance)&&a(t=t._vnode)&&a(t.data)&&R(t,e),t=0;t<r.remove.length;++t)r.remove[t](n,e);a(t=n.data.hook)&&a(t=t.remove)?t(n,e):e()}else u(n.elm)}function w(n,e,t,r){for(var i=t;i<r;i++){var s=e[i];if(a(s)&&Ir(n,s))return i}}function x(n,e,t,i,l,d){if(n!==e){a(e.elm)&&a(i)&&(e=i[l]=gn(e));var u=e.elm=n.elm;if(o(n.isAsyncPlaceholder))a(e.asyncFactory.resolved)?A(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(o(e.isStatic)&&o(n.isStatic)&&e.key===n.key&&(o(e.isCloned)||o(e.isOnce)))e.componentInstance=n.componentInstance;else{var f,h=e.data;a(h)&&a(f=h.hook)&&a(f=f.prepatch)&&f(n,e);var m=n.children,_=e.children;if(a(h)&&v(e)){for(f=0;f<r.update.length;++f)r.update[f](n,e);a(f=h.hook)&&a(f=f.update)&&f(n,e)}s(e.text)?a(m)&&a(_)?m!==_&&function(n,e,t,r,i){var o,l,d,u=0,f=0,h=e.length-1,m=e[0],v=e[h],_=t.length-1,g=t[0],y=t[_],k=!i;for(0;u<=h&&f<=_;)s(m)?m=e[++u]:s(v)?v=e[--h]:Ir(m,g)?(x(m,g,r,t,f),m=e[++u],g=t[++f]):Ir(v,y)?(x(v,y,r,t,_),v=e[--h],y=t[--_]):Ir(m,y)?(x(m,y,r,t,_),k&&c.insertBefore(n,m.elm,c.nextSibling(v.elm)),m=e[++u],y=t[--_]):Ir(v,g)?(x(v,g,r,t,f),k&&c.insertBefore(n,v.elm,m.elm),v=e[--h],g=t[++f]):(s(o)&&(o=Ar(e,u,h)),s(l=a(g.key)?o[g.key]:w(g,e,u,h))?p(g,r,n,m.elm,!1,t,f):Ir(d=e[l],g)?(x(d,g,r,t,f),e[l]=void 0,k&&c.insertBefore(n,d.elm,m.elm)):p(g,r,n,m.elm,!1,t,f),g=t[++f]);u>h?b(n,s(t[_+1])?null:t[_+1].elm,t,f,_,r):f>_&&E(e,u,h)}(u,m,_,t,d):a(_)?(a(n.text)&&c.setTextContent(u,""),b(u,null,_,0,_.length-1,t)):a(m)?E(m,0,m.length-1):a(n.text)&&c.setTextContent(u,""):n.text!==e.text&&c.setTextContent(u,e.text),a(h)&&a(f=h.hook)&&a(f=f.postpatch)&&f(n,e)}}}function S(n,e,t){if(o(t)&&a(n.parent))n.parent.data.pendingInsert=e;else for(var r=0;r<e.length;++r)e[r].data.hook.insert(e[r])}var I=y("attrs,class,staticClass,staticStyle,key");function A(n,e,t,r){var i,s=e.tag,l=e.data,d=e.children;if(r=r||l&&l.pre,e.elm=n,o(e.isComment)&&a(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(a(l)&&(a(i=l.hook)&&a(i=i.init)&&i(e,!0),a(i=e.componentInstance)))return f(e,t),!0;if(a(s)){if(a(d))if(n.hasChildNodes())if(a(i=l)&&a(i=i.domProps)&&a(i=i.innerHTML)){if(i!==n.innerHTML)return!1}else{for(var c=!0,u=n.firstChild,p=0;p<d.length;p++){if(!u||!A(u,d[p],t,r)){c=!1;break}u=u.nextSibling}if(!c||u)return!1}else m(e,d,t);if(a(l)){var h=!1;for(var v in l)if(!I(v)){h=!0,_(e,t);break}!h&&l.class&&Ke(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,i){if(!s(e)){var l,d=!1,u=[];if(s(n))d=!0,p(e,u);else{var f=a(n.nodeType);if(!f&&Ir(n,e))x(n,e,u,null,null,i);else{if(f){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),o(t)&&A(n,e,u))return S(e,u,!0),n;l=n,n=new mn(c.tagName(l).toLowerCase(),{},[],void 0,l)}var h=n.elm,m=c.parentNode(h);if(p(e,u,h._leaveCb?null:m,c.nextSibling(h)),a(e.parent))for(var _=e.parent,g=v(e);_;){for(var b=0;b<r.destroy.length;++b)r.destroy[b](_);if(_.elm=e.elm,g){for(var y=0;y<r.create.length;++y)r.create[y](xr,_);var R=_.data.hook.insert;if(R.merged)for(var w=R.fns.slice(1),I=0;I<w.length;I++)w[I]()}else Rr(_);_=_.parent}a(m)?E([n],0,0):a(n.tag)&&k(n)}}return S(e,u,d),e.elm}a(n)&&k(n)}}({nodeOps:kr,modules:[Fr,qr,Zr,Jr,di,Y?{create:Ci,activate:Ci,remove:function(n,e){!0!==n.data.show?Oi(n,e):e()}}:{}].concat(Pr)});Q&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&qi(n,"input")}));var Pi={inserted:function(n,e,t,r){"select"===t.tag?(r.elm&&!r.elm._vOptions?Zn(t,"postpatch",(function(){Pi.componentUpdated(n,e,t)})):Di(n,e,t.context),n._vOptions=[].map.call(n.options,Fi)):("textarea"===t.tag||yr(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",Ui),n.addEventListener("compositionend",Mi),n.addEventListener("change",Mi),Q&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){Di(n,e,t.context);var r=n._vOptions,i=n._vOptions=[].map.call(n.options,Fi);if(i.some((function(n,e){return!N(n,r[e])})))(n.multiple?e.value.some((function(n){return Ni(n,i)})):e.value!==e.oldValue&&Ni(e.value,i))&&qi(n,"change")}}};function Di(n,e,t){ji(n,e,t),(J||nn)&&setTimeout((function(){ji(n,e,t)}),0)}function ji(n,e,t){var r=e.value,i=n.multiple;if(!i||Array.isArray(r)){for(var s,a,o=0,l=n.options.length;o<l;o++)if(a=n.options[o],i)s=F(r,Fi(a))>-1,a.selected!==s&&(a.selected=s);else if(N(Fi(a),r))return void(n.selectedIndex!==o&&(n.selectedIndex=o));i||(n.selectedIndex=-1)}}function Ni(n,e){return e.every((function(e){return!N(e,n)}))}function Fi(n){return"_value"in n?n._value:n.value}function Ui(n){n.target.composing=!0}function Mi(n){n.target.composing&&(n.target.composing=!1,qi(n.target,"input"))}function qi(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function $i(n){return!n.componentInstance||n.data&&n.data.transition?n:$i(n.componentInstance._vnode)}var Ki={model:Pi,show:{bind:function(n,e,t){var r=e.value,i=(t=$i(t)).data&&t.data.transition,s=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;r&&i?(t.data.show=!0,Ti(t,(function(){n.style.display=s}))):n.style.display=r?s:"none"},update:function(n,e,t){var r=e.value;!r!=!e.oldValue&&((t=$i(t)).data&&t.data.transition?(t.data.show=!0,r?Ti(t,(function(){n.style.display=n.__vOriginalDisplay})):Oi(t,(function(){n.style.display="none"}))):n.style.display=r?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,r,i){i||(n.style.display=n.__vOriginalDisplay)}}},Gi={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Hi(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?Hi(Se(e.children)):n}function Wi(n){var e={},t=n.$options;for(var r in t.propsData)e[r]=n[r];var i=t._parentListeners;for(var r in i)e[I(r)]=i[r];return e}function Vi(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var Zi=function(n){return n.tag||ve(n)},Yi=function(n){return"show"===n.name},Xi={name:"transition",props:Gi,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(Zi)).length){0;var r=this.mode;0;var i=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return i;var s=Hi(i);if(!s)return i;if(this._leaving)return Vi(n,i);var a="__transition-".concat(this._uid,"-");s.key=null==s.key?s.isComment?a+"comment":a+s.tag:l(s.key)?0===String(s.key).indexOf(a)?s.key:a+s.key:s.key;var o=(s.data||(s.data={})).transition=Wi(this),d=this._vnode,c=Hi(d);if(s.data.directives&&s.data.directives.some(Yi)&&(s.data.show=!0),c&&c.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(s,c)&&!ve(c)&&(!c.componentInstance||!c.componentInstance._vnode.isComment)){var u=c.data.transition=C({},o);if("out-in"===r)return this._leaving=!0,Zn(u,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),Vi(n,i);if("in-out"===r){if(ve(s))return d;var p,f=function(){p()};Zn(o,"afterEnter",f),Zn(o,"enterCancelled",f),Zn(u,"delayLeave",(function(n){p=n}))}}return i}}},Ji=C({tag:String,moveClass:String},Gi);function Qi(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function ns(n){n.data.newPos=n.elm.getBoundingClientRect()}function es(n){var e=n.data.pos,t=n.data.newPos,r=e.left-t.left,i=e.top-t.top;if(r||i){n.data.moved=!0;var s=n.elm.style;s.transform=s.WebkitTransform="translate(".concat(r,"px,").concat(i,"px)"),s.transitionDuration="0s"}}delete Ji.mode;var ts={Transition:Xi,TransitionGroup:{props:Ji,beforeMount:function(){var n=this,e=this._update;this._update=function(t,r){var i=Qe(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,i(),e.call(n,t,r)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),r=this.prevChildren=this.children,i=this.$slots.default||[],s=this.children=[],a=Wi(this),o=0;o<i.length;o++){if((c=i[o]).tag)if(null!=c.key&&0!==String(c.key).indexOf("__vlist"))s.push(c),t[c.key]=c,(c.data||(c.data={})).transition=a;else;}if(r){var l=[],d=[];for(o=0;o<r.length;o++){var c;(c=r[o]).data.transition=a,c.data.pos=c.elm.getBoundingClientRect(),t[c.key]?l.push(c):d.push(c)}this.kept=n(e,null,l),this.removed=d}return n(e,null,s)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Qi),n.forEach(ns),n.forEach(es),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,r=t.style;Ei(t,e),r.transform=r.WebkitTransform=r.transitionDuration="",t.addEventListener(_i,t._moveCb=function n(r){r&&r.target!==t||r&&!/transform$/.test(r.propertyName)||(t.removeEventListener(_i,n),t._moveCb=null,Ri(t,e))})}})))},methods:{hasMove:function(n,e){if(!mi)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){pi(t,n)})),ui(t,e),t.style.display="none",this.$el.appendChild(t);var r=Si(t);return this.$el.removeChild(t),this._hasMove=r.hasTransform}}}};function rs(n,e){for(var t in e)n[t]=e[t];return n}Vt.config.mustUseProp=function(n,e,t){return"value"===t&&rr(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},Vt.config.isReservedTag=gr,Vt.config.isReservedAttr=tr,Vt.config.getTagNamespace=function(n){return _r(n)?"svg":"math"===n?"math":void 0},Vt.config.isUnknownElement=function(n){if(!Y)return!0;if(gr(n))return!1;if(n=n.toLowerCase(),null!=br[n])return br[n];var e=document.createElement(n);return n.indexOf("-")>-1?br[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:br[n]=/HTMLUnknownElement/.test(e.toString())},C(Vt.options.directives,Ki),C(Vt.options.components,ts),Vt.prototype.__patch__=Y?Bi:P,Vt.prototype.$mount=function(n,e){return function(n,e,t){var r;n.$el=e,n.$options.render||(n.$options.render=vn),tt(n,"beforeMount"),r=function(){n._update(n._render(),t)},new We(n,r,P,{before:function(){n._isMounted&&!n._isDestroyed&&tt(n,"beforeUpdate")}},!0),t=!1;var i=n._preWatchers;if(i)for(var s=0;s<i.length;s++)i[s].run();return null==n.$vnode&&(n._isMounted=!0,tt(n,"mounted")),n}(this,n=n&&Y?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},Y&&setTimeout((function(){K.devtools&&dn&&dn.emit("init",Vt)}),0);var is=/[!'()*]/g,ss=function(n){return"%"+n.charCodeAt(0).toString(16)},as=/%2C/g,os=function(n){return encodeURIComponent(n).replace(is,ss).replace(as,",")};function ls(n){try{return decodeURIComponent(n)}catch(n){0}return n}var ds=function(n){return null==n||"object"==typeof n?n:String(n)};function cs(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),r=ls(t.shift()),i=t.length>0?ls(t.join("=")):null;void 0===e[r]?e[r]=i:Array.isArray(e[r])?e[r].push(i):e[r]=[e[r],i]})),e):e}function us(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return os(e);if(Array.isArray(t)){var r=[];return t.forEach((function(n){void 0!==n&&(null===n?r.push(os(e)):r.push(os(e)+"="+os(n)))})),r.join("&")}return os(e)+"="+os(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var ps=/\/?$/;function fs(n,e,t,r){var i=r&&r.options.stringifyQuery,s=e.query||{};try{s=hs(s)}catch(n){}var a={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:s,params:e.params||{},fullPath:_s(e,i),matched:n?vs(n):[]};return t&&(a.redirectedFrom=_s(t,i)),Object.freeze(a)}function hs(n){if(Array.isArray(n))return n.map(hs);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=hs(n[t]);return e}return n}var ms=fs(null,{path:"/"});function vs(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function _s(n,e){var t=n.path,r=n.query;void 0===r&&(r={});var i=n.hash;return void 0===i&&(i=""),(t||"/")+(e||us)(r)+i}function gs(n,e,t){return e===ms?n===e:!!e&&(n.path&&e.path?n.path.replace(ps,"")===e.path.replace(ps,"")&&(t||n.hash===e.hash&&bs(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&bs(n.query,e.query)&&bs(n.params,e.params))))}function bs(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),r=Object.keys(e).sort();return t.length===r.length&&t.every((function(t,i){var s=n[t];if(r[i]!==t)return!1;var a=e[t];return null==s||null==a?s===a:"object"==typeof s&&"object"==typeof a?bs(s,a):String(s)===String(a)}))}function ys(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var r in t.instances){var i=t.instances[r],s=t.enteredCbs[r];if(i&&s){delete t.enteredCbs[r];for(var a=0;a<s.length;a++)i._isBeingDestroyed||s[a](i)}}}}var ks={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,r=e.children,i=e.parent,s=e.data;s.routerView=!0;for(var a=i.$createElement,o=t.name,l=i.$route,d=i._routerViewCache||(i._routerViewCache={}),c=0,u=!1;i&&i._routerRoot!==i;){var p=i.$vnode?i.$vnode.data:{};p.routerView&&c++,p.keepAlive&&i._directInactive&&i._inactive&&(u=!0),i=i.$parent}if(s.routerViewDepth=c,u){var f=d[o],h=f&&f.component;return h?(f.configProps&&Es(h,s,f.route,f.configProps),a(h,s,r)):a()}var m=l.matched[c],v=m&&m.components[o];if(!m||!v)return d[o]=null,a();d[o]={component:v},s.registerRouteInstance=function(n,e){var t=m.instances[o];(e&&t!==n||!e&&t===n)&&(m.instances[o]=e)},(s.hook||(s.hook={})).prepatch=function(n,e){m.instances[o]=e.componentInstance},s.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==m.instances[o]&&(m.instances[o]=n.componentInstance),ys(l)};var _=m.props&&m.props[o];return _&&(rs(d[o],{route:l,configProps:_}),Es(v,s,l,_)),a(v,s,r)}};function Es(n,e,t,r){var i=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,r);if(i){i=e.props=rs({},i);var s=e.attrs=e.attrs||{};for(var a in i)n.props&&a in n.props||(s[a]=i[a],delete i[a])}}function Rs(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var i=e.split("/");t&&i[i.length-1]||i.pop();for(var s=n.replace(/^\//,"").split("/"),a=0;a<s.length;a++){var o=s[a];".."===o?i.pop():"."!==o&&i.push(o)}return""!==i[0]&&i.unshift(""),i.join("/")}function ws(n){return n.replace(/\/(?:\s*\/)+/g,"/")}var xs=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},Ss=Us,Is=Ls,As=function(n,e){return Bs(Ls(n,e),e)},Ts=Bs,Os=Fs,zs=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function Ls(n,e){for(var t,r=[],i=0,s=0,a="",o=e&&e.delimiter||"/";null!=(t=zs.exec(n));){var l=t[0],d=t[1],c=t.index;if(a+=n.slice(s,c),s=c+l.length,d)a+=d[1];else{var u=n[s],p=t[2],f=t[3],h=t[4],m=t[5],v=t[6],_=t[7];a&&(r.push(a),a="");var g=null!=p&&null!=u&&u!==p,b="+"===v||"*"===v,y="?"===v||"*"===v,k=t[2]||o,E=h||m;r.push({name:f||i++,prefix:p||"",delimiter:k,optional:y,repeat:b,partial:g,asterisk:!!_,pattern:E?Ds(E):_?".*":"[^"+Ps(k)+"]+?"})}}return s<n.length&&(a+=n.substr(s)),a&&r.push(a),r}function Cs(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function Bs(n,e){for(var t=new Array(n.length),r=0;r<n.length;r++)"object"==typeof n[r]&&(t[r]=new RegExp("^(?:"+n[r].pattern+")$",Ns(e)));return function(e,r){for(var i="",s=e||{},a=(r||{}).pretty?Cs:encodeURIComponent,o=0;o<n.length;o++){var l=n[o];if("string"!=typeof l){var d,c=s[l.name];if(null==c){if(l.optional){l.partial&&(i+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(xs(c)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(c)+"`");if(0===c.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var u=0;u<c.length;u++){if(d=a(c[u]),!t[o].test(d))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(d)+"`");i+=(0===u?l.prefix:l.delimiter)+d}}else{if(d=l.asterisk?encodeURI(c).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):a(c),!t[o].test(d))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+d+'"');i+=l.prefix+d}}else i+=l}return i}}function Ps(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function Ds(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function js(n,e){return n.keys=e,n}function Ns(n){return n&&n.sensitive?"":"i"}function Fs(n,e,t){xs(e)||(t=e||t,e=[]);for(var r=(t=t||{}).strict,i=!1!==t.end,s="",a=0;a<n.length;a++){var o=n[a];if("string"==typeof o)s+=Ps(o);else{var l=Ps(o.prefix),d="(?:"+o.pattern+")";e.push(o),o.repeat&&(d+="(?:"+l+d+")*"),s+=d=o.optional?o.partial?l+"("+d+")?":"(?:"+l+"("+d+"))?":l+"("+d+")"}}var c=Ps(t.delimiter||"/"),u=s.slice(-c.length)===c;return r||(s=(u?s.slice(0,-c.length):s)+"(?:"+c+"(?=$))?"),s+=i?"$":r&&u?"":"(?="+c+"|$)",js(new RegExp("^"+s,Ns(t)),e)}function Us(n,e,t){return xs(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var r=0;r<t.length;r++)e.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return js(n,e)}(n,e):xs(n)?function(n,e,t){for(var r=[],i=0;i<n.length;i++)r.push(Us(n[i],e,t).source);return js(new RegExp("(?:"+r.join("|")+")",Ns(t)),e)}(n,e,t):function(n,e,t){return Fs(Ls(n,t),e,t)}(n,e,t)}Ss.parse=Is,Ss.compile=As,Ss.tokensToFunction=Ts,Ss.tokensToRegExp=Os;var Ms=Object.create(null);function qs(n,e,t){e=e||{};try{var r=Ms[n]||(Ms[n]=Ss.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),r(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function $s(n,e,t,r){var i="string"==typeof n?{path:n}:n;if(i._normalized)return i;if(i.name){var s=(i=rs({},n)).params;return s&&"object"==typeof s&&(i.params=rs({},s)),i}if(!i.path&&i.params&&e){(i=rs({},i))._normalized=!0;var a=rs(rs({},e.params),i.params);if(e.name)i.name=e.name,i.params=a;else if(e.matched.length){var o=e.matched[e.matched.length-1].path;i.path=qs(o,a,e.path)}else 0;return i}var l=function(n){var e="",t="",r=n.indexOf("#");r>=0&&(e=n.slice(r),n=n.slice(0,r));var i=n.indexOf("?");return i>=0&&(t=n.slice(i+1),n=n.slice(0,i)),{path:n,query:t,hash:e}}(i.path||""),d=e&&e.path||"/",c=l.path?Rs(l.path,d,t||i.append):d,u=function(n,e,t){void 0===e&&(e={});var r,i=t||cs;try{r=i(n||"")}catch(n){r={}}for(var s in e){var a=e[s];r[s]=Array.isArray(a)?a.map(ds):ds(a)}return r}(l.query,i.query,r&&r.options.parseQuery),p=i.hash||l.hash;return p&&"#"!==p.charAt(0)&&(p="#"+p),{_normalized:!0,path:c,query:u,hash:p}}var Ks,Gs=function(){},Hs={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,r=this.$route,i=t.resolve(this.to,r,this.append),s=i.location,a=i.route,o=i.href,l={},d=t.options.linkActiveClass,c=t.options.linkExactActiveClass,u=null==d?"router-link-active":d,p=null==c?"router-link-exact-active":c,f=null==this.activeClass?u:this.activeClass,h=null==this.exactActiveClass?p:this.exactActiveClass,m=a.redirectedFrom?fs(null,$s(a.redirectedFrom),null,t):a;l[h]=gs(r,m,this.exactPath),l[f]=this.exact||this.exactPath?l[h]:function(n,e){return 0===n.path.replace(ps,"/").indexOf(e.path.replace(ps,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(r,m);var v=l[h]?this.ariaCurrentValue:null,_=function(n){Ws(n)&&(e.replace?t.replace(s,Gs):t.push(s,Gs))},g={click:Ws};Array.isArray(this.event)?this.event.forEach((function(n){g[n]=_})):g[this.event]=_;var b={class:l},y=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:o,route:a,navigate:_,isActive:l[f],isExactActive:l[h]});if(y){if(1===y.length)return y[0];if(y.length>1||!y.length)return 0===y.length?n():n("span",{},y)}if("a"===this.tag)b.on=g,b.attrs={href:o,"aria-current":v};else{var k=function n(e){var t;if(e)for(var r=0;r<e.length;r++){if("a"===(t=e[r]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(k){k.isStatic=!1;var E=k.data=rs({},k.data);for(var R in E.on=E.on||{},E.on){var w=E.on[R];R in g&&(E.on[R]=Array.isArray(w)?w:[w])}for(var x in g)x in E.on?E.on[x].push(g[x]):E.on[x]=_;var S=k.data.attrs=rs({},k.data.attrs);S.href=o,S["aria-current"]=v}else b.on=g}return n(this.tag,b,this.$slots.default)}};function Ws(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var Vs="undefined"!=typeof window;function Zs(n,e,t,r,i){var s=e||[],a=t||Object.create(null),o=r||Object.create(null);n.forEach((function(n){!function n(e,t,r,i,s,a){var o=i.path,l=i.name;0;var d=i.pathToRegexpOptions||{},c=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return ws(e.path+"/"+n)}(o,s,d.strict);"boolean"==typeof i.caseSensitive&&(d.sensitive=i.caseSensitive);var u={path:c,regex:Ys(c,d),components:i.components||{default:i.component},alias:i.alias?"string"==typeof i.alias?[i.alias]:i.alias:[],instances:{},enteredCbs:{},name:l,parent:s,matchAs:a,redirect:i.redirect,beforeEnter:i.beforeEnter,meta:i.meta||{},props:null==i.props?{}:i.components?i.props:{default:i.props}};i.children&&i.children.forEach((function(i){var s=a?ws(a+"/"+i.path):void 0;n(e,t,r,i,u,s)}));t[u.path]||(e.push(u.path),t[u.path]=u);if(void 0!==i.alias)for(var p=Array.isArray(i.alias)?i.alias:[i.alias],f=0;f<p.length;++f){0;var h={path:p[f],children:i.children};n(e,t,r,h,s,u.path||"/")}l&&(r[l]||(r[l]=u))}(s,a,o,n,i)}));for(var l=0,d=s.length;l<d;l++)"*"===s[l]&&(s.push(s.splice(l,1)[0]),d--,l--);return{pathList:s,pathMap:a,nameMap:o}}function Ys(n,e){return Ss(n,[],e)}function Xs(n,e){var t=Zs(n),r=t.pathList,i=t.pathMap,s=t.nameMap;function a(n,t,a){var o=$s(n,t,!1,e),d=o.name;if(d){var c=s[d];if(!c)return l(null,o);var u=c.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof o.params&&(o.params={}),t&&"object"==typeof t.params)for(var p in t.params)!(p in o.params)&&u.indexOf(p)>-1&&(o.params[p]=t.params[p]);return o.path=qs(c.path,o.params),l(c,o,a)}if(o.path){o.params={};for(var f=0;f<r.length;f++){var h=r[f],m=i[h];if(Js(m.regex,o.path,o.params))return l(m,o,a)}}return l(null,o)}function o(n,t){var r=n.redirect,i="function"==typeof r?r(fs(n,t,null,e)):r;if("string"==typeof i&&(i={path:i}),!i||"object"!=typeof i)return l(null,t);var o=i,d=o.name,c=o.path,u=t.query,p=t.hash,f=t.params;if(u=o.hasOwnProperty("query")?o.query:u,p=o.hasOwnProperty("hash")?o.hash:p,f=o.hasOwnProperty("params")?o.params:f,d){s[d];return a({_normalized:!0,name:d,query:u,hash:p,params:f},void 0,t)}if(c){var h=function(n,e){return Rs(n,e.parent?e.parent.path:"/",!0)}(c,n);return a({_normalized:!0,path:qs(h,f),query:u,hash:p},void 0,t)}return l(null,t)}function l(n,t,r){return n&&n.redirect?o(n,r||t):n&&n.matchAs?function(n,e,t){var r=a({_normalized:!0,path:qs(t,e.params)});if(r){var i=r.matched,s=i[i.length-1];return e.params=r.params,l(s,e)}return l(null,e)}(0,t,n.matchAs):fs(n,t,r,e)}return{match:a,addRoute:function(n,e){var t="object"!=typeof n?s[n]:void 0;Zs([e||n],r,i,s,t),t&&t.alias.length&&Zs(t.alias.map((function(n){return{path:n,children:[e]}})),r,i,s,t)},getRoutes:function(){return r.map((function(n){return i[n]}))},addRoutes:function(n){Zs(n,r,i,s)}}}function Js(n,e,t){var r=e.match(n);if(!r)return!1;if(!t)return!0;for(var i=1,s=r.length;i<s;++i){var a=n.keys[i-1];a&&(t[a.name||"pathMatch"]="string"==typeof r[i]?ls(r[i]):r[i])}return!0}var Qs=Vs&&window.performance&&window.performance.now?window.performance:Date;function na(){return Qs.now().toFixed(3)}var ea=na();function ta(){return ea}function ra(n){return ea=n}var ia=Object.create(null);function sa(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=rs({},window.history.state);return t.key=ta(),window.history.replaceState(t,"",e),window.addEventListener("popstate",la),function(){window.removeEventListener("popstate",la)}}function aa(n,e,t,r){if(n.app){var i=n.options.scrollBehavior;i&&n.app.$nextTick((function(){var s=function(){var n=ta();if(n)return ia[n]}(),a=i.call(n,e,t,r?s:null);a&&("function"==typeof a.then?a.then((function(n){fa(n,s)})).catch((function(n){0})):fa(a,s))}))}}function oa(){var n=ta();n&&(ia[n]={x:window.pageXOffset,y:window.pageYOffset})}function la(n){oa(),n.state&&n.state.key&&ra(n.state.key)}function da(n){return ua(n.x)||ua(n.y)}function ca(n){return{x:ua(n.x)?n.x:window.pageXOffset,y:ua(n.y)?n.y:window.pageYOffset}}function ua(n){return"number"==typeof n}var pa=/^#\d/;function fa(n,e){var t,r="object"==typeof n;if(r&&"string"==typeof n.selector){var i=pa.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(i){var s=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),r=n.getBoundingClientRect();return{x:r.left-t.left-e.x,y:r.top-t.top-e.y}}(i,s={x:ua((t=s).x)?t.x:0,y:ua(t.y)?t.y:0})}else da(n)&&(e=ca(n))}else r&&da(n)&&(e=ca(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var ha,ma=Vs&&((-1===(ha=window.navigator.userAgent).indexOf("Android 2.")&&-1===ha.indexOf("Android 4.0")||-1===ha.indexOf("Mobile Safari")||-1!==ha.indexOf("Chrome")||-1!==ha.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function va(n,e){oa();var t=window.history;try{if(e){var r=rs({},t.state);r.key=ta(),t.replaceState(r,"",n)}else t.pushState({key:ra(na())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function _a(n){va(n,!0)}var ga={redirected:2,aborted:4,cancelled:8,duplicated:16};function ba(n,e){return ka(n,e,ga.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return Ea.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function ya(n,e){return ka(n,e,ga.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function ka(n,e,t,r){var i=new Error(r);return i._isRouter=!0,i.from=n,i.to=e,i.type=t,i}var Ea=["params","query","hash"];function Ra(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function wa(n,e){return Ra(n)&&n._isRouter&&(null==e||n.type===e)}function xa(n,e,t){var r=function(i){i>=n.length?t():n[i]?e(n[i],(function(){r(i+1)})):r(i+1)};r(0)}function Sa(n){return function(e,t,r){var i=!1,s=0,a=null;Ia(n,(function(n,e,t,o){if("function"==typeof n&&void 0===n.cid){i=!0,s++;var l,d=Oa((function(e){var i;((i=e).__esModule||Ta&&"Module"===i[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:Ks.extend(e),t.components[o]=e,--s<=0&&r()})),c=Oa((function(n){var e="Failed to resolve async component "+o+": "+n;a||(a=Ra(n)?n:new Error(e),r(a))}));try{l=n(d,c)}catch(n){c(n)}if(l)if("function"==typeof l.then)l.then(d,c);else{var u=l.component;u&&"function"==typeof u.then&&u.then(d,c)}}})),i||r()}}function Ia(n,e){return Aa(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function Aa(n){return Array.prototype.concat.apply([],n)}var Ta="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Oa(n){var e=!1;return function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];if(!e)return e=!0,n.apply(this,t)}}var za=function(n,e){this.router=n,this.base=function(n){if(!n)if(Vs){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=ms,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function La(n,e,t,r){var i=Ia(n,(function(n,r,i,s){var a=function(n,e){"function"!=typeof n&&(n=Ks.extend(n));return n.options[e]}(n,e);if(a)return Array.isArray(a)?a.map((function(n){return t(n,r,i,s)})):t(a,r,i,s)}));return Aa(r?i.reverse():i)}function Ca(n,e){if(e)return function(){return n.apply(e,arguments)}}za.prototype.listen=function(n){this.cb=n},za.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},za.prototype.onError=function(n){this.errorCbs.push(n)},za.prototype.transitionTo=function(n,e,t){var r,i=this;try{r=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var s=this.current;this.confirmTransition(r,(function(){i.updateRoute(r),e&&e(r),i.ensureURL(),i.router.afterHooks.forEach((function(n){n&&n(r,s)})),i.ready||(i.ready=!0,i.readyCbs.forEach((function(n){n(r)})))}),(function(n){t&&t(n),n&&!i.ready&&(wa(n,ga.redirected)&&s===ms||(i.ready=!0,i.readyErrorCbs.forEach((function(e){e(n)}))))}))},za.prototype.confirmTransition=function(n,e,t){var r=this,i=this.current;this.pending=n;var s,a,o=function(n){!wa(n)&&Ra(n)&&(r.errorCbs.length?r.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},l=n.matched.length-1,d=i.matched.length-1;if(gs(n,i)&&l===d&&n.matched[l]===i.matched[d])return this.ensureURL(),n.hash&&aa(this.router,i,n,!1),o(((a=ka(s=i,n,ga.duplicated,'Avoided redundant navigation to current location: "'+s.fullPath+'".')).name="NavigationDuplicated",a));var c=function(n,e){var t,r=Math.max(n.length,e.length);for(t=0;t<r&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),u=c.updated,p=c.deactivated,f=c.activated,h=[].concat(function(n){return La(n,"beforeRouteLeave",Ca,!0)}(p),this.router.beforeHooks,function(n){return La(n,"beforeRouteUpdate",Ca)}(u),f.map((function(n){return n.beforeEnter})),Sa(f)),m=function(e,t){if(r.pending!==n)return o(ya(i,n));try{e(n,i,(function(e){!1===e?(r.ensureURL(!0),o(function(n,e){return ka(n,e,ga.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(i,n))):Ra(e)?(r.ensureURL(!0),o(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(o(ba(i,n)),"object"==typeof e&&e.replace?r.replace(e):r.push(e)):t(e)}))}catch(n){o(n)}};xa(h,m,(function(){xa(function(n){return La(n,"beforeRouteEnter",(function(n,e,t,r){return function(n,e,t){return function(r,i,s){return n(r,i,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),s(n)}))}}(n,t,r)}))}(f).concat(r.router.resolveHooks),m,(function(){if(r.pending!==n)return o(ya(i,n));r.pending=null,e(n),r.router.app&&r.router.app.$nextTick((function(){ys(n)}))}))}))},za.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},za.prototype.setupListeners=function(){},za.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=ms,this.pending=null};var Ba=function(n){function e(e,t){n.call(this,e,t),this._startLocation=Pa(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,r=ma&&t;r&&this.listeners.push(sa());var i=function(){var t=n.current,i=Pa(n.base);n.current===ms&&i===n._startLocation||n.transitionTo(i,(function(n){r&&aa(e,n,t,!0)}))};window.addEventListener("popstate",i),this.listeners.push((function(){window.removeEventListener("popstate",i)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var r=this,i=this.current;this.transitionTo(n,(function(n){va(ws(r.base+n.fullPath)),aa(r.router,n,i,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,i=this.current;this.transitionTo(n,(function(n){_a(ws(r.base+n.fullPath)),aa(r.router,n,i,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(Pa(this.base)!==this.current.fullPath){var e=ws(this.base+this.current.fullPath);n?va(e):_a(e)}},e.prototype.getCurrentLocation=function(){return Pa(this.base)},e}(za);function Pa(n){var e=window.location.pathname,t=e.toLowerCase(),r=n.toLowerCase();return!n||t!==r&&0!==t.indexOf(ws(r+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var Da=function(n){function e(e,t,r){n.call(this,e,t),r&&function(n){var e=Pa(n);if(!/^\/#/.test(e))return window.location.replace(ws(n+"/#"+e)),!0}(this.base)||ja()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=ma&&e;t&&this.listeners.push(sa());var r=function(){var e=n.current;ja()&&n.transitionTo(Na(),(function(r){t&&aa(n.router,r,e,!0),ma||Ma(r.fullPath)}))},i=ma?"popstate":"hashchange";window.addEventListener(i,r),this.listeners.push((function(){window.removeEventListener(i,r)}))}},e.prototype.push=function(n,e,t){var r=this,i=this.current;this.transitionTo(n,(function(n){Ua(n.fullPath),aa(r.router,n,i,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,i=this.current;this.transitionTo(n,(function(n){Ma(n.fullPath),aa(r.router,n,i,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;Na()!==e&&(n?Ua(e):Ma(e))},e.prototype.getCurrentLocation=function(){return Na()},e}(za);function ja(){var n=Na();return"/"===n.charAt(0)||(Ma("/"+n),!1)}function Na(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function Fa(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function Ua(n){ma?va(Fa(n)):window.location.hash=n}function Ma(n){ma?_a(Fa(n)):window.location.replace(Fa(n))}var qa=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index+1).concat(n),r.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var r=this.stack[t];this.confirmTransition(r,(function(){var n=e.current;e.index=t,e.updateRoute(r),e.router.afterHooks.forEach((function(e){e&&e(r,n)}))}),(function(n){wa(n,ga.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(za),$a=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Xs(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!ma&&!1!==n.fallback,this.fallback&&(e="hash"),Vs||(e="abstract"),this.mode=e,e){case"history":this.history=new Ba(this,n.base);break;case"hash":this.history=new Da(this,n.base,this.fallback);break;case"abstract":this.history=new qa(this,n.base);break;default:0}},Ka={currentRoute:{configurable:!0}};$a.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},Ka.currentRoute.get=function(){return this.history&&this.history.current},$a.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof Ba||t instanceof Da){var r=function(n){t.setupListeners(),function(n){var r=t.current,i=e.options.scrollBehavior;ma&&i&&"fullPath"in n&&aa(e,n,r,!1)}(n)};t.transitionTo(t.getCurrentLocation(),r,r)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},$a.prototype.beforeEach=function(n){return Ha(this.beforeHooks,n)},$a.prototype.beforeResolve=function(n){return Ha(this.resolveHooks,n)},$a.prototype.afterEach=function(n){return Ha(this.afterHooks,n)},$a.prototype.onReady=function(n,e){this.history.onReady(n,e)},$a.prototype.onError=function(n){this.history.onError(n)},$a.prototype.push=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.push(n,e,t)}));this.history.push(n,e,t)},$a.prototype.replace=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.replace(n,e,t)}));this.history.replace(n,e,t)},$a.prototype.go=function(n){this.history.go(n)},$a.prototype.back=function(){this.go(-1)},$a.prototype.forward=function(){this.go(1)},$a.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},$a.prototype.resolve=function(n,e,t){var r=$s(n,e=e||this.history.current,t,this),i=this.match(r,e),s=i.redirectedFrom||i.fullPath;return{location:r,route:i,href:function(n,e,t){var r="hash"===t?"#"+e:e;return n?ws(n+"/"+r):r}(this.history.base,s,this.mode),normalizedTo:r,resolved:i}},$a.prototype.getRoutes=function(){return this.matcher.getRoutes()},$a.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==ms&&this.history.transitionTo(this.history.getCurrentLocation())},$a.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==ms&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties($a.prototype,Ka);var Ga=$a;function Ha(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}$a.install=function n(e){if(!n.installed||Ks!==e){n.installed=!0,Ks=e;var t=function(n){return void 0!==n},r=function(n,e){var r=n.$options._parentVnode;t(r)&&t(r=r.data)&&t(r=r.registerRouteInstance)&&r(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,r(this,this)},destroyed:function(){r(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",ks),e.component("RouterLink",Hs);var i=e.config.optionMergeStrategies;i.beforeRouteEnter=i.beforeRouteLeave=i.beforeRouteUpdate=i.created}},$a.version="3.6.5",$a.isNavigationFailure=wa,$a.NavigationFailureType=ga,$a.START_LOCATION=ms,Vs&&window.Vue&&window.Vue.use($a);t(108);t(105),t(16);var Wa={NotFound:()=>Promise.all([t.e(0),t.e(6)]).then(t.bind(null,343)),Layout:()=>Promise.all([t.e(0),t.e(2)]).then(t.bind(null,341))},Va={"v-5571592e":()=>t.e(8).then(t.bind(null,345)),"v-9fff2bd8":()=>t.e(9).then(t.bind(null,346)),"v-5f386e4e":()=>t.e(10).then(t.bind(null,347)),"v-7c6bc912":()=>t.e(11).then(t.bind(null,348)),"v-9181d446":()=>t.e(12).then(t.bind(null,349)),"v-bbe08a1e":()=>t.e(13).then(t.bind(null,350)),"v-9cd213c0":()=>t.e(14).then(t.bind(null,351)),"v-01d9b103":()=>t.e(15).then(t.bind(null,352)),"v-5e87f18c":()=>t.e(16).then(t.bind(null,353)),"v-ac71d918":()=>t.e(17).then(t.bind(null,354)),"v-0a2a64b8":()=>t.e(19).then(t.bind(null,355)),"v-103a83f1":()=>t.e(18).then(t.bind(null,356)),"v-18ed35f2":()=>t.e(20).then(t.bind(null,357)),"v-6d051b22":()=>t.e(21).then(t.bind(null,358)),"v-85ed8394":()=>t.e(22).then(t.bind(null,359)),"v-03f67d9c":()=>t.e(24).then(t.bind(null,360)),"v-20acbeab":()=>t.e(23).then(t.bind(null,361)),"v-2e62ec57":()=>t.e(25).then(t.bind(null,362)),"v-c64b35a2":()=>t.e(26).then(t.bind(null,363)),"v-68adaa3b":()=>t.e(27).then(t.bind(null,364)),"v-34254e21":()=>t.e(28).then(t.bind(null,365)),"v-09a4b954":()=>t.e(29).then(t.bind(null,366)),"v-459bc1c5":()=>t.e(32).then(t.bind(null,367)),"v-5f69e48d":()=>t.e(30).then(t.bind(null,368)),"v-4f04879e":()=>t.e(31).then(t.bind(null,369)),"v-21c8ee34":()=>t.e(33).then(t.bind(null,370)),"v-555cf7cc":()=>t.e(35).then(t.bind(null,371)),"v-ded59338":()=>t.e(36).then(t.bind(null,372)),"v-9c3df33c":()=>t.e(37).then(t.bind(null,373)),"v-72e80b6c":()=>t.e(34).then(t.bind(null,374)),"v-61d0fb85":()=>t.e(38).then(t.bind(null,375)),"v-36b789a1":()=>t.e(39).then(t.bind(null,376)),"v-70b3216c":()=>t.e(40).then(t.bind(null,377)),"v-2e4b12ee":()=>t.e(41).then(t.bind(null,378)),"v-715b1431":()=>t.e(42).then(t.bind(null,379)),"v-1cdf4c57":()=>t.e(43).then(t.bind(null,380)),"v-d9d599d2":()=>t.e(44).then(t.bind(null,381)),"v-f7a1c94e":()=>t.e(45).then(t.bind(null,382)),"v-6691f86c":()=>t.e(46).then(t.bind(null,383)),"v-2f9ca694":()=>t.e(47).then(t.bind(null,384)),"v-e7847138":()=>t.e(48).then(t.bind(null,385)),"v-4fd191da":()=>t.e(49).then(t.bind(null,386)),"v-640684d7":()=>t.e(50).then(t.bind(null,387)),"v-443fe0ec":()=>t.e(51).then(t.bind(null,388)),"v-78e6cfd6":()=>t.e(52).then(t.bind(null,389)),"v-525e0584":()=>t.e(53).then(t.bind(null,390)),"v-67439d1c":()=>t.e(54).then(t.bind(null,391)),"v-459c5fdc":()=>t.e(55).then(t.bind(null,392)),"v-1aeed160":()=>t.e(56).then(t.bind(null,393)),"v-2060fe15":()=>t.e(57).then(t.bind(null,394)),"v-0c090e56":()=>t.e(58).then(t.bind(null,395)),"v-00b1d986":()=>t.e(59).then(t.bind(null,396)),"v-1db3f2e7":()=>t.e(60).then(t.bind(null,397)),"v-a11d1cc4":()=>Promise.all([t.e(0),t.e(3)]).then(t.bind(null,398))};function Za(n){const e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}const Ya=/-(\w)/g,Xa=Za(n=>n.replace(Ya,(n,e)=>e?e.toUpperCase():"")),Ja=/\B([A-Z])/g,Qa=Za(n=>n.replace(Ja,"-$1").toLowerCase()),no=Za(n=>n.charAt(0).toUpperCase()+n.slice(1));function eo(n,e){if(!e)return;if(n(e))return n(e);return e.includes("-")?n(no(Xa(e))):n(no(e))||n(Qa(e))}const to=Object.assign({},Wa,Va),ro=n=>to[n],io=n=>Va[n],so=n=>Wa[n],ao=n=>Vt.component(n);function oo(n){return eo(io,n)}function lo(n){return eo(so,n)}function co(n){return eo(ro,n)}function uo(n){return eo(ao,n)}function po(...n){return Promise.all(n.filter(n=>n).map(async n=>{if(!uo(n)&&co(n)){const e=await co(n)();Vt.component(n,e.default)}}))}function fo(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var ho=t(92),mo=t.n(ho),vo=t(93),_o=t.n(vo),go={created(){if(this.siteMeta=this.$site.headTags.filter(([n])=>"meta"===n).map(([n,e])=>e),this.$ssrContext){const e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(n=e)?n.map(n=>{let e="<meta";return Object.keys(n).forEach(t=>{e+=` ${t}="${_o()(n[t])}"`}),e+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=yo(this.$canonicalUrl)}var n},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const n=this.getMergedMetaTags();this.currentMetaTags=ko(n,this.currentMetaTags)},getMergedMetaTags(){const n=this.$page.frontmatter.meta||[];return mo()([{name:"description",content:this.$description}],n,this.siteMeta,Eo)},updateCanonicalLink(){bo(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",yo(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){ko(null,this.currentMetaTags),bo()}};function bo(){const n=document.querySelector("link[rel='canonical']");n&&n.remove()}function yo(n=""){return n?`<link href="${n}" rel="canonical" />`:""}function ko(n,e){if(e&&[...e].filter(n=>n.parentNode===document.head).forEach(n=>document.head.removeChild(n)),n)return n.map(n=>{const e=document.createElement("meta");return Object.keys(n).forEach(t=>{e.setAttribute(t,n[t])}),document.head.appendChild(e),e})}function Eo(n){for(const e of["name","property","itemprop"])if(n.hasOwnProperty(e))return n[e]+e;return JSON.stringify(n)}var Ro=t(51),wo={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(Ro)()((function(){this.setActiveHash()}),300),setActiveHash(){const n=[].slice.call(document.querySelectorAll(".sidebar-link")),e=[].slice.call(document.querySelectorAll(".header-anchor")).filter(e=>n.some(n=>n.hash===e.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),r=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),i=window.innerHeight+t;for(let n=0;n<e.length;n++){const s=e[n],a=e[n+1],o=0===n&&0===t||t>=s.parentElement.offsetTop+10&&(!a||t<a.parentElement.offsetTop-10),l=decodeURIComponent(this.$route.hash);if(o&&l!==decodeURIComponent(s.hash)){const t=s;if(i===r)for(let t=n+1;t<e.length;t++)if(l===decodeURIComponent(e[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},xo=t(24),So=t.n(xo),Io={mounted(){So.a.configure({showSpinner:!1}),this.$router.beforeEach((n,e,t)=>{n.path===e.path||Vt.component(n.name)||So.a.start(),t()}),this.$router.afterEach(()=>{So.a.done(),this.isSidebarOpen=!1})}},Ao=t(94),To={noCopy:!0,noSelect:!1,disabled:!1,minLength:100,authorName:""},Oo={props:{html:String,lang:String},created(){this.authorName="string"==typeof To.authorName?To.authorName:this.getI18nValue(To.authorName),this.text=this.getI18nValue(Ao),this.location=String(location).replace(/#.+$/,"")},methods:{getI18nValue(n){return this.lang in n?n[this.lang]:n["en-US"]}}},zo=t(4),Lo=Object(zo.a)(Oo,(function(){var n=this,e=n._self._c;return e("div",[e("p",[n._v(n._s(n.text.beforeAuthor)+n._s(n.authorName||n.text.author)+n._s(n.text.afterAuthor)),e("a",{attrs:{href:n.location}},[n._v(n._s(decodeURIComponent(n.location)))])]),n._v("\n\n"),e("div",{domProps:{innerHTML:n._s(n.html)}})])}),[],!1,null,null,null).exports,Co={data:()=>({isElement:!1}),created(){this.onCopy=n=>{const e=getSelection().getRangeAt(0);if(String(e).length<this.minLength)return;if(n.preventDefault(),this.noCopy)return;const t=document.createElement("div");t.appendChild(getSelection().getRangeAt(0).cloneContents());const r=this.$lang,i=new Vt({render:n=>n(Lo,{props:{html:t.innerHTML,lang:r}})}).$mount(),{innerHTML:s,innerText:a}=i.$el;n.clipboardData?(n.clipboardData.setData("text/html",s),n.clipboardData.setData("text/plain",a)):window.clipboardData&&window.clipboardData.setData("text",a)}},watch:{isElement(n){if(!n)return;let{copyright:e=!To.disabled}=this.$frontmatter;if(!e)return;"object"!=typeof e&&(e={});const t=e.noSelect||To.noSelect;this.minLength=e.minLength||To.minLength,this.noCopy=e.noCopy||To.noCopy,t?this.$el.style.userSelect="none":this.$el.addEventListener("copy",this.onCopy)}},updated(){this.isElement="#comment"!==this.$el.nodeName},beforeDestory(){this.$el.removeEventListener("copy",this.onCopy)}};t(241),t(242);class Bo{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:n="",duration:e=3e3}){let t=document.createElement("div");t.className="message move-in",t.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${n}</div>\n    `,this.containerEl.appendChild(t),e>0&&setTimeout(()=>{this.close(t)},e)}close(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",()=>{n.remove()})}}var Po={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(n=>{document.querySelectorAll(n).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(n){if(n.classList.contains("codecopy-enabled"))return;const e=document.createElement("i");e.className="code-copy",e.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',e.title="Copy to clipboard",e.addEventListener("click",()=>{this.copyToClipboard(n.innerText)}),n.appendChild(e),n.classList.add("codecopy-enabled")},copyToClipboard(n){const e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);const t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy");(new Bo).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}};!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var r=document.head||document.getElementsByTagName("head")[0],i=document.createElement("style");i.type="text/css","top"===t&&r.firstChild?r.insertBefore(i,r.firstChild):r.appendChild(i),i.styleSheet?i.styleSheet.cssText=n:i.appendChild(document.createTextNode(n))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var Do={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},jo={},No=function(n){return'<div id="app">\n'.concat(n,"\n</div>")},Fo=function(n){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[n]?window.$VUEPRESS_DEMO_BLOCK[n]:Do[n]},Uo=function n(e,t,r){var i=document.createElement(e);return t&&Object.keys(t).forEach((function(n){if(n.indexOf("data"))i[n]=t[n];else{var e=n.replace("data","");i.dataset[e]=t[n]}})),r&&r.forEach((function(e){var t=e.tag,r=e.attrs,s=e.children;i.appendChild(n(t,r,s))})),i},Mo=function(n,e,t){var r,i=(r=n.querySelectorAll(".".concat(e)),Array.prototype.slice.call(r));return 1!==i.length||t?i:i[0]},qo=function(n,e){var t,r,i=n.match(/<style>([\s\S]+)<\/style>/),s=n.match(/<template>([\s\S]+)<\/template>/),a=n.match(/<script>([\s\S]+)<\/script>/),o={css:i&&i[1].replace(/^\n|\n$/g,""),html:s&&s[1].replace(/^\n|\n$/g,""),js:a&&a[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};o.htmlTpl=No(o.html),o.jsTpl=(t=o.js,r=t.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(r,"\n})")),o.script=function(n,e){var t=n.split(/export\s+default/),r="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),i=window.Babel?window.Babel.transform(r,{presets:["es2015"]}).code:r,s=[eval][0](i);return s.template=e,s}(o.js,o.html);var l=Fo("vue");return o.jsLib.unshift(l),o},$o=function(n,e){var t,r=n.match(/<style>([\s\S]+)<\/style>/),i=n.match(/<html>([\s\S]+)<\/html>/),s=n.match(/<script>([\s\S]+)<\/script>/),a={css:r&&r[1].replace(/^\n|\n$/g,""),html:i&&i[1].replace(/^\n|\n$/g,""),js:s&&s[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};return a.htmlTpl=a.html,a.jsTpl=a.js,a.script=(t=a.js,window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t),a},Ko=function(n){return n=n.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),n+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function Go(){var n=Mo(document,"vuepress-plugin-demo-block__wrapper",!0);n.length?n.forEach((function(n){if("true"!==n.dataset.created){n.style.display="block";var e=Mo(n,"vuepress-plugin-demo-block__code"),t=Mo(n,"vuepress-plugin-demo-block__display"),r=Mo(n,"vuepress-plugin-demo-block__footer"),i=Mo(t,"vuepress-plugin-demo-block__app"),s=decodeURIComponent(n.dataset.code),a=decodeURIComponent(n.dataset.config),o=decodeURIComponent(n.dataset.type);a=a?JSON.parse(a):{};var l=e.querySelector("div").clientHeight,d="react"===o?function(n,e){var t=(0,window.Babel.transform)(n,{presets:["es2015","react"]}).code,r="(function(exports){var module={};module.exports=exports;".concat(t,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),i=new Function("return ".concat(r))(),s={js:i,css:i.__style__||"",jsLib:e.jsLib||[],cssLib:e.cssLib||[],jsTpl:Ko(n),htmlTpl:No("")},a=Fo("react"),o=Fo("reactDOM");return s.jsLib.unshift(a,o),s}(s,a):"vanilla"===o?$o(s,a):qo(s,a),c=Uo("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(r.appendChild(c),c.addEventListener("click",Ho.bind(null,c,l,e,r)),Fo("jsfiddle")&&r.appendChild(function(n){var e=n.css,t=n.htmlTpl,r=n.jsTpl,i=n.jsLib,s=n.cssLib,a=i.concat(s).concat(Fo("cssLib")).concat(Fo("jsLib")).join(",");return Uo("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:e}},{tag:"input",attrs:{type:"hidden",name:"html",value:t}},{tag:"input",attrs:{type:"hidden",name:"js",value:r}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:a}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}])}(d)),Fo("codepen")&&r.appendChild(function(n){var e=n.css,t=n.htmlTpl,r=n.jsTpl,i=n.jsLib,s=n.cssLib,a=JSON.stringify({css:e,html:t,js:r,js_external:i.concat(Fo("jsLib")).join(";"),css_external:s.concat(Fo("cssLib")).join(";"),layout:Fo("codepenLayout"),js_pre_processor:Fo("codepenJsProcessor"),editors:Fo("codepenEditors")});return Uo("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:a}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(d)),void 0!==a.horizontal?a.horizontal:Fo("horizontal")){n.classList.add("vuepress-plugin-demo-block__horizontal");var u=e.firstChild.cloneNode(!0);u.classList.add("vuepress-plugin-demo-block__h_code"),t.appendChild(u)}if(d.css&&function(n){if(!jo[n]){var e=Uo("style",{innerHTML:n});document.body.appendChild(e),jo[n]=!0}}(d.css),"react"===o)ReactDOM.render(React.createElement(d.js),i);else if("vue"===o){var p=(new(Vue.extend(d.script))).$mount();i.appendChild(p.$el)}else"vanilla"===o&&(i.innerHTML=d.html,new Function("return (function(){".concat(d.script,"})()"))());n.dataset.created="true"}})):setTimeout((function(n){Go()}),300)}function Ho(n,e,t,r){var i="1"!==n.dataset.isExpand;t.style.height=i?"".concat(e,"px"):0,i?r.classList.add("vuepress-plugin-demo-block__show-link"):r.classList.remove("vuepress-plugin-demo-block__show-link"),n.dataset.isExpand=i?"1":"0"}var Wo={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},Go()},updated:function(){Go()}},Vo="auto",Zo="zoom-in",Yo="zoom-out",Xo="grab",Jo="move";function Qo(n,e,t){var r=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],i={passive:!1};r?n.addEventListener(e,t,i):n.removeEventListener(e,t,i)}function nl(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function el(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function tl(n,e,t){!function(n){var e=rl,t=il;if(n.transition){var r=n.transition;delete n.transition,n[e]=r}if(n.transform){var i=n.transform;delete n.transform,n[t]=i}}(e);var r=n.style,i={};for(var s in e)t&&(i[s]=r[s]||""),r[s]=e[s];return i}var rl="transition",il="transform",sl="transform",al="transitionend";var ol=function(){},ll={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:ol,onClose:ol,onGrab:ol,onMove:ol,onRelease:ol,onBeforeOpen:ol,onBeforeClose:ol,onBeforeGrab:ol,onBeforeRelease:ol,onImageLoading:ol,onImageLoaded:ol},dl={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),ul(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var r=this.lastScrollPosition.x-e,i=this.lastScrollPosition.y-t,s=this.options.scrollThreshold;(Math.abs(i)>=s||Math.abs(r)>=s)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(cl(n)&&!ul(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){cl(n)&&!ul(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,r=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,r)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,r=e.clientY;this.move(t,r)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function cl(n){return 0===n.button}function ul(n){return n.metaKey||n.ctrlKey}var pl={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,tl(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),Qo(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){tl(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},fl="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},hl=function(){function n(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}return function(e,t,r){return t&&n(e.prototype,t),r&&n(e,r),e}}(),ml=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(n[r]=t[r])}return n},vl={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=el(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,r=n.transitionDuration,i=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?Xo:Yo,transition:sl+"\n        "+r+"s\n        "+i,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=tl(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,tl(this.el,{transform:"none"})},grab:function(n,e,t){var r=_l(),i=r.x-n,s=r.y-e;tl(this.el,{cursor:Jo,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+s)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var r=_l(),i=r.x-n,s=r.y-e;tl(this.el,{transition:sl,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+s)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){tl(this.el,this.styleClose)},restoreOpenStyle:function(){tl(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=_l(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,r=this.instance.options,i=r.customSize,s=r.scaleBase;if(!i&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(i&&"object"===(void 0===i?"undefined":fl(i)))return{x:i.width/this.rect.width,y:i.height/this.rect.height};var a=this.rect.width/2,o=this.rect.height/2,l=_l(),d={x:l.x-a,y:l.y-o},c=d.x/a,u=d.y/o,p=s+Math.min(c,u);if(i&&"string"==typeof i){var f=t||this.el.naturalWidth,h=e||this.el.naturalHeight,m=parseFloat(i)*f/(100*this.rect.width),v=parseFloat(i)*h/(100*this.rect.height);if(p>m||p>v)return{x:m,y:v}}return{x:p,y:p}}};function _l(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function gl(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(r){Qo(n,r,e[r],t)}))}var bl=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(vl),this.overlay=Object.create(pl),this.handler=Object.create(dl),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=ml({},ll,e),this.overlay.init(this),this.handler.init(this)}return hl(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=Zo,Qo(n,"click",this.handler.click),this.options.preloadImage&&nl(el(n)));return this}},{key:"config",value:function(n){return n?(ml(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var r="string"==typeof n?document.querySelector(n):n;if("IMG"===r.tagName){if(this.options.onBeforeOpen(r),this.target.init(r,this),!this.options.preloadImage){var i=this.target.srcOriginal;null!=i&&(this.options.onImageLoading(r),nl(i,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Qo(document,"scroll",this.handler.scroll),Qo(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Qo(window,"resize",this.handler.resizeWindow);var s=function n(){Qo(r,al,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&gl(document,e.handler,!0),t(r)};return Qo(r,al,s),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=Vo,this.overlay.fadeOut(),this.target.zoomOut(),Qo(document,"scroll",this.handler.scroll,!1),Qo(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Qo(window,"resize",this.handler.resizeWindow,!1);var r=function r(){Qo(t,al,r,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&gl(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return Qo(t,al,r),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var i=this.target.el;this.options.onBeforeGrab(i),this.released=!1,this.target.grab(n,e,t);var s=function n(){Qo(i,al,n,!1),r(i)};return Qo(i,al,s),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Jo,this.target.move(n,e,t);var i=this.target.el,s=function n(){Qo(i,al,n,!1),r(i)};return Qo(i,al,s),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=Vo,this.target.restoreOpenStyle();var r=function r(){Qo(t,al,r,!1),n.lock=!1,n.released=!0,e(t)};return Qo(t,al,r),this}}}]),n}();const yl=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),kl=Number("500");class El{constructor(){this.instance=new bl(yl)}update(n=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(n)}updateDelay(n=".theme-vdoing-content img:not(.no-zoom)",e=kl){setTimeout(()=>this.update(n),e)}}var Rl=[go,wo,Io,Co,Po,Wo,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new El,this.$vuepress.zooming.updateDelay()}}],wl={name:"GlobalLayout",computed:{layout(){const n=this.getLayout();return fo("layout",n),Vt.component(n)}},methods:{getLayout(){if(this.$page.path){const n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},xl=Object(zo.a)(wl,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),n[e].push(...t);break;default:throw new Error("Unknown option name.")}}(xl,"mixins",Rl);const Sl=[{name:"v-5571592e",path:"/pages/fccd91/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-5571592e").then(t)}},{path:"/pages/fccd91/index.html",redirect:"/pages/fccd91/"},{path:"/01.热门算法/01.热门算法/01.Bloom Filter.html",redirect:"/pages/fccd91/"},{name:"v-9fff2bd8",path:"/pages/1e28a2/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-9fff2bd8").then(t)}},{path:"/pages/1e28a2/index.html",redirect:"/pages/1e28a2/"},{path:"/01.热门算法/01.热门算法/02.Consistent Hashing.html",redirect:"/pages/1e28a2/"},{name:"v-5f386e4e",path:"/pages/8624c5/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-5f386e4e").then(t)}},{path:"/pages/8624c5/index.html",redirect:"/pages/8624c5/"},{path:"/01.热门算法/01.热门算法/03.Count-Min Sketch.html",redirect:"/pages/8624c5/"},{name:"v-7c6bc912",path:"/pages/87589a/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-7c6bc912").then(t)}},{path:"/pages/87589a/index.html",redirect:"/pages/87589a/"},{path:"/01.热门算法/01.热门算法/04.LRU.html",redirect:"/pages/87589a/"},{name:"v-9181d446",path:"/pages/7d22be/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-9181d446").then(t)}},{path:"/pages/7d22be/index.html",redirect:"/pages/7d22be/"},{path:"/01.热门算法/01.热门算法/05.LFU.html",redirect:"/pages/7d22be/"},{name:"v-bbe08a1e",path:"/pages/2d43d1/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-bbe08a1e").then(t)}},{path:"/pages/2d43d1/index.html",redirect:"/pages/2d43d1/"},{path:"/01.热门算法/01.热门算法/06.hash & rehash.html",redirect:"/pages/2d43d1/"},{name:"v-9cd213c0",path:"/pages/44dcc2/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-9cd213c0").then(t)}},{path:"/pages/44dcc2/index.html",redirect:"/pages/44dcc2/"},{path:"/01.热门算法/01.热门算法/10.Timing Wheels.html",redirect:"/pages/44dcc2/"},{name:"v-01d9b103",path:"/pages/b9733b/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-01d9b103").then(t)}},{path:"/pages/b9733b/index.html",redirect:"/pages/b9733b/"},{path:"/02.Kafka  系统设计/01.Kafka 系统设计/01.介绍.html",redirect:"/pages/b9733b/"},{name:"v-5e87f18c",path:"/pages/e21d7f/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-5e87f18c").then(t)}},{path:"/pages/e21d7f/index.html",redirect:"/pages/e21d7f/"},{path:"/02.Kafka  系统设计/01.前言/01.指南.html",redirect:"/pages/e21d7f/"},{name:"v-ac71d918",path:"/pages/4601ca/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-ac71d918").then(t)}},{path:"/pages/4601ca/index.html",redirect:"/pages/4601ca/"},{path:"/03.Nginx 系统设计/01.Nginx 系统设计/01.介绍.html",redirect:"/pages/4601ca/"},{name:"v-0a2a64b8",path:"/pages/84cb49/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-0a2a64b8").then(t)}},{path:"/pages/84cb49/index.html",redirect:"/pages/84cb49/"},{path:"/20.设计基础设施/01.设计基础设施/01.分布式缓存.html",redirect:"/pages/84cb49/"},{name:"v-103a83f1",path:"/pages/52ebd8/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-103a83f1").then(t)}},{path:"/pages/52ebd8/index.html",redirect:"/pages/52ebd8/"},{path:"/06.动态/01.碎碎念.html",redirect:"/pages/52ebd8/"},{name:"v-18ed35f2",path:"/pages/57d5a5/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-18ed35f2").then(t)}},{path:"/pages/57d5a5/index.html",redirect:"/pages/57d5a5/"},{path:"/20.设计基础设施/01.设计基础设施/02.限流器.html",redirect:"/pages/57d5a5/"},{name:"v-6d051b22",path:"/pages/5dcb6b/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-6d051b22").then(t)}},{path:"/pages/5dcb6b/index.html",redirect:"/pages/5dcb6b/"},{path:"/20.设计基础设施/01.设计基础设施/03.热点探查（Top k）.html",redirect:"/pages/5dcb6b/"},{name:"v-85ed8394",path:"/pages/567090/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-85ed8394").then(t)}},{path:"/pages/567090/index.html",redirect:"/pages/567090/"},{path:"/20.设计基础设施/01.设计基础设施/04.消息队列.html",redirect:"/pages/567090/"},{name:"v-03f67d9c",path:"/pages/d81a42/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-03f67d9c").then(t)}},{path:"/pages/d81a42/index.html",redirect:"/pages/d81a42/"},{path:"/20.设计基础设施/01.设计基础设施/06.动态线程池.html",redirect:"/pages/d81a42/"},{name:"v-20acbeab",path:"/pages/8416e6/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-20acbeab").then(t)}},{path:"/pages/8416e6/index.html",redirect:"/pages/8416e6/"},{path:"/20.设计基础设施/01.设计基础设施/05.订阅发布.html",redirect:"/pages/8416e6/"},{name:"v-2e62ec57",path:"/pages/a95d7d/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-2e62ec57").then(t)}},{path:"/pages/a95d7d/index.html",redirect:"/pages/a95d7d/"},{path:"/25.设计热门应用/01.设计热门应用/01.设计 微信.html",redirect:"/pages/a95d7d/"},{name:"v-c64b35a2",path:"/pages/90ad66/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-c64b35a2").then(t)}},{path:"/pages/90ad66/index.html",redirect:"/pages/90ad66/"},{path:"/25.设计热门应用/01.设计热门应用/02.设计Twitter.html",redirect:"/pages/90ad66/"},{name:"v-68adaa3b",path:"/pages/def08a/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-68adaa3b").then(t)}},{path:"/pages/def08a/index.html",redirect:"/pages/def08a/"},{path:"/30.经典场景设计/01.经典场景设计/01.双写一致性.html",redirect:"/pages/def08a/"},{name:"v-34254e21",path:"/pages/1e9e8e/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-34254e21").then(t)}},{path:"/pages/1e9e8e/index.html",redirect:"/pages/1e9e8e/"},{path:"/30.经典场景设计/01.经典场景设计/02.缓存穿透.html",redirect:"/pages/1e9e8e/"},{name:"v-09a4b954",path:"/pages/1d96b2/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-09a4b954").then(t)}},{path:"/pages/1d96b2/index.html",redirect:"/pages/1d96b2/"},{path:"/30.经典场景设计/01.经典场景设计/03.缓存击穿.html",redirect:"/pages/1d96b2/"},{name:"v-459bc1c5",path:"/pages/8a57f2/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-459bc1c5").then(t)}},{path:"/pages/8a57f2/index.html",redirect:"/pages/8a57f2/"},{path:"/30.经典场景设计/01.经典场景设计/06.超卖.html",redirect:"/pages/8a57f2/"},{name:"v-5f69e48d",path:"/pages/24abe0/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-5f69e48d").then(t)}},{path:"/pages/24abe0/index.html",redirect:"/pages/24abe0/"},{path:"/30.经典场景设计/01.经典场景设计/04.任务补偿.html",redirect:"/pages/24abe0/"},{name:"v-4f04879e",path:"/pages/a72629/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-4f04879e").then(t)}},{path:"/pages/a72629/index.html",redirect:"/pages/a72629/"},{path:"/30.经典场景设计/01.经典场景设计/05.秒杀.html",redirect:"/pages/a72629/"},{name:"v-21c8ee34",path:"/pages/51aa8b/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-21c8ee34").then(t)}},{path:"/pages/51aa8b/index.html",redirect:"/pages/51aa8b/"},{path:"/30.经典场景设计/01.经典场景设计/07.多级缓存.html",redirect:"/pages/51aa8b/"},{name:"v-555cf7cc",path:"/pages/4fc8cb/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-555cf7cc").then(t)}},{path:"/pages/4fc8cb/index.html",redirect:"/pages/4fc8cb/"},{path:"/30.经典场景设计/01.经典场景设计/09.幂等&防重.html",redirect:"/pages/4fc8cb/"},{name:"v-ded59338",path:"/pages/f3295f/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-ded59338").then(t)}},{path:"/pages/f3295f/index.html",redirect:"/pages/f3295f/"},{path:"/30.经典场景设计/01.经典场景设计/10.海量数据计数.html",redirect:"/pages/f3295f/"},{name:"v-9c3df33c",path:"/pages/6b9d68/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-9c3df33c").then(t)}},{path:"/pages/6b9d68/index.html",redirect:"/pages/6b9d68/"},{path:"/30.经典场景设计/01.经典场景设计/11.消息未读数系统.html",redirect:"/pages/6b9d68/"},{name:"v-72e80b6c",path:"/pages/0dfb49/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-72e80b6c").then(t)}},{path:"/pages/0dfb49/index.html",redirect:"/pages/0dfb49/"},{path:"/30.经典场景设计/01.经典场景设计/08.超时&重试.html",redirect:"/pages/0dfb49/"},{name:"v-61d0fb85",path:"/archives/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-61d0fb85").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-36b789a1",path:"/pages/252196/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-36b789a1").then(t)}},{path:"/pages/252196/index.html",redirect:"/pages/252196/"},{path:"/Redis 系统设计/01.前言/01.指南.html",redirect:"/pages/252196/"},{name:"v-70b3216c",path:"/pages/69fbd7/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-70b3216c").then(t)}},{path:"/pages/69fbd7/index.html",redirect:"/pages/69fbd7/"},{path:"/Redis 系统设计/01.前言/05.Redis 伪码蓝图【必看】.html",redirect:"/pages/69fbd7/"},{name:"v-2e4b12ee",path:"/pages/bdae41/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-2e4b12ee").then(t)}},{path:"/pages/bdae41/index.html",redirect:"/pages/bdae41/"},{path:"/Redis 系统设计/02.基础/01.String 设计与实现.html",redirect:"/pages/bdae41/"},{name:"v-715b1431",path:"/pages/bd1e41/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-715b1431").then(t)}},{path:"/pages/bd1e41/index.html",redirect:"/pages/bd1e41/"},{path:"/Redis 系统设计/02.基础/02.List 设计与实现.html",redirect:"/pages/bd1e41/"},{name:"v-1cdf4c57",path:"/pages/2d4311/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-1cdf4c57").then(t)}},{path:"/pages/2d4311/index.html",redirect:"/pages/2d4311/"},{path:"/Redis 系统设计/02.基础/05.Hash 设计与实现.html",redirect:"/pages/2d4311/"},{name:"v-d9d599d2",path:"/pages/2d4312/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-d9d599d2").then(t)}},{path:"/pages/2d4312/index.html",redirect:"/pages/2d4312/"},{path:"/Redis 系统设计/02.基础/10.ZSet 设计与实现.html",redirect:"/pages/2d4312/"},{name:"v-f7a1c94e",path:"/pages/34fa27/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-f7a1c94e").then(t)}},{path:"/pages/34fa27/index.html",redirect:"/pages/34fa27/"},{path:"/Redis 系统设计/03.主线/01.Linux 中的 IO 多路复用.html",redirect:"/pages/34fa27/"},{name:"v-6691f86c",path:"/pages/d4ecb9/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-6691f86c").then(t)}},{path:"/pages/d4ecb9/index.html",redirect:"/pages/d4ecb9/"},{path:"/Redis 系统设计/03.主线/03.Redis Server 初始化.html",redirect:"/pages/d4ecb9/"},{name:"v-2f9ca694",path:"/pages/d6b00d/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-2f9ca694").then(t)}},{path:"/pages/d6b00d/index.html",redirect:"/pages/d6b00d/"},{path:"/Redis 系统设计/03.主线/05.Redis 的 Reactor 模型.html",redirect:"/pages/d6b00d/"},{name:"v-e7847138",path:"/pages/264b06/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-e7847138").then(t)}},{path:"/pages/264b06/index.html",redirect:"/pages/264b06/"},{path:"/Redis 系统设计/03.主线/08.深入 Redis 事件驱动框架.html",redirect:"/pages/264b06/"},{name:"v-4fd191da",path:"/pages/e6d8ef/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-4fd191da").then(t)}},{path:"/pages/e6d8ef/index.html",redirect:"/pages/e6d8ef/"},{path:"/Redis 系统设计/03.主线/09.Redis 的执行模式.html",redirect:"/pages/e6d8ef/"},{name:"v-640684d7",path:"/pages/0850b6/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-640684d7").then(t)}},{path:"/pages/0850b6/index.html",redirect:"/pages/0850b6/"},{path:"/Redis 系统设计/03.主线/16.Redis 多IO线程.html",redirect:"/pages/0850b6/"},{name:"v-443fe0ec",path:"/pages/b43a19/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-443fe0ec").then(t)}},{path:"/pages/b43a19/index.html",redirect:"/pages/b43a19/"},{path:"/Redis 系统设计/04.支线/05.LRU 策略.html",redirect:"/pages/b43a19/"},{name:"v-78e6cfd6",path:"/pages/b43a89/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-78e6cfd6").then(t)}},{path:"/pages/b43a89/index.html",redirect:"/pages/b43a89/"},{path:"/Redis 系统设计/04.支线/10.LFU 策略.html",redirect:"/pages/b43a89/"},{name:"v-525e0584",path:"/pages/f44fbe/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-525e0584").then(t)}},{path:"/pages/f44fbe/index.html",redirect:"/pages/f44fbe/"},{path:"/Redis 系统设计/04.支线/13.Redis 过期策略.html",redirect:"/pages/f44fbe/"},{name:"v-67439d1c",path:"/pages/9b17a6/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-67439d1c").then(t)}},{path:"/pages/9b17a6/index.html",redirect:"/pages/9b17a6/"},{path:"/Redis 系统设计/04.支线/15.RDB 持久化.html",redirect:"/pages/9b17a6/"},{name:"v-459c5fdc",path:"/pages/9b17a7/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-459c5fdc").then(t)}},{path:"/pages/9b17a7/index.html",redirect:"/pages/9b17a7/"},{path:"/Redis 系统设计/04.支线/17.AOF 持久化.html",redirect:"/pages/9b17a7/"},{name:"v-1aeed160",path:"/pages/aa75e9/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-1aeed160").then(t)}},{path:"/pages/aa75e9/index.html",redirect:"/pages/aa75e9/"},{path:"/Redis 系统设计/04.支线/20.Redis 中的延迟监控.html",redirect:"/pages/aa75e9/"},{name:"v-2060fe15",path:"/pages/61d908/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-2060fe15").then(t)}},{path:"/pages/61d908/index.html",redirect:"/pages/61d908/"},{path:"/Redis 系统设计/04.支线/25.发布与订阅.html",redirect:"/pages/61d908/"},{name:"v-0c090e56",path:"/pages/ebc8dc/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-0c090e56").then(t)}},{path:"/pages/ebc8dc/index.html",redirect:"/pages/ebc8dc/"},{path:"/Redis 系统设计/05.集群/25.主从复制.html",redirect:"/pages/ebc8dc/"},{name:"v-00b1d986",path:"/pages/af8752/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-00b1d986").then(t)}},{path:"/pages/af8752/index.html",redirect:"/pages/af8752/"},{path:"/Redis 系统设计/05.集群/30.哨兵.html",redirect:"/pages/af8752/"},{name:"v-1db3f2e7",path:"/pages/040403/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-1db3f2e7").then(t)}},{path:"/pages/040403/index.html",redirect:"/pages/040403/"},{path:"/Redis 系统设计/05.集群/35.cluster.html",redirect:"/pages/040403/"},{name:"v-a11d1cc4",path:"/",component:xl,beforeEnter:(n,e,t)=>{po("Layout","v-a11d1cc4").then(t)}},{path:"/index.html",redirect:"/"},{path:"*",component:xl}],Il={title:"Echo 系统设计之美",description:"水滴石穿，设计无银弹",base:"/",headTags:[["link",{rel:"icon",href:"/img/favicon.ico"}],["meta",{name:"theme-color",content:"#11a8cd"}]],pages:[{title:"Bloom Filter",frontmatter:{title:"Bloom Filter",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/fccd91/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.Bloom%20Filter.html",relativePath:"01.热门算法/01.热门算法/01.Bloom Filter.md",key:"v-5571592e",path:"/pages/fccd91/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:292},{level:3,title:"优点",slug:"优点",normalizedTitle:"优点",charIndex:467},{level:3,title:"缺点",slug:"缺点",normalizedTitle:"缺点",charIndex:599},{level:3,title:"使用场景",slug:"使用场景",normalizedTitle:"使用场景",charIndex:655},{level:2,title:"布隆过滤器的原理",slug:"布隆过滤器的原理",normalizedTitle:"布隆过滤器的原理",charIndex:936},{level:3,title:"数据结构",slug:"数据结构",normalizedTitle:"数据结构",charIndex:581},{level:3,title:"空间计算",slug:"空间计算",normalizedTitle:"空间计算",charIndex:1220},{level:3,title:"增加元素",slug:"增加元素",normalizedTitle:"增加元素",charIndex:1232},{level:3,title:"查询元素",slug:"查询元素",normalizedTitle:"查询元素",charIndex:484},{level:3,title:"修改元素",slug:"修改元素",normalizedTitle:"修改元素",charIndex:2138},{level:3,title:"删除元素",slug:"删除元素",normalizedTitle:"删除元素",charIndex:260},{level:2,title:"Redis 中的 布隆过滤器",slug:"redis-中的-布隆过滤器",normalizedTitle:"redis 中的 布隆过滤器",charIndex:2275},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:2294}],headersStr:"前言 优点 缺点 使用场景 布隆过滤器的原理 数据结构 空间计算 增加元素 查询元素 修改元素 删除元素 Redis 中的 布隆过滤器 参考文献",content:'提出问题是一切智慧的开端\n\n 1. 如何在超大规模数据集合中快速判断某个元素是否存在，同时避免耗费大量内存？\n 2. 你知道为什么布隆过滤器可以在不存储元素本身的情况下，提供高效的存在性查询吗？\n 3. 为什么布隆过滤器在一些应用场景中会产生误判，但仍然被广泛使用？\n 4. 布隆过滤器是如何通过节省空间来解决 Redis 缓存穿透问题的？\n 5. 如何根据预期的元素数量和允许的错误率，计算布隆过滤器的最佳大小和哈希函数个数？\n 6. 布隆过滤器在数据查询中如何做到 "一定不存在或可能存在" 的判定？\n 7. 为什么删除元素对于布隆过滤器来说是个挑战，是否有改进的方法？\n\n\n# 前言\n\n布隆过滤器（Bloom Filter）是 1970 年由布隆提出的。它实际上是 一个很长的二进制向量 和 一系列随机映射函数。布隆过滤器可以用于 检索一个元素是否在一个集合中。\n\n如果还是不太好理解的话，就可以把布隆过滤器理解为一个 set 集合，我们可以通过 add 往里面添加元素，通过 contains 来判断是否包含某个元素\n\n\n# 优点\n\n * 时间复杂度低，增加和查询元素的时间复杂为 O(N)，（N 为哈希函数的个数，通常情况比较小）\n * 保密性强，布隆过滤器不存储元素本身\n * 存储空间小，如果允许存在一定的误判，布隆过滤器是非常节省空间的（相比其他数据结构如 Set 集合）\n\n\n# 缺点\n\n * 有点一定的误判率，但是可以通过调整参数来降低\n * 无法获取元素本身\n * 很难删除元素\n\n\n# 使用场景\n\n布隆过滤器可以告诉我们 “某样东西一定不存在或者可能存在”，也就是说布隆过滤器说这个数不存在则一定不存，布隆过滤器说这个数存在则可能不存在（误判，后续会讲），利用这个判断是否存在的特点可以做很多有趣的事情。\n\n * 解决 Redis 缓存穿透问题（面试重点）\n * 邮件过滤，使用布隆过滤器来做邮件黑名单过滤\n * 对爬虫网址进行过滤，爬过的不再爬\n * 解决新闻推荐过的不再推荐(类似抖音刷过的往下滑动不再刷到)\n * HBase RocksDB LevelDB 等数据库内置布隆过滤器，用于判断数据是否存在，可以减少数据库的 IO 请求\n\n\n# 布隆过滤器的原理\n\n\n# 数据结构\n\n布隆过滤器它实际上是 一个很长的二进制向量 和 一系列随机映射函数。以 Redis 中的布隆过滤器实现为例，Redis 中的布隆过滤器底层是一个大型位数组（二进制数组）+多个无偏 hash 函数。\n\n\n\n多个无偏 hash 函数\n\n无偏 hash 函数就是能把元素的 hash 值计算的 比较均匀 的 hash 函数，能使得计算后的元素下标比较均匀的映射到位数组中。能有效减少误差。\n\n如下就是一个简单的布隆过滤器示意图，其中 k1、k2 代表增加的元素，a、b、c 即为无偏 hash 函数，最下层则为二进制数组。\n\n\n\n\n# 空间计算\n\n在布隆过滤器增加元素之前，首先需要初始化布隆过滤器的空间，也就是上面说的二进制数组，除此之外还需要计算无偏 hash 函数的个数。\n\n布隆过滤器提供了两个参数，分别是预计加入元素的大小 n，运行的错误率 p。\n\n布隆过滤器中有算法根据这两个参数会计算出二进制数组的大小 m，以及无偏 hash 函数的个数 k。\n\n它们之间的关系比较简单：\n\n如下地址是一个免费的在线布隆过滤器在线计算的网址：\n\n> https://krisives.github.io/bloom-calculator/\n\n\n\n\n# 增加元素\n\n往布隆过滤器增加元素，添加的 key 需要根据k个无偏 hash 函数计算得到多个 hash 值，然后对数组长度进行取模得到数组下标的位置，然后将对应数组下标的位置的值置为 1\n\n * 通过 k 个无偏 hash 函数计算得到 k 个 hash 值\n * 依次取模数组长度，得到数组索引\n * 将计算得到的数组索引下标位置数据修改为 1\n\n例如，key = Liziba，无偏hash函数的个数k=3，分别为 hash1、hash2、hash3。三个 hash 函数计算后得到三个数组下标值，并将其值修改为 1.\n\n如图所示\n\n\n\n\n# 查询元素\n\n布隆过滤器最大的用处就在于判断某样东西一定不存在或者可能存在，而这个就是查询元素的结果。其查询元素的过程如下：\n\n * 通过 k 个无偏 hash 函数计算得到 k 个 hash 值\n * 依次取模数组长度，得到数组索引\n * 判断索引处的值是否全部为 1，如果全部为 1 则存在（这种存在可能是误判），如果存在一个 0 则必定不存在\n\n关于误判，其实非常好理解，hash 函数再怎么牛逼，也无法完全避免 hash 冲突，也就是说可能会存在多个元素计算的 hash 值是相同的，那么它们取模数组长度后的到的数组索引也是相同的，这就是误判的原因。例如李子捌和李子柒的 hash 值取模后得到的数组索引都是 1，但其实这里只有李子捌，如果此时判断李子柒在不在这里，误判就出现啦！因此布隆过滤器最大的缺点误判只要知道其判断元素是否存在的原理就很容易明白了！\n\n\n# 修改元素\n\n不允许修改\n\n\n# 删除元素\n\n布隆过滤器对元素的删除不太支持，目前有一些变形的特定布隆过滤器支持元素的删除！关于为什么对删除不太支持，其实也非常好理解，hash 冲突必然存在，删除肯定是很苦难的！你将 A 的数组下标置为 0，那可能 B 也为受到影响\n\n\n# Redis 中的 布隆过滤器\n\n\n# 参考文献\n\n布隆(Bloom Filter)过滤器——全面讲解，建议收藏-CSDN 博客',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 如何在超大规模数据集合中快速判断某个元素是否存在，同时避免耗费大量内存？\n 2. 你知道为什么布隆过滤器可以在不存储元素本身的情况下，提供高效的存在性查询吗？\n 3. 为什么布隆过滤器在一些应用场景中会产生误判，但仍然被广泛使用？\n 4. 布隆过滤器是如何通过节省空间来解决 redis 缓存穿透问题的？\n 5. 如何根据预期的元素数量和允许的错误率，计算布隆过滤器的最佳大小和哈希函数个数？\n 6. 布隆过滤器在数据查询中如何做到 "一定不存在或可能存在" 的判定？\n 7. 为什么删除元素对于布隆过滤器来说是个挑战，是否有改进的方法？\n\n\n# 前言\n\n布隆过滤器（bloom filter）是 1970 年由布隆提出的。它实际上是 一个很长的二进制向量 和 一系列随机映射函数。布隆过滤器可以用于 检索一个元素是否在一个集合中。\n\n如果还是不太好理解的话，就可以把布隆过滤器理解为一个 set 集合，我们可以通过 add 往里面添加元素，通过 contains 来判断是否包含某个元素\n\n\n# 优点\n\n * 时间复杂度低，增加和查询元素的时间复杂为 o(n)，（n 为哈希函数的个数，通常情况比较小）\n * 保密性强，布隆过滤器不存储元素本身\n * 存储空间小，如果允许存在一定的误判，布隆过滤器是非常节省空间的（相比其他数据结构如 set 集合）\n\n\n# 缺点\n\n * 有点一定的误判率，但是可以通过调整参数来降低\n * 无法获取元素本身\n * 很难删除元素\n\n\n# 使用场景\n\n布隆过滤器可以告诉我们 “某样东西一定不存在或者可能存在”，也就是说布隆过滤器说这个数不存在则一定不存，布隆过滤器说这个数存在则可能不存在（误判，后续会讲），利用这个判断是否存在的特点可以做很多有趣的事情。\n\n * 解决 redis 缓存穿透问题（面试重点）\n * 邮件过滤，使用布隆过滤器来做邮件黑名单过滤\n * 对爬虫网址进行过滤，爬过的不再爬\n * 解决新闻推荐过的不再推荐(类似抖音刷过的往下滑动不再刷到)\n * hbase rocksdb leveldb 等数据库内置布隆过滤器，用于判断数据是否存在，可以减少数据库的 io 请求\n\n\n# 布隆过滤器的原理\n\n\n# 数据结构\n\n布隆过滤器它实际上是 一个很长的二进制向量 和 一系列随机映射函数。以 redis 中的布隆过滤器实现为例，redis 中的布隆过滤器底层是一个大型位数组（二进制数组）+多个无偏 hash 函数。\n\n\n\n多个无偏 hash 函数\n\n无偏 hash 函数就是能把元素的 hash 值计算的 比较均匀 的 hash 函数，能使得计算后的元素下标比较均匀的映射到位数组中。能有效减少误差。\n\n如下就是一个简单的布隆过滤器示意图，其中 k1、k2 代表增加的元素，a、b、c 即为无偏 hash 函数，最下层则为二进制数组。\n\n\n\n\n# 空间计算\n\n在布隆过滤器增加元素之前，首先需要初始化布隆过滤器的空间，也就是上面说的二进制数组，除此之外还需要计算无偏 hash 函数的个数。\n\n布隆过滤器提供了两个参数，分别是预计加入元素的大小 n，运行的错误率 p。\n\n布隆过滤器中有算法根据这两个参数会计算出二进制数组的大小 m，以及无偏 hash 函数的个数 k。\n\n它们之间的关系比较简单：\n\n如下地址是一个免费的在线布隆过滤器在线计算的网址：\n\n> https://krisives.github.io/bloom-calculator/\n\n\n\n\n# 增加元素\n\n往布隆过滤器增加元素，添加的 key 需要根据k个无偏 hash 函数计算得到多个 hash 值，然后对数组长度进行取模得到数组下标的位置，然后将对应数组下标的位置的值置为 1\n\n * 通过 k 个无偏 hash 函数计算得到 k 个 hash 值\n * 依次取模数组长度，得到数组索引\n * 将计算得到的数组索引下标位置数据修改为 1\n\n例如，key = liziba，无偏hash函数的个数k=3，分别为 hash1、hash2、hash3。三个 hash 函数计算后得到三个数组下标值，并将其值修改为 1.\n\n如图所示\n\n\n\n\n# 查询元素\n\n布隆过滤器最大的用处就在于判断某样东西一定不存在或者可能存在，而这个就是查询元素的结果。其查询元素的过程如下：\n\n * 通过 k 个无偏 hash 函数计算得到 k 个 hash 值\n * 依次取模数组长度，得到数组索引\n * 判断索引处的值是否全部为 1，如果全部为 1 则存在（这种存在可能是误判），如果存在一个 0 则必定不存在\n\n关于误判，其实非常好理解，hash 函数再怎么牛逼，也无法完全避免 hash 冲突，也就是说可能会存在多个元素计算的 hash 值是相同的，那么它们取模数组长度后的到的数组索引也是相同的，这就是误判的原因。例如李子捌和李子柒的 hash 值取模后得到的数组索引都是 1，但其实这里只有李子捌，如果此时判断李子柒在不在这里，误判就出现啦！因此布隆过滤器最大的缺点误判只要知道其判断元素是否存在的原理就很容易明白了！\n\n\n# 修改元素\n\n不允许修改\n\n\n# 删除元素\n\n布隆过滤器对元素的删除不太支持，目前有一些变形的特定布隆过滤器支持元素的删除！关于为什么对删除不太支持，其实也非常好理解，hash 冲突必然存在，删除肯定是很苦难的！你将 a 的数组下标置为 0，那可能 b 也为受到影响\n\n\n# redis 中的 布隆过滤器\n\n\n# 参考文献\n\n布隆(bloom filter)过滤器——全面讲解，建议收藏-csdn 博客',charsets:{cjk:!0},lastUpdated:"2024/09/15, 19:31:25",lastUpdatedTimestamp:1726428685e3},{title:"Consistent Hashing",frontmatter:{title:"Consistent Hashing",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/1e28a2/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/02.Consistent%20Hashing.html",relativePath:"01.热门算法/01.热门算法/02.Consistent Hashing.md",key:"v-9fff2bd8",path:"/pages/1e28a2/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:192},{level:3,title:"普通 hash算法 与 使用场景描述",slug:"普通-hash算法-与-使用场景描述",normalizedTitle:"普通 hash算法 与 使用场景描述",charIndex:199},{level:3,title:"缺陷",slug:"缺陷",normalizedTitle:"缺陷",charIndex:973},{level:2,title:"一致性哈希算法",slug:"一致性哈希算法",normalizedTitle:"一致性哈希算法",charIndex:83},{level:3,title:"什么是 一致性 hash 算法",slug:"什么是-一致性-hash-算法",normalizedTitle:"什么是 一致性 hash 算法",charIndex:1330},{level:3,title:"一致性 hash 算法的优点",slug:"一致性-hash-算法的优点",normalizedTitle:"一致性 hash 算法的优点",charIndex:2252},{level:3,title:"hash 环的倾斜与虚拟节点",slug:"hash-环的倾斜与虚拟节点",normalizedTitle:"hash 环的倾斜与虚拟节点",charIndex:2690},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:3089}],headersStr:"前言 普通 hash算法 与 使用场景描述 缺陷 一致性哈希算法 什么是 一致性 hash 算法 一致性 hash 算法的优点 hash 环的倾斜与虚拟节点 参考文献",content:"提出问题是一切智慧的开端\n\n * 当缓存服务器数量发生变化时，如何避免整个系统的缓存失效？\n * 为什么简单的取模算法在分布式缓存中容易导致缓存雪崩？\n * 如何通过一致性哈希算法，确保数据在新增或移除服务器时，尽量减少缓存的重定位？\n * 如果服务器太少，如何避免缓存数据分布不均，导致某台服务器压力过大？\n * 虚拟节点是如何帮助优化一致性哈希算法，解决数据倾斜问题的？\n\n\n# 前言\n\n\n# 普通 hash算法 与 使用场景描述\n\n在了解一致性哈希算法之前，我们先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。\n\n假设我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为 0号、1号、2号，现在有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存1万张左右的图片，那么我们应该怎样做呢？常见的做法是对缓存项的键进行哈希，将hash后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上\n\n我们举例说明，以刚才描述的场景为例，假设图片名称是不重复的，那我们就可以使用图片名称作为访问图片的key，使用如下公式，计算出图片应该存放在哪台服务器上。\n\nhash(图片名称) % N\n\n当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有3台服务器，使用哈希后的结果对3求余，那么余数一定是0、1或者2；如果求余的结果为0， 就把当前图片缓存在0号服务器上，如果余数为1，就缓存在1号服务器上，以此类推；同理，当我们访问任意图片时，只要再次对图片名称进行上述运算，即可得出图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将3万张图片随机的分布到3台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，我们暂时称上述算法为 HASH 算法或者取模算法，取模算法的过程可以用下图表示：\n\n\n# 缺陷\n\n上述 HASH 算法时，会出现一些缺陷：如果服务器已经不能满足缓存需求，就需要增加服务器数量，假设我们增加了一台缓存服务器，此时如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，最终导致所有缓存的位置都要发生改变，也就是说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据；同理，假设突然有一台缓存服务器出现了故障，那么我们则需要将故障机器移除，那么缓存服务器数量从3台变为2台，同样会导致大量缓存在同一时间失效，造成了缓存的雪崩，后端服务器将会承受巨大的压力，整个系统很有可能被压垮。为了解决这种情况，就有了一致性哈希算法。\n\n\n# 一致性哈希算法\n\n\n# 什么是 一致性 hash 算法\n\n一致性哈希算法也是使用取模的方法，但是取模算法是对服务器的数量进行取模，而一致性哈希算法是对 2^32 取模，具体步骤如下：\n\n 1. 一致性哈希算法将整个哈希值空间按照顺时针方向组织成一个虚拟的圆环，称为 Hash 环；\n 2. 接着将各个服务器使用 Hash 函数进行哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，从而确定每台机器在哈希环上的位置\n 3. 最后使用算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针寻找，第一台遇到的服务器就是其应该定位到的服务器\n\n下面我们使用具体案例说明一下一致性哈希算法的具体流程：\n\n步骤一：哈希环的组织\n\n我们将 2^32 想象成一个圆，像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由**2^32个点**组成的圆，示意图如下：\n\n\n\n圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1,也就是说0点左侧的第一个点代表2^32-1，我们把这个由 2^32 个点组成的圆环称为hash环。\n\n步骤二：确定服务器在哈希环的位置\n\n哈希算法：hash(服务器的IP) % 2^32\n\n上述公式的计算结果一定是 0 到 2^32-1 之间的整数，那么上图中的 hash 环上必定有一个点与这个整数对应，所以我们可以使用这个整数代表服务器，也就是服务器就可以映射到这个环上，假设我们有 ABC 三台服务器，那么它们在哈希环上的示意图如下：\n\n步骤三：将数据映射到哈希环上\n\n我们还是使用图片的名称作为 key，所以我们使用下面算法将图片映射在哈希环上：hash（图片名称） % 2^32，假设我们有4张图片，映射后的示意图如下，其中橘黄色的点表示图片：\n\n\n\n那么，怎么算出上图中的图片应该被缓存到哪一台服务上面呢？我们只要从图片的位置开始，沿顺时针方向遇到的第一个服务器就是图片存放的服务器了。最终，1号、2号图片将会被缓存到服务器A上，3号图片将会被缓存到服务器B上，4号图片将会被缓存到服务器C上。\n\n\n# 一致性 hash 算法的优点\n\n前面提到，如果简单对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，而使用一致性哈希算法就可以很好的解决这个问题，因为一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，只有部分缓存会失效，不至于将所有压力都在同一时间集中到后端服务器上，具有较好的容错性和可扩展性。\n\n假设服务器B出现了故障，需要将服务器B移除，那么移除前后的示意图如下图所示\n\n在服务器B未移除时，图片3应该被缓存到服务器B中，可是当服务器B移除以后，按照之前描述的一致性哈希算法的规则，图片3应该被缓存到服务器C中，因为从图片3的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器C，也就是说，如果服务器B出现故障被移除时，图片3的缓存位置会发生改变，但是，图片4仍然会被缓存到服务器C中，图片1与图片2仍然会被缓存到服务器A中，这与服务器B移除之前并没有任何区别，这就是一致性哈希算法的优点。\n\n\n# hash 环的倾斜与虚拟节点\n\n一致性哈希算法在服务节点太少的情况下，容易因为节点分部不均匀而造成数据倾斜问题，也就是被缓存的对象大部分集中缓存在某一台服务器上，从而出现数据分布不均匀的情况，这种情况就称为 hash 环的倾斜。如下图所示：\n\n\n\n上述左图为理想情况，右图为出现了数据倾斜的情况\n\nhash 环的倾斜在极端情况下，仍然有可能引起系统的崩溃，为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点，一个实际物理节点可以对应多个虚拟节点，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大，hash环倾斜所带来的影响就越小，同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射。具体做法可以在服务器ip或主机名的后面增加编号来实现，加入虚拟节点以后的hash环如下：\n\n\n# 参考文献\n\n一致性哈希算法原理详解-CSDN博客\n\n白话解析：一致性哈希算法 consistent hashing-朱双印博客 (zsythink.net)",normalizedContent:"提出问题是一切智慧的开端\n\n * 当缓存服务器数量发生变化时，如何避免整个系统的缓存失效？\n * 为什么简单的取模算法在分布式缓存中容易导致缓存雪崩？\n * 如何通过一致性哈希算法，确保数据在新增或移除服务器时，尽量减少缓存的重定位？\n * 如果服务器太少，如何避免缓存数据分布不均，导致某台服务器压力过大？\n * 虚拟节点是如何帮助优化一致性哈希算法，解决数据倾斜问题的？\n\n\n# 前言\n\n\n# 普通 hash算法 与 使用场景描述\n\n在了解一致性哈希算法之前，我们先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。\n\n假设我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为 0号、1号、2号，现在有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存1万张左右的图片，那么我们应该怎样做呢？常见的做法是对缓存项的键进行哈希，将hash后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上\n\n我们举例说明，以刚才描述的场景为例，假设图片名称是不重复的，那我们就可以使用图片名称作为访问图片的key，使用如下公式，计算出图片应该存放在哪台服务器上。\n\nhash(图片名称) % n\n\n当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有3台服务器，使用哈希后的结果对3求余，那么余数一定是0、1或者2；如果求余的结果为0， 就把当前图片缓存在0号服务器上，如果余数为1，就缓存在1号服务器上，以此类推；同理，当我们访问任意图片时，只要再次对图片名称进行上述运算，即可得出图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将3万张图片随机的分布到3台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，我们暂时称上述算法为 hash 算法或者取模算法，取模算法的过程可以用下图表示：\n\n\n# 缺陷\n\n上述 hash 算法时，会出现一些缺陷：如果服务器已经不能满足缓存需求，就需要增加服务器数量，假设我们增加了一台缓存服务器，此时如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，最终导致所有缓存的位置都要发生改变，也就是说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据；同理，假设突然有一台缓存服务器出现了故障，那么我们则需要将故障机器移除，那么缓存服务器数量从3台变为2台，同样会导致大量缓存在同一时间失效，造成了缓存的雪崩，后端服务器将会承受巨大的压力，整个系统很有可能被压垮。为了解决这种情况，就有了一致性哈希算法。\n\n\n# 一致性哈希算法\n\n\n# 什么是 一致性 hash 算法\n\n一致性哈希算法也是使用取模的方法，但是取模算法是对服务器的数量进行取模，而一致性哈希算法是对 2^32 取模，具体步骤如下：\n\n 1. 一致性哈希算法将整个哈希值空间按照顺时针方向组织成一个虚拟的圆环，称为 hash 环；\n 2. 接着将各个服务器使用 hash 函数进行哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，从而确定每台机器在哈希环上的位置\n 3. 最后使用算法定位数据访问到相应服务器：将数据key使用相同的函数hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针寻找，第一台遇到的服务器就是其应该定位到的服务器\n\n下面我们使用具体案例说明一下一致性哈希算法的具体流程：\n\n步骤一：哈希环的组织\n\n我们将 2^32 想象成一个圆，像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由**2^32个点**组成的圆，示意图如下：\n\n\n\n圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1,也就是说0点左侧的第一个点代表2^32-1，我们把这个由 2^32 个点组成的圆环称为hash环。\n\n步骤二：确定服务器在哈希环的位置\n\n哈希算法：hash(服务器的ip) % 2^32\n\n上述公式的计算结果一定是 0 到 2^32-1 之间的整数，那么上图中的 hash 环上必定有一个点与这个整数对应，所以我们可以使用这个整数代表服务器，也就是服务器就可以映射到这个环上，假设我们有 abc 三台服务器，那么它们在哈希环上的示意图如下：\n\n步骤三：将数据映射到哈希环上\n\n我们还是使用图片的名称作为 key，所以我们使用下面算法将图片映射在哈希环上：hash（图片名称） % 2^32，假设我们有4张图片，映射后的示意图如下，其中橘黄色的点表示图片：\n\n\n\n那么，怎么算出上图中的图片应该被缓存到哪一台服务上面呢？我们只要从图片的位置开始，沿顺时针方向遇到的第一个服务器就是图片存放的服务器了。最终，1号、2号图片将会被缓存到服务器a上，3号图片将会被缓存到服务器b上，4号图片将会被缓存到服务器c上。\n\n\n# 一致性 hash 算法的优点\n\n前面提到，如果简单对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，而使用一致性哈希算法就可以很好的解决这个问题，因为一致性hash算法对于节点的增减都只需重定位环空间中的一小部分数据，只有部分缓存会失效，不至于将所有压力都在同一时间集中到后端服务器上，具有较好的容错性和可扩展性。\n\n假设服务器b出现了故障，需要将服务器b移除，那么移除前后的示意图如下图所示\n\n在服务器b未移除时，图片3应该被缓存到服务器b中，可是当服务器b移除以后，按照之前描述的一致性哈希算法的规则，图片3应该被缓存到服务器c中，因为从图片3的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器c，也就是说，如果服务器b出现故障被移除时，图片3的缓存位置会发生改变，但是，图片4仍然会被缓存到服务器c中，图片1与图片2仍然会被缓存到服务器a中，这与服务器b移除之前并没有任何区别，这就是一致性哈希算法的优点。\n\n\n# hash 环的倾斜与虚拟节点\n\n一致性哈希算法在服务节点太少的情况下，容易因为节点分部不均匀而造成数据倾斜问题，也就是被缓存的对象大部分集中缓存在某一台服务器上，从而出现数据分布不均匀的情况，这种情况就称为 hash 环的倾斜。如下图所示：\n\n\n\n上述左图为理想情况，右图为出现了数据倾斜的情况\n\nhash 环的倾斜在极端情况下，仍然有可能引起系统的崩溃，为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点，一个实际物理节点可以对应多个虚拟节点，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大，hash环倾斜所带来的影响就越小，同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射。具体做法可以在服务器ip或主机名的后面增加编号来实现，加入虚拟节点以后的hash环如下：\n\n\n# 参考文献\n\n一致性哈希算法原理详解-csdn博客\n\n白话解析：一致性哈希算法 consistent hashing-朱双印博客 (zsythink.net)",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"Count-Min Sketch",frontmatter:{title:"Count-Min Sketch",date:"2024-09-14T13:30:19.000Z",permalink:"/pages/8624c5"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/03.Count-Min%20Sketch.html",relativePath:"01.热门算法/01.热门算法/03.Count-Min Sketch.md",key:"v-5f386e4e",path:"/pages/8624c5/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:389},{level:4,title:"hashmap 解决",slug:"hashmap-解决",normalizedTitle:"hashmap 解决",charIndex:460},{level:2,title:"CMS简介",slug:"cms简介",normalizedTitle:"cms简介",charIndex:1228},{level:4,title:"举个栗子",slug:"举个栗子",normalizedTitle:"举个栗子",charIndex:1419},{level:2,title:"CMS 的具体实现",slug:"cms-的具体实现",normalizedTitle:"cms 的具体实现",charIndex:1864},{level:2,title:"CMS 的参数选择",slug:"cms-的参数选择",normalizedTitle:"cms 的参数选择",charIndex:3294},{level:2,title:"Count-Mean-Min-Sketch",slug:"count-mean-min-sketch",normalizedTitle:"count-mean-min-sketch",charIndex:3546},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:4256}],headersStr:"前言 hashmap 解决 CMS简介 举个栗子 CMS 的具体实现 CMS 的参数选择 Count-Mean-Min-Sketch 参考文献",content:"提出问题是一切智慧的开端\n\n 1. 在面对亿级数据流时，如何在不占用过多内存的情况下，实时统计某个元素的出现频率？\n 2. 为什么在大数据场景下，使用普通的哈希表（HashMap）进行频率统计往往不可行？\n 3. Count-Min Sketch 是如何通过牺牲部分准确性，换取高效的频率查询能力的？\n 4. 你如何在 Count-Min Sketch 中确保低频数据的计数不受哈希冲突影响，保证统计结果的合理性？\n 5. 如何根据允许的误差范围和冲突概率，优化 Count-Min Sketch 的哈希函数数量和内存空间配置？\n 6. 在什么场景下 Count-Mean-Min Sketch 能有效改善 CMS 在处理长尾数据时的准确性？\n 7. 如果你正在构建一个需要实时统计用户访问频率的系统，如何在内存和准确性之间权衡选择 Count-Min Sketch？\n\n\n# 前言\n\n问题： 如果老板让你统计一个实时的数据流中元素出现的频率，并且准备随时回答某个元素出现的频率，不需要的精确的计数，那该怎么办？\n\n# hashmap 解决\n\n在大数据场景下，比如网页的 TopK 问题，爬虫的是否访问过的问题，都是一种出现频次相关的问题，那么在系统设计的时候，如何选择策略和数据结构去存储相关的数据是最高效合适的呢？\n\n计算元素的出现频次，如果出现与普通的场景下，简单的方案就是用 hashmap 来记录元素出现的次数：\n\n// 用HashMap存储元素及其频率\nHashMap<String, Integer> freq = new HashMap<>();\n\n// 统计频率\nfor (String e : elements) {\n    if (!freq.containsKey(e)) {\n    \tfreq.put(e, 1);\n    } else {\n    \tfreq.put(e, freq.get(e) + 1);\n    }\n}\n\n\n但是这种方式在大量数据流的情况下，如果存在大量唯一元素的情况下，会占用大量的内存，导致其无法应对大数据场景，因此在”时间换空间” 的策略选择中，这里就需要考虑通过时间，或者准确率等其他的因素来换空间。\n\n通常来说，针对大数据场景，会无限扩张的数据结构显然是不适用的，所以希望能用固定的空间来进行计数的管理，同时希望尽量不要影响到运行的时间，换言之，可以牺牲掉一定的准确性，来实现节省空间的效果。\n\n基于上述需求，我们可以想到 Hash 算法：将无限大的空间映射到固定的 size 的输出上；而大数据场景下的 Hash 会遇到冲突会被无限放大的问题\n\n如何解决冲突是最核心的问题\n\n * 基于概率数据结构实现的 Bloom Filter 算法采取多 Hash 的方法来减少冲突\n * 而其衍生出来的 CMS 算法以同样的思想，基于不同的设计，更为适应这种计数场景 下面介绍该方法的具体实现\n\n\n# CMS简介\n\nCount-min Sketch 算法是一个可以用来计数的算法，在数据大小非常大时，一种高效的计数算法，通过牺牲准确性提高的效率。\n\n * 是一个概率数据机构\n\n * 算法效率高\n * 提供计数上线\n\n其中，重要参数包括\n\n * Hash 哈希函数的数量： k\n * 计数表格列的数量： m\n * 内存中用空间： k x m x size of counter\n\n# 举个栗子\n\n我们规定一个 m=5，k=3 的Count-min Sketch，用来计数，其中所有hash函数如下\n\n\n\n注意，所有hash函数的结果需 mod m\n\n下面开始填表，首先初始状态为\n\n\n\n首先，向里面添加字母B，其ASCII码为66，求hash函数的结果为\n\n\n\n因此，表格变为\n\n\n\n接下来，我们查询字母A，其ASCII码为65，求hash函数的结果为\n\n\n\n用这个结果去读表，发现其对应位置均为0，因此字母A最多出现0次，这个值是准确的。\n\n然后，我们在查询字母G，其ASCII码为71，求hash函数的结果为\n\n\n\n用这个结果去读表，发现其对应位置均为1，因此字母G最多出现1次；**出错了！**我们从未向里面添加过字母G，这就是一次collision。Count-min Sketch的确会有这种问题，因为这个模型是从Bloom Filter衍生过来的。所以说Count-min Sketch是一个概率模型，返回的结果是一个上限值（upper-bound）。\n\n\n# CMS 的具体实现\n\n首先第一点，通过 hash 来实现数值空间的转换，通过哈希函数 H 将输入元素 x 映射到一维数组上，通过该 index 的值来判断元素的 Count（是否存在）\n\nfor (char x : input_element)\n{\n\tidx = Hash(x);\n\tarray[idx] += 1;\n}\n\n\n实际上这就是 Bloom Filter 的基础思想，然而无论是定长数组的”有限”还是 Hash 函数本身，都需要考虑冲突问题（两个元素被映射到同一个 index 上），冲突会导致 Count 比真实的大。\n\n于是接下来面临的问题就是：**如何降低冲突的概率？**如何提高计数的准确性（实际上也包含在降低冲突的概率中）\n\n可以参考 Bloom Filter 的策略，其通过多个 Hash 函数来映射同一个数，从而来降低元素的冲突概率（未考虑超大数据场景），进而也能提高计数的准确性，那么我们看一下 bloom filter 方法：\n\n> Bloom Filter 算法解决的是存在性问题，因此只需要一个 01 向量，当且仅当所有 Hash 计算出来的 index 的值都为 1 的时候，这个元素才可能存在；\n\n考虑将该方法向 Count 问题上迁移：\n\n * 计数过程中：使用 n 个 Hash 函数计算 idx{1~n} ，然后 vec[idx[i]] += 1 对count+1\n * 查询过程中：使用 n 个 Hash 函数计算 idx{1~n}，然后取 vec[idx[i]] 的最小值，考虑冲突场景可知，这个最小值>=实际的 count。\n\nint query_count = INT_MAX;\nfor (size_t i=0; i < function_size; ++i){\n\tint idx = Hash[i](query);\n\tint tmp_count = count_set[idx];\n\tquery_count = (tmp_count < query_count)? tmp_count: query_count;\n}\n\n\n实际上取多个 hash 的最小值就是 Count-Min Sketch 的核心，但如果仅是如此很明显有个问题，就是多个 hash 函数算出的多个 idx 会进一步的“污染”计数，得不偿失，导致 Count 更加不准确。\n\n实际上很容易想到，为了通过多个 hash 来减少冲突，并使得多 hash 的索引更加的唯一，最好的办法就是使得每个 hash 对应的计数空间是独立的，也就是将我们的计数空间在拓展成二维数组,其 size 为 depth × width 其中 depth 就代表 hash 函数的个数。\n\n那么假设每个 Hash 函数的冲突概率是 p~i~ 那么优化后的冲突概率就从 min(P~i~) 减小到\n\n\n\nfor (size_t i=0; i<function_size; ++i){\n\tint idx = Hash[i](query);\n\tint tmp_count = count_set[i][idx];\n\tquery_count = (tmp_count < query_count)? tmp_count: query_count;\n}\n\n\n结合了这个二维数组就是完整的 CMS 算法了，最终求得的 count 是实际 Count 的近似值（上界）。\n\n\n# CMS 的参数选择\n\n如果确定使用 CMS，接下来面对的就是计数的精度问题，那么如何选择这个数组的 shape 才能尽可能的减少误差呢？（很明显都是越大越好，那么怎么样是最优/达标的呢）\n\n确定一些变量参数：\n\n\n\n设定误差范围：\n\n\n\n以及结果在这个范围内的概率为:\n\n\n\n那么可以计算出：e 是自然常数\n\n\n\n计算公式来自论文，有效性分析也可以从论文中阅读\n\n> 添加一个新哈希函数以指数级别迅速降低超出边界异常数据的概率；当然，增加矩阵的宽度也可以增加减少冲突的概率，但这个只是线性级别。\n\n\n# Count-Mean-Min-Sketch\n\n由于 Hash 的冲突，CMS 对于低频的元素误差还是太大了，引入噪音对于高频元素可以接受（topk）但是对于低频长尾来说太不准确了，因此有了以下的改进：\n\n * 首先按照 CMS 的流程取出 d 个 sketch\n * 对于每个 hash 估计出一个噪音，噪音为该行的所有整数（除了被查询元素）的平均值\n * 该行的 sketch 减去该行的噪音，作为真正的 sketch\n * 返回 d 个 sketch 的中位数\n\nclass CountMeanMinSketch {\n    // initialization and addition procedures as in CountMinSketch\n    // n is total number of added elements\n    long estimateFrequency(value) {\n        long e[] = new long[d]\n        for(i = 0; i < d; i++) {\n            sketchCounter = estimators[i][ hash(value, i) ]\n            noiseEstimation = (n - sketchCounter) / (m - 1)\n            e[i] = sketchCounter – noiseEstimator\n        }\n        return median(e)\n    }\n}\n\n\n该算法显著改善了在长尾数据上的精确度。\n\n\n# 参考文献\n\nCount-min Sketch 算法 - 知乎 (zhihu.com)\n\nCount_Min Sketch算法 - AikenH Blogs",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 在面对亿级数据流时，如何在不占用过多内存的情况下，实时统计某个元素的出现频率？\n 2. 为什么在大数据场景下，使用普通的哈希表（hashmap）进行频率统计往往不可行？\n 3. count-min sketch 是如何通过牺牲部分准确性，换取高效的频率查询能力的？\n 4. 你如何在 count-min sketch 中确保低频数据的计数不受哈希冲突影响，保证统计结果的合理性？\n 5. 如何根据允许的误差范围和冲突概率，优化 count-min sketch 的哈希函数数量和内存空间配置？\n 6. 在什么场景下 count-mean-min sketch 能有效改善 cms 在处理长尾数据时的准确性？\n 7. 如果你正在构建一个需要实时统计用户访问频率的系统，如何在内存和准确性之间权衡选择 count-min sketch？\n\n\n# 前言\n\n问题： 如果老板让你统计一个实时的数据流中元素出现的频率，并且准备随时回答某个元素出现的频率，不需要的精确的计数，那该怎么办？\n\n# hashmap 解决\n\n在大数据场景下，比如网页的 topk 问题，爬虫的是否访问过的问题，都是一种出现频次相关的问题，那么在系统设计的时候，如何选择策略和数据结构去存储相关的数据是最高效合适的呢？\n\n计算元素的出现频次，如果出现与普通的场景下，简单的方案就是用 hashmap 来记录元素出现的次数：\n\n// 用hashmap存储元素及其频率\nhashmap<string, integer> freq = new hashmap<>();\n\n// 统计频率\nfor (string e : elements) {\n    if (!freq.containskey(e)) {\n    \tfreq.put(e, 1);\n    } else {\n    \tfreq.put(e, freq.get(e) + 1);\n    }\n}\n\n\n但是这种方式在大量数据流的情况下，如果存在大量唯一元素的情况下，会占用大量的内存，导致其无法应对大数据场景，因此在”时间换空间” 的策略选择中，这里就需要考虑通过时间，或者准确率等其他的因素来换空间。\n\n通常来说，针对大数据场景，会无限扩张的数据结构显然是不适用的，所以希望能用固定的空间来进行计数的管理，同时希望尽量不要影响到运行的时间，换言之，可以牺牲掉一定的准确性，来实现节省空间的效果。\n\n基于上述需求，我们可以想到 hash 算法：将无限大的空间映射到固定的 size 的输出上；而大数据场景下的 hash 会遇到冲突会被无限放大的问题\n\n如何解决冲突是最核心的问题\n\n * 基于概率数据结构实现的 bloom filter 算法采取多 hash 的方法来减少冲突\n * 而其衍生出来的 cms 算法以同样的思想，基于不同的设计，更为适应这种计数场景 下面介绍该方法的具体实现\n\n\n# cms简介\n\ncount-min sketch 算法是一个可以用来计数的算法，在数据大小非常大时，一种高效的计数算法，通过牺牲准确性提高的效率。\n\n * 是一个概率数据机构\n\n * 算法效率高\n * 提供计数上线\n\n其中，重要参数包括\n\n * hash 哈希函数的数量： k\n * 计数表格列的数量： m\n * 内存中用空间： k x m x size of counter\n\n# 举个栗子\n\n我们规定一个 m=5，k=3 的count-min sketch，用来计数，其中所有hash函数如下\n\n\n\n注意，所有hash函数的结果需 mod m\n\n下面开始填表，首先初始状态为\n\n\n\n首先，向里面添加字母b，其ascii码为66，求hash函数的结果为\n\n\n\n因此，表格变为\n\n\n\n接下来，我们查询字母a，其ascii码为65，求hash函数的结果为\n\n\n\n用这个结果去读表，发现其对应位置均为0，因此字母a最多出现0次，这个值是准确的。\n\n然后，我们在查询字母g，其ascii码为71，求hash函数的结果为\n\n\n\n用这个结果去读表，发现其对应位置均为1，因此字母g最多出现1次；**出错了！**我们从未向里面添加过字母g，这就是一次collision。count-min sketch的确会有这种问题，因为这个模型是从bloom filter衍生过来的。所以说count-min sketch是一个概率模型，返回的结果是一个上限值（upper-bound）。\n\n\n# cms 的具体实现\n\n首先第一点，通过 hash 来实现数值空间的转换，通过哈希函数 h 将输入元素 x 映射到一维数组上，通过该 index 的值来判断元素的 count（是否存在）\n\nfor (char x : input_element)\n{\n\tidx = hash(x);\n\tarray[idx] += 1;\n}\n\n\n实际上这就是 bloom filter 的基础思想，然而无论是定长数组的”有限”还是 hash 函数本身，都需要考虑冲突问题（两个元素被映射到同一个 index 上），冲突会导致 count 比真实的大。\n\n于是接下来面临的问题就是：**如何降低冲突的概率？**如何提高计数的准确性（实际上也包含在降低冲突的概率中）\n\n可以参考 bloom filter 的策略，其通过多个 hash 函数来映射同一个数，从而来降低元素的冲突概率（未考虑超大数据场景），进而也能提高计数的准确性，那么我们看一下 bloom filter 方法：\n\n> bloom filter 算法解决的是存在性问题，因此只需要一个 01 向量，当且仅当所有 hash 计算出来的 index 的值都为 1 的时候，这个元素才可能存在；\n\n考虑将该方法向 count 问题上迁移：\n\n * 计数过程中：使用 n 个 hash 函数计算 idx{1~n} ，然后 vec[idx[i]] += 1 对count+1\n * 查询过程中：使用 n 个 hash 函数计算 idx{1~n}，然后取 vec[idx[i]] 的最小值，考虑冲突场景可知，这个最小值>=实际的 count。\n\nint query_count = int_max;\nfor (size_t i=0; i < function_size; ++i){\n\tint idx = hash[i](query);\n\tint tmp_count = count_set[idx];\n\tquery_count = (tmp_count < query_count)? tmp_count: query_count;\n}\n\n\n实际上取多个 hash 的最小值就是 count-min sketch 的核心，但如果仅是如此很明显有个问题，就是多个 hash 函数算出的多个 idx 会进一步的“污染”计数，得不偿失，导致 count 更加不准确。\n\n实际上很容易想到，为了通过多个 hash 来减少冲突，并使得多 hash 的索引更加的唯一，最好的办法就是使得每个 hash 对应的计数空间是独立的，也就是将我们的计数空间在拓展成二维数组,其 size 为 depth × width 其中 depth 就代表 hash 函数的个数。\n\n那么假设每个 hash 函数的冲突概率是 p~i~ 那么优化后的冲突概率就从 min(p~i~) 减小到\n\n\n\nfor (size_t i=0; i<function_size; ++i){\n\tint idx = hash[i](query);\n\tint tmp_count = count_set[i][idx];\n\tquery_count = (tmp_count < query_count)? tmp_count: query_count;\n}\n\n\n结合了这个二维数组就是完整的 cms 算法了，最终求得的 count 是实际 count 的近似值（上界）。\n\n\n# cms 的参数选择\n\n如果确定使用 cms，接下来面对的就是计数的精度问题，那么如何选择这个数组的 shape 才能尽可能的减少误差呢？（很明显都是越大越好，那么怎么样是最优/达标的呢）\n\n确定一些变量参数：\n\n\n\n设定误差范围：\n\n\n\n以及结果在这个范围内的概率为:\n\n\n\n那么可以计算出：e 是自然常数\n\n\n\n计算公式来自论文，有效性分析也可以从论文中阅读\n\n> 添加一个新哈希函数以指数级别迅速降低超出边界异常数据的概率；当然，增加矩阵的宽度也可以增加减少冲突的概率，但这个只是线性级别。\n\n\n# count-mean-min-sketch\n\n由于 hash 的冲突，cms 对于低频的元素误差还是太大了，引入噪音对于高频元素可以接受（topk）但是对于低频长尾来说太不准确了，因此有了以下的改进：\n\n * 首先按照 cms 的流程取出 d 个 sketch\n * 对于每个 hash 估计出一个噪音，噪音为该行的所有整数（除了被查询元素）的平均值\n * 该行的 sketch 减去该行的噪音，作为真正的 sketch\n * 返回 d 个 sketch 的中位数\n\nclass countmeanminsketch {\n    // initialization and addition procedures as in countminsketch\n    // n is total number of added elements\n    long estimatefrequency(value) {\n        long e[] = new long[d]\n        for(i = 0; i < d; i++) {\n            sketchcounter = estimators[i][ hash(value, i) ]\n            noiseestimation = (n - sketchcounter) / (m - 1)\n            e[i] = sketchcounter – noiseestimator\n        }\n        return median(e)\n    }\n}\n\n\n该算法显著改善了在长尾数据上的精确度。\n\n\n# 参考文献\n\ncount-min sketch 算法 - 知乎 (zhihu.com)\n\ncount_min sketch算法 - aikenh blogs",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"LRU",frontmatter:{title:"LRU",date:"2024-09-14T16:39:57.000Z",permalink:"/pages/87589a"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/04.LRU.html",relativePath:"01.热门算法/01.热门算法/04.LRU.md",key:"v-7c6bc912",path:"/pages/87589a/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:335},{level:2,title:"基于 HashMap 和 双向链表 实现 LRU",slug:"基于-hashmap-和-双向链表-实现-lru",normalizedTitle:"基于 hashmap 和 双向链表 实现 lru",charIndex:764},{level:2,title:"Redis 如何实现 LRU",slug:"redis-如何实现-lru",normalizedTitle:"redis 如何实现 lru",charIndex:3212},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:3245},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:3777}],headersStr:"前言 基于 HashMap 和 双向链表 实现 LRU Redis 如何实现 LRU 总结 参考文献",content:"提出问题是一切智慧的开端\n\n 1. 当系统内存资源不足时，如何有效决定哪些数据应该被优先淘汰？\n 2. 为什么 LRU 算法能够在缓存系统中提供高效的数据置换策略？它的核心原理是什么？\n 3. 基于 HashMap 和双向链表实现的 LRU 缓存机制，是如何确保快速查找和淘汰数据的？\n 4. 如果缓存容量已满，LRU 是如何在插入新数据时，选择最久未使用的数据进行淘汰的？\n 5. 在实际项目中，使用 LRU 缓存时，你该如何选择缓存容量，以及衡量淘汰策略的有效性？\n 6. Redis 为什么没有使用标准的 LRU 算法，而是采用近似 LRU 算法来优化性能？\n 7. 如何通过合理配置 Redis 的内存淘汰策略，确保缓存系统在高并发下的稳定性和高效性？\n\n\n# 前言\n\nLRU 是 Least Recently Used 的缩写，即最近最少使用置换算法，最经典的场景是作为虚拟页式存储管理服务的，是根据页面调入内存后的使用情况进行决策了。由于无法预测各页面将来的使用情况，只能利用“最近的过去”作为“最近的将来”的近似，因此，LRU 算法就是将最近最久未使用的页面予以淘汰。\n\n操作系统课程里有学过，在内存不够的场景下，淘汰旧内容的策略。LRU … Least Recent Used，淘汰掉最不经常使用的。可以稍微多补充两句，因为计算机体系结构中，最大的最可靠的存储是硬盘，它容量很大，并且内容可以固化，但是访问速度很慢，所以需要把使用的内容载入内存中；内存速度很快，但是容量有限，并且断电后内容会丢失，并且为了进一步提升性能，还有 CPU 内部的 L1 Cache，L2 Cache 等概念。因为速度越快的地方，它的单位成本越高，容量越小，新的内容不断被载入，旧的内容肯定要被淘汰，所以就有这样的使用背景。\n\n\n# 基于 HashMap 和 双向链表 实现 LRU\n\n146. LRU 缓存 - 力扣（LeetCode）\n\npublic class LRUCache {\n    class DLinkedNode {\n        int key;\n        int value;\n        DLinkedNode prev;\n        DLinkedNode next;\n        public DLinkedNode() {}\n        public DLinkedNode(int _key, int _value) {key = _key; value = _value;}\n    }\n\n    private Map<Integer, DLinkedNode> cache = new HashMap<Integer, DLinkedNode>();\n    private int size;\n    private int capacity;\n    private DLinkedNode head, tail;\n\n    public LRUCache(int capacity) {\n        this.size = 0;\n        this.capacity = capacity;\n        // 使用伪头部和伪尾部节点\n        head = new DLinkedNode();\n        tail = new DLinkedNode();\n        head.next = tail;\n        tail.prev = head;\n    }\n\n    public int get(int key) {\n        DLinkedNode node = cache.get(key);\n        if (node == null) {\n            return -1;\n        }\n        // 如果 key 存在，先通过哈希表定位，再移到头部\n        moveToHead(node);\n        return node.value;\n    }\n\n    public void put(int key, int value) {\n        DLinkedNode node = cache.get(key);\n        if (node == null) {\n            // 如果 key 不存在，创建一个新的节点\n            DLinkedNode newNode = new DLinkedNode(key, value);\n            // 添加进哈希表\n            cache.put(key, newNode);\n            // 添加至双向链表的头部\n            addToHead(newNode);\n            ++size;\n            if (size > capacity) {\n                // 如果超出容量，删除双向链表的尾部节点\n                DLinkedNode tail = removeTail();\n                // 删除哈希表中对应的项\n                cache.remove(tail.key);\n                --size;\n            }\n        }\n        else {\n            // 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部\n            node.value = value;\n            moveToHead(node);\n        }\n    }\n\n    private void addToHead(DLinkedNode node) {\n        node.prev = head;\n        node.next = head.next;\n        head.next.prev = node;\n        head.next = node;\n    }\n\n    private void removeNode(DLinkedNode node) {\n        node.prev.next = node.next;\n        node.next.prev = node.prev;\n    }\n\n    private void moveToHead(DLinkedNode node) {\n        removeNode(node);\n        addToHead(node);\n    }\n\n    private DLinkedNode removeTail() {\n        DLinkedNode res = tail.prev;\n        removeNode(res);\n        return res;\n    }\n}\n\n\nLRU 算法的执行，可以分成三种情况来掌握\n\n * 当有新数据插入时，LRU 算法会把该数据插入到链表头部，同时把原来链表头部的数据及其之后的数据，都向尾部移动一位。\n * 当有数据刚被访问了一次之后，LRU 算法就会把该数据从它在链表中的当前位置，移动到链表头部。同时，把从链表头部到它当前位置的其他数据，都向尾部移动一位。\n * 情况三：当链表长度无法再容纳更多数据时，若再有新数据插入，LRU 算法就会去除链表尾部的数据，这也相当于将数据从缓存中淘汰掉。\n\n\n# Redis 如何实现 LRU\n\nRedis 中的 LRU\n\n\n# 总结\n\n你现在应该知道了 Redis 是如何实现 LRU 算法来进行缓存数据替换的。其中，我们根据 LRU 算法的基本原理，可以发现如果严格按照原理来实现 LRU 算法，那么开发的系统就需要用额外的内存空间来保存 LRU 链表，而且系统运行时也会受到 LRU 链表操作的开销影响。\n\n而对于 Redis 来说，内存资源和性能都很重要，所以 Redis 实现了近似 LRU 算法。而为了实现近似 LRU 算法，Redis 首先是设置了全局 LRU 时钟，并在键值对创建时获取全局 LRU 时钟值作为访问时间戳，以及在每次访问时获取全局 LRU 时钟值，更新访问时间戳。\n\n然后，当 Redis 每处理一个命令时，都会调用 freeMemoryIfNeeded 函数来判断是否需要释放内存。如果已使用内存超出了 maxmemory，那么，近似 LRU 算法就会随机选择一些键值对，组成待淘汰候选集合，并根据它们的访问时间戳，选出最旧的数据，将其淘汰。\n\nRedis 计算实例内存时，不会把「主从复制」的缓冲区计算在内，也就是说不管一个实例后面挂了多少个从库，主库不会把主从复制所需的「缓冲区」内存，计算到实例内存中，即这部分内存增加，不会对数据淘汰产生影响。\n\n\n# 参考文献\n\nRedis 源码剖析与实战 (geekbang.org)",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 当系统内存资源不足时，如何有效决定哪些数据应该被优先淘汰？\n 2. 为什么 lru 算法能够在缓存系统中提供高效的数据置换策略？它的核心原理是什么？\n 3. 基于 hashmap 和双向链表实现的 lru 缓存机制，是如何确保快速查找和淘汰数据的？\n 4. 如果缓存容量已满，lru 是如何在插入新数据时，选择最久未使用的数据进行淘汰的？\n 5. 在实际项目中，使用 lru 缓存时，你该如何选择缓存容量，以及衡量淘汰策略的有效性？\n 6. redis 为什么没有使用标准的 lru 算法，而是采用近似 lru 算法来优化性能？\n 7. 如何通过合理配置 redis 的内存淘汰策略，确保缓存系统在高并发下的稳定性和高效性？\n\n\n# 前言\n\nlru 是 least recently used 的缩写，即最近最少使用置换算法，最经典的场景是作为虚拟页式存储管理服务的，是根据页面调入内存后的使用情况进行决策了。由于无法预测各页面将来的使用情况，只能利用“最近的过去”作为“最近的将来”的近似，因此，lru 算法就是将最近最久未使用的页面予以淘汰。\n\n操作系统课程里有学过，在内存不够的场景下，淘汰旧内容的策略。lru … least recent used，淘汰掉最不经常使用的。可以稍微多补充两句，因为计算机体系结构中，最大的最可靠的存储是硬盘，它容量很大，并且内容可以固化，但是访问速度很慢，所以需要把使用的内容载入内存中；内存速度很快，但是容量有限，并且断电后内容会丢失，并且为了进一步提升性能，还有 cpu 内部的 l1 cache，l2 cache 等概念。因为速度越快的地方，它的单位成本越高，容量越小，新的内容不断被载入，旧的内容肯定要被淘汰，所以就有这样的使用背景。\n\n\n# 基于 hashmap 和 双向链表 实现 lru\n\n146. lru 缓存 - 力扣（leetcode）\n\npublic class lrucache {\n    class dlinkednode {\n        int key;\n        int value;\n        dlinkednode prev;\n        dlinkednode next;\n        public dlinkednode() {}\n        public dlinkednode(int _key, int _value) {key = _key; value = _value;}\n    }\n\n    private map<integer, dlinkednode> cache = new hashmap<integer, dlinkednode>();\n    private int size;\n    private int capacity;\n    private dlinkednode head, tail;\n\n    public lrucache(int capacity) {\n        this.size = 0;\n        this.capacity = capacity;\n        // 使用伪头部和伪尾部节点\n        head = new dlinkednode();\n        tail = new dlinkednode();\n        head.next = tail;\n        tail.prev = head;\n    }\n\n    public int get(int key) {\n        dlinkednode node = cache.get(key);\n        if (node == null) {\n            return -1;\n        }\n        // 如果 key 存在，先通过哈希表定位，再移到头部\n        movetohead(node);\n        return node.value;\n    }\n\n    public void put(int key, int value) {\n        dlinkednode node = cache.get(key);\n        if (node == null) {\n            // 如果 key 不存在，创建一个新的节点\n            dlinkednode newnode = new dlinkednode(key, value);\n            // 添加进哈希表\n            cache.put(key, newnode);\n            // 添加至双向链表的头部\n            addtohead(newnode);\n            ++size;\n            if (size > capacity) {\n                // 如果超出容量，删除双向链表的尾部节点\n                dlinkednode tail = removetail();\n                // 删除哈希表中对应的项\n                cache.remove(tail.key);\n                --size;\n            }\n        }\n        else {\n            // 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部\n            node.value = value;\n            movetohead(node);\n        }\n    }\n\n    private void addtohead(dlinkednode node) {\n        node.prev = head;\n        node.next = head.next;\n        head.next.prev = node;\n        head.next = node;\n    }\n\n    private void removenode(dlinkednode node) {\n        node.prev.next = node.next;\n        node.next.prev = node.prev;\n    }\n\n    private void movetohead(dlinkednode node) {\n        removenode(node);\n        addtohead(node);\n    }\n\n    private dlinkednode removetail() {\n        dlinkednode res = tail.prev;\n        removenode(res);\n        return res;\n    }\n}\n\n\nlru 算法的执行，可以分成三种情况来掌握\n\n * 当有新数据插入时，lru 算法会把该数据插入到链表头部，同时把原来链表头部的数据及其之后的数据，都向尾部移动一位。\n * 当有数据刚被访问了一次之后，lru 算法就会把该数据从它在链表中的当前位置，移动到链表头部。同时，把从链表头部到它当前位置的其他数据，都向尾部移动一位。\n * 情况三：当链表长度无法再容纳更多数据时，若再有新数据插入，lru 算法就会去除链表尾部的数据，这也相当于将数据从缓存中淘汰掉。\n\n\n# redis 如何实现 lru\n\nredis 中的 lru\n\n\n# 总结\n\n你现在应该知道了 redis 是如何实现 lru 算法来进行缓存数据替换的。其中，我们根据 lru 算法的基本原理，可以发现如果严格按照原理来实现 lru 算法，那么开发的系统就需要用额外的内存空间来保存 lru 链表，而且系统运行时也会受到 lru 链表操作的开销影响。\n\n而对于 redis 来说，内存资源和性能都很重要，所以 redis 实现了近似 lru 算法。而为了实现近似 lru 算法，redis 首先是设置了全局 lru 时钟，并在键值对创建时获取全局 lru 时钟值作为访问时间戳，以及在每次访问时获取全局 lru 时钟值，更新访问时间戳。\n\n然后，当 redis 每处理一个命令时，都会调用 freememoryifneeded 函数来判断是否需要释放内存。如果已使用内存超出了 maxmemory，那么，近似 lru 算法就会随机选择一些键值对，组成待淘汰候选集合，并根据它们的访问时间戳，选出最旧的数据，将其淘汰。\n\nredis 计算实例内存时，不会把「主从复制」的缓冲区计算在内，也就是说不管一个实例后面挂了多少个从库，主库不会把主从复制所需的「缓冲区」内存，计算到实例内存中，即这部分内存增加，不会对数据淘汰产生影响。\n\n\n# 参考文献\n\nredis 源码剖析与实战 (geekbang.org)",charsets:{cjk:!0},lastUpdated:"2024/09/16, 06:30:11",lastUpdatedTimestamp:1726468211e3},{title:"LFU",frontmatter:{title:"LFU",date:"2024-09-14T18:14:42.000Z",permalink:"/pages/7d22be/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/05.LFU.html",relativePath:"01.热门算法/01.热门算法/05.LFU.md",key:"v-9181d446",path:"/pages/7d22be/",headers:[{level:3,title:"Redis 如何实现 LFU",slug:"redis-如何实现-lfu",normalizedTitle:"redis 如何实现 lfu",charIndex:2}],headersStr:"Redis 如何实现 LFU",content:"# Redis 如何实现 LFU\n\nRedis 中的 LFU",normalizedContent:"# redis 如何实现 lfu\n\nredis 中的 lfu",charsets:{cjk:!0},lastUpdated:"2024/09/16, 06:30:11",lastUpdatedTimestamp:1726468211e3},{title:"hash & rehash",frontmatter:{title:"hash & rehash",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d43d1/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/06.hash%20&%20rehash.html",relativePath:"01.热门算法/01.热门算法/06.hash & rehash.md",key:"v-bbe08a1e",path:"/pages/2d43d1/",headersStr:null,content:"Redis 中的 Hash",normalizedContent:"redis 中的 hash",charsets:{cjk:!0},lastUpdated:"2024/09/16, 09:48:07",lastUpdatedTimestamp:1726480087e3},{title:"Timing Wheels",frontmatter:{title:"Timing Wheels",date:"2024-09-15T02:25:42.000Z",permalink:"/pages/44dcc2/"},regularPath:"/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/01.%E7%83%AD%E9%97%A8%E7%AE%97%E6%B3%95/10.Timing%20Wheels.html",relativePath:"01.热门算法/01.热门算法/10.Timing Wheels.md",key:"v-9cd213c0",path:"/pages/44dcc2/",headers:[{level:2,title:"带着疑问",slug:"带着疑问",normalizedTitle:"带着疑问",charIndex:295},{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:1520},{level:2,title:"添加定时任务",slug:"添加定时任务",normalizedTitle:"添加定时任务",charIndex:2073},{level:2,title:'"动态"时间轮',slug:"动态-时间轮",normalizedTitle:"&quot;动态&quot;时间轮",charIndex:null},{level:3,title:"复用时间格",slug:"复用时间格",normalizedTitle:"复用时间格",charIndex:2688},{level:3,title:"时间轮升级",slug:"时间轮升级",normalizedTitle:"时间轮升级",charIndex:2937},{level:3,title:"层级时间轮",slug:"层级时间轮",normalizedTitle:"层级时间轮",charIndex:3225},{level:3,title:"添加定时任务",slug:"添加定时任务-2",normalizedTitle:"添加定时任务",charIndex:2073},{level:3,title:'"动态"层级时间轮',slug:"动态-层级时间轮",normalizedTitle:"&quot;动态&quot;层级时间轮",charIndex:null},{level:3,title:"时间轮降级",slug:"时间轮降级",normalizedTitle:"时间轮降级",charIndex:4189},{level:3,title:"时间轮的推进",slug:"时间轮的推进",normalizedTitle:"时间轮的推进",charIndex:4625},{level:2,title:"时间轮在 Kafka 中的实现",slug:"时间轮在-kafka-中的实现",normalizedTitle:"时间轮在 kafka 中的实现",charIndex:4873},{level:3,title:"时间轮的数据结构",slug:"时间轮的数据结构",normalizedTitle:"时间轮的数据结构",charIndex:5242},{level:3,title:"时间轮中的任务存放",slug:"时间轮中的任务存放",normalizedTitle:"时间轮中的任务存放",charIndex:6065},{level:3,title:"时间轮的升降级",slug:"时间轮的升降级",normalizedTitle:"时间轮的升降级",charIndex:6572},{level:3,title:"任务添加和驱动时间轮滚动核心流程图",slug:"任务添加和驱动时间轮滚动核心流程图",normalizedTitle:"任务添加和驱动时间轮滚动核心流程图",charIndex:7828},{level:3,title:"重点代码介绍",slug:"重点代码介绍",normalizedTitle:"重点代码介绍",charIndex:7852},{level:3,title:"DelayQueue 与 kafka 时间轮",slug:"delayqueue-与-kafka-时间轮",normalizedTitle:"delayqueue 与 kafka 时间轮",charIndex:10994},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:11690},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:11914}],headersStr:'带着疑问 前言 添加定时任务 "动态"时间轮 复用时间格 时间轮升级 层级时间轮 添加定时任务 "动态"层级时间轮 时间轮降级 时间轮的推进 时间轮在 Kafka 中的实现 时间轮的数据结构 时间轮中的任务存放 时间轮的升降级 任务添加和驱动时间轮滚动核心流程图 重点代码介绍 DelayQueue 与 kafka 时间轮 总结 参考文献',content:'提出问题是一切智慧的开端\n\n 1. 如何在高并发场景下高效管理数十万个定时任务，而不会造成性能瓶颈？\n 2. 为什么常规的定时任务扫描方式效率低下，时间轮如何通过环形队列提升任务调度的性能？\n 3. 当定时任务的延迟时间超过了当前时间轮的范围时，时间轮是如何通过层级机制处理这种复杂情况的？\n 4. 如何避免时间轮在处理少量定时任务时的“空推进”问题，从而提升资源利用率？\n 5. Kafka 等高性能系统为什么选择时间轮来管理延迟任务，而不使用JDK自带的Timer或DelayQueue？\n 6. 在一个拥有数百万级别任务的系统中，如何通过多层时间轮实现任务的精确调度和执行？\n\n\n# 带着疑问\n\n第一个问题：如果一台机器上有 10w 个定时任务，如何做到高效触发？\n\n具体场景是：\n\n> 有一个 APP 实时消息通道系统，对每个用户会维护一个 APP 到服务器的 TCP 连接，用来实时收发消息，对这个 TCP 连接，有这样一个需求：“如果连续 30s 没有请求包（例如登录，消息，keepalive 包），服务端就要将这个用户的状态置为离线”。\n> \n> 其中，单机 TCP 同时在线量约在 10w 级别，keepalive 请求包较分散大概 30s 一次，吞吐量约在 3000qps。\n\n怎么做？\n\n常用方案使用 time 定时任务，每秒扫描一次所有连接的集合 Map<uid, last_packet_time>，把连接时间（每次有新的请求更新对应连接的连接时间）比当前时间的差值大 30s 的连接找出来处理。\n\n另一种方案，使用环形队列法：\n\n三个重要的数据结构：\n\n 1. 30s 超时，就创建一个 index 从 0 到 30 的环形队列（本质是个数组）\n 2. 环上每一个 slot 是一个 Set，任务集合\n 3. 同时还有一个 Map<uid, index>，记录 uid 落在环上的哪个 slot 里\n\n这样当有某用户 uid 有请求包到达时：\n\n 1. 从 Map 结构中，查找出这个 uid 存储在哪一个 slot 里\n 2. 从这个 slot 的 Set 结构中，删除这个 uid\n 3. 将 uid 重新加入到新的 slot 中，具体是哪一个 slot 呢 => Current Index 指针所指向的上一个 slot，因为这个 slot，会被 timer 在 30s 之后扫描到\n 4. 更新 Map，这个 uid 对应 slot 的 index 值\n\n哪些元素会被超时掉呢？\n\nCurrent Index 每秒种移动一个 slot，这个 slot 对应的 Set中所有 uid 都应该被集体超时！如果最近 30s 有请求包来到，一定被放到 Current Index 的前一个 slot 了，Current Index 所在的 slot 对应 Set 中所有元素，都是最近 30s 没有请求包来到的。\n\n所以，当没有超时时，Current Index 扫到的每一个 slot 的 Set 中应该都没有元素。\n\n两种方案对比：\n\n方案一每次都要轮询所有数据，而方案二使用环形队列只需要轮询这一刻需要过期的数据，如果没有数据过期则没有数据要处理，并且是批量超时，并且由于是环形结构更加节约空间，这很适合高性能场景。\n\n第二个问题： 在开发过程中有延迟一定时间的任务要执行，怎么做？\n\n如果不重复造轮子的话，我们的选择当然是延迟队列或者 Timer。\n\n延迟队列和在 Timer 中增 加延时任务采用数组表示的最小堆的数据结构实现，每次放入新元素和移除队首元素时间复杂度为 O(nlog(n))。\n\n\n# 前言\n\n时间轮，是一种实现延迟功能（定时器）的巧妙算法，在 Netty，Zookeeper，Kafka 等各种框架中，甚至Linux内核中都有用到。\n\n设计源于生活\n\n时间轮，其设计正是来源于生活中的时钟。\n\n如图就是一个简单的时间轮：\n\n\n\n图中大圆的圆心位置表示的是当前的时间，随着时间推移, 圆心处的时间也会不断跳动。\n\n下面我们对着这个图，来说说Kafka的时间轮TimingWheel。\n\nKafka时间轮的底层就是一个环形数组，而数组中每个元素都存放一个双向链表TimerTaskList，链表中封装了很多延时任务。\n\nKafka中一个时间轮TimingWheel是由20个时间格组成，wheelSize = 20；每格的时间跨度是1ms，tickMs = 1ms。参照Kafka，上图中也用了20个灰边小圆表示时间格，为了动画演示可以看得清楚，我们这里每个小圆的时间跨度是1s。\n\n所以现在整个时间轮的时间跨度就是 tickMs * wheelSize ，也就是 20s。从0s到19s，我们都分别有一个灰边小圆来承载。\n\nKafka的时间轮还有一个表盘指针 currentTime，表示时间轮当前所处的时间。也就是图中用黑色粗线表示的圆，随着时间推移, 这个指针也会不断前进;\n\n\n\n\n# 添加定时任务\n\n有了时间轮，现在可以往里面添加定时任务了。我们用一个粉红色的小圆来表示一个定时任务。\n\n\n\n这里先讲一下设定，每一个定时任务都有延时时间delayTime，和过期时间ExpiredTime。比如当前时间是10s，我们添加了个延时时间为2s的任务，那么这个任务的过期时间就是12s，也就是当前时间10s再走两秒，变成了12s的时候，就到了触发这个定时任务的时间。\n\n而时间轮上代表时间格的灰边小圆上显示的数字，可以理解为任务的过期时间。\n\n\n\n讲清楚这些设定后，我们就开始添加定时任务吧。\n\n初始的时候, 时间轮的指针定格在0。此时添加一个超时时间为2s的任务, 那么这个任务将会插入到第二个时间格中。\n\n\n\n当时间轮的指针到达第二个时间格时, 会处理该时间格上对应的任务。在动画上就是让红色的小圆消失!\n\n\n\n如果这个时候又插入一个延时时间为8s的任务进来, 这个任务的过期时间就是在当前时间2s的基础上加8s, 也就是10s, 那么这个任务将会插入到过期时间为10s的时间格中。\n\n\n\n\n# "动态"时间轮\n\n到目前为止，一切都很好理解。\n\n那么如果在当前时间是2s的时候, 插入一个延时时间为19s的任务时,这个任务的过期时间就是在当前时间2s的基础上加19s, 也就是21s。\n\n请看下图，当前的时间轮是没有过期时间为21s的时间格。这个任务将会插入到过期时间为1s的时间格中，这是怎么回事呢？\n\n\n\n\n# 复用时间格\n\n为了解答上面的问题，我们先来点魔法， 让时间轮上的时间都动起来！\n\n\n\n其实呢，当指针定格在2s的位置时, 时间格0s, 1s和2s就已经是过期的时间格。\n\n也就是说指针可以用来划分过期的时间格[0,2]和未来的时间格 [3,19]。而过期的时间格可以继续复用。比如过期的时间格0s就变成了20s, 存放过期时间为20s的任务。\n\n理解了时间格的复用之后，再看回刚刚的例子，当前时间是2s时，添加延时时间为19s的任务，那么这个任务就会插入到过期时间为21s的时间格中。\n\n\n\n\n# 时间轮升级\n\n下面，新的问题来了，请坐好扶稳。\n\n如果在当前时间是2s的时候, 插入一个延时时间为22s的任务, 这个任务的过期时间就是在2s的基础上加22s，也就是24s。\n\n\n\n显然当前时间轮是无法找到过期时间格为24秒的时间格，因为当前过期时间最大的时间格才到21s。而且我们也没办法像前面那样再复用时间格，因为除了过期时间为2s的时间格，其他的时间格都还没过期呢。当前时间轮无法承载这个定时任务,那么应该怎么办呢?\n\n当然我们可以选择扩展时间轮上的时间格, 但是这样一来，时间轮就失去了意义。\n\n是时候要升级时间轮了！\n\n我们先来理解下多层时间轮之间的联系。\n\n\n# 层级时间轮\n\n如图是一个两层的时间轮:\n\n\n\n第二层时间轮也是由20个时间格组成, 每个时间格的跨度是20s。\n\n图中展示了每个时间格对应的过期时间范围, 我们可以清晰地看到, 第二层时间轮的第0个时间格的过期时间范围是 [0,19]。也就是说, 第二层时间轮的一个时间格就可以表示第一层时间轮的所有(20个)时间格;\n\n为了进一步理清第一层时间轮和第二层时间轮的关系, 我们拉着时间的小手, 一起观看下面的动图:\n\n\n\n可以看到，第二层时间轮同样也有自己的指针, 每当第一层时间轮走完一个周期，第二层时间轮的指针就会推进一格。\n\n\n# 添加定时任务\n\n回到一开始的问题，在当前时间是2s的时候, 插入一个延时时间为22s的任务，该任务过期时间为24s。\n\n\n\n当第一层时间轮容纳不下时，进入第二层时间轮，并插入到过期时间为[20,39]的时间格中。\n\n我们再来个例子，如果在当前时间是2s的时候, 插入一个延时时间为350s的任务, 这个任务的过期时间就是在2s的基础上加350s，也就是352s。\n\n\n\n从图中可以看到，该任务插入到第二层时间轮过期时间为[340,359]s的时间格中，也就是第17格的位置。\n\n\n# "动态"层级时间轮\n\n通常来说, 第二层时间轮的第0个时间格是用来表示第一层时间轮的, 这一格是存放不了任务的, 因为超时时间0-20s的任务, 第一层时间轮就可以处理了。\n\n但是! 事情往往没这么简单, 我们时间轮上的时间格都是可以复用的! 那么这在第二层时间轮上又是怎么体现的呢?\n\n下面是魔法时间， 我们让时间轮上的过期时间都动起来！\n\n\n\n从图中可以看到，当第一层时间轮的指针定格在1s时，超时时间0s的时间格就过期了。而这个时候，第二层时间轮第0个时间格的时间范围就从[0,19]分为了过期的[0],和未过期的[1,19]。而过期的[0]就会被新的过期时间[400]复用。\n\n[0-19]\n\n[400][1,19]\n\n[400,401][2,19]\n\n......\n\n[400,419]\n\n\n所以，如果在当前时间是2s的时候, 插入一个延时时间为399s的任务, 这个任务的过期时间就是在2s的基础上加399s，也就是401s。如图，这个任务还是会插到第二层时间轮第0个时间格中去。\n\n\n\n\n# 时间轮降级\n\n还是用回这个大家都已经耳熟能详的例子，在当前时间是2s的时候, 插入一个延时时间为22s的任务，该任务过期时间为24s。最后进入第二层时间轮，并插入到过期时间为[20,39]的时间格中。\n\n当二层时间轮上的定时任务到期后，时间轮是怎么做的呢？\n\n\n\n从图中可以看到，随着当前时间从2s继续往前推进，一直到20s的时候，总共经过了18s。此时第二层时间轮中，超时时间为[20-39s]的时间格上的任务到期。\n\n原本超时时间为24s的任务会被取出来，重新加入时间轮。此时该定时任务的延时时间从原本的22s，到现在还剩下4s（22s-18s）。最后停留在第一层时间轮超时时间为24s的时间格，也就是第4个时间格。\n\n随着当前时间继续推进，再经过4s后，该定时任务到期被执行。\n\n从这里可以看出时间轮的巧妙之处，两层时间轮只用了40个数组元素，却可以承载[0-399s]的定时任务。而三层时间轮用60个数组元素，就可以承载[0-7999s]的定时任务！\n\n\n\n\n# 时间轮的推进\n\n从动画中可以注意到, 随着时间推进, 时间轮的指针循环往复地定格在每一个时间格上, 每一次都要判断当前定格的时间格里是不是有任务存在;\n\n其中有很多时间格都是没有任务的, 指针定格在这种空的时间格中, 就是一次"空推进";\n\n比如说, 插入一个延时时间400s的任务, 指针就要执行399次"空推进", 这是一种浪费!\n\n那么Kafka是怎么解决这个问题的呢？这就要从延迟队列DelayQueue开始讲起了！时间轮搭配延迟队列DelayQueue，会发生什么化学反应呢？\n\n\n# 时间轮在 Kafka 中的实现\n\n方案二所采用的环形队列，就是时间轮的底层数据结构，它能够让需要处理的数据（任务的抽象）集中，在 Kafka 中存在大量的延迟操作，比如延迟生产、延迟拉取以及延迟删除等。Kafka 并没有使用 JDK 自带的 Timer 或者 DelayQueue 来实现延迟的功能，而是基于时间轮自定义了一个用于实现延迟功能的定时器（SystemTimer）。JDK 的 Timer 和 DelayQueue 插入和删除操作的平均时间复杂度为 O(nlog(n))，并不能满足 Kafka 的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为 O(1)。时间轮的应用并非 Kafka 独有，其应用场景还有很多，在 Netty、Akka、Quartz、Zookeeper 等组件中都存在时间轮的踪影。\n\n\n# 时间轮的数据结构\n\n参考下图，Kafka 中的时间轮（TimingWheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表（TimerTaskList）。TimerTaskList 是一个环形的双向链表，链表中的每一项表示的都是定时任务项（TimerTaskEntry），其中封装了真正的定时任务 TimerTask。在 Kafka 源码中对这个 TimeTaskList 是用一个名称为 buckets 的数组表示的，所以后面介绍中可能 TimerTaskList 也会被称为 bucket。\n\n\n\n针对上图的几个名词简单解释下：\n\n * tickMs： 时间轮由多个时间格组成，每个时间格就是 tickMs，它代表当前时间轮的基本时间跨度。\n * wheelSize： 代表每一层时间轮的格数\n * interval： 当前时间轮的总体时间跨度，interval=tickMs × wheelSize\n * startMs： 构造当层时间轮时候的当前时间，第一层的时间轮的 startMs 是 TimeUnit.NANOSECONDS.toMillis(nanoseconds()),上层时间轮的 startMs 为下层时间轮的 currentTime。\n * currentTime： 表示时间轮当前所处的时间，currentTime 是 tickMs 的整数倍（通过 currentTime=startMs - (startMs % tickMs 来保正 currentTime 一定是 tickMs 的整数倍），这个运算类比钟表中分钟里 65 秒分钟指针指向的还是 1 分钟）。currentTime 可以将整个时间轮划分为到期部分和未到期部分，currentTime 当前指向的时间格也属于到期部分，表示刚好到期，需要处理此时间格所对应的 TimerTaskList 的所有任务。\n\n\n# 时间轮中的任务存放\n\n若时间轮的 tickMs=1ms，wheelSize=20，那么可以计算得出 interval 为 20ms。初始情况下表盘指针 currentTime 指向时间格 0，此时有一个定时为 2ms 的任务插入进来会存放到时间格为 2 的 TimerTaskList 中。随着时间的不断推移，指针 currentTime 不断向前推进，过了 2ms 之后，当到达时间格 2 时，就需要将时间格 2 所对应的 TimeTaskList 中的任务做相应的到期操作。此时若又有一个定时为 8ms 的任务插入进来，则会存放到时间格 10 中，currentTime 再过 8ms 后会指向时间格 10。如果同时有一个定时为 19ms 的任务插入进来怎么办？新来的 TimerTaskEntry 会复用原来的 TimerTaskList，所以它会插入到原本已经到期的时间格 1 中。总之，整个时间轮的总体跨度是不变的，随着指针 currentTime 的不断推进，当前时间轮所能处理的时间段也在不断后移，总体时间范围在 currentTime 和 currentTime+interval 之间。\n\n\n# 时间轮的升降级\n\n如果此时有个定时为 350ms 的任务该如何处理？直接扩充 wheelSize 的大小么？Kafka 中不乏几万甚至几十万毫秒的定时任务，这个 wheelSize 的扩充没有底线，就算将所有的定时任务的到期时间都设定一个上限，比如 100 万毫秒，那么这个 wheelSize 为 100 万毫秒的时间轮不仅占用很大的内存空间，而且效率也会拉低。Kafka 为此引入了层级时间轮的概念，当任务的到期时间超过了当前时间轮所表示的时间范围时，就会尝试添加到上层时间轮中。\n\n\n\n参考上图，复用之前的案例，第一层的时间轮 tickMs=1ms, wheelSize=20, interval=20ms。第二层的时间轮的 tickMs 为第一层时间轮的 interval，即为 20ms。每一层时间轮的 wheelSize 是固定的，都是 20，那么第二层的时间轮的总体时间跨度 interval 为 400ms。以此类推，这个 400ms 也是第三层的 tickMs 的大小，第三层的时间轮的总体时间跨度为 8000ms。\n\n刚才提到的 350ms 的任务，不会插入到第一层时间轮，会插入到 interval=20*20 的第二层时间轮中，具体插入到时间轮的哪个 bucket 呢？先用 350/tickMs(20)=virtualId(17)，然后 virtualId(17) %wheelSize (20) = 17，所以 350 会放在第 17 个 bucket。如果此时有一个 450ms 后执行的任务，那么会放在第三层时间轮中，按照刚才的计算公式，会放在第 0 个 bucket。第 0 个 bucket 里会包含[400,800)ms 的任务。随着时间流逝，当时间过去了 400ms，那么 450ms 后就要执行的任务还剩下 50ms 的时间才能执行，此时有一个时间轮降级的操作，将 50ms 任务重新提交到层级时间轮中，那么此时 50ms 的任务根据公式会放入第二个时间轮的第 2 个 bucket 中，此 bucket 的时间范围为[40,60)ms，然后再经过 40ms，这个 50ms 的任务又会被监控到，此时距离任务执行还有 10ms，同样将 10ms 的任务提交到层级时间轮，此时会加入到第一层时间轮的第 10 个 bucket，所以再经过 10ms 后，此任务到期，最终执行。\n\n整个时间轮的升级降级操作是不是很类似于我们的时钟？ 第一层时间轮 tickMs=1s, wheelSize=60，interval=1min，此为秒钟；第二层 tickMs=1min，wheelSize=60，interval=1hour，此为分钟；第三层 tickMs=1hour，wheelSize 为 12，interval 为 12hours，此为时钟。而钟表的指针就对应程序中的 currentTime，这个后面分析代码时候会讲到（对这个的理解也是时间轮理解的重点和难点）。\n\n\n# 任务添加和驱动时间轮滚动核心流程图\n\n\n\n\n# 重点代码介绍\n\n这是往 SystenTimer 中添加一个任务。\n\n//在Systemtimer中添加一个任务，任务被包装为一个TimerTaskEntry\nprivate def addTimerTaskEntry(timerTaskEntry: TimerTaskEntry): Unit = {\n//先判断是否可以添加进时间轮中，如果不可以添加进去代表任务已经过期或者任务被取消，注意这里的timingWheel持有上一层时间轮的引用，所以可能存在递归调用\n  if (!timingWheel.add(timerTaskEntry)) {\n    // Already expired or cancelled\n    if (!timerTaskEntry.cancelled)\n     //过期任务直接线程池异步执行掉\n      taskExecutor.submit(timerTaskEntry.timerTask)\n  }\n}\n//timingWheel添加任务，递归添加直到添加该任务进合适的时间轮的bucket中\ndef add(timerTaskEntry: TimerTaskEntry): Boolean = {\n  val expiration = timerTaskEntry.expirationMs\n  //任务取消\n  if (timerTaskEntry.cancelled) {\n    // Cancelled\n    false\n  } else if (expiration < currentTime + tickMs) {\n    // 任务过期后会被执行\n    false\n  } else if (expiration < currentTime + interval) {//任务过期时间比当前时间轮时间加周期小说明任务过期时间在本时间轮周期内\n    val virtualId = expiration / tickMs\n    //找到任务对应本时间轮的bucket\n    val bucket = buckets((virtualId % wheelSize.toLong).toInt)\n    bucket.add(timerTaskEntry)\n    // Set the bucket expiration time\n   //只有本bucket内的任务都过期后才会bucket.setExpiration返回true此时将bucket放入延迟队列\n    if (bucket.setExpiration(virtualId * tickMs)) {\n     //bucket是一个TimerTaskList，它实现了java.util.concurrent.Delayed接口，里面是一个多任务组成的链表，图2有说明\n      queue.offer(bucket)\n    }\n    true\n  } else {\n    // Out of the interval. Put it into the parent timer\n    //任务的过期时间不在本时间轮周期内说明需要升级时间轮，如果不存在则构造上一层时间轮，继续用上一层时间轮添加任务\n    if (overflowWheel == null) addOverflowWheel()\n    overflowWheel.add(timerTaskEntry)\n  }\n}\n\n\n在本层级时间轮里添加上一层时间轮里的过程，注意的是在下一层时间轮的 interval 为上一层时间轮的 tickMs。\n\nprivate[this] def addOverflowWheel(): Unit = {\n  synchronized {\n    if (overflowWheel == null) {\n      overflowWheel = new TimingWheel(\n        tickMs = interval,\n        wheelSize = wheelSize,\n        startMs = currentTime,\n        taskCounter = taskCounter,\n        queue\n      )\n    }\n  }\n}\n\n\n驱动时间轮滚动过程：\n\n注意这里会存在一个递归，一直驱动时间轮的指针滚动直到时间不足于驱动上层的时间轮滚动。\n\ndef advanceClock(timeMs: Long): Unit = {\n  if (timeMs >= currentTime + tickMs) {\n   //把当前时间打平为时间轮tickMs的整数倍\n    currentTime = timeMs - (timeMs % tickMs)\n    // Try to advance the clock of the overflow wheel if present\n    //驱动上层时间轮，这里的传给上层的currentTime时间是本层时间轮打平过的，但是在上层时间轮还是会继续打平\n    if (overflowWheel != null) overflowWheel.advanceClock(currentTime)\n  }\n}\n\n\n驱动源：\n\n//循环bucket里面的任务列表，一个个重新添加进时间轮，对符合条件的时间轮进行升降级或者执行任务\nprivate[this] val reinsert = (timerTaskEntry: TimerTaskEntry) => addTimerTaskEntry(timerTaskEntry)\n \n/*\n * Advances the clock if there is an expired bucket. If there isn\'t any expired bucket when called,\n * waits up to timeoutMs before giving up.\n */\ndef advanceClock(timeoutMs: Long): Boolean = {\n  var bucket = delayQueue.poll(timeoutMs, TimeUnit.MILLISECONDS)\n  if (bucket != null) {\n    writeLock.lock()\n    try {\n      while (bucket != null) {\n        //驱动时间轮\n        timingWheel.advanceClock(bucket.getExpiration())\n       //循环buckek也就是任务列表，任务列表一个个继续添加进时间轮以此来升级或者降级时间轮，把过期任务找出来执行\n        bucket.flush(reinsert)\n       //循环\n        //这里就是从延迟队列取出bucket，bucket是有延迟时间的，取出代表该bucket过期，我们通过bucket能取到bucket包含的任务列表\n        bucket = delayQueue.poll()\n      }\n    } finally {\n      writeLock.unlock()\n    }\n    true\n  } else {\n    false\n  }\n}\n\n\n\n# DelayQueue 与 kafka 时间轮\n\nkafka 的延迟队列使用时间轮实现，能够支持大量任务的高效触发，但是在 kafka 延迟队列实现方案里还是看到了 delayQueue 的影子，使用 delayQueue 是对时间轮里面的 bucket 放入延迟队列，以此来推动时间轮滚动，但是基于将插入和删除操作则放入时间轮中，将这些操作的时间复杂度都降为 O(1)，提升效率。Kafka 对性能的极致追求让它把最合适的组件放在最适合的位置。\n\n如何推进时间轮的前进，让时间轮的时间往前走。\n\n * Netty 中的时间轮是通过工作线程按照固定的时间间隔 tickDuration 推进的\n   * 如果长时间没有到期任务，这种方案会带来空推进的问题，从而造成一定的性能损耗；\n * Kafka 则是通过 DelayQueue 来推进，是一种空间换时间的思想；\n   * DelayQueue 中保存着所有的 TimerTaskList 对象，根据时间来排序，这样延时越小的任务排在越前面。\n   * 外部通过一个线程（叫做ExpiredOperationReaper）从 DelayQueue 中获取超时的任务列表 TimerTaskList，然后根据 TimerTaskList 的 过期时间来精确推进时间轮的时间，这样就不会存在空推进的问题啦。\n\n其实 Kafka 采用的是一种权衡的策略，把 DelayQueue 用在了合适的地方。DelayQueue 只存放了 TimerTaskList，并不是所有的 TimerTask，数量并不多，相比空推进带来的影响是利大于弊的。\n\n\n# 总结\n\n * Kafka 使用时间轮来实现延时队列，因为其底层是任务的添加和删除是基于链表实现的，是 O(1) 的时间复杂度，满足高性能的要求；\n * 对于时间跨度大的延时任务，Kafka 引入了层级时间轮，能更好控制时间粒度，可以应对更加复杂的定时任务处理场景；\n * 对于如何实现时间轮的推进和避免空推进影响性能，Kafka 采用空间换时间的思想，通过 DelayQueue 来推进时间轮，算是一个经典的 trade off（权衡）。\n\n\n# 参考文献\n\n一张图理解Kafka时间轮(TimingWheel),看不懂算我输!时间轮，是一种实现延迟功能（定时器）的巧妙算法，在N - 掘金 (juejin.cn)\n\n面试官：你给我说一下什么是时间轮吧？今天我带大家来卷一下时间轮吧，这个玩意其实还是挺实用的。 常见于各种框架之中，偶现于 - 掘金 (juejin.cn)\n\n任务调度之时间轮实现 | 京东云技术团队在生活中太阳的东升西落，鸟类的南飞北归，四级的轮换，每天的上下班，海水的潮汐，每 - 掘金 (juejin.cn)\n\n一张图理解Kafka时间轮(TimingWheel) - 知乎 (zhihu.com)\n\n时间轮在Kafka的实践_移动_滴滴技术_InfoQ精选文章',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 如何在高并发场景下高效管理数十万个定时任务，而不会造成性能瓶颈？\n 2. 为什么常规的定时任务扫描方式效率低下，时间轮如何通过环形队列提升任务调度的性能？\n 3. 当定时任务的延迟时间超过了当前时间轮的范围时，时间轮是如何通过层级机制处理这种复杂情况的？\n 4. 如何避免时间轮在处理少量定时任务时的“空推进”问题，从而提升资源利用率？\n 5. kafka 等高性能系统为什么选择时间轮来管理延迟任务，而不使用jdk自带的timer或delayqueue？\n 6. 在一个拥有数百万级别任务的系统中，如何通过多层时间轮实现任务的精确调度和执行？\n\n\n# 带着疑问\n\n第一个问题：如果一台机器上有 10w 个定时任务，如何做到高效触发？\n\n具体场景是：\n\n> 有一个 app 实时消息通道系统，对每个用户会维护一个 app 到服务器的 tcp 连接，用来实时收发消息，对这个 tcp 连接，有这样一个需求：“如果连续 30s 没有请求包（例如登录，消息，keepalive 包），服务端就要将这个用户的状态置为离线”。\n> \n> 其中，单机 tcp 同时在线量约在 10w 级别，keepalive 请求包较分散大概 30s 一次，吞吐量约在 3000qps。\n\n怎么做？\n\n常用方案使用 time 定时任务，每秒扫描一次所有连接的集合 map<uid, last_packet_time>，把连接时间（每次有新的请求更新对应连接的连接时间）比当前时间的差值大 30s 的连接找出来处理。\n\n另一种方案，使用环形队列法：\n\n三个重要的数据结构：\n\n 1. 30s 超时，就创建一个 index 从 0 到 30 的环形队列（本质是个数组）\n 2. 环上每一个 slot 是一个 set，任务集合\n 3. 同时还有一个 map<uid, index>，记录 uid 落在环上的哪个 slot 里\n\n这样当有某用户 uid 有请求包到达时：\n\n 1. 从 map 结构中，查找出这个 uid 存储在哪一个 slot 里\n 2. 从这个 slot 的 set 结构中，删除这个 uid\n 3. 将 uid 重新加入到新的 slot 中，具体是哪一个 slot 呢 => current index 指针所指向的上一个 slot，因为这个 slot，会被 timer 在 30s 之后扫描到\n 4. 更新 map，这个 uid 对应 slot 的 index 值\n\n哪些元素会被超时掉呢？\n\ncurrent index 每秒种移动一个 slot，这个 slot 对应的 set中所有 uid 都应该被集体超时！如果最近 30s 有请求包来到，一定被放到 current index 的前一个 slot 了，current index 所在的 slot 对应 set 中所有元素，都是最近 30s 没有请求包来到的。\n\n所以，当没有超时时，current index 扫到的每一个 slot 的 set 中应该都没有元素。\n\n两种方案对比：\n\n方案一每次都要轮询所有数据，而方案二使用环形队列只需要轮询这一刻需要过期的数据，如果没有数据过期则没有数据要处理，并且是批量超时，并且由于是环形结构更加节约空间，这很适合高性能场景。\n\n第二个问题： 在开发过程中有延迟一定时间的任务要执行，怎么做？\n\n如果不重复造轮子的话，我们的选择当然是延迟队列或者 timer。\n\n延迟队列和在 timer 中增 加延时任务采用数组表示的最小堆的数据结构实现，每次放入新元素和移除队首元素时间复杂度为 o(nlog(n))。\n\n\n# 前言\n\n时间轮，是一种实现延迟功能（定时器）的巧妙算法，在 netty，zookeeper，kafka 等各种框架中，甚至linux内核中都有用到。\n\n设计源于生活\n\n时间轮，其设计正是来源于生活中的时钟。\n\n如图就是一个简单的时间轮：\n\n\n\n图中大圆的圆心位置表示的是当前的时间，随着时间推移, 圆心处的时间也会不断跳动。\n\n下面我们对着这个图，来说说kafka的时间轮timingwheel。\n\nkafka时间轮的底层就是一个环形数组，而数组中每个元素都存放一个双向链表timertasklist，链表中封装了很多延时任务。\n\nkafka中一个时间轮timingwheel是由20个时间格组成，wheelsize = 20；每格的时间跨度是1ms，tickms = 1ms。参照kafka，上图中也用了20个灰边小圆表示时间格，为了动画演示可以看得清楚，我们这里每个小圆的时间跨度是1s。\n\n所以现在整个时间轮的时间跨度就是 tickms * wheelsize ，也就是 20s。从0s到19s，我们都分别有一个灰边小圆来承载。\n\nkafka的时间轮还有一个表盘指针 currenttime，表示时间轮当前所处的时间。也就是图中用黑色粗线表示的圆，随着时间推移, 这个指针也会不断前进;\n\n\n\n\n# 添加定时任务\n\n有了时间轮，现在可以往里面添加定时任务了。我们用一个粉红色的小圆来表示一个定时任务。\n\n\n\n这里先讲一下设定，每一个定时任务都有延时时间delaytime，和过期时间expiredtime。比如当前时间是10s，我们添加了个延时时间为2s的任务，那么这个任务的过期时间就是12s，也就是当前时间10s再走两秒，变成了12s的时候，就到了触发这个定时任务的时间。\n\n而时间轮上代表时间格的灰边小圆上显示的数字，可以理解为任务的过期时间。\n\n\n\n讲清楚这些设定后，我们就开始添加定时任务吧。\n\n初始的时候, 时间轮的指针定格在0。此时添加一个超时时间为2s的任务, 那么这个任务将会插入到第二个时间格中。\n\n\n\n当时间轮的指针到达第二个时间格时, 会处理该时间格上对应的任务。在动画上就是让红色的小圆消失!\n\n\n\n如果这个时候又插入一个延时时间为8s的任务进来, 这个任务的过期时间就是在当前时间2s的基础上加8s, 也就是10s, 那么这个任务将会插入到过期时间为10s的时间格中。\n\n\n\n\n# "动态"时间轮\n\n到目前为止，一切都很好理解。\n\n那么如果在当前时间是2s的时候, 插入一个延时时间为19s的任务时,这个任务的过期时间就是在当前时间2s的基础上加19s, 也就是21s。\n\n请看下图，当前的时间轮是没有过期时间为21s的时间格。这个任务将会插入到过期时间为1s的时间格中，这是怎么回事呢？\n\n\n\n\n# 复用时间格\n\n为了解答上面的问题，我们先来点魔法， 让时间轮上的时间都动起来！\n\n\n\n其实呢，当指针定格在2s的位置时, 时间格0s, 1s和2s就已经是过期的时间格。\n\n也就是说指针可以用来划分过期的时间格[0,2]和未来的时间格 [3,19]。而过期的时间格可以继续复用。比如过期的时间格0s就变成了20s, 存放过期时间为20s的任务。\n\n理解了时间格的复用之后，再看回刚刚的例子，当前时间是2s时，添加延时时间为19s的任务，那么这个任务就会插入到过期时间为21s的时间格中。\n\n\n\n\n# 时间轮升级\n\n下面，新的问题来了，请坐好扶稳。\n\n如果在当前时间是2s的时候, 插入一个延时时间为22s的任务, 这个任务的过期时间就是在2s的基础上加22s，也就是24s。\n\n\n\n显然当前时间轮是无法找到过期时间格为24秒的时间格，因为当前过期时间最大的时间格才到21s。而且我们也没办法像前面那样再复用时间格，因为除了过期时间为2s的时间格，其他的时间格都还没过期呢。当前时间轮无法承载这个定时任务,那么应该怎么办呢?\n\n当然我们可以选择扩展时间轮上的时间格, 但是这样一来，时间轮就失去了意义。\n\n是时候要升级时间轮了！\n\n我们先来理解下多层时间轮之间的联系。\n\n\n# 层级时间轮\n\n如图是一个两层的时间轮:\n\n\n\n第二层时间轮也是由20个时间格组成, 每个时间格的跨度是20s。\n\n图中展示了每个时间格对应的过期时间范围, 我们可以清晰地看到, 第二层时间轮的第0个时间格的过期时间范围是 [0,19]。也就是说, 第二层时间轮的一个时间格就可以表示第一层时间轮的所有(20个)时间格;\n\n为了进一步理清第一层时间轮和第二层时间轮的关系, 我们拉着时间的小手, 一起观看下面的动图:\n\n\n\n可以看到，第二层时间轮同样也有自己的指针, 每当第一层时间轮走完一个周期，第二层时间轮的指针就会推进一格。\n\n\n# 添加定时任务\n\n回到一开始的问题，在当前时间是2s的时候, 插入一个延时时间为22s的任务，该任务过期时间为24s。\n\n\n\n当第一层时间轮容纳不下时，进入第二层时间轮，并插入到过期时间为[20,39]的时间格中。\n\n我们再来个例子，如果在当前时间是2s的时候, 插入一个延时时间为350s的任务, 这个任务的过期时间就是在2s的基础上加350s，也就是352s。\n\n\n\n从图中可以看到，该任务插入到第二层时间轮过期时间为[340,359]s的时间格中，也就是第17格的位置。\n\n\n# "动态"层级时间轮\n\n通常来说, 第二层时间轮的第0个时间格是用来表示第一层时间轮的, 这一格是存放不了任务的, 因为超时时间0-20s的任务, 第一层时间轮就可以处理了。\n\n但是! 事情往往没这么简单, 我们时间轮上的时间格都是可以复用的! 那么这在第二层时间轮上又是怎么体现的呢?\n\n下面是魔法时间， 我们让时间轮上的过期时间都动起来！\n\n\n\n从图中可以看到，当第一层时间轮的指针定格在1s时，超时时间0s的时间格就过期了。而这个时候，第二层时间轮第0个时间格的时间范围就从[0,19]分为了过期的[0],和未过期的[1,19]。而过期的[0]就会被新的过期时间[400]复用。\n\n[0-19]\n\n[400][1,19]\n\n[400,401][2,19]\n\n......\n\n[400,419]\n\n\n所以，如果在当前时间是2s的时候, 插入一个延时时间为399s的任务, 这个任务的过期时间就是在2s的基础上加399s，也就是401s。如图，这个任务还是会插到第二层时间轮第0个时间格中去。\n\n\n\n\n# 时间轮降级\n\n还是用回这个大家都已经耳熟能详的例子，在当前时间是2s的时候, 插入一个延时时间为22s的任务，该任务过期时间为24s。最后进入第二层时间轮，并插入到过期时间为[20,39]的时间格中。\n\n当二层时间轮上的定时任务到期后，时间轮是怎么做的呢？\n\n\n\n从图中可以看到，随着当前时间从2s继续往前推进，一直到20s的时候，总共经过了18s。此时第二层时间轮中，超时时间为[20-39s]的时间格上的任务到期。\n\n原本超时时间为24s的任务会被取出来，重新加入时间轮。此时该定时任务的延时时间从原本的22s，到现在还剩下4s（22s-18s）。最后停留在第一层时间轮超时时间为24s的时间格，也就是第4个时间格。\n\n随着当前时间继续推进，再经过4s后，该定时任务到期被执行。\n\n从这里可以看出时间轮的巧妙之处，两层时间轮只用了40个数组元素，却可以承载[0-399s]的定时任务。而三层时间轮用60个数组元素，就可以承载[0-7999s]的定时任务！\n\n\n\n\n# 时间轮的推进\n\n从动画中可以注意到, 随着时间推进, 时间轮的指针循环往复地定格在每一个时间格上, 每一次都要判断当前定格的时间格里是不是有任务存在;\n\n其中有很多时间格都是没有任务的, 指针定格在这种空的时间格中, 就是一次"空推进";\n\n比如说, 插入一个延时时间400s的任务, 指针就要执行399次"空推进", 这是一种浪费!\n\n那么kafka是怎么解决这个问题的呢？这就要从延迟队列delayqueue开始讲起了！时间轮搭配延迟队列delayqueue，会发生什么化学反应呢？\n\n\n# 时间轮在 kafka 中的实现\n\n方案二所采用的环形队列，就是时间轮的底层数据结构，它能够让需要处理的数据（任务的抽象）集中，在 kafka 中存在大量的延迟操作，比如延迟生产、延迟拉取以及延迟删除等。kafka 并没有使用 jdk 自带的 timer 或者 delayqueue 来实现延迟的功能，而是基于时间轮自定义了一个用于实现延迟功能的定时器（systemtimer）。jdk 的 timer 和 delayqueue 插入和删除操作的平均时间复杂度为 o(nlog(n))，并不能满足 kafka 的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为 o(1)。时间轮的应用并非 kafka 独有，其应用场景还有很多，在 netty、akka、quartz、zookeeper 等组件中都存在时间轮的踪影。\n\n\n# 时间轮的数据结构\n\n参考下图，kafka 中的时间轮（timingwheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表（timertasklist）。timertasklist 是一个环形的双向链表，链表中的每一项表示的都是定时任务项（timertaskentry），其中封装了真正的定时任务 timertask。在 kafka 源码中对这个 timetasklist 是用一个名称为 buckets 的数组表示的，所以后面介绍中可能 timertasklist 也会被称为 bucket。\n\n\n\n针对上图的几个名词简单解释下：\n\n * tickms： 时间轮由多个时间格组成，每个时间格就是 tickms，它代表当前时间轮的基本时间跨度。\n * wheelsize： 代表每一层时间轮的格数\n * interval： 当前时间轮的总体时间跨度，interval=tickms × wheelsize\n * startms： 构造当层时间轮时候的当前时间，第一层的时间轮的 startms 是 timeunit.nanoseconds.tomillis(nanoseconds()),上层时间轮的 startms 为下层时间轮的 currenttime。\n * currenttime： 表示时间轮当前所处的时间，currenttime 是 tickms 的整数倍（通过 currenttime=startms - (startms % tickms 来保正 currenttime 一定是 tickms 的整数倍），这个运算类比钟表中分钟里 65 秒分钟指针指向的还是 1 分钟）。currenttime 可以将整个时间轮划分为到期部分和未到期部分，currenttime 当前指向的时间格也属于到期部分，表示刚好到期，需要处理此时间格所对应的 timertasklist 的所有任务。\n\n\n# 时间轮中的任务存放\n\n若时间轮的 tickms=1ms，wheelsize=20，那么可以计算得出 interval 为 20ms。初始情况下表盘指针 currenttime 指向时间格 0，此时有一个定时为 2ms 的任务插入进来会存放到时间格为 2 的 timertasklist 中。随着时间的不断推移，指针 currenttime 不断向前推进，过了 2ms 之后，当到达时间格 2 时，就需要将时间格 2 所对应的 timetasklist 中的任务做相应的到期操作。此时若又有一个定时为 8ms 的任务插入进来，则会存放到时间格 10 中，currenttime 再过 8ms 后会指向时间格 10。如果同时有一个定时为 19ms 的任务插入进来怎么办？新来的 timertaskentry 会复用原来的 timertasklist，所以它会插入到原本已经到期的时间格 1 中。总之，整个时间轮的总体跨度是不变的，随着指针 currenttime 的不断推进，当前时间轮所能处理的时间段也在不断后移，总体时间范围在 currenttime 和 currenttime+interval 之间。\n\n\n# 时间轮的升降级\n\n如果此时有个定时为 350ms 的任务该如何处理？直接扩充 wheelsize 的大小么？kafka 中不乏几万甚至几十万毫秒的定时任务，这个 wheelsize 的扩充没有底线，就算将所有的定时任务的到期时间都设定一个上限，比如 100 万毫秒，那么这个 wheelsize 为 100 万毫秒的时间轮不仅占用很大的内存空间，而且效率也会拉低。kafka 为此引入了层级时间轮的概念，当任务的到期时间超过了当前时间轮所表示的时间范围时，就会尝试添加到上层时间轮中。\n\n\n\n参考上图，复用之前的案例，第一层的时间轮 tickms=1ms, wheelsize=20, interval=20ms。第二层的时间轮的 tickms 为第一层时间轮的 interval，即为 20ms。每一层时间轮的 wheelsize 是固定的，都是 20，那么第二层的时间轮的总体时间跨度 interval 为 400ms。以此类推，这个 400ms 也是第三层的 tickms 的大小，第三层的时间轮的总体时间跨度为 8000ms。\n\n刚才提到的 350ms 的任务，不会插入到第一层时间轮，会插入到 interval=20*20 的第二层时间轮中，具体插入到时间轮的哪个 bucket 呢？先用 350/tickms(20)=virtualid(17)，然后 virtualid(17) %wheelsize (20) = 17，所以 350 会放在第 17 个 bucket。如果此时有一个 450ms 后执行的任务，那么会放在第三层时间轮中，按照刚才的计算公式，会放在第 0 个 bucket。第 0 个 bucket 里会包含[400,800)ms 的任务。随着时间流逝，当时间过去了 400ms，那么 450ms 后就要执行的任务还剩下 50ms 的时间才能执行，此时有一个时间轮降级的操作，将 50ms 任务重新提交到层级时间轮中，那么此时 50ms 的任务根据公式会放入第二个时间轮的第 2 个 bucket 中，此 bucket 的时间范围为[40,60)ms，然后再经过 40ms，这个 50ms 的任务又会被监控到，此时距离任务执行还有 10ms，同样将 10ms 的任务提交到层级时间轮，此时会加入到第一层时间轮的第 10 个 bucket，所以再经过 10ms 后，此任务到期，最终执行。\n\n整个时间轮的升级降级操作是不是很类似于我们的时钟？ 第一层时间轮 tickms=1s, wheelsize=60，interval=1min，此为秒钟；第二层 tickms=1min，wheelsize=60，interval=1hour，此为分钟；第三层 tickms=1hour，wheelsize 为 12，interval 为 12hours，此为时钟。而钟表的指针就对应程序中的 currenttime，这个后面分析代码时候会讲到（对这个的理解也是时间轮理解的重点和难点）。\n\n\n# 任务添加和驱动时间轮滚动核心流程图\n\n\n\n\n# 重点代码介绍\n\n这是往 systentimer 中添加一个任务。\n\n//在systemtimer中添加一个任务，任务被包装为一个timertaskentry\nprivate def addtimertaskentry(timertaskentry: timertaskentry): unit = {\n//先判断是否可以添加进时间轮中，如果不可以添加进去代表任务已经过期或者任务被取消，注意这里的timingwheel持有上一层时间轮的引用，所以可能存在递归调用\n  if (!timingwheel.add(timertaskentry)) {\n    // already expired or cancelled\n    if (!timertaskentry.cancelled)\n     //过期任务直接线程池异步执行掉\n      taskexecutor.submit(timertaskentry.timertask)\n  }\n}\n//timingwheel添加任务，递归添加直到添加该任务进合适的时间轮的bucket中\ndef add(timertaskentry: timertaskentry): boolean = {\n  val expiration = timertaskentry.expirationms\n  //任务取消\n  if (timertaskentry.cancelled) {\n    // cancelled\n    false\n  } else if (expiration < currenttime + tickms) {\n    // 任务过期后会被执行\n    false\n  } else if (expiration < currenttime + interval) {//任务过期时间比当前时间轮时间加周期小说明任务过期时间在本时间轮周期内\n    val virtualid = expiration / tickms\n    //找到任务对应本时间轮的bucket\n    val bucket = buckets((virtualid % wheelsize.tolong).toint)\n    bucket.add(timertaskentry)\n    // set the bucket expiration time\n   //只有本bucket内的任务都过期后才会bucket.setexpiration返回true此时将bucket放入延迟队列\n    if (bucket.setexpiration(virtualid * tickms)) {\n     //bucket是一个timertasklist，它实现了java.util.concurrent.delayed接口，里面是一个多任务组成的链表，图2有说明\n      queue.offer(bucket)\n    }\n    true\n  } else {\n    // out of the interval. put it into the parent timer\n    //任务的过期时间不在本时间轮周期内说明需要升级时间轮，如果不存在则构造上一层时间轮，继续用上一层时间轮添加任务\n    if (overflowwheel == null) addoverflowwheel()\n    overflowwheel.add(timertaskentry)\n  }\n}\n\n\n在本层级时间轮里添加上一层时间轮里的过程，注意的是在下一层时间轮的 interval 为上一层时间轮的 tickms。\n\nprivate[this] def addoverflowwheel(): unit = {\n  synchronized {\n    if (overflowwheel == null) {\n      overflowwheel = new timingwheel(\n        tickms = interval,\n        wheelsize = wheelsize,\n        startms = currenttime,\n        taskcounter = taskcounter,\n        queue\n      )\n    }\n  }\n}\n\n\n驱动时间轮滚动过程：\n\n注意这里会存在一个递归，一直驱动时间轮的指针滚动直到时间不足于驱动上层的时间轮滚动。\n\ndef advanceclock(timems: long): unit = {\n  if (timems >= currenttime + tickms) {\n   //把当前时间打平为时间轮tickms的整数倍\n    currenttime = timems - (timems % tickms)\n    // try to advance the clock of the overflow wheel if present\n    //驱动上层时间轮，这里的传给上层的currenttime时间是本层时间轮打平过的，但是在上层时间轮还是会继续打平\n    if (overflowwheel != null) overflowwheel.advanceclock(currenttime)\n  }\n}\n\n\n驱动源：\n\n//循环bucket里面的任务列表，一个个重新添加进时间轮，对符合条件的时间轮进行升降级或者执行任务\nprivate[this] val reinsert = (timertaskentry: timertaskentry) => addtimertaskentry(timertaskentry)\n \n/*\n * advances the clock if there is an expired bucket. if there isn\'t any expired bucket when called,\n * waits up to timeoutms before giving up.\n */\ndef advanceclock(timeoutms: long): boolean = {\n  var bucket = delayqueue.poll(timeoutms, timeunit.milliseconds)\n  if (bucket != null) {\n    writelock.lock()\n    try {\n      while (bucket != null) {\n        //驱动时间轮\n        timingwheel.advanceclock(bucket.getexpiration())\n       //循环buckek也就是任务列表，任务列表一个个继续添加进时间轮以此来升级或者降级时间轮，把过期任务找出来执行\n        bucket.flush(reinsert)\n       //循环\n        //这里就是从延迟队列取出bucket，bucket是有延迟时间的，取出代表该bucket过期，我们通过bucket能取到bucket包含的任务列表\n        bucket = delayqueue.poll()\n      }\n    } finally {\n      writelock.unlock()\n    }\n    true\n  } else {\n    false\n  }\n}\n\n\n\n# delayqueue 与 kafka 时间轮\n\nkafka 的延迟队列使用时间轮实现，能够支持大量任务的高效触发，但是在 kafka 延迟队列实现方案里还是看到了 delayqueue 的影子，使用 delayqueue 是对时间轮里面的 bucket 放入延迟队列，以此来推动时间轮滚动，但是基于将插入和删除操作则放入时间轮中，将这些操作的时间复杂度都降为 o(1)，提升效率。kafka 对性能的极致追求让它把最合适的组件放在最适合的位置。\n\n如何推进时间轮的前进，让时间轮的时间往前走。\n\n * netty 中的时间轮是通过工作线程按照固定的时间间隔 tickduration 推进的\n   * 如果长时间没有到期任务，这种方案会带来空推进的问题，从而造成一定的性能损耗；\n * kafka 则是通过 delayqueue 来推进，是一种空间换时间的思想；\n   * delayqueue 中保存着所有的 timertasklist 对象，根据时间来排序，这样延时越小的任务排在越前面。\n   * 外部通过一个线程（叫做expiredoperationreaper）从 delayqueue 中获取超时的任务列表 timertasklist，然后根据 timertasklist 的 过期时间来精确推进时间轮的时间，这样就不会存在空推进的问题啦。\n\n其实 kafka 采用的是一种权衡的策略，把 delayqueue 用在了合适的地方。delayqueue 只存放了 timertasklist，并不是所有的 timertask，数量并不多，相比空推进带来的影响是利大于弊的。\n\n\n# 总结\n\n * kafka 使用时间轮来实现延时队列，因为其底层是任务的添加和删除是基于链表实现的，是 o(1) 的时间复杂度，满足高性能的要求；\n * 对于时间跨度大的延时任务，kafka 引入了层级时间轮，能更好控制时间粒度，可以应对更加复杂的定时任务处理场景；\n * 对于如何实现时间轮的推进和避免空推进影响性能，kafka 采用空间换时间的思想，通过 delayqueue 来推进时间轮，算是一个经典的 trade off（权衡）。\n\n\n# 参考文献\n\n一张图理解kafka时间轮(timingwheel),看不懂算我输!时间轮，是一种实现延迟功能（定时器）的巧妙算法，在n - 掘金 (juejin.cn)\n\n面试官：你给我说一下什么是时间轮吧？今天我带大家来卷一下时间轮吧，这个玩意其实还是挺实用的。 常见于各种框架之中，偶现于 - 掘金 (juejin.cn)\n\n任务调度之时间轮实现 | 京东云技术团队在生活中太阳的东升西落，鸟类的南飞北归，四级的轮换，每天的上下班，海水的潮汐，每 - 掘金 (juejin.cn)\n\n一张图理解kafka时间轮(timingwheel) - 知乎 (zhihu.com)\n\n时间轮在kafka的实践_移动_滴滴技术_infoq精选文章',charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"介绍",frontmatter:{title:"介绍",date:"2024-09-15T21:10:46.000Z",permalink:"/pages/b9733b/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.Kafka%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E4%BB%8B%E7%BB%8D.html",relativePath:"02.Kafka  系统设计/01.Kafka 系统设计/01.介绍.md",key:"v-01d9b103",path:"/pages/b9733b/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"指南",frontmatter:{title:"指南",date:"2024-09-17T16:51:40.000Z",permalink:"/pages/e21d7f/"},regularPath:"/02.Kafka%20%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E5%89%8D%E8%A8%80/01.%E6%8C%87%E5%8D%97.html",relativePath:"02.Kafka  系统设计/01.前言/01.指南.md",key:"v-5e87f18c",path:"/pages/e21d7f/",headers:[{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:2},{level:2,title:"阅读方法",slug:"阅读方法",normalizedTitle:"阅读方法",charIndex:156},{level:3,title:"蓝图",slug:"蓝图",normalizedTitle:"蓝图",charIndex:165},{level:3,title:"基础",slug:"基础",normalizedTitle:"基础",charIndex:136},{level:3,title:"主线",slug:"主线",normalizedTitle:"主线",charIndex:179},{level:3,title:"支线",slug:"支线",normalizedTitle:"支线",charIndex:186},{level:2,title:"学习资料推荐",slug:"学习资料推荐",normalizedTitle:"学习资料推荐",charIndex:193}],headersStr:"前置知识 阅读方法 蓝图 基础 主线 支线 学习资料推荐",content:"# 前置知识\n\n * 常用数据结构：数组、链表、哈希表、跳表\n * 网络协议：TCP 协议\n * 网络 IO 模型：IO 多路复用、非阻塞 IO、Reactor 网络模型\n * 操作系统：零拷贝（Copy On Write）、常见系统调用、磁盘 IO 机制\n * 编程语言基础：循环、分支、结构体、指针\n\n\n# 阅读方法\n\n\n# 蓝图\n\n\n# 基础\n\n\n# 主线\n\n\n# 支线\n\n\n# 学习资料推荐\n\n * https://kafka.apache.org/\n\n * A Deep Dive into Apache Kafka This is Event Streaming by Andrew Dunnings & Katherine Stanley (youtube.com)\n\n * ",normalizedContent:"# 前置知识\n\n * 常用数据结构：数组、链表、哈希表、跳表\n * 网络协议：tcp 协议\n * 网络 io 模型：io 多路复用、非阻塞 io、reactor 网络模型\n * 操作系统：零拷贝（copy on write）、常见系统调用、磁盘 io 机制\n * 编程语言基础：循环、分支、结构体、指针\n\n\n# 阅读方法\n\n\n# 蓝图\n\n\n# 基础\n\n\n# 主线\n\n\n# 支线\n\n\n# 学习资料推荐\n\n * https://kafka.apache.org/\n\n * a deep dive into apache kafka this is event streaming by andrew dunnings & katherine stanley (youtube.com)\n\n * ",charsets:{cjk:!0},lastUpdated:"2024/09/17, 14:54:21",lastUpdatedTimestamp:1726584861e3},{title:"介绍",frontmatter:{title:"介绍",date:"2024-09-15T21:10:47.000Z",permalink:"/pages/4601ca/"},regularPath:"/03.Nginx%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.Nginx%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E4%BB%8B%E7%BB%8D.html",relativePath:"03.Nginx 系统设计/01.Nginx 系统设计/01.介绍.md",key:"v-ac71d918",path:"/pages/4601ca/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"分布式缓存",frontmatter:{title:"分布式缓存",date:"2024-09-14T02:09:39.000Z",permalink:"/pages/84cb49/"},regularPath:"/20.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/01.%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98.html",relativePath:"20.设计基础设施/01.设计基础设施/01.分布式缓存.md",key:"v-0a2a64b8",path:"/pages/84cb49/",headers:[{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:2},{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:124},{level:2,title:"渐进式设计",slug:"渐进式设计",normalizedTitle:"渐进式设计",charIndex:251},{level:3,title:"本地缓存",slug:"本地缓存",normalizedTitle:"本地缓存",charIndex:261},{level:3,title:"分布式缓存",slug:"分布式缓存",normalizedTitle:"分布式缓存",charIndex:272},{level:4,title:"认识分布式缓存",slug:"认识分布式缓存",normalizedTitle:"认识分布式缓存",charIndex:281},{level:4,title:"如何选择缓存节点？",slug:"如何选择缓存节点",normalizedTitle:"如何选择缓存节点？",charIndex:294},{level:4,title:"缓存客户端是什么？",slug:"缓存客户端是什么",normalizedTitle:"缓存客户端是什么？",charIndex:311},{level:4,title:"获取缓存节点列表",slug:"获取缓存节点列表",normalizedTitle:"获取缓存节点列表",charIndex:332},{level:2,title:"非功能设计",slug:"非功能设计",normalizedTitle:"非功能设计",charIndex:383},{level:3,title:"高可用",slug:"高可用",normalizedTitle:"高可用",charIndex:74},{level:3,title:"高可靠",slug:"高可靠",normalizedTitle:"高可靠",charIndex:403},{level:2,title:"还有什么重要的",slug:"还有什么重要的",normalizedTitle:"还有什么重要的",charIndex:482},{level:3,title:"一致性",slug:"一致性",normalizedTitle:"一致性",charIndex:149},{level:3,title:"数据过期",slug:"数据过期",normalizedTitle:"数据过期",charIndex:221},{level:3,title:"数据淘汰策略",slug:"数据淘汰策略",normalizedTitle:"数据淘汰策略",charIndex:533},{level:3,title:"本地缓存+远程缓存",slug:"本地缓存-远程缓存",normalizedTitle:"本地缓存+远程缓存",charIndex:544},{level:3,title:"安全",slug:"安全",normalizedTitle:"安全",charIndex:234},{level:3,title:"监控和日志",slug:"监控和日志",normalizedTitle:"监控和日志",charIndex:238},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:575}],headersStr:"需求 前置知识 渐进式设计 本地缓存 分布式缓存 认识分布式缓存 如何选择缓存节点？ 缓存客户端是什么？ 获取缓存节点列表 非功能设计 高可用 高可靠 还有什么重要的 一致性 数据过期 数据淘汰策略 本地缓存+远程缓存 安全 监控和日志 总结",content:"# 需求\n\n功能性\n\n * put(key,value)\n * get(key)\n\n非功能性\n\n * 高扩展（随着数据和请求的增加轻松扩展）\n * 高可用（硬件/网络故障下仍然可用）\n * 高性能（put和get的高性能！）\n * 持久化\n\n\n# 前置知识\n\n * LRU 算法\n\n * 哈希取模与一致性哈希算法\n\n * 专用缓存集群与共存缓存\n\n * 缓存客户端\n\n * 静态与动态缓存服务器列表配置\n\n * 主从复制\n\n * 缓存一致性、数据过期、本地和远程缓存、安全性、监控和日志记录。\n\n\n# 渐进式设计\n\n\n# 本地缓存\n\n\n\n\n# 分布式缓存\n\n# 认识分布式缓存\n\n\n\n# 如何选择缓存节点？\n\n\n\n\n\n# 缓存客户端是什么？\n\n\n\n * 客户端如何获取缓存节点列表？\n * 当扩容或缩容时，缓存节点列表如何更新？\n\n# 获取缓存节点列表\n\n\n\n\n# 非功能设计\n\n\n# 高可用\n\n\n\n\n# 高可靠\n\n * 数据异步复制到replica\n * 数据同步复制到replica\n * 数据同步复制到所有replica然后返回？\n\n我们需要权衡！\n\n\n# 还有什么重要的\n\n\n# 一致性\n\n引入同步复制，确保所有缓存节点的视图一致\n\n\n# 数据过期\n\n\n# 数据淘汰策略\n\n\n# 本地缓存+远程缓存\n\n\n# 安全\n\n\n# 监控和日志\n\n\n# 总结\n\n",normalizedContent:"# 需求\n\n功能性\n\n * put(key,value)\n * get(key)\n\n非功能性\n\n * 高扩展（随着数据和请求的增加轻松扩展）\n * 高可用（硬件/网络故障下仍然可用）\n * 高性能（put和get的高性能！）\n * 持久化\n\n\n# 前置知识\n\n * lru 算法\n\n * 哈希取模与一致性哈希算法\n\n * 专用缓存集群与共存缓存\n\n * 缓存客户端\n\n * 静态与动态缓存服务器列表配置\n\n * 主从复制\n\n * 缓存一致性、数据过期、本地和远程缓存、安全性、监控和日志记录。\n\n\n# 渐进式设计\n\n\n# 本地缓存\n\n\n\n\n# 分布式缓存\n\n# 认识分布式缓存\n\n\n\n# 如何选择缓存节点？\n\n\n\n\n\n# 缓存客户端是什么？\n\n\n\n * 客户端如何获取缓存节点列表？\n * 当扩容或缩容时，缓存节点列表如何更新？\n\n# 获取缓存节点列表\n\n\n\n\n# 非功能设计\n\n\n# 高可用\n\n\n\n\n# 高可靠\n\n * 数据异步复制到replica\n * 数据同步复制到replica\n * 数据同步复制到所有replica然后返回？\n\n我们需要权衡！\n\n\n# 还有什么重要的\n\n\n# 一致性\n\n引入同步复制，确保所有缓存节点的视图一致\n\n\n# 数据过期\n\n\n# 数据淘汰策略\n\n\n# 本地缓存+远程缓存\n\n\n# 安全\n\n\n# 监控和日志\n\n\n# 总结\n\n",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"碎碎念",frontmatter:{title:"碎碎念",date:"2024-09-15T01:18:31.000Z",permalink:"/pages/52ebd8/"},regularPath:"/06.%E5%8A%A8%E6%80%81/01.%E7%A2%8E%E7%A2%8E%E5%BF%B5.html",relativePath:"06.动态/01.碎碎念.md",key:"v-103a83f1",path:"/pages/52ebd8/",headers:[{level:2,title:"如何写好一篇文档",slug:"如何写好一篇文档",normalizedTitle:"如何写好一篇文档",charIndex:2},{level:2,title:"如何拥有持续输出的能力",slug:"如何拥有持续输出的能力",normalizedTitle:"如何拥有持续输出的能力",charIndex:345}],headersStr:"如何写好一篇文档 如何拥有持续输出的能力",content:"# 如何写好一篇文档\n\n 1.  总分总结构\n 2.  多用图片 数据 代码块 引用文献\n 3.  排版简约 避免一段文字超过 8 行 尽量有主句\n 4.  每发一篇文章之前，至少要看 5 篇类似的优质文章做调研，如果找不到，就别发了\n 5.  要有自己的特色，我认为我要在文章中多穿插问答题，以及多插入自己的见解，例如：echo 认为 xxx\n 6.  把每一个栏目做好做精致，切勿急于求成，把观众留下\n 7.  每一篇文章末尾都要搜集相关问答\n 8.  在开篇设置悬念是一个很好的办法\n 9.  定期进行 “Code Review”\n 10. AI 审查 建议收藏：2024新版GPT/Claude超详细论文润色指南「更新至24-08-03」_chatgpt润色-CSDN博客\n\n\n# 如何拥有持续输出的能力\n\n 1. 对每一个栏目有一个总的理解，然后对该栏目需要发布哪些内容有个抽象的理解，然后对每一个抽象去做调研\n 2. 多看优质文章，学习他们的思路与表达方式\n 3. 学习英语，外网也有很多优质的文章\n 4. 写好自己的每一篇文章，每一篇文章都要做到最优",normalizedContent:"# 如何写好一篇文档\n\n 1.  总分总结构\n 2.  多用图片 数据 代码块 引用文献\n 3.  排版简约 避免一段文字超过 8 行 尽量有主句\n 4.  每发一篇文章之前，至少要看 5 篇类似的优质文章做调研，如果找不到，就别发了\n 5.  要有自己的特色，我认为我要在文章中多穿插问答题，以及多插入自己的见解，例如：echo 认为 xxx\n 6.  把每一个栏目做好做精致，切勿急于求成，把观众留下\n 7.  每一篇文章末尾都要搜集相关问答\n 8.  在开篇设置悬念是一个很好的办法\n 9.  定期进行 “code review”\n 10. ai 审查 建议收藏：2024新版gpt/claude超详细论文润色指南「更新至24-08-03」_chatgpt润色-csdn博客\n\n\n# 如何拥有持续输出的能力\n\n 1. 对每一个栏目有一个总的理解，然后对该栏目需要发布哪些内容有个抽象的理解，然后对每一个抽象去做调研\n 2. 多看优质文章，学习他们的思路与表达方式\n 3. 学习英语，外网也有很多优质的文章\n 4. 写好自己的每一篇文章，每一篇文章都要做到最优",charsets:{cjk:!0},lastUpdated:"2024/09/17, 11:51:50",lastUpdatedTimestamp:172657391e4},{title:"限流器",frontmatter:{title:"限流器",date:"2024-09-14T02:09:39.000Z",permalink:"/pages/57d5a5/"},regularPath:"/20.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/02.%E9%99%90%E6%B5%81%E5%99%A8.html",relativePath:"20.设计基础设施/01.设计基础设施/02.限流器.md",key:"v-18ed35f2",path:"/pages/57d5a5/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:190},{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:299},{level:2,title:"组件架构",slug:"组件架构",normalizedTitle:"组件架构",charIndex:438},{level:2,title:"接口和类",slug:"接口和类",normalizedTitle:"接口和类",charIndex:449},{level:2,title:"消息广播",slug:"消息广播",normalizedTitle:"消息广播",charIndex:355},{level:2,title:"如何集成",slug:"如何集成",normalizedTitle:"如何集成",charIndex:471},{level:2,title:"答疑解惑",slug:"答疑解惑",normalizedTitle:"答疑解惑",charIndex:482},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:753}],headersStr:"概述 需求 前置知识 组件架构 接口和类 消息广播 如何集成 答疑解惑 总结",content:"# 概述\n\n\n\n * 我们需要编写一个可扩展以处理高负载的软件吗？\n\n * 负载均衡器中已经实现了最大连接数和服务端点上的最大线程数呢。我们还需要节流吗？\n\n * * 这种机制是不加区别的，有些操作快，有些操作慢，负载均衡器不了解每个操作的成本，如果我们想对某个操作实现特定的限流，负载均衡器就不好使了\n\n * 我们如何单独控制每个主机呢？\n\n * * 主机间相互通信！\n\n\n# 需求\n\n功能性\n\n * allowRequest(request)\n\n非功能性\n\n * 低延迟（尽快做出决定）\n * 准确性（尽可能准确）\n * 可扩展（支持集群中任意数量的主机）\n * 高可用\n * 高可靠\n\n\n# 前置知识\n\n * 令牌桶算法\n * 速率限制解决方案的面向对象设计\n * 负载均衡器最大连接数，自动扩展\n * 消息广播：全网状网络拓扑、gossip 通信、分布式缓存、协调服务\n * 通信协议：TCP、UDP\n * 嵌入式速率限制器与守护进程\n * 存储桶管理、同步\n\n\n# 组件架构\n\n\n\n\n# 接口和类\n\n\n\n\n# 消息广播\n\n\n\n\n# 如何集成\n\n\n\n\n# 答疑解惑\n\n * 我的服务非常受欢迎，有数百万用户。这是否意味着内存中存储了数百万个桶？\n\n * * 可以瞬间增加很多桶，但是当桶没用的时候，可以删除桶\n\n * 有哪些失败的场景？\n\n * * 守护线程失败\n   * 网络分区，无法传播消息，每个主机将允许更多的请求\n\n * 我们是否需要一个自动配置管理服务？\n\n * 我有点担心同步问题。这不是瓶颈吗？\n\n * * 原则上来说 我们要重视并发安全\n   * 但是我们没有必要实现，会对性能造成影响\n\n * 当客户的请求被拒绝时，他们应该做什么？\n\n * * 重试（指数退避、抖动）\n\n\n# 总结\n\n",normalizedContent:"# 概述\n\n\n\n * 我们需要编写一个可扩展以处理高负载的软件吗？\n\n * 负载均衡器中已经实现了最大连接数和服务端点上的最大线程数呢。我们还需要节流吗？\n\n * * 这种机制是不加区别的，有些操作快，有些操作慢，负载均衡器不了解每个操作的成本，如果我们想对某个操作实现特定的限流，负载均衡器就不好使了\n\n * 我们如何单独控制每个主机呢？\n\n * * 主机间相互通信！\n\n\n# 需求\n\n功能性\n\n * allowrequest(request)\n\n非功能性\n\n * 低延迟（尽快做出决定）\n * 准确性（尽可能准确）\n * 可扩展（支持集群中任意数量的主机）\n * 高可用\n * 高可靠\n\n\n# 前置知识\n\n * 令牌桶算法\n * 速率限制解决方案的面向对象设计\n * 负载均衡器最大连接数，自动扩展\n * 消息广播：全网状网络拓扑、gossip 通信、分布式缓存、协调服务\n * 通信协议：tcp、udp\n * 嵌入式速率限制器与守护进程\n * 存储桶管理、同步\n\n\n# 组件架构\n\n\n\n\n# 接口和类\n\n\n\n\n# 消息广播\n\n\n\n\n# 如何集成\n\n\n\n\n# 答疑解惑\n\n * 我的服务非常受欢迎，有数百万用户。这是否意味着内存中存储了数百万个桶？\n\n * * 可以瞬间增加很多桶，但是当桶没用的时候，可以删除桶\n\n * 有哪些失败的场景？\n\n * * 守护线程失败\n   * 网络分区，无法传播消息，每个主机将允许更多的请求\n\n * 我们是否需要一个自动配置管理服务？\n\n * 我有点担心同步问题。这不是瓶颈吗？\n\n * * 原则上来说 我们要重视并发安全\n   * 但是我们没有必要实现，会对性能造成影响\n\n * 当客户的请求被拒绝时，他们应该做什么？\n\n * * 重试（指数退避、抖动）\n\n\n# 总结\n\n",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"热点探查（Top k）",frontmatter:{title:"热点探查（Top k）",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/5dcb6b/"},regularPath:"/20.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/03.%E7%83%AD%E7%82%B9%E6%8E%A2%E6%9F%A5%EF%BC%88Top%20k%EF%BC%89.html",relativePath:"20.设计基础设施/01.设计基础设施/03.热点探查（Top k）.md",key:"v-6d051b22",path:"/pages/5dcb6b/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:313},{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:373},{level:2,title:"渐进设计",slug:"渐进设计",normalizedTitle:"渐进设计",charIndex:600},{level:3,title:"哈希表+单主机",slug:"哈希表-单主机",normalizedTitle:"哈希表+单主机",charIndex:609},{level:3,title:"哈希表+多主机",slug:"哈希表-多主机",normalizedTitle:"哈希表+多主机",charIndex:739},{level:3,title:"哈希表+多主机+分区",slug:"哈希表-多主机-分区",normalizedTitle:"哈希表+多主机+分区",charIndex:830},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:959},{level:2,title:"高层架构",slug:"高层架构",normalizedTitle:"高层架构",charIndex:1630},{level:2,title:"客户端检索数据",slug:"客户端检索数据",normalizedTitle:"客户端检索数据",charIndex:2400},{level:2,title:"解答疑惑",slug:"解答疑惑",normalizedTitle:"解答疑惑",charIndex:2485},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:5033}],headersStr:"概述 前置知识 需求 渐进设计 哈希表+单主机 哈希表+多主机 哈希表+多主机+分区 总结 高层架构 客户端检索数据 解答疑惑 参考资料",content:"# 概述\n\nTop k 问题解决方案的各种应用（Google/Twitter/YouTube 趋势、热门产品、波动性股票、DDoS 攻击预防）。\n\n在这样的规模下，数据库或分布式缓存不是一个选项。我们可能正在处理 1M RPS。如果我们使用 DB 来跟踪视图计数，首先写入/更新会非常慢，然后找到前 K 项需要扫描整个数据集。\n\n也许 MapReduce 可以提供帮助。但这还不够。我们需要尽可能接近实时地返回重磅统计数据。\n\n例如：\n\n * Calculate top 100 list for last\n * 1 min, 5 min, 15 mins, 60 mins etc.\n\n这使得这个问题成为流处理问题\n\n\n# 前置知识\n\nMapReduce\n\nCount-min sketch\n\n数据聚合原理\n\n合并 N 个排序列表问题\n\n\n# 需求\n\n功能\n\n * topK(k,startTime,endTime)\n\n非功能\n\n * 高扩展（随着数据量的增加而扩展：视频、推文、帖子等）\n * 高可用性（在硬件/网络故障中幸存，没有 SPOF）\n * 高性能（返回前100名列表需要几十毫秒，考虑到性能要求，这暗示了最终列表应该预先计算，我们应该避免在调用 top K API 时进行繁重的计算。）\n * 准确性（例如，通过使用数据采样，我们可能不会计算每个元素，而只计算一小部分事件）\n\n\n# 渐进设计\n\n\n# 哈希表+单主机\n\n * 在 hashmap 中保留传入事件列表的计数\n\n * * 按频率对 hashmap 中的条目列表进行排序，并返回前 K 个元素。时间 O(nLogN)\n   * 将元素放在大小为 K 的堆上。时间复杂度O(nLogK)\n\n\n\n\n\n\n# 哈希表+多主机\n\n * 如果您将所有 youtube 视频 ID 存储在您作为主机的内存中，内存将是一个问题\n\n\n\n虽然增加了吞吐量，但是有内存瓶颈，hash 表会越来越大\n\n\n# 哈希表+多主机+分区\n\n * 我们不会将所有哈希表数据从所有主机发送到存储主机。相反，我们会在每个主机上单独计算 topK 列表，最后我们需要在存储主机上合并这些排序的列表。\n\n\n\n数据分区可以不将所有数据存储到一个主机上，减少了每个主机内存的压力\n\n\n# 总结\n\n问题\n\n * 我们认为数据集是无界的，这就是为什么我们能够考虑将其划分为多个块。但流数据没有界限。它不断涌现。在这种情况下，处理器主机只能在一段时间内继续积累数据，在此之前它将耗尽内存。比如 1 分钟。\n\n * * 我们将 1 min 数据刷新到存储主机\n   * 存储主机存储每分钟的热点列表\n   * 我们故意丢失了有关非 topK 元素的所有信息。我们负担不起在内存中存储每个视频的信息。\n   * 但是，如果我们想找到过去 1 小时或过去 1 天的 topK，我们如何使用 60 个 1 分钟 list 来构建它呢。鉴于当前的方法，没有解决此问题的正确方法。要找到当天的 topK，我们需要全天的完整数据集。\n   * 需求冲突，保留完整的 1 天数据（以满足需求）或丢失它以负担存储。让我们把所有数据存储在磁盘上，因为它无法放入内存，并使用批处理框架来做 topK 列表。Map Reduce 架构将发挥作用\n\n * 每次引入数据分区时，我们都必须考虑数据复制，以便将每个分区的副本存储在多个节点上。我们需要考虑在集群中添加/删除新节点时重新平衡。我们需要处理热分区。\n\n解决方案\n\n在进入上面讨论的方法之前，让我们想一想是否有一个简单的解决方案来解决 topK 问题？\n\n权衡利弊权衡利弊，所有的选择，不过是权衡利弊罢了\n\n我们需要在此过程中做出牺牲。准确性就是牺牲\n\n我们要合理利用数据结构，这将帮助我们使用固定大小的内存计算 topK，但结果可能不是 100% 准确\n\nCount-Min Sketch！\n\n\n\n\n# 高层架构\n\n\n\n * API Gateway：连接到视频内容交付系统，该系统将提供视频请求\n\n * * 对于我们的用例，我们对 API 网关的 1 个功能感兴趣，即日志生成，其中记录对 API 的每次调用。通常这些日志用于监控、日志记录和审计。我们将使用这些日志来计算每个视频的观看次数。我们可能有一个后台进程，它从日志中读取数据，进行一些初始聚合并发送数据进行进一步处理。\n   * 为 API 网关服务上的缓冲区分配内存，读取日志行，并构建频率计数哈希表。此缓冲区的大小应有限，当缓冲区已满时，将刷新数据。如果缓冲区在某个时间段内未满，我们可以根据时间段进行 flush。\n   * 其他选项可以是动态聚合数据，而不写入日志文件。或者完全跳过 API 网关端的数据聚合，并将有关每个事件（正在查看的视频）的信息进一步发送以进行处理。评估每个选项的优缺点。\n   * 我们可以通过以紧凑的二进制格式（例如 Apache Avro）序列化数据来节省网络 IO 利用率，并让 CPU 付出代价。所有这些注意事项都取决于 API 网关主机上可用的资源，即内存、CPU、网络和磁盘 IO。\n\n * distributed messaging system：初始聚合数据将发送到分布式消息传递系统，如 Apache Kafka。\n\n * Fast path and Slow path\n\n * * 在 Fast path 中，我们将大致计算 topK hitter 的结果。结果将在几秒钟内提供。\n   * 在 slow path 中，我们将精确计算 topK hitter 的结果。几分钟/几小时内即可获得结果。\n   * 根据系统对系统时序的限制（是否需要近实时结果，或者延迟是否可以接受以实现精度），您应该选择任一路径。\n\n\n\n\n\n\n\n\n# 客户端检索数据\n\n\n\n合并 2 个不同的结果集来回答 API 调用并不准确，但这是一种权衡。您无法在任何时间聚合数据。您必须了解确切需要的东西，并据此进行构建。\n\n\n# 解答疑惑\n\n * 我们是否可以使用哈希映射，但每隔几秒钟将其内容（转换为堆后）刷新到存储中，而不是使用 CMS？\n\n * * 对于小规模，使用哈希映射是完全可以的。当规模增长时，哈希映射可能会变得太大（使用大量内存）。为了防止这种情况，我们可以对数据进行分区，以便只有所有数据的子集进入 Fast Processor 服务主机。但它使架构复杂化。CMS 的美妙之处在于它消耗有限的（定义的）内存，并且无需对数据进行分区。CMS 的缺点是它大约计算数字。权衡，权衡......\n\n * 我们如何将 count-min sketch 和 heap 存储到数据库中？如何设计 table 架构？\n\n * * Heap 只是一个一维数组。CMS 是一个二维数组。这意味着两者都可以很容易地序列化为字节数组。使用语言原生序列化 API 或备受推崇的序列化框架（Protobufs、Thrift、Avro）。我们可以将它们以这种形式存储在数据库中。\n\n * 虽然 CMS 是为了节省内存，但我们还有 n log k 时间来获得前 k，对吧？\n\n * * 是的。它是 O(nlogk)（用于堆）+ O(klogk)（用于对最终列表进行排序）。N 通常比 k 大得多。所以，O(nlogk) 是主导的。\n\n * 如果 CMS 只用于 1 min 计数，为什么我们不直接使用哈希表来计数呢？毕竟，数据集的大小不会无限增长。\n\n * * 对于中小规模，哈希表解决方案可能效果很好。但请记住，如果我们尝试创建一个需要为许多不同场景查找前 K 个列表的服务，则可能会有很多这样的哈希表，并且它不会很好地扩展。例如，最常喜欢/不喜欢的视频、观看次数最多（基于时间）的视频、评论次数最多的视频、视频打开期间异常数量最多的前 K 名等。类似的统计数据可以按渠道级别、每个国家/地区等进行计算。长话短说，我们可能需要使用我们的服务来计算许多不同的前 K 名列表\n\n * 如何合并两个 1 小时的 top k 列表，以获得 2 小时的 top k？\n\n * * 我们需要对相同标识符的值求和。换句话说，我们会汇总两个列表中相同视频的观看次数。并获取合并列表的前 K （通过排序或使用 Heap）。 [不过，这不一定是 100% 准确的结果]\n\n * 当存在您提到的不同情况时，CMS 如何工作......最赞/最不喜欢的视频。我们需要构建多个 CMS 吗？我们是否需要为每个类别指定哈希值？无论哪种方式，它们都需要更多的内存，就像哈希表一样。\n\n * * 正确。我们需要特定的 CMS 来计算不同的事件类型：视频观看次数、喜欢、不喜欢、提交评论等。\n\n * 关于慢路径，我对数据分区器感到困惑。我们是否可以删除第一个 Distribute Messaging System 和 data partitioner？API 网关将根据其分区直接向第二个 Distribute Messaging System 发送消息。例如，API 网关会将所有 B 消息发送到分区 1，将所有 A 消息发送到分区 2，将所有 C 消息发送到分区 3。为什么我们需要第一个 Distribute Messaging System 和data partitioner？如果我们使用 Kalfa 作为 Distribute Messaging System，我们可以只为一组消息类型创建一个主题。\n\n * * 在大规模（例如 YouTube 规模）的情况下，API Gateway 集群将处理大量请求。我假设这些是数千甚至数万台 CPU 密集型计算机。主要目标是提供视频内容并尽可能少地做 “其他” 事情。在这样的机器上，我们通常希望避免任何繁重的聚合或逻辑。我们能做的最简单的事情是将每个视频观看请求批处理在一起。我的意思是根本不做任何聚合。创建包含如下内容的单个消息：{A = 1， B = 1， C = 1}，并将其发送以进行进一步处理。在您提到的选项中，我们仍然需要在 API Gateway 端进行聚合。由于规模很大，我们无法承受每个视频观看请求向第二个 DMS 发送一条消息的后果。我的意思是我们不能有三条消息，比如：{A = 1}、{B = 1}、{C = 1}。如视频中所述，我们希望在每个下一个阶段降低请求率。\n\n * 我有一个关于快速路径的问题，似乎您将聚合后的 CMS 存储在存储系统中，但这足以计算前 k 个吗？我觉得我们需要有一个网站列表，并在某个地方维护一个大小为 k 的堆，以找出前 k 个。\n\n * * 你是对的。我们始终保留两种数据结构：一个 count-min sketch 和一个 Fast Processor 中的堆。我们使用 count-min sketch 进行计数，而 heap 存储前 k 个列表。在 Storage 服务中，我们也可以同时保留两者或仅保留堆。但是 heap 始终存在。\n\n * 所以总的来说，我们仍然需要存储 keys。。。Count-min Sketch 无需单独维护 Key 的计数，从而有助于节省成本...当必须找到前 k 个元素时，必须遍历每个键并使用 count-min sketch 来找到前 k 个元素......这种理解准确吗？\n\n * * 我们需要存储 keys ，但只需要存储其中的 K（或更多）。并非全部。\n   * 当每个 key 到来时，我们执行以下操作：\n * * * 将其添加到 count-min 草图中。\n     * 从 count-min 草图中获取密钥计数。\n     * 检查当前 key 是否在堆中。如果它出现在堆中，我们在那里更新它的 count 值。如果它不存在于堆中，我们检查堆是否已满。如果未满，我们将此键添加到堆中。如果 heap 已满，则检查最小 heap 元素并将其值与当前 key count 值进行比较。此时，我们可以删除最小元素并添加当前键（如果当前键计数 > 最小元素值）。\n * * 这样我们只保留预定义数量的 key。这保证了我们永远不会超过内存，因为 count-min sketch 和堆的大小都是有限的\n\n\n# 参考资料\n\nhttps://www.youtube.com/watch?v=kx-XDoPjoHw",normalizedContent:"# 概述\n\ntop k 问题解决方案的各种应用（google/twitter/youtube 趋势、热门产品、波动性股票、ddos 攻击预防）。\n\n在这样的规模下，数据库或分布式缓存不是一个选项。我们可能正在处理 1m rps。如果我们使用 db 来跟踪视图计数，首先写入/更新会非常慢，然后找到前 k 项需要扫描整个数据集。\n\n也许 mapreduce 可以提供帮助。但这还不够。我们需要尽可能接近实时地返回重磅统计数据。\n\n例如：\n\n * calculate top 100 list for last\n * 1 min, 5 min, 15 mins, 60 mins etc.\n\n这使得这个问题成为流处理问题\n\n\n# 前置知识\n\nmapreduce\n\ncount-min sketch\n\n数据聚合原理\n\n合并 n 个排序列表问题\n\n\n# 需求\n\n功能\n\n * topk(k,starttime,endtime)\n\n非功能\n\n * 高扩展（随着数据量的增加而扩展：视频、推文、帖子等）\n * 高可用性（在硬件/网络故障中幸存，没有 spof）\n * 高性能（返回前100名列表需要几十毫秒，考虑到性能要求，这暗示了最终列表应该预先计算，我们应该避免在调用 top k api 时进行繁重的计算。）\n * 准确性（例如，通过使用数据采样，我们可能不会计算每个元素，而只计算一小部分事件）\n\n\n# 渐进设计\n\n\n# 哈希表+单主机\n\n * 在 hashmap 中保留传入事件列表的计数\n\n * * 按频率对 hashmap 中的条目列表进行排序，并返回前 k 个元素。时间 o(nlogn)\n   * 将元素放在大小为 k 的堆上。时间复杂度o(nlogk)\n\n\n\n\n\n\n# 哈希表+多主机\n\n * 如果您将所有 youtube 视频 id 存储在您作为主机的内存中，内存将是一个问题\n\n\n\n虽然增加了吞吐量，但是有内存瓶颈，hash 表会越来越大\n\n\n# 哈希表+多主机+分区\n\n * 我们不会将所有哈希表数据从所有主机发送到存储主机。相反，我们会在每个主机上单独计算 topk 列表，最后我们需要在存储主机上合并这些排序的列表。\n\n\n\n数据分区可以不将所有数据存储到一个主机上，减少了每个主机内存的压力\n\n\n# 总结\n\n问题\n\n * 我们认为数据集是无界的，这就是为什么我们能够考虑将其划分为多个块。但流数据没有界限。它不断涌现。在这种情况下，处理器主机只能在一段时间内继续积累数据，在此之前它将耗尽内存。比如 1 分钟。\n\n * * 我们将 1 min 数据刷新到存储主机\n   * 存储主机存储每分钟的热点列表\n   * 我们故意丢失了有关非 topk 元素的所有信息。我们负担不起在内存中存储每个视频的信息。\n   * 但是，如果我们想找到过去 1 小时或过去 1 天的 topk，我们如何使用 60 个 1 分钟 list 来构建它呢。鉴于当前的方法，没有解决此问题的正确方法。要找到当天的 topk，我们需要全天的完整数据集。\n   * 需求冲突，保留完整的 1 天数据（以满足需求）或丢失它以负担存储。让我们把所有数据存储在磁盘上，因为它无法放入内存，并使用批处理框架来做 topk 列表。map reduce 架构将发挥作用\n\n * 每次引入数据分区时，我们都必须考虑数据复制，以便将每个分区的副本存储在多个节点上。我们需要考虑在集群中添加/删除新节点时重新平衡。我们需要处理热分区。\n\n解决方案\n\n在进入上面讨论的方法之前，让我们想一想是否有一个简单的解决方案来解决 topk 问题？\n\n权衡利弊权衡利弊，所有的选择，不过是权衡利弊罢了\n\n我们需要在此过程中做出牺牲。准确性就是牺牲\n\n我们要合理利用数据结构，这将帮助我们使用固定大小的内存计算 topk，但结果可能不是 100% 准确\n\ncount-min sketch！\n\n\n\n\n# 高层架构\n\n\n\n * api gateway：连接到视频内容交付系统，该系统将提供视频请求\n\n * * 对于我们的用例，我们对 api 网关的 1 个功能感兴趣，即日志生成，其中记录对 api 的每次调用。通常这些日志用于监控、日志记录和审计。我们将使用这些日志来计算每个视频的观看次数。我们可能有一个后台进程，它从日志中读取数据，进行一些初始聚合并发送数据进行进一步处理。\n   * 为 api 网关服务上的缓冲区分配内存，读取日志行，并构建频率计数哈希表。此缓冲区的大小应有限，当缓冲区已满时，将刷新数据。如果缓冲区在某个时间段内未满，我们可以根据时间段进行 flush。\n   * 其他选项可以是动态聚合数据，而不写入日志文件。或者完全跳过 api 网关端的数据聚合，并将有关每个事件（正在查看的视频）的信息进一步发送以进行处理。评估每个选项的优缺点。\n   * 我们可以通过以紧凑的二进制格式（例如 apache avro）序列化数据来节省网络 io 利用率，并让 cpu 付出代价。所有这些注意事项都取决于 api 网关主机上可用的资源，即内存、cpu、网络和磁盘 io。\n\n * distributed messaging system：初始聚合数据将发送到分布式消息传递系统，如 apache kafka。\n\n * fast path and slow path\n\n * * 在 fast path 中，我们将大致计算 topk hitter 的结果。结果将在几秒钟内提供。\n   * 在 slow path 中，我们将精确计算 topk hitter 的结果。几分钟/几小时内即可获得结果。\n   * 根据系统对系统时序的限制（是否需要近实时结果，或者延迟是否可以接受以实现精度），您应该选择任一路径。\n\n\n\n\n\n\n\n\n# 客户端检索数据\n\n\n\n合并 2 个不同的结果集来回答 api 调用并不准确，但这是一种权衡。您无法在任何时间聚合数据。您必须了解确切需要的东西，并据此进行构建。\n\n\n# 解答疑惑\n\n * 我们是否可以使用哈希映射，但每隔几秒钟将其内容（转换为堆后）刷新到存储中，而不是使用 cms？\n\n * * 对于小规模，使用哈希映射是完全可以的。当规模增长时，哈希映射可能会变得太大（使用大量内存）。为了防止这种情况，我们可以对数据进行分区，以便只有所有数据的子集进入 fast processor 服务主机。但它使架构复杂化。cms 的美妙之处在于它消耗有限的（定义的）内存，并且无需对数据进行分区。cms 的缺点是它大约计算数字。权衡，权衡......\n\n * 我们如何将 count-min sketch 和 heap 存储到数据库中？如何设计 table 架构？\n\n * * heap 只是一个一维数组。cms 是一个二维数组。这意味着两者都可以很容易地序列化为字节数组。使用语言原生序列化 api 或备受推崇的序列化框架（protobufs、thrift、avro）。我们可以将它们以这种形式存储在数据库中。\n\n * 虽然 cms 是为了节省内存，但我们还有 n log k 时间来获得前 k，对吧？\n\n * * 是的。它是 o(nlogk)（用于堆）+ o(klogk)（用于对最终列表进行排序）。n 通常比 k 大得多。所以，o(nlogk) 是主导的。\n\n * 如果 cms 只用于 1 min 计数，为什么我们不直接使用哈希表来计数呢？毕竟，数据集的大小不会无限增长。\n\n * * 对于中小规模，哈希表解决方案可能效果很好。但请记住，如果我们尝试创建一个需要为许多不同场景查找前 k 个列表的服务，则可能会有很多这样的哈希表，并且它不会很好地扩展。例如，最常喜欢/不喜欢的视频、观看次数最多（基于时间）的视频、评论次数最多的视频、视频打开期间异常数量最多的前 k 名等。类似的统计数据可以按渠道级别、每个国家/地区等进行计算。长话短说，我们可能需要使用我们的服务来计算许多不同的前 k 名列表\n\n * 如何合并两个 1 小时的 top k 列表，以获得 2 小时的 top k？\n\n * * 我们需要对相同标识符的值求和。换句话说，我们会汇总两个列表中相同视频的观看次数。并获取合并列表的前 k （通过排序或使用 heap）。 [不过，这不一定是 100% 准确的结果]\n\n * 当存在您提到的不同情况时，cms 如何工作......最赞/最不喜欢的视频。我们需要构建多个 cms 吗？我们是否需要为每个类别指定哈希值？无论哪种方式，它们都需要更多的内存，就像哈希表一样。\n\n * * 正确。我们需要特定的 cms 来计算不同的事件类型：视频观看次数、喜欢、不喜欢、提交评论等。\n\n * 关于慢路径，我对数据分区器感到困惑。我们是否可以删除第一个 distribute messaging system 和 data partitioner？api 网关将根据其分区直接向第二个 distribute messaging system 发送消息。例如，api 网关会将所有 b 消息发送到分区 1，将所有 a 消息发送到分区 2，将所有 c 消息发送到分区 3。为什么我们需要第一个 distribute messaging system 和data partitioner？如果我们使用 kalfa 作为 distribute messaging system，我们可以只为一组消息类型创建一个主题。\n\n * * 在大规模（例如 youtube 规模）的情况下，api gateway 集群将处理大量请求。我假设这些是数千甚至数万台 cpu 密集型计算机。主要目标是提供视频内容并尽可能少地做 “其他” 事情。在这样的机器上，我们通常希望避免任何繁重的聚合或逻辑。我们能做的最简单的事情是将每个视频观看请求批处理在一起。我的意思是根本不做任何聚合。创建包含如下内容的单个消息：{a = 1， b = 1， c = 1}，并将其发送以进行进一步处理。在您提到的选项中，我们仍然需要在 api gateway 端进行聚合。由于规模很大，我们无法承受每个视频观看请求向第二个 dms 发送一条消息的后果。我的意思是我们不能有三条消息，比如：{a = 1}、{b = 1}、{c = 1}。如视频中所述，我们希望在每个下一个阶段降低请求率。\n\n * 我有一个关于快速路径的问题，似乎您将聚合后的 cms 存储在存储系统中，但这足以计算前 k 个吗？我觉得我们需要有一个网站列表，并在某个地方维护一个大小为 k 的堆，以找出前 k 个。\n\n * * 你是对的。我们始终保留两种数据结构：一个 count-min sketch 和一个 fast processor 中的堆。我们使用 count-min sketch 进行计数，而 heap 存储前 k 个列表。在 storage 服务中，我们也可以同时保留两者或仅保留堆。但是 heap 始终存在。\n\n * 所以总的来说，我们仍然需要存储 keys。。。count-min sketch 无需单独维护 key 的计数，从而有助于节省成本...当必须找到前 k 个元素时，必须遍历每个键并使用 count-min sketch 来找到前 k 个元素......这种理解准确吗？\n\n * * 我们需要存储 keys ，但只需要存储其中的 k（或更多）。并非全部。\n   * 当每个 key 到来时，我们执行以下操作：\n * * * 将其添加到 count-min 草图中。\n     * 从 count-min 草图中获取密钥计数。\n     * 检查当前 key 是否在堆中。如果它出现在堆中，我们在那里更新它的 count 值。如果它不存在于堆中，我们检查堆是否已满。如果未满，我们将此键添加到堆中。如果 heap 已满，则检查最小 heap 元素并将其值与当前 key count 值进行比较。此时，我们可以删除最小元素并添加当前键（如果当前键计数 > 最小元素值）。\n * * 这样我们只保留预定义数量的 key。这保证了我们永远不会超过内存，因为 count-min sketch 和堆的大小都是有限的\n\n\n# 参考资料\n\nhttps://www.youtube.com/watch?v=kx-xdopjohw",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"消息队列",frontmatter:{title:"消息队列",date:"2024-09-14T16:43:00.000Z",permalink:"/pages/567090/"},regularPath:"/20.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/04.%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97.html",relativePath:"20.设计基础设施/01.设计基础设施/04.消息队列.md",key:"v-85ed8394",path:"/pages/567090/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:2},{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:11},{level:2,title:"高层抽象",slug:"高层抽象",normalizedTitle:"高层抽象",charIndex:78},{level:2,title:"虚拟IP和负载均衡",slug:"虚拟ip和负载均衡",normalizedTitle:"虚拟ip和负载均衡",charIndex:89},{level:2,title:"前端服务",slug:"前端服务",normalizedTitle:"前端服务",charIndex:105},{level:2,title:"元数据服务",slug:"元数据服务",normalizedTitle:"元数据服务",charIndex:116},{level:2,title:"后端服务",slug:"后端服务",normalizedTitle:"后端服务",charIndex:128},{level:2,title:"还有什么重要的？",slug:"还有什么重要的",normalizedTitle:"还有什么重要的？",charIndex:145},{level:3,title:"队列的创建与删除",slug:"队列的创建与删除",normalizedTitle:"队列的创建与删除",charIndex:160},{level:3,title:"消息的删除",slug:"消息的删除",normalizedTitle:"消息的删除",charIndex:173},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:183}],headersStr:"概述 需求 高层抽象 虚拟IP和负载均衡 前端服务 元数据服务 后端服务 还有什么重要的？ 队列的创建与删除 消息的删除 总结",content:"# 概述\n\n\n\n\n# 需求\n\n功能性\n\n * sendMessage(messageBody)\n * receiveMessage()\n\n非功能性\n\n\n# 高层抽象\n\n\n\n\n# 虚拟IP和负载均衡\n\n\n\n\n# 前端服务\n\n\n\n\n# 元数据服务\n\n\n\n\n# 后端服务\n\n\n\n\n\n\n\n\n\n\n# 还有什么重要的？\n\n\n\n\n# 队列的创建与删除\n\n\n# 消息的删除\n\n\n# 总结\n\n",normalizedContent:"# 概述\n\n\n\n\n# 需求\n\n功能性\n\n * sendmessage(messagebody)\n * receivemessage()\n\n非功能性\n\n\n# 高层抽象\n\n\n\n\n# 虚拟ip和负载均衡\n\n\n\n\n# 前端服务\n\n\n\n\n# 元数据服务\n\n\n\n\n# 后端服务\n\n\n\n\n\n\n\n\n\n\n# 还有什么重要的？\n\n\n\n\n# 队列的创建与删除\n\n\n# 消息的删除\n\n\n# 总结\n\n",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"动态线程池",frontmatter:{title:"动态线程池",date:"2024-09-14T23:28:32.000Z",permalink:"/pages/d81a42/"},regularPath:"/20.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/06.%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0.html",relativePath:"20.设计基础设施/01.设计基础设施/06.动态线程池.md",key:"v-03f67d9c",path:"/pages/d81a42/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"服务通知",frontmatter:{title:"服务通知",date:"2024-09-14T16:43:28.000Z",permalink:"/pages/8416e6/"},regularPath:"/20.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/01.%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/05.%E8%AE%A2%E9%98%85%E5%8F%91%E5%B8%83.html",relativePath:"20.设计基础设施/01.设计基础设施/05.订阅发布.md",key:"v-20acbeab",path:"/pages/8416e6/",headers:[{level:2,title:"需求",slug:"需求",normalizedTitle:"需求",charIndex:2},{level:2,title:"高层架构",slug:"高层架构",normalizedTitle:"高层架构",charIndex:142},{level:2,title:"客户端主机",slug:"客户端主机",normalizedTitle:"客户端主机",charIndex:153}],headersStr:"需求 高层架构 客户端主机",content:"# 需求\n\n功能性\n\n * createTopic(topicname)\n * publish(topicName,message)\n * subscribe(topicName,endpoint)\n\n非功能性\n\n * 高可扩展性\n * 高可用性\n * 高性能\n * 持久性\n\n\n# 高层架构\n\n\n\n\n# 客户端主机",normalizedContent:"# 需求\n\n功能性\n\n * createtopic(topicname)\n * publish(topicname,message)\n * subscribe(topicname,endpoint)\n\n非功能性\n\n * 高可扩展性\n * 高可用性\n * 高性能\n * 持久性\n\n\n# 高层架构\n\n\n\n\n# 客户端主机",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"设计 微信",frontmatter:{title:"设计 微信",date:"2024-09-14T02:08:51.000Z",permalink:"/pages/a95d7d/"},regularPath:"/25.%E8%AE%BE%E8%AE%A1%E7%83%AD%E9%97%A8%E5%BA%94%E7%94%A8/01.%E8%AE%BE%E8%AE%A1%E7%83%AD%E9%97%A8%E5%BA%94%E7%94%A8/01.%E8%AE%BE%E8%AE%A1%20%E5%BE%AE%E4%BF%A1.html",relativePath:"25.设计热门应用/01.设计热门应用/01.设计 微信.md",key:"v-2e62ec57",path:"/pages/a95d7d/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"设计Twitter",frontmatter:{title:"设计Twitter",date:"2024-09-14T02:08:51.000Z",permalink:"/pages/90ad66/"},regularPath:"/25.%E8%AE%BE%E8%AE%A1%E7%83%AD%E9%97%A8%E5%BA%94%E7%94%A8/01.%E8%AE%BE%E8%AE%A1%E7%83%AD%E9%97%A8%E5%BA%94%E7%94%A8/02.%E8%AE%BE%E8%AE%A1Twitter.html",relativePath:"25.设计热门应用/01.设计热门应用/02.设计Twitter.md",key:"v-c64b35a2",path:"/pages/90ad66/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"双写一致性",frontmatter:{title:"双写一致性",date:"2024-09-14T16:50:17.000Z",permalink:"/pages/def08a/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.html",relativePath:"30.经典场景设计/01.经典场景设计/01.双写一致性.md",key:"v-68adaa3b",path:"/pages/def08a/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"引入缓存提高性能",slug:"引入缓存提高性能",normalizedTitle:"引入缓存提高性能",charIndex:136},{level:2,title:"缓存利用率和一致性问题",slug:"缓存利用率和一致性问题",normalizedTitle:"缓存利用率和一致性问题",charIndex:841},{level:3,title:"先来看第一个问题，如何提高缓存利用率？",slug:"先来看第一个问题-如何提高缓存利用率",normalizedTitle:"先来看第一个问题，如何提高缓存利用率？",charIndex:857},{level:2,title:"并发引发的一致性问题",slug:"并发引发的一致性问题",normalizedTitle:"并发引发的一致性问题",charIndex:1822},{level:2,title:"删除缓存可以保证一致性吗？",slug:"删除缓存可以保证一致性吗",normalizedTitle:"删除缓存可以保证一致性吗？",charIndex:2434},{level:2,title:"如何保证两步都执行成功？",slug:"如何保证两步都执行成功",normalizedTitle:"如何保证两步都执行成功？",charIndex:3376},{level:2,title:"主从库延迟和延迟双删问题",slug:"主从库延迟和延迟双删问题",normalizedTitle:"主从库延迟和延迟双删问题",charIndex:4973},{level:2,title:"可以做到强一致吗？",slug:"可以做到强一致吗",normalizedTitle:"可以做到强一致吗？",charIndex:6103},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6604},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:7140}],headersStr:"前言 引入缓存提高性能 缓存利用率和一致性问题 先来看第一个问题，如何提高缓存利用率？ 并发引发的一致性问题 删除缓存可以保证一致性吗？ 如何保证两步都执行成功？ 主从库延迟和延迟双删问题 可以做到强一致吗？ 总结 参考文献",content:"# 前言\n\n感觉这是一个很宏大的命题\n\n我个人觉得引入缓存之后，如果为了短时间的不一致性问题，选择让系统设计变得更加复杂的话，完全没必要\n\n可以将请求分流，要保证强一致的请求走数据库，能忍受不一致的请求走缓存\n\necho 带着你沿着场景渐进式的了解双写一致性问题\n\n\n# 引入缓存提高性能\n\n我们从最简单的场景开始讲起。\n\n如果你的业务处于起步阶段，流量非常小，那无论是读请求还是写请求，直接操作数据库即可，这时你的架构模型是这样的：\n\n\n\n但随着业务量的增长，你的项目请求量越来越大，这时如果每次都从数据库中读数据，那肯定会有性能问题。\n\n这个阶段通常的做法是，引入「缓存」来提高读性能，架构模型就变成了这样：\n\n\n\n当下优秀的缓存中间件，当属 Redis 莫属，它不仅性能非常高，还提供了很多友好的数据类型，可以很好地满足我们的业务需求。\n\n但引入缓存之后，你就会面临一个问题：之前数据只存在数据库中，现在要放到缓存中读取，具体要怎么存呢？\n\n有三种方案：\n\n * Cache Aside Pattern（旁路缓存模式）\n * Read/Write Through Pattern（读写穿透）\n * Write Behind Pattern（异步缓存写入）\n\n最简单直接的方案是「全量数据刷到缓存中」：\n\n * 数据库的数据，全量刷入缓存（不设置失效时间）\n * 写请求只更新数据库，不更新缓存\n * 启动一个定时任务，定时把数据库的数据，更新到缓存中\n\n\n\n这个方案的优点是，所有读请求都可以直接「命中」缓存，不需要再查数据库，性能非常高。\n\n但缺点也很明显，有 2 个问题：\n\n 1. 缓存利用率低：不经常访问的数据，还一直留在缓存中\n 2. 数据不一致：因为是「定时」刷新缓存，缓存和数据库存在不一致（取决于定时任务的执行频率）\n\n所以，这种方案一般更适合业务「体量小」，且对数据一致性要求不高的业务场景。\n\n那如果我们的业务体量很大，怎么解决这 2 个问题呢？\n\n\n# 缓存利用率和一致性问题\n\n\n# 先来看第一个问题，如何提高缓存利用率？\n\n想要缓存利用率「最大化」，我们很容易想到的方案是，缓存中只保留最近访问的「热数据」。但具体要怎么做呢？\n\n我们可以这样优化：\n\n * 写请求依旧只写数据库\n * 读请求先读缓存，如果缓存不存在，则从数据库读取，并重建缓存\n * 同时，写入缓存中的数据，都设置失效时间\n\n\n\n这样一来，缓存中不经常访问的数据，随着时间的推移，都会逐渐「过期」淘汰掉，最终缓存中保留的，都是经常被访问的「热数据」，缓存利用率得以最大化。\n\n再来看数据一致性问题。\n\n要想保证缓存和数据库「实时」一致，那就不能再用定时任务刷新缓存了。\n\n所以，当数据发生更新时，我们不仅要操作数据库，还要一并操作缓存。具体操作就是，修改一条数据时，不仅要更新数据库，也要连带缓存一起更新。\n\n但数据库和缓存都更新，又存在先后问题，那对应的方案就有 2 个：\n\n 1. 先更新缓存，后更新数据库\n 2. 先更新数据库，后更新缓存\n\n哪个方案更好呢？\n\n先不考虑并发问题，正常情况下，无论谁先谁后，都可以让两者保持一致，但现在我们需要重点考虑「异常」情况。\n\n因为操作分为两步，那么就很有可能存在「第一步成功、第二步失败」的情况发生。\n\n这 2 种方案我们一个个来分析。\n\n1) 先更新缓存，后更新数据库\n\n如果缓存更新成功了，但数据库更新失败，那么此时缓存中是最新值，但数据库中是「旧值」。\n\n虽然此时读请求可以命中缓存，拿到正确的值，但是，一旦缓存「失效」，就会从数据库中读取到「旧值」，重建缓存也是这个旧值。\n\n这时用户会发现自己之前修改的数据又「变回去」了，对业务造成影响。\n\n2) 先更新数据库，后更新缓存\n\n如果数据库更新成功了，但缓存更新失败，那么此时数据库中是最新值，缓存中是「旧值」。\n\n之后的读请求读到的都是旧数据，只有当缓存「失效」后，才能从数据库中得到正确的值。\n\n这时用户会发现，自己刚刚修改了数据，但却看不到变更，一段时间过后，数据才变更过来，对业务也会有影响。\n\n可见，无论谁先谁后，但凡后者发生异常，就会对业务造成影响。那怎么解决这个问题呢？\n\n别急，后面我会详细给出对应的解决方案。\n\n我们继续分析，除了操作失败问题，还有什么场景会影响数据一致性？\n\n这里我们还需要重点关注：并发问题。\n\n\n# 并发引发的一致性问题\n\n假设我们采用**「先更新数据库，再更新缓存」**的方案，并且两步都可以「成功执行」的前提下，如果存在并发，情况会是怎样的呢？\n\n有线程 A 和线程 B 两个线程，需要更新「同一条」数据，会发生这样的场景：\n\n 1. 线程 A 更新数据库（X = 1）\n 2. 线程 B 更新数据库（X = 2）\n 3. 线程 B 更新缓存（X = 2）\n 4. 线程 A 更新缓存（X = 1）\n\n最终 X 的值在缓存中是 1，在数据库中是 2，发生不一致。\n\n也就是说，A 虽然先于 B 发生，但 B 操作数据库和缓存的时间，却要比 A 的时间短，执行时序发生「错乱」，最终这条数据结果是不符合预期的。\n\n> 同样地，采用「先更新缓存，再更新数据库」的方案，也会有类似问题，这里不再详述。\n\n除此之外，我们从「缓存利用率」的角度来评估这个方案，也是不太推荐的。\n\n这是因为每次数据发生变更，都「无脑」更新缓存，但是缓存中的数据不一定会被「马上读取」，这就会导致缓存中可能存放了很多不常访问的数据，浪费缓存资源。\n\n而且很多情况下，写到缓存中的值，并不是与数据库中的值一一对应的，很有可能是先查询数据库，再经过一系列「计算」得出一个值，才把这个值才写到缓存中。\n\n由此可见，这种「更新数据库 + 更新缓存」的方案，不仅缓存利用率不高，还会造成机器性能的浪费。\n\n所以此时我们需要考虑另外一种方案：删除缓存。\n\n\n# 删除缓存可以保证一致性吗？\n\n删除缓存对应的方案也有 2 种：\n\n 1. 先删除缓存，后更新数据库\n 2. 先更新数据库，后删除缓存\n\n经过前面的分析我们已经得知，但凡「第二步」操作失败，都会导致数据不一致。\n\n这里我不再详述具体场景，你可以按照前面的思路推演一下，就可以看到依旧存在数据不一致的情况。\n\n这里我们重点来看「并发」问题。\n\n1) 先删除缓存，后更新数据库\n\n如果有 2 个线程要并发「读写」数据，可能会发生以下场景：\n\n 1. 线程 A 要更新 X = 2（原值 X = 1）\n 2. 线程 A 先删除缓存\n 3. 线程 B 读缓存，发现不存在，从数据库中读取到旧值（X = 1）\n 4. 线程 A 将新值写入数据库（X = 2）\n 5. 线程 B 将旧值写入缓存（X = 1）\n\n最终 X 的值在缓存中是 1（旧值），在数据库中是 2（新值），发生不一致。\n\n可见，先删除缓存，后更新数据库，当发生「读+写」并发时，还是存在数据不一致的情况。\n\n2) 先更新数据库，后删除缓存\n\n依旧是 2 个线程并发「读写」数据：\n\n 1. 缓存中 X 不存在（数据库 X = 1）\n 2. 线程 A 读取数据库，得到旧值（X = 1）\n 3. 线程 B 更新数据库（X = 2)\n 4. 线程 B 删除缓存\n 5. 线程 A 将旧值写入缓存（X = 1）\n\n最终 X 的值在缓存中是 1（旧值），在数据库中是 2（新值），也发生不一致。\n\n这种情况「理论」来说是可能发生的，但实际真的有可能发生吗？\n\n其实概率「很低」，这是因为它必须满足 3 个条件：\n\n 1. 缓存刚好已失效\n 2. 读请求 + 写请求并发\n 3. 更新数据库 + 删除缓存的时间（步骤 3-4），要比读数据库 + 写缓存时间短（步骤 2 和 5）\n\n仔细想一下，条件 3 发生的概率其实是非常低的。\n\n因为写数据库一般会先「加锁」，所以写数据库，通常是要比读数据库的时间更长的。\n\n这么来看，「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的。\n\n所以，我们应该采用这种方案，来操作数据库和缓存。\n\n好，解决了并发问题，我们继续来看前面遗留的，第二步执行「失败」导致数据不一致的问题。\n\n\n# 如何保证两步都执行成功？\n\n前面我们分析到，无论是更新缓存还是删除缓存，只要第二步发生失败，那么就会导致数据库和缓存不一致。\n\n前面我们分析到，无论是更新缓存还是删除缓存，只要第二步发生失败，那么就会导致数据库和缓存不一致。\n\n保证第二步成功执行，就是解决问题的关键。\n\n想一下，程序在执行过程中发生异常，最简单的解决办法是什么？\n\n答案是：重试。\n\n是的，其实这里我们也可以这样做。\n\n无论是先操作缓存，还是先操作数据库，但凡后者执行失败了，我们就可以发起重试，尽可能地去做「补偿」。\n\n那这是不是意味着，只要执行失败，我们「无脑重试」就可以了呢？\n\n答案是否定的。现实情况往往没有想的这么简单，失败后立即重试的问题在于：\n\n * 立即重试很大概率「还会失败」\n * 「重试次数」设置多少才合理？\n * 重试会一直「占用」这个线程资源，无法服务其它客户端请求\n\n看到了么，虽然我们想通过重试的方式解决问题，但这种「同步」重试的方案依旧不严谨。\n\n那更好的方案应该怎么做？\n\n答案是：异步重试。什么是异步重试？\n\n其实就是把重试请求写到「消息队列」中，然后由专门的消费者来重试，直到成功。\n\n或者更直接的做法，为了避免第二步执行失败，我们可以把操作缓存这一步，直接放到消息队列中，由消费者来操作缓存。\n\n到这里你可能会问，写消息队列也有可能会失败啊？而且，引入消息队列，这又增加了更多的维护成本，这样做值得吗？\n\n这个问题很好，但我们思考这样一个问题：如果在执行失败的线程中一直重试，还没等执行成功，此时如果项目「重启」了，那这次重试请求也就「丢失」了，那这条数据就一直不一致了。\n\n所以，这里我们必须把重试或第二步操作放到另一个「服务」中，这个服务用「消息队列」最为合适。这是因为消息队列的特性，正好符合我们的需求：\n\n * 消息队列保证可靠性：写到队列中的消息，成功消费之前不会丢失（重启项目也不担心）\n * 消息队列保证消息成功投递：下游从队列拉取消息，成功消费后才会删除消息，否则还会继续投递消息给消费者（符合我们重试的场景）\n\n至于写队列失败和消息队列的维护成本问题：\n\n * 写队列失败：操作缓存和写消息队列，「同时失败」的概率其实是很小的\n * 维护成本：我们项目中一般都会用到消息队列，维护成本并没有新增很多\n\n所以，引入消息队列来解决这个问题，是比较合适的。这时架构模型就变成了这样：\n\n那如果你确实不想在应用中去写消息队列，是否有更简单的方案，同时又可以保证一致性呢？\n\n方案还是有的，这就是近几年比较流行的解决方案：订阅数据库变更日志，再操作缓存。\n\n具体来讲就是，我们的业务应用在修改数据时，「只需」修改数据库，无需操作缓存。\n\n那什么时候操作缓存呢？这就和数据库的「变更日志」有关了。\n\n拿 MySQL 举例，当一条数据发生修改时，MySQL 就会产生一条变更日志（Binlog），我们可以订阅这个日志，拿到具体操作的数据，然后再根据这条数据，去删除对应的缓存。\n\n订阅变更日志，目前也有了比较成熟的开源中间件，例如阿里的 canal，使用这种方案的优点在于：\n\n * 无需考虑写消息队列失败情况：只要写 MySQL 成功，Binlog 肯定会有\n * 自动投递到下游队列：canal 自动把数据库变更日志「投递」给下游的消息队列\n\n当然，与此同时，我们需要投入精力去维护 canal 的高可用和稳定性。\n\n> 如果你有留意观察很多数据库的特性，就会发现其实很多数据库都逐渐开始提供「订阅变更日志」的功能了，相信不远的将来，我们就不用通过中间件来拉取日志，自己写程序就可以订阅变更日志了，这样可以进一步简化流程。\n\n至此，我们可以得出结论，想要保证数据库和缓存一致性，推荐采用「先更新数据库，再删除缓存」方案，并配合「消息队列」或「订阅变更日志」的方式来做。\n\n\n# 主从库延迟和延迟双删问题\n\n到这里，还有 2 个问题，是我们没有重点分析过的。\n\n第一个问题，还记得前面讲到的「先删除缓存，再更新数据库」方案，导致不一致的场景么？\n\n这里我再把例子拿过来让你复习一下：\n\n2 个线程要并发「读写」数据，可能会发生以下场景：\n\n 1. 线程 A 要更新 X = 2（原值 X = 1）\n 2. 线程 A 先删除缓存\n 3. 线程 B 读缓存，发现不存在，从数据库中读取到旧值（X = 1）\n 4. 线程 A 将新值写入数据库（X = 2）\n 5. 线程 B 将旧值写入缓存（X = 1）\n\n最终 X 的值在缓存中是 1（旧值），在数据库中是 2（新值），发生不一致。\n\n第二个问题：是关于「读写分离 + 主从复制延迟」情况下，缓存和数据库一致性的问题。\n\n在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」其实也会导致不一致：\n\n 1. 线程 A 更新主库 X = 2（原值 X = 1）\n 2. 线程 A 删除缓存\n 3. 线程 B 查询缓存，没有命中，查询「从库」得到旧值（从库 X = 1）\n 4. 从库「同步」完成（主从库 X = 2）\n 5. 线程 B 将「旧值」写入缓存（X = 1）\n\n最终 X 的值在缓存中是 1（旧值），在主从库中是 2（新值），也发生不一致。\n\n看到了么？这 2 个问题的核心在于：缓存都被回种了「旧值」。\n\n那怎么解决这类问题呢？\n\n最有效的办法就是，把缓存删掉。\n\n但是，不能立即删，而是需要「延迟删」，这就是业界给出的方案：缓存延迟双删策略。\n\n按照延时双删策略，这 2 个问题的解决方案是这样的：\n\n解决第一个问题：在线程 A 删除缓存、更新完数据库之后，先「休眠一会」，再「删除」一次缓存。\n\n解决第二个问题：线程 A 可以生成一条「延时消息」，写到消息队列中，消费者延时「删除」缓存。\n\n这两个方案的目的，都是为了把缓存清掉，这样一来，下次就可以从数据库读取到最新值，写入缓存。\n\n但问题来了，这个「延迟删除」缓存，延迟时间到底设置要多久呢？\n\n * 问题1：延迟时间要大于「主从复制」的延迟时间\n * 问题2：延迟时间要大于线程 B 读取数据库 + 写入缓存的时间\n\n但是，这个时间在分布式和高并发场景下，其实是很难评估的\n\n很多时候，我们都是凭借经验大致估算这个延迟时间，例如延迟 1-5s，只能尽可能地降低不一致的概率。\n\n所以你看，采用这种方案，也只是尽可能保证一致性而已，极端情况下，还是有可能发生不一致。\n\n所以实际使用中，我还是建议你采用「先更新数据库，再删除缓存」的方案，同时，要尽可能地保证「主从复制」不要有太大延迟，降低出问题的概率。\n\n\n# 可以做到强一致吗？\n\n看到这里你可能会想，这些方案还是不够完美，我就想让缓存和数据库「强一致」，到底能不能做到呢？\n\n其实很难。\n\n要想做到强一致，最常见的方案是 2PC、3PC、Paxos、Raft 这类一致性协议，但它们的性能往往比较差，而且这些方案也比较复杂，还要考虑各种容错问题。\n\n相反，这时我们换个角度思考一下，我们引入缓存的目的是什么？\n\n没错，性能。\n\n一旦我们决定使用缓存，那必然要面临一致性问题。性能和一致性就像天平的两端，无法做到都满足要求。\n\n而且，就拿我们前面讲到的方案来说，当操作数据库和缓存完成之前，只要有其它请求可以进来，都有可能查到「中间状态」的数据。\n\n所以如果非要追求强一致，那必须要求所有更新操作完成之前期间，不能有「任何请求」进来。\n\n虽然我们可以通过加「分布锁」的方式来实现，但我们要付出的代价，很可能会超过引入缓存带来的性能提升。\n\n所以，既然决定使用缓存，就必须容忍「一致性」问题，我们只能尽可能地去降低问题出现的概率。\n\n同时我们也要知道，缓存都是有「失效时间」的，就算在这期间存在短期不一致，我们依旧有失效时间来兜底，这样也能达到最终一致。\n\n\n# 总结\n\n好了，总结一下这篇文章的重点。\n\n 1. 想要提高应用的性能，可以引入「缓存」来解决\n 2. 引入缓存后，需要考虑缓存和数据库一致性问题，可选的方案有：「更新数据库 + 更新缓存」、「更新数据库 + 删除缓存」\n 3. 更新数据库 + 更新缓存方案，在「并发」场景下无法保证缓存和数据一致性，且存在「缓存资源浪费」和「机器性能浪费」的情况发生\n 4. 在更新数据库 + 删除缓存的方案中，「先删除缓存，再更新数据库」在「并发」场景下依旧有数据不一致问题，解决方案是「延迟双删」，但这个延迟时间很难评估，所以推荐用「先更新数据库，再删除缓存」的方案\n 5. 在「先更新数据库，再删除缓存」方案下，为了保证两步都成功执行，需配合「消息队列」或「订阅变更日志」的方案来做，本质是通过「重试」的方式保证数据一致性\n 6. 在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」也会导致缓存和数据库不一致，缓解此问题的方案是「延迟双删」，凭借经验发送「延迟消息」到队列中，延迟删除缓存，同时也要控制主从库延迟，尽可能降低不一致发生的概率\n 7. 如果要实现强一致性可以采用的方案是 2PC、3PC、Paxos、Raft 这类一致性协议，或者使用分布式锁\n\n\n# 参考文献\n\n缓存和数据库一致性问题，看这篇就够了 - 水滴与银弹",normalizedContent:"# 前言\n\n感觉这是一个很宏大的命题\n\n我个人觉得引入缓存之后，如果为了短时间的不一致性问题，选择让系统设计变得更加复杂的话，完全没必要\n\n可以将请求分流，要保证强一致的请求走数据库，能忍受不一致的请求走缓存\n\necho 带着你沿着场景渐进式的了解双写一致性问题\n\n\n# 引入缓存提高性能\n\n我们从最简单的场景开始讲起。\n\n如果你的业务处于起步阶段，流量非常小，那无论是读请求还是写请求，直接操作数据库即可，这时你的架构模型是这样的：\n\n\n\n但随着业务量的增长，你的项目请求量越来越大，这时如果每次都从数据库中读数据，那肯定会有性能问题。\n\n这个阶段通常的做法是，引入「缓存」来提高读性能，架构模型就变成了这样：\n\n\n\n当下优秀的缓存中间件，当属 redis 莫属，它不仅性能非常高，还提供了很多友好的数据类型，可以很好地满足我们的业务需求。\n\n但引入缓存之后，你就会面临一个问题：之前数据只存在数据库中，现在要放到缓存中读取，具体要怎么存呢？\n\n有三种方案：\n\n * cache aside pattern（旁路缓存模式）\n * read/write through pattern（读写穿透）\n * write behind pattern（异步缓存写入）\n\n最简单直接的方案是「全量数据刷到缓存中」：\n\n * 数据库的数据，全量刷入缓存（不设置失效时间）\n * 写请求只更新数据库，不更新缓存\n * 启动一个定时任务，定时把数据库的数据，更新到缓存中\n\n\n\n这个方案的优点是，所有读请求都可以直接「命中」缓存，不需要再查数据库，性能非常高。\n\n但缺点也很明显，有 2 个问题：\n\n 1. 缓存利用率低：不经常访问的数据，还一直留在缓存中\n 2. 数据不一致：因为是「定时」刷新缓存，缓存和数据库存在不一致（取决于定时任务的执行频率）\n\n所以，这种方案一般更适合业务「体量小」，且对数据一致性要求不高的业务场景。\n\n那如果我们的业务体量很大，怎么解决这 2 个问题呢？\n\n\n# 缓存利用率和一致性问题\n\n\n# 先来看第一个问题，如何提高缓存利用率？\n\n想要缓存利用率「最大化」，我们很容易想到的方案是，缓存中只保留最近访问的「热数据」。但具体要怎么做呢？\n\n我们可以这样优化：\n\n * 写请求依旧只写数据库\n * 读请求先读缓存，如果缓存不存在，则从数据库读取，并重建缓存\n * 同时，写入缓存中的数据，都设置失效时间\n\n\n\n这样一来，缓存中不经常访问的数据，随着时间的推移，都会逐渐「过期」淘汰掉，最终缓存中保留的，都是经常被访问的「热数据」，缓存利用率得以最大化。\n\n再来看数据一致性问题。\n\n要想保证缓存和数据库「实时」一致，那就不能再用定时任务刷新缓存了。\n\n所以，当数据发生更新时，我们不仅要操作数据库，还要一并操作缓存。具体操作就是，修改一条数据时，不仅要更新数据库，也要连带缓存一起更新。\n\n但数据库和缓存都更新，又存在先后问题，那对应的方案就有 2 个：\n\n 1. 先更新缓存，后更新数据库\n 2. 先更新数据库，后更新缓存\n\n哪个方案更好呢？\n\n先不考虑并发问题，正常情况下，无论谁先谁后，都可以让两者保持一致，但现在我们需要重点考虑「异常」情况。\n\n因为操作分为两步，那么就很有可能存在「第一步成功、第二步失败」的情况发生。\n\n这 2 种方案我们一个个来分析。\n\n1) 先更新缓存，后更新数据库\n\n如果缓存更新成功了，但数据库更新失败，那么此时缓存中是最新值，但数据库中是「旧值」。\n\n虽然此时读请求可以命中缓存，拿到正确的值，但是，一旦缓存「失效」，就会从数据库中读取到「旧值」，重建缓存也是这个旧值。\n\n这时用户会发现自己之前修改的数据又「变回去」了，对业务造成影响。\n\n2) 先更新数据库，后更新缓存\n\n如果数据库更新成功了，但缓存更新失败，那么此时数据库中是最新值，缓存中是「旧值」。\n\n之后的读请求读到的都是旧数据，只有当缓存「失效」后，才能从数据库中得到正确的值。\n\n这时用户会发现，自己刚刚修改了数据，但却看不到变更，一段时间过后，数据才变更过来，对业务也会有影响。\n\n可见，无论谁先谁后，但凡后者发生异常，就会对业务造成影响。那怎么解决这个问题呢？\n\n别急，后面我会详细给出对应的解决方案。\n\n我们继续分析，除了操作失败问题，还有什么场景会影响数据一致性？\n\n这里我们还需要重点关注：并发问题。\n\n\n# 并发引发的一致性问题\n\n假设我们采用**「先更新数据库，再更新缓存」**的方案，并且两步都可以「成功执行」的前提下，如果存在并发，情况会是怎样的呢？\n\n有线程 a 和线程 b 两个线程，需要更新「同一条」数据，会发生这样的场景：\n\n 1. 线程 a 更新数据库（x = 1）\n 2. 线程 b 更新数据库（x = 2）\n 3. 线程 b 更新缓存（x = 2）\n 4. 线程 a 更新缓存（x = 1）\n\n最终 x 的值在缓存中是 1，在数据库中是 2，发生不一致。\n\n也就是说，a 虽然先于 b 发生，但 b 操作数据库和缓存的时间，却要比 a 的时间短，执行时序发生「错乱」，最终这条数据结果是不符合预期的。\n\n> 同样地，采用「先更新缓存，再更新数据库」的方案，也会有类似问题，这里不再详述。\n\n除此之外，我们从「缓存利用率」的角度来评估这个方案，也是不太推荐的。\n\n这是因为每次数据发生变更，都「无脑」更新缓存，但是缓存中的数据不一定会被「马上读取」，这就会导致缓存中可能存放了很多不常访问的数据，浪费缓存资源。\n\n而且很多情况下，写到缓存中的值，并不是与数据库中的值一一对应的，很有可能是先查询数据库，再经过一系列「计算」得出一个值，才把这个值才写到缓存中。\n\n由此可见，这种「更新数据库 + 更新缓存」的方案，不仅缓存利用率不高，还会造成机器性能的浪费。\n\n所以此时我们需要考虑另外一种方案：删除缓存。\n\n\n# 删除缓存可以保证一致性吗？\n\n删除缓存对应的方案也有 2 种：\n\n 1. 先删除缓存，后更新数据库\n 2. 先更新数据库，后删除缓存\n\n经过前面的分析我们已经得知，但凡「第二步」操作失败，都会导致数据不一致。\n\n这里我不再详述具体场景，你可以按照前面的思路推演一下，就可以看到依旧存在数据不一致的情况。\n\n这里我们重点来看「并发」问题。\n\n1) 先删除缓存，后更新数据库\n\n如果有 2 个线程要并发「读写」数据，可能会发生以下场景：\n\n 1. 线程 a 要更新 x = 2（原值 x = 1）\n 2. 线程 a 先删除缓存\n 3. 线程 b 读缓存，发现不存在，从数据库中读取到旧值（x = 1）\n 4. 线程 a 将新值写入数据库（x = 2）\n 5. 线程 b 将旧值写入缓存（x = 1）\n\n最终 x 的值在缓存中是 1（旧值），在数据库中是 2（新值），发生不一致。\n\n可见，先删除缓存，后更新数据库，当发生「读+写」并发时，还是存在数据不一致的情况。\n\n2) 先更新数据库，后删除缓存\n\n依旧是 2 个线程并发「读写」数据：\n\n 1. 缓存中 x 不存在（数据库 x = 1）\n 2. 线程 a 读取数据库，得到旧值（x = 1）\n 3. 线程 b 更新数据库（x = 2)\n 4. 线程 b 删除缓存\n 5. 线程 a 将旧值写入缓存（x = 1）\n\n最终 x 的值在缓存中是 1（旧值），在数据库中是 2（新值），也发生不一致。\n\n这种情况「理论」来说是可能发生的，但实际真的有可能发生吗？\n\n其实概率「很低」，这是因为它必须满足 3 个条件：\n\n 1. 缓存刚好已失效\n 2. 读请求 + 写请求并发\n 3. 更新数据库 + 删除缓存的时间（步骤 3-4），要比读数据库 + 写缓存时间短（步骤 2 和 5）\n\n仔细想一下，条件 3 发生的概率其实是非常低的。\n\n因为写数据库一般会先「加锁」，所以写数据库，通常是要比读数据库的时间更长的。\n\n这么来看，「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的。\n\n所以，我们应该采用这种方案，来操作数据库和缓存。\n\n好，解决了并发问题，我们继续来看前面遗留的，第二步执行「失败」导致数据不一致的问题。\n\n\n# 如何保证两步都执行成功？\n\n前面我们分析到，无论是更新缓存还是删除缓存，只要第二步发生失败，那么就会导致数据库和缓存不一致。\n\n前面我们分析到，无论是更新缓存还是删除缓存，只要第二步发生失败，那么就会导致数据库和缓存不一致。\n\n保证第二步成功执行，就是解决问题的关键。\n\n想一下，程序在执行过程中发生异常，最简单的解决办法是什么？\n\n答案是：重试。\n\n是的，其实这里我们也可以这样做。\n\n无论是先操作缓存，还是先操作数据库，但凡后者执行失败了，我们就可以发起重试，尽可能地去做「补偿」。\n\n那这是不是意味着，只要执行失败，我们「无脑重试」就可以了呢？\n\n答案是否定的。现实情况往往没有想的这么简单，失败后立即重试的问题在于：\n\n * 立即重试很大概率「还会失败」\n * 「重试次数」设置多少才合理？\n * 重试会一直「占用」这个线程资源，无法服务其它客户端请求\n\n看到了么，虽然我们想通过重试的方式解决问题，但这种「同步」重试的方案依旧不严谨。\n\n那更好的方案应该怎么做？\n\n答案是：异步重试。什么是异步重试？\n\n其实就是把重试请求写到「消息队列」中，然后由专门的消费者来重试，直到成功。\n\n或者更直接的做法，为了避免第二步执行失败，我们可以把操作缓存这一步，直接放到消息队列中，由消费者来操作缓存。\n\n到这里你可能会问，写消息队列也有可能会失败啊？而且，引入消息队列，这又增加了更多的维护成本，这样做值得吗？\n\n这个问题很好，但我们思考这样一个问题：如果在执行失败的线程中一直重试，还没等执行成功，此时如果项目「重启」了，那这次重试请求也就「丢失」了，那这条数据就一直不一致了。\n\n所以，这里我们必须把重试或第二步操作放到另一个「服务」中，这个服务用「消息队列」最为合适。这是因为消息队列的特性，正好符合我们的需求：\n\n * 消息队列保证可靠性：写到队列中的消息，成功消费之前不会丢失（重启项目也不担心）\n * 消息队列保证消息成功投递：下游从队列拉取消息，成功消费后才会删除消息，否则还会继续投递消息给消费者（符合我们重试的场景）\n\n至于写队列失败和消息队列的维护成本问题：\n\n * 写队列失败：操作缓存和写消息队列，「同时失败」的概率其实是很小的\n * 维护成本：我们项目中一般都会用到消息队列，维护成本并没有新增很多\n\n所以，引入消息队列来解决这个问题，是比较合适的。这时架构模型就变成了这样：\n\n那如果你确实不想在应用中去写消息队列，是否有更简单的方案，同时又可以保证一致性呢？\n\n方案还是有的，这就是近几年比较流行的解决方案：订阅数据库变更日志，再操作缓存。\n\n具体来讲就是，我们的业务应用在修改数据时，「只需」修改数据库，无需操作缓存。\n\n那什么时候操作缓存呢？这就和数据库的「变更日志」有关了。\n\n拿 mysql 举例，当一条数据发生修改时，mysql 就会产生一条变更日志（binlog），我们可以订阅这个日志，拿到具体操作的数据，然后再根据这条数据，去删除对应的缓存。\n\n订阅变更日志，目前也有了比较成熟的开源中间件，例如阿里的 canal，使用这种方案的优点在于：\n\n * 无需考虑写消息队列失败情况：只要写 mysql 成功，binlog 肯定会有\n * 自动投递到下游队列：canal 自动把数据库变更日志「投递」给下游的消息队列\n\n当然，与此同时，我们需要投入精力去维护 canal 的高可用和稳定性。\n\n> 如果你有留意观察很多数据库的特性，就会发现其实很多数据库都逐渐开始提供「订阅变更日志」的功能了，相信不远的将来，我们就不用通过中间件来拉取日志，自己写程序就可以订阅变更日志了，这样可以进一步简化流程。\n\n至此，我们可以得出结论，想要保证数据库和缓存一致性，推荐采用「先更新数据库，再删除缓存」方案，并配合「消息队列」或「订阅变更日志」的方式来做。\n\n\n# 主从库延迟和延迟双删问题\n\n到这里，还有 2 个问题，是我们没有重点分析过的。\n\n第一个问题，还记得前面讲到的「先删除缓存，再更新数据库」方案，导致不一致的场景么？\n\n这里我再把例子拿过来让你复习一下：\n\n2 个线程要并发「读写」数据，可能会发生以下场景：\n\n 1. 线程 a 要更新 x = 2（原值 x = 1）\n 2. 线程 a 先删除缓存\n 3. 线程 b 读缓存，发现不存在，从数据库中读取到旧值（x = 1）\n 4. 线程 a 将新值写入数据库（x = 2）\n 5. 线程 b 将旧值写入缓存（x = 1）\n\n最终 x 的值在缓存中是 1（旧值），在数据库中是 2（新值），发生不一致。\n\n第二个问题：是关于「读写分离 + 主从复制延迟」情况下，缓存和数据库一致性的问题。\n\n在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」其实也会导致不一致：\n\n 1. 线程 a 更新主库 x = 2（原值 x = 1）\n 2. 线程 a 删除缓存\n 3. 线程 b 查询缓存，没有命中，查询「从库」得到旧值（从库 x = 1）\n 4. 从库「同步」完成（主从库 x = 2）\n 5. 线程 b 将「旧值」写入缓存（x = 1）\n\n最终 x 的值在缓存中是 1（旧值），在主从库中是 2（新值），也发生不一致。\n\n看到了么？这 2 个问题的核心在于：缓存都被回种了「旧值」。\n\n那怎么解决这类问题呢？\n\n最有效的办法就是，把缓存删掉。\n\n但是，不能立即删，而是需要「延迟删」，这就是业界给出的方案：缓存延迟双删策略。\n\n按照延时双删策略，这 2 个问题的解决方案是这样的：\n\n解决第一个问题：在线程 a 删除缓存、更新完数据库之后，先「休眠一会」，再「删除」一次缓存。\n\n解决第二个问题：线程 a 可以生成一条「延时消息」，写到消息队列中，消费者延时「删除」缓存。\n\n这两个方案的目的，都是为了把缓存清掉，这样一来，下次就可以从数据库读取到最新值，写入缓存。\n\n但问题来了，这个「延迟删除」缓存，延迟时间到底设置要多久呢？\n\n * 问题1：延迟时间要大于「主从复制」的延迟时间\n * 问题2：延迟时间要大于线程 b 读取数据库 + 写入缓存的时间\n\n但是，这个时间在分布式和高并发场景下，其实是很难评估的\n\n很多时候，我们都是凭借经验大致估算这个延迟时间，例如延迟 1-5s，只能尽可能地降低不一致的概率。\n\n所以你看，采用这种方案，也只是尽可能保证一致性而已，极端情况下，还是有可能发生不一致。\n\n所以实际使用中，我还是建议你采用「先更新数据库，再删除缓存」的方案，同时，要尽可能地保证「主从复制」不要有太大延迟，降低出问题的概率。\n\n\n# 可以做到强一致吗？\n\n看到这里你可能会想，这些方案还是不够完美，我就想让缓存和数据库「强一致」，到底能不能做到呢？\n\n其实很难。\n\n要想做到强一致，最常见的方案是 2pc、3pc、paxos、raft 这类一致性协议，但它们的性能往往比较差，而且这些方案也比较复杂，还要考虑各种容错问题。\n\n相反，这时我们换个角度思考一下，我们引入缓存的目的是什么？\n\n没错，性能。\n\n一旦我们决定使用缓存，那必然要面临一致性问题。性能和一致性就像天平的两端，无法做到都满足要求。\n\n而且，就拿我们前面讲到的方案来说，当操作数据库和缓存完成之前，只要有其它请求可以进来，都有可能查到「中间状态」的数据。\n\n所以如果非要追求强一致，那必须要求所有更新操作完成之前期间，不能有「任何请求」进来。\n\n虽然我们可以通过加「分布锁」的方式来实现，但我们要付出的代价，很可能会超过引入缓存带来的性能提升。\n\n所以，既然决定使用缓存，就必须容忍「一致性」问题，我们只能尽可能地去降低问题出现的概率。\n\n同时我们也要知道，缓存都是有「失效时间」的，就算在这期间存在短期不一致，我们依旧有失效时间来兜底，这样也能达到最终一致。\n\n\n# 总结\n\n好了，总结一下这篇文章的重点。\n\n 1. 想要提高应用的性能，可以引入「缓存」来解决\n 2. 引入缓存后，需要考虑缓存和数据库一致性问题，可选的方案有：「更新数据库 + 更新缓存」、「更新数据库 + 删除缓存」\n 3. 更新数据库 + 更新缓存方案，在「并发」场景下无法保证缓存和数据一致性，且存在「缓存资源浪费」和「机器性能浪费」的情况发生\n 4. 在更新数据库 + 删除缓存的方案中，「先删除缓存，再更新数据库」在「并发」场景下依旧有数据不一致问题，解决方案是「延迟双删」，但这个延迟时间很难评估，所以推荐用「先更新数据库，再删除缓存」的方案\n 5. 在「先更新数据库，再删除缓存」方案下，为了保证两步都成功执行，需配合「消息队列」或「订阅变更日志」的方案来做，本质是通过「重试」的方式保证数据一致性\n 6. 在「先更新数据库，再删除缓存」方案下，「读写分离 + 主从库延迟」也会导致缓存和数据库不一致，缓解此问题的方案是「延迟双删」，凭借经验发送「延迟消息」到队列中，延迟删除缓存，同时也要控制主从库延迟，尽可能降低不一致发生的概率\n 7. 如果要实现强一致性可以采用的方案是 2pc、3pc、paxos、raft 这类一致性协议，或者使用分布式锁\n\n\n# 参考文献\n\n缓存和数据库一致性问题，看这篇就够了 - 水滴与银弹",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"缓存穿透",frontmatter:{title:"缓存穿透",date:"2024-09-14T16:50:37.000Z",permalink:"/pages/1e9e8e/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/02.%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F.html",relativePath:"30.经典场景设计/01.经典场景设计/02.缓存穿透.md",key:"v-34254e21",path:"/pages/1e9e8e/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"什么是缓存穿透",slug:"什么是缓存穿透",normalizedTitle:"什么是缓存穿透",charIndex:149},{level:2,title:"缓存穿透如何解决",slug:"缓存穿透如何解决",normalizedTitle:"缓存穿透如何解决",charIndex:606},{level:3,title:"设置空值",slug:"设置空值",normalizedTitle:"设置空值",charIndex:908},{level:3,title:"布隆过滤器",slug:"布隆过滤器",normalizedTitle:"布隆过滤器",charIndex:96},{level:3,title:"接口限流",slug:"接口限流",normalizedTitle:"接口限流",charIndex:1390}],headersStr:"前言 什么是缓存穿透 缓存穿透如何解决 设置空值 布隆过滤器 接口限流",content:"# 前言\n\n先来一个问题：如何解决复杂 where 下的缓存穿透？\n\n * 分为正常请求和非法请求\n * 正常请求：给 where 进行 hash，然后存 redis，合理设置过期时间，并利用布隆过滤器过滤\n * 非法请求：记录查询 null 的次数，如果太大，进行封禁，采用指数退避算法\n\n\n# 什么是缓存穿透\n\n缓存穿透最直白的意思就是，我们的业务系统在接收到请求时在缓存中并没有查到数据，从而穿透到了后端数据库里面查数据的过程。\n\n当然，既然使用了缓存，肯定会难免有穿透的发生，正常的少量穿透是对我们业务来说是不会造成任何影响的，因为:\n\n * 毕竟我们的缓存容量有限，不可能去缓存所有数据，当面临较大请求时，查询到未被缓存的数据时，就会发生穿透。\n * 互联网业务的数据访问模型一般是遵循“二八”原则的，即 20% 的数据为热点数据，80% 的数据是非热点不被常访问的数据。\n\n现在既然我们的缓存容量有限，然后 20% 的数据为热点数据，也就是说，我们可以利用有限的容量去缓存那 20% 的数据，其实就是可以保护我们的后端系统的，至于80%非热点不常用的数据发生穿透了，是我们能够接受的。\n\n那究竟什么的缓存穿透会影响到我们的系统呢？是大量的穿透请求超过了我们后端系统的承受范围，比如恶意的穿透攻击，这样的穿透就很有可能把我们的系统给干崩溃。接下来，我们就来基于相关应用场景来解决这种缓存穿透。\n\n\n# 缓存穿透如何解决\n\n在我们APP的在线搜索相关系统里面，有个产品product 1 并没有在数据库中进行存储，现在通过cache aside pattern 策略（缓存读写策略，开发必备）查这个product 1 。\n\n那查询一个数据库中本身就没有的数据后面会怎样呢？依照cache aside 策略，读取时，首先会读取缓存，没读到数据就会穿透到读数据库，现在数据库也没有，也就没有数据写回缓存。那么，再来个请求依然如此，更多的请求来还是一样，这样的缓存就没意义了。\n\n通过上面场景我们可以看到，这样的系统面临非正常的穿透是会崩溃掉的，那我们该怎么去解决呢？一般我们对此有两种方案，都是有用的：\n\n * 设置空值\n * 布隆过滤器\n\n\n# 设置空值\n\n通过上面场景我们知道，当有大量恶意的穿透请求到数据库，就会给我们系统带来灾难。\n\n所以，当我们请求数据中没有数据或者因为代码bug带来的异常造成的数据为空，这个时候我们就可以回写一个空值null到缓存中。同时，我们还要给这个null值设置过期时间，因为这个空值不具有实际业务性，而且还占用空间。\n\n可见设置空值是可以阻挡大量穿透请求的，但是如果有大量的获取并不存在数据的穿透请求的话例如恶意攻击，则会浪费缓存空间，如果这种null值过量的话，还会淘汰掉本身缓存存在的数据，这就会使我们的缓存命中率下降。\n\n**生产建议，**在使用设置空值方案时，我们要做好监控，预防缓存空间被过多null值占领造成的缓存空间浪费，如果这种数据量太大，就不再建议使用，那就使用另一种方案，即布隆过滤器。\n\n\n# 布隆过滤器\n\n生产建议：\n\n * 采用多个hash 算法计算hash 值，这样可以减少误判的几率。\n * 布隆过滤器会消耗一定内存空间，根据业务场景进行评估需要多大内存，最后依据公司资源以及成本，看是否能够接受。\n\n\n# 接口限流\n\n根据用户或者 IP 对接口进行限流，对于异常频繁的访问行为，还可以采取黑名单机制，例如将异常 IP 列入黑名单。\n\n后面提到的缓存击穿和雪崩都可以配合接口限流来解决，毕竟这些问题的关键都是有很多请求落到了数据库上造成数据库压力过大。",normalizedContent:"# 前言\n\n先来一个问题：如何解决复杂 where 下的缓存穿透？\n\n * 分为正常请求和非法请求\n * 正常请求：给 where 进行 hash，然后存 redis，合理设置过期时间，并利用布隆过滤器过滤\n * 非法请求：记录查询 null 的次数，如果太大，进行封禁，采用指数退避算法\n\n\n# 什么是缓存穿透\n\n缓存穿透最直白的意思就是，我们的业务系统在接收到请求时在缓存中并没有查到数据，从而穿透到了后端数据库里面查数据的过程。\n\n当然，既然使用了缓存，肯定会难免有穿透的发生，正常的少量穿透是对我们业务来说是不会造成任何影响的，因为:\n\n * 毕竟我们的缓存容量有限，不可能去缓存所有数据，当面临较大请求时，查询到未被缓存的数据时，就会发生穿透。\n * 互联网业务的数据访问模型一般是遵循“二八”原则的，即 20% 的数据为热点数据，80% 的数据是非热点不被常访问的数据。\n\n现在既然我们的缓存容量有限，然后 20% 的数据为热点数据，也就是说，我们可以利用有限的容量去缓存那 20% 的数据，其实就是可以保护我们的后端系统的，至于80%非热点不常用的数据发生穿透了，是我们能够接受的。\n\n那究竟什么的缓存穿透会影响到我们的系统呢？是大量的穿透请求超过了我们后端系统的承受范围，比如恶意的穿透攻击，这样的穿透就很有可能把我们的系统给干崩溃。接下来，我们就来基于相关应用场景来解决这种缓存穿透。\n\n\n# 缓存穿透如何解决\n\n在我们app的在线搜索相关系统里面，有个产品product 1 并没有在数据库中进行存储，现在通过cache aside pattern 策略（缓存读写策略，开发必备）查这个product 1 。\n\n那查询一个数据库中本身就没有的数据后面会怎样呢？依照cache aside 策略，读取时，首先会读取缓存，没读到数据就会穿透到读数据库，现在数据库也没有，也就没有数据写回缓存。那么，再来个请求依然如此，更多的请求来还是一样，这样的缓存就没意义了。\n\n通过上面场景我们可以看到，这样的系统面临非正常的穿透是会崩溃掉的，那我们该怎么去解决呢？一般我们对此有两种方案，都是有用的：\n\n * 设置空值\n * 布隆过滤器\n\n\n# 设置空值\n\n通过上面场景我们知道，当有大量恶意的穿透请求到数据库，就会给我们系统带来灾难。\n\n所以，当我们请求数据中没有数据或者因为代码bug带来的异常造成的数据为空，这个时候我们就可以回写一个空值null到缓存中。同时，我们还要给这个null值设置过期时间，因为这个空值不具有实际业务性，而且还占用空间。\n\n可见设置空值是可以阻挡大量穿透请求的，但是如果有大量的获取并不存在数据的穿透请求的话例如恶意攻击，则会浪费缓存空间，如果这种null值过量的话，还会淘汰掉本身缓存存在的数据，这就会使我们的缓存命中率下降。\n\n**生产建议，**在使用设置空值方案时，我们要做好监控，预防缓存空间被过多null值占领造成的缓存空间浪费，如果这种数据量太大，就不再建议使用，那就使用另一种方案，即布隆过滤器。\n\n\n# 布隆过滤器\n\n生产建议：\n\n * 采用多个hash 算法计算hash 值，这样可以减少误判的几率。\n * 布隆过滤器会消耗一定内存空间，根据业务场景进行评估需要多大内存，最后依据公司资源以及成本，看是否能够接受。\n\n\n# 接口限流\n\n根据用户或者 ip 对接口进行限流，对于异常频繁的访问行为，还可以采取黑名单机制，例如将异常 ip 列入黑名单。\n\n后面提到的缓存击穿和雪崩都可以配合接口限流来解决，毕竟这些问题的关键都是有很多请求落到了数据库上造成数据库压力过大。",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"缓存击穿",frontmatter:{title:"缓存击穿",date:"2024-09-14T16:50:56.000Z",permalink:"/pages/1d96b2/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/03.%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF.html",relativePath:"30.经典场景设计/01.经典场景设计/03.缓存击穿.md",key:"v-09a4b954",path:"/pages/1d96b2/",headers:[{level:2,title:"前言：什么是缓存击穿",slug:"前言-什么是缓存击穿",normalizedTitle:"前言：什么是缓存击穿",charIndex:2},{level:2,title:"有哪些解决办法",slug:"有哪些解决办法",normalizedTitle:"有哪些解决办法",charIndex:209},{level:2,title:"双检锁解决缓存击穿",slug:"双检锁解决缓存击穿",normalizedTitle:"双检锁解决缓存击穿",charIndex:367}],headersStr:"前言：什么是缓存击穿 有哪些解决办法 双检锁解决缓存击穿",content:'# 前言：什么是缓存击穿\n\n缓存击穿中，请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 。这就可能会导致瞬时大量的请求直接打到了数据库上，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了。\n\n\n\n举个例子：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力\n\n\n# 有哪些解决办法\n\n 1. 永不过期（不推荐）：设置热点数据永不过期或者过期时间比较长。\n 2. 提前预热（推荐）：针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期。\n 3. 加锁（看情况）：在缓存失效后，通过设置互斥锁确保只有一个请求去查询数据库并更新缓存。\n\n\n# 双检锁解决缓存击穿\n\n> 单例模式的一种实现，双重检测，其中的一层检测是为了提高效率。由于项目中采用了多线程，所以在第一个线程没有从缓存中获取到数据之后，有可能其他线程已经完成了读取数据库写入缓存的操作，也就是说，第一个线程再次得到时间片的时候，就没有必要访问数据库获取数据了。第二层检测是为了避免额外的访库操作。\n\npublic  Student getStudentById(Integer id) {\n    redisTemplate.setKeySerializer(new StringRedisSerializer());\n    //查询缓存\n    Student student = (Student) redisTemplate.opsForValue().get("studentKey");\n    //判断缓存是否为空\n    if (null == student) {\n\n        //双重检测锁实现\n        synchronized (this) {\n\n            student = (Student) redisTemplate.opsForValue().get("studentKey");\n\n            if (null == student) {\n                System.out.println("查询了数据库......");\n                //查询数据库\n                student = studentMapper.selectByPrimaryKey(id);\n                //放入缓存\n                redisTemplate.opsForValue().set("studentKey", student);\n            }\n        }\n    } else {\n        System.out.println("查询了缓存......");\n    }\n    return student;\n}\n',normalizedContent:'# 前言：什么是缓存击穿\n\n缓存击穿中，请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 。这就可能会导致瞬时大量的请求直接打到了数据库上，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了。\n\n\n\n举个例子：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力\n\n\n# 有哪些解决办法\n\n 1. 永不过期（不推荐）：设置热点数据永不过期或者过期时间比较长。\n 2. 提前预热（推荐）：针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期。\n 3. 加锁（看情况）：在缓存失效后，通过设置互斥锁确保只有一个请求去查询数据库并更新缓存。\n\n\n# 双检锁解决缓存击穿\n\n> 单例模式的一种实现，双重检测，其中的一层检测是为了提高效率。由于项目中采用了多线程，所以在第一个线程没有从缓存中获取到数据之后，有可能其他线程已经完成了读取数据库写入缓存的操作，也就是说，第一个线程再次得到时间片的时候，就没有必要访问数据库获取数据了。第二层检测是为了避免额外的访库操作。\n\npublic  student getstudentbyid(integer id) {\n    redistemplate.setkeyserializer(new stringredisserializer());\n    //查询缓存\n    student student = (student) redistemplate.opsforvalue().get("studentkey");\n    //判断缓存是否为空\n    if (null == student) {\n\n        //双重检测锁实现\n        synchronized (this) {\n\n            student = (student) redistemplate.opsforvalue().get("studentkey");\n\n            if (null == student) {\n                system.out.println("查询了数据库......");\n                //查询数据库\n                student = studentmapper.selectbyprimarykey(id);\n                //放入缓存\n                redistemplate.opsforvalue().set("studentkey", student);\n            }\n        }\n    } else {\n        system.out.println("查询了缓存......");\n    }\n    return student;\n}\n',charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"超卖",frontmatter:{title:"超卖",date:"2024-09-14T22:14:25.000Z",permalink:"/pages/8a57f2/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/06.%E8%B6%85%E5%8D%96.html",relativePath:"30.经典场景设计/01.经典场景设计/06.超卖.md",key:"v-459bc1c5",path:"/pages/8a57f2/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"实现方案",slug:"实现方案",normalizedTitle:"实现方案",charIndex:96},{level:3,title:"数据库扣减库存",slug:"数据库扣减库存",normalizedTitle:"数据库扣减库存",charIndex:105},{level:3,title:"redis扣减库存",slug:"redis扣减库存",normalizedTitle:"redis扣减库存",charIndex:764},{level:3,title:"lua脚本扣减库存",slug:"lua脚本扣减库存",normalizedTitle:"lua脚本扣减库存",charIndex:2141},{level:3,title:"redis decr + 分布式锁",slug:"redis-decr-分布式锁",normalizedTitle:"redis decr + 分布式锁",charIndex:2879},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:3513}],headersStr:"前言 实现方案 数据库扣减库存 redis扣减库存 lua脚本扣减库存 redis decr + 分布式锁 参考文献",content:'# 前言\n\n超卖问题，简单来说就是卖出的商品数量超出库存数\n\n一般是难点在于秒杀场景下的超卖\n\necho 觉得 在秒杀场景下这个库存是可以不用回退的，因为本就是瞬时的，回退也没有意义\n\n\n# 实现方案\n\n\n# 数据库扣减库存\n\nupdate product set stock=stock-1 where id=123;\n\n\n这种写法对于扣减库存是没有问题的，但如何控制库存不足的情况下，不让用户操作呢？\n\n这就需要在update之前，先查一下库存是否足够了。\n\n伪代码如下：\n\nint stock = mapper.getStockById(123);\nif(stock > 0) {\n  int count = mapper.updateStock(123);\n  if(count > 0) {\n    addOrder(123);\n  }\n}\n\n\n大家有没有发现这段代码的问题？\n\n没错，查询操作和更新操作不是原子性的，会导致在并发的场景下，出现库存超卖的情况。\n\n有人可能会说，这样好办，加把锁，不就搞定了，比如使用synchronized关键字。\n\n确实，可以，但是性能不够好。\n\n还有更优雅的处理方案，即基于数据库的乐观锁，这样会少一次数据库查询，而且能够天然的保证数据操作的原子性。\n\n只需将上面的sql稍微调整一下：\n\nupdate product set stock=stock-1 where id=product and stock > 0;\n\n\n在sql最后加上：stock > 0，就能保证不会出现超卖的情况。\n\n但需要频繁访问数据库，我们都知道数据库连接是非常昂贵的资源。在高并发的场景下，可能会造成系统雪崩。而且，容易出现多个请求，同时竞争行锁的情况，造成相互等待，从而出现死锁的问题。\n\n\n# redis扣减库存\n\nredis的incr方法是原子性的，可以用该方法扣减库存。\n\nboolean exist = redisClient.query(productId,userId);\nif(exist) {\n\treturn -1;\n}\nint stock = redisClient.queryStock(productId);\nif(stock <=0) {\n\treturn 0;\n}\nredisClient.incrby(productId, -1);\nredisClient.add(productId,userId);\nreturn 1;\n\n\n代码流程如下：\n\n 1. 先判断该用户有没有秒杀过该商品，如果已经秒杀过，则直接返回-1。\n 2. 查询库存，如果库存小于等于0，则直接返回0，表示库存不足。\n 3. 如果库存充足，则扣减库存，然后将本次秒杀记录保存起来。然后返回1，表示成功。\n\n估计很多小伙伴，一开始都会按这样的思路写代码。但如果仔细想想会发现，这段代码有问题。\n\n有什么问题呢？\n\n如果在高并发下，有多个请求同时查询库存，当时都大于0。由于查询库存和更新库存非原则操作，则会出现库存为负数的情况，即库存超卖。\n\n当然有人可能会说，加个synchronized不就解决问题？\n\n调整后代码如下：\n\nboolean exist = redisClient.query(productId,userId);\nif(exist) {\n\treturn -1;\n}\nsynchronized(this) {\nint stock = redisClient.queryStock(productId);\nif(stock <=0) {\n\treturn 0;\n}\nredisClient.incrby(productId, -1);\nredisClient.add(productId,userId);\n}\n\nreturn 1;\n\n\n加synchronized确实能解决库存为负数问题，但是这样会导致接口性能急剧下降，每次查询都需要竞争同一把锁，显然不太合理。\n\n为了解决上面的问题，代码优化如下：\n\nboolean exist = redisClient.query(productId,userId);\nif(exist) {\n  return -1;\n}\nif(redisClient.incrby(productId, -1)<0) {\n  return 0;\n}\nredisClient.add(productId,userId);\nreturn 1;\n\n\n该代码主要流程如下：\n\n 1. 先判断该用户有没有秒杀过该商品，如果已经秒杀过，则直接返回-1。\n 2. 扣减库存，判断返回值是否小于0，如果小于0，则直接返回0，表示库存不足。\n 3. 如果扣减库存后，返回值大于或等于0，则将本次秒杀记录保存起来。然后返回1，表示成功。\n\n该方案咋一看，好像没问题。\n\n但如果在高并发场景中，有多个请求同时扣减库存，大多数请求的incrby操作之后，结果都会小于0。\n\n虽说，库存出现负数，不会出现超卖的问题。但由于这里是预减库存，如果负数值负的太多的话，后面万一要回退库存时，就会导致库存不准。\n\n那么，有没有更好的方案呢？\n\n\n# lua脚本扣减库存\n\n我们都知道lua脚本，是能够保证原子性的，它跟redis一起配合使用，能够完美解决上面的问题。\n\nlua脚本有段非常经典的代码：\n\n  StringBuilder lua = new StringBuilder();\n  lua.append("if (redis.call(\'exists\', KEYS[1]) == 1) then");\n  lua.append("    local stock = tonumber(redis.call(\'get\', KEYS[1]));");\n  lua.append("    if (stock == -1) then");\n  lua.append("        return 1;");\n  lua.append("    end;");\n  lua.append("    if (stock > 0) then");\n  lua.append("        redis.call(\'incrby\', KEYS[1], -1);");\n  lua.append("        return stock;");\n  lua.append("    end;");\n  lua.append("    return 0;");\n  lua.append("end;");\n  lua.append("return -1;");\n\n\n该代码的主要流程如下：\n\n 1. 先判断商品id是否存在，如果不存在则直接返回。\n 2. 获取该商品id的库存，判断库存如果是-1，则直接返回，表示不限制库存。\n 3. 如果库存大于0，则扣减库存。\n 4. 如果库存等于0，是直接返回，表示库存不足。\n\n\n# redis decr + 分布式锁\n\n\n\n先用 decr 去扣减库存，然后用分布式锁，锁住当前库存，防止库存回退现象的发生\n\n 1. 在 redis 集群模式下【以我们的场景为例】，incr 请求操作也可能在请求时发生网络抖动超时返回。这个时候incr有可能成功，也有可能失败。可能是请求超时，也可能是请求完的应答超时。那么incr 的值可能就不准。【实际使用中10万次，可能会有10万零1和不足10万】，那么为了这样一个临界状态的可靠性，所以添加 setNx 加锁只有成功和失败。\n 2. setNx 因为是非独占锁，所以key不存在释放。setNx 的key 可以过期时间可以优化为活动的有效期时间为结束。—— 而独占锁，其实你永远也不好把握释放时间，因为秒杀都是瞬态的，释放的晚了活动用户都走了，释放的早了，流程可能还没处理完。\n 3. 对于 setNx 可能还有些时候，集群主从切换，或者活动出问题的时候恢复。如果恢复的 incr 值多了，那么有 setNx 锁拦截后，会更加可靠。\n 4. 关于库存恢复，一般这类抽奖都是瞬态的，且redis集群非常稳定。所以很少有需要恢复库存，如果需要恢复库存，那么是把失败的秒杀incr对应的值的key，加入到待消费队列中。等整体库存消耗后，开始消耗队列库存。\n 5. 这里的锁的颗粒度在于一个用户一个锁的key，所以没有个人释放再需要被让别人抢占的需要，因为这不是独占锁。所以锁的key可以设置活动结束后释放。\n\n\n# 参考文献\n\n面试必考：秒杀系统如何设计？-腾讯云开发者社区-腾讯云 (tencent.com)',normalizedContent:'# 前言\n\n超卖问题，简单来说就是卖出的商品数量超出库存数\n\n一般是难点在于秒杀场景下的超卖\n\necho 觉得 在秒杀场景下这个库存是可以不用回退的，因为本就是瞬时的，回退也没有意义\n\n\n# 实现方案\n\n\n# 数据库扣减库存\n\nupdate product set stock=stock-1 where id=123;\n\n\n这种写法对于扣减库存是没有问题的，但如何控制库存不足的情况下，不让用户操作呢？\n\n这就需要在update之前，先查一下库存是否足够了。\n\n伪代码如下：\n\nint stock = mapper.getstockbyid(123);\nif(stock > 0) {\n  int count = mapper.updatestock(123);\n  if(count > 0) {\n    addorder(123);\n  }\n}\n\n\n大家有没有发现这段代码的问题？\n\n没错，查询操作和更新操作不是原子性的，会导致在并发的场景下，出现库存超卖的情况。\n\n有人可能会说，这样好办，加把锁，不就搞定了，比如使用synchronized关键字。\n\n确实，可以，但是性能不够好。\n\n还有更优雅的处理方案，即基于数据库的乐观锁，这样会少一次数据库查询，而且能够天然的保证数据操作的原子性。\n\n只需将上面的sql稍微调整一下：\n\nupdate product set stock=stock-1 where id=product and stock > 0;\n\n\n在sql最后加上：stock > 0，就能保证不会出现超卖的情况。\n\n但需要频繁访问数据库，我们都知道数据库连接是非常昂贵的资源。在高并发的场景下，可能会造成系统雪崩。而且，容易出现多个请求，同时竞争行锁的情况，造成相互等待，从而出现死锁的问题。\n\n\n# redis扣减库存\n\nredis的incr方法是原子性的，可以用该方法扣减库存。\n\nboolean exist = redisclient.query(productid,userid);\nif(exist) {\n\treturn -1;\n}\nint stock = redisclient.querystock(productid);\nif(stock <=0) {\n\treturn 0;\n}\nredisclient.incrby(productid, -1);\nredisclient.add(productid,userid);\nreturn 1;\n\n\n代码流程如下：\n\n 1. 先判断该用户有没有秒杀过该商品，如果已经秒杀过，则直接返回-1。\n 2. 查询库存，如果库存小于等于0，则直接返回0，表示库存不足。\n 3. 如果库存充足，则扣减库存，然后将本次秒杀记录保存起来。然后返回1，表示成功。\n\n估计很多小伙伴，一开始都会按这样的思路写代码。但如果仔细想想会发现，这段代码有问题。\n\n有什么问题呢？\n\n如果在高并发下，有多个请求同时查询库存，当时都大于0。由于查询库存和更新库存非原则操作，则会出现库存为负数的情况，即库存超卖。\n\n当然有人可能会说，加个synchronized不就解决问题？\n\n调整后代码如下：\n\nboolean exist = redisclient.query(productid,userid);\nif(exist) {\n\treturn -1;\n}\nsynchronized(this) {\nint stock = redisclient.querystock(productid);\nif(stock <=0) {\n\treturn 0;\n}\nredisclient.incrby(productid, -1);\nredisclient.add(productid,userid);\n}\n\nreturn 1;\n\n\n加synchronized确实能解决库存为负数问题，但是这样会导致接口性能急剧下降，每次查询都需要竞争同一把锁，显然不太合理。\n\n为了解决上面的问题，代码优化如下：\n\nboolean exist = redisclient.query(productid,userid);\nif(exist) {\n  return -1;\n}\nif(redisclient.incrby(productid, -1)<0) {\n  return 0;\n}\nredisclient.add(productid,userid);\nreturn 1;\n\n\n该代码主要流程如下：\n\n 1. 先判断该用户有没有秒杀过该商品，如果已经秒杀过，则直接返回-1。\n 2. 扣减库存，判断返回值是否小于0，如果小于0，则直接返回0，表示库存不足。\n 3. 如果扣减库存后，返回值大于或等于0，则将本次秒杀记录保存起来。然后返回1，表示成功。\n\n该方案咋一看，好像没问题。\n\n但如果在高并发场景中，有多个请求同时扣减库存，大多数请求的incrby操作之后，结果都会小于0。\n\n虽说，库存出现负数，不会出现超卖的问题。但由于这里是预减库存，如果负数值负的太多的话，后面万一要回退库存时，就会导致库存不准。\n\n那么，有没有更好的方案呢？\n\n\n# lua脚本扣减库存\n\n我们都知道lua脚本，是能够保证原子性的，它跟redis一起配合使用，能够完美解决上面的问题。\n\nlua脚本有段非常经典的代码：\n\n  stringbuilder lua = new stringbuilder();\n  lua.append("if (redis.call(\'exists\', keys[1]) == 1) then");\n  lua.append("    local stock = tonumber(redis.call(\'get\', keys[1]));");\n  lua.append("    if (stock == -1) then");\n  lua.append("        return 1;");\n  lua.append("    end;");\n  lua.append("    if (stock > 0) then");\n  lua.append("        redis.call(\'incrby\', keys[1], -1);");\n  lua.append("        return stock;");\n  lua.append("    end;");\n  lua.append("    return 0;");\n  lua.append("end;");\n  lua.append("return -1;");\n\n\n该代码的主要流程如下：\n\n 1. 先判断商品id是否存在，如果不存在则直接返回。\n 2. 获取该商品id的库存，判断库存如果是-1，则直接返回，表示不限制库存。\n 3. 如果库存大于0，则扣减库存。\n 4. 如果库存等于0，是直接返回，表示库存不足。\n\n\n# redis decr + 分布式锁\n\n\n\n先用 decr 去扣减库存，然后用分布式锁，锁住当前库存，防止库存回退现象的发生\n\n 1. 在 redis 集群模式下【以我们的场景为例】，incr 请求操作也可能在请求时发生网络抖动超时返回。这个时候incr有可能成功，也有可能失败。可能是请求超时，也可能是请求完的应答超时。那么incr 的值可能就不准。【实际使用中10万次，可能会有10万零1和不足10万】，那么为了这样一个临界状态的可靠性，所以添加 setnx 加锁只有成功和失败。\n 2. setnx 因为是非独占锁，所以key不存在释放。setnx 的key 可以过期时间可以优化为活动的有效期时间为结束。—— 而独占锁，其实你永远也不好把握释放时间，因为秒杀都是瞬态的，释放的晚了活动用户都走了，释放的早了，流程可能还没处理完。\n 3. 对于 setnx 可能还有些时候，集群主从切换，或者活动出问题的时候恢复。如果恢复的 incr 值多了，那么有 setnx 锁拦截后，会更加可靠。\n 4. 关于库存恢复，一般这类抽奖都是瞬态的，且redis集群非常稳定。所以很少有需要恢复库存，如果需要恢复库存，那么是把失败的秒杀incr对应的值的key，加入到待消费队列中。等整体库存消耗后，开始消耗队列库存。\n 5. 这里的锁的颗粒度在于一个用户一个锁的key，所以没有个人释放再需要被让别人抢占的需要，因为这不是独占锁。所以锁的key可以设置活动结束后释放。\n\n\n# 参考文献\n\n面试必考：秒杀系统如何设计？-腾讯云开发者社区-腾讯云 (tencent.com)',charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"任务补偿",frontmatter:{title:"任务补偿",date:"2024-09-14T16:51:13.000Z",permalink:"/pages/24abe0/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/04.%E4%BB%BB%E5%8A%A1%E8%A1%A5%E5%81%BF.html",relativePath:"30.经典场景设计/01.经典场景设计/04.任务补偿.md",key:"v-5f69e48d",path:"/pages/24abe0/",headers:[{level:2,title:"前言：补偿机制的意义？",slug:"前言-补偿机制的意义",normalizedTitle:"前言：补偿机制的意义？",charIndex:2},{level:2,title:"补偿 该怎么做？",slug:"补偿-该怎么做",normalizedTitle:"补偿 该怎么做？",charIndex:726},{level:3,title:"回滚",slug:"回滚",normalizedTitle:"回滚",charIndex:771},{level:3,title:"重试",slug:"重试",normalizedTitle:"重试",charIndex:318},{level:2,title:"重试 的最佳实践",slug:"重试-的最佳实践",normalizedTitle:"重试 的最佳实践",charIndex:3585},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4108},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:4244}],headersStr:"前言：补偿机制的意义？ 补偿 该怎么做？ 回滚 重试 重试 的最佳实践 总结 参考文献",content:"# 前言：补偿机制的意义？\n\n以电商的购物场景为例：\n\n客户端 ----\x3e购物车微服务 ----\x3e订单微服务 ----\x3e 支付微服务。\n\n这种调用链非常普遍。\n\n那么为什么需要考虑补偿机制呢？\n\n正如之前几篇文章所说，一次跨机器的通信可能会经过DNS 服务，网卡、交换机、路由器、负载均衡等设备，这些设备都不一定是一直稳定的，在数据传输的整个过程中，只要任意一个环节出错，都会导致问题的产生。\n\n而在分布式场景中，一个完整的业务又是由多次跨机器通信组成的，所以产生问题的概率成倍数增加。\n\n但是，这些问题并不完全代表真正的系统无法处理请求，所以我们应当尽可能的自动消化掉这些异常。\n\n可能你会问，之前也看到过「补偿」和「事务补偿」或者「重试」，它们之间的关系是什么？\n\n你其实可以不用太纠结这些名字，从目的来说都是一样的。就是一旦某个操作发生了异常，如何通过内部机制将这个异常产生的「不一致」状态消除掉。\n\n> 在 echo 看来，不管用什么方式，只要通过额外的方式解决了问题都可以理解为是「补偿」，所以「事务补偿」和「重试」都是「补偿」的子集。前者是一个逆向操作，而后者则是一个正向操作。\n\n只是从结果来看，两者的意义不同。「事务补偿」意味着“放弃”，当前操作必然会失败。\n\n事务补偿\n\n\n\n「重试」则还有处理成功的机会。这两种方式分别适用于不同的场景。\n\n重试\n\n\n\n因为「补偿」已经是一个额外流程了，既然能够走这个额外流程，说明时效性并不是第一考虑的因素，所以做补偿的核心要点是：宁可慢，不可错。\n\n因此，不要草率的就确定了补偿的实施方案，需要谨慎的评估。虽说错误无法100%避免，但是抱着这样的一个心态或多或少可以减少一些错误的发生。\n\n\n# 补偿 该怎么做？\n\n做「补偿」的主流方式就前面提到的「事务补偿」和「重试」，以下会被称作「回滚」和「重试」。\n\n我们先来聊聊「回滚」。相比「重试」，它逻辑上更简单一些。\n\n\n# 回滚\n\necho 将回滚分为2种模式，一种叫「显式回滚」（调用逆向接口），一种叫「隐式回滚」（无需调用逆向接口）。\n\n最常见的就是「显式回滚」。这个方案无非就是做2个事情：\n\n首先要确定失败的步骤和状态，从而确定需要回滚的范围。一个业务的流程，往往在设计之初就制定好了，所以确定回滚的范围比较容易。但这里唯一需要注意的一点就是：如果在一个业务处理中涉及到的服务并不是都提供了「回滚接口」，那么在编排服务时应该把提供「回滚接口」的服务放在前面，这样当后面的工作服务错误时还有机会「回滚」。\n\n其次要能提供「回滚」操作使用到的业务数据。「回滚」时提供的数据越多，越有益于程序的健壮性。因为程序可以在收到「回滚」操作的时候可以做业务的检查，比如检查账户是否相等，金额是否一致等等。\n\n由于这个中间状态的数据结构和数据大小并不固定，所以echo建议你在实现这点的时候可以将相关的数据序列化成一个json，然后存放到一个nosql类型的存储中。\n\n「隐式回滚」相对来说运用场景比较少。它意味着这个回滚动作你不需要进行额外处理，下游服务内部有类似“预占”并且“超时失效”的机制的。例如：\n\n电商场景中，会将订单中的商品先预占库存，等待用户在 15 分钟内支付。如果没有收到用户的支付，则释放库存。\n\n下面聊聊可以有很多玩法，也更容易陷入坑里的「重试」。\n\n\n# 重试\n\n「重试」最大的好处在于，业务系统可以不需要提供「逆向接口」，这是一个对长期开发成本特别大的利好，毕竟业务是天天在变的。所以，在可能的情况下，应该优先考虑使用「重试」。\n\n不过，相比「回滚」来说「重试」的适用场景更少一些，所以我们第一步首先要判断，当前场景是否适合「重试」。比如：\n\n * 下游系统返回「请求超时」、「被限流中」等临时状态的时候，我们可以考虑重试\n * 而如果是返回“余额不足”、“无权限”等明确无法继续的业务性错误的时候就不需要重试了\n * 一些中间件或者rpc框架中返回Http503、404等没有何时恢复的预期的时候，也不需要重试\n\n如果确定要进行「重试」，我们还需要选定一个合适的「重试策略」。主流的「重试策略」主要是以下几种。\n\n策略1.立即重试。有时故障是候暂时性，可能是因网络数据包冲突或硬件组件流量高峰等事件造成的。在此情况下，适合立即重试操作。不过，立即重试次数不应超过一次，如果立即重试失败，应改用其它的策略。\n\n策略2.固定间隔。应用程序每次尝试的间隔时间相同。 这个好理解，例如，固定每 3 秒重试操作。（以下所有示例代码中的具体的数字仅供参考。）\n\n策略1和策略2多用于前端系统的交互式操作中。\n\n策略3.增量间隔。每一次的重试间隔时间增量递增。比如，第一次0秒、第二次3秒、第三次6秒，9、12、15这样。\n\nreturn (retryCount - 1) * incrementInterval;\n\n\n使得失败次数越多的重试请求优先级排到越后面，给新进入的重试请求让道。\n\n策略4.指数间隔。每一次的重试间隔呈指数级增加。和增量间隔“殊途同归”，都是想让失败次数越多的重试请求优先级排到越后面，只不过这个方案的增长幅度更大一些。\n\nreturn 2 ^ retryCount;\n\n\n策略5.全抖动。在递增的基础上，增加随机性（可以把其中的指数增长部分替换成增量增长。）。适用于将某一时刻集中产生的大量重试请求进行压力分散的场景。\n\nreturn random(0 , 2 ^ retryCount);\n\n\n策略6.等抖动。在「指数间隔」和「全抖动」之间寻求一个中庸的方案，降低随机性的作用。适用场景和「全抖动」一样。\n\nvar baseNum = 2 ^ retryCount;return baseNum + random(0 , baseNum);\n\n\n3、4、5、6策略的表现情况大致是这样。(x轴为重试次数)\n\n\n\n为什么说「重试」有坑呢？\n\n正如前面聊到的那样，出于对开发成本考虑，你在做「重试」的时候可能是复用的常规调用的接口。那么此时就不得不提一个「幂等性」问题。\n\n如果实现「重试」选用的技术方案不能100%确保不会重复发起重试，那么「幂等性」问题是一个必须要考虑的问题。哪怕技术方案可以确保100%不会重复发起重试，出于对意外情况的考量，尽量也考虑一下「幂等性」问题。\n\n**幂等性：**不管对程序发起几次重复调用，程序表现的状态（所有相关的数据变化）与调用一次的结果是一致的话，就是保证了幂等性。\n\n这意味着可以根据需要重复或重试操作，而不会导致意外的影响。对于非幂等操作，算法可能必须跟踪操作是否已经执行。\n\n所以，一旦某个功能支持「重试」，那么整个链路上的接口都需要考虑幂等性问题，不能因为服务的多次调用而导致业务数据的累计增加或减少。\n\n满足「幂等性」其实就是需要想办法识别重复的请求，并且将其过滤掉。思路就是：\n\n 1. 给每个请求定义一个唯一标识。\n 2. 在进行「重试」的时候判断这个请求是否已经被执行或者正在被执行，如果是则抛弃该请求。\n\n**第1点，**我们可以使用一个全局唯一id生成器或者生成服务（可以扩展阅读，分布式系统中的必备良药 —— 全局唯一单据号生成）。 或者简单粗暴一些，使用官方类库自带的Guid、uuid之类的也行。\n\n然后通过rpc框架在发起调用的客户端中，对每个请求增加一个唯一标识的字段进行赋值。\n\n**第2点，**我们可以在服务端通过Aop的方式切入到实际的处理逻辑代码之前和之后，一起配合做验证。\n\n\n\n大致的代码思路如下。\n\n【方法执行前】if(isExistLog(requestId)){  //1.判断请求是否已被接收过。  对应序号3\n    var lastResult = getLastResult();  //2.获取用于判断之前的请求是否已经处理完成。  对应序号4\n    if(lastResult == null){  \n        var result = waitResult();  //挂起等待处理完成\n        return result;\n    }\n    else{\n        return lastResult;\n    }  \n}\nelse{\n    log(requestId);  //3.记录该请求已接收\n}\n\n//do something..【方法执行后】\n\nlogResult(requestId, result);  //4.将结果也更新一下。\n\n\n如果「补偿」这个工作是通过MQ来进行的话，这事就可以直接在对接MQ所封装的SDK中做。在生产端赋值全局唯一标识，在消费端通过唯一标识消重。\n\n\n# 重试 的最佳实践\n\n再聊一些 echo 积累的最佳实践吧，都是针对「重试」的，的确这也是工作中最常用的方案。\n\n「重试」特别适合在高负载情况下被「降级」，当然也应当受到「限流」和「熔断」机制的影响。当「重试」的“矛”与「限流」和「熔断」的“盾”搭配使用，效果才是最好。\n\n需要衡量增加补偿机制的投入产出比。一些不是很重要的问题时，应该「快速失败」而不是「重试」。\n\n过度积极的重试策略（例如间隔太短或重试次数过多）会对下游服务造成不利影响，这点一定要注意。\n\n一定要给「重试」制定一个终止策略。\n\n当回滚的过程很困难或代价很大的情况下，可以接受很长的间隔及大量的重试次数，DDD中经常被提到的「saga」模式其实也是这样的思路。不过，前提是不会因为保留或锁定稀缺资源而阻止其他操作（比如1、2、3、4、5几个串行操作。由于2一直没处理完成导致3、4、5没法继续进行）。\n\n可以离线的事务一致性维护机制\n\n 1. 第一步：在线业务生成可疑记录\n 2. 第二步：离线服务诊断可疑记录，生成故障记录\n 3. 第三步：离线服务尝试对故障记录进行智能修复（补偿或重试）\n 4. 第四步：对于无法修复，或者修复过程失败的记录发出告警，交由人工处理。\n\n\n# 总结\n\n这篇我们先聊了下做「补偿」的意义，以及做补偿的2个方式「回滚」和「重试」的实现思路。\n\n然后，提醒你要注意「重试」的时候需要考虑幂等性问题，并且z哥也给出了一个解决思路。\n\n最后，分享了几个 echo 总结的针对「重试」的最佳实践。\n\n希望对你有所帮助。\n\n\n# 参考文献\n\n99%的人都能看懂的分布式系统「补偿」机制 - 知乎 (zhihu.com)",normalizedContent:"# 前言：补偿机制的意义？\n\n以电商的购物场景为例：\n\n客户端 ----\x3e购物车微服务 ----\x3e订单微服务 ----\x3e 支付微服务。\n\n这种调用链非常普遍。\n\n那么为什么需要考虑补偿机制呢？\n\n正如之前几篇文章所说，一次跨机器的通信可能会经过dns 服务，网卡、交换机、路由器、负载均衡等设备，这些设备都不一定是一直稳定的，在数据传输的整个过程中，只要任意一个环节出错，都会导致问题的产生。\n\n而在分布式场景中，一个完整的业务又是由多次跨机器通信组成的，所以产生问题的概率成倍数增加。\n\n但是，这些问题并不完全代表真正的系统无法处理请求，所以我们应当尽可能的自动消化掉这些异常。\n\n可能你会问，之前也看到过「补偿」和「事务补偿」或者「重试」，它们之间的关系是什么？\n\n你其实可以不用太纠结这些名字，从目的来说都是一样的。就是一旦某个操作发生了异常，如何通过内部机制将这个异常产生的「不一致」状态消除掉。\n\n> 在 echo 看来，不管用什么方式，只要通过额外的方式解决了问题都可以理解为是「补偿」，所以「事务补偿」和「重试」都是「补偿」的子集。前者是一个逆向操作，而后者则是一个正向操作。\n\n只是从结果来看，两者的意义不同。「事务补偿」意味着“放弃”，当前操作必然会失败。\n\n事务补偿\n\n\n\n「重试」则还有处理成功的机会。这两种方式分别适用于不同的场景。\n\n重试\n\n\n\n因为「补偿」已经是一个额外流程了，既然能够走这个额外流程，说明时效性并不是第一考虑的因素，所以做补偿的核心要点是：宁可慢，不可错。\n\n因此，不要草率的就确定了补偿的实施方案，需要谨慎的评估。虽说错误无法100%避免，但是抱着这样的一个心态或多或少可以减少一些错误的发生。\n\n\n# 补偿 该怎么做？\n\n做「补偿」的主流方式就前面提到的「事务补偿」和「重试」，以下会被称作「回滚」和「重试」。\n\n我们先来聊聊「回滚」。相比「重试」，它逻辑上更简单一些。\n\n\n# 回滚\n\necho 将回滚分为2种模式，一种叫「显式回滚」（调用逆向接口），一种叫「隐式回滚」（无需调用逆向接口）。\n\n最常见的就是「显式回滚」。这个方案无非就是做2个事情：\n\n首先要确定失败的步骤和状态，从而确定需要回滚的范围。一个业务的流程，往往在设计之初就制定好了，所以确定回滚的范围比较容易。但这里唯一需要注意的一点就是：如果在一个业务处理中涉及到的服务并不是都提供了「回滚接口」，那么在编排服务时应该把提供「回滚接口」的服务放在前面，这样当后面的工作服务错误时还有机会「回滚」。\n\n其次要能提供「回滚」操作使用到的业务数据。「回滚」时提供的数据越多，越有益于程序的健壮性。因为程序可以在收到「回滚」操作的时候可以做业务的检查，比如检查账户是否相等，金额是否一致等等。\n\n由于这个中间状态的数据结构和数据大小并不固定，所以echo建议你在实现这点的时候可以将相关的数据序列化成一个json，然后存放到一个nosql类型的存储中。\n\n「隐式回滚」相对来说运用场景比较少。它意味着这个回滚动作你不需要进行额外处理，下游服务内部有类似“预占”并且“超时失效”的机制的。例如：\n\n电商场景中，会将订单中的商品先预占库存，等待用户在 15 分钟内支付。如果没有收到用户的支付，则释放库存。\n\n下面聊聊可以有很多玩法，也更容易陷入坑里的「重试」。\n\n\n# 重试\n\n「重试」最大的好处在于，业务系统可以不需要提供「逆向接口」，这是一个对长期开发成本特别大的利好，毕竟业务是天天在变的。所以，在可能的情况下，应该优先考虑使用「重试」。\n\n不过，相比「回滚」来说「重试」的适用场景更少一些，所以我们第一步首先要判断，当前场景是否适合「重试」。比如：\n\n * 下游系统返回「请求超时」、「被限流中」等临时状态的时候，我们可以考虑重试\n * 而如果是返回“余额不足”、“无权限”等明确无法继续的业务性错误的时候就不需要重试了\n * 一些中间件或者rpc框架中返回http503、404等没有何时恢复的预期的时候，也不需要重试\n\n如果确定要进行「重试」，我们还需要选定一个合适的「重试策略」。主流的「重试策略」主要是以下几种。\n\n策略1.立即重试。有时故障是候暂时性，可能是因网络数据包冲突或硬件组件流量高峰等事件造成的。在此情况下，适合立即重试操作。不过，立即重试次数不应超过一次，如果立即重试失败，应改用其它的策略。\n\n策略2.固定间隔。应用程序每次尝试的间隔时间相同。 这个好理解，例如，固定每 3 秒重试操作。（以下所有示例代码中的具体的数字仅供参考。）\n\n策略1和策略2多用于前端系统的交互式操作中。\n\n策略3.增量间隔。每一次的重试间隔时间增量递增。比如，第一次0秒、第二次3秒、第三次6秒，9、12、15这样。\n\nreturn (retrycount - 1) * incrementinterval;\n\n\n使得失败次数越多的重试请求优先级排到越后面，给新进入的重试请求让道。\n\n策略4.指数间隔。每一次的重试间隔呈指数级增加。和增量间隔“殊途同归”，都是想让失败次数越多的重试请求优先级排到越后面，只不过这个方案的增长幅度更大一些。\n\nreturn 2 ^ retrycount;\n\n\n策略5.全抖动。在递增的基础上，增加随机性（可以把其中的指数增长部分替换成增量增长。）。适用于将某一时刻集中产生的大量重试请求进行压力分散的场景。\n\nreturn random(0 , 2 ^ retrycount);\n\n\n策略6.等抖动。在「指数间隔」和「全抖动」之间寻求一个中庸的方案，降低随机性的作用。适用场景和「全抖动」一样。\n\nvar basenum = 2 ^ retrycount;return basenum + random(0 , basenum);\n\n\n3、4、5、6策略的表现情况大致是这样。(x轴为重试次数)\n\n\n\n为什么说「重试」有坑呢？\n\n正如前面聊到的那样，出于对开发成本考虑，你在做「重试」的时候可能是复用的常规调用的接口。那么此时就不得不提一个「幂等性」问题。\n\n如果实现「重试」选用的技术方案不能100%确保不会重复发起重试，那么「幂等性」问题是一个必须要考虑的问题。哪怕技术方案可以确保100%不会重复发起重试，出于对意外情况的考量，尽量也考虑一下「幂等性」问题。\n\n**幂等性：**不管对程序发起几次重复调用，程序表现的状态（所有相关的数据变化）与调用一次的结果是一致的话，就是保证了幂等性。\n\n这意味着可以根据需要重复或重试操作，而不会导致意外的影响。对于非幂等操作，算法可能必须跟踪操作是否已经执行。\n\n所以，一旦某个功能支持「重试」，那么整个链路上的接口都需要考虑幂等性问题，不能因为服务的多次调用而导致业务数据的累计增加或减少。\n\n满足「幂等性」其实就是需要想办法识别重复的请求，并且将其过滤掉。思路就是：\n\n 1. 给每个请求定义一个唯一标识。\n 2. 在进行「重试」的时候判断这个请求是否已经被执行或者正在被执行，如果是则抛弃该请求。\n\n**第1点，**我们可以使用一个全局唯一id生成器或者生成服务（可以扩展阅读，分布式系统中的必备良药 —— 全局唯一单据号生成）。 或者简单粗暴一些，使用官方类库自带的guid、uuid之类的也行。\n\n然后通过rpc框架在发起调用的客户端中，对每个请求增加一个唯一标识的字段进行赋值。\n\n**第2点，**我们可以在服务端通过aop的方式切入到实际的处理逻辑代码之前和之后，一起配合做验证。\n\n\n\n大致的代码思路如下。\n\n【方法执行前】if(isexistlog(requestid)){  //1.判断请求是否已被接收过。  对应序号3\n    var lastresult = getlastresult();  //2.获取用于判断之前的请求是否已经处理完成。  对应序号4\n    if(lastresult == null){  \n        var result = waitresult();  //挂起等待处理完成\n        return result;\n    }\n    else{\n        return lastresult;\n    }  \n}\nelse{\n    log(requestid);  //3.记录该请求已接收\n}\n\n//do something..【方法执行后】\n\nlogresult(requestid, result);  //4.将结果也更新一下。\n\n\n如果「补偿」这个工作是通过mq来进行的话，这事就可以直接在对接mq所封装的sdk中做。在生产端赋值全局唯一标识，在消费端通过唯一标识消重。\n\n\n# 重试 的最佳实践\n\n再聊一些 echo 积累的最佳实践吧，都是针对「重试」的，的确这也是工作中最常用的方案。\n\n「重试」特别适合在高负载情况下被「降级」，当然也应当受到「限流」和「熔断」机制的影响。当「重试」的“矛”与「限流」和「熔断」的“盾”搭配使用，效果才是最好。\n\n需要衡量增加补偿机制的投入产出比。一些不是很重要的问题时，应该「快速失败」而不是「重试」。\n\n过度积极的重试策略（例如间隔太短或重试次数过多）会对下游服务造成不利影响，这点一定要注意。\n\n一定要给「重试」制定一个终止策略。\n\n当回滚的过程很困难或代价很大的情况下，可以接受很长的间隔及大量的重试次数，ddd中经常被提到的「saga」模式其实也是这样的思路。不过，前提是不会因为保留或锁定稀缺资源而阻止其他操作（比如1、2、3、4、5几个串行操作。由于2一直没处理完成导致3、4、5没法继续进行）。\n\n可以离线的事务一致性维护机制\n\n 1. 第一步：在线业务生成可疑记录\n 2. 第二步：离线服务诊断可疑记录，生成故障记录\n 3. 第三步：离线服务尝试对故障记录进行智能修复（补偿或重试）\n 4. 第四步：对于无法修复，或者修复过程失败的记录发出告警，交由人工处理。\n\n\n# 总结\n\n这篇我们先聊了下做「补偿」的意义，以及做补偿的2个方式「回滚」和「重试」的实现思路。\n\n然后，提醒你要注意「重试」的时候需要考虑幂等性问题，并且z哥也给出了一个解决思路。\n\n最后，分享了几个 echo 总结的针对「重试」的最佳实践。\n\n希望对你有所帮助。\n\n\n# 参考文献\n\n99%的人都能看懂的分布式系统「补偿」机制 - 知乎 (zhihu.com)",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"秒杀",frontmatter:{title:"秒杀",date:"2024-09-14T16:51:28.000Z",permalink:"/pages/a72629/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/05.%E7%A7%92%E6%9D%80.html",relativePath:"30.经典场景设计/01.经典场景设计/05.秒杀.md",key:"v-4f04879e",path:"/pages/a72629/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"秒杀会有哪些问题",slug:"秒杀会有哪些问题",normalizedTitle:"秒杀会有哪些问题",charIndex:21},{level:2,title:"解决方案",slug:"解决方案",normalizedTitle:"解决方案",charIndex:1438},{level:3,title:"前端",slug:"前端",normalizedTitle:"前端",charIndex:1447},{level:3,title:"Nginx",slug:"nginx",normalizedTitle:"nginx",charIndex:2626},{level:3,title:"风控",slug:"风控",normalizedTitle:"风控",charIndex:2613},{level:3,title:"后端",slug:"后端",normalizedTitle:"后端",charIndex:1535},{level:3,title:"数据库",slug:"数据库",normalizedTitle:"数据库",charIndex:1180},{level:3,title:"分布式事务",slug:"分布式事务",normalizedTitle:"分布式事务",charIndex:4905},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:5198},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:5347}],headersStr:"前言 秒杀会有哪些问题 解决方案 前端 Nginx 风控 后端 数据库 分布式事务 总结 参考文献",content:"# 前言\n\n秒杀系统的架构图\n\n\n\n\n# 秒杀会有哪些问题\n\n高并发\n\n是的高并发这个是我们想都不用想的一个点，一瞬间这么多人进来这不是高并发什么时候是呢？\n\n是吧，秒杀的特点就是这样时间极短、 瞬间用户量大。\n\n正常的店铺营销都是用极低的价格配合上短信、APP的精准推送，吸引特别多的用户来参与这场秒杀，爽了商家苦了开发呀。\n\n秒杀大家都知道如果真的营销到位，价格诱人，几十万的流量我觉得完全不是问题，那单机的Redis我感觉3-4W的QPS还是能顶得住的，但是再高了就没办法了，那这个数据随便搞个热销商品的秒杀可能都不止了。\n\n大量的请求进来，我们需要考虑的点就很多了，缓存雪崩，缓存击穿，缓存穿透这些我之前提到的点都是有可能发生的，出现问题打挂DB那就很难受了，活动失败用户体验差，活动人气没了，最后背锅的还是开发\n\n超卖\n\n但凡是个秒杀，都怕超卖，如果卖的是尿不湿还好，要是换成100个MacBook Pro，商家的预算经费卖100个可以赚点还可以造势，结果你写错程序多卖出去200个，你不发货用户投诉你，平台封你店，你发货就血亏，你怎么办？\n\n那最后只能杀个开发祭天解气了，秒杀的价格本来就低了，基本上都是不怎么赚钱的，超卖了就恐怖了呀，所以超卖也是很关键的一个点。\n\n恶意请求\n\n你这么低的价格，假如我抢到了，我转手卖掉我不是血赚？就算我不卖我也不亏啊，那用户知道，你知道，别的别有用心的人（黑客、黄牛…）肯定也知道的。\n\n那简单啊，我知道你什么时候抢，我搞个几十台机器搞点脚本，我也模拟出来十几万个人左右的请求，那我是不是意味着我基本上有80%的成功率了。\n\n真实情况可能远远不止，因为机器请求的速度比人的手速往往快太多了，在贵州的敖丙我每年回家抢高铁票都是秒光的，我也不知道有没有黄牛的功劳，我要Diss你，黄牛。杰伦演唱会门票抢不到，我也Diss你。\n\nTip：科普下，小道消息了解到的，黄牛的抢票系统，比国内很多小公司的系统还吊很多，架构设计都是顶级的，我用顶配的服务加上顶配的架构设计，你还想看演唱会？还想回家？\n\n不过不用黄牛我回家都难，我们云贵川跟我一样要回家过年的仔太多了555！\n\n链接暴露\n\n前面几个问题大家可能都很好理解，一看到这个有的小伙伴可能会比较疑惑，啥是链接暴露呀？\n\n相信是个开发同学都对这个画面一点都不陌生吧，懂点行的仔都可以打开谷歌的开发者模式，然后看看你的网页代码，有的就有URL，但是我写VUE的时候是事件触发然后去调用文件里面的接口看源码看不到，但是我可以点击一下查看你的请求地址啊，不过你好像可以对按钮在秒杀前置灰。\n\n不管怎么样子都有危险，撇开外面的所有的东西你都挡住了，你卖这个东西实在便宜得过分，有诱惑力，你能保证开发不动心？开发知道地址，在秒杀的时候自己提前请求。。。（开发：怎么TM又是我）\n\n数据库\n\n每秒上万甚至十几万的QPS（每秒请求数）直接打到数据库，基本上都要把库打挂掉，而且你服务不单单是做秒杀的还涉及其他的业务，你没做降级、限流、熔断啥的，别的一起挂，小公司的话可能全站崩溃404。\n\n反正不管你秒杀怎么挂，你别把别的搞挂了对吧，搞挂了就不是杀一个程序员能搞定的。\n\n程序员：我TM好难啊！\n\n问题都列出来了，那怎么设计，怎么解决这些问题就是接下去要考虑的了，我们对症下药。\n\n我会从我设计的秒杀系统从上到下去给大家介绍我们正常电商秒杀系统在每一层做了些什么，每一层存在的问题，难点等。\n\n\n# 解决方案\n\n\n# 前端\n\n秒杀系统普遍都是商城网页、H5、APP、小程序这几项。\n\n在前端这一层其实我们可以做的事情有很多，如果用node去做，甚至能直接处理掉整个秒杀，但是node其实应该属于后端，所以我不讨论node Service了。\n\n资源静态化\n\n秒杀一般都是特定的商品还有页面模板，现在一般都是前后端分离的，页面一般都是不会经过后端的，但是前端也要自己的服务器啊，那就把能提前放入cdn服务器的东西都放进去，反正把所有能提升效率的步骤都做一下，减少真正秒杀时候服务器的压力。\n\n秒杀链接加盐\n\n我们上面说了链接要是提前暴露出去可能有人直接访问url就提前秒杀了，那又有小伙伴要说了我做个时间的校验就好了呀，那我告诉你，知道链接的地址比起页面人工点击的还是有很大优势。\n\n我知道url了，那我通过程序不断获取最新的北京时间，可以达到毫秒级别的，我就在00毫秒的时候请求，我敢说绝对比你人工点的成功率大太多了，而且我可以一毫秒发送N次请求，搞不好你卖100个产品我全拿了。\n\n那这种情况怎么避免？把URL动态化，就连写代码的人都不知道，你就通过MD5之类的摘要算法加密随机的字符串去做url，然后通过前端代码获取url后台校验才能通过。\n\n这个只能防止一部分没耐心继续破解下去的黑客，有耐心的人研究出来还是能破解，在电商场景存在很多这样的羊毛党，那怎么做呢？后面我会说。\n\n限流\n\n限流这里我觉得应该分为前端限流和后端限流。\n\n物理控制\n\n大家有没有发现没到秒杀前，一般按钮都是置灰的，只有时间到了，才能点击。\n\n这是因为怕大家在时间快到的最后几秒秒疯狂请求服务器，然后还没到秒杀的时候基本上服务器就挂了。\n\n这个时候就需要前端的配合，定时去请求你的后端服务器，获取最新的北京时间，到时间点再给按钮可用状态。\n\n按钮可以点击之后也得给他置灰几秒，不然他一样在开始之后一直点的。\n\n你敢说你们秒杀的时候不是这样的？\n\n前端限流：这个很简单，一般秒杀不会让你一直点的，一般都是点击一下或者两下然后几秒之后才可以继续点击，这也是保护服务器的一种手段。\n\n后端限流：秒杀的时候肯定是涉及到后续的订单生成和支付等操作，但是都只是成功的幸运儿才会走到那一步，那一旦100个产品卖光了，return了一个false，前端直接秒杀结束，然后你后端也关闭后续无效请求的介入了。\n\nTip：真正的限流还会有限流组件的加入例如：阿里的Sentinel、Hystrix等。我这里就不展开了，就说一下物理的限流。\n\n我们卖1000件商品，请求有10W，我们不需要把十万都放进来，你可以放1W请求进来，然后再进行操作，因为秒杀对于用户本身就是黑盒的，所以你怎么做的他们是没感知的，至于为啥放1W进来，而不是刚好1000，是因为会丢掉一些薅羊毛的用户，至于怎么判断，后面的风控阶段我会说。\n\n\n# Nginx\n\nNginx大家想必都不陌生了吧，这玩意是高性能的web服务器，并发也随便顶几万不是梦，但是我们的Tomcat只能顶几百的并发呀，那简单呀负载均衡嘛，一台服务几百，那就多搞点，在秒杀的时候多租点流量机。\n\nTip：据我所知国内某大厂就是在去年春节活动期间租光了亚洲所有的服务器，小公司也很喜欢在双十一期间买流量机来顶住压力。\n\n恶意请求拦截也需要用到它，一般单个用户请求次数太夸张，不像人为的请求在网关那一层就得拦截掉了，不然请求多了他抢不抢得到是一回事，服务器压力上去了，可能占用网络带宽或者把服务器打崩、缓存击穿等等\n\n\n# 风控\n\n我可以明确的告诉大家，前面的所有措施还是拦不住很多羊毛党，因为他们是专业的团队，他们可以注册很多账号来薅你的羊毛，而且不用机器请求，就用群控，操作几乎跟真实用户一模一样。\n\n那怎么办，是不是无解了？\n\n这个时候就需要风控同学的介入了，在请求到达后端之前，风控可以根据账号行为分析出这个账号机器人的概率大不大，我现在负责公司的某些特殊系统，每个用户的行为都是会送到我们大数据团队进行分析处理，给你打上对应标签的。\n\n那黑客其实也有办法：养号\n\n他们去黑市买真实用户有过很多记录的账号，买到了还不闲着，帮他们去购物啥的，让系统无法识别他们是黑号还是真实用户的号。\n\n怎么办？\n\n通杀！是的没有办法，只能通杀了，通杀的意思就是，我们通过风管分析出来这个用户是真实用户的概率没有其他用户概率大，那就认为他是机器了，丢弃他的请求。\n\n之前的限流我们放进来10000个请求，但是我们真正的库存只有1000个，那我们就算出最有可能是真实用户的1000人进行秒杀，丢弃其他请求，因为秒杀本来就是黑盒操作的，用户层面是无感知的，这样设计能让真实的用户买到东西，还可以减少自己被薅羊毛的概率。\n\n风控可以说是流量进入的最后一道门槛了，所以很多公司的风控是很强的，蚂蚁金服的风控大家如果了解过就知道了，你的资金在支付宝被盗了，他们是能做到全款补偿是有原因的。\n\n\n# 后端\n\n服务单一职责\n\n设计个能抗住高并发的系统，我觉得还是得单一职责。\n\n什么意思呢，大家都知道现在设计都是微服务的设计思想，然后再用分布式的部署方式。\n\n也就是我们下单是有个订单服务，用户登录管理等有个用户服务等等，那为啥我们不给秒杀也开个服务，我们把秒杀的代码业务逻辑放一起。\n\n单一职责的好处就是就算秒杀没抗住，秒杀库崩了，服务挂了，也不会影响到其他的服务。（高可用）\n\nRedis 集群\n\n之前不是说单机的Redis顶不住嘛，那简单多找几个兄弟啊，秒杀本来就是读多写少，那你们是不是瞬间想起来我之前跟你们提到过的，Redis集群，主从同步、读写分离，我们还搞点哨兵，开启持久化直接无敌高可用！\n\n库存预热\n\n秒杀的本质，就是对库存的抢夺，每个秒杀的用户来你都去数据库查询库存校验库存，然后扣减库存，撇开性能因数，你不觉得这样好繁琐，对业务开发人员都不友好，而且数据库顶不住啊。\n\n我们都知道数据库顶不住但是他的兄弟非关系型的数据库Redis能顶啊！\n\n那不简单了，我们要开始秒杀前你通过定时任务或者运维同学提前把商品的库存加载到Redis中去，让整个流程都在Redis里面去做，然后等秒杀介绍了，再异步的去修改库存就好了。\n\n但是用了Redis就有一个问题了，我们上面说了我们采用主从，就是我们会去读取库存然后再判断然后有库存才去减库存，正常情况没问题，但是高并发的情况问题就很大了。\n\n**多品几遍！！！**就比如现在库存只剩下1个了，我们高并发嘛，4个服务器一起查询了发现都是还有1个，那大家都觉得是自己抢到了，就都去扣库存，那结果就变成了-3，是的只有一个是真的抢到了，别的都是超卖的。咋办？\n\n事务\n\nRedis本身是支持事务的，而且他有很多原子命令的，大家也可以用LUA，还可以用他的管道，乐观锁他也知支持。\n\n限流&降级&熔断&隔离\n\n这个为啥要做呢，不怕一万就怕万一，万一你真的顶不住了，限流，顶不住就挡一部分出去但是不能说不行，降级，降级了还是被打挂了，熔断，至少不要影响别的系统，隔离，你本身就独立的，但是你会调用其他的系统嘛，你快不行了你别拖累兄弟们啊。\n\n消息队列（削峰填谷）\n\n一说到这个名词，很多小伙伴就知道了，对的MQ，你买东西少了你直接100个请求改库我觉得没问题，但是万一秒杀一万个，10万个呢？服务器挂了，程序员又要背锅的。\n\n秒杀就是这种瞬间流量很高，但是平时又没有流量的场景，那消息队列完全契合这样的场景了呀，削峰填谷。\n\n\n\nTip：可能小伙伴说我们业务达不到这个量级，没必要。但是我想说我们写代码，就不应该写出有逻辑漏洞的代码，至少以后公司体量上去了，别人一看居然不用改代码，一看代码作者是xxx？有点东西！\n\n你可以把它放消息队列，然后一点点消费去改库存就好了嘛，不过单个商品其实一次修改就够了，我这里说的是某个点多个商品一起秒杀的场景，像极了双十一零点。\n\n\n# 数据库\n\n数据库用MySQL只要连接池设置合理一般问题是不大的，不过一般大公司不缺钱而且秒杀这样的活动十分频繁，我之前所在的公司就是这样秒杀特卖这样的场景一直都是不间断的。\n\n单独给秒杀建立一个数据库，为秒杀服务，表的设计也是竟可能的简单点，现在的互联网架构部署都是分库的。\n\n至于表就看大家怎么设计了，该设置索引的地方还是要设置索引的，建完后记得用explain看看SQL的执行计划。（不了解的小伙伴也没事，MySQL章节去康康）\n\n\n# 分布式事务\n\n这为啥我不放在后端而放到最后来讲呢？\n\n因为上面的任何一步都是可能出错的，而且我们是在不同的服务里面出错的，那就涉及分布式事务了，但是分布式事务大家想的是一定要成功什么的那就不对了，还是那句话，几个请求丢了就丢了，要保证时效和服务的可用可靠。\n\n所以TCC和最终一致性其实不是很适合，TCC开发成本很大，所有接口都要写三次，因为涉及TCC的三个阶段。\n\n最终一致性基本上都是靠轮训的操作去保证一个操作一定成功，那时效性就大打折扣了。\n\n大家觉得不那么可靠的**两段式（2PC）和三段式（3PC）**就派上用场了，他们不一定能保证数据最终一致，但是效率上还算ok。\n\n\n# 总结\n\n到这里我想我已经基本上把该考虑的点还有对应的解决方案也都说了一下，不知道还有没有没考虑到的，但是就算没考虑到我想我这个设计，应该也能撑住一个完整的秒杀流程。\n\n最后大家再看看这个秒杀系统或许会有新的感悟，是不是一个系统真的没有大家想的那么简单，而且我还是有漏掉的细节，这是一定的。\n\n\n# 参考文献\n\n面试了十个应届生九个都是秒杀系统，你确定你们那是秒杀？_小公司的qps只有几十-CSDN博客",normalizedContent:"# 前言\n\n秒杀系统的架构图\n\n\n\n\n# 秒杀会有哪些问题\n\n高并发\n\n是的高并发这个是我们想都不用想的一个点，一瞬间这么多人进来这不是高并发什么时候是呢？\n\n是吧，秒杀的特点就是这样时间极短、 瞬间用户量大。\n\n正常的店铺营销都是用极低的价格配合上短信、app的精准推送，吸引特别多的用户来参与这场秒杀，爽了商家苦了开发呀。\n\n秒杀大家都知道如果真的营销到位，价格诱人，几十万的流量我觉得完全不是问题，那单机的redis我感觉3-4w的qps还是能顶得住的，但是再高了就没办法了，那这个数据随便搞个热销商品的秒杀可能都不止了。\n\n大量的请求进来，我们需要考虑的点就很多了，缓存雪崩，缓存击穿，缓存穿透这些我之前提到的点都是有可能发生的，出现问题打挂db那就很难受了，活动失败用户体验差，活动人气没了，最后背锅的还是开发\n\n超卖\n\n但凡是个秒杀，都怕超卖，如果卖的是尿不湿还好，要是换成100个macbook pro，商家的预算经费卖100个可以赚点还可以造势，结果你写错程序多卖出去200个，你不发货用户投诉你，平台封你店，你发货就血亏，你怎么办？\n\n那最后只能杀个开发祭天解气了，秒杀的价格本来就低了，基本上都是不怎么赚钱的，超卖了就恐怖了呀，所以超卖也是很关键的一个点。\n\n恶意请求\n\n你这么低的价格，假如我抢到了，我转手卖掉我不是血赚？就算我不卖我也不亏啊，那用户知道，你知道，别的别有用心的人（黑客、黄牛…）肯定也知道的。\n\n那简单啊，我知道你什么时候抢，我搞个几十台机器搞点脚本，我也模拟出来十几万个人左右的请求，那我是不是意味着我基本上有80%的成功率了。\n\n真实情况可能远远不止，因为机器请求的速度比人的手速往往快太多了，在贵州的敖丙我每年回家抢高铁票都是秒光的，我也不知道有没有黄牛的功劳，我要diss你，黄牛。杰伦演唱会门票抢不到，我也diss你。\n\ntip：科普下，小道消息了解到的，黄牛的抢票系统，比国内很多小公司的系统还吊很多，架构设计都是顶级的，我用顶配的服务加上顶配的架构设计，你还想看演唱会？还想回家？\n\n不过不用黄牛我回家都难，我们云贵川跟我一样要回家过年的仔太多了555！\n\n链接暴露\n\n前面几个问题大家可能都很好理解，一看到这个有的小伙伴可能会比较疑惑，啥是链接暴露呀？\n\n相信是个开发同学都对这个画面一点都不陌生吧，懂点行的仔都可以打开谷歌的开发者模式，然后看看你的网页代码，有的就有url，但是我写vue的时候是事件触发然后去调用文件里面的接口看源码看不到，但是我可以点击一下查看你的请求地址啊，不过你好像可以对按钮在秒杀前置灰。\n\n不管怎么样子都有危险，撇开外面的所有的东西你都挡住了，你卖这个东西实在便宜得过分，有诱惑力，你能保证开发不动心？开发知道地址，在秒杀的时候自己提前请求。。。（开发：怎么tm又是我）\n\n数据库\n\n每秒上万甚至十几万的qps（每秒请求数）直接打到数据库，基本上都要把库打挂掉，而且你服务不单单是做秒杀的还涉及其他的业务，你没做降级、限流、熔断啥的，别的一起挂，小公司的话可能全站崩溃404。\n\n反正不管你秒杀怎么挂，你别把别的搞挂了对吧，搞挂了就不是杀一个程序员能搞定的。\n\n程序员：我tm好难啊！\n\n问题都列出来了，那怎么设计，怎么解决这些问题就是接下去要考虑的了，我们对症下药。\n\n我会从我设计的秒杀系统从上到下去给大家介绍我们正常电商秒杀系统在每一层做了些什么，每一层存在的问题，难点等。\n\n\n# 解决方案\n\n\n# 前端\n\n秒杀系统普遍都是商城网页、h5、app、小程序这几项。\n\n在前端这一层其实我们可以做的事情有很多，如果用node去做，甚至能直接处理掉整个秒杀，但是node其实应该属于后端，所以我不讨论node service了。\n\n资源静态化\n\n秒杀一般都是特定的商品还有页面模板，现在一般都是前后端分离的，页面一般都是不会经过后端的，但是前端也要自己的服务器啊，那就把能提前放入cdn服务器的东西都放进去，反正把所有能提升效率的步骤都做一下，减少真正秒杀时候服务器的压力。\n\n秒杀链接加盐\n\n我们上面说了链接要是提前暴露出去可能有人直接访问url就提前秒杀了，那又有小伙伴要说了我做个时间的校验就好了呀，那我告诉你，知道链接的地址比起页面人工点击的还是有很大优势。\n\n我知道url了，那我通过程序不断获取最新的北京时间，可以达到毫秒级别的，我就在00毫秒的时候请求，我敢说绝对比你人工点的成功率大太多了，而且我可以一毫秒发送n次请求，搞不好你卖100个产品我全拿了。\n\n那这种情况怎么避免？把url动态化，就连写代码的人都不知道，你就通过md5之类的摘要算法加密随机的字符串去做url，然后通过前端代码获取url后台校验才能通过。\n\n这个只能防止一部分没耐心继续破解下去的黑客，有耐心的人研究出来还是能破解，在电商场景存在很多这样的羊毛党，那怎么做呢？后面我会说。\n\n限流\n\n限流这里我觉得应该分为前端限流和后端限流。\n\n物理控制\n\n大家有没有发现没到秒杀前，一般按钮都是置灰的，只有时间到了，才能点击。\n\n这是因为怕大家在时间快到的最后几秒秒疯狂请求服务器，然后还没到秒杀的时候基本上服务器就挂了。\n\n这个时候就需要前端的配合，定时去请求你的后端服务器，获取最新的北京时间，到时间点再给按钮可用状态。\n\n按钮可以点击之后也得给他置灰几秒，不然他一样在开始之后一直点的。\n\n你敢说你们秒杀的时候不是这样的？\n\n前端限流：这个很简单，一般秒杀不会让你一直点的，一般都是点击一下或者两下然后几秒之后才可以继续点击，这也是保护服务器的一种手段。\n\n后端限流：秒杀的时候肯定是涉及到后续的订单生成和支付等操作，但是都只是成功的幸运儿才会走到那一步，那一旦100个产品卖光了，return了一个false，前端直接秒杀结束，然后你后端也关闭后续无效请求的介入了。\n\ntip：真正的限流还会有限流组件的加入例如：阿里的sentinel、hystrix等。我这里就不展开了，就说一下物理的限流。\n\n我们卖1000件商品，请求有10w，我们不需要把十万都放进来，你可以放1w请求进来，然后再进行操作，因为秒杀对于用户本身就是黑盒的，所以你怎么做的他们是没感知的，至于为啥放1w进来，而不是刚好1000，是因为会丢掉一些薅羊毛的用户，至于怎么判断，后面的风控阶段我会说。\n\n\n# nginx\n\nnginx大家想必都不陌生了吧，这玩意是高性能的web服务器，并发也随便顶几万不是梦，但是我们的tomcat只能顶几百的并发呀，那简单呀负载均衡嘛，一台服务几百，那就多搞点，在秒杀的时候多租点流量机。\n\ntip：据我所知国内某大厂就是在去年春节活动期间租光了亚洲所有的服务器，小公司也很喜欢在双十一期间买流量机来顶住压力。\n\n恶意请求拦截也需要用到它，一般单个用户请求次数太夸张，不像人为的请求在网关那一层就得拦截掉了，不然请求多了他抢不抢得到是一回事，服务器压力上去了，可能占用网络带宽或者把服务器打崩、缓存击穿等等\n\n\n# 风控\n\n我可以明确的告诉大家，前面的所有措施还是拦不住很多羊毛党，因为他们是专业的团队，他们可以注册很多账号来薅你的羊毛，而且不用机器请求，就用群控，操作几乎跟真实用户一模一样。\n\n那怎么办，是不是无解了？\n\n这个时候就需要风控同学的介入了，在请求到达后端之前，风控可以根据账号行为分析出这个账号机器人的概率大不大，我现在负责公司的某些特殊系统，每个用户的行为都是会送到我们大数据团队进行分析处理，给你打上对应标签的。\n\n那黑客其实也有办法：养号\n\n他们去黑市买真实用户有过很多记录的账号，买到了还不闲着，帮他们去购物啥的，让系统无法识别他们是黑号还是真实用户的号。\n\n怎么办？\n\n通杀！是的没有办法，只能通杀了，通杀的意思就是，我们通过风管分析出来这个用户是真实用户的概率没有其他用户概率大，那就认为他是机器了，丢弃他的请求。\n\n之前的限流我们放进来10000个请求，但是我们真正的库存只有1000个，那我们就算出最有可能是真实用户的1000人进行秒杀，丢弃其他请求，因为秒杀本来就是黑盒操作的，用户层面是无感知的，这样设计能让真实的用户买到东西，还可以减少自己被薅羊毛的概率。\n\n风控可以说是流量进入的最后一道门槛了，所以很多公司的风控是很强的，蚂蚁金服的风控大家如果了解过就知道了，你的资金在支付宝被盗了，他们是能做到全款补偿是有原因的。\n\n\n# 后端\n\n服务单一职责\n\n设计个能抗住高并发的系统，我觉得还是得单一职责。\n\n什么意思呢，大家都知道现在设计都是微服务的设计思想，然后再用分布式的部署方式。\n\n也就是我们下单是有个订单服务，用户登录管理等有个用户服务等等，那为啥我们不给秒杀也开个服务，我们把秒杀的代码业务逻辑放一起。\n\n单一职责的好处就是就算秒杀没抗住，秒杀库崩了，服务挂了，也不会影响到其他的服务。（高可用）\n\nredis 集群\n\n之前不是说单机的redis顶不住嘛，那简单多找几个兄弟啊，秒杀本来就是读多写少，那你们是不是瞬间想起来我之前跟你们提到过的，redis集群，主从同步、读写分离，我们还搞点哨兵，开启持久化直接无敌高可用！\n\n库存预热\n\n秒杀的本质，就是对库存的抢夺，每个秒杀的用户来你都去数据库查询库存校验库存，然后扣减库存，撇开性能因数，你不觉得这样好繁琐，对业务开发人员都不友好，而且数据库顶不住啊。\n\n我们都知道数据库顶不住但是他的兄弟非关系型的数据库redis能顶啊！\n\n那不简单了，我们要开始秒杀前你通过定时任务或者运维同学提前把商品的库存加载到redis中去，让整个流程都在redis里面去做，然后等秒杀介绍了，再异步的去修改库存就好了。\n\n但是用了redis就有一个问题了，我们上面说了我们采用主从，就是我们会去读取库存然后再判断然后有库存才去减库存，正常情况没问题，但是高并发的情况问题就很大了。\n\n**多品几遍！！！**就比如现在库存只剩下1个了，我们高并发嘛，4个服务器一起查询了发现都是还有1个，那大家都觉得是自己抢到了，就都去扣库存，那结果就变成了-3，是的只有一个是真的抢到了，别的都是超卖的。咋办？\n\n事务\n\nredis本身是支持事务的，而且他有很多原子命令的，大家也可以用lua，还可以用他的管道，乐观锁他也知支持。\n\n限流&降级&熔断&隔离\n\n这个为啥要做呢，不怕一万就怕万一，万一你真的顶不住了，限流，顶不住就挡一部分出去但是不能说不行，降级，降级了还是被打挂了，熔断，至少不要影响别的系统，隔离，你本身就独立的，但是你会调用其他的系统嘛，你快不行了你别拖累兄弟们啊。\n\n消息队列（削峰填谷）\n\n一说到这个名词，很多小伙伴就知道了，对的mq，你买东西少了你直接100个请求改库我觉得没问题，但是万一秒杀一万个，10万个呢？服务器挂了，程序员又要背锅的。\n\n秒杀就是这种瞬间流量很高，但是平时又没有流量的场景，那消息队列完全契合这样的场景了呀，削峰填谷。\n\n\n\ntip：可能小伙伴说我们业务达不到这个量级，没必要。但是我想说我们写代码，就不应该写出有逻辑漏洞的代码，至少以后公司体量上去了，别人一看居然不用改代码，一看代码作者是xxx？有点东西！\n\n你可以把它放消息队列，然后一点点消费去改库存就好了嘛，不过单个商品其实一次修改就够了，我这里说的是某个点多个商品一起秒杀的场景，像极了双十一零点。\n\n\n# 数据库\n\n数据库用mysql只要连接池设置合理一般问题是不大的，不过一般大公司不缺钱而且秒杀这样的活动十分频繁，我之前所在的公司就是这样秒杀特卖这样的场景一直都是不间断的。\n\n单独给秒杀建立一个数据库，为秒杀服务，表的设计也是竟可能的简单点，现在的互联网架构部署都是分库的。\n\n至于表就看大家怎么设计了，该设置索引的地方还是要设置索引的，建完后记得用explain看看sql的执行计划。（不了解的小伙伴也没事，mysql章节去康康）\n\n\n# 分布式事务\n\n这为啥我不放在后端而放到最后来讲呢？\n\n因为上面的任何一步都是可能出错的，而且我们是在不同的服务里面出错的，那就涉及分布式事务了，但是分布式事务大家想的是一定要成功什么的那就不对了，还是那句话，几个请求丢了就丢了，要保证时效和服务的可用可靠。\n\n所以tcc和最终一致性其实不是很适合，tcc开发成本很大，所有接口都要写三次，因为涉及tcc的三个阶段。\n\n最终一致性基本上都是靠轮训的操作去保证一个操作一定成功，那时效性就大打折扣了。\n\n大家觉得不那么可靠的**两段式（2pc）和三段式（3pc）**就派上用场了，他们不一定能保证数据最终一致，但是效率上还算ok。\n\n\n# 总结\n\n到这里我想我已经基本上把该考虑的点还有对应的解决方案也都说了一下，不知道还有没有没考虑到的，但是就算没考虑到我想我这个设计，应该也能撑住一个完整的秒杀流程。\n\n最后大家再看看这个秒杀系统或许会有新的感悟，是不是一个系统真的没有大家想的那么简单，而且我还是有漏掉的细节，这是一定的。\n\n\n# 参考文献\n\n面试了十个应届生九个都是秒杀系统，你确定你们那是秒杀？_小公司的qps只有几十-csdn博客",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"多级缓存",frontmatter:{title:"多级缓存",date:"2024-09-14T16:52:24.000Z",permalink:"/pages/51aa8b/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/07.%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98.html",relativePath:"30.经典场景设计/01.经典场景设计/07.多级缓存.md",key:"v-21c8ee34",path:"/pages/51aa8b/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"幂等&防重",frontmatter:{title:"幂等&防重",date:"2024-09-14T16:52:57.000Z",permalink:"/pages/4fc8cb/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/09.%E5%B9%82%E7%AD%89&%E9%98%B2%E9%87%8D.html",relativePath:"30.经典场景设计/01.经典场景设计/09.幂等&防重.md",key:"v-555cf7cc",path:"/pages/4fc8cb/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"1. insert 前先 select",slug:"_1-insert-前先-select",normalizedTitle:"1. insert 前先 select",charIndex:653},{level:2,title:"2. 加悲观锁",slug:"_2-加悲观锁",normalizedTitle:"2. 加悲观锁",charIndex:881},{level:2,title:"3. 加乐观锁",slug:"_3-加乐观锁",normalizedTitle:"3. 加乐观锁",charIndex:1715},{level:2,title:"4. 加唯一索引",slug:"_4-加唯一索引",normalizedTitle:"4. 加唯一索引",charIndex:2528},{level:2,title:"5. 建防重表",slug:"_5-建防重表",normalizedTitle:"5. 建防重表",charIndex:3040},{level:2,title:"6. 根据状态机",slug:"_6-根据状态机",normalizedTitle:"6. 根据状态机",charIndex:3397},{level:2,title:"7. 加分布式锁",slug:"_7-加分布式锁",normalizedTitle:"7. 加分布式锁",charIndex:3982},{level:2,title:"8. 获取token",slug:"_8-获取token",normalizedTitle:"8. 获取token",charIndex:4516},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4950},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:5048}],headersStr:"前言 1. insert 前先 select 2. 加悲观锁 3. 加乐观锁 4. 加唯一索引 5. 建防重表 6. 根据状态机 7. 加分布式锁 8. 获取token 总结 参考文献",content:"# 前言\n\n接口幂等性问题，对于开发人员来说，是一个跟语言无关的公共问题。本文分享了一些解决这类问题非常实用的办法，绝大部分内容我在项目中实践过的，给有需要的小伙伴一个参考。\n\n不知道你有没有遇到过这些场景：\n\n 1. 有时我们在填写某些form表单时，保存按钮不小心快速点了两次，表中竟然产生了两条重复的数据，只是id不一样。\n 2. 我们在项目中为了解决接口超时问题，通常会引入了重试机制。第一次请求接口超时了，请求方没能及时获取返回结果（此时有可能已经成功了），为了避免返回错误的结果（这种情况不可能直接返回失败吧？），于是会对该请求重试几次，这样也会产生重复的数据。\n 3. mq消费者在读取消息时，有时候会读取到重复消息（至于什么原因这里先不说，有兴趣的小伙伴，可以找我私聊），如果处理不好，也会产生重复的数据。\n\n没错，这些都是幂等性问题。\n\n接口幂等性是指用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。\n\n这类问题多发于接口的：\n\n * insert操作，这种情况下多次请求，可能会产生重复数据。\n * update操作，如果只是单纯的更新数据，比如：update user set status=1 where id=1，是没有问题的。如果还有计算，比如：update user set status=status+1 where id=1，这种情况下多次请求，可能会导致数据错误。\n\n那么我们要如何保证接口幂等性？本文将会告诉你答案。\n\n\n# 1. insert 前先 select\n\n通常情况下，在保存数据的接口中，我们为了防止产生重复数据，一般会在insert前，先根据name或code字段select一下数据。如果该数据已存在，则执行update操作，如果不存在，才执行 insert操作。\n\n\n\n该方案可能是我们平时在防止产生重复数据时，使用最多的方案。但是该方案不适用于并发场景，在并发场景中，要配合其他方案一起使用，否则同样会产生重复数据。我在这里提一下，是为了避免大家踩坑。\n\n\n# 2. 加悲观锁\n\n在支付场景中，用户A的账号余额有150元，想转出100元，正常情况下用户A的余额只剩50元。一般情况下，sql是这样的：\n\nupdate user amount = amount-100 where id=123;\n\n\n如果出现多次相同的请求，可能会导致用户A的余额变成负数。这种情况，用户A来可能要哭了。于此同时，系统开发人员可能也要哭了，因为这是很严重的系统bug。\n\n为了解决这个问题，可以加悲观锁，将用户A的那行数据锁住，在同一时刻只允许一个请求获得锁，更新数据，其他的请求则等待。\n\n通常情况下通过如下sql锁住单行数据：\n\nselect * from user id=123 for update;\n\n\n具体流程如下：\n\n\n\n具体步骤：\n\n 1. 多个请求同时根据id查询用户信息。\n 2. 判断余额是否不足100，如果余额不足，则直接返回余额不足。\n 3. 如果余额充足，则通过for update再次查询用户信息，并且尝试获取锁。\n 4. 只有第一个请求能获取到行锁，其余没有获取锁的请求，则等待下一次获取锁的机会。\n 5. 第一个请求获取到锁之后，判断余额是否不足100，如果余额足够，则进行update操作。\n 6. 如果余额不足，说明是重复请求，则直接返回成功。\n\n> 需要特别注意的是：如果使用的是mysql数据库，存储引擎必须用innodb，因为它才支持事务。此外，这里id字段一定要是主键或者唯一索引，不然会锁住整张表。\n\n悲观锁需要在同一个事务操作过程中锁住一行数据，如果事务耗时比较长，会造成大量的请求等待，影响接口性能。此外，每次请求接口很难保证都有相同的返回值，所以不适合幂等性设计场景，但是在防重场景中是可以的使用的。在这里顺便说一下，防重设计 和 幂等设计，其实是有区别的。防重设计主要为了避免产生重复数据，对接口返回没有太多要求。而幂等设计除了避免产生重复数据之外，还要求每次请求都返回一样的结果。\n\n\n# 3. 加乐观锁\n\n既然悲观锁有性能问题，为了提升接口性能，我们可以使用乐观锁。需要在表中增加一个timestamp或者version字段，这里以version字段为例。\n\n在更新数据之前先查询一下数据：\n\nselect id,amount,version from user id=123;\n\n\n如果数据存在，假设查到的version等于1，再使用id和version字段作为查询条件更新数据：\n\nupdate user set amount=amount+100,version=version+1\nwhere id=123 and version=1;\n\n\n更新数据的同时version+1，然后判断本次update操作的影响行数，如果大于0，则说明本次更新成功，如果等于0，则说明本次更新没有让数据变更。\n\n由于第一次请求version等于1是可以成功的，操作成功后version变成2了。这时如果并发的请求过来，再执行相同的sql：\n\nupdate user set amount=amount+100,version=version+1\nwhere id=123 and version=1;\n\n\n该update操作不会真正更新数据，最终sql的执行结果影响行数是0，因为version已经变成2了，where中的version=1肯定无法满足条件。但为了保证接口幂等性，接口可以直接返回成功，因为version值已经修改了，那么前面必定已经成功过一次，后面都是重复的请求。\n\n具体流程如下：\n\n\n\n具体步骤：\n\n 1. 先根据id查询用户信息，包含version字段\n 2. 根据id和version字段值作为where条件的参数，更新用户信息，同时version+1\n 3. 判断操作影响行数，如果影响1行，则说明是一次请求，可以做其他数据操作。\n 4. 如果影响0行，说明是重复请求，则直接返回成功。\n\n\n# 4. 加唯一索引\n\n绝大数情况下，为了防止重复数据的产生，我们都会在表中加唯一索引，这是一个非常简单，并且有效的方案。\n\nalter table `order` add UNIQUE KEY `un_code` (`code`);\n\n\n加了唯一索引之后，第一次请求数据可以插入成功。但后面的相同请求，插入数据时会报Duplicate entry '002' for key 'order.un_code异常，表示唯一索引有冲突。\n\n虽说抛异常对数据来说没有影响，不会造成错误数据。但是为了保证接口幂等性，我们需要对该异常进行捕获，然后返回成功。\n\n如果是java程序需要捕获：DuplicateKeyException异常，如果使用了spring框架还需要捕获：MySQLIntegrityConstraintViolationException异常。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 将该数据插入mysql\n 3. 判断是否执行成功，如果成功，则操作其他数据（可能还有其他的业务逻辑）。\n 4. 如果执行失败，捕获唯一索引冲突异常，直接返回成功。\n\n\n# 5. 建防重表\n\n有时候表中并非所有的场景都不允许产生重复的数据，只有某些特定场景才不允许。这时候，直接在表中加唯一索引，显然是不太合适的。\n\n针对这种情况，我们可以通过建防重表来解决问题。\n\n该表可以只包含两个字段：id 和 唯一索引，唯一索引可以是多个字段比如：name、code等组合起来的唯一标识，例如：susan_0001。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 将该数据插入mysql防重表\n 3. 判断是否执行成功，如果成功，则做mysql其他的数据操作（可能还有其他的业务逻辑）。\n 4. 如果执行失败，捕获唯一索引冲突异常，直接返回成功。\n\n> 需要特别注意的是：防重表和业务表必须在同一个数据库中，并且操作要在同一个事务中。\n\n\n# 6. 根据状态机\n\n很多时候业务表是有状态的，比如订单表中有：1-下单、2-已支付、3-完成、4-撤销等状态。如果这些状态的值是有规律的，按照业务节点正好是从小到大，我们就能通过它来保证接口的幂等性。\n\n假如id=123的订单状态是已支付，现在要变成完成状态。\n\nupdate `order` set status=3 where id=123 and status=2;\n\n\n第一次请求时，该订单的状态是已支付，值是2，所以该update语句可以正常更新数据，sql执行结果的影响行数是1，订单状态变成了3。\n\n后面有相同的请求过来，再执行相同的sql时，由于订单状态变成了3，再用status=2作为条件，无法查询出需要更新的数据，所以最终sql执行结果的影响行数是0，即不会真正的更新数据。但为了保证接口幂等性，影响行数是0时，接口也可以直接返回成功。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 根据id和当前状态作为条件，更新成下一个状态\n 3. 判断操作影响行数，如果影响了1行，说明当前操作成功，可以进行其他数据操作。\n 4. 如果影响了0行，说明是重复请求，直接返回成功。\n\n> 主要特别注意的是，该方案仅限于要更新的表有状态字段，并且刚好要更新状态字段的这种特殊情况，并非所有场景都适用。\n\n\n# 7. 加分布式锁\n\n其实前面介绍过的加唯一索引或者加防重表，本质是使用了数据库的分布式锁，也属于分布式锁的一种。但由于数据库分布式锁的性能不太好，我们可以改用：redis或zookeeper。\n\n鉴于现在很多公司分布式配置中心改用apollo或nacos，已经很少用zookeeper了，我们以redis为例介绍分布式锁。\n\n目前主要有三种方式实现redis的分布式锁：\n\n 1. setNx命令\n 2. set命令\n 3. Redission框架\n\n每种方案各有利弊，具体实现细节我就不说了，有兴趣的朋友可以加我微信找我私聊。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端会收集数据，并且生成订单号code作为唯一业务字段。\n 2. 使用redis的set命令，将该订单code设置到redis中，同时设置超时时间。\n 3. 判断是否设置成功，如果设置成功，说明是第一次请求，则进行数据操作。\n 4. 如果设置失败，说明是重复请求，则直接返回成功。\n\n> 需要特别注意的是：分布式锁一定要设置一个合理的过期时间，如果设置过短，无法有效的防止重复请求。如果设置过长，可能会浪费redis的存储空间，需要根据实际业务情况而定。\n\n\n# 8. 获取token\n\n除了上述方案之外，还有最后一种使用token的方案。该方案跟之前的所有方案都有点不一样，需要两次请求才能完成一次业务操作。\n\n 1. 第一次请求获取token\n 2. 第二次请求带着这个token，完成业务操作。\n\n具体流程图如下：\n\n第一步，先获取token。\n\n\n\n第二步，做具体业务操作。\n\n\n\n具体步骤：\n\n 1. 用户访问页面时，浏览器自动发起获取token请求。\n 2. 服务端生成token，保存到redis中，然后返回给浏览器。\n 3. 用户通过浏览器发起请求时，携带该token。\n 4. 在redis中查询该token是否存在，如果不存在，说明是第一次请求，做则后续的数据操作。\n 5. 如果存在，说明是重复请求，则直接返回成功。\n 6. 在redis中token会在过期时间之后，被自动删除。\n\n以上方案是针对幂等设计的。\n\n如果是防重设计，流程图要改改：\n\n\n\n> 需要特别注意的是：token必须是全局唯一的\n\n\n# 总结\n\n 1. insert 前先 select\n 2. 加悲观锁\n 3. 加乐观锁\n 4. 加唯一索引\n 5. 建防重表\n 6. 根据状态机\n 7. 加分布式锁\n 8. 获取token\n\n\n# 参考文献\n\n高并发下如何保证接口的幂等性？ - 苏三说技术 - 博客园 (cnblogs.com)",normalizedContent:"# 前言\n\n接口幂等性问题，对于开发人员来说，是一个跟语言无关的公共问题。本文分享了一些解决这类问题非常实用的办法，绝大部分内容我在项目中实践过的，给有需要的小伙伴一个参考。\n\n不知道你有没有遇到过这些场景：\n\n 1. 有时我们在填写某些form表单时，保存按钮不小心快速点了两次，表中竟然产生了两条重复的数据，只是id不一样。\n 2. 我们在项目中为了解决接口超时问题，通常会引入了重试机制。第一次请求接口超时了，请求方没能及时获取返回结果（此时有可能已经成功了），为了避免返回错误的结果（这种情况不可能直接返回失败吧？），于是会对该请求重试几次，这样也会产生重复的数据。\n 3. mq消费者在读取消息时，有时候会读取到重复消息（至于什么原因这里先不说，有兴趣的小伙伴，可以找我私聊），如果处理不好，也会产生重复的数据。\n\n没错，这些都是幂等性问题。\n\n接口幂等性是指用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。\n\n这类问题多发于接口的：\n\n * insert操作，这种情况下多次请求，可能会产生重复数据。\n * update操作，如果只是单纯的更新数据，比如：update user set status=1 where id=1，是没有问题的。如果还有计算，比如：update user set status=status+1 where id=1，这种情况下多次请求，可能会导致数据错误。\n\n那么我们要如何保证接口幂等性？本文将会告诉你答案。\n\n\n# 1. insert 前先 select\n\n通常情况下，在保存数据的接口中，我们为了防止产生重复数据，一般会在insert前，先根据name或code字段select一下数据。如果该数据已存在，则执行update操作，如果不存在，才执行 insert操作。\n\n\n\n该方案可能是我们平时在防止产生重复数据时，使用最多的方案。但是该方案不适用于并发场景，在并发场景中，要配合其他方案一起使用，否则同样会产生重复数据。我在这里提一下，是为了避免大家踩坑。\n\n\n# 2. 加悲观锁\n\n在支付场景中，用户a的账号余额有150元，想转出100元，正常情况下用户a的余额只剩50元。一般情况下，sql是这样的：\n\nupdate user amount = amount-100 where id=123;\n\n\n如果出现多次相同的请求，可能会导致用户a的余额变成负数。这种情况，用户a来可能要哭了。于此同时，系统开发人员可能也要哭了，因为这是很严重的系统bug。\n\n为了解决这个问题，可以加悲观锁，将用户a的那行数据锁住，在同一时刻只允许一个请求获得锁，更新数据，其他的请求则等待。\n\n通常情况下通过如下sql锁住单行数据：\n\nselect * from user id=123 for update;\n\n\n具体流程如下：\n\n\n\n具体步骤：\n\n 1. 多个请求同时根据id查询用户信息。\n 2. 判断余额是否不足100，如果余额不足，则直接返回余额不足。\n 3. 如果余额充足，则通过for update再次查询用户信息，并且尝试获取锁。\n 4. 只有第一个请求能获取到行锁，其余没有获取锁的请求，则等待下一次获取锁的机会。\n 5. 第一个请求获取到锁之后，判断余额是否不足100，如果余额足够，则进行update操作。\n 6. 如果余额不足，说明是重复请求，则直接返回成功。\n\n> 需要特别注意的是：如果使用的是mysql数据库，存储引擎必须用innodb，因为它才支持事务。此外，这里id字段一定要是主键或者唯一索引，不然会锁住整张表。\n\n悲观锁需要在同一个事务操作过程中锁住一行数据，如果事务耗时比较长，会造成大量的请求等待，影响接口性能。此外，每次请求接口很难保证都有相同的返回值，所以不适合幂等性设计场景，但是在防重场景中是可以的使用的。在这里顺便说一下，防重设计 和 幂等设计，其实是有区别的。防重设计主要为了避免产生重复数据，对接口返回没有太多要求。而幂等设计除了避免产生重复数据之外，还要求每次请求都返回一样的结果。\n\n\n# 3. 加乐观锁\n\n既然悲观锁有性能问题，为了提升接口性能，我们可以使用乐观锁。需要在表中增加一个timestamp或者version字段，这里以version字段为例。\n\n在更新数据之前先查询一下数据：\n\nselect id,amount,version from user id=123;\n\n\n如果数据存在，假设查到的version等于1，再使用id和version字段作为查询条件更新数据：\n\nupdate user set amount=amount+100,version=version+1\nwhere id=123 and version=1;\n\n\n更新数据的同时version+1，然后判断本次update操作的影响行数，如果大于0，则说明本次更新成功，如果等于0，则说明本次更新没有让数据变更。\n\n由于第一次请求version等于1是可以成功的，操作成功后version变成2了。这时如果并发的请求过来，再执行相同的sql：\n\nupdate user set amount=amount+100,version=version+1\nwhere id=123 and version=1;\n\n\n该update操作不会真正更新数据，最终sql的执行结果影响行数是0，因为version已经变成2了，where中的version=1肯定无法满足条件。但为了保证接口幂等性，接口可以直接返回成功，因为version值已经修改了，那么前面必定已经成功过一次，后面都是重复的请求。\n\n具体流程如下：\n\n\n\n具体步骤：\n\n 1. 先根据id查询用户信息，包含version字段\n 2. 根据id和version字段值作为where条件的参数，更新用户信息，同时version+1\n 3. 判断操作影响行数，如果影响1行，则说明是一次请求，可以做其他数据操作。\n 4. 如果影响0行，说明是重复请求，则直接返回成功。\n\n\n# 4. 加唯一索引\n\n绝大数情况下，为了防止重复数据的产生，我们都会在表中加唯一索引，这是一个非常简单，并且有效的方案。\n\nalter table `order` add unique key `un_code` (`code`);\n\n\n加了唯一索引之后，第一次请求数据可以插入成功。但后面的相同请求，插入数据时会报duplicate entry '002' for key 'order.un_code异常，表示唯一索引有冲突。\n\n虽说抛异常对数据来说没有影响，不会造成错误数据。但是为了保证接口幂等性，我们需要对该异常进行捕获，然后返回成功。\n\n如果是java程序需要捕获：duplicatekeyexception异常，如果使用了spring框架还需要捕获：mysqlintegrityconstraintviolationexception异常。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 将该数据插入mysql\n 3. 判断是否执行成功，如果成功，则操作其他数据（可能还有其他的业务逻辑）。\n 4. 如果执行失败，捕获唯一索引冲突异常，直接返回成功。\n\n\n# 5. 建防重表\n\n有时候表中并非所有的场景都不允许产生重复的数据，只有某些特定场景才不允许。这时候，直接在表中加唯一索引，显然是不太合适的。\n\n针对这种情况，我们可以通过建防重表来解决问题。\n\n该表可以只包含两个字段：id 和 唯一索引，唯一索引可以是多个字段比如：name、code等组合起来的唯一标识，例如：susan_0001。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 将该数据插入mysql防重表\n 3. 判断是否执行成功，如果成功，则做mysql其他的数据操作（可能还有其他的业务逻辑）。\n 4. 如果执行失败，捕获唯一索引冲突异常，直接返回成功。\n\n> 需要特别注意的是：防重表和业务表必须在同一个数据库中，并且操作要在同一个事务中。\n\n\n# 6. 根据状态机\n\n很多时候业务表是有状态的，比如订单表中有：1-下单、2-已支付、3-完成、4-撤销等状态。如果这些状态的值是有规律的，按照业务节点正好是从小到大，我们就能通过它来保证接口的幂等性。\n\n假如id=123的订单状态是已支付，现在要变成完成状态。\n\nupdate `order` set status=3 where id=123 and status=2;\n\n\n第一次请求时，该订单的状态是已支付，值是2，所以该update语句可以正常更新数据，sql执行结果的影响行数是1，订单状态变成了3。\n\n后面有相同的请求过来，再执行相同的sql时，由于订单状态变成了3，再用status=2作为条件，无法查询出需要更新的数据，所以最终sql执行结果的影响行数是0，即不会真正的更新数据。但为了保证接口幂等性，影响行数是0时，接口也可以直接返回成功。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端收集数据。\n 2. 根据id和当前状态作为条件，更新成下一个状态\n 3. 判断操作影响行数，如果影响了1行，说明当前操作成功，可以进行其他数据操作。\n 4. 如果影响了0行，说明是重复请求，直接返回成功。\n\n> 主要特别注意的是，该方案仅限于要更新的表有状态字段，并且刚好要更新状态字段的这种特殊情况，并非所有场景都适用。\n\n\n# 7. 加分布式锁\n\n其实前面介绍过的加唯一索引或者加防重表，本质是使用了数据库的分布式锁，也属于分布式锁的一种。但由于数据库分布式锁的性能不太好，我们可以改用：redis或zookeeper。\n\n鉴于现在很多公司分布式配置中心改用apollo或nacos，已经很少用zookeeper了，我们以redis为例介绍分布式锁。\n\n目前主要有三种方式实现redis的分布式锁：\n\n 1. setnx命令\n 2. set命令\n 3. redission框架\n\n每种方案各有利弊，具体实现细节我就不说了，有兴趣的朋友可以加我微信找我私聊。\n\n具体流程图如下：\n\n\n\n具体步骤：\n\n 1. 用户通过浏览器发起请求，服务端会收集数据，并且生成订单号code作为唯一业务字段。\n 2. 使用redis的set命令，将该订单code设置到redis中，同时设置超时时间。\n 3. 判断是否设置成功，如果设置成功，说明是第一次请求，则进行数据操作。\n 4. 如果设置失败，说明是重复请求，则直接返回成功。\n\n> 需要特别注意的是：分布式锁一定要设置一个合理的过期时间，如果设置过短，无法有效的防止重复请求。如果设置过长，可能会浪费redis的存储空间，需要根据实际业务情况而定。\n\n\n# 8. 获取token\n\n除了上述方案之外，还有最后一种使用token的方案。该方案跟之前的所有方案都有点不一样，需要两次请求才能完成一次业务操作。\n\n 1. 第一次请求获取token\n 2. 第二次请求带着这个token，完成业务操作。\n\n具体流程图如下：\n\n第一步，先获取token。\n\n\n\n第二步，做具体业务操作。\n\n\n\n具体步骤：\n\n 1. 用户访问页面时，浏览器自动发起获取token请求。\n 2. 服务端生成token，保存到redis中，然后返回给浏览器。\n 3. 用户通过浏览器发起请求时，携带该token。\n 4. 在redis中查询该token是否存在，如果不存在，说明是第一次请求，做则后续的数据操作。\n 5. 如果存在，说明是重复请求，则直接返回成功。\n 6. 在redis中token会在过期时间之后，被自动删除。\n\n以上方案是针对幂等设计的。\n\n如果是防重设计，流程图要改改：\n\n\n\n> 需要特别注意的是：token必须是全局唯一的\n\n\n# 总结\n\n 1. insert 前先 select\n 2. 加悲观锁\n 3. 加乐观锁\n 4. 加唯一索引\n 5. 建防重表\n 6. 根据状态机\n 7. 加分布式锁\n 8. 获取token\n\n\n# 参考文献\n\n高并发下如何保证接口的幂等性？ - 苏三说技术 - 博客园 (cnblogs.com)",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"海量数据计数",frontmatter:{title:"海量数据计数",date:"2024-09-14T16:52:01.000Z",permalink:"/pages/f3295f/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/10.%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E8%AE%A1%E6%95%B0.html",relativePath:"30.经典场景设计/01.经典场景设计/10.海量数据计数.md",key:"v-ded59338",path:"/pages/f3295f/",headers:[{level:2,title:"引子",slug:"引子",normalizedTitle:"引子",charIndex:2},{level:2,title:"计数在业务上的特点",slug:"计数在业务上的特点",normalizedTitle:"计数在业务上的特点",charIndex:410},{level:2,title:"支撑高并发的计数系统要如何设计",slug:"支撑高并发的计数系统要如何设计",normalizedTitle:"支撑高并发的计数系统要如何设计",charIndex:808},{level:2,title:"如何降低计数系统的存储成本",slug:"如何降低计数系统的存储成本",normalizedTitle:"如何降低计数系统的存储成本",charIndex:2153},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4116}],headersStr:"引子 计数在业务上的特点 支撑高并发的计数系统要如何设计 如何降低计数系统的存储成本 总结",content:"# 引子\n\n在地铁上，你也许会经常刷微博、点赞热搜，如果有抽奖活动，再转 发一波，而这些与微博息息相关的数据，其实就是微博场景下的计数数据，细说起来，它主要有几类\n\n * 微博的评论数、点赞数、转发数、浏览数、表态数等等；\n * 用户的粉丝数、关注数、发布微博数、私信数等等。\n\n微博维度的计数代表了这条微博受欢迎的程度，用户维度的数据（尤其是粉丝数），代表了这个用户的影响力，因此大家会普遍看重这些计数信息。并且在很多场景下，我们都需要查询计数数据（比如首页信息流页面、个人主页面），计数数据访问量巨大，所以需要设计计数系统维护它\n\n但在设计计数系统时，不少人会出现性能不高、存储成本很大的问题，比如，把计数与微博数据存储在一起，这样每次更新计数的时候都需要锁住这一行记录，降低了写入的并发。在我看来，之所以出现这些问题，还是因为你对计数系统的设计和优化不甚了解，所以要想解决痛点，你有必要形成完备的设计方案\n\n\n# 计数在业务上的特点\n\n * 数据量巨大，微博系统中微博条目的数量早已经超过了千亿级别，仅仅计算 微博的转发、评论、点赞、浏览等核心计数，其数据量级就已经在几千亿的级别。更何 况微博条目的数量还在不断高速地增长，并且随着微博业务越来越复杂，微博维度的计数种类也可能会持续扩展（比如说增加了表态数），因此，仅仅是微博维度上的计数量 级就已经过了万亿级别。除此之外，微博的用户量级已经超过了 10 亿，用户维度的计数 量级相比微博维度来说虽然相差很大，但是也达到了百亿级别。那么如何存储这些过万 亿级别的数字，对我们来说就是一大挑战\n * 访问量大，对于性能的要求高。微博的日活用户超过 2 亿，月活用户接近 5 亿，核心服 务（比如首页信息流）访问量级到达每秒几十万次，计数系统的访问量级也超过了每秒 百万级别，而且在性能方面，它要求要毫秒级别返回结果\n * 对于可用性、数字的准确性要求高\n\n\n# 支撑高并发的计数系统要如何设计\n\n刚开始设计计数系统的时候，微博的流量还没有现在这么夸张，我们本着 KISS（Keep It Simple and Stupid）原则，尽量将系统设计的简单易维护，所以，我们使用 MySQL 存储计数的数据，因为它是我们最熟悉的，团队在运维上经验也会比较丰富。举个具体的例子。\n\nselect repost_count, comment_count, praise_count, view_count from t_weibo_count\n\n\n随着微博的不断壮大，之前的计数系统面临了很多的问题和挑战。\n\n比如微博用户量和发布的微博量增加迅猛，计数存储数据量级也飞速增长，而 MySQL 数据库单表的存储量级达到几千万的时候，性能上就会有损耗。所以我们考虑使用分库分表的 方式分散数据量，提升读取计数的性能。\n\n我们用“weibo_id”作为分区键，在选择分库分表的方式时，考虑了下面两种\n\n * 一种方式是选择一种哈希算法对 weibo_id 计算哈希值，然后依据这个哈希值计算出需 要存储到哪一个库哪一张表中\n * 另一种方式是按照 weibo_id 生成的时间来做分库分表，我们在第 10 讲谈到发号器的时候曾经提到，ID 的生成最好带有业务意义的字段，比如生成 ID 的时间戳\n\n分表的时候，可以先依据发号器的算法反解出时间戳，然后按照时间戳来做分库分表， 比如，一天一张表或者一个月一张表等等。\n\n因为越是最近发布的微博，计数数据的访问量就越大，所以虽然我考虑了两种方案，但是按照时间来分库分表会造成数据访问的不均匀，最后用了哈希的方式来做分库分表。\n\n\n\n与此同时，计数的访问量级也有质的飞越。在微博最初的版本中，首页信息流里面是不展示 计数数据的，那么使用 MySQL 也可以承受当时读取计数的访问量。但是后来在首页信息流中也要展示转发、评论和点赞等计数数据了。而信息流的访问量巨大，仅仅靠数据库已经 完全不能承担如此高的并发量了。于是我们考虑使用 Redis 来加速读请求，通过部署多个从节点来提升可用性和性能，并且通过 Hash 的方式对数据做分片，也基本上可以保证计数的读取性能。然而，这种数据库 + 缓存的方式有一个弊端：无法保证数据的一致性，比如，如果数据库写入成功而缓存更新失败，就会导致数据的不一致，影响计数的准确性。所以，我们完全抛弃了 MySQL，全面使用 Redis 来作为计数的存储组\n\n\n\n除了考虑计数的读取性能之外，由于热门微博的计数变化频率相当快，也需要考虑如何提升 计数的写入性能。比如，每次在转发一条微博的时候，都需要增加这条微博的转发数，那么 如果明星发布结婚、离婚的微博，瞬时就可能会产生几万甚至几十万的转发。如果是你的话，要如何降低写压力呢？\n\n你可能已经想到用消息队列来削峰填谷了，也就是说，我们在转发微博的时候向消息队列写 入一条消息，然后在消息处理程序中给这条微博的转发计数加 1。这里需要注意的一点， 我们可以通过批量处理消息的方式进一步减小 Redis 的写压力，比如像下面这样连续更改 三次转发数（我用 SQL 来表示来方便你理解）：\n\n\n\n这个时候，你可以把它们合并成一次更新：\n\n\n\n\n# 如何降低计数系统的存储成本\n\n讲到这里，我其实已经告诉你一个支撑高并发查询请求的计数系统是如何实现的了。但是在微博的场景下，计数的量级是万亿的级别，这也给我们提了更高的要求，就是如何在有限的存储成本下实现对于全量计数数据的存取。\n\n你知道，Redis 是使用内存来存储信息，相比于使用磁盘存储数据的 MySQL 来说，存储的成本不可同日而语，比如一台服务器磁盘可以挂载到 2 个 T，但是内存可能只有 128G，这样磁盘的存储空间就是内存的 16 倍。而 Redis 基于通用性的考虑，对于内存的使用比较粗 放，存在大量的指针以及额外数据结构的开销，如果要存储一个 KV 类型的计数信息，Key 是 8 字节 Long 类型的 weibo_id，Value 是 4 字节 int 类型的转发数，存储在 Redis 中之后会占用超过 70 个字节的空间，空间的浪费是巨大的。如果你面临这个问题，要如何优化呢\n\n我建议你先对原生 Redis 做一些改造，采用新的数据结构和数据类型来存储计数数据。我 在改造时，主要涉及了两点\n\n * 一是原生的 Redis 在存储 Key 时是按照字符串类型来存储的，比如一个 8 字节的 Long 类型的数据，需要 8（sdshdr 数据结构长度）+ 19（8 字节数字的长度）+1（’\\0’） =28 个字节，如果我们使用 Long 类型来存储就只需要 8 个字节，会节省 20 个字节的 空间；\n * 二是去除了原生 Redis 中多余的指针，如果要存储一个 KV 信息就只需要 8（weibo_id）+4（转发数）=12 个字节，相比之前有很大的改进。\n\n同时，我们也会使用一个大的数组来存储计数信息，存储的位置是基于 weibo_id 的哈希值来计算出来的，具体的算法像下面展示的这样：\n\n插入时:\nh1 = hash1(weibo_id) // 根据微博 ID 计算 Hash\nh2 = hash2(weibo_id) // 根据微博 ID 计算另一个 Hash，用以解决前一个 Hash 算法带来的冲突\nfor s in 0,1000\n\tpos = (h1 + h2*s) % tsize // 如果发生冲突，就多算几次 Hash2\n\t\tif(isempty(pos) || isdelete(pos))\n\t\t\tt[ pos ] = item  // 写入数组\n查询时:\nfor s in 0,1000\n\tpos = (h1 + h2*s) % tsize  // 依照插入数据时候的逻辑，计算出存储在数组中的位置\n\t\tif(!isempty(pos) && t[pos]==weibo_id)\n\t\t\treturn t[pos]\nreturn 0 \n删除时:\ninsert(FFFF) // 插入一个特殊的标\n\n\n在对原生的 Redis 做了改造之后，你还需要进一步考虑如何节省内存的使用。比如，微博的计数有转发数、评论数、浏览数、点赞数等等，如果每一个计数都需要存储 weibo_id， 那么总共就需要 8（weibo_id）*4（4 个微博 ID）+4（转发数） + 4（评论数） + 4（点 赞数） + 4（浏览数）= 48 字节。但是我们可以把相同微博 ID 的计数存储在一起，这样就只需要记录一个微博 ID，省掉了多余的三个微博 ID 的存储开销，存储空间就进一步减少 了。\n\n不过，即使经过上面的优化，由于计数的量级实在是太过巨大，并且还在以极快的速度增 长，所以如果我们以全内存的方式来存储计数信息，就需要使用非常多的机器来支撑。\n\n冷热分离\n\n然而微博计数的数据具有明显的热点属性：越是最近的微博越是会被访问到，时间上久远的微博被访问的几率很小。所以为了尽量减少服务器的使用，我们考虑给计数服务增加 SSD 磁盘，然后将时间上比较久远的数据 dump 到磁盘上，内存中只保留最近的数据。当我们要读取冷数据的时候，使用单独的 I/O 线程异步地将冷数据从 SSD 磁盘中加载到一块儿单 独的 Cold Cache 中\n\n\n\n在经过了上面这些优化之后，我们的计数服务就可以支撑高并发大数据量的考验，无论是在 性能上、成本上和可用性上都能够达到业务的需求了。\n\n总的来说，我用微博设计计数系统的例子，并不是仅仅告诉你计数系统是如何做的，而是想 告诉你在做系统设计的时候需要了解自己系统目前的痛点是什么，然后再针对痛点来做细致 的优化。比如，微博计数系统的痛点是存储的成本，那么我们后期做的事情很多都是围绕着 如何使用有限的服务器存储全量的计数数据，即使是对开源组件（Redis）做深度的定制会 带来很大的运维成本，也只能被认为是为了实现计数系统而必须要做的权衡。\n\n\n# 总结\n\n 1. 数据库 + 缓存的方案是计数系统的初级阶段，完全可以支撑中小访问量和存储量的存储 服务。如果你的项目还处在初级阶段，量级还不是很大，那么你一开始可以考虑使用这 种方案。\n 2. 通过对原生 Redis 组件的改造，我们可以极大地减小存储数据的内存开销。\n 3. 使用 SSD+ 内存的方案可以最终解决存储计数数据的成本问题。这个方式适用于冷热数 据明显的场景，你在使用时需要考虑如何将内存中的数据做换入换出",normalizedContent:"# 引子\n\n在地铁上，你也许会经常刷微博、点赞热搜，如果有抽奖活动，再转 发一波，而这些与微博息息相关的数据，其实就是微博场景下的计数数据，细说起来，它主要有几类\n\n * 微博的评论数、点赞数、转发数、浏览数、表态数等等；\n * 用户的粉丝数、关注数、发布微博数、私信数等等。\n\n微博维度的计数代表了这条微博受欢迎的程度，用户维度的数据（尤其是粉丝数），代表了这个用户的影响力，因此大家会普遍看重这些计数信息。并且在很多场景下，我们都需要查询计数数据（比如首页信息流页面、个人主页面），计数数据访问量巨大，所以需要设计计数系统维护它\n\n但在设计计数系统时，不少人会出现性能不高、存储成本很大的问题，比如，把计数与微博数据存储在一起，这样每次更新计数的时候都需要锁住这一行记录，降低了写入的并发。在我看来，之所以出现这些问题，还是因为你对计数系统的设计和优化不甚了解，所以要想解决痛点，你有必要形成完备的设计方案\n\n\n# 计数在业务上的特点\n\n * 数据量巨大，微博系统中微博条目的数量早已经超过了千亿级别，仅仅计算 微博的转发、评论、点赞、浏览等核心计数，其数据量级就已经在几千亿的级别。更何 况微博条目的数量还在不断高速地增长，并且随着微博业务越来越复杂，微博维度的计数种类也可能会持续扩展（比如说增加了表态数），因此，仅仅是微博维度上的计数量 级就已经过了万亿级别。除此之外，微博的用户量级已经超过了 10 亿，用户维度的计数 量级相比微博维度来说虽然相差很大，但是也达到了百亿级别。那么如何存储这些过万 亿级别的数字，对我们来说就是一大挑战\n * 访问量大，对于性能的要求高。微博的日活用户超过 2 亿，月活用户接近 5 亿，核心服 务（比如首页信息流）访问量级到达每秒几十万次，计数系统的访问量级也超过了每秒 百万级别，而且在性能方面，它要求要毫秒级别返回结果\n * 对于可用性、数字的准确性要求高\n\n\n# 支撑高并发的计数系统要如何设计\n\n刚开始设计计数系统的时候，微博的流量还没有现在这么夸张，我们本着 kiss（keep it simple and stupid）原则，尽量将系统设计的简单易维护，所以，我们使用 mysql 存储计数的数据，因为它是我们最熟悉的，团队在运维上经验也会比较丰富。举个具体的例子。\n\nselect repost_count, comment_count, praise_count, view_count from t_weibo_count\n\n\n随着微博的不断壮大，之前的计数系统面临了很多的问题和挑战。\n\n比如微博用户量和发布的微博量增加迅猛，计数存储数据量级也飞速增长，而 mysql 数据库单表的存储量级达到几千万的时候，性能上就会有损耗。所以我们考虑使用分库分表的 方式分散数据量，提升读取计数的性能。\n\n我们用“weibo_id”作为分区键，在选择分库分表的方式时，考虑了下面两种\n\n * 一种方式是选择一种哈希算法对 weibo_id 计算哈希值，然后依据这个哈希值计算出需 要存储到哪一个库哪一张表中\n * 另一种方式是按照 weibo_id 生成的时间来做分库分表，我们在第 10 讲谈到发号器的时候曾经提到，id 的生成最好带有业务意义的字段，比如生成 id 的时间戳\n\n分表的时候，可以先依据发号器的算法反解出时间戳，然后按照时间戳来做分库分表， 比如，一天一张表或者一个月一张表等等。\n\n因为越是最近发布的微博，计数数据的访问量就越大，所以虽然我考虑了两种方案，但是按照时间来分库分表会造成数据访问的不均匀，最后用了哈希的方式来做分库分表。\n\n\n\n与此同时，计数的访问量级也有质的飞越。在微博最初的版本中，首页信息流里面是不展示 计数数据的，那么使用 mysql 也可以承受当时读取计数的访问量。但是后来在首页信息流中也要展示转发、评论和点赞等计数数据了。而信息流的访问量巨大，仅仅靠数据库已经 完全不能承担如此高的并发量了。于是我们考虑使用 redis 来加速读请求，通过部署多个从节点来提升可用性和性能，并且通过 hash 的方式对数据做分片，也基本上可以保证计数的读取性能。然而，这种数据库 + 缓存的方式有一个弊端：无法保证数据的一致性，比如，如果数据库写入成功而缓存更新失败，就会导致数据的不一致，影响计数的准确性。所以，我们完全抛弃了 mysql，全面使用 redis 来作为计数的存储组\n\n\n\n除了考虑计数的读取性能之外，由于热门微博的计数变化频率相当快，也需要考虑如何提升 计数的写入性能。比如，每次在转发一条微博的时候，都需要增加这条微博的转发数，那么 如果明星发布结婚、离婚的微博，瞬时就可能会产生几万甚至几十万的转发。如果是你的话，要如何降低写压力呢？\n\n你可能已经想到用消息队列来削峰填谷了，也就是说，我们在转发微博的时候向消息队列写 入一条消息，然后在消息处理程序中给这条微博的转发计数加 1。这里需要注意的一点， 我们可以通过批量处理消息的方式进一步减小 redis 的写压力，比如像下面这样连续更改 三次转发数（我用 sql 来表示来方便你理解）：\n\n\n\n这个时候，你可以把它们合并成一次更新：\n\n\n\n\n# 如何降低计数系统的存储成本\n\n讲到这里，我其实已经告诉你一个支撑高并发查询请求的计数系统是如何实现的了。但是在微博的场景下，计数的量级是万亿的级别，这也给我们提了更高的要求，就是如何在有限的存储成本下实现对于全量计数数据的存取。\n\n你知道，redis 是使用内存来存储信息，相比于使用磁盘存储数据的 mysql 来说，存储的成本不可同日而语，比如一台服务器磁盘可以挂载到 2 个 t，但是内存可能只有 128g，这样磁盘的存储空间就是内存的 16 倍。而 redis 基于通用性的考虑，对于内存的使用比较粗 放，存在大量的指针以及额外数据结构的开销，如果要存储一个 kv 类型的计数信息，key 是 8 字节 long 类型的 weibo_id，value 是 4 字节 int 类型的转发数，存储在 redis 中之后会占用超过 70 个字节的空间，空间的浪费是巨大的。如果你面临这个问题，要如何优化呢\n\n我建议你先对原生 redis 做一些改造，采用新的数据结构和数据类型来存储计数数据。我 在改造时，主要涉及了两点\n\n * 一是原生的 redis 在存储 key 时是按照字符串类型来存储的，比如一个 8 字节的 long 类型的数据，需要 8（sdshdr 数据结构长度）+ 19（8 字节数字的长度）+1（’\\0’） =28 个字节，如果我们使用 long 类型来存储就只需要 8 个字节，会节省 20 个字节的 空间；\n * 二是去除了原生 redis 中多余的指针，如果要存储一个 kv 信息就只需要 8（weibo_id）+4（转发数）=12 个字节，相比之前有很大的改进。\n\n同时，我们也会使用一个大的数组来存储计数信息，存储的位置是基于 weibo_id 的哈希值来计算出来的，具体的算法像下面展示的这样：\n\n插入时:\nh1 = hash1(weibo_id) // 根据微博 id 计算 hash\nh2 = hash2(weibo_id) // 根据微博 id 计算另一个 hash，用以解决前一个 hash 算法带来的冲突\nfor s in 0,1000\n\tpos = (h1 + h2*s) % tsize // 如果发生冲突，就多算几次 hash2\n\t\tif(isempty(pos) || isdelete(pos))\n\t\t\tt[ pos ] = item  // 写入数组\n查询时:\nfor s in 0,1000\n\tpos = (h1 + h2*s) % tsize  // 依照插入数据时候的逻辑，计算出存储在数组中的位置\n\t\tif(!isempty(pos) && t[pos]==weibo_id)\n\t\t\treturn t[pos]\nreturn 0 \n删除时:\ninsert(ffff) // 插入一个特殊的标\n\n\n在对原生的 redis 做了改造之后，你还需要进一步考虑如何节省内存的使用。比如，微博的计数有转发数、评论数、浏览数、点赞数等等，如果每一个计数都需要存储 weibo_id， 那么总共就需要 8（weibo_id）*4（4 个微博 id）+4（转发数） + 4（评论数） + 4（点 赞数） + 4（浏览数）= 48 字节。但是我们可以把相同微博 id 的计数存储在一起，这样就只需要记录一个微博 id，省掉了多余的三个微博 id 的存储开销，存储空间就进一步减少 了。\n\n不过，即使经过上面的优化，由于计数的量级实在是太过巨大，并且还在以极快的速度增 长，所以如果我们以全内存的方式来存储计数信息，就需要使用非常多的机器来支撑。\n\n冷热分离\n\n然而微博计数的数据具有明显的热点属性：越是最近的微博越是会被访问到，时间上久远的微博被访问的几率很小。所以为了尽量减少服务器的使用，我们考虑给计数服务增加 ssd 磁盘，然后将时间上比较久远的数据 dump 到磁盘上，内存中只保留最近的数据。当我们要读取冷数据的时候，使用单独的 i/o 线程异步地将冷数据从 ssd 磁盘中加载到一块儿单 独的 cold cache 中\n\n\n\n在经过了上面这些优化之后，我们的计数服务就可以支撑高并发大数据量的考验，无论是在 性能上、成本上和可用性上都能够达到业务的需求了。\n\n总的来说，我用微博设计计数系统的例子，并不是仅仅告诉你计数系统是如何做的，而是想 告诉你在做系统设计的时候需要了解自己系统目前的痛点是什么，然后再针对痛点来做细致 的优化。比如，微博计数系统的痛点是存储的成本，那么我们后期做的事情很多都是围绕着 如何使用有限的服务器存储全量的计数数据，即使是对开源组件（redis）做深度的定制会 带来很大的运维成本，也只能被认为是为了实现计数系统而必须要做的权衡。\n\n\n# 总结\n\n 1. 数据库 + 缓存的方案是计数系统的初级阶段，完全可以支撑中小访问量和存储量的存储 服务。如果你的项目还处在初级阶段，量级还不是很大，那么你一开始可以考虑使用这 种方案。\n 2. 通过对原生 redis 组件的改造，我们可以极大地减小存储数据的内存开销。\n 3. 使用 ssd+ 内存的方案可以最终解决存储计数数据的成本问题。这个方式适用于冷热数 据明显的场景，你在使用时需要考虑如何将内存中的数据做换入换出",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"消息未读数系统",frontmatter:{title:"消息未读数系统",date:"2024-09-14T23:57:17.000Z",permalink:"/pages/6b9d68/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/11.%E6%B6%88%E6%81%AF%E6%9C%AA%E8%AF%BB%E6%95%B0%E7%B3%BB%E7%BB%9F.html",relativePath:"30.经典场景设计/01.经典场景设计/11.消息未读数系统.md",key:"v-9c3df33c",path:"/pages/6b9d68/",headers:[{level:2,title:"引子",slug:"引子",normalizedTitle:"引子",charIndex:2},{level:2,title:"系统通知的未读数要如何设计",slug:"系统通知的未读数要如何设计",normalizedTitle:"系统通知的未读数要如何设计",charIndex:242},{level:2,title:"如何为信息流的未读数设计方案",slug:"如何为信息流的未读数设计方案",normalizedTitle:"如何为信息流的未读数设计方案",charIndex:2043},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:3457}],headersStr:"引子 系统通知的未读数要如何设计 如何为信息流的未读数设计方案 总结",content:"# 引子\n\n未读数也是系统中一个常见的模块，以微博系统为例，你可看到有多个未读计数的场景，\n\n比如：\n\n * 当有人 @你、评论你、给你的博文点赞或者给你发送私信的时候，你会收到相应的未读提醒；\n * 在早期的微博版本中有系统通知的功能，也就是系统会给全部用户发送消息，通知用户有新的版本或者有一些好玩的运营活动，如果用户没有看，系统就会给他展示有多少条未读的提醒。\n * 我们在浏览信息流的时候，如果长时间没有刷新页面，那么信息流上方就会提示你在这段时间有多少条信息没有看\n\n\n# 系统通知的未读数要如何设计\n\n来看具体的例子。假如你的系统中只有 A、B、C 三个用户，那么你可以在通用计数系统中 增加一块儿内存区域，并且以用户 ID 为 Key 来存储这三个用户的未读通知数据，当系统发 送一个新的通知时，我们会循环给每一个用户的未读数加 1，这个处理逻辑的伪代码就像下 面这样\n\nList<Long> userIds = getAllUserIds();\n    for(Long id : userIds) {\n    incrUnreadCount(id);\n}\n\n\n但随着系统中的用户越来越多，这个方案存在两个致命的问题\n\n * 首先，获取全量用户就是一个比较耗时的操作，相当于对用户库做一次全表的扫描，这不仅会对数据库造成很大的压力，而且查询全量用户数据的响应时间是很长的，对于在线业务来说是难以接受的。如果你的用户库已经做了分库分表，那么就要扫描所有的库表，响应时间就更长了。不过有一个折中的方法， 那就是在发送系统通知之前，先从线下的数据仓库中 获取全量的用户 ID，并且存储在一个本地的文件中，然后再轮询所有的用户 ID，给这些用 户增加未读计数。\n   * 这似乎是一个可行的技术方案，然而它给所有人增加未读计数，会消耗非常长的时间。你计 算一下，假如你的系统中有一个亿的用户，给一个用户增加未读数需要消耗 1ms，那么给 所有人都增加未读计数就需要 100000000 * 1 /1000 = 100000 秒，也就是超过一天的时 间；即使你启动 100 个线程并发的设置，也需要十几分钟的时间才能完成，而用户很难接 受这么长的延迟时间。\n * 另外，使用这种方式需要给系统中的每一个用户都记一个未读数的值，而在系统中，活跃用 户只是很少的一部分，大部分的用户是不活跃的，甚至从来没有打开过系统通知，为这些用 户记录未读数显然是一种浪费。\n\n通过上面的内容，你可以知道为什么我们不能用通用计数系统实现系统通知未读数了吧？那正确的做法是什么呢\n\n要知道，系统通知实际上是存储在一个大的列表中的，这个列表对所有用户共享，也就是所有人看到的都是同一份系统通知的数据。不过不同的人最近看到的消息不同，所以每个人会有不同的未读数。因此，你可以记录一下在这个列表中每个人看过最后一条消息的 ID，然 后统计这个 ID 之后有多少条消息，这就是未读数了\n\n\n\n上述就是 timeline 模型\n\n这个方案在实现时有这样几个关键点：\n\n * 用户访问系统通知页面需要设置未读数为 0，我们需要将用户最近看过的通知 ID 设置为最新的一条系统通知 ID\n * 如果最近看过的通知 ID 为空，则认为是一个新的用户，返回未读数为 0；\n * 对于非活跃用户，比如最近一个月都没有登录和使用过系统的用户，可以把用户最近看过的通知 ID 清空，节省内存空间。\n\n这是一种比较通用的方案，即节省内存，又能尽量减少获取未读数的延迟。\n\n这个方案适用 的另一个业务场景是全量用户打点的场景，比如像下面这张微博截图中的红点\n\n\n\n这个红点和系统通知类似，也是一种通知全量用户的手段，如果逐个通知用户，延迟也是无法接受的。因此你可以采用和系统通知类似的方案。\n\n首先，我们为每一个用户存储一个时间戳，代表最近点过这个红点的时间，用户点了红点， 就把这个时间戳设置为当前时间；然后，我们也记录一个全局的时间戳，这个时间戳标识最新的一次打点时间，如果你在后台操作给全体用户打点，就更新这个时间戳为当前时间。而 我们在判断是否需要展示红点时，只需要判断用户的时间戳和全局时间戳的大小，如果用户时间戳小于全局时间戳，代表在用户最后一次点击红点之后又有新的红点推送，那么就要展 示红点，反之，就不展示红点了。\n\n\n\n这两个场景的共性是全部用户共享一份有限的存储数据，每个人只记录自己在这份存储中的 偏移量，就可以得到未读数了。\n\n你可以看到，系统消息未读的实现方案不是很复杂，它通过设计避免了操作全量数据未读数，如果你的系统中有这种打红点的需求，那我建议你可以结合实际工作灵活使用上述方案。\n\n最后一个需求关注的是微博信息流的未读数，在现在的社交系统中，关注关系已经成为标配的功能，而基于关注关系的信息流也是一种非常重要的信息聚合方式，因此，如何设计信息流的未读数系统就成了你必须面对的一个问题。\n\n\n# 如何为信息流的未读数设计方案\n\n信息流的未读数之所以复杂主要有这样几点原因\n\n首先，微博的信息流是基于关注关系的，未读数也是基于关注关系的，就是说，你关注的人发布了新的微博，那么你作为粉丝未读数就要增加 1。\n\n如果微博用户都是像我这样只有几百粉丝的“小透明”就简单了，你发微博的时候系统给你粉丝的未读数增加 1 不 是什么难事儿。但是对于一些动辄几千万甚至上亿粉丝的微博大 V 就麻烦了，增加未读 数可能需要几个小时。假设你是杨幂的粉丝，想了解她实时发布的博文，那么如果当她 发布博文几个小时之后，你才收到提醒，这显然是不能接受的。所以未读数的延迟是你在设计方案时首先要考虑的内容。\n\n其次，信息流未读数请求量极大、并发极高，这是因为接口是客户端轮询请求的，不是用户触发的。\n\n也就是说，用户即使打开微博客户端什么都不做，这个接口也会被请求到。在几年前，请求未读数接口的量级就已经接近每秒 50 万次，这几年随着微博量级的增长，请求量也变得更高。而作为微博的非核心接口，我们不太可能使用大量的机器来抗未读数请求，因此，如何使用有限的资源来支撑如此高的流量是这个方案的难点。 最后，它不像系统通知那样有共享的存储，因为每个人关注的人不同，信息流的列表也就不同，所以也就没办法采用系统通知未读数的方案。\n\n那要如何设计能够承接每秒几十万次请求的信息流未读数系统呢？你可以这样做：\n\n首先，在通用计数器中记录每一个用户发布的博文数； 然后在 Redis 或者 Memcached 中记录一个人所有关注人的博文数快照，当用户点击未读消息重置未读数为 0 时，将他关注所有人的博文数刷新到快照中； 这样，他关注所有人的博文总数减去快照中的博文总数就是他的信息流未读数。\n\n\n\n假如用户 A，像上图这样关注了用户 B、C、D，其中 B 发布的博文数是 10，C 发布的博 文数是 8，D 发布的博文数是 14，而在用户 A 最近一次查看未读消息时，记录在快照中的 这三个用户的博文数分别是 6、7、12，因此用户 A 的未读数就是（10-6）+（8-7）+（14-12）=7。\n\n这个方案设计简单，并且是全内存操作，性能足够好，能够支撑比较高的并发，事实上微博团队仅仅用 16 台普通的服务器就支撑了每秒接近 50 万次的请求，这就足以证明这个方案的性能有多出色，因此，它完全能够满足信息流未读数的需求。\n\n当然了这个方案也有一些缺陷，比如说快照中需要存储关注关系，如果关注关系变更的时候 更新不及时，那么就会造成未读数不准确；快照采用的是全缓存存储，如果缓存满了就会剔 除一些数据，那么被剔除用户的未读数就变为 0 了。但是好在用户对于未读数的准确度要求不高（未读 10 条还是 11 条，其实用户有时候看不出来），因此，这些缺陷也是可以接受的\n\n通过分享未读数系统设计这个案例，我想给你一些建议：\n\n 1. 缓存是提升系统性能和抵抗大并发量的神器，像是微博信息流未读数这么大的量级我们 仅仅使用十几台服务器就可以支撑，这全都是缓存的功劳；\n 2. 要围绕系统设计的关键困难点想解决办法，就像我们解决系统通知未读数的延迟问题一 样；\n 3. 合理分析业务场景，明确哪些是可以权衡的，哪些是不行的，会对你的系统设计增益良多，比如对于长久不登录用户，我们就会记录未读数为 0，通过这样的权衡，可以极大 地减少内存的占用，减少成本。\n\n\n# 总结\n\n 1. 评论未读、@未读、赞未读等一对一关系的未读数可以使用上节课讲到的通用计数方案来解决；\n 2. 在系统通知未读、全量用户打点等存在有限的共享存储的场景下，可以通过记录用户上次操作的时间或者偏移量，来实现未读方案；\n 3. 最后，信息流未读方案最为复杂，采用的是记录用户博文数快照的方式",normalizedContent:"# 引子\n\n未读数也是系统中一个常见的模块，以微博系统为例，你可看到有多个未读计数的场景，\n\n比如：\n\n * 当有人 @你、评论你、给你的博文点赞或者给你发送私信的时候，你会收到相应的未读提醒；\n * 在早期的微博版本中有系统通知的功能，也就是系统会给全部用户发送消息，通知用户有新的版本或者有一些好玩的运营活动，如果用户没有看，系统就会给他展示有多少条未读的提醒。\n * 我们在浏览信息流的时候，如果长时间没有刷新页面，那么信息流上方就会提示你在这段时间有多少条信息没有看\n\n\n# 系统通知的未读数要如何设计\n\n来看具体的例子。假如你的系统中只有 a、b、c 三个用户，那么你可以在通用计数系统中 增加一块儿内存区域，并且以用户 id 为 key 来存储这三个用户的未读通知数据，当系统发 送一个新的通知时，我们会循环给每一个用户的未读数加 1，这个处理逻辑的伪代码就像下 面这样\n\nlist<long> userids = getalluserids();\n    for(long id : userids) {\n    incrunreadcount(id);\n}\n\n\n但随着系统中的用户越来越多，这个方案存在两个致命的问题\n\n * 首先，获取全量用户就是一个比较耗时的操作，相当于对用户库做一次全表的扫描，这不仅会对数据库造成很大的压力，而且查询全量用户数据的响应时间是很长的，对于在线业务来说是难以接受的。如果你的用户库已经做了分库分表，那么就要扫描所有的库表，响应时间就更长了。不过有一个折中的方法， 那就是在发送系统通知之前，先从线下的数据仓库中 获取全量的用户 id，并且存储在一个本地的文件中，然后再轮询所有的用户 id，给这些用 户增加未读计数。\n   * 这似乎是一个可行的技术方案，然而它给所有人增加未读计数，会消耗非常长的时间。你计 算一下，假如你的系统中有一个亿的用户，给一个用户增加未读数需要消耗 1ms，那么给 所有人都增加未读计数就需要 100000000 * 1 /1000 = 100000 秒，也就是超过一天的时 间；即使你启动 100 个线程并发的设置，也需要十几分钟的时间才能完成，而用户很难接 受这么长的延迟时间。\n * 另外，使用这种方式需要给系统中的每一个用户都记一个未读数的值，而在系统中，活跃用 户只是很少的一部分，大部分的用户是不活跃的，甚至从来没有打开过系统通知，为这些用 户记录未读数显然是一种浪费。\n\n通过上面的内容，你可以知道为什么我们不能用通用计数系统实现系统通知未读数了吧？那正确的做法是什么呢\n\n要知道，系统通知实际上是存储在一个大的列表中的，这个列表对所有用户共享，也就是所有人看到的都是同一份系统通知的数据。不过不同的人最近看到的消息不同，所以每个人会有不同的未读数。因此，你可以记录一下在这个列表中每个人看过最后一条消息的 id，然 后统计这个 id 之后有多少条消息，这就是未读数了\n\n\n\n上述就是 timeline 模型\n\n这个方案在实现时有这样几个关键点：\n\n * 用户访问系统通知页面需要设置未读数为 0，我们需要将用户最近看过的通知 id 设置为最新的一条系统通知 id\n * 如果最近看过的通知 id 为空，则认为是一个新的用户，返回未读数为 0；\n * 对于非活跃用户，比如最近一个月都没有登录和使用过系统的用户，可以把用户最近看过的通知 id 清空，节省内存空间。\n\n这是一种比较通用的方案，即节省内存，又能尽量减少获取未读数的延迟。\n\n这个方案适用 的另一个业务场景是全量用户打点的场景，比如像下面这张微博截图中的红点\n\n\n\n这个红点和系统通知类似，也是一种通知全量用户的手段，如果逐个通知用户，延迟也是无法接受的。因此你可以采用和系统通知类似的方案。\n\n首先，我们为每一个用户存储一个时间戳，代表最近点过这个红点的时间，用户点了红点， 就把这个时间戳设置为当前时间；然后，我们也记录一个全局的时间戳，这个时间戳标识最新的一次打点时间，如果你在后台操作给全体用户打点，就更新这个时间戳为当前时间。而 我们在判断是否需要展示红点时，只需要判断用户的时间戳和全局时间戳的大小，如果用户时间戳小于全局时间戳，代表在用户最后一次点击红点之后又有新的红点推送，那么就要展 示红点，反之，就不展示红点了。\n\n\n\n这两个场景的共性是全部用户共享一份有限的存储数据，每个人只记录自己在这份存储中的 偏移量，就可以得到未读数了。\n\n你可以看到，系统消息未读的实现方案不是很复杂，它通过设计避免了操作全量数据未读数，如果你的系统中有这种打红点的需求，那我建议你可以结合实际工作灵活使用上述方案。\n\n最后一个需求关注的是微博信息流的未读数，在现在的社交系统中，关注关系已经成为标配的功能，而基于关注关系的信息流也是一种非常重要的信息聚合方式，因此，如何设计信息流的未读数系统就成了你必须面对的一个问题。\n\n\n# 如何为信息流的未读数设计方案\n\n信息流的未读数之所以复杂主要有这样几点原因\n\n首先，微博的信息流是基于关注关系的，未读数也是基于关注关系的，就是说，你关注的人发布了新的微博，那么你作为粉丝未读数就要增加 1。\n\n如果微博用户都是像我这样只有几百粉丝的“小透明”就简单了，你发微博的时候系统给你粉丝的未读数增加 1 不 是什么难事儿。但是对于一些动辄几千万甚至上亿粉丝的微博大 v 就麻烦了，增加未读 数可能需要几个小时。假设你是杨幂的粉丝，想了解她实时发布的博文，那么如果当她 发布博文几个小时之后，你才收到提醒，这显然是不能接受的。所以未读数的延迟是你在设计方案时首先要考虑的内容。\n\n其次，信息流未读数请求量极大、并发极高，这是因为接口是客户端轮询请求的，不是用户触发的。\n\n也就是说，用户即使打开微博客户端什么都不做，这个接口也会被请求到。在几年前，请求未读数接口的量级就已经接近每秒 50 万次，这几年随着微博量级的增长，请求量也变得更高。而作为微博的非核心接口，我们不太可能使用大量的机器来抗未读数请求，因此，如何使用有限的资源来支撑如此高的流量是这个方案的难点。 最后，它不像系统通知那样有共享的存储，因为每个人关注的人不同，信息流的列表也就不同，所以也就没办法采用系统通知未读数的方案。\n\n那要如何设计能够承接每秒几十万次请求的信息流未读数系统呢？你可以这样做：\n\n首先，在通用计数器中记录每一个用户发布的博文数； 然后在 redis 或者 memcached 中记录一个人所有关注人的博文数快照，当用户点击未读消息重置未读数为 0 时，将他关注所有人的博文数刷新到快照中； 这样，他关注所有人的博文总数减去快照中的博文总数就是他的信息流未读数。\n\n\n\n假如用户 a，像上图这样关注了用户 b、c、d，其中 b 发布的博文数是 10，c 发布的博 文数是 8，d 发布的博文数是 14，而在用户 a 最近一次查看未读消息时，记录在快照中的 这三个用户的博文数分别是 6、7、12，因此用户 a 的未读数就是（10-6）+（8-7）+（14-12）=7。\n\n这个方案设计简单，并且是全内存操作，性能足够好，能够支撑比较高的并发，事实上微博团队仅仅用 16 台普通的服务器就支撑了每秒接近 50 万次的请求，这就足以证明这个方案的性能有多出色，因此，它完全能够满足信息流未读数的需求。\n\n当然了这个方案也有一些缺陷，比如说快照中需要存储关注关系，如果关注关系变更的时候 更新不及时，那么就会造成未读数不准确；快照采用的是全缓存存储，如果缓存满了就会剔 除一些数据，那么被剔除用户的未读数就变为 0 了。但是好在用户对于未读数的准确度要求不高（未读 10 条还是 11 条，其实用户有时候看不出来），因此，这些缺陷也是可以接受的\n\n通过分享未读数系统设计这个案例，我想给你一些建议：\n\n 1. 缓存是提升系统性能和抵抗大并发量的神器，像是微博信息流未读数这么大的量级我们 仅仅使用十几台服务器就可以支撑，这全都是缓存的功劳；\n 2. 要围绕系统设计的关键困难点想解决办法，就像我们解决系统通知未读数的延迟问题一 样；\n 3. 合理分析业务场景，明确哪些是可以权衡的，哪些是不行的，会对你的系统设计增益良多，比如对于长久不登录用户，我们就会记录未读数为 0，通过这样的权衡，可以极大 地减少内存的占用，减少成本。\n\n\n# 总结\n\n 1. 评论未读、@未读、赞未读等一对一关系的未读数可以使用上节课讲到的通用计数方案来解决；\n 2. 在系统通知未读、全量用户打点等存在有限的共享存储的场景下，可以通过记录用户上次操作的时间或者偏移量，来实现未读方案；\n 3. 最后，信息流未读方案最为复杂，采用的是记录用户博文数快照的方式",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"超时&重试",frontmatter:{title:"超时&重试",date:"2024-09-14T16:52:35.000Z",permalink:"/pages/0dfb49/"},regularPath:"/30.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/01.%E7%BB%8F%E5%85%B8%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1/08.%E8%B6%85%E6%97%B6&%E9%87%8D%E8%AF%95.html",relativePath:"30.经典场景设计/01.经典场景设计/08.超时&重试.md",key:"v-72e80b6c",path:"/pages/0dfb49/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:35},{level:3,title:"重试的风险",slug:"重试的风险",normalizedTitle:"重试的风险",charIndex:147},{level:3,title:"重试的使用成本",slug:"重试的使用成本",normalizedTitle:"重试的使用成本",charIndex:754},{level:2,title:"重试治理",slug:"重试治理",normalizedTitle:"重试治理",charIndex:1089},{level:3,title:"动态配置",slug:"动态配置",normalizedTitle:"动态配置",charIndex:1191},{level:3,title:"退避策略",slug:"退避策略",normalizedTitle:"退避策略",charIndex:1835},{level:3,title:"防止 retry storm",slug:"防止-retry-storm",normalizedTitle:"防止 retry storm",charIndex:2122},{level:4,title:"限制单点重试",slug:"限制单点重试",normalizedTitle:"限制单点重试",charIndex:2176},{level:4,title:"限制链路重试",slug:"限制链路重试",normalizedTitle:"限制链路重试",charIndex:2562},{level:4,title:"超时处理",slug:"超时处理",normalizedTitle:"超时处理",charIndex:3355},{level:4,title:"超时场景优化",slug:"超时场景优化",normalizedTitle:"超时场景优化",charIndex:4118},{level:4,title:"结合 DDL",slug:"结合-ddl",normalizedTitle:"结合 ddl",charIndex:5186},{level:4,title:"实际的链路放大效应",slug:"实际的链路放大效应",normalizedTitle:"实际的链路放大效应",charIndex:5645},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6011},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:6196}],headersStr:"前言 重试的风险 重试的使用成本 重试治理 动态配置 退避策略 防止 retry storm 限制单点重试 限制链路重试 超时处理 超时场景优化 结合 DDL 实际的链路放大效应 总结 参考文献",content:"下述是引用字节某大佬的一篇文章，讲的是如何在微服务中进行重试\n\n\n# 前言\n\n在微服务架构中，一个大系统被拆分成多个小服务，小服务之间大量 RPC 调用，经常可能因为网络抖动等原因导致 RPC 调用失败，这时候使用重试机制可以提高请求的最终成功率，减少故障影响，让系统运行更稳定。\n\n\n\n\n# 重试的风险\n\n重试能够提高服务稳定性，但是一般情况下大家都不会轻易去重试，或者说不敢重试，主要是因为重试有放大故障的风险。\n\n首先，重试会加大直接下游的负载。如下图，假设 A 服务调用 B 服务，重试次数设置为 r（包括首次请求），当 B 高负载时很可能调用不成功，这时 A 调用失败重试 B ，B 服务的被调用量快速增大，最坏情况下可能放大到 r 倍，不仅不能请求成功，还可能导致 B 的负载继续升高，甚至直接打挂。\n\n\n\n更可怕的是，重试还会存在链路放大的效应，结合下图说明一下：\n\n\n\n假设现在场景是 Backend A 调用 Backend B，Backend B 调用 DB Frontend，均设置重试次数为 3 。如果 Backend B 调用 DB Frontend，请求 3 次都失败了，这时 Backend B 会给 Backend A 返回失败。但是 Backend A 也有重试的逻辑，Backend A 重试 Backend B 三次，每一次 Backend B 都会请求 DB Frontend 3 次，这样算起来，DB Frontend 就会被请求了 9 次，实际是指数级扩大。假设正常访问量是 n，链路一共有 m 层，每层重试次数为 r，则最后一层受到的访问量最大，为 n * r ^ (m - 1) 。这种指数放大的效应很可怕，可能导致链路上多层都被打挂，整个系统雪崩。\n\n\n# 重试的使用成本\n\n另外使用重试的成本也比较高。之前在字节跳动的内部框架和服务治理平台中都没有支持重试，在一些很需要重试的业务场景下（比如调用一些第三方业务经常失败），业务方可能用简单 for 循环来实现，基本不会考虑重试的放大效应，这样很不安全，公司内部出现过多次因为重试而导致的事故，且出事故的时候还需要修改代码上线才能关闭重试，导致事故恢复也不迅速。\n\n另外也有一些业务使用开源的重试组件，这些组件通常会考虑对直接下游的保护，但不会考虑链路级别的重试放大，另外需要业务方修改 RPC 调用代码才能使用，对业务代码入侵较多，而且也是静态配置，需要修改配置时都必须重新上线。\n\n基于以上的背景，为了让业务方能够灵活安全的使用重试，我们字节跳动直播中台团队设计和实现了一个重试治理组件，具有以下优点：\n\n 1. 能够在链路级别防重试风暴。\n\n 2. 保证易用性，业务接入成本小。\n\n 3. 具有灵活性，能够动态调整配置。\n\n下面介绍具体的实现方案。\n\n\n# 重试治理\n\n\n# 动态配置\n\n如何让业务方简单接入是首先要解决的问题。如果还是普通组件库的方式，依旧免不了要大量入侵用户代码，且很难动态调整。\n\n字节跳动的 Golang 开发框架支持中间件 (Milddleware) 模式，可以注册多个自定义 Middleware 并依次递归调用，通常是用于完成打印日志、上报监控等非业务逻辑，能够有效将业务和非业务代码功能进行解耦。因此我们决定使用 Middleware 的方式来实现重试功能，定义一个 Middleware 并在内部实现对 RPC 的重复调用，把重试的配置信息用字节跳动的分布式配置存储中心存储，这样 Middleware 中能够读取配置中心的配置并进行重试，对用户来说不需要修改调用 RPC 的代码，而只需要在服务中引入一个全局的 Middleware 即可。\n\n如下面的整体架构图所示，我们提供配置的网页和后台，用户能够在专门进行服务治理的页面上很方便的对 RPC 进行配置修改并自动生效，内部的实现逻辑对用户透明，对业务代码无入侵。\n\n\n\n配置的维度按照字节跳动的 RPC 调用特点，选定 [调用方服务，调用方集群，被调用服务， 被调用方法] 为一个元组，按照元组来进行配置。Middleware 中封装了读取配置的方法，在 RPC 调用的时候会自动读取并生效。\n\n这种 Middleware 的方式能够让业务方很容易接入，相对于之前普通组件库的方式要方便很多，并且一次接入以后就具有动态配置的能力，可能很方便地调整或者关闭重试配置。\n\n\n# 退避策略\n\n确定了接入方式以后就可以开始实现重试组件的具体功能，一个重试组件所包含的基本功能中，除了重试次数和总延时这样的基础配置外，还需要有退避策略。\n\n对于一些暂时性的错误，如网络抖动等，可能立即重试还是会失败，通常等待一小会儿再重试的话成功率会较高，并且也可能打散上游重试的时间，较少因为同时都重试而导致的下游瞬间流量高峰。决定等待多久之后再重试的方法叫做退避策略，我们实现了常见的退避策略，如：\n\n * 线性退避：每次等待固定时间后重试。\n\n * 随机退避：在一定范围内随机等待一个时间后重试。\n\n * 指数退避：连续重试时，每次等待时间都是前一次的倍数。\n\n\n# 防止 retry storm\n\n如何安全重试，防止 retry storm 是我们面临的最大的难题。\n\n# 限制单点重试\n\n首先要在单点进行限制，一个服务不能不受限制的重试下游，很容易造成下游被打挂。除了限制用户设定的重试次数上限外，更重要的是限制重试请求的成功率。\n\n实现的方案很简单，基于断路器的思想，限制 请求失败/请求成功 的比率，给重试增加熔断功能。我们采用了常见的滑动窗口的方法来实现，如下图，内存中为每一类 RPC 调用维护一个滑动窗口，比如窗口分 10 个 bucket ，每个 bucket 里面记录了 1s 内 RPC 的请求结果数据（成功、失败）。新的一秒到来时，生成新的 bucket ，并淘汰最早的一个 bucket ，只维持 10s 的数据。在新请求这个 RPC 失败时，根据前 10s 内的 失败/成功 是否超过阈值来判断是否可以重试。默认阈值是 0.1 ，即下游最多承受 1.1 倍的 QPS ，用户可以根据需要自行调整熔断开关和阈值。\n\n\n\n# 限制链路重试\n\n前面说过在多级链路中如果每层都配置重试可能导致调用量指数级扩大，虽然有了重试熔断之后，重试不再是指数增长(每一单节点重试扩大限制了 1.1 倍)，但还是会随着链路的级数增长而扩大调用次数，因此还是需要从链路层面来考虑重试的安全性。\n\n链路层面的防重试风暴的核心是限制每层都发生重试，理想情况下只有最下一层发生重试。Google SRE 中指出了 Google 内部使用特殊错误码的方式来实现：\n\n * 统一约定一个特殊的 status code ，它表示：调用失败，但别重试。\n\n * 任何一级重试失败后，生成该 status code 并返回给上层。\n\n * 上层收到该 status code 后停止对这个下游的重试，并将错误码再传给自己的上层。\n\n这种方式理想情况下只有最下一层发生重试，它的上游收到错误码后都不会重试，链路整体放大倍数也就是 r 倍(单层的重试次数)。但是这种策略依赖于业务方传递错误码，对业务代码有一定入侵，而且通常业务方的代码差异很大，调用 RPC 的方式和场景也各不相同，需要业务方配合进行大量改造，很可能因为漏改等原因导致没有把从下游拿到的错误码传递给上游。\n\n好在字节跳动内部用的 RPC 协议中有扩展字段，我们在 Middleware 中做了很多尝试，封装了错误码处理和传递的逻辑，在 RPC 的 Response 扩展字段中传递错误码标识 nomore_retry ，它告诉上游不要再重试了。Middleware 完成错误码的生成、识别、传递等整个生命周期的管理，不需要业务方修改本身的 RPC 逻辑，错误码的方案对业务来说是透明的。\n\n\n\n在链路中，推进每层都接入重试组件，这样每一层都可以通过识别这个标志位来停止重试，并逐层往上传递，上层也都停止重试，做到链路层面的防护，达到“只有最靠近错误发生的那一层才重试”的效果。\n\n# 超时处理\n\n在测试错误码上传的方案时，我们发现超时的情况可能导致传递错误码的方案失效。\n\n对于 A -> B -> C 的场景，假设 B -> C 超时，B 重试请求 C ，这时候很可能 A -> B 也超时了，所以 A 没有拿到 B 返回的错误码，而是也会重试 B , 这个时候虽然 B 重试 C 且生成了重试失败的错误码，但是却不能再传递给 A 。这种情况下，A 还是会重试 B ，如果链路中每一层都超时，那么还是会出现链路指数扩大的效应。\n\n因此为了处理这种情况，除了下游传递重试错误标志以外，我们还实现了“对重试请求不重试”的方案。\n\n对于重试的请求，我们在 Request 中打上一个特殊的 retry flag ，在上面 A -> B -> C 的链路，当 B 收到 A 的请求时会先读取这个 flag 判断这个请求是不是重试请求，如果是，那它调用 C 即使失败也不会重试；否则调用 C 失败后会重试 C 。同时 B 也会把这个 retry flag 下传，它发出的请求也会有这个标志，它的下游也不会再对这个请求重试。\n\n\n\n这样即使 A 因为超时而拿不到 B 的返回，对 B 发出重试请求后，B 能感知到并且不会对 C 重试，这样 A 最多请求 r 次，B 最多请求 r + r - 1，如果后面还有更下层次的话，C 最多请求 r + r + r - 2 次， 第 i 层最多请求 i * r - (i-1) 次，最坏情况下是倍数增长，不是指数增长了。加上实际还有重试熔断的限制，增长的幅度要小很多。\n\n通过重试熔断来限制单点的放大倍数，通过重试错误标志链路回传的方式来保证只有最下层发生重试，又通过重试请求 flag 链路下传的方式来保证对重试请求不重试，多种控制策略结合，可以有效地较少重试放大效应。\n\n# 超时场景优化\n\n分布式系统中，RPC 请求的结果有三种状态：成功、失败、超时，其中最难处理的就是超时的情况。但是超时往往又是最经常发生的那一个，我们统计了字节跳动直播业务线上一些重要服务的 RPC 错误分布，发现占比最高的就是超时错误，怕什么偏来什么。\n\n在超时重试的场景中，虽然给重试请求添加 retry flag 能防止指数扩大，但是却不能提高请求成功率。如下图，假如 A 和 B 的超时时间都是 1000ms ，当 C 负载很高导致 B 访问 C 超时，这时 B 会重试 C ，但是时间已经超过了 1000ms ，时间 A 这里也超时了并且断开了和 B 的连接，所以 B 这次重试 C 不管是否成功都是无用功，从 A 的视角看，本次请求已经失败了。\n\n\n\n这种情况的本质原因是因为链路上的超时时间设置得不合理，上游和下游的超时时间设置的一样，甚至上游的超时时间比下游还要短。在实际情况中业务一般都没有专门配置过 RPC 的超时时间，所以可能上下游都是默认的超时，时长是一样的。为了应对这种情况，我们需要有一个机制来优化超时情况下的稳定性，并减少无用的重试。\n\n如下图，正常重试的场景是等拿到 Resp1 (或者拿到超时结果) 后再发起第二次请求，整体耗时是 t1 + t2 。我们分析下，service A 在发出去 Req1 之后可能等待很长的时间，比如 1s ，但是这个请求的 pct99 或者 pct999 可能通常只有 100ms 以内，如果超过了 100ms ，有很大概率是这次访问最终会超时，能不能不要傻等，而是提前重试呢？\n\n\n\n基于这种思想，我们引入并实现了 Backup Requests 的方案。如下图，我们预先设定一个阈值 t3（比超时时间小，通常建议是 RPC 请求延时的 pct99 ），当 Req1 发出去后超过 t3 时间都没有返回，那我们直接发起重试请求 Req2 ，这样相当于同时有两个请求运行。然后等待请求返回，只要 Resp1 或者 Resp2 任意一个返回成功的结果，就可以立即结束这次请求，这样整体的耗时就是 t4 ，它表示从第一个请求发出到第一个成功结果返回之间的时间，相比于等待超时后再发出请求，这种机制能大大减少整体延时。\n\n\n\n实际上 Backup Requests 是一种用访问量来换成功率 (或者说低延时) 的思想，当然我们会控制它的访问量增大比率，在发起重试之前，会为第一次的请求记录一次失败，并检查当前失败率是否超过了熔断阈值，这样整体的访问比率还是会在控制之内。\n\n# 结合 DDL\n\nBackup Requests 的思路能在缩短整体请求延时的同时减少一部分的无效请求，但不是所有业务场景下都适合配置 Backup Requests ，因此我们又结合了 DDL 来控制无效重试。\n\nDDL 是“ Deadline Request 调用链超时”的简称，我们知道 TCP/IP 协议中的 TTL 用于判断数据包在网络中的时间是否太长而应被丢弃，DDL 与之类似，它是一种全链路式的调用超时，可以用来判断当前的 RPC 请求是否还需要继续下去。如下图，字节跳动的基础团队已经实现了 DDL 功能，在 RPC 请求调用链中会带上超时时间，并且每经过一层就减去该层处理的时间，如果剩下的时间已经小于等于 0 ，则可以不需要再请求下游，直接返回失败即可。\n\n\n\nDDL 的方式能有效减少对下游的无效调用，我们在重试治理中也结合了 DDL 的数据，在每一次发起重试前都会判断 DDL 的剩余值是否还大于 0 ，如果已经不满足条件了，那也就没必要对下游重试，这样能做到最大限度的减少无用的重试。\n\n# 实际的链路放大效应\n\n之前说的链路指数放大是理想情况下的分析，实际的情况要复杂很多，因为有很多影响因素：\n\n策略         说明\n重试熔断       请求失败 / 成功 > 0.1 时停止重试\n链路上传错误标志   下层重试失败后上传错误标志，上层不再重试\n链路下传重试标志   重试请求特殊标记，下层对重试请求不会重试\nDDL        当剩余时间不够时不再发起重试请求\n框架熔断       微服务框架本身熔断、过载保护等机制也会影响重试效果\n\n各种因素综合下来，最终实际方法情况不是一个简单的计算公式能说明，我们构造了多层调用链路，在线上实际测试和记录了在不同错误类型、不同错误率的情况下使用重试治理组件的效果，发现接入重试治理组件后能够在链路层面有效的控制重试放大倍数，大幅减少重试导致系统雪崩的概率。\n\n\n# 总结\n\n如上所述，基于服务治理的思想我们开发了重试治理的功能，支持动态配置，接入方式基本无需入侵业务代码，并使用多种策略结合的方式在链路层面控制重试放大效应，兼顾易用性、灵活性、安全性，在字节跳动内部已经有包括直播在内的很多服务接入使用并上线验证，对提高服务本身稳定性有良好的效果。目前方案已经被验证并在字节跳动直播等业务推广，后续将为更多的字节跳动业务服务。\n\n\n# 参考文献\n\n如何优雅地重试 (qq.com)",normalizedContent:"下述是引用字节某大佬的一篇文章，讲的是如何在微服务中进行重试\n\n\n# 前言\n\n在微服务架构中，一个大系统被拆分成多个小服务，小服务之间大量 rpc 调用，经常可能因为网络抖动等原因导致 rpc 调用失败，这时候使用重试机制可以提高请求的最终成功率，减少故障影响，让系统运行更稳定。\n\n\n\n\n# 重试的风险\n\n重试能够提高服务稳定性，但是一般情况下大家都不会轻易去重试，或者说不敢重试，主要是因为重试有放大故障的风险。\n\n首先，重试会加大直接下游的负载。如下图，假设 a 服务调用 b 服务，重试次数设置为 r（包括首次请求），当 b 高负载时很可能调用不成功，这时 a 调用失败重试 b ，b 服务的被调用量快速增大，最坏情况下可能放大到 r 倍，不仅不能请求成功，还可能导致 b 的负载继续升高，甚至直接打挂。\n\n\n\n更可怕的是，重试还会存在链路放大的效应，结合下图说明一下：\n\n\n\n假设现在场景是 backend a 调用 backend b，backend b 调用 db frontend，均设置重试次数为 3 。如果 backend b 调用 db frontend，请求 3 次都失败了，这时 backend b 会给 backend a 返回失败。但是 backend a 也有重试的逻辑，backend a 重试 backend b 三次，每一次 backend b 都会请求 db frontend 3 次，这样算起来，db frontend 就会被请求了 9 次，实际是指数级扩大。假设正常访问量是 n，链路一共有 m 层，每层重试次数为 r，则最后一层受到的访问量最大，为 n * r ^ (m - 1) 。这种指数放大的效应很可怕，可能导致链路上多层都被打挂，整个系统雪崩。\n\n\n# 重试的使用成本\n\n另外使用重试的成本也比较高。之前在字节跳动的内部框架和服务治理平台中都没有支持重试，在一些很需要重试的业务场景下（比如调用一些第三方业务经常失败），业务方可能用简单 for 循环来实现，基本不会考虑重试的放大效应，这样很不安全，公司内部出现过多次因为重试而导致的事故，且出事故的时候还需要修改代码上线才能关闭重试，导致事故恢复也不迅速。\n\n另外也有一些业务使用开源的重试组件，这些组件通常会考虑对直接下游的保护，但不会考虑链路级别的重试放大，另外需要业务方修改 rpc 调用代码才能使用，对业务代码入侵较多，而且也是静态配置，需要修改配置时都必须重新上线。\n\n基于以上的背景，为了让业务方能够灵活安全的使用重试，我们字节跳动直播中台团队设计和实现了一个重试治理组件，具有以下优点：\n\n 1. 能够在链路级别防重试风暴。\n\n 2. 保证易用性，业务接入成本小。\n\n 3. 具有灵活性，能够动态调整配置。\n\n下面介绍具体的实现方案。\n\n\n# 重试治理\n\n\n# 动态配置\n\n如何让业务方简单接入是首先要解决的问题。如果还是普通组件库的方式，依旧免不了要大量入侵用户代码，且很难动态调整。\n\n字节跳动的 golang 开发框架支持中间件 (milddleware) 模式，可以注册多个自定义 middleware 并依次递归调用，通常是用于完成打印日志、上报监控等非业务逻辑，能够有效将业务和非业务代码功能进行解耦。因此我们决定使用 middleware 的方式来实现重试功能，定义一个 middleware 并在内部实现对 rpc 的重复调用，把重试的配置信息用字节跳动的分布式配置存储中心存储，这样 middleware 中能够读取配置中心的配置并进行重试，对用户来说不需要修改调用 rpc 的代码，而只需要在服务中引入一个全局的 middleware 即可。\n\n如下面的整体架构图所示，我们提供配置的网页和后台，用户能够在专门进行服务治理的页面上很方便的对 rpc 进行配置修改并自动生效，内部的实现逻辑对用户透明，对业务代码无入侵。\n\n\n\n配置的维度按照字节跳动的 rpc 调用特点，选定 [调用方服务，调用方集群，被调用服务， 被调用方法] 为一个元组，按照元组来进行配置。middleware 中封装了读取配置的方法，在 rpc 调用的时候会自动读取并生效。\n\n这种 middleware 的方式能够让业务方很容易接入，相对于之前普通组件库的方式要方便很多，并且一次接入以后就具有动态配置的能力，可能很方便地调整或者关闭重试配置。\n\n\n# 退避策略\n\n确定了接入方式以后就可以开始实现重试组件的具体功能，一个重试组件所包含的基本功能中，除了重试次数和总延时这样的基础配置外，还需要有退避策略。\n\n对于一些暂时性的错误，如网络抖动等，可能立即重试还是会失败，通常等待一小会儿再重试的话成功率会较高，并且也可能打散上游重试的时间，较少因为同时都重试而导致的下游瞬间流量高峰。决定等待多久之后再重试的方法叫做退避策略，我们实现了常见的退避策略，如：\n\n * 线性退避：每次等待固定时间后重试。\n\n * 随机退避：在一定范围内随机等待一个时间后重试。\n\n * 指数退避：连续重试时，每次等待时间都是前一次的倍数。\n\n\n# 防止 retry storm\n\n如何安全重试，防止 retry storm 是我们面临的最大的难题。\n\n# 限制单点重试\n\n首先要在单点进行限制，一个服务不能不受限制的重试下游，很容易造成下游被打挂。除了限制用户设定的重试次数上限外，更重要的是限制重试请求的成功率。\n\n实现的方案很简单，基于断路器的思想，限制 请求失败/请求成功 的比率，给重试增加熔断功能。我们采用了常见的滑动窗口的方法来实现，如下图，内存中为每一类 rpc 调用维护一个滑动窗口，比如窗口分 10 个 bucket ，每个 bucket 里面记录了 1s 内 rpc 的请求结果数据（成功、失败）。新的一秒到来时，生成新的 bucket ，并淘汰最早的一个 bucket ，只维持 10s 的数据。在新请求这个 rpc 失败时，根据前 10s 内的 失败/成功 是否超过阈值来判断是否可以重试。默认阈值是 0.1 ，即下游最多承受 1.1 倍的 qps ，用户可以根据需要自行调整熔断开关和阈值。\n\n\n\n# 限制链路重试\n\n前面说过在多级链路中如果每层都配置重试可能导致调用量指数级扩大，虽然有了重试熔断之后，重试不再是指数增长(每一单节点重试扩大限制了 1.1 倍)，但还是会随着链路的级数增长而扩大调用次数，因此还是需要从链路层面来考虑重试的安全性。\n\n链路层面的防重试风暴的核心是限制每层都发生重试，理想情况下只有最下一层发生重试。google sre 中指出了 google 内部使用特殊错误码的方式来实现：\n\n * 统一约定一个特殊的 status code ，它表示：调用失败，但别重试。\n\n * 任何一级重试失败后，生成该 status code 并返回给上层。\n\n * 上层收到该 status code 后停止对这个下游的重试，并将错误码再传给自己的上层。\n\n这种方式理想情况下只有最下一层发生重试，它的上游收到错误码后都不会重试，链路整体放大倍数也就是 r 倍(单层的重试次数)。但是这种策略依赖于业务方传递错误码，对业务代码有一定入侵，而且通常业务方的代码差异很大，调用 rpc 的方式和场景也各不相同，需要业务方配合进行大量改造，很可能因为漏改等原因导致没有把从下游拿到的错误码传递给上游。\n\n好在字节跳动内部用的 rpc 协议中有扩展字段，我们在 middleware 中做了很多尝试，封装了错误码处理和传递的逻辑，在 rpc 的 response 扩展字段中传递错误码标识 nomore_retry ，它告诉上游不要再重试了。middleware 完成错误码的生成、识别、传递等整个生命周期的管理，不需要业务方修改本身的 rpc 逻辑，错误码的方案对业务来说是透明的。\n\n\n\n在链路中，推进每层都接入重试组件，这样每一层都可以通过识别这个标志位来停止重试，并逐层往上传递，上层也都停止重试，做到链路层面的防护，达到“只有最靠近错误发生的那一层才重试”的效果。\n\n# 超时处理\n\n在测试错误码上传的方案时，我们发现超时的情况可能导致传递错误码的方案失效。\n\n对于 a -> b -> c 的场景，假设 b -> c 超时，b 重试请求 c ，这时候很可能 a -> b 也超时了，所以 a 没有拿到 b 返回的错误码，而是也会重试 b , 这个时候虽然 b 重试 c 且生成了重试失败的错误码，但是却不能再传递给 a 。这种情况下，a 还是会重试 b ，如果链路中每一层都超时，那么还是会出现链路指数扩大的效应。\n\n因此为了处理这种情况，除了下游传递重试错误标志以外，我们还实现了“对重试请求不重试”的方案。\n\n对于重试的请求，我们在 request 中打上一个特殊的 retry flag ，在上面 a -> b -> c 的链路，当 b 收到 a 的请求时会先读取这个 flag 判断这个请求是不是重试请求，如果是，那它调用 c 即使失败也不会重试；否则调用 c 失败后会重试 c 。同时 b 也会把这个 retry flag 下传，它发出的请求也会有这个标志，它的下游也不会再对这个请求重试。\n\n\n\n这样即使 a 因为超时而拿不到 b 的返回，对 b 发出重试请求后，b 能感知到并且不会对 c 重试，这样 a 最多请求 r 次，b 最多请求 r + r - 1，如果后面还有更下层次的话，c 最多请求 r + r + r - 2 次， 第 i 层最多请求 i * r - (i-1) 次，最坏情况下是倍数增长，不是指数增长了。加上实际还有重试熔断的限制，增长的幅度要小很多。\n\n通过重试熔断来限制单点的放大倍数，通过重试错误标志链路回传的方式来保证只有最下层发生重试，又通过重试请求 flag 链路下传的方式来保证对重试请求不重试，多种控制策略结合，可以有效地较少重试放大效应。\n\n# 超时场景优化\n\n分布式系统中，rpc 请求的结果有三种状态：成功、失败、超时，其中最难处理的就是超时的情况。但是超时往往又是最经常发生的那一个，我们统计了字节跳动直播业务线上一些重要服务的 rpc 错误分布，发现占比最高的就是超时错误，怕什么偏来什么。\n\n在超时重试的场景中，虽然给重试请求添加 retry flag 能防止指数扩大，但是却不能提高请求成功率。如下图，假如 a 和 b 的超时时间都是 1000ms ，当 c 负载很高导致 b 访问 c 超时，这时 b 会重试 c ，但是时间已经超过了 1000ms ，时间 a 这里也超时了并且断开了和 b 的连接，所以 b 这次重试 c 不管是否成功都是无用功，从 a 的视角看，本次请求已经失败了。\n\n\n\n这种情况的本质原因是因为链路上的超时时间设置得不合理，上游和下游的超时时间设置的一样，甚至上游的超时时间比下游还要短。在实际情况中业务一般都没有专门配置过 rpc 的超时时间，所以可能上下游都是默认的超时，时长是一样的。为了应对这种情况，我们需要有一个机制来优化超时情况下的稳定性，并减少无用的重试。\n\n如下图，正常重试的场景是等拿到 resp1 (或者拿到超时结果) 后再发起第二次请求，整体耗时是 t1 + t2 。我们分析下，service a 在发出去 req1 之后可能等待很长的时间，比如 1s ，但是这个请求的 pct99 或者 pct999 可能通常只有 100ms 以内，如果超过了 100ms ，有很大概率是这次访问最终会超时，能不能不要傻等，而是提前重试呢？\n\n\n\n基于这种思想，我们引入并实现了 backup requests 的方案。如下图，我们预先设定一个阈值 t3（比超时时间小，通常建议是 rpc 请求延时的 pct99 ），当 req1 发出去后超过 t3 时间都没有返回，那我们直接发起重试请求 req2 ，这样相当于同时有两个请求运行。然后等待请求返回，只要 resp1 或者 resp2 任意一个返回成功的结果，就可以立即结束这次请求，这样整体的耗时就是 t4 ，它表示从第一个请求发出到第一个成功结果返回之间的时间，相比于等待超时后再发出请求，这种机制能大大减少整体延时。\n\n\n\n实际上 backup requests 是一种用访问量来换成功率 (或者说低延时) 的思想，当然我们会控制它的访问量增大比率，在发起重试之前，会为第一次的请求记录一次失败，并检查当前失败率是否超过了熔断阈值，这样整体的访问比率还是会在控制之内。\n\n# 结合 ddl\n\nbackup requests 的思路能在缩短整体请求延时的同时减少一部分的无效请求，但不是所有业务场景下都适合配置 backup requests ，因此我们又结合了 ddl 来控制无效重试。\n\nddl 是“ deadline request 调用链超时”的简称，我们知道 tcp/ip 协议中的 ttl 用于判断数据包在网络中的时间是否太长而应被丢弃，ddl 与之类似，它是一种全链路式的调用超时，可以用来判断当前的 rpc 请求是否还需要继续下去。如下图，字节跳动的基础团队已经实现了 ddl 功能，在 rpc 请求调用链中会带上超时时间，并且每经过一层就减去该层处理的时间，如果剩下的时间已经小于等于 0 ，则可以不需要再请求下游，直接返回失败即可。\n\n\n\nddl 的方式能有效减少对下游的无效调用，我们在重试治理中也结合了 ddl 的数据，在每一次发起重试前都会判断 ddl 的剩余值是否还大于 0 ，如果已经不满足条件了，那也就没必要对下游重试，这样能做到最大限度的减少无用的重试。\n\n# 实际的链路放大效应\n\n之前说的链路指数放大是理想情况下的分析，实际的情况要复杂很多，因为有很多影响因素：\n\n策略         说明\n重试熔断       请求失败 / 成功 > 0.1 时停止重试\n链路上传错误标志   下层重试失败后上传错误标志，上层不再重试\n链路下传重试标志   重试请求特殊标记，下层对重试请求不会重试\nddl        当剩余时间不够时不再发起重试请求\n框架熔断       微服务框架本身熔断、过载保护等机制也会影响重试效果\n\n各种因素综合下来，最终实际方法情况不是一个简单的计算公式能说明，我们构造了多层调用链路，在线上实际测试和记录了在不同错误类型、不同错误率的情况下使用重试治理组件的效果，发现接入重试治理组件后能够在链路层面有效的控制重试放大倍数，大幅减少重试导致系统雪崩的概率。\n\n\n# 总结\n\n如上所述，基于服务治理的思想我们开发了重试治理的功能，支持动态配置，接入方式基本无需入侵业务代码，并使用多种策略结合的方式在链路层面控制重试放大效应，兼顾易用性、灵活性、安全性，在字节跳动内部已经有包括直播在内的很多服务接入使用并上线验证，对提高服务本身稳定性有良好的效果。目前方案已经被验证并在字节跳动直播等业务推广，后续将为更多的字节跳动业务服务。\n\n\n# 参考文献\n\n如何优雅地重试 (qq.com)",charsets:{cjk:!0},lastUpdated:"2024/09/15, 13:32:51",lastUpdatedTimestamp:1726407171e3},{title:"归档",frontmatter:{archivesPage:!0,title:"归档",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-61d0fb85",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/16, 16:44:31",lastUpdatedTimestamp:1726505071e3},{title:"指南",frontmatter:{title:"指南",date:"2024-09-15T17:31:05.000Z",permalink:"/pages/252196/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E5%89%8D%E8%A8%80/01.%E6%8C%87%E5%8D%97.html",relativePath:"Redis 系统设计/01.前言/01.指南.md",key:"v-36b789a1",path:"/pages/252196/",headers:[{level:2,title:"前置知识",slug:"前置知识",normalizedTitle:"前置知识",charIndex:2},{level:2,title:"阅读方法",slug:"阅读方法",normalizedTitle:"阅读方法",charIndex:157},{level:3,title:"伪码蓝图",slug:"伪码蓝图",normalizedTitle:"伪码蓝图",charIndex:166},{level:3,title:"基础",slug:"基础",normalizedTitle:"基础",charIndex:137},{level:3,title:"主线",slug:"主线",normalizedTitle:"主线",charIndex:232},{level:3,title:"支线",slug:"支线",normalizedTitle:"支线",charIndex:777},{level:2,title:"学习资料推荐",slug:"学习资料推荐",normalizedTitle:"学习资料推荐",charIndex:1200}],headersStr:"前置知识 阅读方法 伪码蓝图 基础 主线 支线 学习资料推荐",content:"# 前置知识\n\n * 常用数据结构：数组、链表、哈希表、跳表\n * 网络协议：TCP 协议\n * 网络 IO 模型：IO 多路复用、非阻塞 IO、Reactor 网络模型\n * 操作系统：写时复制（Copy On Write）、常见系统调用、磁盘 IO 机制\n * C 语言基础：循环、分支、结构体、指针\n\n\n# 阅读方法\n\n\n# 伪码蓝图\n\n特别重要！务必反复观看\n\n\n# 基础\n\n这些基础模块就相当于一座大厦的地基，地基打好了，才能做到高楼耸立。\n\n\n\n\n# 主线\n\n掌握了数据结构模块之后，这时我们的重点就需要放在「核心主线」上来了。\n\n那么在读 Redis 源码时，什么才是它的核心主线呢？\n\n这里我分享一个非常好用的技巧，就是根据「Redis 究竟是怎么处理客户端发来的命令的？」为主线来梳理。\n\n举个例子，当我们在执行 SET testkey testval EX 60 这样一条命令时，就需要搞清楚 Redis 是怎么执行这条命令的\n\n也就是要明确，Redis 从收到客户端请求，到把数据存到 Redis 中、设置过期时间，最后把响应结果返回给客户端，整个过程的每一个环节，到底是如何处理的。\n\n有了这条主线，我们就有了非常明确的目标，而且沿着这条主线去读代码，我们还可以很清晰地把多个模块「串联」起来。比如从前面的例子中，我们会看到一条命令的执行，主要包含了这样几个阶段\n\n * Redis Server 初始化：加载配置、监听端口、注册连接建立事件、启动事件循环\n * 接收、解析客户端请求：初始化 client、注册读事件、读客户端\n * 处理具体的命令：找到对应的命令函数、执行命令\n * 返回响应给客户端：写客户端缓冲区、注册写事件、写客户端 socket\n\n\n\n沿着这条主线去读代码，我们就可以掌握一条命令的执行全过程\n\n\n# 支线\n\n不过，在攻打主线的过程中，我们肯定还会遇到各种「支线」逻辑，比如数据过期、替换淘汰、持久化、主从复制等。\n\n其实，在阅读主线逻辑的时候，我们并不需要去重点关注这些支线，而当整个主线逻辑「清晰」起来之后，我们再去读这些支线模块，就会容易很多了。\n\n这时，我们就可以从这些支线中，选取下一个「目标」，带着这个目标去阅读，比如说：\n\n * 过期策略是怎么实现的？（expire.c、lazyfree.c）\n * 淘汰策略是如何实现的？（evict.c）\n * 持久化 RDB、AOF 是怎么做的？（rdb.c、aof.c）\n * 主从复制是怎么做的？（replication.c）\n * 哨兵如何完成故障自动切换？（sentinel.c）\n * 分片逻辑如何实现？（cluster.c）\n * …\n\n有了新的支线目标后，我们依旧可以采用前面提到的「先整体后细节」的思路阅读相关模块，这样下来，整个项目的每个模块，就可以被「逐一击破」了\n\n\n# 学习资料推荐\n\n * 极客时间：Redis源码剖析与实战\n * Redis设计与实现\n * https://book-redis-design.netlify.app/\n * Github：redis 源码\n * Redis Basics & Notes - Yves Wiki (imzye.com)\n * 栏目：服务端技术 - 铁蕾的个人博客 (zhangtielei.com)\n * Category: Redis Source Code Analysis | Johnson Lin (linjiangxiong.com)\n * Redis源码解析 (youzan.com)\n * JasonLai256/the-little-redis-book (github.com)\n * huangzworks/redis-3.0-annotated: 带有详细注释的 Redis 3.0 代码（annotated Redis 3.0 source code）。 (github.com)\n * 通用业务场景_云数据库 Redis 版(Redis)-阿里云帮助中心 (aliyun.com)\n * 得分最高的 'redis' 问题 - Stack Overflow --- Highest scored 'redis' questions - Stack Overflow",normalizedContent:"# 前置知识\n\n * 常用数据结构：数组、链表、哈希表、跳表\n * 网络协议：tcp 协议\n * 网络 io 模型：io 多路复用、非阻塞 io、reactor 网络模型\n * 操作系统：写时复制（copy on write）、常见系统调用、磁盘 io 机制\n * c 语言基础：循环、分支、结构体、指针\n\n\n# 阅读方法\n\n\n# 伪码蓝图\n\n特别重要！务必反复观看\n\n\n# 基础\n\n这些基础模块就相当于一座大厦的地基，地基打好了，才能做到高楼耸立。\n\n\n\n\n# 主线\n\n掌握了数据结构模块之后，这时我们的重点就需要放在「核心主线」上来了。\n\n那么在读 redis 源码时，什么才是它的核心主线呢？\n\n这里我分享一个非常好用的技巧，就是根据「redis 究竟是怎么处理客户端发来的命令的？」为主线来梳理。\n\n举个例子，当我们在执行 set testkey testval ex 60 这样一条命令时，就需要搞清楚 redis 是怎么执行这条命令的\n\n也就是要明确，redis 从收到客户端请求，到把数据存到 redis 中、设置过期时间，最后把响应结果返回给客户端，整个过程的每一个环节，到底是如何处理的。\n\n有了这条主线，我们就有了非常明确的目标，而且沿着这条主线去读代码，我们还可以很清晰地把多个模块「串联」起来。比如从前面的例子中，我们会看到一条命令的执行，主要包含了这样几个阶段\n\n * redis server 初始化：加载配置、监听端口、注册连接建立事件、启动事件循环\n * 接收、解析客户端请求：初始化 client、注册读事件、读客户端\n * 处理具体的命令：找到对应的命令函数、执行命令\n * 返回响应给客户端：写客户端缓冲区、注册写事件、写客户端 socket\n\n\n\n沿着这条主线去读代码，我们就可以掌握一条命令的执行全过程\n\n\n# 支线\n\n不过，在攻打主线的过程中，我们肯定还会遇到各种「支线」逻辑，比如数据过期、替换淘汰、持久化、主从复制等。\n\n其实，在阅读主线逻辑的时候，我们并不需要去重点关注这些支线，而当整个主线逻辑「清晰」起来之后，我们再去读这些支线模块，就会容易很多了。\n\n这时，我们就可以从这些支线中，选取下一个「目标」，带着这个目标去阅读，比如说：\n\n * 过期策略是怎么实现的？（expire.c、lazyfree.c）\n * 淘汰策略是如何实现的？（evict.c）\n * 持久化 rdb、aof 是怎么做的？（rdb.c、aof.c）\n * 主从复制是怎么做的？（replication.c）\n * 哨兵如何完成故障自动切换？（sentinel.c）\n * 分片逻辑如何实现？（cluster.c）\n * …\n\n有了新的支线目标后，我们依旧可以采用前面提到的「先整体后细节」的思路阅读相关模块，这样下来，整个项目的每个模块，就可以被「逐一击破」了\n\n\n# 学习资料推荐\n\n * 极客时间：redis源码剖析与实战\n * redis设计与实现\n * https://book-redis-design.netlify.app/\n * github：redis 源码\n * redis basics & notes - yves wiki (imzye.com)\n * 栏目：服务端技术 - 铁蕾的个人博客 (zhangtielei.com)\n * category: redis source code analysis | johnson lin (linjiangxiong.com)\n * redis源码解析 (youzan.com)\n * jasonlai256/the-little-redis-book (github.com)\n * huangzworks/redis-3.0-annotated: 带有详细注释的 redis 3.0 代码（annotated redis 3.0 source code）。 (github.com)\n * 通用业务场景_云数据库 redis 版(redis)-阿里云帮助中心 (aliyun.com)\n * 得分最高的 'redis' 问题 - stack overflow --- highest scored 'redis' questions - stack overflow",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Redis 伪码蓝图【必看】",frontmatter:{title:"Redis 伪码蓝图【必看】",date:"2024-09-16T01:33:42.000Z",permalink:"/pages/69fbd7/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/01.%E5%89%8D%E8%A8%80/05.Redis%20%E4%BC%AA%E7%A0%81%E8%93%9D%E5%9B%BE%E3%80%90%E5%BF%85%E7%9C%8B%E3%80%91.html",relativePath:"Redis 系统设计/01.前言/05.Redis 伪码蓝图【必看】.md",key:"v-70b3216c",path:"/pages/69fbd7/",headersStr:null,content:'请点击下方窗口右下角进行全屏预览\n\n# Redis 启动流程\nfunction startRedis():\n    # 1. Redis Server 初始化\n    loadConfig()                         # 加载 redis.conf 配置文件\n    createServerSocket()                 # 创建 TCP 监听端口，准备接受客户端连接\n    initEventLoop()                      # 初始化事件循环（epoll/kqueue）\n\n    # 初始化支线功能\n    initPersistence()                    # 初始化 RDB 和 AOF 持久化\n    initReplication()                    # 初始化主从复制机制\n    initSentinel()                       # 如果是哨兵模式，初始化哨兵机制\n    initCluster()                        # 如果是集群模式，初始化集群路由和分片\n\n    # 启动主事件循环\n    eventLoop()\n\n# 事件循环，负责处理所有事件\nfunction eventLoop():\n    while server is running:\n        processTimers()                  # 处理定时任务（如过期 key、AOF 持久化等）\n        acceptClientConnections()        # 接受新的客户端连接\n        processClientRequests()          # 处理客户端命令请求\n        handleReplication()              # 处理主从复制数据同步\n        handleSentinel()                 # 处理哨兵监控和故障切换\n        handleCluster()                  # 集群模式下，处理分片路由和数据迁移\n\n# 处理定时任务，如过期 key、持久化\nfunction processTimers():\n    expireKeysIfNeeded()                 # 检查并删除过期的键\n    handleAOFAndRDBPersistence()         # 根据策略触发 AOF 和 RDB 持久化\n    evictKeysIfNeeded()                  # 如果内存超出上限，触发淘汰策略\n\n# 处理新客户端连接\nfunction acceptClientConnections():\n    when new client connects:\n        client = createClient()          # 创建客户端对象\n        registerReadEvent(client)        # 注册读事件，监听客户端请求\n\n# 处理客户端命令请求\nfunction processClientRequests():\n    for each client in server.clients:\n        if client has data to read:\n            readDataFromClient(client)   # 从 socket 中读取请求数据\n            if data is valid:\n                command = parseCommand(client)  # 解析 Redis 命令\n                executeCommand(client, command) # 执行命令\n\n# 执行 Redis 命令\nfunction executeCommand(client, command):\n    if command is valid:\n        cmdFunction = lookupCommand(command)  # 查找命令函数\n        result = cmdFunction(client)          # 执行命令\n        addToClientOutputBuffer(client, result) # 将结果写入输出缓冲区\n        replicateCommandToSlaves(command)     # 同步命令到从服务器（主从复制）\n\n        # 在命令执行过程中，可能触发支线操作\n        checkKeyExpiration(client, command)  # 检查 key 是否过期，进行惰性删除\n        handlePersistence(command)           # 根据配置，触发 AOF 或 RDB 持久化\n        evictKeysIfNeeded()                  # 当内存不足时，触发淘汰策略\n\n    else:\n        sendError(client, "Invalid command") # 返回错误消息\n\n# 返回响应给客户端\nfunction addToClientOutputBuffer(client, result):\n    client.outputBuffer.append(result)       # 将命令执行结果添加到输出缓冲区\n    registerWriteEvent(client)               # 注册写事件，准备发送给客户端\n\n# 发送响应\nfunction sendResponseToClient(client):\n    writeDataToSocket(client, client.outputBuffer)  # 将数据发送给客户端\n    if write success:\n        clearClientOutputBuffer(client)      # 清空缓冲区\n    else:\n        handleSocketError(client)            # 处理 socket 错误\n\n# 支线：检查 key 过期，过期策略\nfunction checkKeyExpiration(client, command):\n    key = getKeyFromCommand(command)\n    if isKeyExpired(key):\n        deleteKey(client.db, key)            # 惰性删除过期的 key\n\nfunction expireKeysIfNeeded():\n    for each db in server.databases:\n        for each key in db.keys:\n            if isKeyExpired(key):\n                deleteKey(db, key)           # 主动删除过期的 key\n\n# 支线：持久化，处理 RDB 和 AOF 逻辑\nfunction handlePersistence(command):\n    if AOF is enabled:\n        writeCommandToAOF(command)           # 将命令写入 AOF 文件\n    if RDB snapshot is needed:\n        saveRDBSnapshot()                    # 触发 RDB 快照生成\n\nfunction handleAOFAndRDBPersistence():\n    if timeToSaveRDB():\n        saveRDB()                            # 持久化 RDB 快照\n    if timeToRewriteAOF():\n        rewriteAOF()                         # 触发 AOF 重写以压缩日志\n\n# 支线：淘汰策略，LRU、LFU\nfunction evictKeysIfNeeded():\n    while memoryUsageExceedsLimit():\n        candidates = sampleKeys()            # 随机采样 key\n        keyToEvict = findLRUOrLFUKey(candidates) # 根据 LRU/LFU 策略选择淘汰 key\n        deleteKey(db, keyToEvict)            # 删除 key 以释放内存\n\n# 支线：主从复制，主服务器将命令同步给从服务器\nfunction replicateCommandToSlaves(command):\n    if server.isMaster():\n        for each slave in server.slaves:\n            sendCommandToSlave(slave, command)  # 复制命令到从服务器\n\nfunction handleReplication():\n    if server.isMaster():\n        sendReplicationDataToSlaves()          # 主服务器向从服务器同步数据\n    if server.isSlave():\n        syncWithMaster()                       # 从服务器与主服务器同步数据\n\n# 支线：哨兵监控与故障转移\nfunction handleSentinel():\n    if server.isSentinel():\n        monitorMasterStatus()                  # 哨兵模式，监控主服务器状态\n        if masterIsDown():\n            performFailover()                  # 主服务器故障时，触发故障转移\n\nfunction performFailover():\n    newMaster = selectNewMaster()              # 选举新的主服务器\n    promoteSlaveToMaster(newMaster)            # 提升从服务器为主服务器\n    notifyOtherSlavesAndClients(newMaster)     # 通知其他从服务器和客户端\n\n# 支线：集群模式下的分片与数据路由\nfunction handleCluster():\n    if server.isCluster():\n        for each command in client.commands:\n            keyHashSlot = computeKeyHashSlot(command.key) # 计算 key 的哈希槽\n            if hashSlotIsLocal(keyHashSlot):\n                executeCommandLocally(command)   # 如果哈希槽属于本节点，执行命令\n            else:\n                forwardCommandToCorrectNode(command) # 将命令转发到正确的节点\n\nfunction migrateKeysDuringRebalance():\n    if clusterNeedsRebalancing():\n        for each slot in migratingSlots:\n            migrateKeys(slot, targetNode)      # 将哈希槽的数据迁移到目标节点\n\n',normalizedContent:'请点击下方窗口右下角进行全屏预览\n\n# redis 启动流程\nfunction startredis():\n    # 1. redis server 初始化\n    loadconfig()                         # 加载 redis.conf 配置文件\n    createserversocket()                 # 创建 tcp 监听端口，准备接受客户端连接\n    initeventloop()                      # 初始化事件循环（epoll/kqueue）\n\n    # 初始化支线功能\n    initpersistence()                    # 初始化 rdb 和 aof 持久化\n    initreplication()                    # 初始化主从复制机制\n    initsentinel()                       # 如果是哨兵模式，初始化哨兵机制\n    initcluster()                        # 如果是集群模式，初始化集群路由和分片\n\n    # 启动主事件循环\n    eventloop()\n\n# 事件循环，负责处理所有事件\nfunction eventloop():\n    while server is running:\n        processtimers()                  # 处理定时任务（如过期 key、aof 持久化等）\n        acceptclientconnections()        # 接受新的客户端连接\n        processclientrequests()          # 处理客户端命令请求\n        handlereplication()              # 处理主从复制数据同步\n        handlesentinel()                 # 处理哨兵监控和故障切换\n        handlecluster()                  # 集群模式下，处理分片路由和数据迁移\n\n# 处理定时任务，如过期 key、持久化\nfunction processtimers():\n    expirekeysifneeded()                 # 检查并删除过期的键\n    handleaofandrdbpersistence()         # 根据策略触发 aof 和 rdb 持久化\n    evictkeysifneeded()                  # 如果内存超出上限，触发淘汰策略\n\n# 处理新客户端连接\nfunction acceptclientconnections():\n    when new client connects:\n        client = createclient()          # 创建客户端对象\n        registerreadevent(client)        # 注册读事件，监听客户端请求\n\n# 处理客户端命令请求\nfunction processclientrequests():\n    for each client in server.clients:\n        if client has data to read:\n            readdatafromclient(client)   # 从 socket 中读取请求数据\n            if data is valid:\n                command = parsecommand(client)  # 解析 redis 命令\n                executecommand(client, command) # 执行命令\n\n# 执行 redis 命令\nfunction executecommand(client, command):\n    if command is valid:\n        cmdfunction = lookupcommand(command)  # 查找命令函数\n        result = cmdfunction(client)          # 执行命令\n        addtoclientoutputbuffer(client, result) # 将结果写入输出缓冲区\n        replicatecommandtoslaves(command)     # 同步命令到从服务器（主从复制）\n\n        # 在命令执行过程中，可能触发支线操作\n        checkkeyexpiration(client, command)  # 检查 key 是否过期，进行惰性删除\n        handlepersistence(command)           # 根据配置，触发 aof 或 rdb 持久化\n        evictkeysifneeded()                  # 当内存不足时，触发淘汰策略\n\n    else:\n        senderror(client, "invalid command") # 返回错误消息\n\n# 返回响应给客户端\nfunction addtoclientoutputbuffer(client, result):\n    client.outputbuffer.append(result)       # 将命令执行结果添加到输出缓冲区\n    registerwriteevent(client)               # 注册写事件，准备发送给客户端\n\n# 发送响应\nfunction sendresponsetoclient(client):\n    writedatatosocket(client, client.outputbuffer)  # 将数据发送给客户端\n    if write success:\n        clearclientoutputbuffer(client)      # 清空缓冲区\n    else:\n        handlesocketerror(client)            # 处理 socket 错误\n\n# 支线：检查 key 过期，过期策略\nfunction checkkeyexpiration(client, command):\n    key = getkeyfromcommand(command)\n    if iskeyexpired(key):\n        deletekey(client.db, key)            # 惰性删除过期的 key\n\nfunction expirekeysifneeded():\n    for each db in server.databases:\n        for each key in db.keys:\n            if iskeyexpired(key):\n                deletekey(db, key)           # 主动删除过期的 key\n\n# 支线：持久化，处理 rdb 和 aof 逻辑\nfunction handlepersistence(command):\n    if aof is enabled:\n        writecommandtoaof(command)           # 将命令写入 aof 文件\n    if rdb snapshot is needed:\n        saverdbsnapshot()                    # 触发 rdb 快照生成\n\nfunction handleaofandrdbpersistence():\n    if timetosaverdb():\n        saverdb()                            # 持久化 rdb 快照\n    if timetorewriteaof():\n        rewriteaof()                         # 触发 aof 重写以压缩日志\n\n# 支线：淘汰策略，lru、lfu\nfunction evictkeysifneeded():\n    while memoryusageexceedslimit():\n        candidates = samplekeys()            # 随机采样 key\n        keytoevict = findlruorlfukey(candidates) # 根据 lru/lfu 策略选择淘汰 key\n        deletekey(db, keytoevict)            # 删除 key 以释放内存\n\n# 支线：主从复制，主服务器将命令同步给从服务器\nfunction replicatecommandtoslaves(command):\n    if server.ismaster():\n        for each slave in server.slaves:\n            sendcommandtoslave(slave, command)  # 复制命令到从服务器\n\nfunction handlereplication():\n    if server.ismaster():\n        sendreplicationdatatoslaves()          # 主服务器向从服务器同步数据\n    if server.isslave():\n        syncwithmaster()                       # 从服务器与主服务器同步数据\n\n# 支线：哨兵监控与故障转移\nfunction handlesentinel():\n    if server.issentinel():\n        monitormasterstatus()                  # 哨兵模式，监控主服务器状态\n        if masterisdown():\n            performfailover()                  # 主服务器故障时，触发故障转移\n\nfunction performfailover():\n    newmaster = selectnewmaster()              # 选举新的主服务器\n    promoteslavetomaster(newmaster)            # 提升从服务器为主服务器\n    notifyotherslavesandclients(newmaster)     # 通知其他从服务器和客户端\n\n# 支线：集群模式下的分片与数据路由\nfunction handlecluster():\n    if server.iscluster():\n        for each command in client.commands:\n            keyhashslot = computekeyhashslot(command.key) # 计算 key 的哈希槽\n            if hashslotislocal(keyhashslot):\n                executecommandlocally(command)   # 如果哈希槽属于本节点，执行命令\n            else:\n                forwardcommandtocorrectnode(command) # 将命令转发到正确的节点\n\nfunction migratekeysduringrebalance():\n    if clusterneedsrebalancing():\n        for each slot in migratingslots:\n            migratekeys(slot, targetnode)      # 将哈希槽的数据迁移到目标节点\n\n',charsets:{cjk:!0},lastUpdated:"2024/09/17, 10:37:22",lastUpdatedTimestamp:1726569442e3},{title:"String 设计与实现",frontmatter:{title:"String 设计与实现",date:"2024-09-16T02:46:18.000Z",permalink:"/pages/bdae41/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/02.%E5%9F%BA%E7%A1%80/01.String%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Redis 系统设计/02.基础/01.String 设计与实现.md",key:"v-2e4b12ee",path:"/pages/bdae41/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:233},{level:2,title:"SDS 的定义",slug:"sds-的定义",normalizedTitle:"sds 的定义",charIndex:537},{level:2,title:"SDS  与 C 字符串的区别",slug:"sds-与-c-字符串的区别",normalizedTitle:"sds  与 c 字符串的区别",charIndex:null},{level:3,title:"常数复杂度获取字符串长度",slug:"常数复杂度获取字符串长度",normalizedTitle:"常数复杂度获取字符串长度",charIndex:1300},{level:3,title:"杜绝缓冲区溢出",slug:"杜绝缓冲区溢出",normalizedTitle:"杜绝缓冲区溢出",charIndex:1511},{level:3,title:"减少修改字符串时带来的内存重分配次数",slug:"减少修改字符串时带来的内存重分配次数",normalizedTitle:"减少修改字符串时带来的内存重分配次数",charIndex:1825},{level:4,title:"空间预分配",slug:"空间预分配",normalizedTitle:"空间预分配",charIndex:98},{level:4,title:"惰性空间释放",slug:"惰性空间释放",normalizedTitle:"惰性空间释放",charIndex:2338},{level:3,title:"二进制安全",slug:"二进制安全",normalizedTitle:"二进制安全",charIndex:138},{level:3,title:"兼容部分 C 字符串函数",slug:"兼容部分-c-字符串函数",normalizedTitle:"兼容部分 c 字符串函数",charIndex:3427},{level:2,title:"SDS 的内存友好设计",slug:"sds-的内存友好设计",normalizedTitle:"sds 的内存友好设计",charIndex:3447},{level:4,title:"redisObject 结构体与位域定义方法",slug:"redisobject-结构体与位域定义方法",normalizedTitle:"redisobject 结构体与位域定义方法",charIndex:3943},{level:4,title:"嵌入式字符串",slug:"嵌入式字符串",normalizedTitle:"嵌入式字符串",charIndex:201},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:8863},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:8964}],headersStr:"前言 SDS 的定义 SDS  与 C 字符串的区别 常数复杂度获取字符串长度 杜绝缓冲区溢出 减少修改字符串时带来的内存重分配次数 空间预分配 惰性空间释放 二进制安全 兼容部分 C 字符串函数 SDS 的内存友好设计 redisObject 结构体与位域定义方法 嵌入式字符串 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. 为什么C语言的字符串在性能和安全性上不足以满足Redis需求？\n 2. SDS如何将获取字符串长度的操作从O(N)优化为O(1)？\n 3. 高并发下，SDS的“空间预分配”如何提升频繁字符串操作的性能？\n 4. Redis为什么需要SDS的二进制安全？传统字符串处理方法有什么局限？\n 5. 为什么SDS缩短时不立即释放多余内存？何时需要手动释放？\n 6. SDS的嵌入式字符串如何减少内存碎片？适用于多大长度的字符串？\n\n\n# 前言\n\n一个优雅的字符串设计，需要尽量满足以下三个要求：\n\n * 能支持丰富的 API 操作，比如字符串追加、拷贝、比较、获取长度等\n * 能保存任意的二进制数据，比如图片等\n * 能尽可能地节省内存开销\n\nRedis 设计了简单动态字符串（Simple Dynamic String，SDS）的结构，用来表示字符串。\n\n相比于 C 语言中的字符串实现，SDS 更适合 Redis 的特性，我会在本文一一叙述\n\nSDS 使用场景\n\n * 数据库中的字符串值\n * AOF 模块中的 AOF 缓冲区\n * 客户端状态中的输入缓冲区\n * 等等等等\n\nRedis 很多用到了字符串的地方都是使用 SDS\n\n\n# SDS 的定义\n\nstruct sdshdr{\n\t//记录buf数组中已使用的字节数 等于SDS所保存的字符串的长度\n\tint len;\n\t// 记录buf数组中未使用的字节数\n\tint free;\n\t//字节数组 用于保存字符串\n\tchar buf[];\n}\n\n\n\n\n笔记\n\nSDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作，都是由SDS函数自动完成的，所以这个空字符对于 SDS 的使用者来说是完全透明的。遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。\n\n\n# SDS 与 C 字符串的区别\n\n\n\nC 语言字符串使用长度为 N+1 的字符串数组来存放长度为 N 的字符串，并且字符数组的最后一个元素总是空字符串 \\0\n\n这并不能满足 Redis 对字符串在效率、安全性以及功能方面的要求\n\n接下来说明 SDS 比 C 字符串更适用于 Redis 的原因\n\n特性                  C 字符串            SDS (简单动态字符串)\n获取字符串长度的复杂度         O(N)             O(1)\nAPI 安全性             不安全，可能导致缓冲区溢出    安全，不会导致缓冲区溢出\n修改字符串长度时的内存重分配次数    修改N次，执行N次内存重分配   修改N次，最多执行N次内存重分配\n数据类型支持              只能保存文本数据         可以保存文本或二进制数据\n<string.h>库函数使用情况   可使用所有函数          可使用部分函数\n\n\n# 常数复杂度获取字符串长度\n\n * C 字符串并不记录自身的长度信息，所以为了获取一个C字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N)\n\n * SDS 中有 len 属性来存储字符串长度，使得 STRLEN 命令的复杂度仅为 O(1)\n\nlen 属性的设置和更新是由 SDS 的 API 在执行的时候自动完成的，我们无需手动设置\n\n\n\n\n# 杜绝缓冲区溢出\n\nchar *strcat(char *dest，const char *src);\n\n\n * C 字符串不记录自身长度，所以当调用 strcat 函数时，系统假定用户为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立，就会产生缓冲区溢出。需要用户对字符串进行扩容。\n\n * SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：SDS API需要对SDS进行修改的时候，API会先检查SDS的空间是否满足修改所需的要求，如果不满足的话，API会自动将SDS的空间进行扩容到指定的大小，然后再次执行实际的修改操作，所以SDS既不需要手动扩容，也不会缓冲区溢出\n\n\n# 减少修改字符串时带来的内存重分配次数\n\n因为 C 字符串的长度和底层数组的长度之间存在关联性，所以每次改变字符串长度的时候，程序总要对保存这个C字符串的数组进行一次内存重分配操作\n\n> 内存重分配：\n> \n>  * 如果程序执行的是增长字符串的操作，比如拼接操作append，那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小，如果忘了这一步就会产生缓冲区溢出\n>  * 如果程序执行的是缩短字符串的操作，比如截断操作trim，那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间，如果忘了这一步就会产生内存泄漏\n\n注意\n\n一般情况下，如果修改字符串长度的情况不常见，那么一般是可以接受内存重分配的。但是Redis作为数据库，为了极致化性能体验，就会尽量避免内存重分配\n\n所以SDS 通过使用 **free 属性 **记录未使用字节，从而解除了字符串长度和底层数组长度之间的关联：\n\n * 在SDS中，buf 数组的长度不一定就是字符数量加一\n * 并且数组里面可以包含未使用的字节\n\n那 SDS 是如何利用 free 属性去避免内存重分配的？\n\nSDS实现了「空间预分配」和「惰性空间释放」两种优化策略\n\n# 空间预分配\n\n「空间预分配」用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间\n\n额外分配的未使用空间数量有以下规则\n\n * 当对 SDS 进行修改之后，SDS 的 len 属性 小于 1 MB，那么程序分配和 len属性同样大小的未使用空间，这是 SDS len属性的值将和 free属性的值相同\n * 当对 SDS 进行修改之后，SDS 的 len 属性 小于等于 1 MB，那么程序会分配 1MB的使用空间\n\n通过「空间预分配」策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数\n\n# 惰性空间释放\n\n「惰性空间释放」用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用 举个例子，SDS 的 API：sdstrim函数接受一个 SDS 和一个 C 字符串作为参数，移除 SDS 中所有在C字符串中出现过的字符\n\n\n\n注意，执行 sdstrim 之后的 SDS 并没有释放多出来的 8 字节空间，而是将这 8 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场\n\n所以也就避免了缩短字符串时所需的内存重分配操作，并未将来可能有的操作提供了优化\n\n同时，SDS 也提供了相应的 API，让我们可以在有需要的时候释放 SDS 的未使用空间，不用担心惰性空间释放策略会造成内存浪费\n\n思考\n\n什么时候 Redis 会去主动释放 SDS 中的未使用空间？\n\n\n# 二进制安全\n\nC 字符串中的字符必须符合某种编码，比如ASCII，并且除了字符串的末尾之外，字符串里面不能包含空字符 \\0，否则最先被程序读入的空字符将被误认为是字符串结尾，这些限制使得C字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据\n\n为了确保 Redis 可以适用于各种不同的使用场景（文本数据或者二进制数据），SDS 的 API 都是二进制安全的，所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 buf 数组里的数据程序，同时不会对其中的数据做任何限制、过滤、或者假设，数据在写入时是什么样的，它被读取时就是什么样\n\n\n\n\n# 兼容部分 C 字符串函数\n\n略\n\n\n# SDS 的内存友好设计\n\n在 Redis 3.X 之后，SDS 设计了不同类型的结构头，包括 sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。这些不同类型的结构头可以适配不同大小的字符串，从而避免了内存浪费\n\nstruct __attribute__ ((__packed__)) sdshdr8 {\n    uint8_t len;  // 已使用的字符串长度（8位，最大255字节）\n    uint8_t alloc; // 已分配的空间（8位，最大255字节）\n    unsigned char flags; // 标志，用于标识使用的是哪种sdshdr类型\n    char buf[];   // 字符串内容\n};\n\n\n而且，Redis 在保存较小字符串时，其实还使用了嵌入式字符串的设计方法。这种方法避免了给字符串分配额外的空间，而是可以让字符串直接保存在 Redis 的基本数据对象结构体中。\n\n所以这也就是说，要想理解嵌入式字符串的设计与实现，我们就需要先来了解下，Redis 使用的基本数据对象结构体 redisObject 是什么样的\n\n# redisObject 结构体与位域定义方法\n\nredisObject 结构体是在 server.h 文件中定义的，主要功能是用来保存「键值对中的值」\n\n这个结构一共定义了「4 个元数据」和 「一个指针」\n\n * type：redisObject 的数据类型，是应用程序在 Redis 中保存的数据类型，包括 String、List、Hash 等。\n * encoding：redisObject 的编码类型，是 Redis 内部实现各种数据类型所用的数据结构。\n * lru：redisObject 的 LRU 时间。\n * refcount：redisObject 的引用计数。\n * ptr：指向值的指针。\n\n下面的代码展示了 redisObject 结构体的定义：\n\ntypedef struct redisObject {\n    unsigned type:4; //redisObject的数据类型，4个bits\n    unsigned encoding:4; //redisObject的编码类型，4个bits\n    unsigned lru:LRU_BITS;  //redisObject的LRU时间，LRU_BITS为24个bits\n    int refcount; //redisObject的引用计数，4个字节\n    void *ptr; //指向值的指针，8个字节\n} robj;\n\n\n从代码中我们可以看到，在 type、encoding 和 lru 三个变量后面都有一个冒号，并紧跟着一个数值，表示该元数据占用的比特数。其中，type 和 encoding 分别占 4bits。而 lru 占用的比特数，是由 server.h 中的宏定义 LRU_BITS 决定的，它的默认值是 24bits，如下所示：\n\n#define LRU_BITS 24\n\n\n而这里我想让你学习掌握的，就是这种变量后使用冒号和数值的定义方法。这实际上是 C 语言中的位域定义方法，可以用来有效地节省内存开销。\n\n这种方法比较适用的场景是，当一个变量占用不了一个数据类型的所有 bits 时，就可以使用位域定义方法，把一个数据类型中的 bits，划分成多个位域，每个位域占一定的 bit 数。这样一来，一个数据类型的所有 bits 就可以定义多个变量了，从而也就有效节省了内存开销。\n\n此外，你可能还会发现，对于 type、encoding 和 lru 三个变量来说，它们的数据类型都是 unsigned。已知一个 unsigned 类型是 4 字节，但这三个变量，是分别占用了一个 unsigned 类型 4 字节中的 4bits、4bits 和 24bits。因此，相较于三个变量，每个变量用一个 4 字节的 unsigned 类型定义来说，使用位域定义方法可以让三个变量只用 4 字节，最后就能节省 8 字节的开销。\n\n所以，当你在设计开发内存敏感型的软件时，就可以把这种位域定义方法使用起来。\n\n好，了解了 redisObject 结构体和它使用的位域定义方法以后，我们再来看嵌入式字符串是如何实现的。\n\n# 嵌入式字符串\n\n前面我说过，SDS 在保存比较小的字符串时，会使用嵌入式字符串的设计方法，将字符串直接保存在 redisObject 结构体中。然后在 redisObject 结构体中，存在一个指向值的指针 ptr，而一般来说，这个 ptr 指针会指向值的数据结构。\n\n这里我们就以创建一个 String 类型的值为例，Redis 会调用 createStringObject 函数，来创建相应的 redisObject，而这个 redisObject 中的 ptr 指针，就会指向 SDS 数据结构，如下图所示。\n\n\n\n在 Redis 源码中，createStringObject 函数会根据要创建的字符串的长度，决定具体调用哪个函数来完成创建。\n\n那么针对这个 createStringObject 函数来说，它的参数是字符串 ptr 和字符串长度 len。当 len 的长度大于 OBJ_ENCODING_EMBSTR_SIZE_LIMIT 这个宏定义时，createStringObject 函数会调用 createRawStringObject 函数，否则就调用 createEmbeddedStringObject 函数。\n\n而在我们分析的 Redis 5.0.8 源码版本中，这个 OBJ_ENCODING_EMBSTR_SIZE_LIMIT 默认定义为 44 字节。\n\n这部分代码如下所示：\n\n#define OBJ_ENCODING_EMBSTR_SIZE_LIMIT 44\nrobj *createStringObject(const char *ptr, size_t len) {\n    //创建嵌入式字符串，字符串长度小于等于44字节\n    if (len <= OBJ_ENCODING_EMBSTR_SIZE_LIMIT)\n        return createEmbeddedStringObject(ptr,len);\n    //创建普通字符串，字符串长度大于44字节\n    else\n        return createRawStringObject(ptr,len);\n}\n\n\n现在，我们就来分析一下 createStringObject 函数的源码实现，以此了解大于 44 字节的普通字符串和小于等于 44 字节的嵌入式字符串分别是如何创建的。\n\n首先，对于 createRawStringObject 函数 来说，它在创建 String 类型的值的时候，会调用 createObject 函数。\n\n> 补充：createObject 函数主要是用来创建 Redis 的数据对象的。因为 Redis 的数据对象有很多类型，比如 String、List、Hash 等，所以在 createObject 函数的两个参数中，有一个就是用来表示所要创建的数据对象类型，而另一个是指向数据对象的指针。\n\n然后，createRawStringObject 函数在调用 createObject 函数时，会传递 OBJ_STRING 类型，表示要创建 String 类型的对象，以及传递指向 SDS 结构的指针，如以下代码所示。这里需要注意的是，指向 SDS 结构的指针是由 sdsnewlen 函数返回的，而 sdsnewlen 函数正是用来创建 SDS 结构的。\n\nrobj *createRawStringObject(const char *ptr, size_t len) {\n    return createObject(OBJ_STRING, sdsnewlen(ptr,len));\n}\n\n\n最后，我们再来进一步看下 createObject 函数。这个函数会把参数中传入的、指向 SDS 结构体的指针直接赋值给 redisObject 中的 ptr，这部分的代码如下所示：\n\nrobj *createObject(int type, void *ptr) {\n    //给redisObject结构体分配空间\n    robj *o = zmalloc(sizeof(*o));\n    //设置redisObject的类型\n    o->type = type;\n    //设置redisObject的编码类型，此处是OBJ_ENCODING_RAW，表示常规的SDS\n    o->encoding = OBJ_ENCODING_RAW;\n    //直接将传入的指针赋值给redisObject中的指针。\n    o->ptr = ptr;\n    o->refcount = 1;\n    …\n    return o;\n}\n\n\n为了方便理解普通字符串创建方法，我画了一张图，你可以看下。\n\n\n\n这也就是说，在创建普通字符串时，Redis 需要分别给 redisObject 和 SDS 分别分配一次内存，这样就既带来了内存分配开销，同时也会导致内存碎片。因此，当字符串小于等于 44 字节时，Redis 就使用了嵌入式字符串的创建方法，以此减少内存分配和内存碎片。\n\n而这个创建方法，就是由我们前面提到的 createEmbeddedStringObject 函数来完成的，该函数会使用一块连续的内存空间，来同时保存 redisObject 和 SDS 结构。这样一来，内存分配只有一次，而且也避免了内存碎片。\n\ncreateEmbeddedStringObject 函数的原型定义如下，它的参数就是从 createStringObject 函数参数中获得的字符串指针 ptr，以及字符串长度 len\n\nrobj *createEmbeddedStringObject(const char *ptr, size_t len)\n\n\n那么下面，我们就来具体看看，createEmbeddedStringObject 函数是如何把 redisObject 和 SDS 放置在一起的。\n\n首先，createEmbeddedStringObject 函数会分配一块连续的内存空间，这块内存空间的大小等于 redisObject 结构体的大小、SDS 结构头 sdshdr8 的大小和字符串大小的总和，并且再加上 1 字节。注意，这里最后的 1 字节是 SDS 中加在字符串最后的结束字符“\\0”。\n\n这块连续内存空间的分配情况如以下代码所示：\n\nrobj *o = zmalloc(sizeof(robj)+sizeof(struct sdshdr8)+len+1);\n\n\n你也可以参考下图，其中展示了这块内存空间的布局。\n\n\n\n好，那么 createEmbeddedStringObject 函数在分配了内存空间之后，就会 创建 SDS 结构的指针 sh，并把 sh 指向这块连续空间中 SDS 结构头所在的位置，下面的代码显示了这步操作。其中，o 是 redisObject 结构体的变量，o+1 表示将内存地址从变量 o 开始移动一段距离，而移动的距离等于 redisObject 这个结构体的大小。\n\nstruct sdshdr8 *sh = (void*)(o+1);\n\n\n经过这步操作后，sh 指向的位置就如下图所示：\n\n\n\n紧接着，createEmbeddedStringObject 函数会 把 redisObject 中的指针 ptr，指向 SDS 结构中的字符数组。\n\n如以下代码所示，其中 sh 是刚才介绍的指向 SDS 结构的指针，属于 sdshdr8 类型。而 sh+1 表示把内存地址从 sh 起始地址开始移动一定的大小，移动的距离等于 sdshdr8 结构体的大小。\n\no->ptr = sh+1;\n\n\n这步操作完成后，redisObject 结构体中的指针 ptr 的指向位置就如下图所示，它会指向 SDS 结构头的末尾，同时也是字符数组的起始位置：\n\n\n\n最后，createEmbeddedStringObject 函数会把参数中传入的指针 ptr 指向的字符串，拷贝到 SDS 结构体中的字符数组，并在数组最后添加结束字符。这部分代码如下所示：\n\nmemcpy(sh->buf,ptr,len);\nsh->buf[len] = '\\0';\n\n\n下面这张图，也展示了 createEmbeddedStringObject 创建嵌入式字符串的过程，你可以再整体来看看。\n\n\n\n总之，你可以记住，Redis 会通过设计实现一块连续的内存空间，把 redisObject 结构体和 SDS 结构体紧凑地放置在一起。这样一来，对于不超过 44 字节的字符串来说，就可以避免内存碎片和两次内存分配的开销了。\n\n\n# 总结\n\n 1. Redis 使用 SDS 的原因\n 2. SDS 和 C字符串 的 5 个区别\n 3. SDS 的内存友好设计\n 4. SDS 在不同的 len 属性下对应了底层数组的不同行为\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 为什么c语言的字符串在性能和安全性上不足以满足redis需求？\n 2. sds如何将获取字符串长度的操作从o(n)优化为o(1)？\n 3. 高并发下，sds的“空间预分配”如何提升频繁字符串操作的性能？\n 4. redis为什么需要sds的二进制安全？传统字符串处理方法有什么局限？\n 5. 为什么sds缩短时不立即释放多余内存？何时需要手动释放？\n 6. sds的嵌入式字符串如何减少内存碎片？适用于多大长度的字符串？\n\n\n# 前言\n\n一个优雅的字符串设计，需要尽量满足以下三个要求：\n\n * 能支持丰富的 api 操作，比如字符串追加、拷贝、比较、获取长度等\n * 能保存任意的二进制数据，比如图片等\n * 能尽可能地节省内存开销\n\nredis 设计了简单动态字符串（simple dynamic string，sds）的结构，用来表示字符串。\n\n相比于 c 语言中的字符串实现，sds 更适合 redis 的特性，我会在本文一一叙述\n\nsds 使用场景\n\n * 数据库中的字符串值\n * aof 模块中的 aof 缓冲区\n * 客户端状态中的输入缓冲区\n * 等等等等\n\nredis 很多用到了字符串的地方都是使用 sds\n\n\n# sds 的定义\n\nstruct sdshdr{\n\t//记录buf数组中已使用的字节数 等于sds所保存的字符串的长度\n\tint len;\n\t// 记录buf数组中未使用的字节数\n\tint free;\n\t//字节数组 用于保存字符串\n\tchar buf[];\n}\n\n\n\n\n笔记\n\nsds 遵循 c 字符串以空字符结尾的惯例，保存空字符的 1 字节空间不计算在 sds 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作，都是由sds函数自动完成的，所以这个空字符对于 sds 的使用者来说是完全透明的。遵循空字符结尾这一惯例的好处是，sds 可以直接重用一部分 c 字符串函数库里面的函数。\n\n\n# sds 与 c 字符串的区别\n\n\n\nc 语言字符串使用长度为 n+1 的字符串数组来存放长度为 n 的字符串，并且字符数组的最后一个元素总是空字符串 \\0\n\n这并不能满足 redis 对字符串在效率、安全性以及功能方面的要求\n\n接下来说明 sds 比 c 字符串更适用于 redis 的原因\n\n特性                  c 字符串            sds (简单动态字符串)\n获取字符串长度的复杂度         o(n)             o(1)\napi 安全性             不安全，可能导致缓冲区溢出    安全，不会导致缓冲区溢出\n修改字符串长度时的内存重分配次数    修改n次，执行n次内存重分配   修改n次，最多执行n次内存重分配\n数据类型支持              只能保存文本数据         可以保存文本或二进制数据\n<string.h>库函数使用情况   可使用所有函数          可使用部分函数\n\n\n# 常数复杂度获取字符串长度\n\n * c 字符串并不记录自身的长度信息，所以为了获取一个c字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 o(n)\n\n * sds 中有 len 属性来存储字符串长度，使得 strlen 命令的复杂度仅为 o(1)\n\nlen 属性的设置和更新是由 sds 的 api 在执行的时候自动完成的，我们无需手动设置\n\n\n\n\n# 杜绝缓冲区溢出\n\nchar *strcat(char *dest，const char *src);\n\n\n * c 字符串不记录自身长度，所以当调用 strcat 函数时，系统假定用户为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立，就会产生缓冲区溢出。需要用户对字符串进行扩容。\n\n * sds 的空间分配策略完全杜绝了发生缓冲区溢出的可能性：sds api需要对sds进行修改的时候，api会先检查sds的空间是否满足修改所需的要求，如果不满足的话，api会自动将sds的空间进行扩容到指定的大小，然后再次执行实际的修改操作，所以sds既不需要手动扩容，也不会缓冲区溢出\n\n\n# 减少修改字符串时带来的内存重分配次数\n\n因为 c 字符串的长度和底层数组的长度之间存在关联性，所以每次改变字符串长度的时候，程序总要对保存这个c字符串的数组进行一次内存重分配操作\n\n> 内存重分配：\n> \n>  * 如果程序执行的是增长字符串的操作，比如拼接操作append，那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小，如果忘了这一步就会产生缓冲区溢出\n>  * 如果程序执行的是缩短字符串的操作，比如截断操作trim，那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间，如果忘了这一步就会产生内存泄漏\n\n注意\n\n一般情况下，如果修改字符串长度的情况不常见，那么一般是可以接受内存重分配的。但是redis作为数据库，为了极致化性能体验，就会尽量避免内存重分配\n\n所以sds 通过使用 **free 属性 **记录未使用字节，从而解除了字符串长度和底层数组长度之间的关联：\n\n * 在sds中，buf 数组的长度不一定就是字符数量加一\n * 并且数组里面可以包含未使用的字节\n\n那 sds 是如何利用 free 属性去避免内存重分配的？\n\nsds实现了「空间预分配」和「惰性空间释放」两种优化策略\n\n# 空间预分配\n\n「空间预分配」用于优化 sds 的字符串增长操作：当 sds 的 api 对一个 sds 进行修改，并且需要对 sds 进行空间扩展的时候，程序不仅会为 sds 分配修改所必须要的空间，还会为 sds 分配额外的未使用空间\n\n额外分配的未使用空间数量有以下规则\n\n * 当对 sds 进行修改之后，sds 的 len 属性 小于 1 mb，那么程序分配和 len属性同样大小的未使用空间，这是 sds len属性的值将和 free属性的值相同\n * 当对 sds 进行修改之后，sds 的 len 属性 小于等于 1 mb，那么程序会分配 1mb的使用空间\n\n通过「空间预分配」策略，redis 可以减少连续执行字符串增长操作所需的内存重分配次数\n\n# 惰性空间释放\n\n「惰性空间释放」用于优化 sds 的字符串缩短操作：当 sds 的 api 需要缩短 sds 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用 举个例子，sds 的 api：sdstrim函数接受一个 sds 和一个 c 字符串作为参数，移除 sds 中所有在c字符串中出现过的字符\n\n\n\n注意，执行 sdstrim 之后的 sds 并没有释放多出来的 8 字节空间，而是将这 8 字节空间作为未使用空间保留在了 sds 里面，如果将来要对 sds 进行增长操作的话，这些未使用空间就可能会派上用场\n\n所以也就避免了缩短字符串时所需的内存重分配操作，并未将来可能有的操作提供了优化\n\n同时，sds 也提供了相应的 api，让我们可以在有需要的时候释放 sds 的未使用空间，不用担心惰性空间释放策略会造成内存浪费\n\n思考\n\n什么时候 redis 会去主动释放 sds 中的未使用空间？\n\n\n# 二进制安全\n\nc 字符串中的字符必须符合某种编码，比如ascii，并且除了字符串的末尾之外，字符串里面不能包含空字符 \\0，否则最先被程序读入的空字符将被误认为是字符串结尾，这些限制使得c字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据\n\n为了确保 redis 可以适用于各种不同的使用场景（文本数据或者二进制数据），sds 的 api 都是二进制安全的，所有 sds api 都会以处理二进制的方式来处理 sds 存放在 buf 数组里的数据程序，同时不会对其中的数据做任何限制、过滤、或者假设，数据在写入时是什么样的，它被读取时就是什么样\n\n\n\n\n# 兼容部分 c 字符串函数\n\n略\n\n\n# sds 的内存友好设计\n\n在 redis 3.x 之后，sds 设计了不同类型的结构头，包括 sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。这些不同类型的结构头可以适配不同大小的字符串，从而避免了内存浪费\n\nstruct __attribute__ ((__packed__)) sdshdr8 {\n    uint8_t len;  // 已使用的字符串长度（8位，最大255字节）\n    uint8_t alloc; // 已分配的空间（8位，最大255字节）\n    unsigned char flags; // 标志，用于标识使用的是哪种sdshdr类型\n    char buf[];   // 字符串内容\n};\n\n\n而且，redis 在保存较小字符串时，其实还使用了嵌入式字符串的设计方法。这种方法避免了给字符串分配额外的空间，而是可以让字符串直接保存在 redis 的基本数据对象结构体中。\n\n所以这也就是说，要想理解嵌入式字符串的设计与实现，我们就需要先来了解下，redis 使用的基本数据对象结构体 redisobject 是什么样的\n\n# redisobject 结构体与位域定义方法\n\nredisobject 结构体是在 server.h 文件中定义的，主要功能是用来保存「键值对中的值」\n\n这个结构一共定义了「4 个元数据」和 「一个指针」\n\n * type：redisobject 的数据类型，是应用程序在 redis 中保存的数据类型，包括 string、list、hash 等。\n * encoding：redisobject 的编码类型，是 redis 内部实现各种数据类型所用的数据结构。\n * lru：redisobject 的 lru 时间。\n * refcount：redisobject 的引用计数。\n * ptr：指向值的指针。\n\n下面的代码展示了 redisobject 结构体的定义：\n\ntypedef struct redisobject {\n    unsigned type:4; //redisobject的数据类型，4个bits\n    unsigned encoding:4; //redisobject的编码类型，4个bits\n    unsigned lru:lru_bits;  //redisobject的lru时间，lru_bits为24个bits\n    int refcount; //redisobject的引用计数，4个字节\n    void *ptr; //指向值的指针，8个字节\n} robj;\n\n\n从代码中我们可以看到，在 type、encoding 和 lru 三个变量后面都有一个冒号，并紧跟着一个数值，表示该元数据占用的比特数。其中，type 和 encoding 分别占 4bits。而 lru 占用的比特数，是由 server.h 中的宏定义 lru_bits 决定的，它的默认值是 24bits，如下所示：\n\n#define lru_bits 24\n\n\n而这里我想让你学习掌握的，就是这种变量后使用冒号和数值的定义方法。这实际上是 c 语言中的位域定义方法，可以用来有效地节省内存开销。\n\n这种方法比较适用的场景是，当一个变量占用不了一个数据类型的所有 bits 时，就可以使用位域定义方法，把一个数据类型中的 bits，划分成多个位域，每个位域占一定的 bit 数。这样一来，一个数据类型的所有 bits 就可以定义多个变量了，从而也就有效节省了内存开销。\n\n此外，你可能还会发现，对于 type、encoding 和 lru 三个变量来说，它们的数据类型都是 unsigned。已知一个 unsigned 类型是 4 字节，但这三个变量，是分别占用了一个 unsigned 类型 4 字节中的 4bits、4bits 和 24bits。因此，相较于三个变量，每个变量用一个 4 字节的 unsigned 类型定义来说，使用位域定义方法可以让三个变量只用 4 字节，最后就能节省 8 字节的开销。\n\n所以，当你在设计开发内存敏感型的软件时，就可以把这种位域定义方法使用起来。\n\n好，了解了 redisobject 结构体和它使用的位域定义方法以后，我们再来看嵌入式字符串是如何实现的。\n\n# 嵌入式字符串\n\n前面我说过，sds 在保存比较小的字符串时，会使用嵌入式字符串的设计方法，将字符串直接保存在 redisobject 结构体中。然后在 redisobject 结构体中，存在一个指向值的指针 ptr，而一般来说，这个 ptr 指针会指向值的数据结构。\n\n这里我们就以创建一个 string 类型的值为例，redis 会调用 createstringobject 函数，来创建相应的 redisobject，而这个 redisobject 中的 ptr 指针，就会指向 sds 数据结构，如下图所示。\n\n\n\n在 redis 源码中，createstringobject 函数会根据要创建的字符串的长度，决定具体调用哪个函数来完成创建。\n\n那么针对这个 createstringobject 函数来说，它的参数是字符串 ptr 和字符串长度 len。当 len 的长度大于 obj_encoding_embstr_size_limit 这个宏定义时，createstringobject 函数会调用 createrawstringobject 函数，否则就调用 createembeddedstringobject 函数。\n\n而在我们分析的 redis 5.0.8 源码版本中，这个 obj_encoding_embstr_size_limit 默认定义为 44 字节。\n\n这部分代码如下所示：\n\n#define obj_encoding_embstr_size_limit 44\nrobj *createstringobject(const char *ptr, size_t len) {\n    //创建嵌入式字符串，字符串长度小于等于44字节\n    if (len <= obj_encoding_embstr_size_limit)\n        return createembeddedstringobject(ptr,len);\n    //创建普通字符串，字符串长度大于44字节\n    else\n        return createrawstringobject(ptr,len);\n}\n\n\n现在，我们就来分析一下 createstringobject 函数的源码实现，以此了解大于 44 字节的普通字符串和小于等于 44 字节的嵌入式字符串分别是如何创建的。\n\n首先，对于 createrawstringobject 函数 来说，它在创建 string 类型的值的时候，会调用 createobject 函数。\n\n> 补充：createobject 函数主要是用来创建 redis 的数据对象的。因为 redis 的数据对象有很多类型，比如 string、list、hash 等，所以在 createobject 函数的两个参数中，有一个就是用来表示所要创建的数据对象类型，而另一个是指向数据对象的指针。\n\n然后，createrawstringobject 函数在调用 createobject 函数时，会传递 obj_string 类型，表示要创建 string 类型的对象，以及传递指向 sds 结构的指针，如以下代码所示。这里需要注意的是，指向 sds 结构的指针是由 sdsnewlen 函数返回的，而 sdsnewlen 函数正是用来创建 sds 结构的。\n\nrobj *createrawstringobject(const char *ptr, size_t len) {\n    return createobject(obj_string, sdsnewlen(ptr,len));\n}\n\n\n最后，我们再来进一步看下 createobject 函数。这个函数会把参数中传入的、指向 sds 结构体的指针直接赋值给 redisobject 中的 ptr，这部分的代码如下所示：\n\nrobj *createobject(int type, void *ptr) {\n    //给redisobject结构体分配空间\n    robj *o = zmalloc(sizeof(*o));\n    //设置redisobject的类型\n    o->type = type;\n    //设置redisobject的编码类型，此处是obj_encoding_raw，表示常规的sds\n    o->encoding = obj_encoding_raw;\n    //直接将传入的指针赋值给redisobject中的指针。\n    o->ptr = ptr;\n    o->refcount = 1;\n    …\n    return o;\n}\n\n\n为了方便理解普通字符串创建方法，我画了一张图，你可以看下。\n\n\n\n这也就是说，在创建普通字符串时，redis 需要分别给 redisobject 和 sds 分别分配一次内存，这样就既带来了内存分配开销，同时也会导致内存碎片。因此，当字符串小于等于 44 字节时，redis 就使用了嵌入式字符串的创建方法，以此减少内存分配和内存碎片。\n\n而这个创建方法，就是由我们前面提到的 createembeddedstringobject 函数来完成的，该函数会使用一块连续的内存空间，来同时保存 redisobject 和 sds 结构。这样一来，内存分配只有一次，而且也避免了内存碎片。\n\ncreateembeddedstringobject 函数的原型定义如下，它的参数就是从 createstringobject 函数参数中获得的字符串指针 ptr，以及字符串长度 len\n\nrobj *createembeddedstringobject(const char *ptr, size_t len)\n\n\n那么下面，我们就来具体看看，createembeddedstringobject 函数是如何把 redisobject 和 sds 放置在一起的。\n\n首先，createembeddedstringobject 函数会分配一块连续的内存空间，这块内存空间的大小等于 redisobject 结构体的大小、sds 结构头 sdshdr8 的大小和字符串大小的总和，并且再加上 1 字节。注意，这里最后的 1 字节是 sds 中加在字符串最后的结束字符“\\0”。\n\n这块连续内存空间的分配情况如以下代码所示：\n\nrobj *o = zmalloc(sizeof(robj)+sizeof(struct sdshdr8)+len+1);\n\n\n你也可以参考下图，其中展示了这块内存空间的布局。\n\n\n\n好，那么 createembeddedstringobject 函数在分配了内存空间之后，就会 创建 sds 结构的指针 sh，并把 sh 指向这块连续空间中 sds 结构头所在的位置，下面的代码显示了这步操作。其中，o 是 redisobject 结构体的变量，o+1 表示将内存地址从变量 o 开始移动一段距离，而移动的距离等于 redisobject 这个结构体的大小。\n\nstruct sdshdr8 *sh = (void*)(o+1);\n\n\n经过这步操作后，sh 指向的位置就如下图所示：\n\n\n\n紧接着，createembeddedstringobject 函数会 把 redisobject 中的指针 ptr，指向 sds 结构中的字符数组。\n\n如以下代码所示，其中 sh 是刚才介绍的指向 sds 结构的指针，属于 sdshdr8 类型。而 sh+1 表示把内存地址从 sh 起始地址开始移动一定的大小，移动的距离等于 sdshdr8 结构体的大小。\n\no->ptr = sh+1;\n\n\n这步操作完成后，redisobject 结构体中的指针 ptr 的指向位置就如下图所示，它会指向 sds 结构头的末尾，同时也是字符数组的起始位置：\n\n\n\n最后，createembeddedstringobject 函数会把参数中传入的指针 ptr 指向的字符串，拷贝到 sds 结构体中的字符数组，并在数组最后添加结束字符。这部分代码如下所示：\n\nmemcpy(sh->buf,ptr,len);\nsh->buf[len] = '\\0';\n\n\n下面这张图，也展示了 createembeddedstringobject 创建嵌入式字符串的过程，你可以再整体来看看。\n\n\n\n总之，你可以记住，redis 会通过设计实现一块连续的内存空间，把 redisobject 结构体和 sds 结构体紧凑地放置在一起。这样一来，对于不超过 44 字节的字符串来说，就可以避免内存碎片和两次内存分配的开销了。\n\n\n# 总结\n\n 1. redis 使用 sds 的原因\n 2. sds 和 c字符串 的 5 个区别\n 3. sds 的内存友好设计\n 4. sds 在不同的 len 属性下对应了底层数组的不同行为\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"List 设计与实现",frontmatter:{title:"List 设计与实现",date:"2024-09-16T02:46:18.000Z",permalink:"/pages/bd1e41/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/02.%E5%9F%BA%E7%A1%80/02.List%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Redis 系统设计/02.基础/02.List 设计与实现.md",key:"v-715b1431",path:"/pages/bd1e41/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:321},{level:2,title:"ziplist 设计与实现",slug:"ziplist-设计与实现",normalizedTitle:"ziplist 设计与实现",charIndex:789},{level:3,title:"ziplist 和 整数集合 的设计",slug:"ziplist-和-整数集合-的设计",normalizedTitle:"ziplist 和 整数集合 的设计",charIndex:807},{level:3,title:"扩展：节省内存的数据访问",slug:"扩展-节省内存的数据访问",normalizedTitle:"扩展：节省内存的数据访问",charIndex:4474},{level:3,title:"ziplist 的不足",slug:"ziplist-的不足",normalizedTitle:"ziplist 的不足",charIndex:5420},{level:4,title:"查找复杂度高",slug:"查找复杂度高",normalizedTitle:"查找复杂度高",charIndex:5697},{level:4,title:"连锁更新风险",slug:"连锁更新风险",normalizedTitle:"连锁更新风险",charIndex:5710},{level:2,title:"quicklist 设计与实现",slug:"quicklist-设计与实现",normalizedTitle:"quicklist 设计与实现",charIndex:10003},{level:2,title:"listpack 设计与实现",slug:"listpack-设计与实现",normalizedTitle:"listpack 设计与实现",charIndex:12592},{level:3,title:"listpack 列表项编码方法",slug:"listpack-列表项编码方法",normalizedTitle:"listpack 列表项编码方法",charIndex:13863},{level:3,title:"listpack 避免连锁更新的实现方式",slug:"listpack-避免连锁更新的实现方式",normalizedTitle:"listpack 避免连锁更新的实现方式",charIndex:15319},{level:2,title:"对比",slug:"对比",normalizedTitle:"对比",charIndex:18086},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:18631},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:19503}],headersStr:"前言 ziplist 设计与实现 ziplist 和 整数集合 的设计 扩展：节省内存的数据访问 ziplist 的不足 查找复杂度高 连锁更新风险 quicklist 设计与实现 listpack 设计与实现 listpack 列表项编码方法 listpack 避免连锁更新的实现方式 对比 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. 为什么 Redis 设计了三种类似但不同的底层数据结构 ziplist、quicklist、listpack？\n 2. ziplist 为什么会面临查找效率降低和内存连锁更新的问题？\n 3. quicklist 是如何解决 ziplist 的性能瓶颈的？\n 4. listpack 是如何避免 ziplist 的连锁更新问题的？\n 5. 如何在实际开发中选择适合的 Redis List 底层结构，以平衡内存使用和性能？\n 6. ziplist 在 Redis 设计中有哪些优点被保留了下来，即使它存在性能不足？\n 7. 当数据插入量大时，quicklist 如何避免内存频繁重新分配而保持高效？\n\n\n# 前言\n\nRedis 中的 List 的底层有三种数据结构\n\n * ziplist\n * quicklist\n * listpack\n\nziplist 的最大特点，就是它被设计成一种 内存紧凑型 的数据结构，占用一块连续的内存空间，以达到节省内存的目的\n\n但是，在计算机系统中，任何一个设计都是有利有弊的。对于 ziplist 来说，这个道理同样成立。\n\n虽然 ziplist 节省了内存开销，可它也存在两个设计代价\n\n * 「不能保存过多的元素」，否则访问性能会降低\n * 「不能保存过大的元素」，否则容易导致内存重新分配，甚至可能引发连锁更新的问题。所谓的连锁更新，简单来说，就是 ziplist 中的每一项都要被重新分配内存空间，造成 ziplist 的性能降低\n\n因此，针对 ziplist 在设计上的不足，Redis 在演进的过程中，新增设计了两种数据结构\n\n * quicklist\n * istpack\n\n它们设计目标，是 尽可能地保持 ziplist 节省内存的优势，同时避免 ziplist 潜在的性能下降问题\n\n\n# ziplist 设计与实现\n\n\n# ziplist 和 整数集合 的设计\n\n首先你要知道，List、Hash 和 Sorted Set 这三种数据类型，都可以使用压缩列表（ziplist）来保存数据。压缩列表的函数定义和实现代码分别在 ziplist.h 和 ziplist.c 中\n\n不过，我们在 ziplist.h 文件中其实根本看不到压缩列表的结构体定义。这是因为压缩列表本身就是一块连续的内存空间，它通过使用不同的编码来保存数据\n\n这里为了方便理解压缩列表的设计与实现，我们先来看看它的创建函数 ziplistNew，如下所示：\n\nunsigned char *ziplistNew(void) {\n    //初始分配的大小\n    unsigned int bytes = ZIPLIST_HEADER_SIZE+ZIPLIST_END_SIZE;\n    unsigned char *zl = zmalloc(bytes);\n    …\n   //将列表尾设置为ZIP_END\n    zl[bytes-1] = ZIP_END;\n    return zl;\n}\n\n\n实际上，ziplistNew 函数的逻辑很简单，就是创建一块连续的内存空间，大小为 ZIPLIST_HEADER_SIZE 和 ZIPLIST_END_SIZE 的总和，然后再把该连续空间的最后一个字节赋值为 ZIP_END，表示列表结束。\n\n这三个宏分别表示 ziplist 的列表头大小、列表尾大小和列表尾字节内容\n\n//ziplist的列表头大小，包括2个32 bits整数和1个16bits整数，分别表示压缩列表的总字节数，列表最后一个元素的离列表头的偏移，以及列表中的元素个数\n#define ZIPLIST_HEADER_SIZE     (sizeof(uint32_t)*2+sizeof(uint16_t))\n//ziplist的列表尾大小，包括1个8 bits整数，表示列表结束。\n#define ZIPLIST_END_SIZE        (sizeof(uint8_t))\n//ziplist的列表尾字节内容\n#define ZIP_END 255\n\n\n那么，在创建一个新的 ziplist 后，该列表的内存布局就如下图所示。注意，此时列表中还没有实际的数据。\n\n\n\n然后，当我们往 ziplist 中插入数据时，ziplist 就会根据数据是字符串还是整数，以及它们的大小进行不同的编码。这种根据数据大小进行相应编码的设计思想，正是 Redis 为了节省内存而采用的\n\nziplist 列表项包括三部分内容，分别是前一项的长度（prevlen）、当前项长度信息的编码结果（encoding），以及当前项的实际数据（data）。下面的图展示了列表项的结构（图中除列表项之外的内容分别是 ziplist 内存空间的起始和尾部）。\n\n\n\n实际上，所谓的编码技术，就是指 用不同数量的字节来表示保存的信息。在 ziplist 中，编码技术主要应用在列表项中的 prevlen 和 encoding 这两个元数据上。而当前项的实际数据 data，则正常用整数或是字符串来表示。\n\n所以这里，我们就先来看下 prevlen 的编码设计。ziplist 中会包含多个列表项，每个列表项都是紧挨着彼此存放的，如下图所示。\n\n\n\n而为了方便查找，每个列表项中都会记录前一项的长度。因为每个列表项的长度不一样，所以如果使用相同的字节大小来记录 prevlen，就会造成内存空间浪费。\n\n我给你举个例子，假设我们统一使用 4 字节记录 prevlen，如果前一个列表项只是一个字符串“redis”，长度为 5 个字节，那么我们用 1 个字节（8 bits）就能表示 256 字节长度（2 的 8 次方等于 256）的字符串了。此时，prevlen 用 4 字节记录，其中就有 3 字节是浪费掉了。\n\n好，我们再回过头来看，ziplist 在对 prevlen 编码时，会先调用 zipStorePrevEntryLength 函数，用于判断前一个列表项是否小于 254 字节。如果是的话，那么 prevlen 就使用 1 字节表示；否则，zipStorePrevEntryLength 函数就调用 zipStorePrevEntryLengthLarge 函数进一步编码。这部分代码如下所示：\n\n//判断prevlen的长度是否小于ZIP_BIG_PREVLEN，ZIP_BIG_PREVLEN等于254\nif (len < ZIP_BIG_PREVLEN) {\n   //如果小于254字节，那么返回prevlen为1字节\n   p[0] = len;\n   return 1;\n} else {\n   //否则，调用zipStorePrevEntryLengthLarge进行编码\n   return zipStorePrevEntryLengthLarge(p,len);\n}\n\n\n也就是说，zipStorePrevEntryLengthLarge 函数会先将 prevlen 的第 1 字节设置为 254，然后使用内存拷贝函数 memcpy，将前一个列表项的长度值拷贝至 prevlen 的第 2 至第 5 字节。最后，zipStorePrevEntryLengthLarge 函数返回 prevlen 的大小，为 5 字节。\n\nif (p != NULL) {\n    //将prevlen的第1字节设置为ZIP_BIG_PREVLEN，即254\n    p[0] = ZIP_BIG_PREVLEN;\n  //将前一个列表项的长度值拷贝至prevlen的第2至第5字节，其中sizeof(len)的值为4\n    memcpy(p+1,&len,sizeof(len));\n    …\n}\n//返回prevlen的大小，为5字节\nreturn 1+sizeof(len);\n\n\n好，在了解了 prevlen 使用 1 字节和 5 字节两种编码方式后，我们再来学习下 encoding 的编码方法。\n\n我们知道，一个列表项的实际数据，既可以是整数也可以是字符串。整数可以是 16、32、64 等字节长度，同时字符串的长度也可以大小不一。\n\n所以，ziplist 在 zipStoreEntryEncoding 函数中，针对整数和字符串，就分别使用了不同字节长度的编码结果。下面的代码展示了 zipStoreEntryEncoding 函数的部分代码，你可以看到当数据是不同长度字符串或是整数时，编码结果的长度 len 大小不同。\n\n//默认编码结果是1字节\nunsigned char len = 1;\n//如果是字符串数据\nif (ZIP_IS_STR(encoding)) {\n    //字符串长度小于等于63字节（16进制为0x3f）\n    if (rawlen <= 0x3f) {\n        //默认编码结果是1字节\n        …\n    }\n    //字符串长度小于等于16383字节（16进制为0x3fff）\n    else if (rawlen <= 0x3fff) {\n        //编码结果是2字节\n        len += 1;\n        …\n    }\n    //字符串长度大于16383字节\n\n    else {\n        //编码结果是5字节\n        len += 4;\n        …\n    }\n} else {\n    /* 如果数据是整数，编码结果是1字节*/\n    if (!p) return len;\n    ...\n}\n\n\n简而言之，针对不同长度的数据，使用不同大小的元数据信息（prevlen 和 encoding），这种方法可以有效地节省内存开销\n\n除了 ziplist 之外，Redis 还设计了一个内存友好的数据结构，这就是整数集合（intset），它是作为底层结构来实现 Set 数据类型的。\n\n和 SDS 嵌入式字符串、ziplist 类似，整数集合也是一块连续的内存空间，这一点我们从整数集合的定义中就可以看到。intset.h 和 intset.c 分别包括了整数集合的定义和实现\n\n下面的代码展示了 intset 的结构定义。我们可以看到，整数集合结构体中记录数据的部分，就是一个 int8_t 类型的整数数组 contents。从内存使用的角度来看，整数数组就是一块连续内存空间，所以这样就避免了内存碎片，并提升了内存使用效率\n\ntypedef struct intset {\n    uint32_t encoding;\n    uint32_t length;\n    int8_t contents[];\n} intset;\n\n\n\n# 扩展：节省内存的数据访问\n\n我们知道，在 Redis 实例运行时，有些数据是会被经常访问的，比如常见的整数，Redis 协议中常见的回复信息，包括操作成功（“OK”字符串）、操作失败（ERR），以及常见的报错信息。\n\n所以，为了避免在内存中反复创建这些经常被访问的数据，Redis 就采用了共享对象的设计思想。这个设计思想很简单，就是把这些常用数据创建为共享对象，当上层应用需要访问它们时，直接读取就行。\n\n现在我们就来做个假设。有 1000 个客户端，都要保存“3”这个整数。如果 Redis 为每个客户端，都创建了一个值为 3 的 redisObject，那么内存中就会有大量的冗余。而使用了共享对象方法后，Redis 在内存中只用保存一个 3 的 redisObject 就行，这样就有效节省了内存空间。\n\n以下代码展示的是 server.c 文件中，创建共享对象的函数 createSharedObjects，你可以看下。\n\nvoid createSharedObjects(void) {\n   …\n   //常见回复信息\n   shared.ok = createObject(OBJ_STRING,sdsnew("+OK\\r\\n"));\n   shared.err = createObject(OBJ_STRING,sdsnew("-ERR\\r\\n"));\n   …\n   //常见报错信息\n shared.nokeyerr = createObject(OBJ_STRING,sdsnew("-ERR no such key\\r\\n"));\n shared.syntaxerr = createObject(OBJ_STRING,sdsnew("-ERR syntax error\\r\\n"));\n   //0到9999的整数\n   for (j = 0; j < OBJ_SHARED_INTEGERS; j++) {\n        shared.integers[j] =\n          makeObjectShared(createObject(OBJ_STRING,(void*)(long)j));\n        …\n    }\n   …\n}\n\n\n\n# ziplist 的不足\n\n你已经知道，一个 ziplist 数据结构在内存中的布局，就是一块连续的内存空间。这块空间的起始部分是大小固定的 10 字节元数据，其中记录了 ziplist 的总字节数、最后一个元素的偏移量以及列表元素的数量，而这 10 字节后面的内存空间则保存了实际的列表数据。在 ziplist 的最后部分，是一个 1 字节的标识（固定为 255），用来表示 ziplist 的结束，如下图所示：\n\n\n\n不过，虽然 ziplist 通过紧凑的内存布局来保存数据，节省了内存空间，但是 ziplist 也面临着随之而来的两个不足：\n\n * 查找复杂度高\n * 潜在的连锁更新风险\n\n那么下面，我们就分别来了解下这两个问题\n\n# 查找复杂度高\n\n因为 ziplist 头尾元数据的大小是固定的，所以可以很快找到 首部元素和尾部元素，但问题是\n\n * 当要查找中间元素时，ziplist 就得从列表头或列表尾遍历才行\n * 更糟糕的是，如果 ziplist 里面保存的是字符串，ziplist 在查找某个元素时，还需要逐一判断元素的每个字符，这样又进一步增加了复杂度\n * ziplist 在插入元素时，如果内存空间不够了，ziplist 还需要重新分配一块连续的内存空间，而这还会进一步引发连锁更新的问题\n\n也正因为如此，我们在使用 ziplist 保存 Hash 或 Sorted Set 数据时，都会在 redis.conf 文件中，通过 hash-max-ziplist-entries 和 zset-max-ziplist-entries 两个参数，来控制保存在 ziplist 中的元素个数\n\n# 连锁更新风险\n\n我们知道，因为 ziplist 必须使用一块连续的内存空间来保存数据，所以当新插入一个元素时，ziplist 就需要计算其所需的空间大小，并申请相应的内存空间。这一系列操作，我们可以从 ziplist 的元素插入函数 __ziplistInsert 中看到。\n\n__ziplistInsert 函数首先会计算获得当前 ziplist 的长度，这个步骤通过 ZIPLIST_BYTES 宏定义就可以完成，如下所示。同时，该函数还声明了 reqlen 变量，用于记录插入元素后所需的新增空间大小。\n\n//获取当前ziplist长度curlen；声明reqlen变量，用来记录新插入元素所需的长度\nsize_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), reqlen;\n\n\n然后，__ziplistInsert 函数会判断当前要插入的位置是否是列表末尾。如果不是末尾，那么就需要获取位于当前插入位置的元素的 prevlen 和 prevlensize。这部分代码如下所示：\n\n//如果插入的位置不是ziplist末尾，则获取前一项长度\nif (p[0] != ZIP_END) {\n    ZIP_DECODE_PREVLEN(p, prevlensize, prevlen);\n} else {\n    …\n}\n\n\n实际上，在 ziplist 中，每一个元素都会记录其前一项的长度，也就是 prevlen。然后，为了节省内存开销，ziplist 会使用不同的空间记录 prevlen，这个 prevlen 空间大小就是 prevlensize。\n\n举个简单的例子，当在一个元素 A 前插入一个新的元素 B 时，A 的 prevlen 和 prevlensize 都要根据 B 的长度进行相应的变化。\n\n那么现在，我们假设 A 的 prevlen 原本只占用 1 字节（也就是 prevlensize 等于 1），而能记录的前一项长度最大为 253 字节。此时，如果 B 的长度超过了 253 字节，A 的 prevlen 就需要使用 5 个字节来记录（prevlen 具体的编码方式，你可以复习回顾下第 4 讲），这样就需要申请额外的 4 字节空间了。不过，如果元素 B 的插入位置是列表末尾，那么插入元素 B 时，我们就不用考虑后面元素的 prevlen 了\n\n\n\n因此，为了保证 ziplist 有足够的内存空间，来保存插入元素以及插入位置元素的 prevlen 信息，__ziplistInsert 函数在获得插入位置元素的 prevlen 和 prevlensize 后，紧接着就会计算插入元素的长度。\n\n现在我们已知，一个 ziplist 元素包括了 prevlen、encoding 和实际数据 data 三个部分。所以，在计算插入元素的所需空间时，__ziplistInsert 函数也会分别计算这三个部分的长度。这个计算过程一共可以分成四步来完成。\n\n * 第一步，计算实际插入元素的长度。\n\n首先你要知道，这个计算过程和插入元素是整数还是字符串有关。__ziplistInsert 函数会先调用 zipTryEncoding 函数，这个函数会判断插入元素是否为整数。如果是整数，就按照不同的整数大小，计算 encoding 和实际数据 data 各自所需的空间；如果是字符串，那么就先把字符串长度记录为所需的新增空间大小。这一过程的代码如下所示：\n\n  if (zipTryEncoding(s,slen,&value,&encoding)) {\n          reqlen = zipIntSize(encoding);\n      } else {\n          reqlen = slen;\n      }\n\n\n * 第二步，调用 zipStorePrevEntryLength 函数，将插入位置元素的 prevlen 也计算到所需空间中。\n\n这是因为在插入元素后，__ziplistInsert 函数可能要为插入位置的元素分配新增空间。这部分代码如下所示：\n\nreqlen += zipStorePrevEntryLength(NULL,prevlen);\n\n\n * 第三步，调用 zipStoreEntryEncoding 函数，根据字符串的长度，计算相应 encoding 的大小。\n\n在刚才的第一步中，**ziplistInsert 函数对于字符串数据，只是记录了字符串本身的长度，所以在第三步中，**ziplistInsert 函数还会调用 zipStoreEntryEncoding 函数，根据字符串的长度来计算相应的 encoding 大小，如下所示：\n\nreqlen += zipStoreEntryEncoding(NULL,encoding,slen);\n\n\n好了，到这里，__ziplistInsert 函数就已经在 reqlen 变量中，记录了插入元素的 prevlen 长度、encoding 大小，以及实际数据 data 的长度。这样一来，插入元素的整体长度就有了，这也是插入位置元素的 prevlen 所要记录的大小。\n\n * 第四步，调用 zipPrevLenByteDiff 函数，判断插入位置元素的 prevlen 和实际所需的 prevlen 大小。\n\n最后，__ziplistInsert 函数会调用 zipPrevLenByteDiff 函数，用来判断插入位置元素的 prevlen 和实际所需的 prevlen，这两者间的大小差别。这部分代码如下所示，prevlen 的大小差别是使用 nextdiff 来记录的：\n\nnextdiff = (p[0] != ZIP_END) ? zipPrevLenByteDiff(p,reqlen) : 0;\n\n\n那么在这里，如果 nextdiff 大于 0，就表明插入位置元素的空间不够，需要新增 nextdiff 大小的空间，以便能保存新的 prevlen。然后，__ziplistInsert 函数在新增空间时，就会调用 ziplistResize 函数，来重新分配 ziplist 所需的空间。\n\nziplistResize 函数接收的参数分别是待重新分配的 ziplist 和重新分配的空间大小。而 __ziplistInsert 函数传入的重新分配大小的参数，是三个长度之和。\n\n那么是哪三个长度之和呢？\n\n这三个长度分别是 ziplist 现有大小（curlen）、待插入元素自身所需的新增空间（reqlen），以及插入位置元素 prevlen 所需的新增空间（nextdiff）。下面的代码显示了 ziplistResize 函数的调用和参数传递逻辑：\n\nzl = ziplistResize(zl,curlen+reqlen+nextdiff);\n\n\n进一步，那么 ziplistResize 函数在获得三个长度总和之后，具体是如何扩容呢？\n\n我们可以进一步看下 ziplistResize 函数的实现，这个函数会调用 zrealloc 函数，来完成空间的重新分配，而重新分配的空间大小就是由传入参数 len 决定的。这样，我们就了解到了 ziplistResize 函数涉及到内存分配操作，因此如果我们往 ziplist 频繁插入过多数据的话，就可能引起多次内存分配，从而会对 Redis 性能造成影响。\n\n下面的代码显示了 ziplistResize 函数的部分实现，你可以看下。\n\nunsigned char *ziplistResize(unsigned char *zl, unsigned int len) {\n    //对zl进行重新内存空间分配，重新分配的大小是len\n    zl = zrealloc(zl,len);\n    …\n    zl[len-1] = ZIP_END;\n    return zl;\n}\n\n\n好了，到这里，我们就了解了 ziplist 在新插入元素时，会计算其所需的新增空间，并进行重新分配。而当新插入的元素较大时，就会引起插入位置的元素 prevlensize 增加，进而就会导致插入位置的元素所占空间也增加。\n\n而如此一来，这种空间新增就会引起连锁更新的问题。\n\n实际上，所谓的连锁更新，就是指当一个元素插入后，会引起当前位置元素新增 prevlensize 的空间。而当前位置元素的空间增加后，又会进一步引起该元素的后续元素，其 prevlensize 所需空间的增加。\n\n这样，一旦插入位置后续的所有元素，都会因为前序元素的 prevlenszie 增加，而引起自身空间也要增加，这种每个元素的空间都需要增加的现象，就是连锁更新。我画了下面这张图，你可以看下。\n\n\n\n连锁更新一旦发生，就会导致 ziplist 占用的内存空间要多次重新分配，这就会直接影响到 ziplist 的访问性能。\n\n所以说，虽然 ziplist 紧凑型的内存布局能节省内存开销，但是如果保存的元素数量增加了，或是元素变大了，ziplist 就会面临性能问题。那么，有没有什么方法可以避免 ziplist 的问题呢\n\n这就是接下来我要给你介绍的 quicklist 和 listpack，这两种数据结构的设计思想了\n\n\n# quicklist 设计与实现\n\n我们先来学习下 quicklist 的实现思路。\n\nquicklist 的设计，其实是结合了链表和 ziplist 各自的优势。简单来说，一个 quicklist 就是一个链表，而链表中的每个元素又是一个 ziplist\n\n我们来看下 quicklist 的数据结构，这是在quicklist.h文件中定义的，而 quicklist 的具体实现是在quicklist.c文件中。\n\n首先，quicklist 元素的定义，也就是 quicklistNode。因为 quicklist 是一个链表，所以每个 quicklistNode 中，都包含了分别指向它前序和后序节点的指针prev 和 next**。同时，每个 quicklistNode 又是一个 ziplist，所以，在 quicklistNode 的结构体中，还有指向 ziplist 的**指针 zl。\n\n此外，quicklistNode 结构体中还定义了一些属性，比如 ziplist 的字节大小、包含的元素个数、编码格式、存储方式等。下面的代码显示了 quicklistNode 的结构体定义，你可以看下。\n\ntypedef struct quicklistNode {\n    struct quicklistNode *prev;     //前一个quicklistNode\n    struct quicklistNode *next;     //后一个quicklistNode\n    unsigned char *zl;              //quicklistNode指向的ziplist\n    unsigned int sz;                //ziplist的字节大小\n    unsigned int count : 16;        //ziplist中的元素个数\n    unsigned int encoding : 2;   //编码格式，原生字节数组或压缩存储\n    unsigned int container : 2;  //存储方式\n    unsigned int recompress : 1; //数据是否被压缩\n    unsigned int attempted_compress : 1; //数据能否被压缩\n    unsigned int extra : 10; //预留的bit位\n} quicklistNode;\n\n\n了解了 quicklistNode 的定义，我们再来看下 quicklist 的结构体定义。\n\nquicklist 作为一个链表结构，在它的数据结构中，是定义了整个 quicklist 的头、尾指针，这样一来，我们就可以通过 quicklist 的数据结构，来快速定位到 quicklist 的链表头和链表尾。\n\n此外，quicklist 中还定义了 quicklistNode 的个数、所有 ziplist 的总元素个数等属性。quicklist 的结构定义如下所示：\n\ntypedef struct quicklist {\n    quicklistNode *head;      //quicklist的链表头\n    quicklistNode *tail;      //quicklist的链表尾\n    unsigned long count;     //所有ziplist中的总元素个数\n    unsigned long len;       //quicklistNodes的个数\n    ...\n} quicklist;\n\n\n然后，从 quicklistNode 和 quicklist 的结构体定义中，我们就能画出下面这张 quicklist 的示意图。\n\n\n\n而也正因为 quicklist 采用了链表结构，所以当插入一个新的元素时，quicklist 首先就会检查插入位置的 ziplist 是否能容纳该元素，这是通过 _quicklistNodeAllowInsert 函数来完成判断的。\n\n_quicklistNodeAllowInsert 函数会计算新插入元素后的大小（new_sz），这个大小等于 quicklistNode 的当前大小（node->sz）、插入元素的大小（sz），以及插入元素后 ziplist 的 prevlen 占用大小。\n\n在计算完大小之后，_quicklistNodeAllowInsert 函数会依次判断新插入的数据大小（sz）是否满足要求，即单个 ziplist 是否不超过 8KB，或是单个 ziplist 里的元素个数是否满足要求。\n\n只要这里面的一个条件能满足，quicklist 就可以在当前的 quicklistNode 中插入新元素，否则 quicklist 就会新建一个 quicklistNode，以此来保存新插入的元素。\n\n下面代码显示了是否允许在当前 quicklistNode 插入数据的判断逻辑，你可以看下。\n\nunsigned int new_sz = node->sz + sz + ziplist_overhead;\nif (likely(_quicklistNodeSizeMeetsOptimizationRequirement(new_sz, fill)))\n    return 1;\nelse if (!sizeMeetsSafetyLimit(new_sz))\n    return 0;\nelse if ((int)node->count < fill)\n    return 1;\nelse\n    return 0;\n\n\n这样一来，quicklist 通过控制每个 quicklistNode 中，ziplist 的大小或是元素个数，就有效减少了在 ziplist 中新增或修改元素后，发生连锁更新的情况，从而提供了更好的访问性能。\n\n而 Redis 除了设计了 quicklist 结构来应对 ziplist 的问题以外，还在 5.0 版本中新增了 listpack 数据结构，用来彻底避免连锁更新。下面我们就继续来学习下它的设计实现思路。\n\n\n# listpack 设计与实现\n\nlistpack 也叫紧凑列表，它的特点就是用一块连续的内存空间来紧凑地保存数据，同时为了节省内存空间，listpack 列表项使用了多种编码方式，来表示不同长度的数据，这些数据包括整数和字符串。\n\n和 listpack 相关的实现文件是listpack.c，头文件包括listpack.h和listpack_malloc.h。我们先来看下 listpack 的创建函数 lpNew，因为从这个函数的代码逻辑中，我们可以了解到 listpack 的整体结构。\n\nlpNew 函数创建了一个空的 listpack，一开始分配的大小是 LP_HDR_SIZE 再加 1 个字节。LP_HDR_SIZE 宏定义是在 listpack.c 中，它默认是 6 个字节，其中 4 个字节是记录 listpack 的总字节数，2 个字节是记录 listpack 的元素数量。\n\n此外，listpack 的最后一个字节是用来标识 listpack 的结束，其默认值是宏定义 LP_EOF。和 ziplist 列表项的结束标记一样，LP_EOF 的值也是 255。\n\nunsigned char *lpNew(void) {\n    //分配LP_HRD_SIZE+1\n    unsigned char *lp = lp_malloc(LP_HDR_SIZE+1);\n    if (lp == NULL) return NULL;\n    //设置listpack的大小\n    lpSetTotalBytes(lp,LP_HDR_SIZE+1);\n    //设置listpack的元素个数，初始值为0\n    lpSetNumElements(lp,0);\n    //设置listpack的结尾标识为LP_EOF，值为255\n    lp[LP_HDR_SIZE] = LP_EOF;\n    return lp;\n}\n\n\n你可以看看下面这张图，展示的就是大小为 LP_HDR_SIZE 的 listpack 头和值为 255 的 listpack 尾。当有新元素插入时，该元素会被插在 listpack 头和尾之间。\n\n\n\n好了，了解了 listpack 的整体结构后，我们再来看下 listpack 列表项的设计。\n\n和 ziplist 列表项类似，listpack 列表项也包含了元数据信息和数据本身。不过，为了避免 ziplist 引起的连锁更新问题，listpack 中的每个列表项不再像 ziplist 列表项那样，保存其前一个列表项的长度，它只会包含三个方面内容，分别是当前元素的编码类型（entry-encoding）、元素数据 (entry-data)，以及编码类型和元素数据这两部分的长度 (entry-len)，如下图所示。\n\n\n\n这里，关于 listpack 列表项的设计，你需要重点掌握两方面的要点，分别是列表项元素的编码类型，以及列表项避免连锁更新的方法。下面我就带你具体了解下。\n\n\n# listpack 列表项编码方法\n\n我们先来看下 listpack 元素的编码类型。如果你看了 listpack.c 文件，你会发现该文件中有大量类似 LP_ENCODINGXX_BIT_INT 和 LP_ENCODINGXX_BIT_STR 的宏定义，如下所示：\n\n#define LP_ENCODING_7BIT_UINT 0\n#define LP_ENCODING_6BIT_STR 0x80\n#define LP_ENCODING_13BIT_INT 0xC0\n...\n#define LP_ENCODING_64BIT_INT 0xF4\n#define LP_ENCODING_32BIT_STR 0xF0\n\n\n这些宏定义其实就对应了 listpack 的元素编码类型。具体来说，listpack 元素会对不同长度的整数和字符串进行编码，这里我们分别来看下。\n\n首先，对于整数编码来说，当 listpack 元素的编码类型为 LP_ENCODING_7BIT_UINT 时，表示元素的实际数据是一个 7 bit 的无符号整数。又因为 LP_ENCODING_7BIT_UINT 本身的宏定义值为 0，所以编码类型的值也相应为 0，占 1 个 bit。\n\n此时，编码类型和元素实际数据共用 1 个字节，这个字节的最高位为 0，表示编码类型，后续的 7 位用来存储 7 bit 的无符号整数，如下图所示：\n\n\n\n而当编码类型为 LP_ENCODING_13BIT_INT 时，这表示元素的实际数据是 13 bit 的整数。同时，因为 LP_ENCODING_13BIT_INT 的宏定义值为 0xC0，转换为二进制值是 1100 0000，所以，这个二进制值中的后 5 位和后续的 1 个字节，共 13 位，会用来保存 13bit 的整数。而该二进制值中的前 3 位 110，则用来表示当前的编码类型。我画了下面这张图，你可以看下。\n\n\n\n好，在了解了 LP_ENCODING_7BIT_UINT 和 LP_ENCODING_13BIT_INT 这两种编码类型后，剩下的 LP_ENCODING_16BIT_INT、LP_ENCODING_24BIT_INT、LP_ENCODING_32BIT_INT 和 LP_ENCODING_64BIT_INT，你应该也就能知道它们的编码方式了。\n\n这四种类型是分别用 2 字节（16 bit）、3 字节（24 bit）、4 字节（32 bit）和 8 字节（64 bit）来保存整数数据。同时，它们的编码类型本身占 1 字节，编码类型值分别是它们的宏定义值。\n\n然后，对于字符串编码来说，一共有三种类型，分别是 LP_ENCODING_6BIT_STR、LP_ENCODING_12BIT_STR 和 LP_ENCODING_32BIT_STR。从刚才的介绍中，你可以看到，整数编码类型名称中 BIT 前面的数字，表示的是整数的长度。因此类似的，字符串编码类型名称中 BIT 前的数字，表示的就是字符串的长度。\n\n比如，当编码类型为 LP_ENCODING_6BIT_STR 时，编码类型占 1 字节。该类型的宏定义值是 0x80，对应的二进制值是 1000 0000，这其中的前 2 位是用来标识编码类型本身，而后 6 位保存的是字符串长度。然后，列表项中的数据部分保存了实际的字符串。\n\n下面的图展示了三种字符串编码类型和数据的布局，你可以看下。\n\n\n\n\n# listpack 避免连锁更新的实现方式\n\n最后，我们再来了解下 listpack 列表项是如何避免连锁更新的。\n\n在 listpack 中，因为每个列表项只记录自己的长度，而不会像 ziplist 中的列表项那样，会记录前一项的长度。所以，当我们在 listpack 中新增或修改元素时，实际上只会涉及每个列表项自己的操作，而不会影响后续列表项的长度变化，这就避免了连锁更新。\n\n不过，你可能会有疑问：如果 listpack 列表项只记录当前项的长度，那么 listpack 支持从左向右正向查询列表，或是从右向左反向查询列表吗？\n\n其实，listpack 是能支持正、反向查询列表的。\n\n当应用程序从左向右正向查询 listpack 时，我们可以先调用 lpFirst 函数。该函数的参数是指向 listpack 头的指针，它在执行时，会让指针向右偏移 LP_HDR_SIZE 大小，也就是跳过 listpack 头。你可以看下 lpFirst 函数的代码，如下所示：\n\nunsigned char *lpFirst(unsigned char *lp) {\n    lp += LP_HDR_SIZE; //跳过listpack头部6个字节\n    if (lp[0] == LP_EOF) return NULL;  //如果已经是listpack的末尾结束字节，则返回NULL\n    return lp;\n}\n\n\n然后，再调用 lpNext 函数，该函数的参数包括了指向 listpack 某个列表项的指针。lpNext 函数会进一步调用 lpSkip 函数，并传入当前列表项的指针，如下所示：\n\nunsigned char *lpNext(unsigned char *lp, unsigned char *p) {\n    ...\n    p = lpSkip(p);  //调用lpSkip函数，偏移指针指向下一个列表项\n    if (p[0] == LP_EOF) return NULL;\n    return p;\n}\n\n\n最后，lpSkip 函数会先后调用 lpCurrentEncodedSize 和 lpEncodeBacklen 这两个函数。\n\nlpCurrentEncodedSize 函数是根据当前列表项第 1 个字节的取值，来计算当前项的编码类型，并根据编码类型，计算当前项编码类型和实际数据的总长度。然后，lpEncodeBacklen 函数会根据编码类型和实际数据的长度之和，进一步计算列表项最后一部分 entry-len 本身的长度。\n\n这样一来，lpSkip 函数就知道当前项的编码类型、实际数据和 entry-len 的总长度了，也就可以将当前项指针向右偏移相应的长度，从而实现查到下一个列表项的目的。\n\n下面代码展示了 lpEncodeBacklen 函数的基本计算逻辑，你可以看下。\n\nunsigned long lpEncodeBacklen(unsigned char *buf, uint64_t l) {\n    //编码类型和实际数据的总长度小于等于127，entry-len长度为1字节\n    if (l <= 127) {\n        ...\n        return 1;\n    } else if (l < 16383) { //编码类型和实际数据的总长度大于127但小于16383，entry-len长度为2字节\n       ...\n        return 2;\n    } else if (l < 2097151) {//编码类型和实际数据的总长度大于16383但小于2097151，entry-len长度为3字节\n       ...\n        return 3;\n    } else if (l < 268435455) { //编码类型和实际数据的总长度大于2097151但小于268435455，entry-len长度为4字节\n        ...\n        return 4;\n    } else { //否则，entry-len长度为5字节\n       ...\n        return 5;\n    }\n}\n\n\n我也画了一张图，展示了从左向右遍历 listpack 的基本过程，你可以再回顾下。\n\n\n\n好，了解了从左向右正向查询 listpack，我们再来看下从右向左反向查询 listpack。\n\n首先，我们根据 listpack 头中记录的 listpack 总长度，就可以直接定位到 listapck 的尾部结束标记。然后，我们可以调用 lpPrev 函数，该函数的参数包括指向某个列表项的指针，并返回指向当前列表项前一项的指针。\n\nlpPrev 函数中的关键一步就是调用 lpDecodeBacklen 函数。lpDecodeBacklen 函数会从右向左，逐个字节地读取当前列表项的 entry-len。\n\n那么，lpDecodeBacklen 函数如何判断 entry-len 是否结束了呢？\n\n这就依赖于 entry-len 的编码方式了。entry-len 每个字节的最高位，是用来表示当前字节是否为 entry-len 的最后一个字节，这里存在两种情况，分别是：\n\n * 最高位为 1，表示 entry-len 还没有结束，当前字节的左边字节仍然表示 entry-len 的内容；\n * 最高位为 0，表示当前字节已经是 entry-len 最后一个字节了。\n\n而 entry-len 每个字节的低 7 位，则记录了实际的长度信息。这里你需要注意的是，entry-len 每个字节的低 7 位采用了大端模式存储，也就是说，entry-len 的低位字节保存在内存高地址上。\n\n我画了下面这张图，展示了 entry-len 这种特别的编码方式，你可以看下。\n\n\n\n实际上，正是因为有了 entry-len 的特别编码方式，lpDecodeBacklen 函数就可以从当前列表项起始位置的指针开始，向左逐个字节解析，得到前一项的 entry-len 值。这也是 lpDecodeBacklen 函数的返回值。而从刚才的介绍中，我们知道 entry-len 记录了编码类型和实际数据的长度之和。\n\n因此，lpPrev 函数会再调用 lpEncodeBacklen 函数，来计算得到 entry-len 本身长度，这样一来，我们就可以得到前一项的总长度，而 lpPrev 函数也就可以将指针指向前一项的起始位置了。所以按照这个方法，listpack 就实现了从右向左的查询功能。\n\n\n# 对比\n\n特性       ZIPLIST                QUICKLIST   LISTPACK\n设计复杂度    较为复杂，包含前一个节点长度字段       复杂          更加简化，没有前一个节点长度字段\n内存占用     存在冗余字段，内存利用率较低         高           更加紧凑，内存占用低\n操作复杂度    插入、删除操作需要更新前向节点长度，较慢   中           操作简单高效，无需处理前向节点长度，但是也要移动\n内存移动问题   频繁插入删除可能导致大范围内存移动      中           仍存在内存移动问题，但操作更加简单\n适用场景     小 hash，小 zset          list        小 hash，小 zset，小 stream\n\n\n\n笔记\n\necho 认为，ziplist 和 listpack 适用于元素较少时的存储，一旦元素变多就需采用 quicklist 这种类似于 linkedlist 的结构来进行存储，但是 quicklist 的 node 有两种选择，分别是 ziplist 和 listpack，在最新的版本中貌似都是采用 listpack 来实现的\n\n\n# 总结\n\n本文从 ziplist 的设计不足出发，到学习 quicklist 和 listpack 的设计思想\n\n你要知道，ziplist 的不足主要在于一旦 ziplist 中元素个数多了，它的查找效率就会降低。而且如果在 ziplist 里新增或修改数据，ziplist 占用的内存空间还需要重新分配；更糟糕的是，ziplist 新增某个元素或修改某个元素时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起连锁更新问题，导致每个元素的空间都要重新分配，这就会导致 ziplist 的访问性能下降。\n\n所以，为了应对 ziplist 的问题，Redis 先是在 3.0 版本中设计实现了 quicklist。quicklist 结构在 ziplist 基础上，使用链表将 ziplist 串联起来，链表的每个元素就是一个 ziplist。这种设计减少了数据插入时内存空间的重新分配，以及内存数据的拷贝。同时，quicklist 限制了每个节点上 ziplist 的大小，一旦一个 ziplist 过大，就会采用新增 quicklist 节点的方法。\n\n不过，又因为 quicklist 使用 quicklistNode 结构指向每个 ziplist，无疑增加了内存开销。为了减少内存开销，并进一步避免 ziplist 连锁更新问题，Redis 在 5.0 版本中，就设计实现了 listpack 结构。listpack 结构沿用了 ziplist 紧凑型的内存布局，把每个元素都紧挨着放置\n\nlistpack 中每个列表项不再包含前一项的长度了，因此当某个列表项中的数据发生变化，导致列表项长度变化时，其他列表项的长度是不会受影响的，因而这就避免了 ziplist 面临的连锁更新问题。\n\n总而言之，Redis 在内存紧凑型列表的设计与实现上，从 ziplist 到 quicklist，再到 listpack，你可以看到 Redis 在内存空间开销和访问性能之间的设计取舍，这一系列的设计变化，是非常值得你学习的\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 为什么 redis 设计了三种类似但不同的底层数据结构 ziplist、quicklist、listpack？\n 2. ziplist 为什么会面临查找效率降低和内存连锁更新的问题？\n 3. quicklist 是如何解决 ziplist 的性能瓶颈的？\n 4. listpack 是如何避免 ziplist 的连锁更新问题的？\n 5. 如何在实际开发中选择适合的 redis list 底层结构，以平衡内存使用和性能？\n 6. ziplist 在 redis 设计中有哪些优点被保留了下来，即使它存在性能不足？\n 7. 当数据插入量大时，quicklist 如何避免内存频繁重新分配而保持高效？\n\n\n# 前言\n\nredis 中的 list 的底层有三种数据结构\n\n * ziplist\n * quicklist\n * listpack\n\nziplist 的最大特点，就是它被设计成一种 内存紧凑型 的数据结构，占用一块连续的内存空间，以达到节省内存的目的\n\n但是，在计算机系统中，任何一个设计都是有利有弊的。对于 ziplist 来说，这个道理同样成立。\n\n虽然 ziplist 节省了内存开销，可它也存在两个设计代价\n\n * 「不能保存过多的元素」，否则访问性能会降低\n * 「不能保存过大的元素」，否则容易导致内存重新分配，甚至可能引发连锁更新的问题。所谓的连锁更新，简单来说，就是 ziplist 中的每一项都要被重新分配内存空间，造成 ziplist 的性能降低\n\n因此，针对 ziplist 在设计上的不足，redis 在演进的过程中，新增设计了两种数据结构\n\n * quicklist\n * istpack\n\n它们设计目标，是 尽可能地保持 ziplist 节省内存的优势，同时避免 ziplist 潜在的性能下降问题\n\n\n# ziplist 设计与实现\n\n\n# ziplist 和 整数集合 的设计\n\n首先你要知道，list、hash 和 sorted set 这三种数据类型，都可以使用压缩列表（ziplist）来保存数据。压缩列表的函数定义和实现代码分别在 ziplist.h 和 ziplist.c 中\n\n不过，我们在 ziplist.h 文件中其实根本看不到压缩列表的结构体定义。这是因为压缩列表本身就是一块连续的内存空间，它通过使用不同的编码来保存数据\n\n这里为了方便理解压缩列表的设计与实现，我们先来看看它的创建函数 ziplistnew，如下所示：\n\nunsigned char *ziplistnew(void) {\n    //初始分配的大小\n    unsigned int bytes = ziplist_header_size+ziplist_end_size;\n    unsigned char *zl = zmalloc(bytes);\n    …\n   //将列表尾设置为zip_end\n    zl[bytes-1] = zip_end;\n    return zl;\n}\n\n\n实际上，ziplistnew 函数的逻辑很简单，就是创建一块连续的内存空间，大小为 ziplist_header_size 和 ziplist_end_size 的总和，然后再把该连续空间的最后一个字节赋值为 zip_end，表示列表结束。\n\n这三个宏分别表示 ziplist 的列表头大小、列表尾大小和列表尾字节内容\n\n//ziplist的列表头大小，包括2个32 bits整数和1个16bits整数，分别表示压缩列表的总字节数，列表最后一个元素的离列表头的偏移，以及列表中的元素个数\n#define ziplist_header_size     (sizeof(uint32_t)*2+sizeof(uint16_t))\n//ziplist的列表尾大小，包括1个8 bits整数，表示列表结束。\n#define ziplist_end_size        (sizeof(uint8_t))\n//ziplist的列表尾字节内容\n#define zip_end 255\n\n\n那么，在创建一个新的 ziplist 后，该列表的内存布局就如下图所示。注意，此时列表中还没有实际的数据。\n\n\n\n然后，当我们往 ziplist 中插入数据时，ziplist 就会根据数据是字符串还是整数，以及它们的大小进行不同的编码。这种根据数据大小进行相应编码的设计思想，正是 redis 为了节省内存而采用的\n\nziplist 列表项包括三部分内容，分别是前一项的长度（prevlen）、当前项长度信息的编码结果（encoding），以及当前项的实际数据（data）。下面的图展示了列表项的结构（图中除列表项之外的内容分别是 ziplist 内存空间的起始和尾部）。\n\n\n\n实际上，所谓的编码技术，就是指 用不同数量的字节来表示保存的信息。在 ziplist 中，编码技术主要应用在列表项中的 prevlen 和 encoding 这两个元数据上。而当前项的实际数据 data，则正常用整数或是字符串来表示。\n\n所以这里，我们就先来看下 prevlen 的编码设计。ziplist 中会包含多个列表项，每个列表项都是紧挨着彼此存放的，如下图所示。\n\n\n\n而为了方便查找，每个列表项中都会记录前一项的长度。因为每个列表项的长度不一样，所以如果使用相同的字节大小来记录 prevlen，就会造成内存空间浪费。\n\n我给你举个例子，假设我们统一使用 4 字节记录 prevlen，如果前一个列表项只是一个字符串“redis”，长度为 5 个字节，那么我们用 1 个字节（8 bits）就能表示 256 字节长度（2 的 8 次方等于 256）的字符串了。此时，prevlen 用 4 字节记录，其中就有 3 字节是浪费掉了。\n\n好，我们再回过头来看，ziplist 在对 prevlen 编码时，会先调用 zipstorepreventrylength 函数，用于判断前一个列表项是否小于 254 字节。如果是的话，那么 prevlen 就使用 1 字节表示；否则，zipstorepreventrylength 函数就调用 zipstorepreventrylengthlarge 函数进一步编码。这部分代码如下所示：\n\n//判断prevlen的长度是否小于zip_big_prevlen，zip_big_prevlen等于254\nif (len < zip_big_prevlen) {\n   //如果小于254字节，那么返回prevlen为1字节\n   p[0] = len;\n   return 1;\n} else {\n   //否则，调用zipstorepreventrylengthlarge进行编码\n   return zipstorepreventrylengthlarge(p,len);\n}\n\n\n也就是说，zipstorepreventrylengthlarge 函数会先将 prevlen 的第 1 字节设置为 254，然后使用内存拷贝函数 memcpy，将前一个列表项的长度值拷贝至 prevlen 的第 2 至第 5 字节。最后，zipstorepreventrylengthlarge 函数返回 prevlen 的大小，为 5 字节。\n\nif (p != null) {\n    //将prevlen的第1字节设置为zip_big_prevlen，即254\n    p[0] = zip_big_prevlen;\n  //将前一个列表项的长度值拷贝至prevlen的第2至第5字节，其中sizeof(len)的值为4\n    memcpy(p+1,&len,sizeof(len));\n    …\n}\n//返回prevlen的大小，为5字节\nreturn 1+sizeof(len);\n\n\n好，在了解了 prevlen 使用 1 字节和 5 字节两种编码方式后，我们再来学习下 encoding 的编码方法。\n\n我们知道，一个列表项的实际数据，既可以是整数也可以是字符串。整数可以是 16、32、64 等字节长度，同时字符串的长度也可以大小不一。\n\n所以，ziplist 在 zipstoreentryencoding 函数中，针对整数和字符串，就分别使用了不同字节长度的编码结果。下面的代码展示了 zipstoreentryencoding 函数的部分代码，你可以看到当数据是不同长度字符串或是整数时，编码结果的长度 len 大小不同。\n\n//默认编码结果是1字节\nunsigned char len = 1;\n//如果是字符串数据\nif (zip_is_str(encoding)) {\n    //字符串长度小于等于63字节（16进制为0x3f）\n    if (rawlen <= 0x3f) {\n        //默认编码结果是1字节\n        …\n    }\n    //字符串长度小于等于16383字节（16进制为0x3fff）\n    else if (rawlen <= 0x3fff) {\n        //编码结果是2字节\n        len += 1;\n        …\n    }\n    //字符串长度大于16383字节\n\n    else {\n        //编码结果是5字节\n        len += 4;\n        …\n    }\n} else {\n    /* 如果数据是整数，编码结果是1字节*/\n    if (!p) return len;\n    ...\n}\n\n\n简而言之，针对不同长度的数据，使用不同大小的元数据信息（prevlen 和 encoding），这种方法可以有效地节省内存开销\n\n除了 ziplist 之外，redis 还设计了一个内存友好的数据结构，这就是整数集合（intset），它是作为底层结构来实现 set 数据类型的。\n\n和 sds 嵌入式字符串、ziplist 类似，整数集合也是一块连续的内存空间，这一点我们从整数集合的定义中就可以看到。intset.h 和 intset.c 分别包括了整数集合的定义和实现\n\n下面的代码展示了 intset 的结构定义。我们可以看到，整数集合结构体中记录数据的部分，就是一个 int8_t 类型的整数数组 contents。从内存使用的角度来看，整数数组就是一块连续内存空间，所以这样就避免了内存碎片，并提升了内存使用效率\n\ntypedef struct intset {\n    uint32_t encoding;\n    uint32_t length;\n    int8_t contents[];\n} intset;\n\n\n\n# 扩展：节省内存的数据访问\n\n我们知道，在 redis 实例运行时，有些数据是会被经常访问的，比如常见的整数，redis 协议中常见的回复信息，包括操作成功（“ok”字符串）、操作失败（err），以及常见的报错信息。\n\n所以，为了避免在内存中反复创建这些经常被访问的数据，redis 就采用了共享对象的设计思想。这个设计思想很简单，就是把这些常用数据创建为共享对象，当上层应用需要访问它们时，直接读取就行。\n\n现在我们就来做个假设。有 1000 个客户端，都要保存“3”这个整数。如果 redis 为每个客户端，都创建了一个值为 3 的 redisobject，那么内存中就会有大量的冗余。而使用了共享对象方法后，redis 在内存中只用保存一个 3 的 redisobject 就行，这样就有效节省了内存空间。\n\n以下代码展示的是 server.c 文件中，创建共享对象的函数 createsharedobjects，你可以看下。\n\nvoid createsharedobjects(void) {\n   …\n   //常见回复信息\n   shared.ok = createobject(obj_string,sdsnew("+ok\\r\\n"));\n   shared.err = createobject(obj_string,sdsnew("-err\\r\\n"));\n   …\n   //常见报错信息\n shared.nokeyerr = createobject(obj_string,sdsnew("-err no such key\\r\\n"));\n shared.syntaxerr = createobject(obj_string,sdsnew("-err syntax error\\r\\n"));\n   //0到9999的整数\n   for (j = 0; j < obj_shared_integers; j++) {\n        shared.integers[j] =\n          makeobjectshared(createobject(obj_string,(void*)(long)j));\n        …\n    }\n   …\n}\n\n\n\n# ziplist 的不足\n\n你已经知道，一个 ziplist 数据结构在内存中的布局，就是一块连续的内存空间。这块空间的起始部分是大小固定的 10 字节元数据，其中记录了 ziplist 的总字节数、最后一个元素的偏移量以及列表元素的数量，而这 10 字节后面的内存空间则保存了实际的列表数据。在 ziplist 的最后部分，是一个 1 字节的标识（固定为 255），用来表示 ziplist 的结束，如下图所示：\n\n\n\n不过，虽然 ziplist 通过紧凑的内存布局来保存数据，节省了内存空间，但是 ziplist 也面临着随之而来的两个不足：\n\n * 查找复杂度高\n * 潜在的连锁更新风险\n\n那么下面，我们就分别来了解下这两个问题\n\n# 查找复杂度高\n\n因为 ziplist 头尾元数据的大小是固定的，所以可以很快找到 首部元素和尾部元素，但问题是\n\n * 当要查找中间元素时，ziplist 就得从列表头或列表尾遍历才行\n * 更糟糕的是，如果 ziplist 里面保存的是字符串，ziplist 在查找某个元素时，还需要逐一判断元素的每个字符，这样又进一步增加了复杂度\n * ziplist 在插入元素时，如果内存空间不够了，ziplist 还需要重新分配一块连续的内存空间，而这还会进一步引发连锁更新的问题\n\n也正因为如此，我们在使用 ziplist 保存 hash 或 sorted set 数据时，都会在 redis.conf 文件中，通过 hash-max-ziplist-entries 和 zset-max-ziplist-entries 两个参数，来控制保存在 ziplist 中的元素个数\n\n# 连锁更新风险\n\n我们知道，因为 ziplist 必须使用一块连续的内存空间来保存数据，所以当新插入一个元素时，ziplist 就需要计算其所需的空间大小，并申请相应的内存空间。这一系列操作，我们可以从 ziplist 的元素插入函数 __ziplistinsert 中看到。\n\n__ziplistinsert 函数首先会计算获得当前 ziplist 的长度，这个步骤通过 ziplist_bytes 宏定义就可以完成，如下所示。同时，该函数还声明了 reqlen 变量，用于记录插入元素后所需的新增空间大小。\n\n//获取当前ziplist长度curlen；声明reqlen变量，用来记录新插入元素所需的长度\nsize_t curlen = intrev32ifbe(ziplist_bytes(zl)), reqlen;\n\n\n然后，__ziplistinsert 函数会判断当前要插入的位置是否是列表末尾。如果不是末尾，那么就需要获取位于当前插入位置的元素的 prevlen 和 prevlensize。这部分代码如下所示：\n\n//如果插入的位置不是ziplist末尾，则获取前一项长度\nif (p[0] != zip_end) {\n    zip_decode_prevlen(p, prevlensize, prevlen);\n} else {\n    …\n}\n\n\n实际上，在 ziplist 中，每一个元素都会记录其前一项的长度，也就是 prevlen。然后，为了节省内存开销，ziplist 会使用不同的空间记录 prevlen，这个 prevlen 空间大小就是 prevlensize。\n\n举个简单的例子，当在一个元素 a 前插入一个新的元素 b 时，a 的 prevlen 和 prevlensize 都要根据 b 的长度进行相应的变化。\n\n那么现在，我们假设 a 的 prevlen 原本只占用 1 字节（也就是 prevlensize 等于 1），而能记录的前一项长度最大为 253 字节。此时，如果 b 的长度超过了 253 字节，a 的 prevlen 就需要使用 5 个字节来记录（prevlen 具体的编码方式，你可以复习回顾下第 4 讲），这样就需要申请额外的 4 字节空间了。不过，如果元素 b 的插入位置是列表末尾，那么插入元素 b 时，我们就不用考虑后面元素的 prevlen 了\n\n\n\n因此，为了保证 ziplist 有足够的内存空间，来保存插入元素以及插入位置元素的 prevlen 信息，__ziplistinsert 函数在获得插入位置元素的 prevlen 和 prevlensize 后，紧接着就会计算插入元素的长度。\n\n现在我们已知，一个 ziplist 元素包括了 prevlen、encoding 和实际数据 data 三个部分。所以，在计算插入元素的所需空间时，__ziplistinsert 函数也会分别计算这三个部分的长度。这个计算过程一共可以分成四步来完成。\n\n * 第一步，计算实际插入元素的长度。\n\n首先你要知道，这个计算过程和插入元素是整数还是字符串有关。__ziplistinsert 函数会先调用 ziptryencoding 函数，这个函数会判断插入元素是否为整数。如果是整数，就按照不同的整数大小，计算 encoding 和实际数据 data 各自所需的空间；如果是字符串，那么就先把字符串长度记录为所需的新增空间大小。这一过程的代码如下所示：\n\n  if (ziptryencoding(s,slen,&value,&encoding)) {\n          reqlen = zipintsize(encoding);\n      } else {\n          reqlen = slen;\n      }\n\n\n * 第二步，调用 zipstorepreventrylength 函数，将插入位置元素的 prevlen 也计算到所需空间中。\n\n这是因为在插入元素后，__ziplistinsert 函数可能要为插入位置的元素分配新增空间。这部分代码如下所示：\n\nreqlen += zipstorepreventrylength(null,prevlen);\n\n\n * 第三步，调用 zipstoreentryencoding 函数，根据字符串的长度，计算相应 encoding 的大小。\n\n在刚才的第一步中，**ziplistinsert 函数对于字符串数据，只是记录了字符串本身的长度，所以在第三步中，**ziplistinsert 函数还会调用 zipstoreentryencoding 函数，根据字符串的长度来计算相应的 encoding 大小，如下所示：\n\nreqlen += zipstoreentryencoding(null,encoding,slen);\n\n\n好了，到这里，__ziplistinsert 函数就已经在 reqlen 变量中，记录了插入元素的 prevlen 长度、encoding 大小，以及实际数据 data 的长度。这样一来，插入元素的整体长度就有了，这也是插入位置元素的 prevlen 所要记录的大小。\n\n * 第四步，调用 zipprevlenbytediff 函数，判断插入位置元素的 prevlen 和实际所需的 prevlen 大小。\n\n最后，__ziplistinsert 函数会调用 zipprevlenbytediff 函数，用来判断插入位置元素的 prevlen 和实际所需的 prevlen，这两者间的大小差别。这部分代码如下所示，prevlen 的大小差别是使用 nextdiff 来记录的：\n\nnextdiff = (p[0] != zip_end) ? zipprevlenbytediff(p,reqlen) : 0;\n\n\n那么在这里，如果 nextdiff 大于 0，就表明插入位置元素的空间不够，需要新增 nextdiff 大小的空间，以便能保存新的 prevlen。然后，__ziplistinsert 函数在新增空间时，就会调用 ziplistresize 函数，来重新分配 ziplist 所需的空间。\n\nziplistresize 函数接收的参数分别是待重新分配的 ziplist 和重新分配的空间大小。而 __ziplistinsert 函数传入的重新分配大小的参数，是三个长度之和。\n\n那么是哪三个长度之和呢？\n\n这三个长度分别是 ziplist 现有大小（curlen）、待插入元素自身所需的新增空间（reqlen），以及插入位置元素 prevlen 所需的新增空间（nextdiff）。下面的代码显示了 ziplistresize 函数的调用和参数传递逻辑：\n\nzl = ziplistresize(zl,curlen+reqlen+nextdiff);\n\n\n进一步，那么 ziplistresize 函数在获得三个长度总和之后，具体是如何扩容呢？\n\n我们可以进一步看下 ziplistresize 函数的实现，这个函数会调用 zrealloc 函数，来完成空间的重新分配，而重新分配的空间大小就是由传入参数 len 决定的。这样，我们就了解到了 ziplistresize 函数涉及到内存分配操作，因此如果我们往 ziplist 频繁插入过多数据的话，就可能引起多次内存分配，从而会对 redis 性能造成影响。\n\n下面的代码显示了 ziplistresize 函数的部分实现，你可以看下。\n\nunsigned char *ziplistresize(unsigned char *zl, unsigned int len) {\n    //对zl进行重新内存空间分配，重新分配的大小是len\n    zl = zrealloc(zl,len);\n    …\n    zl[len-1] = zip_end;\n    return zl;\n}\n\n\n好了，到这里，我们就了解了 ziplist 在新插入元素时，会计算其所需的新增空间，并进行重新分配。而当新插入的元素较大时，就会引起插入位置的元素 prevlensize 增加，进而就会导致插入位置的元素所占空间也增加。\n\n而如此一来，这种空间新增就会引起连锁更新的问题。\n\n实际上，所谓的连锁更新，就是指当一个元素插入后，会引起当前位置元素新增 prevlensize 的空间。而当前位置元素的空间增加后，又会进一步引起该元素的后续元素，其 prevlensize 所需空间的增加。\n\n这样，一旦插入位置后续的所有元素，都会因为前序元素的 prevlenszie 增加，而引起自身空间也要增加，这种每个元素的空间都需要增加的现象，就是连锁更新。我画了下面这张图，你可以看下。\n\n\n\n连锁更新一旦发生，就会导致 ziplist 占用的内存空间要多次重新分配，这就会直接影响到 ziplist 的访问性能。\n\n所以说，虽然 ziplist 紧凑型的内存布局能节省内存开销，但是如果保存的元素数量增加了，或是元素变大了，ziplist 就会面临性能问题。那么，有没有什么方法可以避免 ziplist 的问题呢\n\n这就是接下来我要给你介绍的 quicklist 和 listpack，这两种数据结构的设计思想了\n\n\n# quicklist 设计与实现\n\n我们先来学习下 quicklist 的实现思路。\n\nquicklist 的设计，其实是结合了链表和 ziplist 各自的优势。简单来说，一个 quicklist 就是一个链表，而链表中的每个元素又是一个 ziplist\n\n我们来看下 quicklist 的数据结构，这是在quicklist.h文件中定义的，而 quicklist 的具体实现是在quicklist.c文件中。\n\n首先，quicklist 元素的定义，也就是 quicklistnode。因为 quicklist 是一个链表，所以每个 quicklistnode 中，都包含了分别指向它前序和后序节点的指针prev 和 next**。同时，每个 quicklistnode 又是一个 ziplist，所以，在 quicklistnode 的结构体中，还有指向 ziplist 的**指针 zl。\n\n此外，quicklistnode 结构体中还定义了一些属性，比如 ziplist 的字节大小、包含的元素个数、编码格式、存储方式等。下面的代码显示了 quicklistnode 的结构体定义，你可以看下。\n\ntypedef struct quicklistnode {\n    struct quicklistnode *prev;     //前一个quicklistnode\n    struct quicklistnode *next;     //后一个quicklistnode\n    unsigned char *zl;              //quicklistnode指向的ziplist\n    unsigned int sz;                //ziplist的字节大小\n    unsigned int count : 16;        //ziplist中的元素个数\n    unsigned int encoding : 2;   //编码格式，原生字节数组或压缩存储\n    unsigned int container : 2;  //存储方式\n    unsigned int recompress : 1; //数据是否被压缩\n    unsigned int attempted_compress : 1; //数据能否被压缩\n    unsigned int extra : 10; //预留的bit位\n} quicklistnode;\n\n\n了解了 quicklistnode 的定义，我们再来看下 quicklist 的结构体定义。\n\nquicklist 作为一个链表结构，在它的数据结构中，是定义了整个 quicklist 的头、尾指针，这样一来，我们就可以通过 quicklist 的数据结构，来快速定位到 quicklist 的链表头和链表尾。\n\n此外，quicklist 中还定义了 quicklistnode 的个数、所有 ziplist 的总元素个数等属性。quicklist 的结构定义如下所示：\n\ntypedef struct quicklist {\n    quicklistnode *head;      //quicklist的链表头\n    quicklistnode *tail;      //quicklist的链表尾\n    unsigned long count;     //所有ziplist中的总元素个数\n    unsigned long len;       //quicklistnodes的个数\n    ...\n} quicklist;\n\n\n然后，从 quicklistnode 和 quicklist 的结构体定义中，我们就能画出下面这张 quicklist 的示意图。\n\n\n\n而也正因为 quicklist 采用了链表结构，所以当插入一个新的元素时，quicklist 首先就会检查插入位置的 ziplist 是否能容纳该元素，这是通过 _quicklistnodeallowinsert 函数来完成判断的。\n\n_quicklistnodeallowinsert 函数会计算新插入元素后的大小（new_sz），这个大小等于 quicklistnode 的当前大小（node->sz）、插入元素的大小（sz），以及插入元素后 ziplist 的 prevlen 占用大小。\n\n在计算完大小之后，_quicklistnodeallowinsert 函数会依次判断新插入的数据大小（sz）是否满足要求，即单个 ziplist 是否不超过 8kb，或是单个 ziplist 里的元素个数是否满足要求。\n\n只要这里面的一个条件能满足，quicklist 就可以在当前的 quicklistnode 中插入新元素，否则 quicklist 就会新建一个 quicklistnode，以此来保存新插入的元素。\n\n下面代码显示了是否允许在当前 quicklistnode 插入数据的判断逻辑，你可以看下。\n\nunsigned int new_sz = node->sz + sz + ziplist_overhead;\nif (likely(_quicklistnodesizemeetsoptimizationrequirement(new_sz, fill)))\n    return 1;\nelse if (!sizemeetssafetylimit(new_sz))\n    return 0;\nelse if ((int)node->count < fill)\n    return 1;\nelse\n    return 0;\n\n\n这样一来，quicklist 通过控制每个 quicklistnode 中，ziplist 的大小或是元素个数，就有效减少了在 ziplist 中新增或修改元素后，发生连锁更新的情况，从而提供了更好的访问性能。\n\n而 redis 除了设计了 quicklist 结构来应对 ziplist 的问题以外，还在 5.0 版本中新增了 listpack 数据结构，用来彻底避免连锁更新。下面我们就继续来学习下它的设计实现思路。\n\n\n# listpack 设计与实现\n\nlistpack 也叫紧凑列表，它的特点就是用一块连续的内存空间来紧凑地保存数据，同时为了节省内存空间，listpack 列表项使用了多种编码方式，来表示不同长度的数据，这些数据包括整数和字符串。\n\n和 listpack 相关的实现文件是listpack.c，头文件包括listpack.h和listpack_malloc.h。我们先来看下 listpack 的创建函数 lpnew，因为从这个函数的代码逻辑中，我们可以了解到 listpack 的整体结构。\n\nlpnew 函数创建了一个空的 listpack，一开始分配的大小是 lp_hdr_size 再加 1 个字节。lp_hdr_size 宏定义是在 listpack.c 中，它默认是 6 个字节，其中 4 个字节是记录 listpack 的总字节数，2 个字节是记录 listpack 的元素数量。\n\n此外，listpack 的最后一个字节是用来标识 listpack 的结束，其默认值是宏定义 lp_eof。和 ziplist 列表项的结束标记一样，lp_eof 的值也是 255。\n\nunsigned char *lpnew(void) {\n    //分配lp_hrd_size+1\n    unsigned char *lp = lp_malloc(lp_hdr_size+1);\n    if (lp == null) return null;\n    //设置listpack的大小\n    lpsettotalbytes(lp,lp_hdr_size+1);\n    //设置listpack的元素个数，初始值为0\n    lpsetnumelements(lp,0);\n    //设置listpack的结尾标识为lp_eof，值为255\n    lp[lp_hdr_size] = lp_eof;\n    return lp;\n}\n\n\n你可以看看下面这张图，展示的就是大小为 lp_hdr_size 的 listpack 头和值为 255 的 listpack 尾。当有新元素插入时，该元素会被插在 listpack 头和尾之间。\n\n\n\n好了，了解了 listpack 的整体结构后，我们再来看下 listpack 列表项的设计。\n\n和 ziplist 列表项类似，listpack 列表项也包含了元数据信息和数据本身。不过，为了避免 ziplist 引起的连锁更新问题，listpack 中的每个列表项不再像 ziplist 列表项那样，保存其前一个列表项的长度，它只会包含三个方面内容，分别是当前元素的编码类型（entry-encoding）、元素数据 (entry-data)，以及编码类型和元素数据这两部分的长度 (entry-len)，如下图所示。\n\n\n\n这里，关于 listpack 列表项的设计，你需要重点掌握两方面的要点，分别是列表项元素的编码类型，以及列表项避免连锁更新的方法。下面我就带你具体了解下。\n\n\n# listpack 列表项编码方法\n\n我们先来看下 listpack 元素的编码类型。如果你看了 listpack.c 文件，你会发现该文件中有大量类似 lp_encodingxx_bit_int 和 lp_encodingxx_bit_str 的宏定义，如下所示：\n\n#define lp_encoding_7bit_uint 0\n#define lp_encoding_6bit_str 0x80\n#define lp_encoding_13bit_int 0xc0\n...\n#define lp_encoding_64bit_int 0xf4\n#define lp_encoding_32bit_str 0xf0\n\n\n这些宏定义其实就对应了 listpack 的元素编码类型。具体来说，listpack 元素会对不同长度的整数和字符串进行编码，这里我们分别来看下。\n\n首先，对于整数编码来说，当 listpack 元素的编码类型为 lp_encoding_7bit_uint 时，表示元素的实际数据是一个 7 bit 的无符号整数。又因为 lp_encoding_7bit_uint 本身的宏定义值为 0，所以编码类型的值也相应为 0，占 1 个 bit。\n\n此时，编码类型和元素实际数据共用 1 个字节，这个字节的最高位为 0，表示编码类型，后续的 7 位用来存储 7 bit 的无符号整数，如下图所示：\n\n\n\n而当编码类型为 lp_encoding_13bit_int 时，这表示元素的实际数据是 13 bit 的整数。同时，因为 lp_encoding_13bit_int 的宏定义值为 0xc0，转换为二进制值是 1100 0000，所以，这个二进制值中的后 5 位和后续的 1 个字节，共 13 位，会用来保存 13bit 的整数。而该二进制值中的前 3 位 110，则用来表示当前的编码类型。我画了下面这张图，你可以看下。\n\n\n\n好，在了解了 lp_encoding_7bit_uint 和 lp_encoding_13bit_int 这两种编码类型后，剩下的 lp_encoding_16bit_int、lp_encoding_24bit_int、lp_encoding_32bit_int 和 lp_encoding_64bit_int，你应该也就能知道它们的编码方式了。\n\n这四种类型是分别用 2 字节（16 bit）、3 字节（24 bit）、4 字节（32 bit）和 8 字节（64 bit）来保存整数数据。同时，它们的编码类型本身占 1 字节，编码类型值分别是它们的宏定义值。\n\n然后，对于字符串编码来说，一共有三种类型，分别是 lp_encoding_6bit_str、lp_encoding_12bit_str 和 lp_encoding_32bit_str。从刚才的介绍中，你可以看到，整数编码类型名称中 bit 前面的数字，表示的是整数的长度。因此类似的，字符串编码类型名称中 bit 前的数字，表示的就是字符串的长度。\n\n比如，当编码类型为 lp_encoding_6bit_str 时，编码类型占 1 字节。该类型的宏定义值是 0x80，对应的二进制值是 1000 0000，这其中的前 2 位是用来标识编码类型本身，而后 6 位保存的是字符串长度。然后，列表项中的数据部分保存了实际的字符串。\n\n下面的图展示了三种字符串编码类型和数据的布局，你可以看下。\n\n\n\n\n# listpack 避免连锁更新的实现方式\n\n最后，我们再来了解下 listpack 列表项是如何避免连锁更新的。\n\n在 listpack 中，因为每个列表项只记录自己的长度，而不会像 ziplist 中的列表项那样，会记录前一项的长度。所以，当我们在 listpack 中新增或修改元素时，实际上只会涉及每个列表项自己的操作，而不会影响后续列表项的长度变化，这就避免了连锁更新。\n\n不过，你可能会有疑问：如果 listpack 列表项只记录当前项的长度，那么 listpack 支持从左向右正向查询列表，或是从右向左反向查询列表吗？\n\n其实，listpack 是能支持正、反向查询列表的。\n\n当应用程序从左向右正向查询 listpack 时，我们可以先调用 lpfirst 函数。该函数的参数是指向 listpack 头的指针，它在执行时，会让指针向右偏移 lp_hdr_size 大小，也就是跳过 listpack 头。你可以看下 lpfirst 函数的代码，如下所示：\n\nunsigned char *lpfirst(unsigned char *lp) {\n    lp += lp_hdr_size; //跳过listpack头部6个字节\n    if (lp[0] == lp_eof) return null;  //如果已经是listpack的末尾结束字节，则返回null\n    return lp;\n}\n\n\n然后，再调用 lpnext 函数，该函数的参数包括了指向 listpack 某个列表项的指针。lpnext 函数会进一步调用 lpskip 函数，并传入当前列表项的指针，如下所示：\n\nunsigned char *lpnext(unsigned char *lp, unsigned char *p) {\n    ...\n    p = lpskip(p);  //调用lpskip函数，偏移指针指向下一个列表项\n    if (p[0] == lp_eof) return null;\n    return p;\n}\n\n\n最后，lpskip 函数会先后调用 lpcurrentencodedsize 和 lpencodebacklen 这两个函数。\n\nlpcurrentencodedsize 函数是根据当前列表项第 1 个字节的取值，来计算当前项的编码类型，并根据编码类型，计算当前项编码类型和实际数据的总长度。然后，lpencodebacklen 函数会根据编码类型和实际数据的长度之和，进一步计算列表项最后一部分 entry-len 本身的长度。\n\n这样一来，lpskip 函数就知道当前项的编码类型、实际数据和 entry-len 的总长度了，也就可以将当前项指针向右偏移相应的长度，从而实现查到下一个列表项的目的。\n\n下面代码展示了 lpencodebacklen 函数的基本计算逻辑，你可以看下。\n\nunsigned long lpencodebacklen(unsigned char *buf, uint64_t l) {\n    //编码类型和实际数据的总长度小于等于127，entry-len长度为1字节\n    if (l <= 127) {\n        ...\n        return 1;\n    } else if (l < 16383) { //编码类型和实际数据的总长度大于127但小于16383，entry-len长度为2字节\n       ...\n        return 2;\n    } else if (l < 2097151) {//编码类型和实际数据的总长度大于16383但小于2097151，entry-len长度为3字节\n       ...\n        return 3;\n    } else if (l < 268435455) { //编码类型和实际数据的总长度大于2097151但小于268435455，entry-len长度为4字节\n        ...\n        return 4;\n    } else { //否则，entry-len长度为5字节\n       ...\n        return 5;\n    }\n}\n\n\n我也画了一张图，展示了从左向右遍历 listpack 的基本过程，你可以再回顾下。\n\n\n\n好，了解了从左向右正向查询 listpack，我们再来看下从右向左反向查询 listpack。\n\n首先，我们根据 listpack 头中记录的 listpack 总长度，就可以直接定位到 listapck 的尾部结束标记。然后，我们可以调用 lpprev 函数，该函数的参数包括指向某个列表项的指针，并返回指向当前列表项前一项的指针。\n\nlpprev 函数中的关键一步就是调用 lpdecodebacklen 函数。lpdecodebacklen 函数会从右向左，逐个字节地读取当前列表项的 entry-len。\n\n那么，lpdecodebacklen 函数如何判断 entry-len 是否结束了呢？\n\n这就依赖于 entry-len 的编码方式了。entry-len 每个字节的最高位，是用来表示当前字节是否为 entry-len 的最后一个字节，这里存在两种情况，分别是：\n\n * 最高位为 1，表示 entry-len 还没有结束，当前字节的左边字节仍然表示 entry-len 的内容；\n * 最高位为 0，表示当前字节已经是 entry-len 最后一个字节了。\n\n而 entry-len 每个字节的低 7 位，则记录了实际的长度信息。这里你需要注意的是，entry-len 每个字节的低 7 位采用了大端模式存储，也就是说，entry-len 的低位字节保存在内存高地址上。\n\n我画了下面这张图，展示了 entry-len 这种特别的编码方式，你可以看下。\n\n\n\n实际上，正是因为有了 entry-len 的特别编码方式，lpdecodebacklen 函数就可以从当前列表项起始位置的指针开始，向左逐个字节解析，得到前一项的 entry-len 值。这也是 lpdecodebacklen 函数的返回值。而从刚才的介绍中，我们知道 entry-len 记录了编码类型和实际数据的长度之和。\n\n因此，lpprev 函数会再调用 lpencodebacklen 函数，来计算得到 entry-len 本身长度，这样一来，我们就可以得到前一项的总长度，而 lpprev 函数也就可以将指针指向前一项的起始位置了。所以按照这个方法，listpack 就实现了从右向左的查询功能。\n\n\n# 对比\n\n特性       ziplist                quicklist   listpack\n设计复杂度    较为复杂，包含前一个节点长度字段       复杂          更加简化，没有前一个节点长度字段\n内存占用     存在冗余字段，内存利用率较低         高           更加紧凑，内存占用低\n操作复杂度    插入、删除操作需要更新前向节点长度，较慢   中           操作简单高效，无需处理前向节点长度，但是也要移动\n内存移动问题   频繁插入删除可能导致大范围内存移动      中           仍存在内存移动问题，但操作更加简单\n适用场景     小 hash，小 zset          list        小 hash，小 zset，小 stream\n\n\n\n笔记\n\necho 认为，ziplist 和 listpack 适用于元素较少时的存储，一旦元素变多就需采用 quicklist 这种类似于 linkedlist 的结构来进行存储，但是 quicklist 的 node 有两种选择，分别是 ziplist 和 listpack，在最新的版本中貌似都是采用 listpack 来实现的\n\n\n# 总结\n\n本文从 ziplist 的设计不足出发，到学习 quicklist 和 listpack 的设计思想\n\n你要知道，ziplist 的不足主要在于一旦 ziplist 中元素个数多了，它的查找效率就会降低。而且如果在 ziplist 里新增或修改数据，ziplist 占用的内存空间还需要重新分配；更糟糕的是，ziplist 新增某个元素或修改某个元素时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起连锁更新问题，导致每个元素的空间都要重新分配，这就会导致 ziplist 的访问性能下降。\n\n所以，为了应对 ziplist 的问题，redis 先是在 3.0 版本中设计实现了 quicklist。quicklist 结构在 ziplist 基础上，使用链表将 ziplist 串联起来，链表的每个元素就是一个 ziplist。这种设计减少了数据插入时内存空间的重新分配，以及内存数据的拷贝。同时，quicklist 限制了每个节点上 ziplist 的大小，一旦一个 ziplist 过大，就会采用新增 quicklist 节点的方法。\n\n不过，又因为 quicklist 使用 quicklistnode 结构指向每个 ziplist，无疑增加了内存开销。为了减少内存开销，并进一步避免 ziplist 连锁更新问题，redis 在 5.0 版本中，就设计实现了 listpack 结构。listpack 结构沿用了 ziplist 紧凑型的内存布局，把每个元素都紧挨着放置\n\nlistpack 中每个列表项不再包含前一项的长度了，因此当某个列表项中的数据发生变化，导致列表项长度变化时，其他列表项的长度是不会受影响的，因而这就避免了 ziplist 面临的连锁更新问题。\n\n总而言之，redis 在内存紧凑型列表的设计与实现上，从 ziplist 到 quicklist，再到 listpack，你可以看到 redis 在内存空间开销和访问性能之间的设计取舍，这一系列的设计变化，是非常值得你学习的\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Hash 设计与实现",frontmatter:{title:"Hash 设计与实现",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d4311/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/02.%E5%9F%BA%E7%A1%80/05.Hash%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Redis 系统设计/02.基础/05.Hash 设计与实现.md",key:"v-1cdf4c57",path:"/pages/2d4311/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:244},{level:2,title:"如何避免 hash 冲突",slug:"如何避免-hash-冲突",normalizedTitle:"如何避免 hash 冲突",charIndex:605},{level:2,title:"如何实现 rehash",slug:"如何实现-rehash",normalizedTitle:"如何实现 rehash",charIndex:1031},{level:3,title:"什么时候触发 rehash",slug:"什么时候触发-rehash",normalizedTitle:"什么时候触发 rehash",charIndex:1815},{level:3,title:"rehash 扩容扩多大？",slug:"rehash-扩容扩多大",normalizedTitle:"rehash 扩容扩多大？",charIndex:1833},{level:3,title:"渐进式 rehash 如何实现",slug:"渐进式-rehash-如何实现",normalizedTitle:"渐进式 rehash 如何实现",charIndex:5804},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:11418},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:11532}],headersStr:"前言 如何避免 hash 冲突 如何实现 rehash 什么时候触发 rehash rehash 扩容扩多大？ 渐进式 rehash 如何实现 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. 如何避免 Hash 表数据量增加导致哈希冲突的性能下降？\n 2. 为什么链式哈希能有效解决冲突，Redis 如何实现？\n 3. Hash 表扩容时，为什么直接 rehash 会影响性能？\n 4. Redis 如何优化 rehash 过程，避免主线程阻塞？\n 5. 渐进式 rehash 如何确保数据迁移时间有限？\n 6. Redis 如何调整 rehash 触发条件，平衡性能与内存？\n 7. rehash 进行时，如何确保新键值对正确存储？\n\n\n# 前言\n\n对于 Redis 键值数据库来说，Hash 表有两种主要场景\n\n * Hash 表既是键值对中的一种值类型\n * 同时，Redis 也使用一个全局 Hash 表来保存所有的键值对，从而既满足应用存取 Hash 结构数据需求，又能提供快速查询功能\n\n> Hash 表应用如此广泛的一个重要原因，就是从理论上来说，它能以 O(1) 的复杂度快速查询数据\n\n但是实际应用 Hash 表时有两个缺点：\n\n * 哈希冲突\n * rehash 开销\n\nRedis 为我们提供了一个经典的 Hash 表实现方案来解决上述问题\n\n * 针对 哈希冲突，Redis 采用了链式哈希\n * 针对 rehash 开销，Redis 采用了 渐进式 rehash 设计，进而缓解了 rehash 操作带来的额外开销对系统的性能影响\n\n\n# 如何避免 hash 冲突\n\n * 第一种方案，就是我接下来要给你介绍的链式哈希。这里你需要先知道，链式哈希的链 不能太长，否则会降低 Hash 表性能\n * 第二种方案，就是当链式哈希的链长达到一定长度时，我们可以使用 rehash。不过， 执行 rehash 本身开销比较大，所以就需要采用我稍后会给你介绍的渐进式 rehash 设计\n\n这样，当我们要查询 key5 时，可以先通过哈希函数计算，得到 key5 的哈希值被映射到了桶 9 中。然后，我们再逐一比较桶 9 中串接的 key，直到查找到 key5。如此一来，我们就能在链式哈希中找到所查的哈希项了。\n\n不过，链式哈希也存在局限性，那就是随着链表长度的增加，Hash 表在一个位置上查询哈希项的耗时就会增加，从而增加了 Hash 表的整体查询时间，这样也会导致 Hash 表的性能下降。\n\n所以 Redis 要控制 Hash 表的长度，就要在长度达到一定阈值时去进行 rehash\n\n\n# 如何实现 rehash\n\nrehash 操作，其实就是指扩大 Hash 表空间。而 Redis 实现 rehash 的基本思路是这样的：\n\n首先，Redis 准备了两个哈希表，用于 rehash 时交替保存数据。\n\nRedis 在 dict.h 文件中使用 dictht 结构体定义了 Hash 表。不过，在实际使用 Hash 表时，Redis 又在 dict.h 文件中，定义了一个 dict 结构体。这个结构体中有一个数组ht[2]，包含了两个 Hash 表 ht[0] 和 ht[1]\n\ntypedef struct dict {\n    dictType *type;\n    void *privdata;\n    //两个Hash表，交替使用，用于rehash操作\n    dictht ht[2];\n    // Hash表是否在进行rehash的标识，-1表示没有进行rehash\n    long rehashidx; /* rehashing not in progress if rehashidx == -1 */\n    int16_t pauserehash; /* If >0 rehashing is paused (<0 indicates coding error) */\n} dict;\n\n\n * 在正常服务请求阶段，所有的键值对写入哈希表 ht[0]\n\n * 当进行 rehash 时，键值对被迁移到哈希表 ht[1] 中\n\n * 当迁移完成后，ht[0] 的空间会被释放，并把 ht[1] 的地址赋值给 ht[0]，ht[1] 的表大小设置为 0。这样一来，又回到了正常服务请求的阶段，ht[0] 接收和服务请求，ht[1] 作为下一次 rehash 时的迁移表\n\n那么，在实现 rehash 时，需要解决哪些问题？\n\n * 什么时候触发 rehash？\n * rehash 扩容扩多大？\n * rehash 如何执行？\n\n\n# 什么时候触发 rehash\n\n首先要知道，Redis 用来判断是否触发 rehash 的函数是 _dictExpandIfNeeded。所以接 下来我们就先看看， _dictExpandIfNeeded 函数中进行扩容的触发条件；然后，我们再来了解下 _dictExpandIfNeeded 又是在哪些函数中被调用的。\n\n实际上， _dictExpandIfNeeded 函数中定义了三个扩容条件。\n\n * 条件一：ht[0] 的大小为 0。\n * 条件二：ht[0] 承载的元素个数已经超过了 ht[0] 的大小，同时 Hash 表可以进行扩容。\n * 条件三：ht[0] 承载的元素个数，是 ht[0] 的大小的 dict_force_resize_ratio 倍，其中， dict_force_resize_ratio 的默认值是 5\n\n/* Expand the hash table if needed */\nstatic int _dictExpandIfNeeded(dict *d)\n{\n    /* Incremental rehashing already in progress. Return. */\n    if (dictIsRehashing(d)) return DICT_OK;\n\n    /* If the hash table is empty expand it to the initial size. */\n    if (d->ht[0].size == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE);\n\n    /* If we reached the 1:1 ratio, and we are allowed to resize the hash\n     * table (global setting) or we should avoid it but the ratio between\n     * elements/buckets is over the \"safe\" threshold, we resize doubling\n     * the number of buckets. */\n    // ht[0]表使用的元素个数超过当前大小\n    // 并且可以扩容或者 ht[0]使用的元素个数/ht[0]表的大小 大于 dict_force_resize_ratio\n    // 并且能够允许扩展\n    if (d->ht[0].used >= d->ht[0].size &&\n        (dict_can_resize ||\n         d->ht[0].used/d->ht[0].size > dict_force_resize_ratio) &&\n        dictTypeExpandAllowed(d))\n    {\n        return dictExpand(d, d->ht[0].used + 1);\n    }\n    return DICT_OK;\n}\n\n\n * 对于条件一来说，此时 Hash 表是空的，所以 Redis 就需要将 Hash 表空间设置为初始大小，而这是初始化的工作，并不属于 rehash 操作。\n\n * 而条件二和三就对应了 rehash 的场景。因为在这两个条件中，都比较了 Hash 表当前承载 的元素个数d->ht[0].used和 Hash 表当前设定的大小d->ht[0].size，这两个值的比值一般称为负载因子（load factor）。也就是说，Redis 判断是否进行 rehash 的条 件，就是看 load factor 是否大于等于 1 和是否大于 5。\n\n提示\n\n当 load factor 大于 5 时，就表明 Hash 表已经过载比较严重了，需要立刻进行库扩容。而当 load factor 大于等于 1 时，Redis 还会再判断 dict_can_resize 这个变量值，查看当前是否可以进行扩容\n\n你可能要问了，这里的 dict_can_resize 变量值是啥呀？其实，这个变量值是在 dictEnableResize 和 dictDisableResize 两个函数中设置的，它们的作用分别是启用和禁止哈希表执行 rehash 功能，如下所示：\n\nvoid dictEnableResize(void) {\n    dict_can_resize = 1;\n}\n\nvoid dictDisableResize(void) {\n    dict_can_resize = 0;\n}\n\n\n然后，这两个函数又被封装在了 updateDictResizePolicy 函数中。\n\nupdateDictResizePolicy 函数是用来启用或禁用 rehash 扩容功能的，这个函数调用 dictEnableResize 函数启用扩容功能的条件是：\n\n * 当前没有 RDB 子进程，并且也没有 AOF 子进程。\n\n这就对应了 Redis 没有执行 RDB 快照和没有进行 AOF 重写的场景。你可以参考下面给出的代码：\n\nvoid updateDictResizePolicy(void) {\nif (server.rdb_child_pid == -1 && server.aof_child_pid == -1)\n  dictEnableResize();\nelse\n  dictDisableResize();\n}\n\n\n上述是 _dictExpandIfNeeded 对 rehash 的判断触发条件\n\n接下来，再来看下 Redis 会在哪些函数中，调用 _dictExpandIfNeeded 进行判断\n\n首先，通过在 dict.c 文件中查看 _dictExpandIfNeeded 的被调用关系，我们可以发现， _dictExpandIfNeeded 是被 _dictKeyIndex 函数调用的，而 _dictKeyIndex 函数又会被 dictAddRaw 函数调用，然后 dictAddRaw 会被以下三个函数调用。\n\n * dictAdd：用来往 Hash 表中添加一个键值对\n * dictRelace：用来往 Hash 表中添加一个键值对，或者键值对存在时，修改键值对\n * dictAddorFind：直接调用 dictAddRaw\n\n因此，当我们往 Redis 中写入新的键值对或是修改键值对时，Redis 都会判断下是否需要进行 rehash。这里你可以参考下面给出的示意图，其中就展示了 _dictExpandIfNeeded 被调用的关系。\n\n简而言之，Redis 中触发 rehash 操作的关键，就是 dictExpandIfNeeded 函数 和 updateDictResizePolicy 函数。\n\ndictExpandIfNeeded 函数会根据下述情况判断是否进行rehash\n\n * Hash 表的负载因子\n * RDB 和 AOF 的执行情况\n\n然后看第二个问题：rehash 扩容扩多大？\n\n\n# rehash 扩容扩多大？\n\n在 Redis 中，rehash 对 Hash 表空间的扩容是通过调用 dictExpand 函数来完成的。 dictExpand 函数的参数有两个\n\n * 一个是要扩容的 Hash 表\n * 另一个是要扩到的容量\n\n int dictExpand(dict *d, unsigned long size);\n\n\n对于一个 Hash 表来说\n\n 1. 我们就可以根据前面提到的 _dictExpandIfNeeded 函数， 来判断是否要对其进行扩容\n\n 2. 一旦判断要扩容，Redis 在执行 rehash 操作时，对 Hash 表扩容的思路也很简单，就是如果当前表的已用空间大小为 size，那么就将表扩容到 size*2 的大小\n\n如下所示，这里你可以看到，rehash 的扩容大小是当前 ht[0]已使用大小的 2 倍\n\ndictExpand(d, d->ht[0].used*2);\n\n\n而在 dictExpand 函数中，具体执行是由 _dictNextPower 函数完成的，以下代码显示的 Hash 表扩容的操作，就是从 Hash 表的初始大小DICT_HT_INITIAL_SIZE，不停地乘以 2，直到达到目标大小\n\nstatic unsigned long _dictNextPower(unsigned long size)\n{\n    // 哈希表的初始大小\n    unsigned long i = DICT_HT_INITIAL_SIZE;\n  \t// 如果要扩容的大小已经超过最大值，则返回最大值加1\n    if (size >= LONG_MAX) \n        return LONG_MAX + 1LU;\n    // 扩容大小没有超过最大值\n    while(1) {\n        if (i >= size)\n            return i;\n        // 每一步扩容都在现有大小基础上乘以2\n        i *= 2;\n    }\n}\n\n\n下面开始第三个问题，即 rehash 要如何执行？而这个问题，本质上就是 Redis 要如何实现渐进式 rehash 设计\n\n\n# 渐进式 rehash 如何实现\n\n为什么要实现渐进式 rehash\n\n因为，Hash 表在执行 rehash 时，由于 Hash 表空间扩大，原本映射到某一位置的键可能会被映射到一个新的位置上，因此，很多键就需要从原来的位置拷贝到新的位 置。而在键拷贝时，由于 Redis 主线程无法执行其他请求，所以键拷贝会阻塞主线程，这样就会产生 rehash 开销，而为了降低 rehash 开销，Redis 就提出了渐进式 rehash 的方法\n\n简述 「渐进式 rehash 」：Redis 并不会一次性把当前 Hash 表中的所有键， 都拷贝到新位置，而是会分批拷贝，每次的键拷贝只拷贝 Hash 表中一个 bucket 中的哈希项。这样一来，每次键拷贝的时长有限，对主线程的影响也就有限了。\n\n渐进式 rehash 在代码层面的实现，有两个关键函数：dictRehash 和 _dictRehashStep。\n\n我们先来看 dictRehash 函数，这个函数实际执行键拷贝，它的输入参数有两个，分别是 全局哈希表（即前面提到的 dict 结构体，包含了 ht[0]和 ht[1]）和需要进行键拷贝的桶数量（bucket 数量）。\n\ndictRehash 函数的整体逻辑包括三部分：\n\n 1. 该函数会执行一个循环，根据要进行键拷贝的 bucket 数量 n，依次完成这些 bucket 内部所有键的迁移。当然，如果 ht[0] 哈希表中的数据已经都迁移完成了，键拷贝的循环也会停止执行\n 2. 在完成了 n 个 bucket 拷贝后，dictRehash 函数的第二部分逻辑，就是判断 ht[0] 表中数据是否都已迁移完。如果都迁移完了，那么 ht[0] 的空间会被释放。因为 Redis 在处理请求时，代码逻辑中都是使用 ht[0]，所以当 rehash 执行完成后，虽然数据都在 ht[1] 中了，但 Redis 仍然会把 ht[1] 赋值给 ht[0]，以便其他部分的代码逻辑正常使用\n 3. 在 ht[1] 赋值给 ht[0] 后，它的大小就会被重置为 0，等待下一次 rehash。与此同时， 全局哈希表中的 rehashidx 变量会被标为 -1，表示 rehash 结束了（这里的 rehashidx 变量用来表示 rehash 的进度，稍后我会给你具体解释）。\n\n\n\nint dictRehash(dict *d, int n) {\n    int empty_visits = n*10; /* Max number of empty buckets to visit. */\n    if (!dictIsRehashing(d)) return 0;\n\n    // 主循环，根据要拷贝的bucket数量n，循环n次后停止或ht[0]中的数据迁移完停止\n    while(n-- && d->ht[0].used != 0) {\n        dictEntry *de, *nextde;\n\n        /* Note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */\n        assert(d->ht[0].size > (unsigned long)d->rehashidx);\n        while(d->ht[0].table[d->rehashidx] == NULL) {\n            d->rehashidx++;\n            if (--empty_visits == 0) return 1;\n        }\n        de = d->ht[0].table[d->rehashidx];\n        /* Move all the keys in this bucket from the old to the new hash HT */\n        while(de) {\n            uint64_t h;\n\n            nextde = de->next;\n            /* Get the index in the new hash table */\n            h = dictHashKey(d, de->key) & d->ht[1].sizemask;\n            de->next = d->ht[1].table[h];\n            d->ht[1].table[h] = de;\n            d->ht[0].used--;\n            d->ht[1].used++;\n            de = nextde;\n        }\n        d->ht[0].table[d->rehashidx] = NULL;\n        d->rehashidx++;\n    }\n\n    //判断ht[0]的数据是否迁移完成\n    /* Check if we already rehashed the whole table... */\n    if (d->ht[0].used == 0) {\n        // ht[0]迁移完后，释放ht[0]内存空间\n        zfree(d->ht[0].table);\n        // 让ht[0]指向ht[1]，以便接受正常的请求\n        d->ht[0] = d->ht[1];\n        // 重置ht[1]的大小为0\n        _dictReset(&d->ht[1]);\n        // 设置全局哈希表的rehashidx标识为-1，表示rehash结束\n        d->rehashidx = -1;\n        // 返回0，表示ht[0]中所有元素都迁移完\n        return 0;\n    }\n\n    //返回1，表示ht[0]中仍然有元素没有迁移完\n    /* More to rehash... */\n    return 1;\n}\n\n\n那么，渐进式 rehash 是如何按照 bucket 粒度拷贝数据的，这其实就和全局哈希表 dict 结构中的 rehashidx 变量相关了\n\nrehashidx 变量表示的是当前 rehash 在对哪个 bucket 做数据迁移。比如，当 rehashidx 等于 0 时，表示对 ht[0]中的第一个 bucket 进行数据迁移；当 rehashidx 等于 1 时，表 示对 ht[0] 中的第二个 bucket 进行数据迁移，以此类推。\n\n而 dictRehash 函数的主循环，首先会判断 rehashidx 指向的 bucket 是否为空，如果为空，那就将 rehashidx 的值加 1，检查下一个 bucket。\n\n那么，有没有可能连续几个 bucket 都为空呢？其实是有可能的，在这种情况下，渐进式 rehash 不会一直递增 rehashidx 进行检查。这是因为一旦执行了 rehash，Redis 主线程就无法处理其他请求了。\n\n所以，渐进式 rehash 在执行时设置了一个变量 empty_visits，用来表示已经检查过的空 bucket，当检查了一定数量的空 bucket 后，这一轮的 rehash 就停止执行，转而继续处理外来请求，避免了对 Redis 性能的影响。下面的代码显示了这部分逻辑，你可以看下。\n\n// 如果当前要迁移的 bucket 中没有元素\nwhile(d->ht[0].table[d->rehashidx] == NULL) {\n    d->rehashidx++;\n    if (--empty_visits == 0) return 1;\n}\n\n\n而如果 rehashidx 指向的 bucket 有数据可以迁移，那么 Redis 就会把这个 bucket 中的哈希项依次取出来，并根据 ht[1] 的表空间大小，重新计算哈希项在 ht[1] 中的 bucket 位置，然后把这个哈希项赋值到 ht[1] 对应 bucket 中\n\n这样，每做完一个哈希项的迁移，ht[0] 和 ht[1] 用来表示承载哈希项多少的变量 used，就 会分别减一和加一。当然，如果当前 rehashidx 指向的 bucket 中数据都迁移完了， rehashidx 就会递增加 1，指向下一个 bucket。下面的代码显示了这一迁移过程。\n\n while(n-- && d->ht[0].used != 0) {\n     dictEntry *de, *nextde;\n\n     /* Note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */\n     assert(d->ht[0].size > (unsigned long)d->rehashidx);\n     while(d->ht[0].table[d->rehashidx] == NULL) {\n         d->rehashidx++;\n         if (--empty_visits == 0) return 1;\n     }\n     // 获得哈希表中哈希项\n     de = d->ht[0].table[d->rehashidx];\n     /* Move all the keys in this bucket from the old to the new hash HT */\n     while(de) {\n         uint64_t h;\n     \t// 获得同一个bucket中下一个哈希项\n         nextde = de->next;\n         /* Get the index in the new hash table */\n         // 根据扩容后的哈希表ht[1]大小，计算当前哈希项在扩容后哈希表中的bucket位置\n         h = dictHashKey(d, de->key) & d->ht[1].sizemask;\n         // 将当前哈希项添加到扩容后的哈希表ht[1]中\n         de->next = d->ht[1].table[h];\n         d->ht[1].table[h] = de;\n         // 减少当前哈希表的哈希项个数\n         d->ht[0].used--;\n         // 增加扩容后哈希表的哈希项个数\n         d->ht[1].used++;\n         de = nextde;\n     }\n     // 如果当前bucket中已经没有哈希项了，将该bucket置为NULL\n     d->ht[0].table[d->rehashidx] = NULL;\n     // 将rehash加1，下一次将迁移下一个bucket中的元素\n     d->rehashidx++;\n }\n\n\n好了，到这里，我们就已经基本了解了 dictRehash 函数的全部逻辑。 现在我们知道，dictRehash 函数本身是按照 bucket 粒度执行哈希项迁移的，它内部执行的 bucket 迁移个数，主要由传入的循环次数变量 n 来决定。但凡 Redis 要进行 rehash 操作，最终都会调用 dictRehash 函数。\n\n接下来，我们来学习和渐进式 rehash 相关的第二个关键函数 _dictRehashStep，这个函数实现了每次只对一个 bucket 执行 rehash。 从 Redis 的源码中我们可以看到，一共会有 5 个函数通过调用 _dictRehashStep 函数，进而调用 dictRehash 函数，来执行 rehash，它们分别是：dictAddRaw， dictGenericDelete，dictFind，dictGetRandomKey，dictGetSomeKeys。\n\n其中，dictAddRaw 和 dictGenericDelete 函数，分别对应了往 Redis 中增加和删除键值对，而后三个函数则对应了在 Redis 中进行查询操作。下图展示了这些函数间的调用关系：\n\n但你要注意，不管是增删查哪种操作，这 5 个函数调用的 _dictRehashStep 函数，给 dictRehash 传入的循环次数变量 n 的值都为 1，下面的代码就显示了这一传参的情况\n\nstatic void _dictRehashStep(dict *d) {\n    // 给dictRehash传入的循环次数参数为1，表明每迁移完一个bucket ，就执行正常操作\n    if (d->pauserehash == 0) dictRehash(d,1);\n}\n\n\n这样一来，每次迁移完一个 bucket，Hash 表就会执行正常的增删查请求操作，这就是在代码层面实现渐进式 rehash 的方法\n\n\n# 总结\n\n 1. 通过「链表」解决Hash冲突\n 2. Redis 通过 「渐进式 rehash」 来解决大量数据 rehash 可能会导致的阻塞问题\n 3. 渐进式 rehash 按照 bucket 粒度拷贝数据的方法\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 如何避免 hash 表数据量增加导致哈希冲突的性能下降？\n 2. 为什么链式哈希能有效解决冲突，redis 如何实现？\n 3. hash 表扩容时，为什么直接 rehash 会影响性能？\n 4. redis 如何优化 rehash 过程，避免主线程阻塞？\n 5. 渐进式 rehash 如何确保数据迁移时间有限？\n 6. redis 如何调整 rehash 触发条件，平衡性能与内存？\n 7. rehash 进行时，如何确保新键值对正确存储？\n\n\n# 前言\n\n对于 redis 键值数据库来说，hash 表有两种主要场景\n\n * hash 表既是键值对中的一种值类型\n * 同时，redis 也使用一个全局 hash 表来保存所有的键值对，从而既满足应用存取 hash 结构数据需求，又能提供快速查询功能\n\n> hash 表应用如此广泛的一个重要原因，就是从理论上来说，它能以 o(1) 的复杂度快速查询数据\n\n但是实际应用 hash 表时有两个缺点：\n\n * 哈希冲突\n * rehash 开销\n\nredis 为我们提供了一个经典的 hash 表实现方案来解决上述问题\n\n * 针对 哈希冲突，redis 采用了链式哈希\n * 针对 rehash 开销，redis 采用了 渐进式 rehash 设计，进而缓解了 rehash 操作带来的额外开销对系统的性能影响\n\n\n# 如何避免 hash 冲突\n\n * 第一种方案，就是我接下来要给你介绍的链式哈希。这里你需要先知道，链式哈希的链 不能太长，否则会降低 hash 表性能\n * 第二种方案，就是当链式哈希的链长达到一定长度时，我们可以使用 rehash。不过， 执行 rehash 本身开销比较大，所以就需要采用我稍后会给你介绍的渐进式 rehash 设计\n\n这样，当我们要查询 key5 时，可以先通过哈希函数计算，得到 key5 的哈希值被映射到了桶 9 中。然后，我们再逐一比较桶 9 中串接的 key，直到查找到 key5。如此一来，我们就能在链式哈希中找到所查的哈希项了。\n\n不过，链式哈希也存在局限性，那就是随着链表长度的增加，hash 表在一个位置上查询哈希项的耗时就会增加，从而增加了 hash 表的整体查询时间，这样也会导致 hash 表的性能下降。\n\n所以 redis 要控制 hash 表的长度，就要在长度达到一定阈值时去进行 rehash\n\n\n# 如何实现 rehash\n\nrehash 操作，其实就是指扩大 hash 表空间。而 redis 实现 rehash 的基本思路是这样的：\n\n首先，redis 准备了两个哈希表，用于 rehash 时交替保存数据。\n\nredis 在 dict.h 文件中使用 dictht 结构体定义了 hash 表。不过，在实际使用 hash 表时，redis 又在 dict.h 文件中，定义了一个 dict 结构体。这个结构体中有一个数组ht[2]，包含了两个 hash 表 ht[0] 和 ht[1]\n\ntypedef struct dict {\n    dicttype *type;\n    void *privdata;\n    //两个hash表，交替使用，用于rehash操作\n    dictht ht[2];\n    // hash表是否在进行rehash的标识，-1表示没有进行rehash\n    long rehashidx; /* rehashing not in progress if rehashidx == -1 */\n    int16_t pauserehash; /* if >0 rehashing is paused (<0 indicates coding error) */\n} dict;\n\n\n * 在正常服务请求阶段，所有的键值对写入哈希表 ht[0]\n\n * 当进行 rehash 时，键值对被迁移到哈希表 ht[1] 中\n\n * 当迁移完成后，ht[0] 的空间会被释放，并把 ht[1] 的地址赋值给 ht[0]，ht[1] 的表大小设置为 0。这样一来，又回到了正常服务请求的阶段，ht[0] 接收和服务请求，ht[1] 作为下一次 rehash 时的迁移表\n\n那么，在实现 rehash 时，需要解决哪些问题？\n\n * 什么时候触发 rehash？\n * rehash 扩容扩多大？\n * rehash 如何执行？\n\n\n# 什么时候触发 rehash\n\n首先要知道，redis 用来判断是否触发 rehash 的函数是 _dictexpandifneeded。所以接 下来我们就先看看， _dictexpandifneeded 函数中进行扩容的触发条件；然后，我们再来了解下 _dictexpandifneeded 又是在哪些函数中被调用的。\n\n实际上， _dictexpandifneeded 函数中定义了三个扩容条件。\n\n * 条件一：ht[0] 的大小为 0。\n * 条件二：ht[0] 承载的元素个数已经超过了 ht[0] 的大小，同时 hash 表可以进行扩容。\n * 条件三：ht[0] 承载的元素个数，是 ht[0] 的大小的 dict_force_resize_ratio 倍，其中， dict_force_resize_ratio 的默认值是 5\n\n/* expand the hash table if needed */\nstatic int _dictexpandifneeded(dict *d)\n{\n    /* incremental rehashing already in progress. return. */\n    if (dictisrehashing(d)) return dict_ok;\n\n    /* if the hash table is empty expand it to the initial size. */\n    if (d->ht[0].size == 0) return dictexpand(d, dict_ht_initial_size);\n\n    /* if we reached the 1:1 ratio, and we are allowed to resize the hash\n     * table (global setting) or we should avoid it but the ratio between\n     * elements/buckets is over the \"safe\" threshold, we resize doubling\n     * the number of buckets. */\n    // ht[0]表使用的元素个数超过当前大小\n    // 并且可以扩容或者 ht[0]使用的元素个数/ht[0]表的大小 大于 dict_force_resize_ratio\n    // 并且能够允许扩展\n    if (d->ht[0].used >= d->ht[0].size &&\n        (dict_can_resize ||\n         d->ht[0].used/d->ht[0].size > dict_force_resize_ratio) &&\n        dicttypeexpandallowed(d))\n    {\n        return dictexpand(d, d->ht[0].used + 1);\n    }\n    return dict_ok;\n}\n\n\n * 对于条件一来说，此时 hash 表是空的，所以 redis 就需要将 hash 表空间设置为初始大小，而这是初始化的工作，并不属于 rehash 操作。\n\n * 而条件二和三就对应了 rehash 的场景。因为在这两个条件中，都比较了 hash 表当前承载 的元素个数d->ht[0].used和 hash 表当前设定的大小d->ht[0].size，这两个值的比值一般称为负载因子（load factor）。也就是说，redis 判断是否进行 rehash 的条 件，就是看 load factor 是否大于等于 1 和是否大于 5。\n\n提示\n\n当 load factor 大于 5 时，就表明 hash 表已经过载比较严重了，需要立刻进行库扩容。而当 load factor 大于等于 1 时，redis 还会再判断 dict_can_resize 这个变量值，查看当前是否可以进行扩容\n\n你可能要问了，这里的 dict_can_resize 变量值是啥呀？其实，这个变量值是在 dictenableresize 和 dictdisableresize 两个函数中设置的，它们的作用分别是启用和禁止哈希表执行 rehash 功能，如下所示：\n\nvoid dictenableresize(void) {\n    dict_can_resize = 1;\n}\n\nvoid dictdisableresize(void) {\n    dict_can_resize = 0;\n}\n\n\n然后，这两个函数又被封装在了 updatedictresizepolicy 函数中。\n\nupdatedictresizepolicy 函数是用来启用或禁用 rehash 扩容功能的，这个函数调用 dictenableresize 函数启用扩容功能的条件是：\n\n * 当前没有 rdb 子进程，并且也没有 aof 子进程。\n\n这就对应了 redis 没有执行 rdb 快照和没有进行 aof 重写的场景。你可以参考下面给出的代码：\n\nvoid updatedictresizepolicy(void) {\nif (server.rdb_child_pid == -1 && server.aof_child_pid == -1)\n  dictenableresize();\nelse\n  dictdisableresize();\n}\n\n\n上述是 _dictexpandifneeded 对 rehash 的判断触发条件\n\n接下来，再来看下 redis 会在哪些函数中，调用 _dictexpandifneeded 进行判断\n\n首先，通过在 dict.c 文件中查看 _dictexpandifneeded 的被调用关系，我们可以发现， _dictexpandifneeded 是被 _dictkeyindex 函数调用的，而 _dictkeyindex 函数又会被 dictaddraw 函数调用，然后 dictaddraw 会被以下三个函数调用。\n\n * dictadd：用来往 hash 表中添加一个键值对\n * dictrelace：用来往 hash 表中添加一个键值对，或者键值对存在时，修改键值对\n * dictaddorfind：直接调用 dictaddraw\n\n因此，当我们往 redis 中写入新的键值对或是修改键值对时，redis 都会判断下是否需要进行 rehash。这里你可以参考下面给出的示意图，其中就展示了 _dictexpandifneeded 被调用的关系。\n\n简而言之，redis 中触发 rehash 操作的关键，就是 dictexpandifneeded 函数 和 updatedictresizepolicy 函数。\n\ndictexpandifneeded 函数会根据下述情况判断是否进行rehash\n\n * hash 表的负载因子\n * rdb 和 aof 的执行情况\n\n然后看第二个问题：rehash 扩容扩多大？\n\n\n# rehash 扩容扩多大？\n\n在 redis 中，rehash 对 hash 表空间的扩容是通过调用 dictexpand 函数来完成的。 dictexpand 函数的参数有两个\n\n * 一个是要扩容的 hash 表\n * 另一个是要扩到的容量\n\n int dictexpand(dict *d, unsigned long size);\n\n\n对于一个 hash 表来说\n\n 1. 我们就可以根据前面提到的 _dictexpandifneeded 函数， 来判断是否要对其进行扩容\n\n 2. 一旦判断要扩容，redis 在执行 rehash 操作时，对 hash 表扩容的思路也很简单，就是如果当前表的已用空间大小为 size，那么就将表扩容到 size*2 的大小\n\n如下所示，这里你可以看到，rehash 的扩容大小是当前 ht[0]已使用大小的 2 倍\n\ndictexpand(d, d->ht[0].used*2);\n\n\n而在 dictexpand 函数中，具体执行是由 _dictnextpower 函数完成的，以下代码显示的 hash 表扩容的操作，就是从 hash 表的初始大小dict_ht_initial_size，不停地乘以 2，直到达到目标大小\n\nstatic unsigned long _dictnextpower(unsigned long size)\n{\n    // 哈希表的初始大小\n    unsigned long i = dict_ht_initial_size;\n  \t// 如果要扩容的大小已经超过最大值，则返回最大值加1\n    if (size >= long_max) \n        return long_max + 1lu;\n    // 扩容大小没有超过最大值\n    while(1) {\n        if (i >= size)\n            return i;\n        // 每一步扩容都在现有大小基础上乘以2\n        i *= 2;\n    }\n}\n\n\n下面开始第三个问题，即 rehash 要如何执行？而这个问题，本质上就是 redis 要如何实现渐进式 rehash 设计\n\n\n# 渐进式 rehash 如何实现\n\n为什么要实现渐进式 rehash\n\n因为，hash 表在执行 rehash 时，由于 hash 表空间扩大，原本映射到某一位置的键可能会被映射到一个新的位置上，因此，很多键就需要从原来的位置拷贝到新的位 置。而在键拷贝时，由于 redis 主线程无法执行其他请求，所以键拷贝会阻塞主线程，这样就会产生 rehash 开销，而为了降低 rehash 开销，redis 就提出了渐进式 rehash 的方法\n\n简述 「渐进式 rehash 」：redis 并不会一次性把当前 hash 表中的所有键， 都拷贝到新位置，而是会分批拷贝，每次的键拷贝只拷贝 hash 表中一个 bucket 中的哈希项。这样一来，每次键拷贝的时长有限，对主线程的影响也就有限了。\n\n渐进式 rehash 在代码层面的实现，有两个关键函数：dictrehash 和 _dictrehashstep。\n\n我们先来看 dictrehash 函数，这个函数实际执行键拷贝，它的输入参数有两个，分别是 全局哈希表（即前面提到的 dict 结构体，包含了 ht[0]和 ht[1]）和需要进行键拷贝的桶数量（bucket 数量）。\n\ndictrehash 函数的整体逻辑包括三部分：\n\n 1. 该函数会执行一个循环，根据要进行键拷贝的 bucket 数量 n，依次完成这些 bucket 内部所有键的迁移。当然，如果 ht[0] 哈希表中的数据已经都迁移完成了，键拷贝的循环也会停止执行\n 2. 在完成了 n 个 bucket 拷贝后，dictrehash 函数的第二部分逻辑，就是判断 ht[0] 表中数据是否都已迁移完。如果都迁移完了，那么 ht[0] 的空间会被释放。因为 redis 在处理请求时，代码逻辑中都是使用 ht[0]，所以当 rehash 执行完成后，虽然数据都在 ht[1] 中了，但 redis 仍然会把 ht[1] 赋值给 ht[0]，以便其他部分的代码逻辑正常使用\n 3. 在 ht[1] 赋值给 ht[0] 后，它的大小就会被重置为 0，等待下一次 rehash。与此同时， 全局哈希表中的 rehashidx 变量会被标为 -1，表示 rehash 结束了（这里的 rehashidx 变量用来表示 rehash 的进度，稍后我会给你具体解释）。\n\n\n\nint dictrehash(dict *d, int n) {\n    int empty_visits = n*10; /* max number of empty buckets to visit. */\n    if (!dictisrehashing(d)) return 0;\n\n    // 主循环，根据要拷贝的bucket数量n，循环n次后停止或ht[0]中的数据迁移完停止\n    while(n-- && d->ht[0].used != 0) {\n        dictentry *de, *nextde;\n\n        /* note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */\n        assert(d->ht[0].size > (unsigned long)d->rehashidx);\n        while(d->ht[0].table[d->rehashidx] == null) {\n            d->rehashidx++;\n            if (--empty_visits == 0) return 1;\n        }\n        de = d->ht[0].table[d->rehashidx];\n        /* move all the keys in this bucket from the old to the new hash ht */\n        while(de) {\n            uint64_t h;\n\n            nextde = de->next;\n            /* get the index in the new hash table */\n            h = dicthashkey(d, de->key) & d->ht[1].sizemask;\n            de->next = d->ht[1].table[h];\n            d->ht[1].table[h] = de;\n            d->ht[0].used--;\n            d->ht[1].used++;\n            de = nextde;\n        }\n        d->ht[0].table[d->rehashidx] = null;\n        d->rehashidx++;\n    }\n\n    //判断ht[0]的数据是否迁移完成\n    /* check if we already rehashed the whole table... */\n    if (d->ht[0].used == 0) {\n        // ht[0]迁移完后，释放ht[0]内存空间\n        zfree(d->ht[0].table);\n        // 让ht[0]指向ht[1]，以便接受正常的请求\n        d->ht[0] = d->ht[1];\n        // 重置ht[1]的大小为0\n        _dictreset(&d->ht[1]);\n        // 设置全局哈希表的rehashidx标识为-1，表示rehash结束\n        d->rehashidx = -1;\n        // 返回0，表示ht[0]中所有元素都迁移完\n        return 0;\n    }\n\n    //返回1，表示ht[0]中仍然有元素没有迁移完\n    /* more to rehash... */\n    return 1;\n}\n\n\n那么，渐进式 rehash 是如何按照 bucket 粒度拷贝数据的，这其实就和全局哈希表 dict 结构中的 rehashidx 变量相关了\n\nrehashidx 变量表示的是当前 rehash 在对哪个 bucket 做数据迁移。比如，当 rehashidx 等于 0 时，表示对 ht[0]中的第一个 bucket 进行数据迁移；当 rehashidx 等于 1 时，表 示对 ht[0] 中的第二个 bucket 进行数据迁移，以此类推。\n\n而 dictrehash 函数的主循环，首先会判断 rehashidx 指向的 bucket 是否为空，如果为空，那就将 rehashidx 的值加 1，检查下一个 bucket。\n\n那么，有没有可能连续几个 bucket 都为空呢？其实是有可能的，在这种情况下，渐进式 rehash 不会一直递增 rehashidx 进行检查。这是因为一旦执行了 rehash，redis 主线程就无法处理其他请求了。\n\n所以，渐进式 rehash 在执行时设置了一个变量 empty_visits，用来表示已经检查过的空 bucket，当检查了一定数量的空 bucket 后，这一轮的 rehash 就停止执行，转而继续处理外来请求，避免了对 redis 性能的影响。下面的代码显示了这部分逻辑，你可以看下。\n\n// 如果当前要迁移的 bucket 中没有元素\nwhile(d->ht[0].table[d->rehashidx] == null) {\n    d->rehashidx++;\n    if (--empty_visits == 0) return 1;\n}\n\n\n而如果 rehashidx 指向的 bucket 有数据可以迁移，那么 redis 就会把这个 bucket 中的哈希项依次取出来，并根据 ht[1] 的表空间大小，重新计算哈希项在 ht[1] 中的 bucket 位置，然后把这个哈希项赋值到 ht[1] 对应 bucket 中\n\n这样，每做完一个哈希项的迁移，ht[0] 和 ht[1] 用来表示承载哈希项多少的变量 used，就 会分别减一和加一。当然，如果当前 rehashidx 指向的 bucket 中数据都迁移完了， rehashidx 就会递增加 1，指向下一个 bucket。下面的代码显示了这一迁移过程。\n\n while(n-- && d->ht[0].used != 0) {\n     dictentry *de, *nextde;\n\n     /* note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */\n     assert(d->ht[0].size > (unsigned long)d->rehashidx);\n     while(d->ht[0].table[d->rehashidx] == null) {\n         d->rehashidx++;\n         if (--empty_visits == 0) return 1;\n     }\n     // 获得哈希表中哈希项\n     de = d->ht[0].table[d->rehashidx];\n     /* move all the keys in this bucket from the old to the new hash ht */\n     while(de) {\n         uint64_t h;\n     \t// 获得同一个bucket中下一个哈希项\n         nextde = de->next;\n         /* get the index in the new hash table */\n         // 根据扩容后的哈希表ht[1]大小，计算当前哈希项在扩容后哈希表中的bucket位置\n         h = dicthashkey(d, de->key) & d->ht[1].sizemask;\n         // 将当前哈希项添加到扩容后的哈希表ht[1]中\n         de->next = d->ht[1].table[h];\n         d->ht[1].table[h] = de;\n         // 减少当前哈希表的哈希项个数\n         d->ht[0].used--;\n         // 增加扩容后哈希表的哈希项个数\n         d->ht[1].used++;\n         de = nextde;\n     }\n     // 如果当前bucket中已经没有哈希项了，将该bucket置为null\n     d->ht[0].table[d->rehashidx] = null;\n     // 将rehash加1，下一次将迁移下一个bucket中的元素\n     d->rehashidx++;\n }\n\n\n好了，到这里，我们就已经基本了解了 dictrehash 函数的全部逻辑。 现在我们知道，dictrehash 函数本身是按照 bucket 粒度执行哈希项迁移的，它内部执行的 bucket 迁移个数，主要由传入的循环次数变量 n 来决定。但凡 redis 要进行 rehash 操作，最终都会调用 dictrehash 函数。\n\n接下来，我们来学习和渐进式 rehash 相关的第二个关键函数 _dictrehashstep，这个函数实现了每次只对一个 bucket 执行 rehash。 从 redis 的源码中我们可以看到，一共会有 5 个函数通过调用 _dictrehashstep 函数，进而调用 dictrehash 函数，来执行 rehash，它们分别是：dictaddraw， dictgenericdelete，dictfind，dictgetrandomkey，dictgetsomekeys。\n\n其中，dictaddraw 和 dictgenericdelete 函数，分别对应了往 redis 中增加和删除键值对，而后三个函数则对应了在 redis 中进行查询操作。下图展示了这些函数间的调用关系：\n\n但你要注意，不管是增删查哪种操作，这 5 个函数调用的 _dictrehashstep 函数，给 dictrehash 传入的循环次数变量 n 的值都为 1，下面的代码就显示了这一传参的情况\n\nstatic void _dictrehashstep(dict *d) {\n    // 给dictrehash传入的循环次数参数为1，表明每迁移完一个bucket ，就执行正常操作\n    if (d->pauserehash == 0) dictrehash(d,1);\n}\n\n\n这样一来，每次迁移完一个 bucket，hash 表就会执行正常的增删查请求操作，这就是在代码层面实现渐进式 rehash 的方法\n\n\n# 总结\n\n 1. 通过「链表」解决hash冲突\n 2. redis 通过 「渐进式 rehash」 来解决大量数据 rehash 可能会导致的阻塞问题\n 3. 渐进式 rehash 按照 bucket 粒度拷贝数据的方法\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"ZSet 设计与实现",frontmatter:{title:"ZSet 设计与实现",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d4312/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/02.%E5%9F%BA%E7%A1%80/10.ZSet%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.html",relativePath:"Redis 系统设计/02.基础/10.ZSet 设计与实现.md",key:"v-d9d599d2",path:"/pages/2d4312/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:244},{level:2,title:"Sorted Set 基本结构",slug:"sorted-set-基本结构",normalizedTitle:"sorted set 基本结构",charIndex:769},{level:2,title:"跳表的设计与实现",slug:"跳表的设计与实现",normalizedTitle:"跳表的设计与实现",charIndex:1618},{level:3,title:"跳表数据结构",slug:"跳表数据结构",normalizedTitle:"跳表数据结构",charIndex:2124},{level:3,title:"跳表结点查询",slug:"跳表结点查询",normalizedTitle:"跳表结点查询",charIndex:3619},{level:3,title:"跳表结点层数设置",slug:"跳表结点层数设置",normalizedTitle:"跳表结点层数设置",charIndex:4325},{level:2,title:"哈希表和跳表的组合使用",slug:"哈希表和跳表的组合使用",normalizedTitle:"哈希表和跳表的组合使用",charIndex:6213},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:8982},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:9771}],headersStr:"前言 Sorted Set 基本结构 跳表的设计与实现 跳表数据结构 跳表结点查询 跳表结点层数设置 哈希表和跳表的组合使用 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. Sorted Set 如何高效支持范围查询和单点查询？\n 2. Redis 为什么使用跳表而非红黑树实现有序集合？\n 3. 跳表的多层链表如何优化查找性能？\n 4. 跳表中随机生成结点层数有什么优势？如何影响插入和查询效率？\n 5. 跳表和哈希表在 Sorted Set 中如何协同工作保持数据一致性？\n 6. 若哈希表和跳表数据不一致，Sorted Set 是否还能高效查询？\n 7. 跳表和哈希表组合的设计对其他数据结构和系统开发的启示？\n\n\n# 前言\n\n有序集合（Sorted Set）：它本身是集合类型，同时也可以支持集合中的元素带有权重，并按权重排序\n\n但是，为什么 Sorted Set 能同时提供以下两种操作接口，以及它们的复杂度分别是 O(logN)+M 和 O(1) 呢？\n\n * ZRANGEBYSCORE：按照元素权重返回一个范围内的元素\n * ZSCORE：返回某个元素的权重值\n\n实际上，这个问题背后的本质是：为什么 Sorted Set 既能支持高效的范围查询，同时还能以 O(1) 复杂度获取元素权重值？\n\n这其实就和 Sorted Set 底层的设计实现有关了\n\n * Sorted Set 能支持范围查询，这是因为它的核心数据结构设计采用了跳表\n * 它又能以常数复杂度获取元素权重，这是因为它同时采用了哈希表进行索引\n\n那么，你是不是很好奇，Sorted Set 是如何把这两种数据结构结合在一起的？它们又是如何进行协作的呢？\n\n让 echo 来给你介绍下 Sorted Set 采用的双索引的设计思想和实现。理解和掌握这种双索引的设计思想，对于我们实现数据库系统是具有非常重要的参考价值的。\n\n好，接下来，我们就先来看看 Sorted Set 的基本结构\n\n\n# Sorted Set 基本结构\n\n> Redis 源码中，Sorted Set 的代码文件和其他数据类型不太一样，它并不像哈希表的 dict.c/dict.h，或是压缩列表的 ziplist.c/ziplist.h，具有专门的数据结构实现和定义文件\n> \n> Sorted Set 的实现代码在 t_zset.c 文件中，包括 Sorted Set 的各种操作实现，同时 Sorted Set 相关的结构定义在server.h文件中。如果你想要了解学习 Sorted Set 的模块和操作，注意要从 t_zset.c 和 server.h 这两个文件中查找\n\n我们可以先来看下它的结构定义。Sorted Set 结构体的名称为 zset，其中包含了两个成员，分别是哈希表 dict 和跳表 zsl，如下所示。\n\ntypedef struct zset {\n    dict *dict;\n    zskiplist *zsl;\n} zset;\n\n\nSorted Set 这种同时采用跳表和哈希表两个索引结构的设计思想。这种设计思想充分融合了：\n\n * 跳表高效支持范围查询（如 ZRANGEBYSCORE 操作）\n * 以及哈希表高效支持单点查询（如 ZSCORE 操作）的特征\n\n这样一来，我们就可以在一个数据结构中，同时高效支持范围查询和单点查询，这是单一索引结构比较难达到的效果\n\n提示\n\n感觉很多数据结构如果要支持 O(1)的查询复杂度的话，底层都得用 hash，比如常规 LRU 算法下的也有 hash 的影子 LRU 算法\n\n既然 Sorted Set 采用了跳表和哈希表两种索引结构来组织数据，我们在实现 Sorted Set 时就会面临以下两个问题：\n\n * 跳表或是哈希表中，各自保存了什么样的数据？\n * 跳表和哈希表保存的数据是如何保持一致的？\n\n因为我已经在 Hash 中给你介绍了 Redis 中哈希表的实现思路，所以接下来，echo 给你介绍下跳表的设计和实现\n\n\n# 跳表的设计与实现\n\n首先，我们来了解下什么是跳表（skiplist）。\n\n「跳表」其实是一种多层的有序链表\n\n> 为了便于说明，我把跳表中的层次从低到高排个序，最底下一层称为 level0，依次往上是 level1、level2 等\n\n下图展示的是一个 3 层的跳表。其中，头结点中包含了三个指针，分别作为 leve0 到 level2 上的头指针。\n\n\n\n可以看到，在 level 0 上一共有 7 个结点，分别是 3、11、23、33、42、51、62，这些结点会通过指针连接起来，同时头结点中的 level0 指针会指向结点 3。然后，在这 7 个结点中，结点 11、33 和 51 又都包含了一个指针，同样也依次连接起来，且头结点的 level 1 指针会指向结点 11。这样一来，这 3 个结点就组成了 level 1 上的所有结点。\n\n最后，结点 33 中还包含了一个指针，这个指针会指向尾结点，同时，头结点的 level 2 指针会指向结点 33，这就形成了 level 2，只不过 level 2 上只有 1 个结点 33。\n\n在对跳表有了直观印象后，我们再来看看跳表实现的具体数据结构\n\n\n# 跳表数据结构\n\n我们先来看下跳表结点的结构定义\n\ntypedef struct zskiplistNode {\n    //Sorted Set中的元素\n    sds ele;\n    //元素权重值\n    double score;\n    //后向指针\n    struct zskiplistNode *backward;\n    //节点的level数组，保存每层上的前向指针和跨度\n    struct zskiplistLevel {\n        struct zskiplistNode *forward;\n        unsigned long span;\n    } level[];\n} zskiplistNode;\n\n\n * 因为 Sorted Set 中既要保存元素，也要保存元素的权重，所以对应到跳表结点的结构定义中，就对应了 sds 类型的变量 ele，以及 double 类型的变量 score。此外，为了便于从跳表的尾结点进行倒序查找，每个跳表结点中还保存了一个后向指针（*backward），指向该结点的前一个结点。\n * 因为跳表是一个多层的有序链表，每一层也是由多个结点通过指针连接起来的。因此在跳表结点的结构定义中，还包含了一个 zskiplistLevel 结构体类型的 level 数组。\n\nlevel 数组中的每一个元素对应了一个 zskiplistLevel 结构体，也对应了跳表的一层。而 zskiplistLevel 结构体定义了一个指向下一结点的前向指针（*forward），这就使得结点可以在某一层上和后续结点连接起来。同时，zskiplistLevel 结构体中还定义了，这是用来记录结点在某一层上的 跨度 *forward 指针和该指针指向的结点之间，跨越了 level0 上的几个结点。\n\n我们来看下面这张图，其中就展示了 33 结点的 level 数组和跨度情况。可以看到，33 结点的 level 数组有三个元素，分别对应了三层 level 上的指针。此外，在 level 数组中，level 2、level1 和 level 0 的跨度 span 值依次是 3、2、1。\n\n\n\n最后，因为跳表中的结点都是按序排列的，所以，对于跳表中的某个结点，我们可以把从头结点到该结点的查询路径上，各个结点在所查询层次上的*forward 指针跨度，做一个累加。这个累加值就可以用来计算该结点在整个跳表中的顺序，另外这个结构特点还可以用来实现 Sorted Set 的 rank 操作，比如 ZRANK、ZREVRANK 等。\n\n了解了跳表结点的定义后，我们可以来看看跳表的定义。在跳表的结构中，定义了跳表的头结点和尾结点、跳表的长度，以及跳表的最大层数\n\ntypedef struct zskiplist {\n    struct zskiplistNode *header, *tail;\n    unsigned long length;\n    int level;\n} zskiplist;\n\n\n因为跳表的每个结点都是通过指针连接起来的，所以我们在使用跳表时，只需要从跳表结构体中获得头结点或尾结点，就可以通过结点指针访问到跳表中的各个结点\n\n那么，当我们在 Sorted Set 中查找元素时，就对应到了 Redis 在跳表中查找结点，而此时，查询代码是否需要像查询常规链表那样，逐一顺序查询比较链表中的每个结点呢？\n\n其实是不用的，因为这里的查询代码，可以使用跳表结点中的 level 数组来加速查询\n\n\n# 跳表结点查询\n\n事实上，当查询一个结点时，跳表会先从头结点的最高层开始，查找下一个结点。而由于跳表结点同时保存了元素和权重，所以跳表在比较结点时，相应地有两个判断条件：\n\n 1. 当查找到的结点保存的元素权重，比要查找的权重小时，跳表就会继续访问该层上的下一个结点。\n 2. 当查找到的结点保存的元素权重，等于要查找的权重时，跳表会再检查该结点保存的 SDS 类型数据，是否比要查找的 SDS 数据小。如果结点数据小于要查找的数据时，跳表仍然会继续访问该层上的下一个结点。\n\n但是，当上述两个条件都不满足时，跳表就会用到当前查找到的结点的 level 数组了。跳表会使用当前结点 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。\n\n这部分的代码逻辑如下所示，因为在跳表中进行查找、插入、更新或删除操作时，都需要用到查询的功能，你可以重点了解下。\n\n//获取跳表的表头\nx = zsl->header;\n//从最大层数开始逐一遍历\nfor (i = zsl->level-1; i >= 0; i--) {\n   ...\n   while (x->level[i].forward && (x->level[i].forward->score < score || (x->level[i].forward->score == score\n    && sdscmp(x->level[i].forward->ele,ele) < 0))) {\n      ...\n      x = x->level[i].forward;\n    }\n    ...\n}\n\n\n\n# 跳表结点层数设置\n\n这样一来，有了 level 数组之后，一个跳表结点就可以在多层上被访问到了。而一个结点的 level 数组的层数也就决定了，该结点可以在几层上被访问到。\n\n所以，当我们要决定结点层数时，实际上是要决定 level 数组具体有几层。\n\n一种设计方法是，让每一层上的结点数约是下一层上结点数的一半，就像下面这张图展示的。第 0 层上的结点数是 7，第 1 层上的结点数是 3，约是第 0 层上结点数的一半。而第 2 层上的结点就 33 一个，约是第 1 层结点数的一半。\n\n\n\n这种设计方法带来的好处是，当跳表从最高层开始进行查找时，由于每一层结点数都约是下一层结点数的一半，这种查找过程就类似于二分查找，查找复杂度可以降低到 O(logN)。\n\n但这种设计方法也会带来负面影响，那就是为了维持相邻两层上结点数的比例为 2:1，一旦有新的结点插入或是有结点被删除，那么插入或删除处的结点，及其后续结点的层数都需要进行调整，而这样就带来了额外的开销。\n\n我先来给你举个例子，看下不维持结点数比例的影响，这样虽然可以不调整层数，但是会增加查询复杂度。\n\n首先，假设当前跳表有 3 个结点，其数值分别是 3、11、23，如下图所示。\n\n\n\n接着，假设现在要插入一个结点 15，如果我们不调整其他结点的层数，而是直接插入结点 15 的话，那么插入后，跳表 level 0 和 level 1 两层上的结点数比例就变成了为 4:1，如下图所示。\n\n\n\n而假设我们持续插入多个结点，但是仍然不调整其他结点的层数，这样一来，level0 上的结点数就会越来越多，如下图所示。\n\n\n\n相应的，如果我们要查找大于 11 的结点，就需要在 level 0 的结点中依次顺序查找，复杂度就是 O(N) 了。所以，为了降低查询复杂度，我们就需要维持相邻层结点数间的关系。\n\n好，接下来，我们再来看下维持相邻层结点数为 2:1 时的影响。\n\n比如，我们可以把结点 23 的 level 数组中增加一层指针，如下图所示。这样一来，level 0 和 level 1 上的结点数就维持在了 2:1。但相应的代价就是，我们也需要给 level 数组重新分配空间，以便增加一层指针。\n\n\n\n类似的，如果我们要在有 7 个结点的跳表中删除结点 33，那么结点 33 后面的所有结点都要进行调整：\n\n\n\n调整后的跳表如下图所示。你可以看到，结点 42 和 62 都要新增 level 数组空间，这样能分别保存 3 层的指针和 2 层的指针，而结点 51 的 level 数组则需要减少一层。也就是说，这样的调整会带来额外的操作开销。\n\n\n\n因此，为了避免上述问题，跳表在创建结点时，采用的是另一种设计方法，即随机生成每个结点的层数。此时，相邻两层链表上的结点数并不需要维持在严格的 2:1 关系。这样一来，当新插入一个结点时，只需要修改前后结点的指针，而其他结点的层数就不需要随之改变了，这就降低了插入操作的复杂度。\n\n在 Redis 源码中，跳表结点层数是由 zslRandomLevel 函数决定。zslRandomLevel 函数会把层数初始化为 1，这也是结点的最小层数。然后，该函数会生成随机数，如果随机数的值小于 ZSKIPLIST_P（指跳表结点增加层数的概率，值为 0.25），那么层数就增加 1 层。因为随机数取值到[0,0.25) 范围内的概率不超过 25%，所以这也就表明了，每增加一层的概率不超过 25%。下面的代码展示了 zslRandomLevel 函数的执行逻辑，你可以看下。\n\n#define ZSKIPLIST_MAXLEVEL 64  //最大层数为64\n#define ZSKIPLIST_P 0.25       //随机数的值为0.25\nint zslRandomLevel(void) {\n    //初始化层为1\n    int level = 1;\n    while ((random()&0xFFFF) < (ZSKIPLIST_P * 0xFFFF))\n        level += 1;\n    return (level<ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;\n}\n\n\n好，现在我们就了解了跳表的基本结构、查询方式和结点层数设置方法，那么下面我们接着来学习下，Sorted Set 中是如何将跳表和哈希表组合起来使用的，以及是如何保持这两个索引结构中的数据是一致的。\n\n\n# 哈希表和跳表的组合使用\n\ntypedef struct zset {\n    dict *dict;\n    zskiplist *zsl;\n} zset;\n\n\nSorted Set 中已经同时包含了 hash 和 skiplist，这就是组合使用两者的第一步。然后，我们还可以在 Sorted Set 的创建代码（t_zset.c文件）中，进一步看到跳表和哈希表被相继创建\n\n当创建一个 zset 时，代码中会相继调用 **dictCreate 函数 **创建 zset 中的哈希表，以及调用 **zslCreate 函数 **创建跳表，如下所示。\n\n zs = zmalloc(sizeof(*zs));\n zs->dict = dictCreate(&zsetDictType,NULL);\n zs->zsl = zslCreate();\n\n\n我们要想组合使用它们，必须保持这两个索引结构中的数据一致。简单来说，这就需要我们在往跳表中插入数据时，同时也向哈希表中插入数据。\n\n而这种保持两个索引结构一致的做法其实也不难，当往 Sorted Set 中插入数据时，zsetAdd 函数就会被调用。所以，我们可以通过阅读 Sorted Set 的元素添加函数 zsetAdd 了解到。下面我们就来分析一下 zsetAdd 函数的执行过程。\n\n首先，zsetAdd 函数会判定 Sorted Set 采用的是 ziplist 还是 skiplist 的编码方式。zsetAdd 函数会判定 Sorted Set 采用的是 ziplist 还是 skiplist 的编码方式。\n\n注意，在不同编码方式下，zsetAdd 函数的执行逻辑也有所区别。这一讲我们重点关注的是 skiplist 的编码方式，所以接下来，我们就主要来看看当采用 skiplist 编码方式时，zsetAdd 函数的逻辑是什么样的。\n\nzsetAdd 函数会先使用哈希表的 dictFind 函数，查找要插入的元素是否存在。如果不存在，就直接调用跳表元素插入函数 zslInsert 和哈希表元素插入函数 dictAdd，将新元素分别插入到跳表和哈希表中。\n\n这里你需要注意的是，Redis 并没有把哈希表的操作嵌入到跳表本身的操作函数中，而是在 zsetAdd 函数中依次执行以上两个函数。这样设计的好处是保持了跳表和哈希表两者操作的独立性。\n\n * 然后，如果 zsetAdd 函数通过 dictFind 函数发现要插入的元素已经存在，那么 zsetAdd 函数会判断是否要增加元素的权重值\n\n如果权重值发生了变化，zsetAdd 函数就会调用 zslUpdateScore 函数，更新跳表中的元素权重值。紧接着，zsetAdd 函数会把哈希表中该元素（对应哈希表中的 key）的 value 指向跳表结点中的权重值，这样一来，哈希表中元素的权重值就可以保持最新值了。\n\n下面的代码显示了 zsetAdd 函数的执行流程，你可以看下。\n\n //如果采用ziplist编码方式时，zsetAdd函数的处理逻辑\n if (zobj->encoding == OBJ_ENCODING_ZIPLIST) {\n   ...\n}\n//如果采用skiplist编码方式时，zsetAdd函数的处理逻辑\nelse if (zobj->encoding == OBJ_ENCODING_SKIPLIST) {\n        zset *zs = zobj->ptr;\n        zskiplistNode *znode;\n        dictEntry *de;\n        //从哈希表中查询新增元素\n        de = dictFind(zs->dict,ele);\n        //如果能查询到该元素\n        if (de != NULL) {\n            /* NX? Return, same element already exists. */\n            if (nx) {\n                *flags |= ZADD_NOP;\n                return 1;\n            }\n            //从哈希表中查询元素的权重\n            curscore = *(double*)dictGetVal(de);\n\n\n            //如果要更新元素权重值\n            if (incr) {\n                //更新权重值\n               ...\n            }\n\n\n            //如果权重发生变化了\n            if (score != curscore) {\n                //更新跳表结点\n                znode = zslUpdateScore(zs->zsl,curscore,ele,score);\n                //让哈希表元素的值指向跳表结点的权重\n                dictGetVal(de) = &znode->score;\n                ...\n            }\n            return 1;\n        }\n       //如果新元素不存在\n        else if (!xx) {\n            ele = sdsdup(ele);\n            //新插入跳表结点\n            znode = zslInsert(zs->zsl,score,ele);\n            //新插入哈希表元素\n            serverAssert(dictAdd(zs->dict,ele,&znode->score) == DICT_OK);\n            ...\n            return 1;\n        }\n        ..\n\n\n总之，你可以记住的是，Sorted Set 先是通过在它的数据结构中同时定义了跳表和哈希表，来实现同时使用这两种索引结构。然后，Sorted Set 在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。\n\n这样一来，Sorted Set 既可以使用跳表支持数据的范围查询，还能使用哈希表支持根据元素直接查询它的权重。\n\n\n# 总结\n\n 1. Sorted Set 的设计目标 Redis 的 Sorted Set 数据类型需要同时支持两种查询需求：\n    * 范围查询：根据元素的权重范围进行查询。\n    * 单点查询：快速查找特定元素及其权重。\n 2. 跳表的设计\n    * 跳表结构：跳表是一个多层的有序链表，顶层结点数最少，底层结点数最多。\n    * 查询过程：查询时，从顶层开始，通过高层节点大跨度跳跃查找，如果找到第一个大于待查元素的结点，就转向下一层继续查找，直到找到待查元素。\n    * 优化查询效率：这种从高层到低层的分层查找方式，极大地减少了查询的时间开销，相比普通链表的线性查找，跳表的查询效率更高。\n    * 随机层数：跳表采用随机算法确定每个结点的层数，避免新增结点时发生连锁更新，提高插入效率。\n 3. 哈希表的引入\n    * 哈希表作为索引：Sorted Set 还将每个元素保存在哈希表中，元素作为哈希表的 key，其权重作为 value。\n    * 单点查询效率提升：通过哈希表可以直接查找到特定元素及其权重，相较于跳表的范围查找，哈希表的查找效率更高，更适合针对单个元素的查询。\n 4. 组合索引设计\n    * 跳表 + 哈希表：Redis Sorted Set 通过组合使用跳表和哈希表两种数据结构，实现了既支持范围查询（跳表），又能快速进行单点查询（哈希表）的设计。\n    * 设计优势：这种组合设计使得 Redis Sorted Set 能在不同的查询场景下兼顾效率，既避免了跳表插入时的性能问题，又利用哈希表提升了单点查询速度。\n 5. 设计思路的应用\n    * 系统开发启示：在实际系统开发中，组合使用多种索引结构可以有效提升数据管理的效率。Redis Sorted Set 的设计思路是一个典型案例，值得在其他开发场景中借鉴。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. sorted set 如何高效支持范围查询和单点查询？\n 2. redis 为什么使用跳表而非红黑树实现有序集合？\n 3. 跳表的多层链表如何优化查找性能？\n 4. 跳表中随机生成结点层数有什么优势？如何影响插入和查询效率？\n 5. 跳表和哈希表在 sorted set 中如何协同工作保持数据一致性？\n 6. 若哈希表和跳表数据不一致，sorted set 是否还能高效查询？\n 7. 跳表和哈希表组合的设计对其他数据结构和系统开发的启示？\n\n\n# 前言\n\n有序集合（sorted set）：它本身是集合类型，同时也可以支持集合中的元素带有权重，并按权重排序\n\n但是，为什么 sorted set 能同时提供以下两种操作接口，以及它们的复杂度分别是 o(logn)+m 和 o(1) 呢？\n\n * zrangebyscore：按照元素权重返回一个范围内的元素\n * zscore：返回某个元素的权重值\n\n实际上，这个问题背后的本质是：为什么 sorted set 既能支持高效的范围查询，同时还能以 o(1) 复杂度获取元素权重值？\n\n这其实就和 sorted set 底层的设计实现有关了\n\n * sorted set 能支持范围查询，这是因为它的核心数据结构设计采用了跳表\n * 它又能以常数复杂度获取元素权重，这是因为它同时采用了哈希表进行索引\n\n那么，你是不是很好奇，sorted set 是如何把这两种数据结构结合在一起的？它们又是如何进行协作的呢？\n\n让 echo 来给你介绍下 sorted set 采用的双索引的设计思想和实现。理解和掌握这种双索引的设计思想，对于我们实现数据库系统是具有非常重要的参考价值的。\n\n好，接下来，我们就先来看看 sorted set 的基本结构\n\n\n# sorted set 基本结构\n\n> redis 源码中，sorted set 的代码文件和其他数据类型不太一样，它并不像哈希表的 dict.c/dict.h，或是压缩列表的 ziplist.c/ziplist.h，具有专门的数据结构实现和定义文件\n> \n> sorted set 的实现代码在 t_zset.c 文件中，包括 sorted set 的各种操作实现，同时 sorted set 相关的结构定义在server.h文件中。如果你想要了解学习 sorted set 的模块和操作，注意要从 t_zset.c 和 server.h 这两个文件中查找\n\n我们可以先来看下它的结构定义。sorted set 结构体的名称为 zset，其中包含了两个成员，分别是哈希表 dict 和跳表 zsl，如下所示。\n\ntypedef struct zset {\n    dict *dict;\n    zskiplist *zsl;\n} zset;\n\n\nsorted set 这种同时采用跳表和哈希表两个索引结构的设计思想。这种设计思想充分融合了：\n\n * 跳表高效支持范围查询（如 zrangebyscore 操作）\n * 以及哈希表高效支持单点查询（如 zscore 操作）的特征\n\n这样一来，我们就可以在一个数据结构中，同时高效支持范围查询和单点查询，这是单一索引结构比较难达到的效果\n\n提示\n\n感觉很多数据结构如果要支持 o(1)的查询复杂度的话，底层都得用 hash，比如常规 lru 算法下的也有 hash 的影子 lru 算法\n\n既然 sorted set 采用了跳表和哈希表两种索引结构来组织数据，我们在实现 sorted set 时就会面临以下两个问题：\n\n * 跳表或是哈希表中，各自保存了什么样的数据？\n * 跳表和哈希表保存的数据是如何保持一致的？\n\n因为我已经在 hash 中给你介绍了 redis 中哈希表的实现思路，所以接下来，echo 给你介绍下跳表的设计和实现\n\n\n# 跳表的设计与实现\n\n首先，我们来了解下什么是跳表（skiplist）。\n\n「跳表」其实是一种多层的有序链表\n\n> 为了便于说明，我把跳表中的层次从低到高排个序，最底下一层称为 level0，依次往上是 level1、level2 等\n\n下图展示的是一个 3 层的跳表。其中，头结点中包含了三个指针，分别作为 leve0 到 level2 上的头指针。\n\n\n\n可以看到，在 level 0 上一共有 7 个结点，分别是 3、11、23、33、42、51、62，这些结点会通过指针连接起来，同时头结点中的 level0 指针会指向结点 3。然后，在这 7 个结点中，结点 11、33 和 51 又都包含了一个指针，同样也依次连接起来，且头结点的 level 1 指针会指向结点 11。这样一来，这 3 个结点就组成了 level 1 上的所有结点。\n\n最后，结点 33 中还包含了一个指针，这个指针会指向尾结点，同时，头结点的 level 2 指针会指向结点 33，这就形成了 level 2，只不过 level 2 上只有 1 个结点 33。\n\n在对跳表有了直观印象后，我们再来看看跳表实现的具体数据结构\n\n\n# 跳表数据结构\n\n我们先来看下跳表结点的结构定义\n\ntypedef struct zskiplistnode {\n    //sorted set中的元素\n    sds ele;\n    //元素权重值\n    double score;\n    //后向指针\n    struct zskiplistnode *backward;\n    //节点的level数组，保存每层上的前向指针和跨度\n    struct zskiplistlevel {\n        struct zskiplistnode *forward;\n        unsigned long span;\n    } level[];\n} zskiplistnode;\n\n\n * 因为 sorted set 中既要保存元素，也要保存元素的权重，所以对应到跳表结点的结构定义中，就对应了 sds 类型的变量 ele，以及 double 类型的变量 score。此外，为了便于从跳表的尾结点进行倒序查找，每个跳表结点中还保存了一个后向指针（*backward），指向该结点的前一个结点。\n * 因为跳表是一个多层的有序链表，每一层也是由多个结点通过指针连接起来的。因此在跳表结点的结构定义中，还包含了一个 zskiplistlevel 结构体类型的 level 数组。\n\nlevel 数组中的每一个元素对应了一个 zskiplistlevel 结构体，也对应了跳表的一层。而 zskiplistlevel 结构体定义了一个指向下一结点的前向指针（*forward），这就使得结点可以在某一层上和后续结点连接起来。同时，zskiplistlevel 结构体中还定义了，这是用来记录结点在某一层上的 跨度 *forward 指针和该指针指向的结点之间，跨越了 level0 上的几个结点。\n\n我们来看下面这张图，其中就展示了 33 结点的 level 数组和跨度情况。可以看到，33 结点的 level 数组有三个元素，分别对应了三层 level 上的指针。此外，在 level 数组中，level 2、level1 和 level 0 的跨度 span 值依次是 3、2、1。\n\n\n\n最后，因为跳表中的结点都是按序排列的，所以，对于跳表中的某个结点，我们可以把从头结点到该结点的查询路径上，各个结点在所查询层次上的*forward 指针跨度，做一个累加。这个累加值就可以用来计算该结点在整个跳表中的顺序，另外这个结构特点还可以用来实现 sorted set 的 rank 操作，比如 zrank、zrevrank 等。\n\n了解了跳表结点的定义后，我们可以来看看跳表的定义。在跳表的结构中，定义了跳表的头结点和尾结点、跳表的长度，以及跳表的最大层数\n\ntypedef struct zskiplist {\n    struct zskiplistnode *header, *tail;\n    unsigned long length;\n    int level;\n} zskiplist;\n\n\n因为跳表的每个结点都是通过指针连接起来的，所以我们在使用跳表时，只需要从跳表结构体中获得头结点或尾结点，就可以通过结点指针访问到跳表中的各个结点\n\n那么，当我们在 sorted set 中查找元素时，就对应到了 redis 在跳表中查找结点，而此时，查询代码是否需要像查询常规链表那样，逐一顺序查询比较链表中的每个结点呢？\n\n其实是不用的，因为这里的查询代码，可以使用跳表结点中的 level 数组来加速查询\n\n\n# 跳表结点查询\n\n事实上，当查询一个结点时，跳表会先从头结点的最高层开始，查找下一个结点。而由于跳表结点同时保存了元素和权重，所以跳表在比较结点时，相应地有两个判断条件：\n\n 1. 当查找到的结点保存的元素权重，比要查找的权重小时，跳表就会继续访问该层上的下一个结点。\n 2. 当查找到的结点保存的元素权重，等于要查找的权重时，跳表会再检查该结点保存的 sds 类型数据，是否比要查找的 sds 数据小。如果结点数据小于要查找的数据时，跳表仍然会继续访问该层上的下一个结点。\n\n但是，当上述两个条件都不满足时，跳表就会用到当前查找到的结点的 level 数组了。跳表会使用当前结点 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。\n\n这部分的代码逻辑如下所示，因为在跳表中进行查找、插入、更新或删除操作时，都需要用到查询的功能，你可以重点了解下。\n\n//获取跳表的表头\nx = zsl->header;\n//从最大层数开始逐一遍历\nfor (i = zsl->level-1; i >= 0; i--) {\n   ...\n   while (x->level[i].forward && (x->level[i].forward->score < score || (x->level[i].forward->score == score\n    && sdscmp(x->level[i].forward->ele,ele) < 0))) {\n      ...\n      x = x->level[i].forward;\n    }\n    ...\n}\n\n\n\n# 跳表结点层数设置\n\n这样一来，有了 level 数组之后，一个跳表结点就可以在多层上被访问到了。而一个结点的 level 数组的层数也就决定了，该结点可以在几层上被访问到。\n\n所以，当我们要决定结点层数时，实际上是要决定 level 数组具体有几层。\n\n一种设计方法是，让每一层上的结点数约是下一层上结点数的一半，就像下面这张图展示的。第 0 层上的结点数是 7，第 1 层上的结点数是 3，约是第 0 层上结点数的一半。而第 2 层上的结点就 33 一个，约是第 1 层结点数的一半。\n\n\n\n这种设计方法带来的好处是，当跳表从最高层开始进行查找时，由于每一层结点数都约是下一层结点数的一半，这种查找过程就类似于二分查找，查找复杂度可以降低到 o(logn)。\n\n但这种设计方法也会带来负面影响，那就是为了维持相邻两层上结点数的比例为 2:1，一旦有新的结点插入或是有结点被删除，那么插入或删除处的结点，及其后续结点的层数都需要进行调整，而这样就带来了额外的开销。\n\n我先来给你举个例子，看下不维持结点数比例的影响，这样虽然可以不调整层数，但是会增加查询复杂度。\n\n首先，假设当前跳表有 3 个结点，其数值分别是 3、11、23，如下图所示。\n\n\n\n接着，假设现在要插入一个结点 15，如果我们不调整其他结点的层数，而是直接插入结点 15 的话，那么插入后，跳表 level 0 和 level 1 两层上的结点数比例就变成了为 4:1，如下图所示。\n\n\n\n而假设我们持续插入多个结点，但是仍然不调整其他结点的层数，这样一来，level0 上的结点数就会越来越多，如下图所示。\n\n\n\n相应的，如果我们要查找大于 11 的结点，就需要在 level 0 的结点中依次顺序查找，复杂度就是 o(n) 了。所以，为了降低查询复杂度，我们就需要维持相邻层结点数间的关系。\n\n好，接下来，我们再来看下维持相邻层结点数为 2:1 时的影响。\n\n比如，我们可以把结点 23 的 level 数组中增加一层指针，如下图所示。这样一来，level 0 和 level 1 上的结点数就维持在了 2:1。但相应的代价就是，我们也需要给 level 数组重新分配空间，以便增加一层指针。\n\n\n\n类似的，如果我们要在有 7 个结点的跳表中删除结点 33，那么结点 33 后面的所有结点都要进行调整：\n\n\n\n调整后的跳表如下图所示。你可以看到，结点 42 和 62 都要新增 level 数组空间，这样能分别保存 3 层的指针和 2 层的指针，而结点 51 的 level 数组则需要减少一层。也就是说，这样的调整会带来额外的操作开销。\n\n\n\n因此，为了避免上述问题，跳表在创建结点时，采用的是另一种设计方法，即随机生成每个结点的层数。此时，相邻两层链表上的结点数并不需要维持在严格的 2:1 关系。这样一来，当新插入一个结点时，只需要修改前后结点的指针，而其他结点的层数就不需要随之改变了，这就降低了插入操作的复杂度。\n\n在 redis 源码中，跳表结点层数是由 zslrandomlevel 函数决定。zslrandomlevel 函数会把层数初始化为 1，这也是结点的最小层数。然后，该函数会生成随机数，如果随机数的值小于 zskiplist_p（指跳表结点增加层数的概率，值为 0.25），那么层数就增加 1 层。因为随机数取值到[0,0.25) 范围内的概率不超过 25%，所以这也就表明了，每增加一层的概率不超过 25%。下面的代码展示了 zslrandomlevel 函数的执行逻辑，你可以看下。\n\n#define zskiplist_maxlevel 64  //最大层数为64\n#define zskiplist_p 0.25       //随机数的值为0.25\nint zslrandomlevel(void) {\n    //初始化层为1\n    int level = 1;\n    while ((random()&0xffff) < (zskiplist_p * 0xffff))\n        level += 1;\n    return (level<zskiplist_maxlevel) ? level : zskiplist_maxlevel;\n}\n\n\n好，现在我们就了解了跳表的基本结构、查询方式和结点层数设置方法，那么下面我们接着来学习下，sorted set 中是如何将跳表和哈希表组合起来使用的，以及是如何保持这两个索引结构中的数据是一致的。\n\n\n# 哈希表和跳表的组合使用\n\ntypedef struct zset {\n    dict *dict;\n    zskiplist *zsl;\n} zset;\n\n\nsorted set 中已经同时包含了 hash 和 skiplist，这就是组合使用两者的第一步。然后，我们还可以在 sorted set 的创建代码（t_zset.c文件）中，进一步看到跳表和哈希表被相继创建\n\n当创建一个 zset 时，代码中会相继调用 **dictcreate 函数 **创建 zset 中的哈希表，以及调用 **zslcreate 函数 **创建跳表，如下所示。\n\n zs = zmalloc(sizeof(*zs));\n zs->dict = dictcreate(&zsetdicttype,null);\n zs->zsl = zslcreate();\n\n\n我们要想组合使用它们，必须保持这两个索引结构中的数据一致。简单来说，这就需要我们在往跳表中插入数据时，同时也向哈希表中插入数据。\n\n而这种保持两个索引结构一致的做法其实也不难，当往 sorted set 中插入数据时，zsetadd 函数就会被调用。所以，我们可以通过阅读 sorted set 的元素添加函数 zsetadd 了解到。下面我们就来分析一下 zsetadd 函数的执行过程。\n\n首先，zsetadd 函数会判定 sorted set 采用的是 ziplist 还是 skiplist 的编码方式。zsetadd 函数会判定 sorted set 采用的是 ziplist 还是 skiplist 的编码方式。\n\n注意，在不同编码方式下，zsetadd 函数的执行逻辑也有所区别。这一讲我们重点关注的是 skiplist 的编码方式，所以接下来，我们就主要来看看当采用 skiplist 编码方式时，zsetadd 函数的逻辑是什么样的。\n\nzsetadd 函数会先使用哈希表的 dictfind 函数，查找要插入的元素是否存在。如果不存在，就直接调用跳表元素插入函数 zslinsert 和哈希表元素插入函数 dictadd，将新元素分别插入到跳表和哈希表中。\n\n这里你需要注意的是，redis 并没有把哈希表的操作嵌入到跳表本身的操作函数中，而是在 zsetadd 函数中依次执行以上两个函数。这样设计的好处是保持了跳表和哈希表两者操作的独立性。\n\n * 然后，如果 zsetadd 函数通过 dictfind 函数发现要插入的元素已经存在，那么 zsetadd 函数会判断是否要增加元素的权重值\n\n如果权重值发生了变化，zsetadd 函数就会调用 zslupdatescore 函数，更新跳表中的元素权重值。紧接着，zsetadd 函数会把哈希表中该元素（对应哈希表中的 key）的 value 指向跳表结点中的权重值，这样一来，哈希表中元素的权重值就可以保持最新值了。\n\n下面的代码显示了 zsetadd 函数的执行流程，你可以看下。\n\n //如果采用ziplist编码方式时，zsetadd函数的处理逻辑\n if (zobj->encoding == obj_encoding_ziplist) {\n   ...\n}\n//如果采用skiplist编码方式时，zsetadd函数的处理逻辑\nelse if (zobj->encoding == obj_encoding_skiplist) {\n        zset *zs = zobj->ptr;\n        zskiplistnode *znode;\n        dictentry *de;\n        //从哈希表中查询新增元素\n        de = dictfind(zs->dict,ele);\n        //如果能查询到该元素\n        if (de != null) {\n            /* nx? return, same element already exists. */\n            if (nx) {\n                *flags |= zadd_nop;\n                return 1;\n            }\n            //从哈希表中查询元素的权重\n            curscore = *(double*)dictgetval(de);\n\n\n            //如果要更新元素权重值\n            if (incr) {\n                //更新权重值\n               ...\n            }\n\n\n            //如果权重发生变化了\n            if (score != curscore) {\n                //更新跳表结点\n                znode = zslupdatescore(zs->zsl,curscore,ele,score);\n                //让哈希表元素的值指向跳表结点的权重\n                dictgetval(de) = &znode->score;\n                ...\n            }\n            return 1;\n        }\n       //如果新元素不存在\n        else if (!xx) {\n            ele = sdsdup(ele);\n            //新插入跳表结点\n            znode = zslinsert(zs->zsl,score,ele);\n            //新插入哈希表元素\n            serverassert(dictadd(zs->dict,ele,&znode->score) == dict_ok);\n            ...\n            return 1;\n        }\n        ..\n\n\n总之，你可以记住的是，sorted set 先是通过在它的数据结构中同时定义了跳表和哈希表，来实现同时使用这两种索引结构。然后，sorted set 在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。\n\n这样一来，sorted set 既可以使用跳表支持数据的范围查询，还能使用哈希表支持根据元素直接查询它的权重。\n\n\n# 总结\n\n 1. sorted set 的设计目标 redis 的 sorted set 数据类型需要同时支持两种查询需求：\n    * 范围查询：根据元素的权重范围进行查询。\n    * 单点查询：快速查找特定元素及其权重。\n 2. 跳表的设计\n    * 跳表结构：跳表是一个多层的有序链表，顶层结点数最少，底层结点数最多。\n    * 查询过程：查询时，从顶层开始，通过高层节点大跨度跳跃查找，如果找到第一个大于待查元素的结点，就转向下一层继续查找，直到找到待查元素。\n    * 优化查询效率：这种从高层到低层的分层查找方式，极大地减少了查询的时间开销，相比普通链表的线性查找，跳表的查询效率更高。\n    * 随机层数：跳表采用随机算法确定每个结点的层数，避免新增结点时发生连锁更新，提高插入效率。\n 3. 哈希表的引入\n    * 哈希表作为索引：sorted set 还将每个元素保存在哈希表中，元素作为哈希表的 key，其权重作为 value。\n    * 单点查询效率提升：通过哈希表可以直接查找到特定元素及其权重，相较于跳表的范围查找，哈希表的查找效率更高，更适合针对单个元素的查询。\n 4. 组合索引设计\n    * 跳表 + 哈希表：redis sorted set 通过组合使用跳表和哈希表两种数据结构，实现了既支持范围查询（跳表），又能快速进行单点查询（哈希表）的设计。\n    * 设计优势：这种组合设计使得 redis sorted set 能在不同的查询场景下兼顾效率，既避免了跳表插入时的性能问题，又利用哈希表提升了单点查询速度。\n 5. 设计思路的应用\n    * 系统开发启示：在实际系统开发中，组合使用多种索引结构可以有效提升数据管理的效率。redis sorted set 的设计思路是一个典型案例，值得在其他开发场景中借鉴。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Linux 中的 IO 多路复用",frontmatter:{title:"Linux 中的 IO 多路复用",date:"2024-09-15T17:16:08.000Z",permalink:"/pages/34fa27"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%BB%E7%BA%BF/01.Linux%20%E4%B8%AD%E7%9A%84%20IO%20%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.html",relativePath:"Redis 系统设计/03.主线/01.Linux 中的 IO 多路复用.md",key:"v-f7a1c94e",path:"/pages/34fa27/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:255},{level:3,title:"简述传统 Socket 模型",slug:"简述传统-socket-模型",normalizedTitle:"简述传统 socket 模型",charIndex:618},{level:3,title:"展望 IO 多路复用",slug:"展望-io-多路复用",normalizedTitle:"展望 io 多路复用",charIndex:2239},{level:2,title:"Linux 如何实现 IO 多路复用",slug:"linux-如何实现-io-多路复用",normalizedTitle:"linux 如何实现 io 多路复用",charIndex:2826},{level:3,title:"select 机制：多路复用的基本实现",slug:"select-机制-多路复用的基本实现",normalizedTitle:"select 机制：多路复用的基本实现",charIndex:2849},{level:3,title:"poll 机制：不受限的文件描述符数量",slug:"poll-机制-不受限的文件描述符数量",normalizedTitle:"poll 机制：不受限的文件描述符数量",charIndex:5632},{level:3,title:"epoll 机制：避免遍历每个描述符",slug:"epoll-机制-避免遍历每个描述符",normalizedTitle:"epoll 机制：避免遍历每个描述符",charIndex:7999},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:10290},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10774}],headersStr:"前言 简述传统 Socket 模型 展望 IO 多路复用 Linux 如何实现 IO 多路复用 select 机制：多路复用的基本实现 poll 机制：不受限的文件描述符数量 epoll 机制：避免遍历每个描述符 总结 参考资料",content:"提出问题是一切智慧的开端\n\n * 为什么 Redis 这样高并发的应用选择了 epoll，而不是 select 或 poll？\n\n * 在设计一个高并发服务器时，如何选择最适合的 IO 模型？\n\n * 当服务器面对成千上万的连接时，什么样的机制可以让你高效管理这些连接？\n\n * 高并发场景中，epoll 是如何通过事件通知机制避免性能瓶颈的？\n\n * 多路复用机制会监听套接字上的哪些事件？\n\n * 多路复用机制可以监听多少个套接字？\n\n * 当有套接字就绪时，多路复用机制要如何找到就绪的套接字？\n\n\n# 前言\n\nRedis 作为一个 Client-Server 架构的数据库，其源码中少不了用来实现网络通信的部分。通常系统实现网络通信的基本方法是 使用 Socket 编程模型，但是，由于基本的 Socket 编程模型是单线程阻塞的 ....\n\n所以当要处理高并发请求时，有两种方案\n\n * 多线程，让每个线程负责处理一个客户端的请求。而 Redis 负责客户端请求解析和处理的线程只有一个，那么如果直接采用基本 Socket 模型，就会影响 Redis 支持高并发的客户端访问。\n * IO 多路复用，为了实现高并发的网络通信，我们常用的 Linux 操作系统，就提供了 select、poll 和 epoll 三种编程模型，而在 Linux 上运行的 Redis，通常就会采用其中的 epoll 模型来进行网络通信。\n\n\n# 简述传统 Socket 模型\n\n我们看下使用 Socket 模型实现网络通信时的关键操作，以此帮助我们分析 Socket 模型中的不足\n\n首先，当我们需要让服务器端和客户端进行通信时，可以在服务器端通过以下三步，来创建监听客户端连接的监听套接字（Listening Socket）：\n\n 1. 调用 socket 函数，创建一个套接字。我们通常把这个套接字称为主动套接字（Active Socket）\n 2. 调用 bind 函数，将主动套接字和当前服务器的 IP 和监听端口进行绑定\n 3. 调用 listen 函数，将主动套接字转换为监听套接字，开始监听客户端的连接\n 4. 调用 accept函数，在完成上述三步之后，服务器端就可以接收客户端的连接请求了。为了能及时地收到客户端的连接请求，我们可以运行一个循环流程，在该流程中调用 accept 函数，用于接收客户端连接请求\n 5. 最后，服务器端可以通过调用 recv 或 send 函数，在刚才返回的已连接套接字上，接收并处理读写请求，或是将数据发送给客户端\n\n注意\n\naccept 函数是阻塞函数，也就是说，如果此时一直没有客户端连接请求，那么，服务器端的执行流程会一直阻塞在 accept 函数。一旦有客户端连接请求到达，accept 将不再阻塞，而是处理连接请求，和客户端建立连接，并返回已连接套接字（Connected Socket）\n\nlistenSocket = socket(); //调用socket系统调用创建一个主动套接字\nbind(listenSocket);  //绑定地址和端口\nlisten(listenSocket); //将默认的主动套接字转换为服务器使用的被动套接字，也就是监听套接字\nwhile (1) { //循环监听是否有客户端连接请求到来\n   connSocket = accept(listenSocket); //接受客户端连接\n   recv(connsocket); //从客户端读取数据，只能同时处理一个客户端\n   send(connsocket); //给客户端返回数据，只能同时处理一个客户端\n}\n\n\n你会发现，虽然上述代码能够实现服务器端和客户端之间的通信，但是程序每调用一次 accept 函数，只能处理一个客户端连接\n\n因此，如果想要处理多个并发客户端的请求，我们就需要使用多线程的方法，来处理通过 accept 函数建立的多个客户端连接上的请求\n\n使用这种方法后，我们需要在 accept 函数返回已连接套接字后，创建一个线程，并将已连接套接字传递给创建的线程，由该线程负责这个连接套接字上后续的数据读写。同时，服务器端的执行流程会再次调用 accept 函数，等待下一个客户端连接\n\n以下给出的示例代码，就展示了使用多线程来提升服务器端的并发客户端处理能力：\n\nlistenSocket = socket(); //调用socket系统调用创建一个主动套接字\nbind(listenSocket);  //绑定地址和端口\nlisten(listenSocket); //将默认的主动套接字转换为服务器使用的被动套接字，即监听套接字\nwhile (1) { //循环监听是否有客户端连接到来\n   connSocket = accept(listenSocket); //接受客户端连接，返回已连接套接字\n   pthread_create(processData, connSocket); //创建新线程对已连接套接字进行处理\n\n}\n\n//处理已连接套接字上的读写请求\nprocessData(connSocket){\n   recv(connsocket); //从客户端读取数据，只能同时处理一个客户端\n   send(connsocket); //给客户端返回数据，只能同时处理一个客户端\n}\n\n\n\n# 展望 IO 多路复用\n\n虽然上述方法能提升服务器端的并发处理能力，遗憾的是\n\nRedis 的主执行流程是由一个线程在执行，无法使用多线程的方式来提升并发处理能力。\n\n所以，该方法对 Redis 并不起作用。那么，还有没有什么其他方法，能帮助 Redis 提升并发客户端的处理能力呢？\n\n这就要用到操作系统提供的 IO 多路复用功能了\n\n在基本的 Socket 编程模型中，accept 函数只能在一个监听套接字上监听客户端的连接，recv 函数也只能在一个已连接套接字上，等待客户端发送的请求。\n\n而 IO 多路复用机制，可以让程序通过调用多路复用函数，同时监听多个套接字上的请求。这里既可以包括监听套接字上的连接请求，也可以包括已连接套接字上的读写请求。这样当有一个或多个套接字上有请求时，多路复用函数就会返回。此时，程序就可以处理这些就绪套接字上的请求，比如读取就绪的已连接套接字上的请求内容。\n\n因为 Linux 操作系统在实际应用中比较广泛，所以讲解 Linux 上的 IO 多路复用机制。\n\nLinux 提供的 IO 多路复用机制主要有三种，分别是\n\n * select\n * poll\n * epoll\n\n下面，我们就分别来学习下这三种机制的实现思路和使用方法。然后，我们再来看看，为什么 Redis 通常是选择使用 epoll 这种机制来实现网络通信\n\n\n# Linux 如何实现 IO 多路复用\n\n\n# select 机制：多路复用的基本实现\n\nselect 机制中的一个重要函数就是 select 函数\n\n对于 select 函数来说，它的参数包括\n\n * 监听的文件描述符数量__nfds\n * 被监听描述符的三个集合*__readfds、*__writefds和*__exceptfds\n * 监听时阻塞等待的超时时长*__timeout\n\nint select (int __nfds, fd_set *__readfds, fd_set *__writefds, fd_set *__exceptfds, struct timeval *__timeout)\n\n\n注意\n\nLinux 针对每一个套接字都会有一个文件描述符，也就是一个非负整数，用来唯一标识该套接字，在多路复用机制的函数中，Linux 通常会用文件描述符作为参数。有了文件描述符，函数也就能找到对应的套接字，进而进行监听、读写等操作\n\nselect 函数的参数__readfds、__writefds和__exceptfds表示的是，被监听描述符的集合，其实就是被监听套接字的集合。那么，为什么会有三个集合呢？\n\n这就和我刚才提出的第一个问题相关，也就是多路复用机制会监听哪些事件。select 函数使用三个集合，表示监听的三类事件，分别是\n\n * 读数据事件（对应__readfds集合）\n * 写数据事件（对应__writefds集合）\n * 异常事件（对应__exceptfds集合）\n\n我们进一步可以看到，参数 readfds、writefds 和 exceptfds 的类型是 fd_set 结构体，它主要定义部分如下所示。其中，__fd_mask类型是 long int 类型的别名，FD_SETSIZE 和NFDBITS 这两个宏定义的大小默认为 1024 和 32\n\ntypedef struct {\n   …\n   __fd_mask  __fds_bits[__FD_SETSIZE / __NFDBITS];\n   …\n} fd_set\n\n\n所以，fd_set 结构体的定义，其实就是一个 long int 类型的数组，该数组中一共有 32 个元素（1024/32=32），每个元素是 32 位（long int 类型的大小），而每一位可以用来表示一个文件描述符的状态\n\n好了，了解了 fd_set 结构体的定义，我们就可以回答刚才提出的第二个问题了。select 函数对每一个描述符集合，都可以监听 1024 个描述符。\n\n接下来，我们再来了解下 如何使用 select 机制来实现网络通信。\n\n 1. 首先，我们在调用 select 函数前，可以先创建好传递给 select 函数的描述符集合，然后再创建监听套接字。而为了让创建的监听套接字能被 select 函数监控，我们需要把这个套接字的描述符加入到创建好的描述符集合中。\n 2. 然后，我们就可以调用 select 函数，并把创建好的描述符集合作为参数传递给 select 函数。程序在调用 select 函数后，会发生阻塞。而当 select 函数检测到有描述符就绪后，就会结束阻塞，并返回就绪的文件描述符个数。\n 3. 此时，我们就可以在描述符集合中查找哪些描述符就绪了。然后，我们对已就绪描述符对应的套接字进行处理。比如，如果是__readfds 集合中有描述符就绪，这就表明这些就绪描述符对应的套接字上，有读事件发生，此时，我们就在该套接字上读取数据\n\n而因为 select 函数一次可以监听 1024 个文件描述符的状态，所以 select 函数在返回时，也可能会一次返回多个就绪的文件描述符。这样一来，我们就可以使用一个循环流程，依次对就绪描述符对应的套接字进行读写或异常处理操作。\n\n\n\n下面的代码展示的是使用 select 函数，进行并发客户端处理的关键步骤和主要函数调用：\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\nfd_set rset;  //被监听的描述符集合，关注描述符上的读事件\n\nint max_fd = sock_fd\n\n//初始化rset数组，使用FD_ZERO宏设置每个元素为0\nFD_ZERO(&rset);\n//使用FD_SET宏设置rset数组中位置为sock_fd的文件描述符为1，表示需要监听该文件描述符\nFD_SET(sock_fd,&rset);\n\n//设置超时时间\nstruct timeval timeout;\ntimeout.tv_sec = 3;\ntimeout.tv_usec = 0;\n\nwhile(1) {\n   //调用select函数，检测rset数组保存的文件描述符是否已有读事件就绪，返回就绪的文件描述符个数\n   n = select(max_fd+1, &rset, NULL, NULL, &timeout);\n\n   //调用FD_ISSET宏，在rset数组中检测sock_fd对应的文件描述符是否就绪\n   if (FD_ISSET(sock_fd, &rset)) {\n       //如果sock_fd已经就绪，表明已有客户端连接；调用accept函数建立连接\n       conn_fd = accept();\n       //设置rset数组中位置为conn_fd的文件描述符为1，表示需要监听该文件描述符\n       FD_SET(conn_fd, &rset);\n   }\n\n   //依次检查已连接套接字的文件描述符\n   for (i = 0; i < maxfd; i++) {\n        //调用FD_ISSET宏，在rset数组中检测文件描述符是否就绪\n       if (FD_ISSET(i, &rset)) {\n         //有数据可读，进行读数据处理\n       }\n   }\n}\n\n\n不过，你或许会发现 select 函数存在 两个设计上的不足：\n\n * select 函数对单个进程能监听的文件描述符数量是有限制的，它能监听的文件描述符个数由__FD_SETSIZE 决定，默认值是 1024。\n * 当 select 函数返回后，我们需要遍历描述符集合，才能找到具体是哪些描述符就绪了。这个遍历过程会产生一定开销，从而降低程序的性能。\n\n所以，为了解决 select 函数受限于 1024 个文件描述符的不足，poll 函数对此做了改进\n\n\n# poll 机制：不受限的文件描述符数量\n\npoll 机制的主要函数是 poll 函数，我们先来看下它的原型定义，如下所示：\n\nint poll(struct pollfd *__fds, nfds_t __nfds, int __timeout);\n\n\n其中，参数 fds 是 pollfd 结构体数组，参数 nfds 表示的是 fds 数组的元素个数，而timeout 表示 poll 函数阻塞的超时时间。\n\npollfd 结构体里包含了要监听的描述符，以及该描述符上要监听的事件类型。这个我们可以从 pollfd 结构体的定义中看出来，如下所示。pollfd 结构体中包含了三个成员变量 fd、events 和 revents，分别表示要监听的文件描述符、要监听的事件类型和实际发生的事件类型。\n\nstruct pollfd {\n    int fd;         //进行监听的文件描述符\n    short int events;       //要监听的事件类型\n    short int revents;      //实际发生的事件类型\n};\n\n\npollfd 结构体中要监听和实际发生的事件类型，是通过以下三个宏定义来表示的，分别是 POLLRDNORM、POLLWRNORM 和 POLLERR，它们分别表示可读、可写和错误事件。\n\n#define POLLRDNORM  0x040       //可读事件\n#define POLLWRNORM  0x100       //可写事件\n#define POLLERR     0x008       //错误事件\n\n\n好了，了解了 poll 函数的参数后，我们来看下如何使用 poll 函数完成网络通信。这个流程主要可以分成三步：\n\n 1. 创建 pollfd 数组和监听套接字，并进行绑定；\n 2. 将监听套接字加入 pollfd 数组，并设置其监听读事件，也就是客户端的连接请求；\n 3. 循环调用 poll 函数，检测 pollfd 数组中是否有就绪的文件描述符。\n\n而在第三步的循环过程中，其处理逻辑又分成了两种情况：\n\n * 如果是连接套接字就绪，这表明是有客户端连接，我们可以调用 accept 接受连接，并创建已连接套接字，并将其加入 pollfd 数组，并监听读事件；\n * 如果是已连接套接字就绪，这表明客户端有读写请求，我们可以调用 recv/send 函数处理读写请求。\n\n我画了下面这张图，展示了使用 poll 函数的流程，你可以学习掌握下。\n\n\n\n另外，为了便于你掌握在代码中使用 poll 函数，我也写了一份示例代码，如下所示：\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\n//poll函数可以监听的文件描述符数量，可以大于1024\n#define MAX_OPEN = 2048\n\n//pollfd结构体数组，对应文件描述符\nstruct pollfd client[MAX_OPEN];\n\n//将创建的监听套接字加入pollfd数组，并监听其可读事件\nclient[0].fd = sock_fd;\nclient[0].events = POLLRDNORM;\nmaxfd = 0;\n\n//初始化client数组其他元素为-1\nfor (i = 1; i < MAX_OPEN; i++)\n    client[i].fd = -1;\n\nwhile(1) {\n   //调用poll函数，检测client数组里的文件描述符是否有就绪的，返回就绪的文件描述符个数\n   n = poll(client, maxfd+1, &timeout);\n   //如果监听套件字的文件描述符有可读事件，则进行处理\n   if (client[0].revents & POLLRDNORM) {\n       //有客户端连接；调用accept函数建立连接\n       conn_fd = accept();\n\n       //保存已建立连接套接字\n       for (i = 1; i < MAX_OPEN; i++){\n         if (client[i].fd < 0) {\n           client[i].fd = conn_fd; //将已建立连接的文件描述符保存到client数组\n           client[i].events = POLLRDNORM; //设置该文件描述符监听可读事件\n           break;\n          }\n       }\n       maxfd = i;\n   }\n\n   //依次检查已连接套接字的文件描述符\n   for (i = 1; i < MAX_OPEN; i++) {\n       if (client[i].revents & (POLLRDNORM | POLLERR)) {\n         //有数据可读或发生错误，进行读数据处理或错误处理\n       }\n   }\n}\n\n\n其实，和 select 函数相比，poll 函数的改进之处主要就在于，它允许一次监听超过 1024 个文件描述符。但是当调用了 poll 函数后，我们仍然需要遍历每个文件描述符，检测该描述符是否就绪，然后再进行处理。\n\n那么，有没有办法可以避免遍历每个描述符呢？ 这就是我接下来向你介绍的 epoll 机制\n\n\n# epoll 机制：避免遍历每个描述符\n\n首先，epoll 机制是使用 epoll_event 结构体，来记录待监听的文件描述符及其监听的事件类型的，这和 poll 机制中使用 pollfd 结构体比较类似。\n\n那么，对于 epoll_event 结构体来说，其中包含了 epoll_data_t 联合体变量，以及整数类型的 events 变量。epoll_data_t 联合体中有记录文件描述符的成员变量 fd，而 events 变量会取值使用不同的宏定义值，来表示 epoll_data_t 变量中的文件描述符所关注的事件类型，比如一些常见的事件类型包括以下这几种。\n\n * EPOLLIN：读事件，表示文件描述符对应套接字有数据可读。\n * EPOLLOUT：写事件，表示文件描述符对应套接字有数据要写。\n * EPOLLERR：错误事件，表示文件描述符对于套接字出错。\n\n下面的代码展示了 epoll_event 结构体以及 epoll_data 联合体的定义，你可以看下。\n\ntypedef union epoll_data\n{\n  ...\n  int fd;  //记录文件描述符\n  ...\n} epoll_data_t;\n\n\nstruct epoll_event\n{\n  uint32_t events;  //epoll监听的事件类型\n  epoll_data_t data; //应用程序数据\n};\n\n\n好了，现在我们知道，在使用 select 或 poll 函数的时候，创建好文件描述符集合或 pollfd 数组后，就可以往数组中添加我们需要监听的文件描述符。\n\n但是对于 epoll 机制来说，我们则需要先调用 epoll_create 函数，创建一个 epoll 实例。这个 epoll 实例内部维护了两个结构，分别是记录要监听的文件描述符和已经就绪的文件描述符，而对于已经就绪的文件描述符来说，它们会被返回给用户程序进行处理。\n\n所以，我们在使用 epoll 机制时，就不用像使用 select 和 poll 一样，遍历查询哪些文件描述符已经就绪了。这样一来， epoll 的效率就比 select 和 poll 有了更高的提升。\n\n在创建了 epoll 实例后，我们需要再使用 epoll_ctl 函数，给被监听的文件描述符添加监听事件类型，以及使用 epoll_wait 函数获取就绪的文件描述符。\n\n我画了一张图，展示了使用 epoll 进行网络通信的流程，你可以看下。\n\n\n\n下面的代码展示了使用 epoll 函数的流程，你也可以看下。\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\nepfd = epoll_create(EPOLL_SIZE); //创建epoll实例，\n//创建epoll_event结构体数组，保存套接字对应文件描述符和监听事件类型\nep_events = (epoll_event*)malloc(sizeof(epoll_event) * EPOLL_SIZE);\n\n//创建epoll_event变量\nstruct epoll_event ee\n//监听读事件\nee.events = EPOLLIN;\n//监听的文件描述符是刚创建的监听套接字\nee.data.fd = sock_fd;\n\n//将监听套接字加入到监听列表中\nepoll_ctl(epfd, EPOLL_CTL_ADD, sock_fd, &ee);\n\nwhile (1) {\n   //等待返回已经就绪的描述符\n   n = epoll_wait(epfd, ep_events, EPOLL_SIZE, -1);\n   //遍历所有就绪的描述符\n   for (int i = 0; i < n; i++) {\n       //如果是监听套接字描述符就绪，表明有一个新客户端连接到来\n       if (ep_events[i].data.fd == sock_fd) {\n          conn_fd = accept(sock_fd); //调用accept()建立连接\n          ee.events = EPOLLIN;\n          ee.data.fd = conn_fd;\n          //添加对新创建的已连接套接字描述符的监听，监听后续在已连接套接字上的读事件\n          epoll_ctl(epfd, EPOLL_CTL_ADD, conn_fd, &ee);\n\n       } else { //如果是已连接套接字描述符就绪，则可以读数据\n           ...//读取数据并处理\n       }\n   }\n}\n\n\n好了，到这里，你就了解了 epoll 函数的使用方法了。实际上，也正是因为 epoll 能自定义监听的描述符数量，以及可以直接返回就绪的描述符，Redis 在设计和实现网络通信框架时，就基于 epoll 机制中的 epoll_create、epoll_ctl 和 epoll_wait 等函数和读写事件，进行了封装开发，实现了用于网络通信的事件驱动框架，从而使得 Redis 虽然是单线程运行，但是仍然能高效应对高并发的客户端访问。\n\n\n# 总结\n\necho 给你介绍了 Redis 网络通信依赖的操作系统底层机制，也就是 IO 多路复用机制\n\n由于 Redis 是单线程程序，如果使用基本的 Socket 编程模型的话，只能对一个监听套接字或一个已连接套接字进行监听。而当 Redis 实例面临很多并发的客户端时，这种处理方式的效率就会很低。\n\n所以，和基本的 Socket 通信相比，使用 IO 多路复用机制，就可以一次性获得就绪的多个套接字，从而避免了逐个检测套接字的开销。\n\n我是以最常用的 Linux 操作系统为例，给你具体介绍了 Linux 系统提供的三种 IO 多路复用机制，分别是 select、poll 和 epoll。这三种机制在能监听的描述符数量和查找就绪描述符的方法上是不一样的\n\n多路复用机制   监听的文件描述符数量   查找就绪的文件描述符\nselect   最多 1024      遍历所有描述符\npoll     不受限          遍历所有描述符\nepoll    与使用          epoll_wait 自动返回就绪的描述符，未就绪的描述符不返回\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n * 为什么 redis 这样高并发的应用选择了 epoll，而不是 select 或 poll？\n\n * 在设计一个高并发服务器时，如何选择最适合的 io 模型？\n\n * 当服务器面对成千上万的连接时，什么样的机制可以让你高效管理这些连接？\n\n * 高并发场景中，epoll 是如何通过事件通知机制避免性能瓶颈的？\n\n * 多路复用机制会监听套接字上的哪些事件？\n\n * 多路复用机制可以监听多少个套接字？\n\n * 当有套接字就绪时，多路复用机制要如何找到就绪的套接字？\n\n\n# 前言\n\nredis 作为一个 client-server 架构的数据库，其源码中少不了用来实现网络通信的部分。通常系统实现网络通信的基本方法是 使用 socket 编程模型，但是，由于基本的 socket 编程模型是单线程阻塞的 ....\n\n所以当要处理高并发请求时，有两种方案\n\n * 多线程，让每个线程负责处理一个客户端的请求。而 redis 负责客户端请求解析和处理的线程只有一个，那么如果直接采用基本 socket 模型，就会影响 redis 支持高并发的客户端访问。\n * io 多路复用，为了实现高并发的网络通信，我们常用的 linux 操作系统，就提供了 select、poll 和 epoll 三种编程模型，而在 linux 上运行的 redis，通常就会采用其中的 epoll 模型来进行网络通信。\n\n\n# 简述传统 socket 模型\n\n我们看下使用 socket 模型实现网络通信时的关键操作，以此帮助我们分析 socket 模型中的不足\n\n首先，当我们需要让服务器端和客户端进行通信时，可以在服务器端通过以下三步，来创建监听客户端连接的监听套接字（listening socket）：\n\n 1. 调用 socket 函数，创建一个套接字。我们通常把这个套接字称为主动套接字（active socket）\n 2. 调用 bind 函数，将主动套接字和当前服务器的 ip 和监听端口进行绑定\n 3. 调用 listen 函数，将主动套接字转换为监听套接字，开始监听客户端的连接\n 4. 调用 accept函数，在完成上述三步之后，服务器端就可以接收客户端的连接请求了。为了能及时地收到客户端的连接请求，我们可以运行一个循环流程，在该流程中调用 accept 函数，用于接收客户端连接请求\n 5. 最后，服务器端可以通过调用 recv 或 send 函数，在刚才返回的已连接套接字上，接收并处理读写请求，或是将数据发送给客户端\n\n注意\n\naccept 函数是阻塞函数，也就是说，如果此时一直没有客户端连接请求，那么，服务器端的执行流程会一直阻塞在 accept 函数。一旦有客户端连接请求到达，accept 将不再阻塞，而是处理连接请求，和客户端建立连接，并返回已连接套接字（connected socket）\n\nlistensocket = socket(); //调用socket系统调用创建一个主动套接字\nbind(listensocket);  //绑定地址和端口\nlisten(listensocket); //将默认的主动套接字转换为服务器使用的被动套接字，也就是监听套接字\nwhile (1) { //循环监听是否有客户端连接请求到来\n   connsocket = accept(listensocket); //接受客户端连接\n   recv(connsocket); //从客户端读取数据，只能同时处理一个客户端\n   send(connsocket); //给客户端返回数据，只能同时处理一个客户端\n}\n\n\n你会发现，虽然上述代码能够实现服务器端和客户端之间的通信，但是程序每调用一次 accept 函数，只能处理一个客户端连接\n\n因此，如果想要处理多个并发客户端的请求，我们就需要使用多线程的方法，来处理通过 accept 函数建立的多个客户端连接上的请求\n\n使用这种方法后，我们需要在 accept 函数返回已连接套接字后，创建一个线程，并将已连接套接字传递给创建的线程，由该线程负责这个连接套接字上后续的数据读写。同时，服务器端的执行流程会再次调用 accept 函数，等待下一个客户端连接\n\n以下给出的示例代码，就展示了使用多线程来提升服务器端的并发客户端处理能力：\n\nlistensocket = socket(); //调用socket系统调用创建一个主动套接字\nbind(listensocket);  //绑定地址和端口\nlisten(listensocket); //将默认的主动套接字转换为服务器使用的被动套接字，即监听套接字\nwhile (1) { //循环监听是否有客户端连接到来\n   connsocket = accept(listensocket); //接受客户端连接，返回已连接套接字\n   pthread_create(processdata, connsocket); //创建新线程对已连接套接字进行处理\n\n}\n\n//处理已连接套接字上的读写请求\nprocessdata(connsocket){\n   recv(connsocket); //从客户端读取数据，只能同时处理一个客户端\n   send(connsocket); //给客户端返回数据，只能同时处理一个客户端\n}\n\n\n\n# 展望 io 多路复用\n\n虽然上述方法能提升服务器端的并发处理能力，遗憾的是\n\nredis 的主执行流程是由一个线程在执行，无法使用多线程的方式来提升并发处理能力。\n\n所以，该方法对 redis 并不起作用。那么，还有没有什么其他方法，能帮助 redis 提升并发客户端的处理能力呢？\n\n这就要用到操作系统提供的 io 多路复用功能了\n\n在基本的 socket 编程模型中，accept 函数只能在一个监听套接字上监听客户端的连接，recv 函数也只能在一个已连接套接字上，等待客户端发送的请求。\n\n而 io 多路复用机制，可以让程序通过调用多路复用函数，同时监听多个套接字上的请求。这里既可以包括监听套接字上的连接请求，也可以包括已连接套接字上的读写请求。这样当有一个或多个套接字上有请求时，多路复用函数就会返回。此时，程序就可以处理这些就绪套接字上的请求，比如读取就绪的已连接套接字上的请求内容。\n\n因为 linux 操作系统在实际应用中比较广泛，所以讲解 linux 上的 io 多路复用机制。\n\nlinux 提供的 io 多路复用机制主要有三种，分别是\n\n * select\n * poll\n * epoll\n\n下面，我们就分别来学习下这三种机制的实现思路和使用方法。然后，我们再来看看，为什么 redis 通常是选择使用 epoll 这种机制来实现网络通信\n\n\n# linux 如何实现 io 多路复用\n\n\n# select 机制：多路复用的基本实现\n\nselect 机制中的一个重要函数就是 select 函数\n\n对于 select 函数来说，它的参数包括\n\n * 监听的文件描述符数量__nfds\n * 被监听描述符的三个集合*__readfds、*__writefds和*__exceptfds\n * 监听时阻塞等待的超时时长*__timeout\n\nint select (int __nfds, fd_set *__readfds, fd_set *__writefds, fd_set *__exceptfds, struct timeval *__timeout)\n\n\n注意\n\nlinux 针对每一个套接字都会有一个文件描述符，也就是一个非负整数，用来唯一标识该套接字，在多路复用机制的函数中，linux 通常会用文件描述符作为参数。有了文件描述符，函数也就能找到对应的套接字，进而进行监听、读写等操作\n\nselect 函数的参数__readfds、__writefds和__exceptfds表示的是，被监听描述符的集合，其实就是被监听套接字的集合。那么，为什么会有三个集合呢？\n\n这就和我刚才提出的第一个问题相关，也就是多路复用机制会监听哪些事件。select 函数使用三个集合，表示监听的三类事件，分别是\n\n * 读数据事件（对应__readfds集合）\n * 写数据事件（对应__writefds集合）\n * 异常事件（对应__exceptfds集合）\n\n我们进一步可以看到，参数 readfds、writefds 和 exceptfds 的类型是 fd_set 结构体，它主要定义部分如下所示。其中，__fd_mask类型是 long int 类型的别名，fd_setsize 和nfdbits 这两个宏定义的大小默认为 1024 和 32\n\ntypedef struct {\n   …\n   __fd_mask  __fds_bits[__fd_setsize / __nfdbits];\n   …\n} fd_set\n\n\n所以，fd_set 结构体的定义，其实就是一个 long int 类型的数组，该数组中一共有 32 个元素（1024/32=32），每个元素是 32 位（long int 类型的大小），而每一位可以用来表示一个文件描述符的状态\n\n好了，了解了 fd_set 结构体的定义，我们就可以回答刚才提出的第二个问题了。select 函数对每一个描述符集合，都可以监听 1024 个描述符。\n\n接下来，我们再来了解下 如何使用 select 机制来实现网络通信。\n\n 1. 首先，我们在调用 select 函数前，可以先创建好传递给 select 函数的描述符集合，然后再创建监听套接字。而为了让创建的监听套接字能被 select 函数监控，我们需要把这个套接字的描述符加入到创建好的描述符集合中。\n 2. 然后，我们就可以调用 select 函数，并把创建好的描述符集合作为参数传递给 select 函数。程序在调用 select 函数后，会发生阻塞。而当 select 函数检测到有描述符就绪后，就会结束阻塞，并返回就绪的文件描述符个数。\n 3. 此时，我们就可以在描述符集合中查找哪些描述符就绪了。然后，我们对已就绪描述符对应的套接字进行处理。比如，如果是__readfds 集合中有描述符就绪，这就表明这些就绪描述符对应的套接字上，有读事件发生，此时，我们就在该套接字上读取数据\n\n而因为 select 函数一次可以监听 1024 个文件描述符的状态，所以 select 函数在返回时，也可能会一次返回多个就绪的文件描述符。这样一来，我们就可以使用一个循环流程，依次对就绪描述符对应的套接字进行读写或异常处理操作。\n\n\n\n下面的代码展示的是使用 select 函数，进行并发客户端处理的关键步骤和主要函数调用：\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\nfd_set rset;  //被监听的描述符集合，关注描述符上的读事件\n\nint max_fd = sock_fd\n\n//初始化rset数组，使用fd_zero宏设置每个元素为0\nfd_zero(&rset);\n//使用fd_set宏设置rset数组中位置为sock_fd的文件描述符为1，表示需要监听该文件描述符\nfd_set(sock_fd,&rset);\n\n//设置超时时间\nstruct timeval timeout;\ntimeout.tv_sec = 3;\ntimeout.tv_usec = 0;\n\nwhile(1) {\n   //调用select函数，检测rset数组保存的文件描述符是否已有读事件就绪，返回就绪的文件描述符个数\n   n = select(max_fd+1, &rset, null, null, &timeout);\n\n   //调用fd_isset宏，在rset数组中检测sock_fd对应的文件描述符是否就绪\n   if (fd_isset(sock_fd, &rset)) {\n       //如果sock_fd已经就绪，表明已有客户端连接；调用accept函数建立连接\n       conn_fd = accept();\n       //设置rset数组中位置为conn_fd的文件描述符为1，表示需要监听该文件描述符\n       fd_set(conn_fd, &rset);\n   }\n\n   //依次检查已连接套接字的文件描述符\n   for (i = 0; i < maxfd; i++) {\n        //调用fd_isset宏，在rset数组中检测文件描述符是否就绪\n       if (fd_isset(i, &rset)) {\n         //有数据可读，进行读数据处理\n       }\n   }\n}\n\n\n不过，你或许会发现 select 函数存在 两个设计上的不足：\n\n * select 函数对单个进程能监听的文件描述符数量是有限制的，它能监听的文件描述符个数由__fd_setsize 决定，默认值是 1024。\n * 当 select 函数返回后，我们需要遍历描述符集合，才能找到具体是哪些描述符就绪了。这个遍历过程会产生一定开销，从而降低程序的性能。\n\n所以，为了解决 select 函数受限于 1024 个文件描述符的不足，poll 函数对此做了改进\n\n\n# poll 机制：不受限的文件描述符数量\n\npoll 机制的主要函数是 poll 函数，我们先来看下它的原型定义，如下所示：\n\nint poll(struct pollfd *__fds, nfds_t __nfds, int __timeout);\n\n\n其中，参数 fds 是 pollfd 结构体数组，参数 nfds 表示的是 fds 数组的元素个数，而timeout 表示 poll 函数阻塞的超时时间。\n\npollfd 结构体里包含了要监听的描述符，以及该描述符上要监听的事件类型。这个我们可以从 pollfd 结构体的定义中看出来，如下所示。pollfd 结构体中包含了三个成员变量 fd、events 和 revents，分别表示要监听的文件描述符、要监听的事件类型和实际发生的事件类型。\n\nstruct pollfd {\n    int fd;         //进行监听的文件描述符\n    short int events;       //要监听的事件类型\n    short int revents;      //实际发生的事件类型\n};\n\n\npollfd 结构体中要监听和实际发生的事件类型，是通过以下三个宏定义来表示的，分别是 pollrdnorm、pollwrnorm 和 pollerr，它们分别表示可读、可写和错误事件。\n\n#define pollrdnorm  0x040       //可读事件\n#define pollwrnorm  0x100       //可写事件\n#define pollerr     0x008       //错误事件\n\n\n好了，了解了 poll 函数的参数后，我们来看下如何使用 poll 函数完成网络通信。这个流程主要可以分成三步：\n\n 1. 创建 pollfd 数组和监听套接字，并进行绑定；\n 2. 将监听套接字加入 pollfd 数组，并设置其监听读事件，也就是客户端的连接请求；\n 3. 循环调用 poll 函数，检测 pollfd 数组中是否有就绪的文件描述符。\n\n而在第三步的循环过程中，其处理逻辑又分成了两种情况：\n\n * 如果是连接套接字就绪，这表明是有客户端连接，我们可以调用 accept 接受连接，并创建已连接套接字，并将其加入 pollfd 数组，并监听读事件；\n * 如果是已连接套接字就绪，这表明客户端有读写请求，我们可以调用 recv/send 函数处理读写请求。\n\n我画了下面这张图，展示了使用 poll 函数的流程，你可以学习掌握下。\n\n\n\n另外，为了便于你掌握在代码中使用 poll 函数，我也写了一份示例代码，如下所示：\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\n//poll函数可以监听的文件描述符数量，可以大于1024\n#define max_open = 2048\n\n//pollfd结构体数组，对应文件描述符\nstruct pollfd client[max_open];\n\n//将创建的监听套接字加入pollfd数组，并监听其可读事件\nclient[0].fd = sock_fd;\nclient[0].events = pollrdnorm;\nmaxfd = 0;\n\n//初始化client数组其他元素为-1\nfor (i = 1; i < max_open; i++)\n    client[i].fd = -1;\n\nwhile(1) {\n   //调用poll函数，检测client数组里的文件描述符是否有就绪的，返回就绪的文件描述符个数\n   n = poll(client, maxfd+1, &timeout);\n   //如果监听套件字的文件描述符有可读事件，则进行处理\n   if (client[0].revents & pollrdnorm) {\n       //有客户端连接；调用accept函数建立连接\n       conn_fd = accept();\n\n       //保存已建立连接套接字\n       for (i = 1; i < max_open; i++){\n         if (client[i].fd < 0) {\n           client[i].fd = conn_fd; //将已建立连接的文件描述符保存到client数组\n           client[i].events = pollrdnorm; //设置该文件描述符监听可读事件\n           break;\n          }\n       }\n       maxfd = i;\n   }\n\n   //依次检查已连接套接字的文件描述符\n   for (i = 1; i < max_open; i++) {\n       if (client[i].revents & (pollrdnorm | pollerr)) {\n         //有数据可读或发生错误，进行读数据处理或错误处理\n       }\n   }\n}\n\n\n其实，和 select 函数相比，poll 函数的改进之处主要就在于，它允许一次监听超过 1024 个文件描述符。但是当调用了 poll 函数后，我们仍然需要遍历每个文件描述符，检测该描述符是否就绪，然后再进行处理。\n\n那么，有没有办法可以避免遍历每个描述符呢？ 这就是我接下来向你介绍的 epoll 机制\n\n\n# epoll 机制：避免遍历每个描述符\n\n首先，epoll 机制是使用 epoll_event 结构体，来记录待监听的文件描述符及其监听的事件类型的，这和 poll 机制中使用 pollfd 结构体比较类似。\n\n那么，对于 epoll_event 结构体来说，其中包含了 epoll_data_t 联合体变量，以及整数类型的 events 变量。epoll_data_t 联合体中有记录文件描述符的成员变量 fd，而 events 变量会取值使用不同的宏定义值，来表示 epoll_data_t 变量中的文件描述符所关注的事件类型，比如一些常见的事件类型包括以下这几种。\n\n * epollin：读事件，表示文件描述符对应套接字有数据可读。\n * epollout：写事件，表示文件描述符对应套接字有数据要写。\n * epollerr：错误事件，表示文件描述符对于套接字出错。\n\n下面的代码展示了 epoll_event 结构体以及 epoll_data 联合体的定义，你可以看下。\n\ntypedef union epoll_data\n{\n  ...\n  int fd;  //记录文件描述符\n  ...\n} epoll_data_t;\n\n\nstruct epoll_event\n{\n  uint32_t events;  //epoll监听的事件类型\n  epoll_data_t data; //应用程序数据\n};\n\n\n好了，现在我们知道，在使用 select 或 poll 函数的时候，创建好文件描述符集合或 pollfd 数组后，就可以往数组中添加我们需要监听的文件描述符。\n\n但是对于 epoll 机制来说，我们则需要先调用 epoll_create 函数，创建一个 epoll 实例。这个 epoll 实例内部维护了两个结构，分别是记录要监听的文件描述符和已经就绪的文件描述符，而对于已经就绪的文件描述符来说，它们会被返回给用户程序进行处理。\n\n所以，我们在使用 epoll 机制时，就不用像使用 select 和 poll 一样，遍历查询哪些文件描述符已经就绪了。这样一来， epoll 的效率就比 select 和 poll 有了更高的提升。\n\n在创建了 epoll 实例后，我们需要再使用 epoll_ctl 函数，给被监听的文件描述符添加监听事件类型，以及使用 epoll_wait 函数获取就绪的文件描述符。\n\n我画了一张图，展示了使用 epoll 进行网络通信的流程，你可以看下。\n\n\n\n下面的代码展示了使用 epoll 函数的流程，你也可以看下。\n\nint sock_fd,conn_fd; //监听套接字和已连接套接字的变量\nsock_fd = socket() //创建套接字\nbind(sock_fd)   //绑定套接字\nlisten(sock_fd) //在套接字上进行监听，将套接字转为监听套接字\n\nepfd = epoll_create(epoll_size); //创建epoll实例，\n//创建epoll_event结构体数组，保存套接字对应文件描述符和监听事件类型\nep_events = (epoll_event*)malloc(sizeof(epoll_event) * epoll_size);\n\n//创建epoll_event变量\nstruct epoll_event ee\n//监听读事件\nee.events = epollin;\n//监听的文件描述符是刚创建的监听套接字\nee.data.fd = sock_fd;\n\n//将监听套接字加入到监听列表中\nepoll_ctl(epfd, epoll_ctl_add, sock_fd, &ee);\n\nwhile (1) {\n   //等待返回已经就绪的描述符\n   n = epoll_wait(epfd, ep_events, epoll_size, -1);\n   //遍历所有就绪的描述符\n   for (int i = 0; i < n; i++) {\n       //如果是监听套接字描述符就绪，表明有一个新客户端连接到来\n       if (ep_events[i].data.fd == sock_fd) {\n          conn_fd = accept(sock_fd); //调用accept()建立连接\n          ee.events = epollin;\n          ee.data.fd = conn_fd;\n          //添加对新创建的已连接套接字描述符的监听，监听后续在已连接套接字上的读事件\n          epoll_ctl(epfd, epoll_ctl_add, conn_fd, &ee);\n\n       } else { //如果是已连接套接字描述符就绪，则可以读数据\n           ...//读取数据并处理\n       }\n   }\n}\n\n\n好了，到这里，你就了解了 epoll 函数的使用方法了。实际上，也正是因为 epoll 能自定义监听的描述符数量，以及可以直接返回就绪的描述符，redis 在设计和实现网络通信框架时，就基于 epoll 机制中的 epoll_create、epoll_ctl 和 epoll_wait 等函数和读写事件，进行了封装开发，实现了用于网络通信的事件驱动框架，从而使得 redis 虽然是单线程运行，但是仍然能高效应对高并发的客户端访问。\n\n\n# 总结\n\necho 给你介绍了 redis 网络通信依赖的操作系统底层机制，也就是 io 多路复用机制\n\n由于 redis 是单线程程序，如果使用基本的 socket 编程模型的话，只能对一个监听套接字或一个已连接套接字进行监听。而当 redis 实例面临很多并发的客户端时，这种处理方式的效率就会很低。\n\n所以，和基本的 socket 通信相比，使用 io 多路复用机制，就可以一次性获得就绪的多个套接字，从而避免了逐个检测套接字的开销。\n\n我是以最常用的 linux 操作系统为例，给你具体介绍了 linux 系统提供的三种 io 多路复用机制，分别是 select、poll 和 epoll。这三种机制在能监听的描述符数量和查找就绪描述符的方法上是不一样的\n\n多路复用机制   监听的文件描述符数量   查找就绪的文件描述符\nselect   最多 1024      遍历所有描述符\npoll     不受限          遍历所有描述符\nepoll    与使用          epoll_wait 自动返回就绪的描述符，未就绪的描述符不返回\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Redis Server 初始化",frontmatter:{title:"Redis Server 初始化",date:"2024-09-16T03:04:10.000Z",permalink:"/pages/d4ecb9/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%BB%E7%BA%BF/03.Redis%20Server%20%E5%88%9D%E5%A7%8B%E5%8C%96.html",relativePath:"Redis 系统设计/03.主线/03.Redis Server 初始化.md",key:"v-6691f86c",path:"/pages/d4ecb9/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:289},{level:2,title:"main 函数：Redis server 的入口",slug:"main-函数-redis-server-的入口",normalizedTitle:"main 函数：redis server 的入口",charIndex:713},{level:3,title:"阶段一：基本初始化",slug:"阶段一-基本初始化",normalizedTitle:"阶段一：基本初始化",charIndex:860},{level:3,title:"阶段二：检查哨兵模式，并检查是否要执行 RDB 检测或 AOF 检测",slug:"阶段二-检查哨兵模式-并检查是否要执行-rdb-检测或-aof-检测",normalizedTitle:"阶段二：检查哨兵模式，并检查是否要执行 rdb 检测或 aof 检测",charIndex:1515},{level:3,title:"阶段三：运行参数解析",slug:"阶段三-运行参数解析",normalizedTitle:"阶段三：运行参数解析",charIndex:2543},{level:3,title:"阶段四：初始化 server",slug:"阶段四-初始化-server",normalizedTitle:"阶段四：初始化 server",charIndex:2680},{level:3,title:"阶段五：执行事件驱动框架",slug:"阶段五-执行事件驱动框架",normalizedTitle:"阶段五：执行事件驱动框架",charIndex:2991},{level:2,title:"Redis 运行参数解析与设置",slug:"redis-运行参数解析与设置",normalizedTitle:"redis 运行参数解析与设置",charIndex:3237},{level:3,title:"Redis 的主要参数类型",slug:"redis-的主要参数类型",normalizedTitle:"redis 的主要参数类型",charIndex:3609},{level:3,title:"Redis 参数的设置方法",slug:"redis-参数的设置方法",normalizedTitle:"redis 参数的设置方法",charIndex:4111},{level:2,title:"initServer：初始化 Redis server",slug:"initserver-初始化-redis-server",normalizedTitle:"initserver：初始化 redis server",charIndex:6839},{level:2,title:"执行事件驱动框架",slug:"执行事件驱动框架",normalizedTitle:"执行事件驱动框架",charIndex:2995},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:9549},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10555}],headersStr:"前言 main 函数：Redis server 的入口 阶段一：基本初始化 阶段二：检查哨兵模式，并检查是否要执行 RDB 检测或 AOF 检测 阶段三：运行参数解析 阶段四：初始化 server 阶段五：执行事件驱动框架 Redis 运行参数解析与设置 Redis 的主要参数类型 Redis 参数的设置方法 initServer：初始化 Redis server 执行事件驱动框架 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. 如何高效执行网络服务器启动的每一步？Redis 的 main 函数能带来什么启示？\n 2. Redis Server 启动后如何确保关键参数正确加载？有哪些常见陷阱？\n 3. Redis 如何通过事件驱动模型处理高并发请求？从 main 函数能学到什么？\n 4. 为什么初始化和加载配置参数对 Redis 启动至关重要？它如何影响客户端请求处理？\n 5. 如何通过 initServer 函数理解并调优 Redis 配置参数以提升性能？\n 6. Redis 启动时如何决定加载 AOF 还是 RDB？这种设计对数据恢复的影响是什么？\n\n\n# 前言\n\n我们知道，main 函数是 Redis 整个运行程序的入口。同时，由于 Redis 是典型的 Client-Server 架构，一旦 Redis 实例开始运行，Redis server 也就会启动，所以 main 函数其实也会负责 Redis server 的启动运行。\n\n> main 函数是在 server.c 中\n\n你平常在设计或实现一个网络服务器程序时，可能会遇到一个问题，那就是\n\n服务器启动时，应该做哪些操作、有没有一个典型的参考实现\n\n所以我就从 main 函数开始，给你介绍下 Redis server 是如何在 main 函数中启动并完成初始化的。通过这节课内容的学习，你可以掌握 Redis 针对以下三个问题的实现思路：\n\n 1. Redis server 启动后具体会做哪些初始化操作？\n 2. Redis server 初始化时有哪些关键配置项？\n 3. Redis server 如何开始处理客户端请求？\n\n\n# main 函数：Redis server 的入口\n\n一般来说，一个使用 C 开发的系统软件启动运行的代码逻辑，都是实现在了 main 函数当中，所以 echo 找到了 main 函数，看看它的执行过程\n\n那么，对于 Redis 的 main 函数来说，我把它执行的工作分成了五个阶段。\n\n\n# 阶段一：基本初始化\n\n在这个阶段，main 函数主要是完成一些基本的初始化工作，包括设置 server 运行的时区、设置哈希函数的随机种子等。这部分工作的主要调用函数如下所示：\n\n//设置时区\nsetlocale(LC_COLLATE,"");\ntzset();\n...\n//设置随机种子\nchar hashseed[16];\ngetRandomHexChars(hashseed,sizeof(hashseed));\ndictSetHashFunctionSeed((uint8_t*)hashseed);\n\n\n这里，你需要注意的是，在 main 函数的开始部分，有一段宏定义覆盖的代码。这部分代码的作用是，如果定义了 REDIS_TEST 宏定义，并且 Redis server 启动时的参数符合测试参数，那么 main 函数就会执行相应的测试程序。\n\n这段宏定义的代码如以下所示，其中的示例代码就是调用 ziplist 的测试函数 ziplistTest：\n\n#ifdef REDIS_TEST\n//如果启动参数有test和ziplist，那么就调用ziplistTest函数进行ziplist的测试\nif (argc == 3 && !strcasecmp(argv[1], "test")) {\n  if (!strcasecmp(argv[2], "ziplist")) {\n     return ziplistTest(argc, argv);\n  }\n  ...\n}\n#endif\n\n\n\n# 阶段二：检查哨兵模式，并检查是否要执行 RDB 检测或 AOF 检测\n\nRedis server 启动后，可能是以哨兵模式运行的，而哨兵模式运行的 server 在参数初始化、参数设置，以及 server 启动过程中要执行的操作等方面，与普通模式 server 有所差别。所以，main 函数在执行过程中需要根据 Redis 配置的参数，检查是否设置了哨兵模式。\n\n如果有设置哨兵模式的话，main 函数会调用 initSentinelConfig 函数，对哨兵模式的参数进行初始化设置，以及调用 initSentinel 函数，初始化设置哨兵模式运行的 server。有关哨兵模式运行的 Redis server 相关机制，我会在第 21 讲中给你详细介绍。\n\n下面的代码展示了 main 函数中对哨兵模式的检查，以及对哨兵模式的初始化，你可以看下：\n\n...\n//判断server是否设置为哨兵模式\nif (server.sentinel_mode) {\n    initSentinelConfig();  //初始化哨兵的配置\n    initSentinel();   //初始化哨兵模式\n}\n...\n\n\n除了检查哨兵模式以外，main 函数还会检查是否要执行 RDB 检测或 AOF 检查，这对应了实际运行的程序是 redis-check-rdb 或 redis-check-aof。在这种情况下，main 函数会调用 redis_check_rdb_main 函数或 redis_check_aof_main 函数，检测 RDB 文件或 AOF 文件。你可以看看下面的代码，其中就展示了 main 函数对这部分内容的检查和调用：\n\n...\n//如果运行的是redis-check-rdb程序，调用redis_check_rdb_main函数检测RDB文件\nif (strstr(argv[0],"redis-check-rdb") != NULL)\n   redis_check_rdb_main(argc,argv,NULL);\n//如果运行的是redis-check-aof程序，调用redis_check_aof_main函数检测AOF文件\nelse if (strstr(argv[0],"redis-check-aof") != NULL)\n   redis_check_aof_main(argc,argv);\n...\n\n\n\n# 阶段三：运行参数解析\n\n在这一阶段，main 函数会对命令行传入的参数进行解析，并且调用 loadServerConfig 函数，对命令行参数和配置文件中的参数进行合并处理，然后为 Redis 各功能模块的关键参数设置合适的取值，以便 server 能高效地运行。\n\n\n# 阶段四：初始化 server\n\n在完成对运行参数的解析和设置后，main 函数会调用 initServer 函数，对 server 运行时的各种资源进行初始化工作。这主要包括了 server 资源管理所需的数据结构初始化、键值对数据库初始化、server 网络框架初始化等。\n\n而在调用完 initServer 后，main 函数还会再次判断当前 server 是否为哨兵模式。如果是哨兵模式，main 函数会调用 sentinelIsRunning 函数，设置启动哨兵模式。否则的话，main 函数会调用 loadDataFromDisk 函数，从磁盘上加载 AOF 或者是 RDB 文件，以便恢复之前的数据。\n\n\n# 阶段五：执行事件驱动框架\n\n为了能高效处理高并发的客户端连接请求，Redis 采用了事件驱动框架，来并发处理不同客户端的连接和读写请求。所以，main 函数执行到最后时，会调用 aeMain 函数进入事件驱动框架，开始循环处理各种触发的事件。\n\n我把刚才介绍的五个阶段涉及到的关键操作，画在了下面的图中，你可以再回顾下。\n\n那么，在这五个阶段当中，阶段三、四和五其实就包括了 Redis server 启动过程中的关键操作。所以接下来，我们就来依次学习下这三个阶段中的主要工作。\n\n\n\n\n# Redis 运行参数解析与设置\n\n我们知道，Redis 提供了丰富的功能，既支持多种键值对数据类型的读写访问，还支持数据持久化保存、主从复制、切片集群等。而这些功能的高效运行，其实都离不开相关功能模块的关键参数配置。\n\n举例来说，Redis 为了节省内存，设计了内存紧凑型的数据结构来保存 Hash、Sorted Set 等键值对类型。但是在使用了内存紧凑型的数据结构之后，如果往数据结构存入的元素个数过多或元素过大的话，键值对的访问性能反而会受到影响。因此，为了平衡内存使用量和系统访问性能，我们就可以通过参数，来设置和调节内存紧凑型数据结构的使用条件。\n\n也就是说，掌握这些关键参数的设置，可以帮助我们提升 Redis 实例的运行效率。\n\n不过，Redis 的参数有很多，我们无法在一节课中掌握所有的参数设置。所以下面，我们可以先来学习下 Redis 的主要参数类型，这样就能对各种参数形成一个全面的了解。同时，我也会给你介绍一些和 server 运行关系密切的参数及其设置方法，以便你可以配置好这些参数，让 server 高效运行起来。\n\n\n# Redis 的主要参数类型\n\n首先，Redis 运行所需的各种参数，都统一定义在了server.h文件的 redisServer 结构体中。根据参数作用的范围，我把各种参数划分为了七大类型，包括通用参数、数据结构参数、网络参数、持久化参数、主从复制参数、切片集群参数、性能优化参数。具体你可以参考下面表格中的内容。\n\n\n\n这样，如果你能按照上面的划分方法给 Redis 参数进行归类，那么你就可以发现，这些参数实际和 Redis 的主要功能机制是相对应的。所以，如果你要深入掌握这些参数的典型配置值，你就需要对相应功能机制的工作原理有所了解。我在接下来的课程中，也会在介绍 Redis 功能模块设计的同时，带你了解下其相应的典型参数配置。\n\n好，现在我们就了解了 Redis 的七大参数类型，以及它们基本的作用范围，那么下面我们就接着来学习下，Redis 是如何针对这些参数进行设置的。\n\n\n# Redis 参数的设置方法\n\nRedis 对运行参数的设置实际上会经过三轮赋值，分别是默认配置值、命令行启动参数，以及配置文件配置值。\n\n首先，Redis 在 main 函数中会先调用 initServerConfig 函数，为各种参数设置默认值。参数的默认值统一定义在 server.h 文件中，都是以 CONFIG_DEFAULT 开头的宏定义变量。下面的代码显示的是部分参数的默认值，你可以看下。\n\n#define CONFIG_DEFAULT_HZ        10   //server后台任务的默认运行频率\n#define CONFIG_MIN_HZ            1    // server后台任务的最小运行频率\n#define CONFIG_MAX_HZ            500 // server后台任务的最大运行频率\n#define CONFIG_DEFAULT_SERVER_PORT  6379  //server监听的默认TCP端口\n#define CONFIG_DEFAULT_CLIENT_TIMEOUT  0  //客户端超时时间，默认为0，表示没有超时限制\n\n\n在 server.h 中提供的默认参数值，一般都是典型的配置值。因此，如果你在部署使用 Redis 实例的过程中，对 Redis 的工作原理不是很了解，就可以使用代码中提供的默认配置。\n\n当然，如果你对 Redis 各功能模块的工作机制比较熟悉的话，也可以自行设置运行参数。你可以在启动 Redis 程序时，在命令行上设置运行参数的值。比如，如果你想将 Redis server 监听端口从默认的 6379 修改为 7379，就可以在命令行上设置 port 参数为 7379，如下所示：\n\n./redis-server --port 7379\n\n\n这里，你需要注意的是，Redis 的命令行参数设置需要使用**两个减号“–”**来表示相应的参数名，否则的话，Redis 就无法识别所设置的运行参数。\n\nRedis 在使用 initServerConfig 函数对参数设置默认配置值后，接下来，main 函数就会对 Redis 程序启动时的命令行参数进行逐一解析。\n\nmain 函数会把解析后的参数及参数值保存成字符串，接着，main 函数会调用 loadServerConfig 函数进行第二和第三轮的赋值。以下代码显示了 main 函数对命令行参数的解析，以及调用 loadServerConfig 函数的过程，你可以看下。\n\nint main(int argc, char **argv) {\n…\n//保存命令行参数\nfor (j = 0; j < argc; j++) server.exec_argv[j] = zstrdup(argv[j]);\n…\nif (argc >= 2) {\n   …\n   //对每个运行时参数进行解析\n   while(j != argc) {\n      …\n   }\n   …\n   //\n   loadServerConfig(configfile,options);\n}\n\n\n这里你要知道的是，loadServerConfig 函数是在config.c文件中实现的，该函数是以 Redis 配置文件和命令行参数的解析字符串为参数，将配置文件中的所有配置项读取出来，形成字符串。紧接着，loadServerConfig 函数会把解析后的命令行参数，追加到配置文件形成的配置项字符串。\n\n这样一来，配置项字符串就同时包含了配置文件中设置的参数，以及命令行设置的参数。\n\n最后，loadServerConfig 函数会进一步调用 loadServerConfigFromString 函数，对配置项字符串中的每一个配置项进行匹配。一旦匹配成功，loadServerConfigFromString 函数就会按照配置项的值设置 server 的参数。\n\n以下代码显示了 loadServerConfigFromString 函数的部分内容。这部分代码是使用了条件分支，来依次比较配置项是否是“timeout”和“tcp-keepalive”，如果匹配上了，就将 server 参数设置为配置项的值。\n\n同时，代码还会检查配置项的值是否合理，比如是否小于 0。如果参数值不合理，程序在运行时就会报错。另外对于其他的配置项，loadServerConfigFromString 函数还会继续使用 elseif 分支进行判断。\n\nloadServerConfigFromString(char *config) {\n   …\n   //参数名匹配，检查参数是否为“timeout“\n   if (!strcasecmp(argv[0],"timeout") && argc == 2) {\n            //设置server的maxidletime参数\n  server.maxidletime = atoi(argv[1]);\n  //检查参数值是否小于0，小于0则报错\n            if (server.maxidletime < 0) {\n                err = "Invalid timeout value"; goto loaderr;\n            }\n   }\n  //参数名匹配，检查参数是否为“tcp-keepalive“\n  else if (!strcasecmp(argv[0],"tcp-keepalive") && argc == 2) {\n            //设置server的tcpkeepalive参数\n  server.tcpkeepalive = atoi(argv[1]);\n  //检查参数值是否小于0，小于0则报错\n            if (server.tcpkeepalive < 0) {\n                err = "Invalid tcp-keepalive value"; goto loaderr;\n            }\n   }\n   …\n}\n\n\n好了，到这里，你应该就了解了 Redis server 运行参数配置的步骤，我也画了一张图，以便你更直观地理解这个过程。\n\n\n\n在完成参数配置后，main 函数会开始调用 initServer 函数，对 server 进行初始化。所以接下来，我们继续来了解 Redis server 初始化时的关键操作。\n\n\n# initServer：初始化 Redis server\n\nRedis server 的初始化操作，主要可以分成三个步骤。\n\n * 第一步，Redis server 运行时需要对多种资源进行管理。\n\n比如说，和 server 连接的客户端、从库等，Redis 用作缓存时的替换候选集，以及 server 运行时的状态信息，这些资源的管理信息都会在 initServer 函数中进行初始化。\n\n我给你举个例子，initServer 函数会创建链表来分别维护客户端和从库，并调用 evictionPoolAlloc 函数（在evict.c中）采样生成用于淘汰的候选 key 集合。同时，initServer 函数还会调用 resetServerStats 函数（在 server.c 中）重置 server 运行状态信息。\n\n * 第二步，在完成资源管理信息的初始化后，initServer 函数会对 Redis 数据库进行初始化。\n\n因为一个 Redis 实例可以同时运行多个数据库，所以 initServer 函数会使用一个循环，依次为每个数据库创建相应的数据结构。\n\n这个代码逻辑是实现在 initServer 函数中，它会为每个数据库执行初始化操作，包括创建全局哈希表，为过期 key、被 BLPOP 阻塞的 key、将被 PUSH 的 key 和被监听的 key 创建相应的信息表。\n\nfor (j = 0; j < server.dbnum; j++) {\n    //创建全局哈希表\n    server.db[j].dict = dictCreate(&dbDictType,NULL);\n    //创建过期key的信息表\n    server.db[j].expires = dictCreate(&keyptrDictType,NULL);\n    //为被BLPOP阻塞的key创建信息表\n    server.db[j].blocking_keys = dictCreate(&keylistDictType,NULL);\n    //为将执行PUSH的阻塞key创建信息表\n    server.db[j].ready_keys = dictCreate(&objectKeyPointerValueDictType,NULL);\n    //为被MULTI/WATCH操作监听的key创建信息表\n    server.db[j].watched_keys = dictCreate(&keylistDictType,NULL);\n    …\n}\n\n\n * 第三步，initServer 函数会为运行的 Redis server 创建事件驱动框架，并开始启动端口监听，用于接收外部请求。\n\n注意，为了高效处理高并发的外部请求，initServer 在创建的事件框架中，针对每个监听 IP 上可能发生的客户端连接，都创建了监听事件，用来监听客户端连接请求。同时，initServer 为监听事件设置了相应的 处理函数 acceptTcpHandler。\n\n这样一来，只要有客户端连接到 server 监听的 IP 和端口，事件驱动框架就会检测到有连接事件发生，然后调用 acceptTcpHandler 函数来处理具体的连接。你可以参考以下代码中展示的处理逻辑：\n\n//创建事件循环框架\nserver.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR);\n…\n//开始监听设置的网络端口\nif (server.port != 0 &&\n        listenToPort(server.port,server.ipfd,&server.ipfd_count) == C_ERR)\n        exit(1);\n…\n//为server后台任务创建定时事件\nif (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR) {\n        serverPanic("Can\'t create event loop timers.");\n        exit(1);\n}\n…\n//为每一个监听的IP设置连接事件的处理函数acceptTcpHandler\nfor (j = 0; j < server.ipfd_count; j++) {\n        if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,\n            acceptTcpHandler,NULL) == AE_ERR)\n       { … }\n}\n\n\n那么到这里，Redis server 在完成运行参数设置和初始化后，就可以开始处理客户端请求了。为了能持续地处理并发的客户端请求，server 在 main 函数的最后，会进入事件驱动循环机制。而这就是接下来，我们要了解的事件驱动框架的执行过程。\n\n\n# 执行事件驱动框架\n\n事件驱动框架是 Redis server 运行的核心。该框架一旦启动后，就会一直循环执行，每次循环会处理一批触发的网络读写事件。关于事件驱动框架本身的设计思想与实现方法，我会在后面给你具体讲解\n\n这节课，我们主要是学习 Redis 入口的 main 函数中，是如何转换到事件驱动框架进行执行的\n\n其实，进入事件驱动框架开始执行并不复杂，main 函数直接调用事件框架的 主体函数 aeMain（在ae.c文件中）后，就进入事件处理循环了。\n\n当然，在进入事件驱动循环前，main 函数会分别调用 aeSetBeforeSleepProc 和 aeSetAfterSleepProc 两个函数，来设置每次进入事件循环前 server 需要执行的操作，以及每次事件循环结束后 server 需要执行的操作。下面代码显示了这部分的执行逻辑，你可以看下。\n\nint main(int argc, char **argv) {\n    ···\n    aeSetBeforeSleepProc(server.el,beforeSleep);\n    aeSetAfterSleepProc(server.el,afterSleep);\n    aeMain(server.el);\n    aeDeleteEventLoop(server.el);\n\t···\n}\n\n\n\n# 总结\n\n今天这节课，我们通过 server.c 文件中 main 函数的设计和实现思路，了解了 Redis server 启动后的五个主要阶段。在这五个阶段中，运行参数解析、server 初始化和执行事件驱动框架则是 Redis sever 启动过程中的三个关键阶段。所以相应的，我们需要重点关注以下三个要点。\n\n第一，main 函数是使用 initServerConfig 给 server 运行参数设置默认值，然后会解析命令行参数，并通过 loadServerConfig 读取配置文件参数值，将命令行参数追加至配置项字符串。最后，Redis 会调用 loadServerConfigFromString 函数，来完成配置文件参数和命令行参数的设置。\n\n第二，在 Redis server 完成参数设置后，initServer 函数会被调用，用来初始化 server 资源管理的主要结构，同时会初始化数据库启动状态，以及完成 server 监听 IP 和端口的设置。\n\n第三，一旦 server 可以接收外部客户端的请求后，main 函数会把程序的主体控制权，交给事件驱动框架的入口函数，也就 aeMain 函数。aeMain 函数会一直循环执行，处理收到的客户端请求。到此为止，server.c 中的 main 函数功能就已经全部完成了，程序控制权也交给了事件驱动循环框架，Redis 也就可以正常处理客户端请求了。\n\n实际上，Redis server 的启动过程从基本的初始化操作，到命令行和配置文件的参数解析设置，再到初始化 server 各种数据结构，以及最后的执行事件驱动框架，这是一个典型的网络服务器执行过程，你在开发网络服务器时，就可以作为参考。\n\n而且，掌握了启动过程中的初始化操作，还可以帮你解答一些使用中的疑惑。比如，Redis 启动时是先读取 RDB 文件，还是先读取 AOF 文件。如果你了解了 Redis server 的启动过程，就可以从 loadDataFromDisk 函数中看到，Redis server 会先读取 AOF；而如果没有 AOF，则再读取 RDB。\n\n所以，掌握 Redis server 启动过程，有助于你更好地了解 Redis 运行细节，这样当你遇到问题时，就知道还可以从启动过程中去溯源 server 的各种初始状态，从而助力你更好地解决问题。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 如何高效执行网络服务器启动的每一步？redis 的 main 函数能带来什么启示？\n 2. redis server 启动后如何确保关键参数正确加载？有哪些常见陷阱？\n 3. redis 如何通过事件驱动模型处理高并发请求？从 main 函数能学到什么？\n 4. 为什么初始化和加载配置参数对 redis 启动至关重要？它如何影响客户端请求处理？\n 5. 如何通过 initserver 函数理解并调优 redis 配置参数以提升性能？\n 6. redis 启动时如何决定加载 aof 还是 rdb？这种设计对数据恢复的影响是什么？\n\n\n# 前言\n\n我们知道，main 函数是 redis 整个运行程序的入口。同时，由于 redis 是典型的 client-server 架构，一旦 redis 实例开始运行，redis server 也就会启动，所以 main 函数其实也会负责 redis server 的启动运行。\n\n> main 函数是在 server.c 中\n\n你平常在设计或实现一个网络服务器程序时，可能会遇到一个问题，那就是\n\n服务器启动时，应该做哪些操作、有没有一个典型的参考实现\n\n所以我就从 main 函数开始，给你介绍下 redis server 是如何在 main 函数中启动并完成初始化的。通过这节课内容的学习，你可以掌握 redis 针对以下三个问题的实现思路：\n\n 1. redis server 启动后具体会做哪些初始化操作？\n 2. redis server 初始化时有哪些关键配置项？\n 3. redis server 如何开始处理客户端请求？\n\n\n# main 函数：redis server 的入口\n\n一般来说，一个使用 c 开发的系统软件启动运行的代码逻辑，都是实现在了 main 函数当中，所以 echo 找到了 main 函数，看看它的执行过程\n\n那么，对于 redis 的 main 函数来说，我把它执行的工作分成了五个阶段。\n\n\n# 阶段一：基本初始化\n\n在这个阶段，main 函数主要是完成一些基本的初始化工作，包括设置 server 运行的时区、设置哈希函数的随机种子等。这部分工作的主要调用函数如下所示：\n\n//设置时区\nsetlocale(lc_collate,"");\ntzset();\n...\n//设置随机种子\nchar hashseed[16];\ngetrandomhexchars(hashseed,sizeof(hashseed));\ndictsethashfunctionseed((uint8_t*)hashseed);\n\n\n这里，你需要注意的是，在 main 函数的开始部分，有一段宏定义覆盖的代码。这部分代码的作用是，如果定义了 redis_test 宏定义，并且 redis server 启动时的参数符合测试参数，那么 main 函数就会执行相应的测试程序。\n\n这段宏定义的代码如以下所示，其中的示例代码就是调用 ziplist 的测试函数 ziplisttest：\n\n#ifdef redis_test\n//如果启动参数有test和ziplist，那么就调用ziplisttest函数进行ziplist的测试\nif (argc == 3 && !strcasecmp(argv[1], "test")) {\n  if (!strcasecmp(argv[2], "ziplist")) {\n     return ziplisttest(argc, argv);\n  }\n  ...\n}\n#endif\n\n\n\n# 阶段二：检查哨兵模式，并检查是否要执行 rdb 检测或 aof 检测\n\nredis server 启动后，可能是以哨兵模式运行的，而哨兵模式运行的 server 在参数初始化、参数设置，以及 server 启动过程中要执行的操作等方面，与普通模式 server 有所差别。所以，main 函数在执行过程中需要根据 redis 配置的参数，检查是否设置了哨兵模式。\n\n如果有设置哨兵模式的话，main 函数会调用 initsentinelconfig 函数，对哨兵模式的参数进行初始化设置，以及调用 initsentinel 函数，初始化设置哨兵模式运行的 server。有关哨兵模式运行的 redis server 相关机制，我会在第 21 讲中给你详细介绍。\n\n下面的代码展示了 main 函数中对哨兵模式的检查，以及对哨兵模式的初始化，你可以看下：\n\n...\n//判断server是否设置为哨兵模式\nif (server.sentinel_mode) {\n    initsentinelconfig();  //初始化哨兵的配置\n    initsentinel();   //初始化哨兵模式\n}\n...\n\n\n除了检查哨兵模式以外，main 函数还会检查是否要执行 rdb 检测或 aof 检查，这对应了实际运行的程序是 redis-check-rdb 或 redis-check-aof。在这种情况下，main 函数会调用 redis_check_rdb_main 函数或 redis_check_aof_main 函数，检测 rdb 文件或 aof 文件。你可以看看下面的代码，其中就展示了 main 函数对这部分内容的检查和调用：\n\n...\n//如果运行的是redis-check-rdb程序，调用redis_check_rdb_main函数检测rdb文件\nif (strstr(argv[0],"redis-check-rdb") != null)\n   redis_check_rdb_main(argc,argv,null);\n//如果运行的是redis-check-aof程序，调用redis_check_aof_main函数检测aof文件\nelse if (strstr(argv[0],"redis-check-aof") != null)\n   redis_check_aof_main(argc,argv);\n...\n\n\n\n# 阶段三：运行参数解析\n\n在这一阶段，main 函数会对命令行传入的参数进行解析，并且调用 loadserverconfig 函数，对命令行参数和配置文件中的参数进行合并处理，然后为 redis 各功能模块的关键参数设置合适的取值，以便 server 能高效地运行。\n\n\n# 阶段四：初始化 server\n\n在完成对运行参数的解析和设置后，main 函数会调用 initserver 函数，对 server 运行时的各种资源进行初始化工作。这主要包括了 server 资源管理所需的数据结构初始化、键值对数据库初始化、server 网络框架初始化等。\n\n而在调用完 initserver 后，main 函数还会再次判断当前 server 是否为哨兵模式。如果是哨兵模式，main 函数会调用 sentinelisrunning 函数，设置启动哨兵模式。否则的话，main 函数会调用 loaddatafromdisk 函数，从磁盘上加载 aof 或者是 rdb 文件，以便恢复之前的数据。\n\n\n# 阶段五：执行事件驱动框架\n\n为了能高效处理高并发的客户端连接请求，redis 采用了事件驱动框架，来并发处理不同客户端的连接和读写请求。所以，main 函数执行到最后时，会调用 aemain 函数进入事件驱动框架，开始循环处理各种触发的事件。\n\n我把刚才介绍的五个阶段涉及到的关键操作，画在了下面的图中，你可以再回顾下。\n\n那么，在这五个阶段当中，阶段三、四和五其实就包括了 redis server 启动过程中的关键操作。所以接下来，我们就来依次学习下这三个阶段中的主要工作。\n\n\n\n\n# redis 运行参数解析与设置\n\n我们知道，redis 提供了丰富的功能，既支持多种键值对数据类型的读写访问，还支持数据持久化保存、主从复制、切片集群等。而这些功能的高效运行，其实都离不开相关功能模块的关键参数配置。\n\n举例来说，redis 为了节省内存，设计了内存紧凑型的数据结构来保存 hash、sorted set 等键值对类型。但是在使用了内存紧凑型的数据结构之后，如果往数据结构存入的元素个数过多或元素过大的话，键值对的访问性能反而会受到影响。因此，为了平衡内存使用量和系统访问性能，我们就可以通过参数，来设置和调节内存紧凑型数据结构的使用条件。\n\n也就是说，掌握这些关键参数的设置，可以帮助我们提升 redis 实例的运行效率。\n\n不过，redis 的参数有很多，我们无法在一节课中掌握所有的参数设置。所以下面，我们可以先来学习下 redis 的主要参数类型，这样就能对各种参数形成一个全面的了解。同时，我也会给你介绍一些和 server 运行关系密切的参数及其设置方法，以便你可以配置好这些参数，让 server 高效运行起来。\n\n\n# redis 的主要参数类型\n\n首先，redis 运行所需的各种参数，都统一定义在了server.h文件的 redisserver 结构体中。根据参数作用的范围，我把各种参数划分为了七大类型，包括通用参数、数据结构参数、网络参数、持久化参数、主从复制参数、切片集群参数、性能优化参数。具体你可以参考下面表格中的内容。\n\n\n\n这样，如果你能按照上面的划分方法给 redis 参数进行归类，那么你就可以发现，这些参数实际和 redis 的主要功能机制是相对应的。所以，如果你要深入掌握这些参数的典型配置值，你就需要对相应功能机制的工作原理有所了解。我在接下来的课程中，也会在介绍 redis 功能模块设计的同时，带你了解下其相应的典型参数配置。\n\n好，现在我们就了解了 redis 的七大参数类型，以及它们基本的作用范围，那么下面我们就接着来学习下，redis 是如何针对这些参数进行设置的。\n\n\n# redis 参数的设置方法\n\nredis 对运行参数的设置实际上会经过三轮赋值，分别是默认配置值、命令行启动参数，以及配置文件配置值。\n\n首先，redis 在 main 函数中会先调用 initserverconfig 函数，为各种参数设置默认值。参数的默认值统一定义在 server.h 文件中，都是以 config_default 开头的宏定义变量。下面的代码显示的是部分参数的默认值，你可以看下。\n\n#define config_default_hz        10   //server后台任务的默认运行频率\n#define config_min_hz            1    // server后台任务的最小运行频率\n#define config_max_hz            500 // server后台任务的最大运行频率\n#define config_default_server_port  6379  //server监听的默认tcp端口\n#define config_default_client_timeout  0  //客户端超时时间，默认为0，表示没有超时限制\n\n\n在 server.h 中提供的默认参数值，一般都是典型的配置值。因此，如果你在部署使用 redis 实例的过程中，对 redis 的工作原理不是很了解，就可以使用代码中提供的默认配置。\n\n当然，如果你对 redis 各功能模块的工作机制比较熟悉的话，也可以自行设置运行参数。你可以在启动 redis 程序时，在命令行上设置运行参数的值。比如，如果你想将 redis server 监听端口从默认的 6379 修改为 7379，就可以在命令行上设置 port 参数为 7379，如下所示：\n\n./redis-server --port 7379\n\n\n这里，你需要注意的是，redis 的命令行参数设置需要使用**两个减号“–”**来表示相应的参数名，否则的话，redis 就无法识别所设置的运行参数。\n\nredis 在使用 initserverconfig 函数对参数设置默认配置值后，接下来，main 函数就会对 redis 程序启动时的命令行参数进行逐一解析。\n\nmain 函数会把解析后的参数及参数值保存成字符串，接着，main 函数会调用 loadserverconfig 函数进行第二和第三轮的赋值。以下代码显示了 main 函数对命令行参数的解析，以及调用 loadserverconfig 函数的过程，你可以看下。\n\nint main(int argc, char **argv) {\n…\n//保存命令行参数\nfor (j = 0; j < argc; j++) server.exec_argv[j] = zstrdup(argv[j]);\n…\nif (argc >= 2) {\n   …\n   //对每个运行时参数进行解析\n   while(j != argc) {\n      …\n   }\n   …\n   //\n   loadserverconfig(configfile,options);\n}\n\n\n这里你要知道的是，loadserverconfig 函数是在config.c文件中实现的，该函数是以 redis 配置文件和命令行参数的解析字符串为参数，将配置文件中的所有配置项读取出来，形成字符串。紧接着，loadserverconfig 函数会把解析后的命令行参数，追加到配置文件形成的配置项字符串。\n\n这样一来，配置项字符串就同时包含了配置文件中设置的参数，以及命令行设置的参数。\n\n最后，loadserverconfig 函数会进一步调用 loadserverconfigfromstring 函数，对配置项字符串中的每一个配置项进行匹配。一旦匹配成功，loadserverconfigfromstring 函数就会按照配置项的值设置 server 的参数。\n\n以下代码显示了 loadserverconfigfromstring 函数的部分内容。这部分代码是使用了条件分支，来依次比较配置项是否是“timeout”和“tcp-keepalive”，如果匹配上了，就将 server 参数设置为配置项的值。\n\n同时，代码还会检查配置项的值是否合理，比如是否小于 0。如果参数值不合理，程序在运行时就会报错。另外对于其他的配置项，loadserverconfigfromstring 函数还会继续使用 elseif 分支进行判断。\n\nloadserverconfigfromstring(char *config) {\n   …\n   //参数名匹配，检查参数是否为“timeout“\n   if (!strcasecmp(argv[0],"timeout") && argc == 2) {\n            //设置server的maxidletime参数\n  server.maxidletime = atoi(argv[1]);\n  //检查参数值是否小于0，小于0则报错\n            if (server.maxidletime < 0) {\n                err = "invalid timeout value"; goto loaderr;\n            }\n   }\n  //参数名匹配，检查参数是否为“tcp-keepalive“\n  else if (!strcasecmp(argv[0],"tcp-keepalive") && argc == 2) {\n            //设置server的tcpkeepalive参数\n  server.tcpkeepalive = atoi(argv[1]);\n  //检查参数值是否小于0，小于0则报错\n            if (server.tcpkeepalive < 0) {\n                err = "invalid tcp-keepalive value"; goto loaderr;\n            }\n   }\n   …\n}\n\n\n好了，到这里，你应该就了解了 redis server 运行参数配置的步骤，我也画了一张图，以便你更直观地理解这个过程。\n\n\n\n在完成参数配置后，main 函数会开始调用 initserver 函数，对 server 进行初始化。所以接下来，我们继续来了解 redis server 初始化时的关键操作。\n\n\n# initserver：初始化 redis server\n\nredis server 的初始化操作，主要可以分成三个步骤。\n\n * 第一步，redis server 运行时需要对多种资源进行管理。\n\n比如说，和 server 连接的客户端、从库等，redis 用作缓存时的替换候选集，以及 server 运行时的状态信息，这些资源的管理信息都会在 initserver 函数中进行初始化。\n\n我给你举个例子，initserver 函数会创建链表来分别维护客户端和从库，并调用 evictionpoolalloc 函数（在evict.c中）采样生成用于淘汰的候选 key 集合。同时，initserver 函数还会调用 resetserverstats 函数（在 server.c 中）重置 server 运行状态信息。\n\n * 第二步，在完成资源管理信息的初始化后，initserver 函数会对 redis 数据库进行初始化。\n\n因为一个 redis 实例可以同时运行多个数据库，所以 initserver 函数会使用一个循环，依次为每个数据库创建相应的数据结构。\n\n这个代码逻辑是实现在 initserver 函数中，它会为每个数据库执行初始化操作，包括创建全局哈希表，为过期 key、被 blpop 阻塞的 key、将被 push 的 key 和被监听的 key 创建相应的信息表。\n\nfor (j = 0; j < server.dbnum; j++) {\n    //创建全局哈希表\n    server.db[j].dict = dictcreate(&dbdicttype,null);\n    //创建过期key的信息表\n    server.db[j].expires = dictcreate(&keyptrdicttype,null);\n    //为被blpop阻塞的key创建信息表\n    server.db[j].blocking_keys = dictcreate(&keylistdicttype,null);\n    //为将执行push的阻塞key创建信息表\n    server.db[j].ready_keys = dictcreate(&objectkeypointervaluedicttype,null);\n    //为被multi/watch操作监听的key创建信息表\n    server.db[j].watched_keys = dictcreate(&keylistdicttype,null);\n    …\n}\n\n\n * 第三步，initserver 函数会为运行的 redis server 创建事件驱动框架，并开始启动端口监听，用于接收外部请求。\n\n注意，为了高效处理高并发的外部请求，initserver 在创建的事件框架中，针对每个监听 ip 上可能发生的客户端连接，都创建了监听事件，用来监听客户端连接请求。同时，initserver 为监听事件设置了相应的 处理函数 accepttcphandler。\n\n这样一来，只要有客户端连接到 server 监听的 ip 和端口，事件驱动框架就会检测到有连接事件发生，然后调用 accepttcphandler 函数来处理具体的连接。你可以参考以下代码中展示的处理逻辑：\n\n//创建事件循环框架\nserver.el = aecreateeventloop(server.maxclients+config_fdset_incr);\n…\n//开始监听设置的网络端口\nif (server.port != 0 &&\n        listentoport(server.port,server.ipfd,&server.ipfd_count) == c_err)\n        exit(1);\n…\n//为server后台任务创建定时事件\nif (aecreatetimeevent(server.el, 1, servercron, null, null) == ae_err) {\n        serverpanic("can\'t create event loop timers.");\n        exit(1);\n}\n…\n//为每一个监听的ip设置连接事件的处理函数accepttcphandler\nfor (j = 0; j < server.ipfd_count; j++) {\n        if (aecreatefileevent(server.el, server.ipfd[j], ae_readable,\n            accepttcphandler,null) == ae_err)\n       { … }\n}\n\n\n那么到这里，redis server 在完成运行参数设置和初始化后，就可以开始处理客户端请求了。为了能持续地处理并发的客户端请求，server 在 main 函数的最后，会进入事件驱动循环机制。而这就是接下来，我们要了解的事件驱动框架的执行过程。\n\n\n# 执行事件驱动框架\n\n事件驱动框架是 redis server 运行的核心。该框架一旦启动后，就会一直循环执行，每次循环会处理一批触发的网络读写事件。关于事件驱动框架本身的设计思想与实现方法，我会在后面给你具体讲解\n\n这节课，我们主要是学习 redis 入口的 main 函数中，是如何转换到事件驱动框架进行执行的\n\n其实，进入事件驱动框架开始执行并不复杂，main 函数直接调用事件框架的 主体函数 aemain（在ae.c文件中）后，就进入事件处理循环了。\n\n当然，在进入事件驱动循环前，main 函数会分别调用 aesetbeforesleepproc 和 aesetaftersleepproc 两个函数，来设置每次进入事件循环前 server 需要执行的操作，以及每次事件循环结束后 server 需要执行的操作。下面代码显示了这部分的执行逻辑，你可以看下。\n\nint main(int argc, char **argv) {\n    ···\n    aesetbeforesleepproc(server.el,beforesleep);\n    aesetaftersleepproc(server.el,aftersleep);\n    aemain(server.el);\n    aedeleteeventloop(server.el);\n\t···\n}\n\n\n\n# 总结\n\n今天这节课，我们通过 server.c 文件中 main 函数的设计和实现思路，了解了 redis server 启动后的五个主要阶段。在这五个阶段中，运行参数解析、server 初始化和执行事件驱动框架则是 redis sever 启动过程中的三个关键阶段。所以相应的，我们需要重点关注以下三个要点。\n\n第一，main 函数是使用 initserverconfig 给 server 运行参数设置默认值，然后会解析命令行参数，并通过 loadserverconfig 读取配置文件参数值，将命令行参数追加至配置项字符串。最后，redis 会调用 loadserverconfigfromstring 函数，来完成配置文件参数和命令行参数的设置。\n\n第二，在 redis server 完成参数设置后，initserver 函数会被调用，用来初始化 server 资源管理的主要结构，同时会初始化数据库启动状态，以及完成 server 监听 ip 和端口的设置。\n\n第三，一旦 server 可以接收外部客户端的请求后，main 函数会把程序的主体控制权，交给事件驱动框架的入口函数，也就 aemain 函数。aemain 函数会一直循环执行，处理收到的客户端请求。到此为止，server.c 中的 main 函数功能就已经全部完成了，程序控制权也交给了事件驱动循环框架，redis 也就可以正常处理客户端请求了。\n\n实际上，redis server 的启动过程从基本的初始化操作，到命令行和配置文件的参数解析设置，再到初始化 server 各种数据结构，以及最后的执行事件驱动框架，这是一个典型的网络服务器执行过程，你在开发网络服务器时，就可以作为参考。\n\n而且，掌握了启动过程中的初始化操作，还可以帮你解答一些使用中的疑惑。比如，redis 启动时是先读取 rdb 文件，还是先读取 aof 文件。如果你了解了 redis server 的启动过程，就可以从 loaddatafromdisk 函数中看到，redis server 会先读取 aof；而如果没有 aof，则再读取 rdb。\n\n所以，掌握 redis server 启动过程，有助于你更好地了解 redis 运行细节，这样当你遇到问题时，就知道还可以从启动过程中去溯源 server 的各种初始状态，从而助力你更好地解决问题。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Redis 的 Reactor 模型",frontmatter:{title:"Redis 的 Reactor 模型",date:"2024-09-15T17:26:35.000Z",permalink:"/pages/d6b00d"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%BB%E7%BA%BF/05.Redis%20%E7%9A%84%20Reactor%20%E6%A8%A1%E5%9E%8B.html",relativePath:"Redis 系统设计/03.主线/05.Redis 的 Reactor 模型.md",key:"v-2f9ca694",path:"/pages/d6b00d/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:159},{level:2,title:"Reactor 模型的工作机制",slug:"reactor-模型的工作机制",normalizedTitle:"reactor 模型的工作机制",charIndex:362},{level:3,title:"事件类型",slug:"事件类型",normalizedTitle:"事件类型",charIndex:52},{level:3,title:"关键角色",slug:"关键角色",normalizedTitle:"关键角色",charIndex:75},{level:3,title:"事件驱动框架",slug:"事件驱动框架",normalizedTitle:"事件驱动框架",charIndex:1524},{level:2,title:"Redis 对 Reactor 模型的实现",slug:"redis-对-reactor-模型的实现",normalizedTitle:"redis 对 reactor 模型的实现",charIndex:2307},{level:3,title:"事件的数据结构定义：以 aeFileEvent 为例",slug:"事件的数据结构定义-以-aefileevent-为例",normalizedTitle:"事件的数据结构定义：以 aefileevent 为例",charIndex:2629},{level:3,title:"主循环：aeMain 函数",slug:"主循环-aemain-函数",normalizedTitle:"主循环：aemain 函数",charIndex:3733},{level:3,title:"事件捕获与分发：aeProcessEvents 函数",slug:"事件捕获与分发-aeprocessevents-函数",normalizedTitle:"事件捕获与分发：aeprocessevents 函数",charIndex:4686},{level:3,title:"事件注册：aeCreateFileEvent 函数",slug:"事件注册-aecreatefileevent-函数",normalizedTitle:"事件注册：aecreatefileevent 函数",charIndex:7314},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:438},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:9228}],headersStr:"前言 Reactor 模型的工作机制 事件类型 关键角色 事件驱动框架 Redis 对 Reactor 模型的实现 事件的数据结构定义：以 aeFileEvent 为例 主循环：aeMain 函数 事件捕获与分发：aeProcessEvents 函数 事件注册：aeCreateFileEvent 函数 总结 参考资料",content:'提出问题是一切智慧的开端\n\n * Reactor 模型中有哪些组件？\n * Reactor 模型中有哪些事件类型？\n * Reactor 模型中有哪些关键角色？每个角色的作用？\n * 代码整体控制逻辑有哪些重要部分？\n * Redis 如何实现 Reactor 模型？\n * 如何注册事件？如何循环监听事件？\n\n\n# 前言\n\nRedis的网络框架是实现了Reactor模型吗？\n\n建议分为两部分来思考：\n\n * Reactor 模型是什么\n * Redis 代码实现是如何与 Reactor 模型相对应的\n\nReactor 模型是高性能网络系统实现高并发请求处理的一个重要技术方案。掌握Reactor模型的设计思想与实现方法，可以指导你设计和实现自己的高并发系统。当你要处理成千上万的网络连接时，就不会一筹莫展了。\n\n\n# Reactor 模型的工作机制\n\n实际上，Reactor 模型就是网络服务器端用来处理高并发网络IO请求的一种编程模型。我把这个模型的特征用两个「三」来总结，也就是：\n\n * 三类处理事件，连接事件、写事件、读事件；\n * 三个关键角色，即 reactor、acceptor、handler。\n\n那么，Reactor 模型是如何基于这三类事件和三个角色来处理高并发请求的呢？下面我们就来具体了解下。\n\n\n# 事件类型\n\n我们先来看看这三类事件和 Reactor 模型的关系。\n\n其实，Reactor 模型处理的是客户端和服务器端的 交互过程，而这三类事件正好对应了客户端和服务器端交互过程中，不同类请求在服务器端引发的待处理事件：\n\n * 连接事件\n   * 当一个客户端要和服务器端进行交互时，客户端会向服务器端发送连接请求，以建立连接，这就对应了服务器端的一个连接事件\n * 写事件\n   * 一旦连接建立后，客户端会给服务器端发送读请求，以便读取数据。服务器端在处理读请求时，需要向客户端写回数据，这对应了服务器端的写事件\n * 读事件\n   * 无论客户端给服务器端发送读或写请求，服务器端都需要从客户端读取请求内容，所以在这里，读或写请求的读取就对应了服务器端的读事件\n\n如下所示的图例中，就展示了客户端和服务器端在交互过程中，不同类请求和Reactor模型事件的对应关系，你可以看下。\n\n\n\n好，在了解了 Reactor 模型的三类事件后，你现在可能还有一个疑问：这三类事件是由谁来处理的呢？\n\n\n# 关键角色\n\n这其实就是模型中 三个关键角色 的作用了：\n\n * acceptor ：连接事件由 acceptor 来处理，负责接收连接；acceptor 在接收连接后，会创建 handler，用于网络连接上对后续读写事件的处理；\n * handler：读写事件由 handler 处理；\n * reactor：在高并发场景中，连接事件、读写事件会同时发生，所以，我们需要有一个角色专门监听和分配事件，这就是 reactor 角色。当有连接请求时，reactor 将产生的连接事件交由 acceptor 处理；当有读写请求时，reactor 将读写事件交由 handler 处理。\n\n下图就展示了这三个角色之间的关系，以及它们和事件的关系，你可以看下\n\n\n\n事实上，这三个角色都是Reactor模型中要实现的功能的抽象。\n\n当我们遵循Reactor模型开发服务器端的网络框架时，就需要在编程的时候，在代码功能模块中实现reactor、acceptor和handler 的逻辑\n\n那么，现在我们已经知道，这三个角色是围绕事件的监听、转发和处理来进行交互的，那么在编程时，我们又该如何实现这三者的交互呢？这就离不开 事件驱动框架了\n\n\n# 事件驱动框架\n\n所谓的事件驱动框架，就是在实现 Reactor 模型时，需要实现的代码整体控制逻辑。\n\n简单来说，事件驱动框架包括了两部分\n\n * 一是 事件初始化\n * 二是 事件捕获、分发和处理主循环\n\n事件初始化是在服务器程序启动时就执行的，它的作用主要是 创建需要监听的事件类型，以及该类事件对应的handler。\n\n而一旦服务器完成初始化后，事件初始化也就相应完成了，服务器程序就需要进入到事件捕获、分发和处理的主循环中。\n\n在开发代码时，我们通常会用一个 while循环 来作为这个主循环。\n\n然后在这个主循环中，我们需要\n\n 1. 捕获发生的事件\n 2. 判断事件类型\n 3. 根据事件类型，调用在初始化时创建好的事件handler来实际处理事件\n\n比如说，当有连接事件发生时，服务器程序需要调用acceptor处理函数，创建和客户端的连接。而当有读事件发生时，就表明有读或写请求发送到了服务器端，服务器程序就要调用具体的请求处理函数，从客户端连接中读取请求内容，进而就完成了读事件的处理。这里你可以参考下面给出的图例，其中显示了事件驱动框架的基本执行过程：\n\n\n\n那么到这里，你应该就已经了解了Reactor模型的基本工作机制：客户端的不同类请求会在服务器端触发连接、读、写三类事件，这三类事件的监听、分发和处理又是由reactor、acceptor、handler三类角色来完成的，然后这三类角色会通过事件驱动框架来实现交互和事件处理。\n\n所以可见，实现一个Reactor模型的关键，就是要实现事件驱动框架。那么，如何开发实现一个事件驱动框架呢？\n\nRedis 提供了一个简洁但有效的参考实现，非常值得我们学习，而且也可以用于自己的网络系统开发。下面，我们就一起来学习下Redis中对 Reactor 模型的实现。\n\n\n# Redis 对 Reactor 模型的实现\n\n首先我们要知道的是，Redis 的网络框架实现了 Reactor 模型，并且自行开发实现了一个事件驱动框架。这个框架对应的Redis代码实现文件是ae.c，对应的头文件是ae.h。\n\n前面我们已经知道，事件驱动框架的实现离不开事件的定义，以及事件注册、捕获、分发和处理等一系列操作。当然，对于整个框架来说，还需要能一直运行，持续地响应发生的事件。\n\n那么由此，我们从ae.h头文件中就可以看到，Redis为了实现事件驱动框架，相应地定义了\n\n * 事件的数据结构\n * 框架主循环函数\n * 事件捕获分发函数\n * 事件\n * handler注册函数\n\n所以接下来，我们就依次来了解学习下\n\n\n# 事件的数据结构定义：以 aeFileEvent 为例\n\n在 Redis 事件驱动框架的实现当中，事件的数据结构是关联事件类型和事件处理函数的关键要素。而Redis的事件驱动框架定义了两类事件：\n\n * IO事件：对应 客户端发送的 网络请求\n * 时间事件：对应 Redis 自身的周期性操作\n\n注意：不同类型事件的数据结构定义是不一样的\n\n为了让你能够理解事件数据结构对框架的作用，我就以 IO 事件 aeFileEvent 为例，给你介绍下它的数据结构定义\n\ntypedef struct aeFileEvent {\n    int mask; /* one of AE_(READABLE|WRITABLE|BARRIER) */\n    aeFileProc *rfileProc;\n    aeFileProc *wfileProc;\n    void *clientData;\n} aeFileEvent;\n\n\n * **mask **是用来 表示事件类型 的掩码。对于 IO事件 来说，主要有 AE_READABLE、AE_WRITABLE 和 AE_BARRIER 三种类型事件。框架在分发事件时，依赖的就是结构体中的事件类型\n * rfileProc 和 wfileProce 分别是指向 AE_READABLE 和 AE_WRITABLE 这两类事件的处理函数，也就是 Reactor 模型中的 handler。框架在分发事件后，就需要调用结构体中定义的函数进行事件处理\n * 最后一个成员变量 clientData 是用来指向客户端私有数据的指针\n\n除了事件的数据结构以外，前面我还提到 Redis 在 ae.h 文件中，定义了支撑框架运行的主要函数\n\n * 负责框架主循环的 aeMain 函数\n * 负责事件捕获与分发的 aeProcessEvents 函数\n * 负责事件和 handler 注册的 aeCreateFileEvent 函数\n\n它们的原型定义如下\n\nvoid aeMain(aeEventLoop *eventLoop);\nint aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData);\nint aeProcessEvents(aeEventLoop *eventLoop, int flags);\n\n\n而这三个函数的实现，都是在对应的 ae.c 文件中，那么接下来，我就给你具体介绍下这三个函数的主体逻辑和关键流程\n\n\n# 主循环：aeMain 函数\n\n我们先来看下 aeMain 函数\n\naeMain 函数的逻辑很简单，就是 用一个循环不停地判断事件循环的停止标记。如果事件循环的停止标记被设置为 true，那么针对事件捕获、分发和处理的整个主循环就停止了；否则，主循环会一直执行。aeMain 函数的主体代码如下所示：\n\nvoid aeMain(aeEventLoop *eventLoop) {\n    eventLoop->stop = 0;\n    while (!eventLoop->stop) {\n        if (eventLoop->beforesleep != NULL)\n            eventLoop->beforesleep(eventLoop);\n        aeProcessEvents(eventLoop, AE_ALL_EVENTS|AE_CALL_AFTER_SLEEP);\n    }\n}\n\n\n那么这里你可能要问了，aeMain 函数是在哪里被调用的呢？\n\nint main(int argc, char **argv) {\n\t···\n\taeSetBeforeSleepProc(server.el,beforeSleep);\n    aeSetAfterSleepProc(server.el,afterSleep);\n    aeMain(server.el);\n    aeDeleteEventLoop(server.el);\n    return 0;\n}\n\n\n按照事件驱动框架的编程规范来说，框架主循环是在服务器程序初始化完成后，就会开始执行。因此，如果我们把目光转向 Redis 服务器初始化的函数，就会发现服务器程序的 main 函数在完成 Redis server 的初始化后，会调用 aeMain 函数开始执行事件驱动框架。如果你想具体查看main函数，main函数在server.c文件中，server.c主要用于初始化服务器和执行服务器整体控制流程，你可以回顾下。\n\n不过，既然aeMain函数包含了事件框架的主循环，**那么在主循环中，事件又是如何被捕获、分发和处理呢？**这就是由 aeProcessEvents 函数来完成的了\n\n\n# 事件捕获与分发：aeProcessEvents 函数\n\naeProcessEvents 函数实现的主要功能，包括\n\n * 捕获事件\n * 判断事件类型\n * 调用具体的事件处理函数，从而实现事件的处理\n\n从 aeProcessEvents 函数的主体结构中，我们可以看到主要有三个 if 条件分支，如下所示：\n\nint aeProcessEvents(aeEventLoop *eventLoop, int flags)\n{\n    int processed = 0, numevents;\n \n    /* 若没有事件处理，则立刻返回 */\n    if (!(flags & AE_TIME_EVENTS) && !(flags & AE_FILE_EVENTS)) return 0;\n    \n    /*如果有IO事件发生，或者紧急的时间事件发生，则开始处理*/\n    if (eventLoop->maxfd != -1 || ((flags & AE_TIME_EVENTS) && !(flags & AE_DONT_WAIT))) {\n       …\n    }\n    \n    /* 检查是否有时间事件，若有，则调用processTimeEvents函数处理 */\n    if (flags & AE_TIME_EVENTS)\n        processed += processTimeEvents(eventLoop);\n    \n    /* 返回已经处理的文件或时间*/\n    return processed; \n}\n\n\n这三个分支分别对应了以下三种情况：\n\n * 情况一：既没有时间事件，也没有网络事件\n * 情况二：有 IO 事件或者有需要紧急处理的时间事件\n * 情况三：只有普通的时间事件\n\n那么对于第一种情况来说，因为没有任何事件需要处理，aeProcessEvents 函数就会直接返回到 aeMain 的主循环，开始下一轮的循环；而对于第三种情况来说，该情况发生时只有普通时间事件发生，所以 aeMain 函数会调用专门处理时间事件的函数 processTimeEvents，对时间事件进行处理\n\n现在，我们再来看看第二种情况\n\n首先，当该情况发生时，Redis需要捕获发生的网络事件，并进行相应的处理。那么从Redis源码中我们可以分析得到，在这种情况下，aeApiPoll 函数会被调用，用来捕获事件，如下所示：\n\nint aeProcessEvents(aeEventLoop *eventLoop, int flags){\n   ...\n   if (eventLoop->maxfd != -1 || ((flags & AE_TIME_EVENTS) && !(flags & AE_DONT_WAIT))) {\n       ...\n       //调用 aeApiPoll 函数捕获事件\n       numevents = aeApiPoll(eventLoop, tvp);\n       ...\n    }\n    ...\n」\n\n\n那么，aeApiPoll是如何捕获事件呢？\n\n实际上，Redis是依赖于操作系统底层提供的 IO多路复用机制，来实现事件捕获，检查是否有新的连接、读写事件发生。为了适配不同的操作系统，Redis对不同操作系统实现的网络IO多路复用函数，都进行了统一的封装，封装后的代码分别通过以下四个文件中实现：\n\n * ae_epoll.c，对应Linux上的IO复用函数epoll；\n * ae_evport.c，对应Solaris上的IO复用函数evport；\n * ae_kqueue.c，对应macOS或FreeBSD上的IO复用函数kqueue；\n * ae_select.c，对应Linux（或Windows）的IO复用函数select。\n\n这样，在有了这些封装代码后，Redis 在不同的操作系统上调用 IO 多路复用 API 时，就可以通过统一的接口来进行调用了。\n\n不过看到这里，你可能还是不太明白 Redis 封装的具体操作，所以这里，我就以在服务器端最常用的 Linux 操作系统为例，给你介绍下Redis 是如何封装 Linux 上提供的 IO 复用 API 的。\n\n首先，Linux 上提供了epoll_wait API，用于检测内核中发生的网络IO事件。在ae_epoll.c文件中，aeApiPoll函数就是封装了对epoll_wait的调用。\n\n这个封装程序如下所示，其中你可以看到，在 aeApiPoll 函数中直接调用了 epoll_wait 函数，并将 epoll 返回的事件信息保存起来的逻辑：\n\nstatic int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) {\n    …\n    //调用 epoll_wait 获取监听到的事件\n    retval = epoll_wait(state->epfd,state->events,eventLoop->setsize,\n            tvp ? (tvp->tv_sec*1000 + tvp->tv_usec/1000) : -1);\n    if (retval > 0) {\n        int j;\n        //获得监听到的事件数量\n        numevents = retval;\n        //针对每一个事件，进行处理\n        for (j = 0; j < numevents; j++) {\n             //保存事件信息\n        }\n    }\n    return numevents;\n}\n\n\n为了让你更加清晰地理解，事件驱动框架是如何实现最终对epoll_wait的调用，这里我也放了一张示意图，你可以看看整个调用链是如何工作和实现的。\n\n\n\nOK，现在我们就已经在 aeMain 函数中，看到了 aeProcessEvents 函数被调用，并用于捕获和分发事件的基本处理逻辑。\n\n**那么，事件具体是由哪个函数来处理的呢？**这就和框架中的 aeCreateFileEvents 函数有关了。\n\n\n# 事件注册：aeCreateFileEvent 函数\n\n我们知道，当Redis启动后，服务器程序的 main 函数会调用 initSever 函数来进行初始化，而在初始化的过程中，aeCreateFileEvent 就会被 initServer 函数调用，用于注册要监听的事件，以及相应的事件处理函数。\n\n具体来说，在 initServer 函数的执行过程中，initServer 函数会根据启用的 IP 端口个数，为每个 IP 端口上的网络事件，调用aeCreateFileEvent，创建对 AE_READABLE 事件的监听，并且注册 AE_READABLE 事件的处理 handler，也就是 acceptTcpHandler 函数。这一过程如下图所示：\n\n\n\n所以这里我们可以看到，AE_READABLE 事件就是客户端的网络连接事件，而对应的处理函数就是接收 TCP 连接请求。下面的示例代码中，显示了 initServer 中调用 aeCreateFileEvent 的部分片段，你可以看下：\n\nvoid initServer(void) {\n    …\n    for (j = 0; j < server.ipfd_count; j++) {\n        if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,\n            acceptTcpHandler,NULL) == AE_ERR)\n            {\n                serverPanic("Unrecoverable error creating server.ipfd file event.");\n            }\n\t}\n\t…\n}\n\n\n**那么，aeCreateFileEvent 如何实现事件和处理函数的注册呢？ **这就和刚才我介绍的 Redis 对底层 IO 多路复用函数封装有关了，下面我仍然以Linux系统为例，来给你说明一下。\n\n首先，Linux提供了epoll_ctl API，用于增加新的观察事件。而 Redis 在此基础上，封装了 aeApiAddEvent 函数，对 epoll_ctl 进行调用。\n\n所以这样一来，aeCreateFileEvent 就会调用 aeApiAddEvent，然后 aeApiAddEvent 再通过调用 epoll_ctl，来注册希望监听的事件和相应的处理函数。等到 aeProceeEvents 函数捕获到实际事件时，它就会调用注册的函数对事件进行处理了。\n\n好了，到这里，我们就已经全部了解了Redis中实现事件驱动框架的三个关键函数：aeMain、aeProcessEvents，以及aeCreateFileEvent。当你要去实现一个事件驱动框架时，Redis的设计思想就具有很好的参考意义。\n\n最后我再带你来简单地回顾下，在实现事件驱动框架的时候，你需要先实现一个主循环函数（对应aeMain），负责一直运行框架。其次，你需要编写事件注册函数（对应aeCreateFileEvent），用来注册监听的事件和事件对应的处理函数。只有对事件和处理函数进行了注册，才能在事件发生时调用相应的函数进行处理。\n\n最后，你需要编写事件监听、分发函数（对应aeProcessEvents），负责调用操作系统底层函数来捕获网络连接、读、写事件，并分发给不同处理函数进一步处理。\n\n\n# 总结\n\nRedis 一直被称为单线程架构，按照我们通常的理解，单个线程只能处理单个客户端的请求，但是在实际使用时，我们会看到Redis能同时和成百上千个客户端进行交互，这就是因为Redis基于Reactor模型，实现了高性能的网络框架，通过事件驱动框架，Redis可以使用一个循环来不断捕获、分发和处理客户端产生的网络连接、数据读写事件。\n\n为了方便你从代码层面掌握Redis事件驱动框架的实现，我总结了一个表格，其中列出了Redis事件驱动框架的主要函数和功能、它们所属的C文件，以及这些函数本身是在Redis代码结构中的哪里被调用。你可以使用这张表格，来巩固今天这节课学习的事件驱动框架。\n\n我也再强调下，这节课我们主要关注的是，事件驱动框架的基本运行流程，并以客户端连接事件为例，将框架主循环、事件捕获分发和事件注册的关键步骤串起来，给你做了介绍。Redis事件驱动框架监听处理的事件，还包括客户端请求、服务器端写数据以及周期性操作等，这也是我下一节课要和你一起学习的主要内容。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n * reactor 模型中有哪些组件？\n * reactor 模型中有哪些事件类型？\n * reactor 模型中有哪些关键角色？每个角色的作用？\n * 代码整体控制逻辑有哪些重要部分？\n * redis 如何实现 reactor 模型？\n * 如何注册事件？如何循环监听事件？\n\n\n# 前言\n\nredis的网络框架是实现了reactor模型吗？\n\n建议分为两部分来思考：\n\n * reactor 模型是什么\n * redis 代码实现是如何与 reactor 模型相对应的\n\nreactor 模型是高性能网络系统实现高并发请求处理的一个重要技术方案。掌握reactor模型的设计思想与实现方法，可以指导你设计和实现自己的高并发系统。当你要处理成千上万的网络连接时，就不会一筹莫展了。\n\n\n# reactor 模型的工作机制\n\n实际上，reactor 模型就是网络服务器端用来处理高并发网络io请求的一种编程模型。我把这个模型的特征用两个「三」来总结，也就是：\n\n * 三类处理事件，连接事件、写事件、读事件；\n * 三个关键角色，即 reactor、acceptor、handler。\n\n那么，reactor 模型是如何基于这三类事件和三个角色来处理高并发请求的呢？下面我们就来具体了解下。\n\n\n# 事件类型\n\n我们先来看看这三类事件和 reactor 模型的关系。\n\n其实，reactor 模型处理的是客户端和服务器端的 交互过程，而这三类事件正好对应了客户端和服务器端交互过程中，不同类请求在服务器端引发的待处理事件：\n\n * 连接事件\n   * 当一个客户端要和服务器端进行交互时，客户端会向服务器端发送连接请求，以建立连接，这就对应了服务器端的一个连接事件\n * 写事件\n   * 一旦连接建立后，客户端会给服务器端发送读请求，以便读取数据。服务器端在处理读请求时，需要向客户端写回数据，这对应了服务器端的写事件\n * 读事件\n   * 无论客户端给服务器端发送读或写请求，服务器端都需要从客户端读取请求内容，所以在这里，读或写请求的读取就对应了服务器端的读事件\n\n如下所示的图例中，就展示了客户端和服务器端在交互过程中，不同类请求和reactor模型事件的对应关系，你可以看下。\n\n\n\n好，在了解了 reactor 模型的三类事件后，你现在可能还有一个疑问：这三类事件是由谁来处理的呢？\n\n\n# 关键角色\n\n这其实就是模型中 三个关键角色 的作用了：\n\n * acceptor ：连接事件由 acceptor 来处理，负责接收连接；acceptor 在接收连接后，会创建 handler，用于网络连接上对后续读写事件的处理；\n * handler：读写事件由 handler 处理；\n * reactor：在高并发场景中，连接事件、读写事件会同时发生，所以，我们需要有一个角色专门监听和分配事件，这就是 reactor 角色。当有连接请求时，reactor 将产生的连接事件交由 acceptor 处理；当有读写请求时，reactor 将读写事件交由 handler 处理。\n\n下图就展示了这三个角色之间的关系，以及它们和事件的关系，你可以看下\n\n\n\n事实上，这三个角色都是reactor模型中要实现的功能的抽象。\n\n当我们遵循reactor模型开发服务器端的网络框架时，就需要在编程的时候，在代码功能模块中实现reactor、acceptor和handler 的逻辑\n\n那么，现在我们已经知道，这三个角色是围绕事件的监听、转发和处理来进行交互的，那么在编程时，我们又该如何实现这三者的交互呢？这就离不开 事件驱动框架了\n\n\n# 事件驱动框架\n\n所谓的事件驱动框架，就是在实现 reactor 模型时，需要实现的代码整体控制逻辑。\n\n简单来说，事件驱动框架包括了两部分\n\n * 一是 事件初始化\n * 二是 事件捕获、分发和处理主循环\n\n事件初始化是在服务器程序启动时就执行的，它的作用主要是 创建需要监听的事件类型，以及该类事件对应的handler。\n\n而一旦服务器完成初始化后，事件初始化也就相应完成了，服务器程序就需要进入到事件捕获、分发和处理的主循环中。\n\n在开发代码时，我们通常会用一个 while循环 来作为这个主循环。\n\n然后在这个主循环中，我们需要\n\n 1. 捕获发生的事件\n 2. 判断事件类型\n 3. 根据事件类型，调用在初始化时创建好的事件handler来实际处理事件\n\n比如说，当有连接事件发生时，服务器程序需要调用acceptor处理函数，创建和客户端的连接。而当有读事件发生时，就表明有读或写请求发送到了服务器端，服务器程序就要调用具体的请求处理函数，从客户端连接中读取请求内容，进而就完成了读事件的处理。这里你可以参考下面给出的图例，其中显示了事件驱动框架的基本执行过程：\n\n\n\n那么到这里，你应该就已经了解了reactor模型的基本工作机制：客户端的不同类请求会在服务器端触发连接、读、写三类事件，这三类事件的监听、分发和处理又是由reactor、acceptor、handler三类角色来完成的，然后这三类角色会通过事件驱动框架来实现交互和事件处理。\n\n所以可见，实现一个reactor模型的关键，就是要实现事件驱动框架。那么，如何开发实现一个事件驱动框架呢？\n\nredis 提供了一个简洁但有效的参考实现，非常值得我们学习，而且也可以用于自己的网络系统开发。下面，我们就一起来学习下redis中对 reactor 模型的实现。\n\n\n# redis 对 reactor 模型的实现\n\n首先我们要知道的是，redis 的网络框架实现了 reactor 模型，并且自行开发实现了一个事件驱动框架。这个框架对应的redis代码实现文件是ae.c，对应的头文件是ae.h。\n\n前面我们已经知道，事件驱动框架的实现离不开事件的定义，以及事件注册、捕获、分发和处理等一系列操作。当然，对于整个框架来说，还需要能一直运行，持续地响应发生的事件。\n\n那么由此，我们从ae.h头文件中就可以看到，redis为了实现事件驱动框架，相应地定义了\n\n * 事件的数据结构\n * 框架主循环函数\n * 事件捕获分发函数\n * 事件\n * handler注册函数\n\n所以接下来，我们就依次来了解学习下\n\n\n# 事件的数据结构定义：以 aefileevent 为例\n\n在 redis 事件驱动框架的实现当中，事件的数据结构是关联事件类型和事件处理函数的关键要素。而redis的事件驱动框架定义了两类事件：\n\n * io事件：对应 客户端发送的 网络请求\n * 时间事件：对应 redis 自身的周期性操作\n\n注意：不同类型事件的数据结构定义是不一样的\n\n为了让你能够理解事件数据结构对框架的作用，我就以 io 事件 aefileevent 为例，给你介绍下它的数据结构定义\n\ntypedef struct aefileevent {\n    int mask; /* one of ae_(readable|writable|barrier) */\n    aefileproc *rfileproc;\n    aefileproc *wfileproc;\n    void *clientdata;\n} aefileevent;\n\n\n * **mask **是用来 表示事件类型 的掩码。对于 io事件 来说，主要有 ae_readable、ae_writable 和 ae_barrier 三种类型事件。框架在分发事件时，依赖的就是结构体中的事件类型\n * rfileproc 和 wfileproce 分别是指向 ae_readable 和 ae_writable 这两类事件的处理函数，也就是 reactor 模型中的 handler。框架在分发事件后，就需要调用结构体中定义的函数进行事件处理\n * 最后一个成员变量 clientdata 是用来指向客户端私有数据的指针\n\n除了事件的数据结构以外，前面我还提到 redis 在 ae.h 文件中，定义了支撑框架运行的主要函数\n\n * 负责框架主循环的 aemain 函数\n * 负责事件捕获与分发的 aeprocessevents 函数\n * 负责事件和 handler 注册的 aecreatefileevent 函数\n\n它们的原型定义如下\n\nvoid aemain(aeeventloop *eventloop);\nint aecreatefileevent(aeeventloop *eventloop, int fd, int mask, aefileproc *proc, void *clientdata);\nint aeprocessevents(aeeventloop *eventloop, int flags);\n\n\n而这三个函数的实现，都是在对应的 ae.c 文件中，那么接下来，我就给你具体介绍下这三个函数的主体逻辑和关键流程\n\n\n# 主循环：aemain 函数\n\n我们先来看下 aemain 函数\n\naemain 函数的逻辑很简单，就是 用一个循环不停地判断事件循环的停止标记。如果事件循环的停止标记被设置为 true，那么针对事件捕获、分发和处理的整个主循环就停止了；否则，主循环会一直执行。aemain 函数的主体代码如下所示：\n\nvoid aemain(aeeventloop *eventloop) {\n    eventloop->stop = 0;\n    while (!eventloop->stop) {\n        if (eventloop->beforesleep != null)\n            eventloop->beforesleep(eventloop);\n        aeprocessevents(eventloop, ae_all_events|ae_call_after_sleep);\n    }\n}\n\n\n那么这里你可能要问了，aemain 函数是在哪里被调用的呢？\n\nint main(int argc, char **argv) {\n\t···\n\taesetbeforesleepproc(server.el,beforesleep);\n    aesetaftersleepproc(server.el,aftersleep);\n    aemain(server.el);\n    aedeleteeventloop(server.el);\n    return 0;\n}\n\n\n按照事件驱动框架的编程规范来说，框架主循环是在服务器程序初始化完成后，就会开始执行。因此，如果我们把目光转向 redis 服务器初始化的函数，就会发现服务器程序的 main 函数在完成 redis server 的初始化后，会调用 aemain 函数开始执行事件驱动框架。如果你想具体查看main函数，main函数在server.c文件中，server.c主要用于初始化服务器和执行服务器整体控制流程，你可以回顾下。\n\n不过，既然aemain函数包含了事件框架的主循环，**那么在主循环中，事件又是如何被捕获、分发和处理呢？**这就是由 aeprocessevents 函数来完成的了\n\n\n# 事件捕获与分发：aeprocessevents 函数\n\naeprocessevents 函数实现的主要功能，包括\n\n * 捕获事件\n * 判断事件类型\n * 调用具体的事件处理函数，从而实现事件的处理\n\n从 aeprocessevents 函数的主体结构中，我们可以看到主要有三个 if 条件分支，如下所示：\n\nint aeprocessevents(aeeventloop *eventloop, int flags)\n{\n    int processed = 0, numevents;\n \n    /* 若没有事件处理，则立刻返回 */\n    if (!(flags & ae_time_events) && !(flags & ae_file_events)) return 0;\n    \n    /*如果有io事件发生，或者紧急的时间事件发生，则开始处理*/\n    if (eventloop->maxfd != -1 || ((flags & ae_time_events) && !(flags & ae_dont_wait))) {\n       …\n    }\n    \n    /* 检查是否有时间事件，若有，则调用processtimeevents函数处理 */\n    if (flags & ae_time_events)\n        processed += processtimeevents(eventloop);\n    \n    /* 返回已经处理的文件或时间*/\n    return processed; \n}\n\n\n这三个分支分别对应了以下三种情况：\n\n * 情况一：既没有时间事件，也没有网络事件\n * 情况二：有 io 事件或者有需要紧急处理的时间事件\n * 情况三：只有普通的时间事件\n\n那么对于第一种情况来说，因为没有任何事件需要处理，aeprocessevents 函数就会直接返回到 aemain 的主循环，开始下一轮的循环；而对于第三种情况来说，该情况发生时只有普通时间事件发生，所以 aemain 函数会调用专门处理时间事件的函数 processtimeevents，对时间事件进行处理\n\n现在，我们再来看看第二种情况\n\n首先，当该情况发生时，redis需要捕获发生的网络事件，并进行相应的处理。那么从redis源码中我们可以分析得到，在这种情况下，aeapipoll 函数会被调用，用来捕获事件，如下所示：\n\nint aeprocessevents(aeeventloop *eventloop, int flags){\n   ...\n   if (eventloop->maxfd != -1 || ((flags & ae_time_events) && !(flags & ae_dont_wait))) {\n       ...\n       //调用 aeapipoll 函数捕获事件\n       numevents = aeapipoll(eventloop, tvp);\n       ...\n    }\n    ...\n」\n\n\n那么，aeapipoll是如何捕获事件呢？\n\n实际上，redis是依赖于操作系统底层提供的 io多路复用机制，来实现事件捕获，检查是否有新的连接、读写事件发生。为了适配不同的操作系统，redis对不同操作系统实现的网络io多路复用函数，都进行了统一的封装，封装后的代码分别通过以下四个文件中实现：\n\n * ae_epoll.c，对应linux上的io复用函数epoll；\n * ae_evport.c，对应solaris上的io复用函数evport；\n * ae_kqueue.c，对应macos或freebsd上的io复用函数kqueue；\n * ae_select.c，对应linux（或windows）的io复用函数select。\n\n这样，在有了这些封装代码后，redis 在不同的操作系统上调用 io 多路复用 api 时，就可以通过统一的接口来进行调用了。\n\n不过看到这里，你可能还是不太明白 redis 封装的具体操作，所以这里，我就以在服务器端最常用的 linux 操作系统为例，给你介绍下redis 是如何封装 linux 上提供的 io 复用 api 的。\n\n首先，linux 上提供了epoll_wait api，用于检测内核中发生的网络io事件。在ae_epoll.c文件中，aeapipoll函数就是封装了对epoll_wait的调用。\n\n这个封装程序如下所示，其中你可以看到，在 aeapipoll 函数中直接调用了 epoll_wait 函数，并将 epoll 返回的事件信息保存起来的逻辑：\n\nstatic int aeapipoll(aeeventloop *eventloop, struct timeval *tvp) {\n    …\n    //调用 epoll_wait 获取监听到的事件\n    retval = epoll_wait(state->epfd,state->events,eventloop->setsize,\n            tvp ? (tvp->tv_sec*1000 + tvp->tv_usec/1000) : -1);\n    if (retval > 0) {\n        int j;\n        //获得监听到的事件数量\n        numevents = retval;\n        //针对每一个事件，进行处理\n        for (j = 0; j < numevents; j++) {\n             //保存事件信息\n        }\n    }\n    return numevents;\n}\n\n\n为了让你更加清晰地理解，事件驱动框架是如何实现最终对epoll_wait的调用，这里我也放了一张示意图，你可以看看整个调用链是如何工作和实现的。\n\n\n\nok，现在我们就已经在 aemain 函数中，看到了 aeprocessevents 函数被调用，并用于捕获和分发事件的基本处理逻辑。\n\n**那么，事件具体是由哪个函数来处理的呢？**这就和框架中的 aecreatefileevents 函数有关了。\n\n\n# 事件注册：aecreatefileevent 函数\n\n我们知道，当redis启动后，服务器程序的 main 函数会调用 initsever 函数来进行初始化，而在初始化的过程中，aecreatefileevent 就会被 initserver 函数调用，用于注册要监听的事件，以及相应的事件处理函数。\n\n具体来说，在 initserver 函数的执行过程中，initserver 函数会根据启用的 ip 端口个数，为每个 ip 端口上的网络事件，调用aecreatefileevent，创建对 ae_readable 事件的监听，并且注册 ae_readable 事件的处理 handler，也就是 accepttcphandler 函数。这一过程如下图所示：\n\n\n\n所以这里我们可以看到，ae_readable 事件就是客户端的网络连接事件，而对应的处理函数就是接收 tcp 连接请求。下面的示例代码中，显示了 initserver 中调用 aecreatefileevent 的部分片段，你可以看下：\n\nvoid initserver(void) {\n    …\n    for (j = 0; j < server.ipfd_count; j++) {\n        if (aecreatefileevent(server.el, server.ipfd[j], ae_readable,\n            accepttcphandler,null) == ae_err)\n            {\n                serverpanic("unrecoverable error creating server.ipfd file event.");\n            }\n\t}\n\t…\n}\n\n\n**那么，aecreatefileevent 如何实现事件和处理函数的注册呢？ **这就和刚才我介绍的 redis 对底层 io 多路复用函数封装有关了，下面我仍然以linux系统为例，来给你说明一下。\n\n首先，linux提供了epoll_ctl api，用于增加新的观察事件。而 redis 在此基础上，封装了 aeapiaddevent 函数，对 epoll_ctl 进行调用。\n\n所以这样一来，aecreatefileevent 就会调用 aeapiaddevent，然后 aeapiaddevent 再通过调用 epoll_ctl，来注册希望监听的事件和相应的处理函数。等到 aeproceeevents 函数捕获到实际事件时，它就会调用注册的函数对事件进行处理了。\n\n好了，到这里，我们就已经全部了解了redis中实现事件驱动框架的三个关键函数：aemain、aeprocessevents，以及aecreatefileevent。当你要去实现一个事件驱动框架时，redis的设计思想就具有很好的参考意义。\n\n最后我再带你来简单地回顾下，在实现事件驱动框架的时候，你需要先实现一个主循环函数（对应aemain），负责一直运行框架。其次，你需要编写事件注册函数（对应aecreatefileevent），用来注册监听的事件和事件对应的处理函数。只有对事件和处理函数进行了注册，才能在事件发生时调用相应的函数进行处理。\n\n最后，你需要编写事件监听、分发函数（对应aeprocessevents），负责调用操作系统底层函数来捕获网络连接、读、写事件，并分发给不同处理函数进一步处理。\n\n\n# 总结\n\nredis 一直被称为单线程架构，按照我们通常的理解，单个线程只能处理单个客户端的请求，但是在实际使用时，我们会看到redis能同时和成百上千个客户端进行交互，这就是因为redis基于reactor模型，实现了高性能的网络框架，通过事件驱动框架，redis可以使用一个循环来不断捕获、分发和处理客户端产生的网络连接、数据读写事件。\n\n为了方便你从代码层面掌握redis事件驱动框架的实现，我总结了一个表格，其中列出了redis事件驱动框架的主要函数和功能、它们所属的c文件，以及这些函数本身是在redis代码结构中的哪里被调用。你可以使用这张表格，来巩固今天这节课学习的事件驱动框架。\n\n我也再强调下，这节课我们主要关注的是，事件驱动框架的基本运行流程，并以客户端连接事件为例，将框架主循环、事件捕获分发和事件注册的关键步骤串起来，给你做了介绍。redis事件驱动框架监听处理的事件，还包括客户端请求、服务器端写数据以及周期性操作等，这也是我下一节课要和你一起学习的主要内容。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"深入 Redis 事件驱动框架",frontmatter:{title:"深入 Redis 事件驱动框架",date:"2024-09-15T01:36:53.000Z",permalink:"/pages/264b06/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%BB%E7%BA%BF/08.%E6%B7%B1%E5%85%A5%20Redis%20%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%A1%86%E6%9E%B6.html",relativePath:"Redis 系统设计/03.主线/08.深入 Redis 事件驱动框架.md",key:"v-e7847138",path:"/pages/264b06/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:140},{level:2,title:"事件概述",slug:"事件概述",normalizedTitle:"事件概述",charIndex:495},{level:3,title:"文件事件处理",slug:"文件事件处理",normalizedTitle:"文件事件处理",charIndex:691},{level:3,title:"时间事件处理器",slug:"时间事件处理器",normalizedTitle:"时间事件处理器",charIndex:1167},{level:3,title:"核心源码的伪代码（自顶向下）",slug:"核心源码的伪代码-自顶向下",normalizedTitle:"核心源码的伪代码（自顶向下）",charIndex:2344},{level:2,title:"事件驱动框架循环流程的初始化",slug:"事件驱动框架循环流程的初始化",normalizedTitle:"事件驱动框架循环流程的初始化",charIndex:3482},{level:3,title:"aeEventLoop 结构体与初始化",slug:"aeeventloop-结构体与初始化",normalizedTitle:"aeeventloop 结构体与初始化",charIndex:3583},{level:3,title:"aeCreateEventLoop 函数的初始化操作",slug:"aecreateeventloop-函数的初始化操作",normalizedTitle:"aecreateeventloop 函数的初始化操作",charIndex:4371},{level:2,title:"IO 事件处理",slug:"io-事件处理",normalizedTitle:"io 事件处理",charIndex:7675},{level:3,title:"IO 事件创建",slug:"io-事件创建",normalizedTitle:"io 事件创建",charIndex:8332},{level:3,title:"读事件处理",slug:"读事件处理",normalizedTitle:"读事件处理",charIndex:11606},{level:3,title:"写事件处理",slug:"写事件处理",normalizedTitle:"写事件处理",charIndex:12550},{level:2,title:"时间事件处理",slug:"时间事件处理",normalizedTitle:"时间事件处理",charIndex:1167},{level:3,title:"时间事件定义",slug:"时间事件定义",normalizedTitle:"时间事件定义",charIndex:17487},{level:3,title:"时间事件创建",slug:"时间事件创建",normalizedTitle:"时间事件创建",charIndex:1324},{level:3,title:"时间事件回调函数",slug:"时间事件回调函数",normalizedTitle:"时间事件回调函数",charIndex:18951},{level:3,title:"时间事件的触发处理",slug:"时间事件的触发处理",normalizedTitle:"时间事件的触发处理",charIndex:20051},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:21002},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:21070}],headersStr:"前言 事件概述 文件事件处理 时间事件处理器 核心源码的伪代码（自顶向下） 事件驱动框架循环流程的初始化 aeEventLoop 结构体与初始化 aeCreateEventLoop 函数的初始化操作 IO 事件处理 IO 事件创建 读事件处理 写事件处理 时间事件处理 时间事件定义 时间事件创建 时间事件回调函数 时间事件的触发处理 总结 参考资料",content:"提出问题是一切智慧的开端\n\n * Redis 事件驱动框架有哪些事件？\n * 这些事件的创建和处理又分别对应了 Redis 源码中的哪些具体操作？\n * 如何在一个框架中，同时处理 IO 事件 和 时间事件？\n * Redis 事件驱动框架有哪些核心函数？struct？\n\n\n# 前言\n\n前置知识\n\n * Linux 中的 IO 多路复用机制\n * Redis 的 Reactor 模型\n\n在 Redis 的 Reactor 模型 主要关注的是框架基本流程，其中介绍了事件驱动框架基于的 Reactor 模型，并以 IO 事件中的客户端连接事件为例，给你介绍了框架运行的基本流程：从 server 初始化时调用 aeCreateFileEvent 函数注册监听事件，到 server 初始化完成后调用 aeMain 函数，而 aeMain 函数循环执行 aeProceeEvent 函数，来捕获和处理客户端请求触发的事件。\n\n本文，echo 带你深入 Redis 事件驱动框架 ，给你介绍下 Redis 事件驱动框架中的两大类事件类型：IO 事件和时间事件，以及它们相应的处理机制\n\n\n# 事件概述\n\n * 文件事件「IO 事件」：Redis 服务器通过套接字与客户端进行连接，而 文件事件就是服务器对套接字操作的抽象。服务器与客户端的通信会产生相应的文件事件，而服务器则通过监听并处理这些事件来完成一系列网络通信操作。\n\n * 时间事件：Redis 服务器中的一些操作「比如 servercron 函数」需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象。\n\n\n# 文件事件处理\n\n\n\n文件事件处理器有四个组成部分：\n\n * 套接字：文件事件是对套接字操作的抽象，每当一个套接字准备好执行连接应答、写入、读取、关闭等操作时，就会产生一个文件事件\n * IO 多路复用：IO 多路复用程序负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。\n * 文件事件分派器：尽管多个文件事件可能会并发地出现，但 IO 多路复用程序总是会将所有产生事件的套接字都放到一个队列里面，然后通过这个队列，以有序(sequentially)、同步(synchronously)、每次一个套接字的方式向文件事件分派器传送套接字。当上一个套接字产生的事件被处理完毕之后(该套接字为事件所关联的事件处理器执行完毕)，I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字\n * 事件处理器：文件事件分派器接收 I/O 多路复用程序传来的套接字，并根据套接字产生的事件的类调用相应的事件处理器。服务器会为执行不同任务的套接字关联不同的事件处理器，这些处理器是一个个函数它们定义了某个事件发生时，服务器应该执行的动作\n\n\n\n\n# 时间事件处理器\n\n时间事件分为两类：\n\n * 定时事件：让一段程序在指定的时间之后执行一次。比如说，让程序 X 在当前时间的 30 毫秒之后执行一次。\n * 周期性事件：让一段程序每隔指定时间就执行一次。比如说，让程序 Y 每隔 30 毫秒就执行一次。\n\n一个时间事件主要由以下三个属性组成\n\n * id：服务器为时间事件创建的全局唯一 ID(标识号)。ID 号按从小到大的顺序递增新事件的 ID 号比旧事件的 ID 号要大。\n * when：毫秒精度的 UNIX 时间戳，记录了时间事件的到达(arrive)时间。\n * timeProc：时间事件处理器，一个函数。当时间事件到达时，服务器就会调用相应的处理器来处理事件。\n\n> 一个时间事件是定时事件还是周期性事件取决于时间事件处理器的返回值:如果事件处理器返回 ae.h/AENOMORE，那么这个事件为定时事件:该事件在达到一次之后就会被删除，之后不再到达。如果事件处理器返回一个非 AENOMORE 的整数值，那么这个事件为周期性时间:当一个时间事件到达之后，服务器会根据事件处理器返回的值，对时间事件的 when 属性进行更新，让这个事件在一段时间之后再次到达，并以这种方式一直更新并运行下去。\n\n\n\nredis 中的所有时间事件都放在一个无序链表中，服务器将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，它就遍历整个链表，查找所有已到达的时间事件，并调用相应的事件处理器。\n\n注意\n\n我们说保存时间事件的链表为无序链表，指的不是链表不按 ID 排序，而是说该链表不按 when 属性的大小排序。正因为链表没有按 when 属性进行排序，所以当时间事件执行器运行的时候，它必须遍历链表中的所有时间事件，这样才能确保服务器中所有已到达的时间事件都会被处理。\n\n正常模式下的 Redis 服务器只使用 serverCron 一个时间事件，而在 benchmark 模式下，服务器也只使用两个时间事件。在这种情况下，服务器几乎是将无序链表退化成一个指针来使用，所以使用无序链表来保存时间事件，并不影响事件执行的性能。\n\n> serverCron\n> \n> 持续运行的 Redis 服务器需要定期对自身的资源和状态进行检查和调整，从而确保服务器可以长期、稳定地运行，这些定期操作由 redis.c/serverCron 函数负责执行，它的主要工作包括:\n> \n>  * 更新服务器的各类统计信息，比如时间、内存占用、数据库占用情况等。\n>  * 清理数据库中的过期键值对。关闭和清理连接失效的客户端。\n>  * 尝试进行 AOF 或 RDB 持久化操作。\n>  * 如果服务器是主服务器，那么对从服务器进行定期同步。\n>  * 如果处于集群模式，对集群进行定期同步和连接测试。\n\n\n# 核心源码的伪代码（自顶向下）\n\n\n\ndef main():\n    #初始化服务器\n    init server()\n    #一直处理事件，直到服务器关闭为止\n    while server_is_not_shutdown():\n    \taeProcessEvents()\n    #服务器关闭，执行清理操作\n    clean server()\n\n\ndef aeProcessEvents():\n    #获取到达时间离当前时间最接近的时间事件\n    time_event=aeSearchNearestTimer()\n\n    #计算最接近的时间事件距离到达还有多少毫秒\n    remaind_ms=time_event.when - unix_ts_now()\n\n    #如果事件已到达，那么remaind ms的值可能为负数，将它设定为0\n    if remaind_ms < 0:\n    \tremaind_ms = 0\n\n    #根据remaind_ms的值，创建timeval结构\n    timeval=create_timeval_with_ms(remaind_ms)\n\n    #阻塞并等待文件事件产生，最大阻塞时间由传入的timeval结构决定#如果remaind_ms的值为0，那么aeApiPo1l调用之后马上返回，不阻塞\n    aeApiPoll(timeval)\n\n    #处理所有已产生的文件事件\n    processFileEvents()\n\n    #处理所有已到达的时间事件\n    processTimeEvents()\n\n\ndef processTimeEvents():\n\t#遍历服务器中的所有时间事件\n\tfor time_event in all_time_event():\n\t#检查事件是否已经到达\n\t\tif time_event.when <= unix_ts_now():\n\t\t\t#事件已到达\n\t\t\t#执行事件处理器，并获取返回值\n\t\t\tretval=time_event.timeProc()\n\t\t\t#如果这是一个定时事件\n\t\t\tif retval==AE_NOMORE:\n\t\t\t\t#那么将该事件从服务器中删除\n\t\t\t\tdelete_time_event_from_server(time event)\n\t\t\t#如果这是一个周期性事件\n\t\t\telse:\n            #那么按照事件处理器的返回值更新时间事件的when 属性\n            #让这个事件在指定的时间之后再次到达\n            \tupdate_when(time_event,retval)\n\n\n\n# 事件驱动框架循环流程的初始化\n\n为了对这两类事件有个相对全面的了解，接下来，我们先从事件驱动框架循环流程的数据结构及其初始化开始学起，因为这里面就包含了针对这两类事件的数据结构定义和初始化操作\n\n\n# aeEventLoop 结构体与初始化\n\n首先，我们来看下 Redis 事件驱动框架循环流程对应的数据结构 aeEventLoop\n\n这个结构体是在事件驱动框架代码 ae.h 中定义的，记录了框架循环运行过程中的信息，其中，就包含了记录两类事件的变量，分别是：\n\n * aeFileEvent 类型的指针 *events，表示 IO 事件。之所以类型名称为 aeFileEvent，是因为所有的 IO 事件都会用文件描述符进行标识\n * aeTimeEvent 类型的指针 *timeEventHead，表示时间事件，即按一定时间周期触发的事件\n\n此外，aeEventLoop 结构体中还有一个 aeFiredEvent 类型的指针 *fired，这个并不是一类专门的事件类型，它只是用来记录已触发事件对应的文件描述符信息\n\n下面的代码显示了 Redis 中事件循环的结构体定义，你可以看下\n\ntypedef struct aeEventLoop {\n    …\n    aeFileEvent *events; //IO事件数组\n    aeFiredEvent *fired; //已触发事件数组\n    aeTimeEvent *timeEventHead; //记录时间事件的链表头\n    …\n    void *apidata; //和API调用接口相关的数据\n    aeBeforeSleepProc *beforesleep; //进入事件循环流程前执行的函数\n    aeBeforeSleepProc *aftersleep;  //退出事件循环流程后执行的函数\n    \n} aeEventLoop;\n\n\n了解了 aeEventLoop 结构体后，我们再来看下，这个结构体是如何初始化的，这其中就包括了 IO 事件数组和时间事件链表的初始化。\n\n\n# aeCreateEventLoop 函数的初始化操作\n\n因为 Redis server 在完成初始化后，就要开始运行事件驱动框架的循环流程，所以，aeEventLoop 结构体在server.c的 initServer 函数中，就通过调用 **aeCreateEventLoop 函数 **进行初始化了。这个函数的参数只有一个，是 setsize\n\n下面的代码展示了 initServer 函数中对 aeCreateEventLoop 函数的调用。\n\ninitServer() {\n    …\n    //调用 aeCreateEventLoop 函数创建 aeEventLoop 结构体，并赋值给 server 结构的 el 变量\n    server.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR);\n    …\n}\n\n\n从这里我们可以看到 参数 setsize 的大小，其实是由 server 结构的 maxclients 变量和宏定义 CONFIG_FDSET_INCR 共同决定的。其中，maxclients 变量的值大小，可以在 Redis 的配置文件 redis.conf 中进行定义，默认值是 1000。而宏定义 CONFIG_FDSET_INCR 的大小，等于宏定义 CONFIG_MIN_RESERVED_FDS 的值再加上 96，如下所示，这里的两个宏定义都是在server.h文件中定义的。\n\n#define CONFIG_MIN_RESERVED_FDS 32\n#define CONFIG_FDSET_INCR (CONFIG_MIN_RESERVED_FDS+96)\n\n\n好了，到这里，你可能有疑问了：aeCreateEventLoop 函数的参数 setsize，设置为最大客户端数量加上一个宏定义值，可是这个参数有什么用呢？这就和 aeCreateEventLoop 函数具体执行的初始化操作有关了。\n\n接下来，我们就来看下 aeCreateEventLoop 函数执行的操作，大致可以分成以下三个步骤。\n\n第一步，aeCreateEventLoop 函数会创建一个 aeEventLoop 结构体类型的变量 eventLoop。然后，该函数会给 eventLoop 的成员变量分配内存空间，比如，按照传入的参数 setsize，给 IO 事件数组和已触发事件数组分配相应的内存空间。此外，该函数还会给 eventLoop 的成员变量赋初始值\n\n第二步，aeCreateEventLoop 函数会调用 aeApiCreate 函数。aeApiCreate 函数封装了操作系统提供的 IO 多路复用函数，假设 Redis 运行在 Linux 操作系统上，并且 IO 多路复用机制是 epoll，那么此时，aeApiCreate 函数就会调用 epoll_create 创建 epoll 实例，同时会创建 epoll_event 结构的数组，数组大小等于参数 setsize\n\n这里你需要注意，aeApiCreate 函数是把创建的 epoll 实例描述符和 epoll_event 数组，保存在了 aeApiState 结构体类型的变量 state，如下所示：\n\ntypedef struct aeApiState {  //aeApiState结构体定义\n    int epfd;   //epoll实例的描述符\n    struct epoll_event *events;   //epoll_event结构体数组，记录监听事件\n} aeApiState;\n\nstatic int aeApiCreate(aeEventLoop *eventLoop) {\n    aeApiState *state = zmalloc(sizeof(aeApiState));\n    ...\n    //将epoll_event数组保存在aeApiState结构体变量state中\n    state->events = zmalloc(sizeof(struct epoll_event)*eventLoop->setsize);\n    ...\n    //将epoll实例描述符保存在aeApiState结构体变量state中\n    state->epfd = epoll_create(1024);\n    ···\n}\n\n\n紧接着，aeApiCreate 函数把 state 变量赋值给 eventLoop 中的 apidata 。这样一来，eventLoop 结构体中就有了 epoll 实例和 epoll_event 数组的信息，这样就可以用来基于 epoll 创建和处理事件了。我一会儿还会给你具体介绍。\n\neventLoop->apidata = state;\n\n\n第三步，aeCreateEventLoop 函数会把所有网络 IO 事件对应文件描述符的掩码，初始化为 AE_NONE，表示暂时不对任何事件进行监听\n\n我把 aeCreateEventLoop 函数的主要部分代码放在这里，你可以看下。\n\naeEventLoop *aeCreateEventLoop(int setsize) {\n    aeEventLoop *eventLoop;\n    int i;\n\n    //给eventLoop变量分配内存空间\n\tif ((eventLoop = zmalloc(sizeof(*eventLoop))) == NULL) goto err;\n\n\t//给IO事件、已触发事件分配内存空间\n    eventLoop->events = zmalloc(sizeof(aeFileEvent)*setsize);\n    eventLoop->fired = zmalloc(sizeof(aeFiredEvent)*setsize);\n    …\n    eventLoop->setsize = setsize;\n    eventLoop->lastTime = time(NULL);\n\n    //设置时间事件的链表头为NULL\n    eventLoop->timeEventHead = NULL;\n\t…\n\t//调用aeApiCreate函数，去实际调用操作系统提供的IO多路复用函数\n\tif (aeApiCreate(eventLoop) == -1) goto err;\n\n    //将所有网络IO事件对应文件描述符的掩码设置为AE_NONE\n    for (i = 0; i < setsize; i++)\n        eventLoop->events[i].mask = AE_NONE;\n    return eventLoop;\n\n    //初始化失败后的处理逻辑，\n    err:\n    …\n}\n\n\n好，那么从 aeCreateEventLoop 函数的执行流程中，我们其实可以看到以下 两个关键点：\n\n * 事件驱动框架监听的 IO 事件数组大小就等于参数 setsize，这样决定了和 Redis server 连接的客户端数量。所以，当你遇到客户端连接 Redis 时报错“max number of clients reached”，你就可以去 redis.conf 文件修改 maxclients 配置项，以扩充框架能监听的客户端数量。\n * 当使用 Linux 系统的 epoll 机制时，框架循环流程初始化操作，会通过 aeApiCreate 函数创建 epoll_event 结构数组，并调用 epoll_create 函数创建 epoll 实例，这都是使用 epoll 机制的准备工作要求\n\n到这里，框架就可以创建和处理具体的 IO 事件和时间事件了。所以接下来，我们就先来了解下 IO 事件及其处理机制。\n\n\n# IO 事件处理\n\nRedis 的 IO 事件主要包括三类，分别是\n\n * 可读事件：从客户端读取数据\n * 可写事件：向客户端写入数据\n * 屏障事件：屏障事件的主要作用是用来反转事件的处理顺序。比如在默认情况下，Redis 会先给客户端返回结果，但是如果面临需要把数据尽快写入磁盘的情况，Redis 就会用到屏障事件，把写数据和回复客户端的顺序做下调整，先把数据落盘，再给客户端回复。\n\n在 Redis 源码中，IO 事件的数据结构是 aeFileEvent 结构体，IO 事件的创建是通过 aeCreateFileEvent 函数来完成的。下面的代码展示了 aeFileEvent 结构体的定义，你可以再回顾下：\n\ntypedef struct aeFileEvent {\n    int mask; //掩码标记，包括可读事件、可写事件和屏障事件\n    aeFileProc *rfileProc;   //处理可读事件的回调函数\n    aeFileProc *wfileProc;   //处理可写事件的回调函数\n    void *clientData;  //私有数据\n} aeFileEvent;\n\n\n而对于 aeCreateFileEvent 函数来说，在上节课我们已经了解了它是通过 aeApiAddEvent 函数来完成事件注册的。那么接下来，我们再从代码级别看下它是如何执行的，这可以帮助我们更加透彻地理解，事件驱动框架对 IO 事件监听是如何基于 epoll 机制对应封装的。\n\n\n# IO 事件创建\n\n首先，我们来看 aeCreateFileEvent 函数，如下所示：\n\nint aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData)\n{\n    if (fd >= eventLoop->setsize) {\n        errno = ERANGE;\n        return AE_ERR;\n    }\n    aeFileEvent *fe = &eventLoop->events[fd];\n\n    if (aeApiAddEvent(eventLoop, fd, mask) == -1)\n        return AE_ERR;\n    fe->mask |= mask;\n    if (mask & AE_READABLE) fe->rfileProc = proc;\n    if (mask & AE_WRITABLE) fe->wfileProc = proc;\n    fe->clientData = clientData;\n    if (fd > eventLoop->maxfd)\n        eventLoop->maxfd = fd;\n    return AE_OK;\n}\n\n\n这个函数的参数有 5 个，分别是\n\n * 循环流程结构体*eventLoop\n * IO 事件对应的文件描述符 fd\n * 事件类型掩码 mask\n * 事件处理回调函数*proc\n * 事件私有数据*clientData。\n\n因为循环流程结构体 *eventLoop 中有 IO 事件数组，这个数组的元素是 aeFileEvent 类型，所以，每个数组元素都对应记录了一个文件描述符（比如一个套接字）相关联的监听事件类型和回调函数。\n\naeCreateFileEvent 函数会先根据传入的文件描述符 fd，在 eventLoop 的 IO 事件数组中，获取该描述符关联的 IO 事件指针变量*fe，如下所示：\n\naeFileEvent *fe = &eventLoop->events[fd];\n\n\n紧接着，aeCreateFileEvent 函数会调用 aeApiAddEvent 函数，添加要监听的事件：\n\nif (aeApiAddEvent(eventLoop, fd, mask) == -1)\n   return AE_ERR;\n\n\naeApiAddEvent 函数实际上会调用操作系统提供的 IO 多路复用函数，来完成事件的添加。我们还是假设 Redis 实例运行在使用 epoll 机制的 Linux 上，那么 aeApiAddEvent 函数就会调用 epoll_ctl 函数，添加要监听的事件。我在xxx中其实已经给你介绍过 epoll_ctl 函数，这个函数会接收 4 个参数，分别是：\n\n * epoll 实例；\n * 要执行的操作类型（是添加还是修改）；\n * 要监听的文件描述符；\n * epoll_event 类型变量\n\n那么，这个调用过程是如何准备 epoll_ctl 函数需要的参数，从而完成执行的呢？\n\n首先，epoll 实例是我刚才给你介绍的 aeCreateEventLoop 函数，它是通过调用 aeApiCreate 函数来创建的，保存在了 eventLoop 结构体的 apidata 变量中，类型是 aeApiState。所以，aeApiAddEvent 函数会先获取该变量，如下所示：\n\nstatic int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) {\n    //从eventLoop结构体中获取aeApiState变量，里面保存了epoll实例\n\taeApiState *state = eventLoop->apidata;\n    ...\n }\n\n\n其次，对于要执行的操作类型的设置，aeApiAddEvent 函数会根据传入的文件描述符 fd，在 eventLoop 结构体中 IO 事件数组中查找该 fd。因为 IO 事件数组的每个元素，都对应了一个文件描述符，而该数组初始化时，每个元素的值都设置为了 AE_NONE。\n\n所以，如果要监听的文件描述符 fd 在数组中的类型不是 AE_NONE，则表明该描述符已做过设置，那么操作类型就是修改操作，对应 epoll 机制中的宏定义 EPOLL_CTL_MOD。否则，操作类型就是添加操作，对应 epoll 机制中的宏定义 EPOLL_CTL_ADD。这部分代码如下所示：\n\n//如果文件描述符fd对应的IO事件已存在，则操作类型为修改，否则为添加\n int op = eventLoop->events[fd].mask == AE_NONE ?\n            EPOLL_CTL_ADD : EPOLL_CTL_MOD;\n\n\n第三，epoll_ctl 函数需要的监听文件描述符，就是 aeApiAddEvent 函数接收到的参数 fd。\n\n最后，epoll_ctl 函数还需要一个 epoll_event 类型变量，因此 aeApiAddEvent 函数在调用 epoll_ctl 函数前，会新创建 epoll_event 类型**变量 ee。**然后，aeApiAddEvent 函数会设置变量 ee 中的监听事件类型和监听文件描述符。\n\naeApiAddEvent 函数的参数 mask，表示的是要监听的事件类型掩码。所以，aeApiAddEvent 函数会根据掩码值是可读（AE_READABLE）或可写（AE_WRITABLE）事件，来设置 ee 监听的事件类型是 EPOLLIN 还是 EPOLLOUT。这样一来，Redis 事件驱动框架中的读写事件就能够和 epoll 机制中的读写事件对应上来。下面的代码展示了这部分逻辑，你可以看下。\n\n…\nstruct epoll_event ee = {0}; //创建epoll_event类型变量\n…\n//将可读或可写IO事件类型转换为epoll监听的类型EPOLLIN或EPOLLOUT\nif (mask & AE_READABLE) ee.events |= EPOLLIN;\nif (mask & AE_WRITABLE) ee.events |= EPOLLOUT;\nee.data.fd = fd;  //将要监听的文件描述符赋值给ee\n…\n\n\n好了，到这里，aeApiAddEvent 函数就准备好了 epoll 实例、操作类型、监听文件描述符以及 epoll_event 类型变量，然后，它就会调用 epoll_ctl 开始实际创建监听事件了，如下所示：\n\nstatic int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) {\n    ...\n    //调用epoll_ctl实际创建监听事件\n    if (epoll_ctl(state->epfd,op,fd,&ee) == -1) return -1;\n        return 0;\n}\n\n\n了解了这些代码后，我们可以学习到事件驱动框架是如何基于 epoll，封装实现了 IO 事件的创建。那么，在 Redis server 启动运行后，最开始监听的 IO 事件是可读事件，对应于客户端的连接请求。具体是 initServer 函数调用了 aeCreateFileEvent 函数，创建可读事件，并设置回调函数为 acceptTcpHandler，用来处理客户端连接\n\n接下来，我们再来看下一旦有了客户端连接请求后，IO 事件具体是如何处理的呢？\n\n\n# 读事件处理\n\n当 Redis server 接收到客户端的连接请求时，就会使用注册好的 acceptTcpHandler 函数 进行处理\n\nacceptTcpHandler 函数会接受客户端连接，并创建已连接套接字 cfd。然后，acceptCommonHandler 函数会被调用，同时，刚刚创建的已连接套接字 cfd 会作为参数，传递给 acceptCommonHandler 函数。\n\nacceptCommonHandler 函数会调用 createClient 函数创建客户端。而在 createClient 函数中，我们就会看到，aeCreateFileEvent 函数被再次调用了\n\n此时，aeCreateFileEvent 函数会针对已连接套接字上，创建监听事件，类型为 AE_READABLE，回调函数是 readQueryFromClient\n\n好了，到这里，事件驱动框架就增加了对一个客户端已连接套接字的监听。一旦客户端有请求发送到 server，框架就会回调 readQueryFromClient 函数处理请求。这样一来，客户端请求就能通过事件驱动框架进行处理了。\n\n下面代码展示了 createClient 函数调用 aeCreateFileEvent 的过程，你可以看下。\n\nclient *createClient(int fd) {\n…\nif (fd != -1) {\n        …\n        //调用aeCreateFileEvent，监听读事件，对应客户端读写请求，使用readQueryFromclient回调函数处理\n        if (aeCreateFileEvent(server.el,fd,AE_READABLE,\n            readQueryFromClient, c) == AE_ERR)\n        {\n            close(fd);\n            zfree(c);\n            return NULL;\n        } }\n…\n}\n\n\n为了便于你掌握从监听客户端连接请求到监听客户端常规读写请求的事件创建过程，我画了下面这张图，你可以看下\n\n\n\n\n# 写事件处理\n\nRedis 实例在收到客户端请求后，会在处理客户端命令后，将要返回的数据写入客户端输出缓冲区。下图就展示了这个过程的函数调用逻辑：\n\n\n\n而在 Redis 事件驱动框架每次循环进入事件处理函数前，也就是在框架主函数 aeMain 中调用 aeProcessEvents，来处理监听到的已触发事件或是到时的时间事件之前，都会调用 server.c 文件中的 beforeSleep 函数，进行一些任务处理，这其中就包括了调用 handleClientsWithPendingWrites 函数，它会将 Redis sever 客户端缓冲区中的数据写回客户端。\n\n下面给出的代码是事件驱动框架的主函数 aeMain。在该函数每次调用 aeProcessEvents 函数前，就会调用 beforeSleep 函数，你可以看下。\n\nvoid aeMain(aeEventLoop *eventLoop) {\n    eventLoop->stop = 0;\n\twhile (!eventLoop->stop) {\n\t    //如果beforeSleep函数不为空，则调用beforeSleep函数\n        if (eventLoop->beforesleep != NULL)\n            eventLoop->beforesleep(eventLoop);\n        //调用完beforeSleep函数，再处理事件\n        aeProcessEvents(eventLoop, AE_ALL_EVENTS|AE_CALL_AFTER_SLEEP);\n    }\n}\n\n\n这里你要知道，beforeSleep 函数调用的 handleClientsWithPendingWrites 函数，会遍历每一个待写回数据的客户端，然后调用 writeToClient 函数，将客户端输出缓冲区中的数据写回。下面这张图展示了这个流程，你可以看下。\n\n\n\n/* \n * 该函数在进入事件循环之前调用，\n * 目的是尽可能直接将响应数据写入客户端的输出缓冲区，\n * 避免使用系统调用来安装可写事件处理程序，\n * 从而提高效率。 \n */\nint handleClientsWithPendingWrites(void) {\n    listIter li;\n    listNode *ln;\n    int processed = listLength(server.clients_pending_write); // 获取当前待写客户端队列的长度\n\n    listRewind(server.clients_pending_write, &li); // 将迭代器设置为从队列头开始\n    while ((ln = listNext(&li))) { // 遍历待写客户端的队列\n        client *c = listNodeValue(ln); // 获取当前节点的客户端\n        c->flags &= ~CLIENT_PENDING_WRITE; // 清除客户端的“待写”标志\n        listUnlinkNode(server.clients_pending_write, ln); // 从待写客户端队列中移除当前节点\n\n        /* 如果客户端是受保护的，不执行任何操作以避免写入错误或重新创建处理程序 */\n        if (c->flags & CLIENT_PROTECTED) continue;\n\n        /* 如果客户端即将关闭，不需要写入操作 */\n        if (c->flags & CLIENT_CLOSE_ASAP) continue;\n\n        /* 尝试将缓冲区中的数据写入客户端的套接字。\n         * 如果写入失败，则跳过该客户端继续处理下一个。 */\n        if (writeToClient(c, 0) == C_ERR) continue;\n\n        /* 如果经过上述同步写入后仍有数据需要输出到客户端，\n         * 则需要为该客户端安装一个可写事件处理程序，以便稍后继续写入。 */\n        if (clientHasPendingReplies(c)) {\n            installClientWriteHandler(c);\n        }\n    }\n    return processed; // 返回处理的客户端数量\n}\n\n\n不过，如果输出缓冲区的数据还没有写完，此时，handleClientsWithPendingWrites 函数就会调用 aeCreateFileEvent 函数，创建可写事件，并设置回调函数 sendReplyToClient。sendReplyToClient 函数里面会调用 writeToClient 函数写回数据。\n\n> aeCreateFileEvent 是 Redis 中的一个底层函数，用于向事件循环中注册一个新的文件事件。文件事件可以是“可读事件”（数据到来时触发）或“可写事件”（缓冲区空闲时触发）。在上述场景中，aeCreateFileEvent 创建的是一个“可写事件”。\n> \n> 当客户端的输出缓冲区还未完全发送完数据时，Redis 不会立刻阻塞，而是通过创建“可写事件”来处理这个情况。这个可写事件表示，当 Redis 发现客户端可以继续接收数据时（输出缓冲区空闲），它就会自动触发这个事件。\n> \n> 当可写事件触发时，Redis 会调用 sendReplyToClient 函数。这个函数负责将剩余的数据从输出缓冲区发送给客户端。具体来说，它内部会调用 writeToClient 函数来真正执行数据发送的操作。\n\necho 认为\n\n>  1. 输出缓冲区：当 Redis 需要将数据返回给客户端时，数据会先存放在一个输出缓冲区中，然后再通过网络传输给客户端。\n>  2. 缓冲区未写完：这个情况可能发生在以下几种情况下：\n>     * 客户端网络不畅：客户端处理速度较慢，或者网络带宽不足，导致一次只能从缓冲区接收一部分数据，剩余的数据暂时无法发送。(可能是 TCP 的滑动窗口中的接收方的接收窗口跟不上)\n>     * 大数据量传输：如果 Redis 需要发送的数据量很大，比如一个大的查询结果，Redis 可能无法在一次 write 操作中将所有数据写入客户端的网络套接字，只能先写入一部分，剩下的放在缓冲区里等待下一次写入。\n>  3. 处理机制：\n>     * 当 Redis 发现缓冲区中的数据没有写完（例如，writeToClient 函数尝试发送数据时只能写入一部分），它不会等待或阻塞主线程。\n>     * 此时 Redis 会调用 aeCreateFileEvent，创建一个可写事件，表示客户端还未完全接收数据。当客户端准备好接收更多数据时，这个可写事件会触发，回调函数 sendReplyToClient 会再次被调用，尝试将剩下的数据发送给客户端。\n> \n>  * 避免阻塞主线程：Redis 是单线程的，如果由于网络问题或客户端处理能力限制，主线程被阻塞在一个客户端的发送过程中，其他客户端的请求就无法得到及时处理。\n>  * 提高性能和吞吐量：通过异步的方式处理缓冲区的剩余数据发送，Redis 能在高并发的情况下更高效地处理多个客户端的请求。\n\nint handleClientsWithPendingWrites(void) {\n    listIter li;\n\tlistNode *ln;\n\t…\n    //获取待写回的客户端列表\n\tlistRewind(server.clients_pending_write,&li);\n\t//遍历每一个待写回的客户端\n\twhile((ln = listNext(&li))) {\n\t   client *c = listNodeValue(ln);\n\t   …\n\t   //调用writeToClient将当前客户端的输出缓冲区数据写回\n\t   if (writeToClient(c->fd,c,0) == C_ERR) continue;\n\t   //如果还有待写回数据\n\t   if (clientHasPendingReplies(c)) {\n\t            int ae_flags = AE_WRITABLE;\n\t            //创建可写事件的监听，以及设置回调函数\n\t             if (aeCreateFileEvent(server.el, c->fd, ae_flags,\n\t                sendReplyToClient, c) == AE_ERR)\n\t            {\n\t                   …\n\t            }\n\t  } }\n}\n\n\n好了，我们刚才了解的是读写事件对应的回调处理函数。实际上，为了能及时处理这些事件，Redis 事件驱动框架的 aeMain 函数还会循环 调用 aeProcessEvents 函数，来检测已触发的事件，并调用相应的回调函数进行处理。\n\n从 aeProcessEvents 函数的代码中，我们可以看到该函数会调用 aeApiPoll 函数，查询监听的文件描述符中，有哪些已经就绪。一旦有描述符就绪，aeProcessEvents 函数就会根据事件的可读或可写类型，调用相应的回调函数进行处理。aeProcessEvents 函数调用的基本流程如下所示：\n\nint aeProcessEvents(aeEventLoop *eventLoop, int flags){\n…\n//调用aeApiPoll获取就绪的描述符\nnumevents = aeApiPoll(eventLoop, tvp);\n…\nfor (j = 0; j < numevents; j++) {\n\taeFileEvent *fe = &eventLoop->events[eventLoop->fired[j].fd];\n\t…\n    //如果触发的是可读事件，调用事件注册时设置的读事件回调处理函数\n\tif (!invert && fe->mask & mask & AE_READABLE) {\n\t      fe->rfileProc(eventLoop,fd,fe->clientData,mask);\n\t                fired++;\n\t}\n\n    //如果触发的是可写事件，调用事件注册时设置的写事件回调处理函数\n\tif (fe->mask & mask & AE_WRITABLE) {\n\t                if (!fired || fe->wfileProc != fe->rfileProc) {\n\t                    fe->wfileProc(eventLoop,fd,fe->clientData,mask);\n\t                    fired++;\n\t                }\n\t            }\n\t…\n\t} }\n\t…\n}\n\n\n到这里，我们就了解了 IO 事件的创建函数 aeCreateFileEvent，以及在处理客户端请求时对应的读写事件和它们的处理函数。那么接下来，我们再来看看事件驱动框架中的时间事件是怎么创建和处理的。\n\n\n# 时间事件处理\n\n其实，相比于 IO 事件有可读、可写、屏障类型，以及不同类型 IO 事件有不同回调函数来说，时间事件的处理就比较简单了。下面，我们就来分别学习下它的定义、创建、回调函数和触发处理。\n\n\n# 时间事件定义\n\n首先，我们来看下时间事件的结构体定义，代码如下所示：\n\ntypedef struct aeTimeEvent {\n    long long id; //时间事件ID\n    long when_sec; //事件到达的秒级时间戳\n    long when_ms; //事件到达的毫秒级时间戳\n    aeTimeProc *timeProc; //时间事件触发后的处理函数\n    aeEventFinalizerProc *finalizerProc;  //事件结束后的处理函数\n    void *clientData; //事件相关的私有数据\n    struct aeTimeEvent *prev;  //时间事件链表的前向指针\n    struct aeTimeEvent *next;  //时间事件链表的后向指针\n} aeTimeEvent;\n\n\n时间事件结构体中主要的变量，包括以秒记录和以毫秒记录的时间事件触发时的时间戳 when_sec 和 when_ms，以及时间事件触发后的处理函数*timeProc。另外，在时间事件的结构体中，还包含了前向和后向指针*prev和*next，这表明时间事件是以链表的形式组织起来的。\n\n在了解了时间事件结构体的定义以后，我们接着来看下，时间事件是如何创建的。\n\n\n# 时间事件创建\n\n与 IO 事件创建使用 aeCreateFileEvent 函数类似，时间事件的创建函数是 aeCreateTimeEvent 函数。这个函数的原型定义如下所示：\n\nlong long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds, aeTimeProc *proc, void *clientData, aeEventFinalizerProc *finalizerProc)\n\n\n在它的参数中，有两个需要我们重点了解下，以便于我们理解时间事件的处理。\n\n * 一个是milliseconds，这是所创建时间事件的触发时间距离当前时间的时长，是用毫秒表示的。\n * 另一个是***proc**，这是所创建时间事件触发后的回调函数。\n\naeCreateTimeEvent 函数的执行逻辑不复杂，主要就是创建一个时间事件的变量 te，对它进行初始化，并把它插入到框架循环流程结构体 eventLoop 中的时间事件链表中。在这个过程中，aeCreateTimeEvent 函数会调用 aeAddMillisecondsToNow 函数，根据传入的 milliseconds 参数，计算所创建时间事件具体的触发时间戳，并赋值给 te。\n\n实际上，Redis server 在初始化时，除了创建监听的 IO 事件外，也会调用 aeCreateTimeEvent 函数创建时间事件。下面代码显示了 initServer 函数对 aeCreateTimeEvent 函数的调用：\n\ninitServer() {\n    …\n    //创建时间事件\n    if (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR){\n    … //报错信息\n    }\n}\n\n\n从代码中，我们可以看到，时间事件触发后的回调函数是 serverCron。所以接下来，我们就来了解下 serverCron 函数。\n\n\n# 时间事件回调函数\n\nserverCron 函数是在 server.c 文件中实现的。一方面，它会顺序调用一些函数，来实现时间事件被触发后，执行一些后台任务。比如，serverCron 函数会检查是否有进程结束信号，若有就执行 server 关闭操作。serverCron 会调用 databaseCron 函数，处理过期 key 或进行 rehash 等。你可以参考下面给出的代码：\n\n//如果收到进程结束信号，则执行server关闭操作\nif (server.shutdown_asap) {\n    if (prepareForShutdown(SHUTDOWN_NOFLAGS) == C_OK) exit(0);\n    ...\n}\n...\nclientCron();  //执行客户端的异步操作\ndatabaseCron(); //执行数据库的后台操作\n\n\n另一方面，serverCron 函数还会以不同的频率周期性执行一些任务，这是通过执行宏 run_with_period 来实现的。\n\nrunwith_period 宏定义如下，该宏定义会根据 Redis 实例配置文件 redis.conf 中定义的 hz 值，来判断参数_ms表示的时间戳是否到达。一旦到达，serverCron 就可以执行相应的任务了。\n\n#define run_with_period(_ms_) if ((_ms_ <= 1000/server.hz) || !(server.cronloops%((_ms_)/(1000/server.hz))))\n\n\n比如，serverCron 函数中会以 1 秒 1 次的频率，检查 AOF 文件是否有写错误。如果有的话，serverCron 就会调用 flushAppendOnlyFile 函数，再次刷回 AOF 文件的缓存数据。下面的代码展示了这一周期性任务：\n\nserverCron() {\n   …\n   //每1秒执行1次，检查AOF是否有写错误\n   run_with_period(1000) {\n        if (server.aof_last_write_status == C_ERR)\n            flushAppendOnlyFile(0);\n    }\n   …\n}\n\n\n如果你想了解更多的周期性任务，可以再详细阅读下 serverCron 函数中，以 run_with_period 宏定义包含的代码块。\n\n好了，了解了时间事件触发后的回调函数 serverCron，我们最后来看下，时间事件是如何触发处理的。\n\n\n# 时间事件的触发处理\n\n其实，时间事件的检测触发比较简单，事件驱动框架的 aeMain 函数会循环调用 aeProcessEvents 函数，来处理各种事件。而 aeProcessEvents 函数在执行流程的最后，会调用 processTimeEvents 函数处理相应到时的任务。\n\naeProcessEvents(){\n    …\n    //检测时间事件是否触发\n    if (flags & AE_TIME_EVENTS)\n            processed += processTimeEvents(eventLoop);\n    …\n}\n\n\n那么，具体到 proecessTimeEvent 函数来说，它的基本流程就是从时间事件链表上逐一取出每一个事件，然后根据当前时间判断该事件的触发时间戳是否已满足。如果已满足，那么就调用该事件对应的回调函数进行处理。这样一来，周期性任务就能在不断循环执行的 aeProcessEvents 函数中，得到执行了。\n\n下面的代码显示了 processTimeEvents 函数的基本流程，你可以再看下。\n\nstatic int processTimeEvents(aeEventLoop *eventLoop) {\n    ...\n    te = eventLoop->timeEventHead;  //从时间事件链表中取出事件\n    while(te) {\n       ...\n      aeGetTime(&now_sec, &now_ms);  //获取当前时间\n      if (now_sec > te->when_sec || (now_sec == te->when_sec && now_ms >= te->when_ms))   //如果当前时间已经满足当前事件的触发时间戳\n      {\n         ...\n        retval = te->timeProc(eventLoop, id, te->clientData); //调用注册的回调函数处理\n        ...\n      }\n      te = te->next;   //获取下一个时间事件\n      ...\n}\n\n\n\n# 总结\n\n 1. 文件事件处理流程和时间事件处理流程\n 2. 文件事件和时间事件如何共同工作的\n 3. 事件驱动框架的初始化过程\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n * redis 事件驱动框架有哪些事件？\n * 这些事件的创建和处理又分别对应了 redis 源码中的哪些具体操作？\n * 如何在一个框架中，同时处理 io 事件 和 时间事件？\n * redis 事件驱动框架有哪些核心函数？struct？\n\n\n# 前言\n\n前置知识\n\n * linux 中的 io 多路复用机制\n * redis 的 reactor 模型\n\n在 redis 的 reactor 模型 主要关注的是框架基本流程，其中介绍了事件驱动框架基于的 reactor 模型，并以 io 事件中的客户端连接事件为例，给你介绍了框架运行的基本流程：从 server 初始化时调用 aecreatefileevent 函数注册监听事件，到 server 初始化完成后调用 aemain 函数，而 aemain 函数循环执行 aeproceeevent 函数，来捕获和处理客户端请求触发的事件。\n\n本文，echo 带你深入 redis 事件驱动框架 ，给你介绍下 redis 事件驱动框架中的两大类事件类型：io 事件和时间事件，以及它们相应的处理机制\n\n\n# 事件概述\n\n * 文件事件「io 事件」：redis 服务器通过套接字与客户端进行连接，而 文件事件就是服务器对套接字操作的抽象。服务器与客户端的通信会产生相应的文件事件，而服务器则通过监听并处理这些事件来完成一系列网络通信操作。\n\n * 时间事件：redis 服务器中的一些操作「比如 servercron 函数」需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象。\n\n\n# 文件事件处理\n\n\n\n文件事件处理器有四个组成部分：\n\n * 套接字：文件事件是对套接字操作的抽象，每当一个套接字准备好执行连接应答、写入、读取、关闭等操作时，就会产生一个文件事件\n * io 多路复用：io 多路复用程序负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。\n * 文件事件分派器：尽管多个文件事件可能会并发地出现，但 io 多路复用程序总是会将所有产生事件的套接字都放到一个队列里面，然后通过这个队列，以有序(sequentially)、同步(synchronously)、每次一个套接字的方式向文件事件分派器传送套接字。当上一个套接字产生的事件被处理完毕之后(该套接字为事件所关联的事件处理器执行完毕)，i/o 多路复用程序才会继续向文件事件分派器传送下一个套接字\n * 事件处理器：文件事件分派器接收 i/o 多路复用程序传来的套接字，并根据套接字产生的事件的类调用相应的事件处理器。服务器会为执行不同任务的套接字关联不同的事件处理器，这些处理器是一个个函数它们定义了某个事件发生时，服务器应该执行的动作\n\n\n\n\n# 时间事件处理器\n\n时间事件分为两类：\n\n * 定时事件：让一段程序在指定的时间之后执行一次。比如说，让程序 x 在当前时间的 30 毫秒之后执行一次。\n * 周期性事件：让一段程序每隔指定时间就执行一次。比如说，让程序 y 每隔 30 毫秒就执行一次。\n\n一个时间事件主要由以下三个属性组成\n\n * id：服务器为时间事件创建的全局唯一 id(标识号)。id 号按从小到大的顺序递增新事件的 id 号比旧事件的 id 号要大。\n * when：毫秒精度的 unix 时间戳，记录了时间事件的到达(arrive)时间。\n * timeproc：时间事件处理器，一个函数。当时间事件到达时，服务器就会调用相应的处理器来处理事件。\n\n> 一个时间事件是定时事件还是周期性事件取决于时间事件处理器的返回值:如果事件处理器返回 ae.h/aenomore，那么这个事件为定时事件:该事件在达到一次之后就会被删除，之后不再到达。如果事件处理器返回一个非 aenomore 的整数值，那么这个事件为周期性时间:当一个时间事件到达之后，服务器会根据事件处理器返回的值，对时间事件的 when 属性进行更新，让这个事件在一段时间之后再次到达，并以这种方式一直更新并运行下去。\n\n\n\nredis 中的所有时间事件都放在一个无序链表中，服务器将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，它就遍历整个链表，查找所有已到达的时间事件，并调用相应的事件处理器。\n\n注意\n\n我们说保存时间事件的链表为无序链表，指的不是链表不按 id 排序，而是说该链表不按 when 属性的大小排序。正因为链表没有按 when 属性进行排序，所以当时间事件执行器运行的时候，它必须遍历链表中的所有时间事件，这样才能确保服务器中所有已到达的时间事件都会被处理。\n\n正常模式下的 redis 服务器只使用 servercron 一个时间事件，而在 benchmark 模式下，服务器也只使用两个时间事件。在这种情况下，服务器几乎是将无序链表退化成一个指针来使用，所以使用无序链表来保存时间事件，并不影响事件执行的性能。\n\n> servercron\n> \n> 持续运行的 redis 服务器需要定期对自身的资源和状态进行检查和调整，从而确保服务器可以长期、稳定地运行，这些定期操作由 redis.c/servercron 函数负责执行，它的主要工作包括:\n> \n>  * 更新服务器的各类统计信息，比如时间、内存占用、数据库占用情况等。\n>  * 清理数据库中的过期键值对。关闭和清理连接失效的客户端。\n>  * 尝试进行 aof 或 rdb 持久化操作。\n>  * 如果服务器是主服务器，那么对从服务器进行定期同步。\n>  * 如果处于集群模式，对集群进行定期同步和连接测试。\n\n\n# 核心源码的伪代码（自顶向下）\n\n\n\ndef main():\n    #初始化服务器\n    init server()\n    #一直处理事件，直到服务器关闭为止\n    while server_is_not_shutdown():\n    \taeprocessevents()\n    #服务器关闭，执行清理操作\n    clean server()\n\n\ndef aeprocessevents():\n    #获取到达时间离当前时间最接近的时间事件\n    time_event=aesearchnearesttimer()\n\n    #计算最接近的时间事件距离到达还有多少毫秒\n    remaind_ms=time_event.when - unix_ts_now()\n\n    #如果事件已到达，那么remaind ms的值可能为负数，将它设定为0\n    if remaind_ms < 0:\n    \tremaind_ms = 0\n\n    #根据remaind_ms的值，创建timeval结构\n    timeval=create_timeval_with_ms(remaind_ms)\n\n    #阻塞并等待文件事件产生，最大阻塞时间由传入的timeval结构决定#如果remaind_ms的值为0，那么aeapipo1l调用之后马上返回，不阻塞\n    aeapipoll(timeval)\n\n    #处理所有已产生的文件事件\n    processfileevents()\n\n    #处理所有已到达的时间事件\n    processtimeevents()\n\n\ndef processtimeevents():\n\t#遍历服务器中的所有时间事件\n\tfor time_event in all_time_event():\n\t#检查事件是否已经到达\n\t\tif time_event.when <= unix_ts_now():\n\t\t\t#事件已到达\n\t\t\t#执行事件处理器，并获取返回值\n\t\t\tretval=time_event.timeproc()\n\t\t\t#如果这是一个定时事件\n\t\t\tif retval==ae_nomore:\n\t\t\t\t#那么将该事件从服务器中删除\n\t\t\t\tdelete_time_event_from_server(time event)\n\t\t\t#如果这是一个周期性事件\n\t\t\telse:\n            #那么按照事件处理器的返回值更新时间事件的when 属性\n            #让这个事件在指定的时间之后再次到达\n            \tupdate_when(time_event,retval)\n\n\n\n# 事件驱动框架循环流程的初始化\n\n为了对这两类事件有个相对全面的了解，接下来，我们先从事件驱动框架循环流程的数据结构及其初始化开始学起，因为这里面就包含了针对这两类事件的数据结构定义和初始化操作\n\n\n# aeeventloop 结构体与初始化\n\n首先，我们来看下 redis 事件驱动框架循环流程对应的数据结构 aeeventloop\n\n这个结构体是在事件驱动框架代码 ae.h 中定义的，记录了框架循环运行过程中的信息，其中，就包含了记录两类事件的变量，分别是：\n\n * aefileevent 类型的指针 *events，表示 io 事件。之所以类型名称为 aefileevent，是因为所有的 io 事件都会用文件描述符进行标识\n * aetimeevent 类型的指针 *timeeventhead，表示时间事件，即按一定时间周期触发的事件\n\n此外，aeeventloop 结构体中还有一个 aefiredevent 类型的指针 *fired，这个并不是一类专门的事件类型，它只是用来记录已触发事件对应的文件描述符信息\n\n下面的代码显示了 redis 中事件循环的结构体定义，你可以看下\n\ntypedef struct aeeventloop {\n    …\n    aefileevent *events; //io事件数组\n    aefiredevent *fired; //已触发事件数组\n    aetimeevent *timeeventhead; //记录时间事件的链表头\n    …\n    void *apidata; //和api调用接口相关的数据\n    aebeforesleepproc *beforesleep; //进入事件循环流程前执行的函数\n    aebeforesleepproc *aftersleep;  //退出事件循环流程后执行的函数\n    \n} aeeventloop;\n\n\n了解了 aeeventloop 结构体后，我们再来看下，这个结构体是如何初始化的，这其中就包括了 io 事件数组和时间事件链表的初始化。\n\n\n# aecreateeventloop 函数的初始化操作\n\n因为 redis server 在完成初始化后，就要开始运行事件驱动框架的循环流程，所以，aeeventloop 结构体在server.c的 initserver 函数中，就通过调用 **aecreateeventloop 函数 **进行初始化了。这个函数的参数只有一个，是 setsize\n\n下面的代码展示了 initserver 函数中对 aecreateeventloop 函数的调用。\n\ninitserver() {\n    …\n    //调用 aecreateeventloop 函数创建 aeeventloop 结构体，并赋值给 server 结构的 el 变量\n    server.el = aecreateeventloop(server.maxclients+config_fdset_incr);\n    …\n}\n\n\n从这里我们可以看到 参数 setsize 的大小，其实是由 server 结构的 maxclients 变量和宏定义 config_fdset_incr 共同决定的。其中，maxclients 变量的值大小，可以在 redis 的配置文件 redis.conf 中进行定义，默认值是 1000。而宏定义 config_fdset_incr 的大小，等于宏定义 config_min_reserved_fds 的值再加上 96，如下所示，这里的两个宏定义都是在server.h文件中定义的。\n\n#define config_min_reserved_fds 32\n#define config_fdset_incr (config_min_reserved_fds+96)\n\n\n好了，到这里，你可能有疑问了：aecreateeventloop 函数的参数 setsize，设置为最大客户端数量加上一个宏定义值，可是这个参数有什么用呢？这就和 aecreateeventloop 函数具体执行的初始化操作有关了。\n\n接下来，我们就来看下 aecreateeventloop 函数执行的操作，大致可以分成以下三个步骤。\n\n第一步，aecreateeventloop 函数会创建一个 aeeventloop 结构体类型的变量 eventloop。然后，该函数会给 eventloop 的成员变量分配内存空间，比如，按照传入的参数 setsize，给 io 事件数组和已触发事件数组分配相应的内存空间。此外，该函数还会给 eventloop 的成员变量赋初始值\n\n第二步，aecreateeventloop 函数会调用 aeapicreate 函数。aeapicreate 函数封装了操作系统提供的 io 多路复用函数，假设 redis 运行在 linux 操作系统上，并且 io 多路复用机制是 epoll，那么此时，aeapicreate 函数就会调用 epoll_create 创建 epoll 实例，同时会创建 epoll_event 结构的数组，数组大小等于参数 setsize\n\n这里你需要注意，aeapicreate 函数是把创建的 epoll 实例描述符和 epoll_event 数组，保存在了 aeapistate 结构体类型的变量 state，如下所示：\n\ntypedef struct aeapistate {  //aeapistate结构体定义\n    int epfd;   //epoll实例的描述符\n    struct epoll_event *events;   //epoll_event结构体数组，记录监听事件\n} aeapistate;\n\nstatic int aeapicreate(aeeventloop *eventloop) {\n    aeapistate *state = zmalloc(sizeof(aeapistate));\n    ...\n    //将epoll_event数组保存在aeapistate结构体变量state中\n    state->events = zmalloc(sizeof(struct epoll_event)*eventloop->setsize);\n    ...\n    //将epoll实例描述符保存在aeapistate结构体变量state中\n    state->epfd = epoll_create(1024);\n    ···\n}\n\n\n紧接着，aeapicreate 函数把 state 变量赋值给 eventloop 中的 apidata 。这样一来，eventloop 结构体中就有了 epoll 实例和 epoll_event 数组的信息，这样就可以用来基于 epoll 创建和处理事件了。我一会儿还会给你具体介绍。\n\neventloop->apidata = state;\n\n\n第三步，aecreateeventloop 函数会把所有网络 io 事件对应文件描述符的掩码，初始化为 ae_none，表示暂时不对任何事件进行监听\n\n我把 aecreateeventloop 函数的主要部分代码放在这里，你可以看下。\n\naeeventloop *aecreateeventloop(int setsize) {\n    aeeventloop *eventloop;\n    int i;\n\n    //给eventloop变量分配内存空间\n\tif ((eventloop = zmalloc(sizeof(*eventloop))) == null) goto err;\n\n\t//给io事件、已触发事件分配内存空间\n    eventloop->events = zmalloc(sizeof(aefileevent)*setsize);\n    eventloop->fired = zmalloc(sizeof(aefiredevent)*setsize);\n    …\n    eventloop->setsize = setsize;\n    eventloop->lasttime = time(null);\n\n    //设置时间事件的链表头为null\n    eventloop->timeeventhead = null;\n\t…\n\t//调用aeapicreate函数，去实际调用操作系统提供的io多路复用函数\n\tif (aeapicreate(eventloop) == -1) goto err;\n\n    //将所有网络io事件对应文件描述符的掩码设置为ae_none\n    for (i = 0; i < setsize; i++)\n        eventloop->events[i].mask = ae_none;\n    return eventloop;\n\n    //初始化失败后的处理逻辑，\n    err:\n    …\n}\n\n\n好，那么从 aecreateeventloop 函数的执行流程中，我们其实可以看到以下 两个关键点：\n\n * 事件驱动框架监听的 io 事件数组大小就等于参数 setsize，这样决定了和 redis server 连接的客户端数量。所以，当你遇到客户端连接 redis 时报错“max number of clients reached”，你就可以去 redis.conf 文件修改 maxclients 配置项，以扩充框架能监听的客户端数量。\n * 当使用 linux 系统的 epoll 机制时，框架循环流程初始化操作，会通过 aeapicreate 函数创建 epoll_event 结构数组，并调用 epoll_create 函数创建 epoll 实例，这都是使用 epoll 机制的准备工作要求\n\n到这里，框架就可以创建和处理具体的 io 事件和时间事件了。所以接下来，我们就先来了解下 io 事件及其处理机制。\n\n\n# io 事件处理\n\nredis 的 io 事件主要包括三类，分别是\n\n * 可读事件：从客户端读取数据\n * 可写事件：向客户端写入数据\n * 屏障事件：屏障事件的主要作用是用来反转事件的处理顺序。比如在默认情况下，redis 会先给客户端返回结果，但是如果面临需要把数据尽快写入磁盘的情况，redis 就会用到屏障事件，把写数据和回复客户端的顺序做下调整，先把数据落盘，再给客户端回复。\n\n在 redis 源码中，io 事件的数据结构是 aefileevent 结构体，io 事件的创建是通过 aecreatefileevent 函数来完成的。下面的代码展示了 aefileevent 结构体的定义，你可以再回顾下：\n\ntypedef struct aefileevent {\n    int mask; //掩码标记，包括可读事件、可写事件和屏障事件\n    aefileproc *rfileproc;   //处理可读事件的回调函数\n    aefileproc *wfileproc;   //处理可写事件的回调函数\n    void *clientdata;  //私有数据\n} aefileevent;\n\n\n而对于 aecreatefileevent 函数来说，在上节课我们已经了解了它是通过 aeapiaddevent 函数来完成事件注册的。那么接下来，我们再从代码级别看下它是如何执行的，这可以帮助我们更加透彻地理解，事件驱动框架对 io 事件监听是如何基于 epoll 机制对应封装的。\n\n\n# io 事件创建\n\n首先，我们来看 aecreatefileevent 函数，如下所示：\n\nint aecreatefileevent(aeeventloop *eventloop, int fd, int mask, aefileproc *proc, void *clientdata)\n{\n    if (fd >= eventloop->setsize) {\n        errno = erange;\n        return ae_err;\n    }\n    aefileevent *fe = &eventloop->events[fd];\n\n    if (aeapiaddevent(eventloop, fd, mask) == -1)\n        return ae_err;\n    fe->mask |= mask;\n    if (mask & ae_readable) fe->rfileproc = proc;\n    if (mask & ae_writable) fe->wfileproc = proc;\n    fe->clientdata = clientdata;\n    if (fd > eventloop->maxfd)\n        eventloop->maxfd = fd;\n    return ae_ok;\n}\n\n\n这个函数的参数有 5 个，分别是\n\n * 循环流程结构体*eventloop\n * io 事件对应的文件描述符 fd\n * 事件类型掩码 mask\n * 事件处理回调函数*proc\n * 事件私有数据*clientdata。\n\n因为循环流程结构体 *eventloop 中有 io 事件数组，这个数组的元素是 aefileevent 类型，所以，每个数组元素都对应记录了一个文件描述符（比如一个套接字）相关联的监听事件类型和回调函数。\n\naecreatefileevent 函数会先根据传入的文件描述符 fd，在 eventloop 的 io 事件数组中，获取该描述符关联的 io 事件指针变量*fe，如下所示：\n\naefileevent *fe = &eventloop->events[fd];\n\n\n紧接着，aecreatefileevent 函数会调用 aeapiaddevent 函数，添加要监听的事件：\n\nif (aeapiaddevent(eventloop, fd, mask) == -1)\n   return ae_err;\n\n\naeapiaddevent 函数实际上会调用操作系统提供的 io 多路复用函数，来完成事件的添加。我们还是假设 redis 实例运行在使用 epoll 机制的 linux 上，那么 aeapiaddevent 函数就会调用 epoll_ctl 函数，添加要监听的事件。我在xxx中其实已经给你介绍过 epoll_ctl 函数，这个函数会接收 4 个参数，分别是：\n\n * epoll 实例；\n * 要执行的操作类型（是添加还是修改）；\n * 要监听的文件描述符；\n * epoll_event 类型变量\n\n那么，这个调用过程是如何准备 epoll_ctl 函数需要的参数，从而完成执行的呢？\n\n首先，epoll 实例是我刚才给你介绍的 aecreateeventloop 函数，它是通过调用 aeapicreate 函数来创建的，保存在了 eventloop 结构体的 apidata 变量中，类型是 aeapistate。所以，aeapiaddevent 函数会先获取该变量，如下所示：\n\nstatic int aeapiaddevent(aeeventloop *eventloop, int fd, int mask) {\n    //从eventloop结构体中获取aeapistate变量，里面保存了epoll实例\n\taeapistate *state = eventloop->apidata;\n    ...\n }\n\n\n其次，对于要执行的操作类型的设置，aeapiaddevent 函数会根据传入的文件描述符 fd，在 eventloop 结构体中 io 事件数组中查找该 fd。因为 io 事件数组的每个元素，都对应了一个文件描述符，而该数组初始化时，每个元素的值都设置为了 ae_none。\n\n所以，如果要监听的文件描述符 fd 在数组中的类型不是 ae_none，则表明该描述符已做过设置，那么操作类型就是修改操作，对应 epoll 机制中的宏定义 epoll_ctl_mod。否则，操作类型就是添加操作，对应 epoll 机制中的宏定义 epoll_ctl_add。这部分代码如下所示：\n\n//如果文件描述符fd对应的io事件已存在，则操作类型为修改，否则为添加\n int op = eventloop->events[fd].mask == ae_none ?\n            epoll_ctl_add : epoll_ctl_mod;\n\n\n第三，epoll_ctl 函数需要的监听文件描述符，就是 aeapiaddevent 函数接收到的参数 fd。\n\n最后，epoll_ctl 函数还需要一个 epoll_event 类型变量，因此 aeapiaddevent 函数在调用 epoll_ctl 函数前，会新创建 epoll_event 类型**变量 ee。**然后，aeapiaddevent 函数会设置变量 ee 中的监听事件类型和监听文件描述符。\n\naeapiaddevent 函数的参数 mask，表示的是要监听的事件类型掩码。所以，aeapiaddevent 函数会根据掩码值是可读（ae_readable）或可写（ae_writable）事件，来设置 ee 监听的事件类型是 epollin 还是 epollout。这样一来，redis 事件驱动框架中的读写事件就能够和 epoll 机制中的读写事件对应上来。下面的代码展示了这部分逻辑，你可以看下。\n\n…\nstruct epoll_event ee = {0}; //创建epoll_event类型变量\n…\n//将可读或可写io事件类型转换为epoll监听的类型epollin或epollout\nif (mask & ae_readable) ee.events |= epollin;\nif (mask & ae_writable) ee.events |= epollout;\nee.data.fd = fd;  //将要监听的文件描述符赋值给ee\n…\n\n\n好了，到这里，aeapiaddevent 函数就准备好了 epoll 实例、操作类型、监听文件描述符以及 epoll_event 类型变量，然后，它就会调用 epoll_ctl 开始实际创建监听事件了，如下所示：\n\nstatic int aeapiaddevent(aeeventloop *eventloop, int fd, int mask) {\n    ...\n    //调用epoll_ctl实际创建监听事件\n    if (epoll_ctl(state->epfd,op,fd,&ee) == -1) return -1;\n        return 0;\n}\n\n\n了解了这些代码后，我们可以学习到事件驱动框架是如何基于 epoll，封装实现了 io 事件的创建。那么，在 redis server 启动运行后，最开始监听的 io 事件是可读事件，对应于客户端的连接请求。具体是 initserver 函数调用了 aecreatefileevent 函数，创建可读事件，并设置回调函数为 accepttcphandler，用来处理客户端连接\n\n接下来，我们再来看下一旦有了客户端连接请求后，io 事件具体是如何处理的呢？\n\n\n# 读事件处理\n\n当 redis server 接收到客户端的连接请求时，就会使用注册好的 accepttcphandler 函数 进行处理\n\naccepttcphandler 函数会接受客户端连接，并创建已连接套接字 cfd。然后，acceptcommonhandler 函数会被调用，同时，刚刚创建的已连接套接字 cfd 会作为参数，传递给 acceptcommonhandler 函数。\n\nacceptcommonhandler 函数会调用 createclient 函数创建客户端。而在 createclient 函数中，我们就会看到，aecreatefileevent 函数被再次调用了\n\n此时，aecreatefileevent 函数会针对已连接套接字上，创建监听事件，类型为 ae_readable，回调函数是 readqueryfromclient\n\n好了，到这里，事件驱动框架就增加了对一个客户端已连接套接字的监听。一旦客户端有请求发送到 server，框架就会回调 readqueryfromclient 函数处理请求。这样一来，客户端请求就能通过事件驱动框架进行处理了。\n\n下面代码展示了 createclient 函数调用 aecreatefileevent 的过程，你可以看下。\n\nclient *createclient(int fd) {\n…\nif (fd != -1) {\n        …\n        //调用aecreatefileevent，监听读事件，对应客户端读写请求，使用readqueryfromclient回调函数处理\n        if (aecreatefileevent(server.el,fd,ae_readable,\n            readqueryfromclient, c) == ae_err)\n        {\n            close(fd);\n            zfree(c);\n            return null;\n        } }\n…\n}\n\n\n为了便于你掌握从监听客户端连接请求到监听客户端常规读写请求的事件创建过程，我画了下面这张图，你可以看下\n\n\n\n\n# 写事件处理\n\nredis 实例在收到客户端请求后，会在处理客户端命令后，将要返回的数据写入客户端输出缓冲区。下图就展示了这个过程的函数调用逻辑：\n\n\n\n而在 redis 事件驱动框架每次循环进入事件处理函数前，也就是在框架主函数 aemain 中调用 aeprocessevents，来处理监听到的已触发事件或是到时的时间事件之前，都会调用 server.c 文件中的 beforesleep 函数，进行一些任务处理，这其中就包括了调用 handleclientswithpendingwrites 函数，它会将 redis sever 客户端缓冲区中的数据写回客户端。\n\n下面给出的代码是事件驱动框架的主函数 aemain。在该函数每次调用 aeprocessevents 函数前，就会调用 beforesleep 函数，你可以看下。\n\nvoid aemain(aeeventloop *eventloop) {\n    eventloop->stop = 0;\n\twhile (!eventloop->stop) {\n\t    //如果beforesleep函数不为空，则调用beforesleep函数\n        if (eventloop->beforesleep != null)\n            eventloop->beforesleep(eventloop);\n        //调用完beforesleep函数，再处理事件\n        aeprocessevents(eventloop, ae_all_events|ae_call_after_sleep);\n    }\n}\n\n\n这里你要知道，beforesleep 函数调用的 handleclientswithpendingwrites 函数，会遍历每一个待写回数据的客户端，然后调用 writetoclient 函数，将客户端输出缓冲区中的数据写回。下面这张图展示了这个流程，你可以看下。\n\n\n\n/* \n * 该函数在进入事件循环之前调用，\n * 目的是尽可能直接将响应数据写入客户端的输出缓冲区，\n * 避免使用系统调用来安装可写事件处理程序，\n * 从而提高效率。 \n */\nint handleclientswithpendingwrites(void) {\n    listiter li;\n    listnode *ln;\n    int processed = listlength(server.clients_pending_write); // 获取当前待写客户端队列的长度\n\n    listrewind(server.clients_pending_write, &li); // 将迭代器设置为从队列头开始\n    while ((ln = listnext(&li))) { // 遍历待写客户端的队列\n        client *c = listnodevalue(ln); // 获取当前节点的客户端\n        c->flags &= ~client_pending_write; // 清除客户端的“待写”标志\n        listunlinknode(server.clients_pending_write, ln); // 从待写客户端队列中移除当前节点\n\n        /* 如果客户端是受保护的，不执行任何操作以避免写入错误或重新创建处理程序 */\n        if (c->flags & client_protected) continue;\n\n        /* 如果客户端即将关闭，不需要写入操作 */\n        if (c->flags & client_close_asap) continue;\n\n        /* 尝试将缓冲区中的数据写入客户端的套接字。\n         * 如果写入失败，则跳过该客户端继续处理下一个。 */\n        if (writetoclient(c, 0) == c_err) continue;\n\n        /* 如果经过上述同步写入后仍有数据需要输出到客户端，\n         * 则需要为该客户端安装一个可写事件处理程序，以便稍后继续写入。 */\n        if (clienthaspendingreplies(c)) {\n            installclientwritehandler(c);\n        }\n    }\n    return processed; // 返回处理的客户端数量\n}\n\n\n不过，如果输出缓冲区的数据还没有写完，此时，handleclientswithpendingwrites 函数就会调用 aecreatefileevent 函数，创建可写事件，并设置回调函数 sendreplytoclient。sendreplytoclient 函数里面会调用 writetoclient 函数写回数据。\n\n> aecreatefileevent 是 redis 中的一个底层函数，用于向事件循环中注册一个新的文件事件。文件事件可以是“可读事件”（数据到来时触发）或“可写事件”（缓冲区空闲时触发）。在上述场景中，aecreatefileevent 创建的是一个“可写事件”。\n> \n> 当客户端的输出缓冲区还未完全发送完数据时，redis 不会立刻阻塞，而是通过创建“可写事件”来处理这个情况。这个可写事件表示，当 redis 发现客户端可以继续接收数据时（输出缓冲区空闲），它就会自动触发这个事件。\n> \n> 当可写事件触发时，redis 会调用 sendreplytoclient 函数。这个函数负责将剩余的数据从输出缓冲区发送给客户端。具体来说，它内部会调用 writetoclient 函数来真正执行数据发送的操作。\n\necho 认为\n\n>  1. 输出缓冲区：当 redis 需要将数据返回给客户端时，数据会先存放在一个输出缓冲区中，然后再通过网络传输给客户端。\n>  2. 缓冲区未写完：这个情况可能发生在以下几种情况下：\n>     * 客户端网络不畅：客户端处理速度较慢，或者网络带宽不足，导致一次只能从缓冲区接收一部分数据，剩余的数据暂时无法发送。(可能是 tcp 的滑动窗口中的接收方的接收窗口跟不上)\n>     * 大数据量传输：如果 redis 需要发送的数据量很大，比如一个大的查询结果，redis 可能无法在一次 write 操作中将所有数据写入客户端的网络套接字，只能先写入一部分，剩下的放在缓冲区里等待下一次写入。\n>  3. 处理机制：\n>     * 当 redis 发现缓冲区中的数据没有写完（例如，writetoclient 函数尝试发送数据时只能写入一部分），它不会等待或阻塞主线程。\n>     * 此时 redis 会调用 aecreatefileevent，创建一个可写事件，表示客户端还未完全接收数据。当客户端准备好接收更多数据时，这个可写事件会触发，回调函数 sendreplytoclient 会再次被调用，尝试将剩下的数据发送给客户端。\n> \n>  * 避免阻塞主线程：redis 是单线程的，如果由于网络问题或客户端处理能力限制，主线程被阻塞在一个客户端的发送过程中，其他客户端的请求就无法得到及时处理。\n>  * 提高性能和吞吐量：通过异步的方式处理缓冲区的剩余数据发送，redis 能在高并发的情况下更高效地处理多个客户端的请求。\n\nint handleclientswithpendingwrites(void) {\n    listiter li;\n\tlistnode *ln;\n\t…\n    //获取待写回的客户端列表\n\tlistrewind(server.clients_pending_write,&li);\n\t//遍历每一个待写回的客户端\n\twhile((ln = listnext(&li))) {\n\t   client *c = listnodevalue(ln);\n\t   …\n\t   //调用writetoclient将当前客户端的输出缓冲区数据写回\n\t   if (writetoclient(c->fd,c,0) == c_err) continue;\n\t   //如果还有待写回数据\n\t   if (clienthaspendingreplies(c)) {\n\t            int ae_flags = ae_writable;\n\t            //创建可写事件的监听，以及设置回调函数\n\t             if (aecreatefileevent(server.el, c->fd, ae_flags,\n\t                sendreplytoclient, c) == ae_err)\n\t            {\n\t                   …\n\t            }\n\t  } }\n}\n\n\n好了，我们刚才了解的是读写事件对应的回调处理函数。实际上，为了能及时处理这些事件，redis 事件驱动框架的 aemain 函数还会循环 调用 aeprocessevents 函数，来检测已触发的事件，并调用相应的回调函数进行处理。\n\n从 aeprocessevents 函数的代码中，我们可以看到该函数会调用 aeapipoll 函数，查询监听的文件描述符中，有哪些已经就绪。一旦有描述符就绪，aeprocessevents 函数就会根据事件的可读或可写类型，调用相应的回调函数进行处理。aeprocessevents 函数调用的基本流程如下所示：\n\nint aeprocessevents(aeeventloop *eventloop, int flags){\n…\n//调用aeapipoll获取就绪的描述符\nnumevents = aeapipoll(eventloop, tvp);\n…\nfor (j = 0; j < numevents; j++) {\n\taefileevent *fe = &eventloop->events[eventloop->fired[j].fd];\n\t…\n    //如果触发的是可读事件，调用事件注册时设置的读事件回调处理函数\n\tif (!invert && fe->mask & mask & ae_readable) {\n\t      fe->rfileproc(eventloop,fd,fe->clientdata,mask);\n\t                fired++;\n\t}\n\n    //如果触发的是可写事件，调用事件注册时设置的写事件回调处理函数\n\tif (fe->mask & mask & ae_writable) {\n\t                if (!fired || fe->wfileproc != fe->rfileproc) {\n\t                    fe->wfileproc(eventloop,fd,fe->clientdata,mask);\n\t                    fired++;\n\t                }\n\t            }\n\t…\n\t} }\n\t…\n}\n\n\n到这里，我们就了解了 io 事件的创建函数 aecreatefileevent，以及在处理客户端请求时对应的读写事件和它们的处理函数。那么接下来，我们再来看看事件驱动框架中的时间事件是怎么创建和处理的。\n\n\n# 时间事件处理\n\n其实，相比于 io 事件有可读、可写、屏障类型，以及不同类型 io 事件有不同回调函数来说，时间事件的处理就比较简单了。下面，我们就来分别学习下它的定义、创建、回调函数和触发处理。\n\n\n# 时间事件定义\n\n首先，我们来看下时间事件的结构体定义，代码如下所示：\n\ntypedef struct aetimeevent {\n    long long id; //时间事件id\n    long when_sec; //事件到达的秒级时间戳\n    long when_ms; //事件到达的毫秒级时间戳\n    aetimeproc *timeproc; //时间事件触发后的处理函数\n    aeeventfinalizerproc *finalizerproc;  //事件结束后的处理函数\n    void *clientdata; //事件相关的私有数据\n    struct aetimeevent *prev;  //时间事件链表的前向指针\n    struct aetimeevent *next;  //时间事件链表的后向指针\n} aetimeevent;\n\n\n时间事件结构体中主要的变量，包括以秒记录和以毫秒记录的时间事件触发时的时间戳 when_sec 和 when_ms，以及时间事件触发后的处理函数*timeproc。另外，在时间事件的结构体中，还包含了前向和后向指针*prev和*next，这表明时间事件是以链表的形式组织起来的。\n\n在了解了时间事件结构体的定义以后，我们接着来看下，时间事件是如何创建的。\n\n\n# 时间事件创建\n\n与 io 事件创建使用 aecreatefileevent 函数类似，时间事件的创建函数是 aecreatetimeevent 函数。这个函数的原型定义如下所示：\n\nlong long aecreatetimeevent(aeeventloop *eventloop, long long milliseconds, aetimeproc *proc, void *clientdata, aeeventfinalizerproc *finalizerproc)\n\n\n在它的参数中，有两个需要我们重点了解下，以便于我们理解时间事件的处理。\n\n * 一个是milliseconds，这是所创建时间事件的触发时间距离当前时间的时长，是用毫秒表示的。\n * 另一个是***proc**，这是所创建时间事件触发后的回调函数。\n\naecreatetimeevent 函数的执行逻辑不复杂，主要就是创建一个时间事件的变量 te，对它进行初始化，并把它插入到框架循环流程结构体 eventloop 中的时间事件链表中。在这个过程中，aecreatetimeevent 函数会调用 aeaddmillisecondstonow 函数，根据传入的 milliseconds 参数，计算所创建时间事件具体的触发时间戳，并赋值给 te。\n\n实际上，redis server 在初始化时，除了创建监听的 io 事件外，也会调用 aecreatetimeevent 函数创建时间事件。下面代码显示了 initserver 函数对 aecreatetimeevent 函数的调用：\n\ninitserver() {\n    …\n    //创建时间事件\n    if (aecreatetimeevent(server.el, 1, servercron, null, null) == ae_err){\n    … //报错信息\n    }\n}\n\n\n从代码中，我们可以看到，时间事件触发后的回调函数是 servercron。所以接下来，我们就来了解下 servercron 函数。\n\n\n# 时间事件回调函数\n\nservercron 函数是在 server.c 文件中实现的。一方面，它会顺序调用一些函数，来实现时间事件被触发后，执行一些后台任务。比如，servercron 函数会检查是否有进程结束信号，若有就执行 server 关闭操作。servercron 会调用 databasecron 函数，处理过期 key 或进行 rehash 等。你可以参考下面给出的代码：\n\n//如果收到进程结束信号，则执行server关闭操作\nif (server.shutdown_asap) {\n    if (prepareforshutdown(shutdown_noflags) == c_ok) exit(0);\n    ...\n}\n...\nclientcron();  //执行客户端的异步操作\ndatabasecron(); //执行数据库的后台操作\n\n\n另一方面，servercron 函数还会以不同的频率周期性执行一些任务，这是通过执行宏 run_with_period 来实现的。\n\nrunwith_period 宏定义如下，该宏定义会根据 redis 实例配置文件 redis.conf 中定义的 hz 值，来判断参数_ms表示的时间戳是否到达。一旦到达，servercron 就可以执行相应的任务了。\n\n#define run_with_period(_ms_) if ((_ms_ <= 1000/server.hz) || !(server.cronloops%((_ms_)/(1000/server.hz))))\n\n\n比如，servercron 函数中会以 1 秒 1 次的频率，检查 aof 文件是否有写错误。如果有的话，servercron 就会调用 flushappendonlyfile 函数，再次刷回 aof 文件的缓存数据。下面的代码展示了这一周期性任务：\n\nservercron() {\n   …\n   //每1秒执行1次，检查aof是否有写错误\n   run_with_period(1000) {\n        if (server.aof_last_write_status == c_err)\n            flushappendonlyfile(0);\n    }\n   …\n}\n\n\n如果你想了解更多的周期性任务，可以再详细阅读下 servercron 函数中，以 run_with_period 宏定义包含的代码块。\n\n好了，了解了时间事件触发后的回调函数 servercron，我们最后来看下，时间事件是如何触发处理的。\n\n\n# 时间事件的触发处理\n\n其实，时间事件的检测触发比较简单，事件驱动框架的 aemain 函数会循环调用 aeprocessevents 函数，来处理各种事件。而 aeprocessevents 函数在执行流程的最后，会调用 processtimeevents 函数处理相应到时的任务。\n\naeprocessevents(){\n    …\n    //检测时间事件是否触发\n    if (flags & ae_time_events)\n            processed += processtimeevents(eventloop);\n    …\n}\n\n\n那么，具体到 proecesstimeevent 函数来说，它的基本流程就是从时间事件链表上逐一取出每一个事件，然后根据当前时间判断该事件的触发时间戳是否已满足。如果已满足，那么就调用该事件对应的回调函数进行处理。这样一来，周期性任务就能在不断循环执行的 aeprocessevents 函数中，得到执行了。\n\n下面的代码显示了 processtimeevents 函数的基本流程，你可以再看下。\n\nstatic int processtimeevents(aeeventloop *eventloop) {\n    ...\n    te = eventloop->timeeventhead;  //从时间事件链表中取出事件\n    while(te) {\n       ...\n      aegettime(&now_sec, &now_ms);  //获取当前时间\n      if (now_sec > te->when_sec || (now_sec == te->when_sec && now_ms >= te->when_ms))   //如果当前时间已经满足当前事件的触发时间戳\n      {\n         ...\n        retval = te->timeproc(eventloop, id, te->clientdata); //调用注册的回调函数处理\n        ...\n      }\n      te = te->next;   //获取下一个时间事件\n      ...\n}\n\n\n\n# 总结\n\n 1. 文件事件处理流程和时间事件处理流程\n 2. 文件事件和时间事件如何共同工作的\n 3. 事件驱动框架的初始化过程\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Redis 的执行模式",frontmatter:{title:"Redis 的执行模式",date:"2024-09-15T20:23:53.000Z",permalink:"/pages/e6d8ef/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%BB%E7%BA%BF/09.Redis%20%E7%9A%84%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%BC%8F.html",relativePath:"Redis 系统设计/03.主线/09.Redis 的执行模式.md",key:"v-4fd191da",path:"/pages/e6d8ef/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:268},{level:2,title:"从 bio.c 文件学习 Redis 的后台线程",slug:"从-bio-c-文件学习-redis-的后台线程",normalizedTitle:"从 bio.c 文件学习 redis 的后台线程",charIndex:621},{level:3,title:"bioInit 函数：初始化数组",slug:"bioinit-函数-初始化数组",normalizedTitle:"bioinit 函数：初始化数组",charIndex:1958},{level:3,title:"bioInit 函数：设置线程属性并创建线程",slug:"bioinit-函数-设置线程属性并创建线程",normalizedTitle:"bioinit 函数：设置线程属性并创建线程",charIndex:3142},{level:3,title:"bioProcessBackgroundJobs 函数：处理后台任务",slug:"bioprocessbackgroundjobs-函数-处理后台任务",normalizedTitle:"bioprocessbackgroundjobs 函数：处理后台任务",charIndex:4792},{level:3,title:"bioCreateBackgroundJob 函数：创建后台任务",slug:"biocreatebackgroundjob-函数-创建后台任务",normalizedTitle:"biocreatebackgroundjob 函数：创建后台任务",charIndex:7208},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:8813},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:9128}],headersStr:"前言 从 bio.c 文件学习 Redis 的后台线程 bioInit 函数：初始化数组 bioInit 函数：设置线程属性并创建线程 bioProcessBackgroundJobs 函数：处理后台任务 bioCreateBackgroundJob 函数：创建后台任务 总结 参考资料",content:'提出问题是一切智慧的开端\n\n * 为什么 Redis 的核心流程采用单线程 IO 架构，而不是多线程？\n * 哪些操作可能导致 Redis 主线程阻塞？如何用后台线程避免？\n * Redis 后台线程如何启动、管理，并与主线程协同？\n * Redis 的惰性删除机制是如何工作的？对性能有何影响？\n * 生产环境中如何根据 Redis 模型优化性能？\n * Redis 后台线程的创建机制及其与任务队列的交互？\n * 如果 Redis 的后台任务队列满了，会发生什么？\n * Redis 如何通过任务优先级或类型优化资源管理？\n\n\n# 前言\n\n所谓的执行模型，就是指 Redis 运行时使用的进程、子进程和线程的个数，以及它们各自负责的工作任务\n\n你在实际使用 Redis 的时候，可能经常会听到类似“Redis 是单线程”、“Redis 的主 IO 线程”、“Redis 包含多线程”等不同说法。我也听到不少同学提出困惑和疑问：Redis 到底是不是一个单线程的程序？\n\n其实，彻底理解这个问题，有助于指导我们保持 Redis 高性能、低延迟的特性。如果说 Redis 就是单线程程序，那么，我们就需要避免所有容易引起线程阻塞的操作；而如果说 Redis 不只是单线程，还有其他线程在工作，那么，我们就需要了解多线程各自负责什么任务，负责请求解析和数据读写的线程有几个，有哪些操作是后台线程在完成，而不会影响请求解析和数据读写的。\n\n\n# 从 bio.c 文件学习 Redis 的后台线程\n\n我们先来看下 main 函数在初始化过程最后调用的 InitServerLast 函数。InitServerLast 函数的作用是进一步调用 bioInit 函数，来创建后台线程，让 Redis 把部分任务交给后台线程处理。这个过程如下所示。\n\nvoid InitServerLast() {\n    bioInit();\n    …\n}\n\n\nbioInit函数是在bio.c文件中实现的，它的主要作用调用pthread_create函数创建多个后台线程。不过在具体了解bioInit函数之前，我们先来看下 bio.c 文件中定义的主要数组，这也是在bioInit函数中要进行初始化的。\n\nbio.c 文件针对要创建的线程，定义了pthread_t类型的数组bio_threads，用来保存创建的线程描述符。此外，bio.c 文件还创建了一个保存互斥锁的数组bio_mutex，以及两个保存条件变量的数组bio_newjob_cond和bio_step_cond。以下代码展示了这些数组的创建逻辑，你可以看下。\n\n//保存线程描述符的数组\nstatic pthread_t bio_threads[BIO_NUM_OPS];\n//保存互斥锁的数组\nstatic pthread_mutex_t bio_mutex[BIO_NUM_OPS];\n//保存条件变量的两个数组\nstatic pthread_cond_t bio_newjob_cond[BIO_NUM_OPS];\nstatic pthread_cond_t bio_step_cond[BIO_NUM_OPS];\n\n\n从中你可以注意到，这些数组的大小都是宏定义 BIO_NUM_OPS，这个宏定义是在bio.h文件中定义的，默认值为 3。\n\n同时在 bio.h 文件中，你还可以看到另外三个宏定义，分别是 BIO_CLOSE_FILE、BIO_AOF_FSYNC 和 BIO_LAZY_FREE。它们的代码如下所示：\n\n#define BIO_CLOSE_FILE    0 /* Deferred close(2) syscall. */\n#define BIO_AOF_FSYNC    1 /* Deferred AOF fsync. */\n#define BIO_LAZY_FREE     2 /* Deferred objects freeing. */\n#define BIO_NUM_OPS       3\n\n\n其中，BIO_NUM_OPS 表示的是 Redis 后台任务的类型有三种。而 BIO_CLOSE_FILE、BIO_AOF_FSYNC 和 BIO_LAZY_FREE，它们分别表示三种后台任务的操作码，这些操作码可以用来标识不同的任务。\n\n * BIO_CLOSE_FILE：文件关闭后台任务\n * BIO_AOF_FSYNC：AOF 日志同步写回后台任务\n * BIO_LAZY_FREE：惰性删除后台任务\n\n实际上，bio.c 文件创建的线程数组、互斥锁数组和条件变量数组，大小都是包含三个元素，也正是对应了这三种任务。\n\n\n# bioInit 函数：初始化数组\n\n接下来，我们再来了解下 bio.c 文件中的初始化和线程创建函数bioInit。我刚才也给你介绍过这个函数，它是main函数执行完server初始化后，通过InitServerLast函数调用的。也就是说，Redis 在完成 server 初始化后，就会创建线程来执行后台任务。\n\n所以，Redis 在运行时其实已经不止是单个线程（也就是主 IO 线程）在运行了，还会有后台线程在运行。\n\nbioInit函数首先会初始化互斥锁数组和条件变量数组。然后，该函数会调用listCreate函数，给bio_jobs这个数组的每个元素创建一个列表，同时给bio_pending数组的每个元素赋值为 0。这部分代码如下所示：\n\nfor (j = 0; j < BIO_NUM_OPS; j++) {\n    pthread_mutex_init(&bio_mutex[j],NULL);\n    pthread_cond_init(&bio_newjob_cond[j],NULL);\n    pthread_cond_init(&bio_step_cond[j],NULL);\n    bio_jobs[j] = listCreate();\n    bio_pending[j] = 0;\n}\n\n\n那么，要想了解给bio_jobs数组和bio_pending数组元素赋值的作用，我们就需要先搞清楚这两个数组的含义：\n\n * bio_jobs 数组的元素是bio_jobs结构体类型，用来表示后台任务。该结构体的成员变量包括了后台任务的创建时间 time，以及任务的参数。为该数组的每个元素创建一个列表，其实就是为每个后台线程创建一个要处理的任务列表。\n * bio_pending 数组的元素类型是unsigned long long，用来表示每种任务中，处于等待状态的任务个数。将该数组每个元素初始化为 0，其实就是表示初始时，每种任务都没有待处理的具体任务。\n\nstruct bio_job {\n    time_t time; //任务创建时间\n    void *arg1, *arg2, *arg3;  //任务参数\n};\n//以后台线程方式运行的任务列表\nstatic list *bio_jobs[BIO_NUM_OPS];\n//被阻塞的后台任务数组\nstatic unsigned long long bio_pending[BIO_NUM_OPS];\n\n\n好了，到这里，你就了解了bioInit函数执行时，会把线程互斥锁、条件变量对应数组初始化为 NULL，同时会给每个后台线程创建一个任务列表（对应bio_jobs数组的元素），以及会设置每种任务的待处理个数为 0（对应 bio_pending 数组的元素）。\n\n\n# bioInit 函数：设置线程属性并创建线程\n\n在完成了初始化之后，接下来，bioInit 函数会先通过 pthread_attr_t 类型的变量，给线程设置属性。然后，bioInit 函数会调用前面我提到的 pthread_create 函数来创建线程。\n\n不过，为了能更好地理解 bioInit 函数设置线程属性和创建线程的过程，我们需要先对 pthread_create 函数本身有所了解，该函数的原型如下所示：\n\nint  pthread_create(pthread_t *tidp, const  pthread_attr_t *attr,( void *)(*start_routine)( void *), void  *arg);\n\n\n可以看到，pthread_create 函数一共有 4 个参数，分别是：\n\n * *tidp，指向线程数据结构 pthread_t 的指针；\n * *attr，指向线程属性结构 pthread_attr_t 的指针；\n * *start_routine，线程所要运行的函数的起始地址，也是指向函数的指针；\n * *arg，传给运行函数的参数。\n\n了解了pthread_create函数之后，我们来看下bioInit函数的具体操作。\n\n首先，bioInit函数会调用pthread_attr_init函数，初始化线程属性变量attr，然后调用pthread_attr_getstacksize函数，获取线程的栈大小这一属性的当前值，并根据当前栈大小和REDIS_THREAD_STACK_SIZE宏定义的大小（默认值为 4MB），来计算最终的栈大小属性值。紧接着，bioInit函数会调用pthread_attr_setstacksize函数，来设置栈大小这一属性值。\n\n下面的代码展示了线程属性的获取、计算和设置逻辑，你可以看下。\n\npthread_attr_init(&attr);\npthread_attr_getstacksize(&attr,&stacksize);\nif (!stacksize) stacksize = 1; /针对Solaris系统做处理\n    while (stacksize < REDIS_THREAD_STACK_SIZE) stacksize *= 2;\n    pthread_attr_setstacksize(&attr, stacksize);\n\n\n我也画了一张图，展示了线程属性的这一操作过程，你可以看下。\n\n\n\n在完成线程属性的设置后，接下来，bioInit函数会通过一个 for 循环，来依次为每种后台任务创建一个线程。循环的次数是由BIO_NUM_OPS宏定义决定的，也就是 3 次。相应的，bioInit函数就会调用 3 次pthread_create函数，并创建 3 个线程。bioInit 函数让这 3 个线程执行的函数都是**bioProcessBackgroundJobs**。\n\n不过这里要注意一点，就是在这三次线程的创建过程中，传给这个函数的参数分别是 0、1、2。这个创建过程如下所示：\n\nfor (j = 0; j < BIO_NUM_OPS; j++) {\n    void *arg = (void*)(unsigned long) j;\n    if (pthread_create(&thread,&attr,bioProcessBackgroundJobs,arg) != 0) {\n    \t… //报错信息\n    }\n    bio_threads[j] = thread;\n}\n\n\n你看了这个代码，可能会有一个小疑问：为什么创建的 3 个线程，它们所运行的 bioProcessBackgroundJobs 函数接收的参数分别是 0、1、2 呢？\n\n这就和 bioProcessBackgroundJobs 函数的实现有关了，我们来具体看下。\n\n\n# bioProcessBackgroundJobs 函数：处理后台任务\n\n首先，bioProcessBackgroundJobs函数会把接收到的参数 arg，转成unsigned long类型，并赋值给 type 变量，如下所示：\n\nvoid *bioProcessBackgroundJobs(void *arg) {\n    …\n\tunsigned long type = (unsigned long) arg;\n\t…\n}\n\n\n而type 变量表示的就是后台任务的操作码。这也是我刚才给你介绍的三种后台任务类型 BIO_CLOSE_FILE、BIO_AOF_FSYNC 和 BIO_LAZY_FREE 对应的操作码，它们的取值分别为 0、1、2。\n\nbioProcessBackgroundJobs 函数的主要执行逻辑是一个 while(1)的循环。在这个循环中，bioProcessBackgroundJobs 函数会从 bio_jobs 这个数组中取出相应任务，并根据任务类型，调用具体的函数来执行。\n\n我刚才已经介绍过，bio_jobs数组的每一个元素是一个队列。而因为bio_jobs数组的元素个数，等于后台任务的类型个数（也就是 BIO_NUM_OPS），所以，bio_jobs数组的每个元素，实际上是对应了某一种后台任务的任务队列。\n\n在了解了这一点后，我们就容易理解bioProcessBackgroundJobs函数中的 while 循环了。因为传给bioProcessBackgroundJobs函数的参数，分别是 0、1、2，对应了三种任务类型，所以在这个循环中，bioProcessBackgroundJobs函数会一直不停地从某一种任务队列中，取出一个任务来执行。\n\n同时，bioProcessBackgroundJobs 函数会根据传入的任务操作类型调用相应函数，具体来说：\n\n * 任务类型是 BIO_CLOSE_FILE，则调用 close 函数；\n * 任务类型是 BIO_AOF_FSYNC，则调用 redis_fsync 函数；\n * 任务类型是 BIO_LAZY_FREE，则再根据参数个数等情况，分别调用 lazyfreeFreeObjectFromBioThread、lazyfreeFreeDatabaseFromBioThread 和 lazyfreeFreeSlotsMapFromBioThread 这三个函数。\n\n最后，当某个任务执行完成后，bioProcessBackgroundJobs 函数会从任务队列中，把这个任务对应的数据结构删除。我把这部分代码放在这里，你可以看下。\n\nwhile(1) {\n        listNode *ln;\n\n        …\n        //从类型为type的任务队列中获取第一个任务\n        ln = listFirst(bio_jobs[type]);\n        job = ln->value;\n\n        …\n        //判断当前处理的后台任务类型是哪一种\n        if (type == BIO_CLOSE_FILE) {\n            close((long)job->arg1);  //如果是关闭文件任务，那就调用close函数\n        } else if (type == BIO_AOF_FSYNC) {\n            redis_fsync((long)job->arg1); //如果是AOF同步写任务，那就调用redis_fsync函数\n        } else if (type == BIO_LAZY_FREE) {\n            //如果是惰性删除任务，那根据任务的参数分别调用不同的惰性删除函数执行\n            if (job->arg1)\n                lazyfreeFreeObjectFromBioThread(job->arg1);\n            else if (job->arg2 && job->arg3)\n                lazyfreeFreeDatabaseFromBioThread(job->arg2,job->arg3);\n            else if (job->arg3)\n                lazyfreeFreeSlotsMapFromBioThread(job->arg3);\n        } else {\n            serverPanic("Wrong job type in bioProcessBackgroundJobs().");\n        }\n        …\n        //任务执行完成后，调用 listDelNode 在任务队列中删除该任务\n        listDelNode(bio_jobs[type],ln);\n        //将对应的等待任务个数减一。\n        bio_pending[type]--;\n        …\n}\n\n\n所以说，bioInit 函数其实就是创建了 3 个线程，每个线程不停地去查看任务队列中是否有任务，如果有任务，就调用具体函数执行。\n\n你可以再参考回顾下图所展示的bioInit函数和bioProcessBackgroundJobs函数的基本处理流程。\n\n\n\n不过接下来你或许还会疑惑：既然 bioProcessBackgroundJobs 函数是负责执行任务的，那么哪个函数负责生成任务呢？\n\n这就是下面，我要给你介绍的 后台任务创建函数 bioCreateBackgroundJob\n\n\n# bioCreateBackgroundJob 函数：创建后台任务\n\nbioCreateBackgroundJob函数的原型如下，它会接收 4 个参数，其中，参数 type 表示该后台任务的类型，剩下来的 3 个参数，则对应了后台任务函数的参数，如下所示：\n\nvoid bioCreateBackgroundJob(int type, void *arg1, void *arg2, void *arg3)\n\n\nbioCreateBackgroundJob函数在执行时，会先创建bio_job，这是后台任务对应的数据结构。然后，后台任务数据结构中的参数，会被设置为bioCreateBackgroundJob函数传入的参数 arg1、arg2 和 arg3。\n\n最后，bioCreateBackgroundJob函数调用listAddNodeTail函数，将刚才创建的任务加入到对应的bio_jobs队列中，同时，将bio_pending数组的对应值加 1，表示有个任务在等待执行。\n\n{\n    //创建新的任务\n    struct bio_job *job = zmalloc(sizeof(*job));\n    //设置任务数据结构中的参数\n    job->time = time(NULL);\n    job->arg1 = arg1;\n    job->arg2 = arg2;\n    job->arg3 = arg3;\n    pthread_mutex_lock(&bio_mutex[type]);\n    listAddNodeTail(bio_jobs[type],job);  //将任务加到bio_jobs数组的对应任务列表中\n    bio_pending[type]++; //将对应任务列表上等待处理的任务个数加1\n    pthread_cond_signal(&bio_newjob_cond[type]);\n    pthread_mutex_unlock(&bio_mutex[type]);\n}\n\n\n好了，这样一来，当 Redis 进程想要启动一个后台任务时，只要调用bioCreateBackgroundJob函数，并设置好该任务对应的类型和参数即可。然后，bioCreateBackgroundJob函数就会把创建好的任务数据结构，放到后台任务对应的队列中。另一方面，bioInit函数在 Redis server 启动时，创建的线程会不断地轮询后台任务队列，一旦发现有任务可以执行，就会将该任务取出并执行。\n\n其实，这种设计方式是典型的生产者-消费者模型。bioCreateBackgroundJob函数是生产者，负责往每种任务队列中加入要执行的后台任务，而bioProcessBackgroundJobs函数是消费者，负责从每种任务队列中取出任务来执行。然后 Redis 创建的后台线程，会调用bioProcessBackgroundJobs函数，从而实现一直循环检查任务队列。\n\n下图展示的就是bioCreateBackgroundJob和bioProcessBackgroundJobs两者间的生产者-消费者模型，你可以看下。\n\n\n\n好了，到这里，我们就学习了 Redis 后台线程的创建和运行机制。简单来说，主要是以下三个关键点：\n\n * Redis 是先通过 bioInit 函数初始化和创建后台线程\n * 后台线程运行的是 bioProcessBackgroundJobs 函数，这个函数会轮询任务队列，并根据要处理的任务类型，调用相应函数进行处理\n * 后台线程要处理的任务是由 bioCreateBackgroundJob函数来创建的，这些任务创建后会被放到任务队列中，等待bioProcessBackgroundJobs 函数处理\n\n\n# 总结\n\necho 你介绍了 Redis 的执行模型，并且也从源码的角度出发，通过分析代码，带你了解了 Redis 进程创建、以子进程方式创建的守护进程、以及后台线程和它们负责的工作任务。同时，这也解答了你在面试中可能经常会被问到的问题：Redis 是单线程程序吗？\n\n事实上，Redis server 启动后，它的主要工作包括接收客户端请求、解析请求和进行数据读写等操作，是由单线程来执行的，这也是我们常说 Redis 是单线程程序的原因。\n\n但是，学完这节课你应该也知道，Redis 还启动了 3 个线程来执行文件关闭、AOF 同步写和惰性删除等操作，从这个角度来说，Redis 又不能算单线程程序，它还是有多线程的。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n * 为什么 redis 的核心流程采用单线程 io 架构，而不是多线程？\n * 哪些操作可能导致 redis 主线程阻塞？如何用后台线程避免？\n * redis 后台线程如何启动、管理，并与主线程协同？\n * redis 的惰性删除机制是如何工作的？对性能有何影响？\n * 生产环境中如何根据 redis 模型优化性能？\n * redis 后台线程的创建机制及其与任务队列的交互？\n * 如果 redis 的后台任务队列满了，会发生什么？\n * redis 如何通过任务优先级或类型优化资源管理？\n\n\n# 前言\n\n所谓的执行模型，就是指 redis 运行时使用的进程、子进程和线程的个数，以及它们各自负责的工作任务\n\n你在实际使用 redis 的时候，可能经常会听到类似“redis 是单线程”、“redis 的主 io 线程”、“redis 包含多线程”等不同说法。我也听到不少同学提出困惑和疑问：redis 到底是不是一个单线程的程序？\n\n其实，彻底理解这个问题，有助于指导我们保持 redis 高性能、低延迟的特性。如果说 redis 就是单线程程序，那么，我们就需要避免所有容易引起线程阻塞的操作；而如果说 redis 不只是单线程，还有其他线程在工作，那么，我们就需要了解多线程各自负责什么任务，负责请求解析和数据读写的线程有几个，有哪些操作是后台线程在完成，而不会影响请求解析和数据读写的。\n\n\n# 从 bio.c 文件学习 redis 的后台线程\n\n我们先来看下 main 函数在初始化过程最后调用的 initserverlast 函数。initserverlast 函数的作用是进一步调用 bioinit 函数，来创建后台线程，让 redis 把部分任务交给后台线程处理。这个过程如下所示。\n\nvoid initserverlast() {\n    bioinit();\n    …\n}\n\n\nbioinit函数是在bio.c文件中实现的，它的主要作用调用pthread_create函数创建多个后台线程。不过在具体了解bioinit函数之前，我们先来看下 bio.c 文件中定义的主要数组，这也是在bioinit函数中要进行初始化的。\n\nbio.c 文件针对要创建的线程，定义了pthread_t类型的数组bio_threads，用来保存创建的线程描述符。此外，bio.c 文件还创建了一个保存互斥锁的数组bio_mutex，以及两个保存条件变量的数组bio_newjob_cond和bio_step_cond。以下代码展示了这些数组的创建逻辑，你可以看下。\n\n//保存线程描述符的数组\nstatic pthread_t bio_threads[bio_num_ops];\n//保存互斥锁的数组\nstatic pthread_mutex_t bio_mutex[bio_num_ops];\n//保存条件变量的两个数组\nstatic pthread_cond_t bio_newjob_cond[bio_num_ops];\nstatic pthread_cond_t bio_step_cond[bio_num_ops];\n\n\n从中你可以注意到，这些数组的大小都是宏定义 bio_num_ops，这个宏定义是在bio.h文件中定义的，默认值为 3。\n\n同时在 bio.h 文件中，你还可以看到另外三个宏定义，分别是 bio_close_file、bio_aof_fsync 和 bio_lazy_free。它们的代码如下所示：\n\n#define bio_close_file    0 /* deferred close(2) syscall. */\n#define bio_aof_fsync    1 /* deferred aof fsync. */\n#define bio_lazy_free     2 /* deferred objects freeing. */\n#define bio_num_ops       3\n\n\n其中，bio_num_ops 表示的是 redis 后台任务的类型有三种。而 bio_close_file、bio_aof_fsync 和 bio_lazy_free，它们分别表示三种后台任务的操作码，这些操作码可以用来标识不同的任务。\n\n * bio_close_file：文件关闭后台任务\n * bio_aof_fsync：aof 日志同步写回后台任务\n * bio_lazy_free：惰性删除后台任务\n\n实际上，bio.c 文件创建的线程数组、互斥锁数组和条件变量数组，大小都是包含三个元素，也正是对应了这三种任务。\n\n\n# bioinit 函数：初始化数组\n\n接下来，我们再来了解下 bio.c 文件中的初始化和线程创建函数bioinit。我刚才也给你介绍过这个函数，它是main函数执行完server初始化后，通过initserverlast函数调用的。也就是说，redis 在完成 server 初始化后，就会创建线程来执行后台任务。\n\n所以，redis 在运行时其实已经不止是单个线程（也就是主 io 线程）在运行了，还会有后台线程在运行。\n\nbioinit函数首先会初始化互斥锁数组和条件变量数组。然后，该函数会调用listcreate函数，给bio_jobs这个数组的每个元素创建一个列表，同时给bio_pending数组的每个元素赋值为 0。这部分代码如下所示：\n\nfor (j = 0; j < bio_num_ops; j++) {\n    pthread_mutex_init(&bio_mutex[j],null);\n    pthread_cond_init(&bio_newjob_cond[j],null);\n    pthread_cond_init(&bio_step_cond[j],null);\n    bio_jobs[j] = listcreate();\n    bio_pending[j] = 0;\n}\n\n\n那么，要想了解给bio_jobs数组和bio_pending数组元素赋值的作用，我们就需要先搞清楚这两个数组的含义：\n\n * bio_jobs 数组的元素是bio_jobs结构体类型，用来表示后台任务。该结构体的成员变量包括了后台任务的创建时间 time，以及任务的参数。为该数组的每个元素创建一个列表，其实就是为每个后台线程创建一个要处理的任务列表。\n * bio_pending 数组的元素类型是unsigned long long，用来表示每种任务中，处于等待状态的任务个数。将该数组每个元素初始化为 0，其实就是表示初始时，每种任务都没有待处理的具体任务。\n\nstruct bio_job {\n    time_t time; //任务创建时间\n    void *arg1, *arg2, *arg3;  //任务参数\n};\n//以后台线程方式运行的任务列表\nstatic list *bio_jobs[bio_num_ops];\n//被阻塞的后台任务数组\nstatic unsigned long long bio_pending[bio_num_ops];\n\n\n好了，到这里，你就了解了bioinit函数执行时，会把线程互斥锁、条件变量对应数组初始化为 null，同时会给每个后台线程创建一个任务列表（对应bio_jobs数组的元素），以及会设置每种任务的待处理个数为 0（对应 bio_pending 数组的元素）。\n\n\n# bioinit 函数：设置线程属性并创建线程\n\n在完成了初始化之后，接下来，bioinit 函数会先通过 pthread_attr_t 类型的变量，给线程设置属性。然后，bioinit 函数会调用前面我提到的 pthread_create 函数来创建线程。\n\n不过，为了能更好地理解 bioinit 函数设置线程属性和创建线程的过程，我们需要先对 pthread_create 函数本身有所了解，该函数的原型如下所示：\n\nint  pthread_create(pthread_t *tidp, const  pthread_attr_t *attr,( void *)(*start_routine)( void *), void  *arg);\n\n\n可以看到，pthread_create 函数一共有 4 个参数，分别是：\n\n * *tidp，指向线程数据结构 pthread_t 的指针；\n * *attr，指向线程属性结构 pthread_attr_t 的指针；\n * *start_routine，线程所要运行的函数的起始地址，也是指向函数的指针；\n * *arg，传给运行函数的参数。\n\n了解了pthread_create函数之后，我们来看下bioinit函数的具体操作。\n\n首先，bioinit函数会调用pthread_attr_init函数，初始化线程属性变量attr，然后调用pthread_attr_getstacksize函数，获取线程的栈大小这一属性的当前值，并根据当前栈大小和redis_thread_stack_size宏定义的大小（默认值为 4mb），来计算最终的栈大小属性值。紧接着，bioinit函数会调用pthread_attr_setstacksize函数，来设置栈大小这一属性值。\n\n下面的代码展示了线程属性的获取、计算和设置逻辑，你可以看下。\n\npthread_attr_init(&attr);\npthread_attr_getstacksize(&attr,&stacksize);\nif (!stacksize) stacksize = 1; /针对solaris系统做处理\n    while (stacksize < redis_thread_stack_size) stacksize *= 2;\n    pthread_attr_setstacksize(&attr, stacksize);\n\n\n我也画了一张图，展示了线程属性的这一操作过程，你可以看下。\n\n\n\n在完成线程属性的设置后，接下来，bioinit函数会通过一个 for 循环，来依次为每种后台任务创建一个线程。循环的次数是由bio_num_ops宏定义决定的，也就是 3 次。相应的，bioinit函数就会调用 3 次pthread_create函数，并创建 3 个线程。bioinit 函数让这 3 个线程执行的函数都是**bioprocessbackgroundjobs**。\n\n不过这里要注意一点，就是在这三次线程的创建过程中，传给这个函数的参数分别是 0、1、2。这个创建过程如下所示：\n\nfor (j = 0; j < bio_num_ops; j++) {\n    void *arg = (void*)(unsigned long) j;\n    if (pthread_create(&thread,&attr,bioprocessbackgroundjobs,arg) != 0) {\n    \t… //报错信息\n    }\n    bio_threads[j] = thread;\n}\n\n\n你看了这个代码，可能会有一个小疑问：为什么创建的 3 个线程，它们所运行的 bioprocessbackgroundjobs 函数接收的参数分别是 0、1、2 呢？\n\n这就和 bioprocessbackgroundjobs 函数的实现有关了，我们来具体看下。\n\n\n# bioprocessbackgroundjobs 函数：处理后台任务\n\n首先，bioprocessbackgroundjobs函数会把接收到的参数 arg，转成unsigned long类型，并赋值给 type 变量，如下所示：\n\nvoid *bioprocessbackgroundjobs(void *arg) {\n    …\n\tunsigned long type = (unsigned long) arg;\n\t…\n}\n\n\n而type 变量表示的就是后台任务的操作码。这也是我刚才给你介绍的三种后台任务类型 bio_close_file、bio_aof_fsync 和 bio_lazy_free 对应的操作码，它们的取值分别为 0、1、2。\n\nbioprocessbackgroundjobs 函数的主要执行逻辑是一个 while(1)的循环。在这个循环中，bioprocessbackgroundjobs 函数会从 bio_jobs 这个数组中取出相应任务，并根据任务类型，调用具体的函数来执行。\n\n我刚才已经介绍过，bio_jobs数组的每一个元素是一个队列。而因为bio_jobs数组的元素个数，等于后台任务的类型个数（也就是 bio_num_ops），所以，bio_jobs数组的每个元素，实际上是对应了某一种后台任务的任务队列。\n\n在了解了这一点后，我们就容易理解bioprocessbackgroundjobs函数中的 while 循环了。因为传给bioprocessbackgroundjobs函数的参数，分别是 0、1、2，对应了三种任务类型，所以在这个循环中，bioprocessbackgroundjobs函数会一直不停地从某一种任务队列中，取出一个任务来执行。\n\n同时，bioprocessbackgroundjobs 函数会根据传入的任务操作类型调用相应函数，具体来说：\n\n * 任务类型是 bio_close_file，则调用 close 函数；\n * 任务类型是 bio_aof_fsync，则调用 redis_fsync 函数；\n * 任务类型是 bio_lazy_free，则再根据参数个数等情况，分别调用 lazyfreefreeobjectfrombiothread、lazyfreefreedatabasefrombiothread 和 lazyfreefreeslotsmapfrombiothread 这三个函数。\n\n最后，当某个任务执行完成后，bioprocessbackgroundjobs 函数会从任务队列中，把这个任务对应的数据结构删除。我把这部分代码放在这里，你可以看下。\n\nwhile(1) {\n        listnode *ln;\n\n        …\n        //从类型为type的任务队列中获取第一个任务\n        ln = listfirst(bio_jobs[type]);\n        job = ln->value;\n\n        …\n        //判断当前处理的后台任务类型是哪一种\n        if (type == bio_close_file) {\n            close((long)job->arg1);  //如果是关闭文件任务，那就调用close函数\n        } else if (type == bio_aof_fsync) {\n            redis_fsync((long)job->arg1); //如果是aof同步写任务，那就调用redis_fsync函数\n        } else if (type == bio_lazy_free) {\n            //如果是惰性删除任务，那根据任务的参数分别调用不同的惰性删除函数执行\n            if (job->arg1)\n                lazyfreefreeobjectfrombiothread(job->arg1);\n            else if (job->arg2 && job->arg3)\n                lazyfreefreedatabasefrombiothread(job->arg2,job->arg3);\n            else if (job->arg3)\n                lazyfreefreeslotsmapfrombiothread(job->arg3);\n        } else {\n            serverpanic("wrong job type in bioprocessbackgroundjobs().");\n        }\n        …\n        //任务执行完成后，调用 listdelnode 在任务队列中删除该任务\n        listdelnode(bio_jobs[type],ln);\n        //将对应的等待任务个数减一。\n        bio_pending[type]--;\n        …\n}\n\n\n所以说，bioinit 函数其实就是创建了 3 个线程，每个线程不停地去查看任务队列中是否有任务，如果有任务，就调用具体函数执行。\n\n你可以再参考回顾下图所展示的bioinit函数和bioprocessbackgroundjobs函数的基本处理流程。\n\n\n\n不过接下来你或许还会疑惑：既然 bioprocessbackgroundjobs 函数是负责执行任务的，那么哪个函数负责生成任务呢？\n\n这就是下面，我要给你介绍的 后台任务创建函数 biocreatebackgroundjob\n\n\n# biocreatebackgroundjob 函数：创建后台任务\n\nbiocreatebackgroundjob函数的原型如下，它会接收 4 个参数，其中，参数 type 表示该后台任务的类型，剩下来的 3 个参数，则对应了后台任务函数的参数，如下所示：\n\nvoid biocreatebackgroundjob(int type, void *arg1, void *arg2, void *arg3)\n\n\nbiocreatebackgroundjob函数在执行时，会先创建bio_job，这是后台任务对应的数据结构。然后，后台任务数据结构中的参数，会被设置为biocreatebackgroundjob函数传入的参数 arg1、arg2 和 arg3。\n\n最后，biocreatebackgroundjob函数调用listaddnodetail函数，将刚才创建的任务加入到对应的bio_jobs队列中，同时，将bio_pending数组的对应值加 1，表示有个任务在等待执行。\n\n{\n    //创建新的任务\n    struct bio_job *job = zmalloc(sizeof(*job));\n    //设置任务数据结构中的参数\n    job->time = time(null);\n    job->arg1 = arg1;\n    job->arg2 = arg2;\n    job->arg3 = arg3;\n    pthread_mutex_lock(&bio_mutex[type]);\n    listaddnodetail(bio_jobs[type],job);  //将任务加到bio_jobs数组的对应任务列表中\n    bio_pending[type]++; //将对应任务列表上等待处理的任务个数加1\n    pthread_cond_signal(&bio_newjob_cond[type]);\n    pthread_mutex_unlock(&bio_mutex[type]);\n}\n\n\n好了，这样一来，当 redis 进程想要启动一个后台任务时，只要调用biocreatebackgroundjob函数，并设置好该任务对应的类型和参数即可。然后，biocreatebackgroundjob函数就会把创建好的任务数据结构，放到后台任务对应的队列中。另一方面，bioinit函数在 redis server 启动时，创建的线程会不断地轮询后台任务队列，一旦发现有任务可以执行，就会将该任务取出并执行。\n\n其实，这种设计方式是典型的生产者-消费者模型。biocreatebackgroundjob函数是生产者，负责往每种任务队列中加入要执行的后台任务，而bioprocessbackgroundjobs函数是消费者，负责从每种任务队列中取出任务来执行。然后 redis 创建的后台线程，会调用bioprocessbackgroundjobs函数，从而实现一直循环检查任务队列。\n\n下图展示的就是biocreatebackgroundjob和bioprocessbackgroundjobs两者间的生产者-消费者模型，你可以看下。\n\n\n\n好了，到这里，我们就学习了 redis 后台线程的创建和运行机制。简单来说，主要是以下三个关键点：\n\n * redis 是先通过 bioinit 函数初始化和创建后台线程\n * 后台线程运行的是 bioprocessbackgroundjobs 函数，这个函数会轮询任务队列，并根据要处理的任务类型，调用相应函数进行处理\n * 后台线程要处理的任务是由 biocreatebackgroundjob函数来创建的，这些任务创建后会被放到任务队列中，等待bioprocessbackgroundjobs 函数处理\n\n\n# 总结\n\necho 你介绍了 redis 的执行模型，并且也从源码的角度出发，通过分析代码，带你了解了 redis 进程创建、以子进程方式创建的守护进程、以及后台线程和它们负责的工作任务。同时，这也解答了你在面试中可能经常会被问到的问题：redis 是单线程程序吗？\n\n事实上，redis server 启动后，它的主要工作包括接收客户端请求、解析请求和进行数据读写等操作，是由单线程来执行的，这也是我们常说 redis 是单线程程序的原因。\n\n但是，学完这节课你应该也知道，redis 还启动了 3 个线程来执行文件关闭、aof 同步写和惰性删除等操作，从这个角度来说，redis 又不能算单线程程序，它还是有多线程的。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Redis 多IO线程",frontmatter:{title:"Redis 多IO线程",date:"2024-09-17T20:15:03.000Z",permalink:"/pages/0850b6/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/03.%E4%B8%BB%E7%BA%BF/16.Redis%20%E5%A4%9AIO%E7%BA%BF%E7%A8%8B.html",relativePath:"Redis 系统设计/03.主线/16.Redis 多IO线程.md",key:"v-640684d7",path:"/pages/0850b6/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:353},{level:2,title:"单线程 IO 及其缺陷",slug:"单线程-io-及其缺陷",normalizedTitle:"单线程 io 及其缺陷",charIndex:705},{level:3,title:"异步处理",slug:"异步处理",normalizedTitle:"异步处理",charIndex:721},{level:3,title:"事件驱动",slug:"事件驱动",normalizedTitle:"事件驱动",charIndex:1411},{level:3,title:"单线程 IO 的瓶颈",slug:"单线程-io-的瓶颈",normalizedTitle:"单线程 io 的瓶颈",charIndex:2646},{level:2,title:"Redis 多线程 IO 的工作原理",slug:"redis-多线程-io-的工作原理",normalizedTitle:"redis 多线程 io 的工作原理",charIndex:611},{level:2,title:"Redis 多线程 IO 核心源码解析",slug:"redis-多线程-io-核心源码解析",normalizedTitle:"redis 多线程 io 核心源码解析",charIndex:634},{level:3,title:"1、初始化 IO 线程",slug:"_1、初始化-io-线程",normalizedTitle:"1、初始化 io 线程",charIndex:4119},{level:3,title:"2. 多线程读",slug:"_2-多线程读",normalizedTitle:"2. 多线程读",charIndex:9938},{level:4,title:"入队：如何推迟客户端读操作？",slug:"入队-如何推迟客户端读操作",normalizedTitle:"入队：如何推迟客户端读操作？",charIndex:10477},{level:4,title:"分配：如何将待读客户端分配给 IO 线程执行？",slug:"分配-如何将待读客户端分配给-io-线程执行",normalizedTitle:"分配：如何将待读客户端分配给 io 线程执行？",charIndex:12673},{level:3,title:"3、多线程写",slug:"_3、多线程写",normalizedTitle:"3、多线程写",charIndex:16350},{level:4,title:"入队：如何决定是否推迟客户端写操作？",slug:"入队-如何决定是否推迟客户端写操作",normalizedTitle:"入队：如何决定是否推迟客户端写操作？",charIndex:16360},{level:4,title:"分配：如何将待读客户端分配给 10 个线程执行？",slug:"分配-如何将待读客户端分配给-10-个线程执行",normalizedTitle:"分配：如何将待读客户端分配给 10 个线程执行？",charIndex:18267},{level:2,title:"Redis 多线程 IO 的性能调优与实际问题",slug:"redis-多线程-io-的性能调优与实际问题",normalizedTitle:"redis 多线程 io 的性能调优与实际问题",charIndex:19946},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1627},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:21906}],headersStr:"前言 单线程 IO 及其缺陷 异步处理 事件驱动 单线程 IO 的瓶颈 Redis 多线程 IO 的工作原理 Redis 多线程 IO 核心源码解析 1、初始化 IO 线程 2. 多线程读 入队：如何推迟客户端读操作？ 分配：如何将待读客户端分配给 IO 线程执行？ 3、多线程写 入队：如何决定是否推迟客户端写操作？ 分配：如何将待读客户端分配给 10 个线程执行？ Redis 多线程 IO 的性能调优与实际问题 总结 参考文献",content:'提出问题是一切智慧的开端\n\n 1. 为什么 Redis 从单线程演变到多线程？单线程模式的瓶颈在哪里？\n 2. Redis 6.0 引入多线程 IO 的主要原理是什么？它如何提升性能？\n 3. 多线程 IO 是如何在 Redis 中分担读写任务的？与单线程 IO 有哪些关键区别？\n 4. 在 Redis 6.0 中，哪些情况下适合启用多线程 IO，线程数该如何配置？\n 5. Redis 如何利用多线程机制分配和处理客户端请求？具体流程是怎样的？\n 6. 多线程 IO 如何解决单线程模式下的性能瓶颈？有哪些场景下效果最显著？\n 7. 什么是 Redis 多线程 IO 的主要性能优化点？如何避免潜在的问题？\n 8. 多 IO 线程对 Redis 命令执行的影响有哪些？是否会带来新的并发挑战？\n\n\n# 前言\n\n复杂的架构系统通常是逐渐演进的，从单线程到多线程，从单体应用到复杂功能的分布式系统，Redis 也经历了类似的发展历程。\n\n在单线程模式下，Redis 能够实现极高的吞吐量，但在某些情况下，处理时间可能会显著增加，导致性能下降。为了解决这些问题，Redis 引入了后台线程来处理一些耗时的操作。随着对更高吞吐量的需求增加，网络模块也成为瓶颈，因此 Redis 在 6.0 版本中引入了多线程来解决这个问题——这也是本文主要探讨的内容。\n\n本文内容包括：\n\n 1. 早期单线程 IO 处理过程及其缺点\n 2. Redis 多线程 IO 的工作原理\n 3. Redis 多线程 IO 核心源码解析\n\n注意\n\n请下载 Redis 6.0.15 的源码，以便查看与多 IO 线程机制相关的代码。\n\n\n# 单线程 IO 及其缺陷\n\n\n# 异步处理\n\nRedis 的核心负载由单线程处理，但为何其性能仍能如此优异？\n\n * 纯内存操作：Redis 的操作大多在内存中完成。\n * 非阻塞 IO：Redis 使用非阻塞的 IO 机制。\n * 异步 IO 处理：每个命令在接收、处理和返回的过程中，经过多个“不连续”的步骤。\n\n> 需要特别指出的是，此处的“异步处理”并非指同步/异步 IO，而是指 IO 处理过程的异步化，即各个处理步骤之间不是同步执行的，而是通过事件循环机制和非阻塞 IO，使 Redis 能在单线程环境下高效处理多个请求。\n\n假设客户端发送以下命令：\n\nGET key-how-to-be-a-better-man\n\n\nRedis 的回应是：\n\n努力加把劲把文章写完\n\n\n处理这个命令的过程包括以下几个步骤：\n\n * 接收：通过 TCP 接收命令，可能经历多次 TCP 包、确认应答 (ack) 和 IO 操作。\n * 解析：从接收到的数据中提取命令。\n * 执行：读取指定位置的值。\n * 返回：通过 TCP 返回值给客户端。如果值较大，IO 负载也更重。\n\n其中，解析和执行步骤主要是 CPU 和内存操作，而接收和返回主要涉及 IO 操作，这是我们关注的重点。以接收为例，Redis 采用了两种策略：\n\n * 同步处理：在接收完整命令之前，始终保持等待状态，接收到完整命令后才进行处理，然后返回结果。在网络状况不佳时，这种方法可能会导致较长的等待时间。\n * 异步处理：通过非阻塞 IO 和事件循环机制，在处理一个请求时，Redis 可以继续处理其他请求，从而避免了阻塞等待。Redis 使用高效的事件驱动机制（如 epoll）来监控 IO 事件，从而提高单线程下的并发处理能力。\n\n以下是对异步处理的类比：\n\n * 同步：当聊天框显示“正在输入”时，你需要等对方输入完成后，才能回答对方的问题。完成回答后，才会转向其他人。\n * 异步：当聊天框显示“正在输入”时，你可以回答其他已完成输入的问题，而不必等对方输入完成，待对方输入完成后再继续回答其他问题。\n\n显然，异步处理的效率更高，因为同步处理在等待上浪费了时间。异步处理策略总结如下：\n\n * 在网络包到达时立即读取并放入缓冲区，读取完成后立即进行其他操作，而不等待下一个包。\n * 解析缓冲区中的数据是否完整。若数据完整，则执行命令；若不完整，则继续处理其他任务。\n * 数据完整后立即执行命令，将结果放入缓冲区。\n * 将数据返回给客户端。如果一次不能全部发送，则等到可以发送时再继续发送，直到全部发送完毕。\n\n\n# 事件驱动\n\n尽管异步处理避免了零散的等待时间，但如何得知“网络包有数据”或“下次可以发送数据”呢？如果通过轮询检查这些时机，效率会很低。Redis 使用事件驱动机制来解决这一问题。\n\n事件驱动框架能够高效地通知 Redis 在何时需要处理事件。Redis 通过事件驱动机制（如 epoll）来监听和处理事件。Linux 中的 epoll 机制专为高效通知而设计。Redis 基于 epoll 等机制抽象出了一套事件驱动框架，整个服务器由事件驱动，当事件发生时进行处理，无事件时则处于空闲状态。\n\n * 可读事件：表示对应的 socket 中有新的 TCP 数据包到达。\n * 可写事件：表示对应的 socket 的写缓冲区已经空了（数据已通过网络发送给客户端）。\n\n处理流程如下：\n\n\n\n * aeMain() 内部为一个死循环，在 epoll_wait 处短暂休眠。\n * epoll_wait 返回当前可读、可写的 socket 列表。\n * beforeSleep 是进入休眠前执行的逻辑，主要是将数据回写到 socket。\n * 核心逻辑由 IO 事件触发，可能是可读事件，也可能是可写事件，否则执行定时任务。\n * 第一次的 IO 可读事件是监听 socket（如监听 6379 端口），当有握手请求时，执行 accept 调用，获取一个连接 socket，并注册可读回调 createClient，此后客户端与 Redis 的数据通过该 socket 进行传输。\n * 一个完整的命令可能通过多次 readQueryFromClient 读取完毕，意味着会有多次可读 IO 事件。\n * 命令执行结果也可能需要通过多次写操作完成。\n * 命令执行完毕后，对应的连接会被加入 clients_pending_write，beforeSleep 会尝试回写到 socket，若写不完则注册可写事件，下次继续写。\n * 整个过程中的 IO 全部是同步非阻塞的，没有时间浪费。\n\n\n# 单线程 IO 的瓶颈\n\n尽管单线程 IO 处理过程避免了等待时间的浪费，并能实现较高的 QPS，但仍然存在一些瓶颈：\n\n * 仅使用一个 CPU 核心（忽略后台线程）。\n * 当数据量较大时，Redis 的 QPS 可能会显著下降，有时一个大的 key 会拖垮整个系统。\n * 难以进一步提升 QPS。\n\nRedis 主线程的时间消耗主要集中在以下两个方面：\n\n * 逻辑计算消耗\n * 同步 IO 读写消耗，包括数据拷贝的消耗。\n\n当数据量较大时，瓶颈主要出现在同步 IO 上（假设带宽和内存充足）。主要的消耗包括：\n\n * 从 socket 中读取请求数据时，会将数据从内核态拷贝到用户态（read 调用）。\n * 将数据回写到 socket 时，会将数据从用户态拷贝到内核态（write 调用）。\n\n这些数据读写操作会占用大量 CPU 时间，并直接导致性能瓶颈。如果能通过多线程来分担这些消耗，Redis 的吞吐量有望得到显著提升，这也是 Redis 引入多线程 IO 的主要目的。\n\n\n# Redis 多线程 IO 的工作原理\n\n接下来将目光放到： 如何用多线程分担IO的负荷。其做法用简单的话来说就是：\n\n * 用一组单独的线程专门进行 read/write socket读写调用 （同步IO）\n * 读回调函数中不再读数据，而是将对应的连接追加到可读 clients_pending_read 的链表\n * 主线程在 beforeSleep 中将IO读任务分给IO线程组\n * 主线程自己也处理一个 IO 读任务，并自旋式等 IO 线程组处理完，再继续往下\n * 主线程在 beforeSleep 中将 IO 写任务分给IO线程组\n * 主线程自己也处理一个 IO 写任务，并自旋式等 IO 线程组处理完，再继续往下\n * IO线程组要么同时在读，要么同时在写\n * 命令的执行由主线程串行执行（保持单线程）\n * IO线程数量可配置\n\n完整流程图如下：\n\n\n\nbeforesleep 中，先让 IO 线程读数据，然后再让 IO 线程写数据。 读写时，多线程能并发执行，利用多核。\n\n 1. 将读任务均匀分发到各个IO线程的任务链表 io_threads_list[i]，将 io_threads_pending[i] 设置为对应的任务数，此时 IO 线程将从死循环中被激活，开始执行任务，执行完毕后，会将 io_threads_pending[i] 清零。 函数名为： handleClientsWithPendingReadsUsingThreads\n 2. 将写任务均匀分发到各个IO线程的任务链表 io_threads_list[i]，将io_threads_pending[i] 设置为对应的任务数，此时IO线程将从死循环中被激活，开始执行任务，执行完毕后，会将 io_threads_pending[i]清零。 函数名为： handleClientsWithPendingWritesUsingThreads\n 3. beforeSleep中主线程也会执行其中一个任务，执行完后自旋等待 IO 线程处理完。\n 4. 读任务要么在 beforeSleep 中被执行，要么在 IO 线程被执行，不会再在读回调中执行\n 5. 写任务会分散到 beforeSleep、IO线程、写回调中执行\n 6. 主线程和 IO 线程交互是无锁的，通过标志位设置进行，不会同时写任务链表\n\n\n# Redis 多线程 IO 核心源码解析\n\n\n# 1、初始化 IO 线程\n\n在 Redis 的执行模式 中提到：Redis 5.0 版本中的三个后台线程是在 server.c 文件的 main 函数启动的最后阶段调用 InitServerLast 函数来初始化的，而 InitServerLast 函数则进一步调用 bioInit 函数来完成初始化。\n\n在 Redis 6.0 中，InitServerLast 函数在调用 bioInit 后，新增了对 initThreadedIO 函数的调用，以初始化多线程 IO 机制。\n\ninitThreadedIO 函数用于初始化多 IO 线程，其代码实现如下所示：\n\n// server.c#InitServerLast\nvoid InitServerLast() {\n    bioInit();\n    initThreadedIO();\n    set_jemalloc_bg_thread(server.jemalloc_bg_thread);\n    server.initial_memory_usage = zmalloc_used_memory();\n}\n\n\n> bioInit 函数用于初始化 Redis 的后台 IO 线程，处理如 RDB/AOF 持久化等耗时操作。而 initThreadedIO 函数在此基础上进一步初始化多线程 IO 机制，以支持更高效的客户端请求处理。\n\ninitThreadedIO 函数的主要任务是初始化 IO 线程，其代码实现如下：\n\n// networking.c#initThreadedIO\nvoid initThreadedIO(void) {\n    server.io_threads_active = 0; /* 初始化时线程未激活。 */\n\n    /* 如果用户选择了单线程，则不创建额外线程：\n     * 所有 I/O 操作将由主线程处理。 */\n    if (server.io_threads_num == 1) return;\n\n    if (server.io_threads_num > IO_THREADS_MAX_NUM) {\n        serverLog(LL_WARNING,"致命错误：配置了过多的 I/O 线程。"\n                             "最大允许数量为 %d。", IO_THREADS_MAX_NUM);\n        exit(1);\n    }\n\n    /* 创建并初始化 I/O 线程。 */\n    for (int i = 0; i < server.io_threads_num; i++) {\n        /* 对所有线程（包括主线程）执行的操作。 */\n        io_threads_list[i] = listCreate();\n        if (i == 0) continue; /* 线程 0 是主线程。 */\n\n        /* 对额外线程执行的操作。 */\n        pthread_t tid;\n        pthread_mutex_init(&io_threads_mutex[i],NULL);\n        setIOPendingCount(i, 0);\n        pthread_mutex_lock(&io_threads_mutex[i]); /* 线程将被暂停。 */\n        // 创建线程，并指定处理方法 IOThreadMain\n        if (pthread_create(&tid,NULL,IOThreadMain,(void*)(long)i) != 0) {\n            serverLog(LL_WARNING,"致命错误：无法初始化 IO 线程。");\n            exit(1);\n        }\n        io_threads[i] = tid;\n    }\n}\n\n\n 1. 首先，initThreadedIO 函数会设置 IO 线程的激活标志。\n 2. 随后，initThreadedIO 函数对设置的 IO 线程数量进行检查：\n    1. 如果 IO 线程数量为 1，则表示只有一个主线程，initThreadedIO 函数将直接返回。在这种情况下，Redis server 的 IO 线程配置与 Redis 6.0 之前的版本相同。\n    2. 如果 IO 线程数量超过宏定义 IO_THREADS_MAX_NUM（默认值为 128），initThreadedIO 函数会报错并退出程序。\n    3. 如果 IO 线程数量在 1 和 IO_THREADS_MAX_NUM 之间，initThreadedIO 函数会执行一个循环，该循环次数等于设置的 IO 线程数量（注意，i == 0 表示主线程）。\n\n> IO_THREADS_MAX_NUM 是一个宏定义，表示 Redis 支持的最大 IO 线程数量，默认值为 128。这一限制旨在防止过多线程对系统性能造成负担。\n\n在该循环中，initThreadedIO 函数会对以下四个数组进行初始化：\n\n * io_threads_list 数组：保存每个 IO 线程要处理的客户端列表，数组的每个元素初始化为一个 List 类型的列表。\n * io_threads_pending 数组：保存等待每个 IO 线程处理的客户端数量。\n * io_threads_mutex 数组：保存线程的互斥锁。\n * io_threads 数组：保存每个 IO 线程的描述符。\n\n这些数组的定义都在 networking.c 文件中，如下所示：\n\npthread_t io_threads[IO_THREADS_MAX_NUM];   // 记录线程描述符的数组\npthread_mutex_t io_threads_mutex[IO_THREADS_MAX_NUM];  // 记录线程互斥锁的数组\n_Atomic unsigned long io_threads_pending[IO_THREADS_MAX_NUM];  // 记录线程待处理的客户端数量\nlist *io_threads_list[IO_THREADS_MAX_NUM];  // 记录线程对应处理的客户端列表\n\n\n在对这些数组进行初始化的同时，initThreadedIO 函数还会根据 IO 线程数量，调用 pthread_create 函数创建相应数量的线程。\n\n在 for 循环中，pthread_create 函数用于创建线程。每个线程执行 IOThreadMain 函数来处理客户端请求。如果 pthread_create 返回非零值，则说明线程创建失败，此时 Redis 会记录错误并退出。\n\n因此，initThreadedIO 函数创建的线程运行的函数是 IOThreadMain，参数为当前创建线程的编号。需要注意的是，该编号从 1 开始，而编号为 0 的线程实际上是运行 Redis server 主流程的主线程。\n\nvoid *IOThreadMain(void *myid) {\n    /* ID 是线程编号（从 0 到 server.iothreads_num-1） */\n    long id = (unsigned long)myid;\n    char thdname[16];\n\n    snprintf(thdname, sizeof(thdname), "io_thd_%ld", id);\n    redis_set_thread_title(thdname);\n    redisSetCpuAffinity(server.server_cpulist);\n    makeThreadKillable();\n\n    while(1) {\n        /* 等待开始 */\n        for (int j = 0; j < 1000000; j++) {\n            if (getIOPendingCount(id) != 0) break;\n        }\n\n        /* 给主线程一个机会来停止此线程。 */\n        if (getIOPendingCount(id) == 0) {\n            pthread_mutex_lock(&io_threads_mutex[id]);\n            pthread_mutex_unlock(&io_threads_mutex[id]);\n            continue;\n        }\n\n        serverAssert(getIOPendingCount(id) != 0);\n\n        if (tio_debug) printf("[%ld] %d to handle\\n", id, (int)listLength(io_threads_list[id]));\n\n        /* 处理：注意主线程不会在我们将待处理数量减少到 0 之前触碰我们的列表 */\n        listIter li;\n        listNode *ln;\n        listRewind(io_threads_list[id],&li);\n        while((ln = listNext(&li))) {\n            client *c = listNodeValue(ln);\n            if (io_threads_op == IO_THREADS_OP_WRITE) {\n                writeToClient(c,0);\n            } else if (io_threads_op == IO_THREADS_OP_READ) {\n                readQueryFromClient(c->conn);\n            } else {\n                serverPanic("io_threads_op 值未知");\n            }\n        }\n        listEmpty(io_threads_list[id]);\n        setIOPendingCount(id, 0);\n\n        if (tio_debug) printf("[%ld] Done\\n", id);\n    }\n}\n\n\nIOThreadMain 函数也在 networking.c 文件中定义，其主要逻辑为一个 while(1) 循环。\n\nIOThreadMain 函数在循环中处理 io_threads_list 数组中每个线程的客户端请求。\n\n正如之前所述，io_threads_list 数组中为每个 IO 线程使用一个列表记录待处理的客户端。因此，IOThreadMain 函数会从每个 IO 线程对应的列表中取出待处理的客户端，并根据操作类型执行相应操作。操作类型由变量 io_threads_op 表示，其值有两种：\n\n * io_threads_op 的值为宏定义 IO_THREADS_OP_WRITE：表示该 IO 线程进行写操作，将数据从 Redis 写回客户端，线程会调用 writeToClient 函数。\n * io_threads_op 的值为宏定义 IO_THREADS_OP_READ：表示该 IO 线程进行读操作，从客户端读取数据，线程会调用 readQueryFromClient 函数。\n\n笔记\n\n如果您对 Java 编程熟悉，可以将 IOThreadMain 函数视为 Runnable 的具体实现。其核心逻辑在于 while(1) 无限循环中。根据源码，IO 线程从 io_threads_list 队列（或列表）中获取待处理的客户端，并根据操作类型选择具体的执行逻辑。这是一种典型的 生产者-消费者模型，主线程负责投递事件，IO 线程负责消费事件（主线程也参与）。\n\n我绘制了下图，以展示 IOThreadMain 函数的基本流程，请参考：\n\n\n\n如上所示，每个 IO 线程在运行过程中，会不断检查是否有待处理的客户端请求。如果存在待处理的客户端，线程会根据操作类型，从客户端读取数据或将数据写回客户端。这些操作涉及 Redis 与客户端之间的 I/O 交互，因此这些线程被称为 IO 线程。\n\n在此，您可能会产生一些疑问：IO 线程如何将客户端添加到 io_threads_list 数组中？\n\n这涉及 Redis server 的全局变量 server。server 变量中包含两个 List 类型的成员变量：clients_pending_write 和 clients_pending_read，分别记录待写回数据的客户端和待读取数据的客户端，如下所示：\n\nstruct redisServer {\n    ...\n    // 待写回数据的客户端\n    list *clients_pending_write;  \n    // 待读取数据的客户端\n    list *clients_pending_read;\n    ...\n}\n\n\nRedis server 在接收客户端请求和返回数据的过程中，会根据特定条件推迟客户端的读写操作，并将这些客户端分别保存到这两个列表中。随后，在每次进入事件循环前，Redis server 会将列表中的客户端添加到 io_threads_list 数组中，由 IO 线程进行处理。\n\n接下来，我们将探讨 Redis 如何推迟客户端的读写操作，并将这些客户端添加到 clients_pending_write 和 clients_pending_read 列表中。\n\n\n# 2. 多线程读\n\n在早期的单线程版本中，当多路复用检测到客户端数据准备就绪时，主事件循环会轮询处理这些就绪的客户端，步骤如下：\n\n 1. 读取数据\n 2. 解析数据\n 3. 执行命令\n 4. 将数据写回客户端缓冲区\n 5. 等待下一轮主事件循环\n 6. 将客户端缓冲数据写回客户端\n\n在多线程模式下（假设配置了多线程读），上述流程有所变化：数据读取和解析操作将被分配给多个 IO 线程（包括主线程）。\n\n所有就绪客户端将暂存至队列中：\n\nstruct redisServer {  \n   ...\n   list *clients_pending_read;\n   ...\n}\n\n\n处理流程如下：\n\n 1. 主线程开始监听 IO 事件\n 2. 主线程调用 readQueryFromClient\n 3. postponeClientRead 将客户端添加至 clients_pending_read\n 4. handleClientsWithPendingReadsUsingThreads 将 clients_pending_read 列表中的客户端分配给所有 IO 线程\n 5. 主线程阻塞并等待所有 IO 线程完成读取\n 6. 主线程循环遍历并处理所有读取到的数据\n\n# 入队：如何推迟客户端读操作？\n\nRedis server 在与客户端建立连接后，会开始监听客户端的可读事件。处理可读事件的回调函数是 readQueryFromClient。我在某处已介绍了这一过程，您可以再次回顾。\n\n在 Redis 6.0 版本中，readQueryFromClient 函数首先从传入的参数 conn 中获取客户端 c，然后调用 postponeClientRead 函数来判断是否推迟从客户端读取数据。执行逻辑如下：\n\nvoid readQueryFromClient(connection *conn) {\n    client *c = connGetPrivateData(conn);  // 从连接数据结构中获取客户端\n    ...\n    if (postponeClientRead(c)) return;  // 判断是否推迟从客户端读取数据\n    ...\n}\n\n\n接下来，我们将分析 postponeClientRead 函数的执行逻辑。该函数会根据以下四个条件判断是否可以推迟从客户端读取数据：\n\n条件一：全局变量 server 的 io_threads_active 值为 1\n\n这表示多 IO 线程已激活。正如前述，该变量在 initThreadedIO 函数中初始化为 0，表明多 IO 线程初始化后默认未激活（后文将详细介绍何时将该变量值设置为 1）。\n\n条件二：全局变量 server 的 io_threads_do_read 值为 1\n\n这表示多 IO 线程可以处理延后执行的客户端读操作。该变量在 Redis 配置文件 redis.conf 中通过 io-threads-do-reads 配置项设置，默认为 no，即多 IO 线程机制默认不用于客户端读操作。若要启用多 IO 线程处理客户端读操作，需将 io-threads-do-reads 配置项设为 yes。\n\n条件三：ProcessingEventsWhileBlocked 变量值为 0\n\n这表示 processEventsWhileBlocked 函数未在执行中。ProcessingEventsWhileBlocked 是一个全局变量，当 processEventsWhileBlocked 函数执行时，该变量值为 1，函数执行完成后值为 0。processEventsWhileBlocked 函数在 networking.c 文件中实现，主要用于在 Redis 读取 RDB 或 AOF 文件时处理事件，避免因读取文件阻塞 Redis 导致事件处理延迟。因此，当 processEventsWhileBlocked 函数处理客户端可读事件时，这些客户端读操作不会被推迟。\n\n条件四：客户端当前标识不能包含 CLIENT_MASTER、CLIENT_SLAVE 和 CLIENT_PENDING_READ\n\n其中，CLIENT_MASTER 和 CLIENT_SLAVE 标识表示客户端用于主从复制，这些客户端的读操作不会被推迟。CLIENT_PENDING_READ 标识表示客户端已设置为推迟读操作，因此，对于已带有 CLIENT_PENDING_READ 标识的客户端，postponeClientRead 函数不会再次推迟其读操作。\n\n只有当上述四个条件均满足时，postponeClientRead 函数才会推迟当前客户端的读操作。具体来说，postponeClientRead 函数会为该客户端设置 CLIENT_PENDING_READ 标识，并调用 listAddNodeHead 函数，将客户端添加到全局变量 server 的 clients_pending_read 列表中。\n\n以下是 postponeClientRead 函数的代码：\n\nint postponeClientRead(client *c) {\n    // 判断 IO 线程是否激活\n    if (server.io_threads_active && server.io_threads_do_reads &&      \n         !ProcessingEventsWhileBlocked &&\n        !(c->flags & (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_PENDING_READ)))\n    {\n        c->flags |= CLIENT_PENDING_READ; // 设置客户端标识为 CLIENT_PENDING_READ，表示推迟该客户端的读操作\n        listAddNodeHead(server.clients_pending_read,c); // 将客户端添加到 clients_pending_read 列表中\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\n\n综上所述，Redis 在客户端读事件回调函数 readQueryFromClient 中，通过调用 postponeClientRead 函数来判断并推迟客户端读操作。接下来，我们将探讨 Redis 如何推迟客户端写操作。\n\n# 分配：如何将待读客户端分配给 IO 线程执行？\n\n首先，我们需要了解 handleClientsWithPendingReadsUsingThreads 函数的作用。该函数在 beforeSleep 函数中被调用。\n\n在 Redis 6.0 版本的实现中，事件驱动框架通过调用 aeMain 函数执行事件循环，aeMain 函数进一步调用 aeProcessEvents 处理各种事件。在 aeProcessEvents 实际调用 aeApiPoll 捕获 IO 事件之前，beforeSleep 函数会被执行。\n\n该过程如图所示：\n\n\n\nhandleClientsWithPendingReadsUsingThreads 函数的执行逻辑可分为四个步骤：\n\n第一步，该函数首先检查全局变量 server 的 io_threads_active 成员变量，确认 IO 线程是否激活，同时依据 io_threads_do_reads 成员变量判断是否允许 IO 线程处理待读客户端。只有在 IO 线程被激活并且允许处理待读客户端的情况下，handleClientsWithPendingReadsUsingThreads 函数才会继续执行，否则函数将直接返回。判断逻辑如下：\n\nif (!server.io_threads_active || !server.io_threads_do_reads) \n\treturn 0;\n\n\n第二步，函数获取 clients_pending_read 列表的长度，表示待处理客户端的数量。随后，函数从 clients_pending_read 列表中逐一取出待处理的客户端，并通过客户端在列表中的序号对 IO 线程数量进行取模运算。\n\n通过这种方式，客户端将被分配给对应的 IO 线程。接着，函数会调用 listAddNodeTail 将分配好的客户端添加到 io_threads_list 数组的相应元素中。io_threads_list 数组的每个元素是一个列表，保存了每个 IO 线程需要处理的客户端。\n\n以下是具体的示例：\n\n假设 IO 线程数量为 3，而 clients_pending_read 列表中有 5 个客户端，其序号分别为 0、1、2、3 和 4。在此步骤中，这些客户端的序号对线程数量 3 取模的结果分别是 0、1、2、0、1，这对应了处理这些客户端的 IO 线程编号。也就是说，客户端 0 由线程 0 处理，客户端 1 由线程 1 处理，以此类推。客户端的分配方式实际上是一种 轮询 方式。\n\n下图展示了这种分配结果：\n\n\n\n以下代码展示了如何以轮询方式将客户端分配给 IO 线程的执行逻辑：\n\nint processed = listLength(server.clients_pending_read);\nlistRewind(server.clients_pending_read, &li);\nint item_id = 0;\nwhile ((ln = listNext(&li))) {\n    client *c = listNodeValue(ln);\n    int target_id = item_id % server.io_threads_num;\n    listAddNodeTail(io_threads_list[target_id], c);\n    item_id++;\n}\n\n\n当 handleClientsWithPendingReadsUsingThreads 函数完成客户端的 IO 线程分配后，它会将 IO 线程的操作标识设置为 读操作，即 IO_THREADS_OP_READ。然后，它会遍历 io_threads_list 数组中的每个元素列表，记录每个线程待处理客户端的数量，并赋值给 io_threads_pending 数组。具体过程如下：\n\nio_threads_op = IO_THREADS_OP_READ;\nfor (int j = 1; j < server.io_threads_num; j++) {\n    int count = listLength(io_threads_list[j]);\n    io_threads_pending[j] = count;\n}\n\n\n第三步，函数会将 io_threads_list 数组中的 0 号列表（即 io_threads_list[0]）中的客户端逐一取出，并调用 readQueryFromClient 函数进行处理。\n\n需要注意的是，handleClientsWithPendingReadsUsingThreads 函数本身由 IO 主线程执行，而 io_threads_list 数组中的 0 号线程即为 IO 主线程，因此此步骤是由主线程处理其待读客户端：\n\nlistRewind(io_threads_list[0], &li);  // 获取 0 号列表中的所有客户端\nwhile ((ln = listNext(&li))) {\n    client *c = listNodeValue(ln);\n    readQueryFromClient(c->conn);\n}\nlistEmpty(io_threads_list[0]); // 处理完后，清空 0 号列表\n\n\n接下来，handleClientsWithPendingReadsUsingThreads 函数会进入一个 while(1) 循环，等待所有 IO 线程完成对待读客户端的处理，如下所示：\n\nwhile (1) {\n    unsigned long pending = 0;\n    for (int j = 1; j < server.io_threads_num; j++)\n        pending += io_threads_pending[j];\n    if (pending == 0) break;\n}\n\n\n第四步，函数会再次遍历 clients_pending_read 列表，逐一取出其中的客户端。接着，函数会检查客户端是否有 CLIENT_PENDING_COMMAND 标识。如果存在，说明该客户端的命令已被某个 IO 线程解析，可以执行。\n\n此时，handleClientsWithPendingReadsUsingThreads 函数会调用 processCommandAndResetClient 执行命令，并直接调用 processInputBuffer 解析客户端中所有命令并执行。\n\n相关代码如下：\n\nwhile (listLength(server.clients_pending_read)) {\n    ln = listFirst(server.clients_pending_read);\n    client *c = listNodeValue(ln);\n    ...\n    // 如果命令已解析，则执行该命令\n    if (c->flags & CLIENT_PENDING_COMMAND) {\n        c->flags &= ~CLIENT_PENDING_COMMAND;\n        if (processCommandAndResetClient(c) == C_ERR) {\n            continue;\n        }\n    }\n    // 解析并执行所有命令\n    processInputBuffer(c);\n}\n\n\n至此，你已经了解了如何将 clients_pending_read 列表中的待读客户端通过上述四个步骤分配给 IO 线程进行处理。下图展示了这一主要过程，你可以进一步回顾：\n\n\n\n接下来，我们将探讨待写客户端的分配和处理方式。\n\n需要注意的是，当 待处理客户端 数量较少时，Redis 认为不需要多线程共同处理，所有任务将由主线程完成：\n\nint stopThreadedIOIfNeeded(void) {\n    int pending = listLength(server.clients_pending_write);\n\n    if (server.io_threads_num == 1) return 1;\n\n    if (pending < (server.io_threads_num * 2)) {\n        if (server.io_threads_active) stopThreadedIO();\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\n\n当 待处理客户端数量 小于 2倍的 IO 线程数 时，所有客户端数据将由主线程处理。\n\n\n# 3、多线程写\n\n# 入队：如何决定是否推迟客户端写操作？\n\n在 Redis 中，当执行客户端命令后，需要向客户端返回结果时，会调用 addReply 函数将待返回的结果写入客户端的输出缓冲区。\n\n在 addReply 函数的开始部分，该函数会调用 prepareClientToWrite 函数来判断是否需要推迟执行客户端的写操作。以下代码展示了 addReply 函数如何调用 prepareClientToWrite 函数：\n\nvoid addReply(client *c, robj *obj) {\n    if (prepareClientToWrite(c) != C_OK) return;\n    ...\n}\n\n\n接下来，我们来看一下 prepareClientToWrite 函数。该函数根据客户端的设置进行一系列判断。其中，clientHasPendingReplies 函数会被调用，用于检查当前客户端的输出缓冲区中是否还有待写回的数据。\n\n如果缓冲区中没有待写回的数据，prepareClientToWrite 函数会进一步调用 clientInstallWriteHandler 函数，以判断是否能够推迟客户端的写操作。以下代码展示了这一调用过程：\n\nint prepareClientToWrite(client *c) {\n    ...\n    // 如果客户端没有待写回数据，则调用 clientInstallWriteHandler 函数\n    if (!clientHasPendingReplies(c)) clientInstallWriteHandler(c);\n    return C_OK;\n}\n\n\n因此，推迟客户端写操作的最终决定由 clientInstallWriteHandler 函数做出。该函数会检查两个条件：\n\n * 条件一：客户端未设置 CLIENT_PENDING_WRITE 标识，即尚未推迟过写操作。\n * 条件二：客户端所在实例未进行主从复制，或即使正在进行主从复制，客户端所在实例作为从节点且全量复制的 RDB 文件已传输完成，可以接收请求。\n\n当上述两个条件都满足时，clientInstallWriteHandler 函数会将客户端标识设置为 CLIENT_PENDING_WRITE，以表示推迟该客户端的写操作。同时，该函数会将客户端添加到全局变量 server 的待写回客户端列表 clients_pending_write 中。\n\nvoid clientInstallWriteHandler(client *c) {\n    // 如果客户端没有设置过 CLIENT_PENDING_WRITE 标识，且客户端不在主从复制中，或作为从节点且已接收请求\n    if (!(c->flags & CLIENT_PENDING_WRITE) &&\n        (c->replstate == REPL_STATE_NONE ||\n         (c->replstate == SLAVE_STATE_ONLINE && !c->repl_put_online_on_ack)))\n    {\n        // 将客户端标识设置为待写回，即 CLIENT_PENDING_WRITE\n        c->flags |= CLIENT_PENDING_WRITE;\n        listAddNodeHead(server.clients_pending_write, c);  // 将客户端添加到 clients_pending_write 列表\n    }\n}\n\n\n为帮助理解，我绘制了一张图，展示了 Redis 推迟客户端写操作的函数调用关系，供参考。\n\n\n\n然而，当 Redis 使用 clients_pending_read 和 clients_pending_write 两个列表保存推迟执行的客户端时，这些客户端如何分配给多个 I/O 线程进行处理呢？ 这涉及到以下两个函数：\n\n * handleClientsWithPendingReadsUsingThreads 函数：负责将 clients_pending_read 列表中的客户端分配给 I/O 线程处理。\n * handleClientsWithPendingWritesUsingThreads 函数：负责将 clients_pending_write 列表中的客户端分配给 I/O 线程处理。\n\n接下来，我们将详细介绍这两个函数的具体操作。\n\n# 分配：如何将待读客户端分配给 10 个线程执行？\n\n与待读客户端的分配类似，待写客户端的分配处理由 handleClientsWithPendingWritesUsingThreads 函数完成，该函数同样在 beforeSleep 函数中被调用。\n\nhandleClientsWithPendingWritesUsingThreads 函数的主要流程可以分为四个步骤，其中第二、第三和第四步的执行逻辑与 handleClientsWithPendingReadsUsingThreads 函数类似。\n\n简言之，在第二步中，handleClientsWithPendingWritesUsingThreads 函数会将待写客户端按照 轮询方式 分配给 I/O 线程，并将其添加到 io_threads_list 数组的各个元素中。\n\n在第三步中，handleClientsWithPendingWritesUsingThreads 函数会让主 I/O 线程处理其待写客户端，并执行 while(1) 循环以等待所有 I/O 线程完成处理。\n\n在第四步中，handleClientsWithPendingWritesUsingThreads 函数会再次检查 clients_pending_write 列表中是否还有待写客户端。如果存在且这些客户端仍有数据待写，函数会调用 connSetWriteHandler 函数注册可写事件，该事件的回调函数为 sendReplyToClient。\n\n当事件循环流程再次执行时，sendReplyToClient 函数会被调用，它会直接调用 writeToClient 函数，将客户端缓冲区中的数据写回。\n\n需要注意的是，connSetWriteHandler 函数最终会映射为 connSocketSetWriteHandler 函数，后者在 connection.c 文件中实现。connSocketSetWriteHandler 函数会调用 aeCreateFileEvent 函数创建 AE_WRITABLE 事件，这即为可写事件的注册（有关 aeCreateFileEvent 函数的使用，可以参见第 11 讲）。\n\n与 handleClientsWithPendingReadsUsingThreads 函数不同的是，在第一步中，handleClientsWithPendingWritesUsingThreads 函数会判断 I/O 线程数量是否为 1，或待写客户端数量是否少于 I/O 线程数量的两倍。\n\n如果满足上述任一条件，则 handleClientsWithPendingWritesUsingThreads 函数不会采用多线程处理客户端，而是调用 handleClientsWithPendingWrites 函数由主 I/O 线程直接处理待写客户端。这主要是为了在待写客户端数量较少时，节省 CPU 开销。\n\n以下是条件判断逻辑：\n\nif (server.io_threads_num == 1 || stopThreadedIOIfNeeded()) {\n    return handleClientsWithPendingWrites();\n}\n\n\n此外，handleClientsWithPendingWritesUsingThreads 函数在第一步中还会判断 I/O 线程是否已激活。如果未激活，则调用 startThreadedIO 函数，将全局变量 server 的 io_threads_active 成员变量设置为 1，以表示 I/O 线程已激活。此判断操作如下：\n\nif (!server.io_threads_active) startThreadedIO();\n\n\n总之，Redis 通过 handleClientsWithPendingWritesUsingThreads 函数将待写客户端按轮询方式分配给各个 I/O 线程，并由这些线程负责数据的写回。\n\n\n# Redis 多线程 IO 的性能调优与实际问题\n\nredis 默认情况下不会开启多线程处理，官方也建议，除非性能达到瓶颈，否则没必要开启多线程。\n\n配置多少合适？\n\n官方文档 redis.conf 中介绍有：\n\n> By default threading is disabled, we suggest enabling it only in machines that have at least 4 or more cores, leaving at least one spare core. Using more than 8 threads is unlikely to help much. We also recommend using threaded I/O only if you actually have performance problems, with Redis instances being able to use a quite big percentage of CPU time, otherwise there is no point in using this feature.\n> \n> So for instance if you have a four cores boxes, try to use 2 or 3 I/O threads, if you have a 8 cores, try to use 6 threads. In order to enable I/O threads use the following configuration directive:\n\nCPU 4 核以上，才考虑开启多线程，其中：\n\n * 4 核开启 2 - 3 个 IO 线程\n * 8 核 开启 6 个 IO 线程\n * 超过 8 个 IO 线程，性能提升已经不大\n\n值得注意的是，以上的 IO 线程其实包含了主线程。\n\n配置：\n\n开启多线程：配置 io-thread 即可。io-thread = 1 表示只使用主线程\n\n> io-threads 4\n\n开启之后，默认写操作会通过多线程来处理，而读操作则不会。\n\n如果读操作也想要开启多线程，则需要配置：\n\n> io-threads-do-reads yes\n\n\n# 总结\n\n今天这节课，我给你介绍了 Redis 6.0 中新设计实现的多 IO 线程机制。这个机制的设计主要是为了使用多个 IO 线程，来并发处理客户端读取数据、解析命令和写回数据。使用了多线程后，Redis 就可以充分利用服务器的多核特性，从而提高 IO 效率。\n\n总结来说，Redis 6.0 先是在初始化过程中，根据用户设置的 IO 线程数量，创建对应数量的 IO 线程。\n\n当 Redis server 初始化完成后正常运行时，它会在 readQueryFromClient 函数中通过调用 postponeClientRead 函数来决定是否推迟客户端读操作。同时，Redis server 会在 addReply 函数中通过调用 prepareClientToWrite 函数，来决定是否推迟客户端写操作。而待读写的客户端会被分别加入到 clients_pending_read 和 clients_pending_write 两个列表中。\n\n这样，每当 Redis server 要进入事件循环流程前，都会在 beforeSleep 函数中分别调用 handleClientsWithPendingReadsUsingThreads 函数和 handleClientsWithPendingWritesUsingThreads 函数，将待读写客户端以轮询方式分配给 IO 线程，加入到 IO 线程的待处理客户端列表 io_threads_list 中。\n\n而 IO 线程一旦运行后，本身会一直检测 io_threads_list 中的客户端，如果有待读写客户端，IO 线程就会调用 readQueryFromClient 或 writeToClient 函数来进行处理。\n\n最后，我也想再提醒你一下，多 IO 线程本身并不会执行命令，它们只是利用多核并行地读取数据和解析命令，或是将 server 数据写回（下节课我还会结合分布式锁的原子性保证，来给你介绍这一部分的源码实现。）。所以，Redis 执行命令的线程还是主线程。这一点对于你理解多 IO 线程机制很重要，可以避免你误解 Redis 有多线程同时执行命令。\n\n这样一来，我们原来针对 Redis 单个主线程做的优化仍然有效，比如避免 bigkey、避免阻塞操作等。\n\n\n# 参考文献\n\n * redis 6.0之多线程，深入解读 - 知乎 (zhihu.com)\n * Redis 6.0 多线程IO处理过程详解 (zhihu.com)\n * Redis 源码剖析与实战 (geekbang.org)',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 为什么 redis 从单线程演变到多线程？单线程模式的瓶颈在哪里？\n 2. redis 6.0 引入多线程 io 的主要原理是什么？它如何提升性能？\n 3. 多线程 io 是如何在 redis 中分担读写任务的？与单线程 io 有哪些关键区别？\n 4. 在 redis 6.0 中，哪些情况下适合启用多线程 io，线程数该如何配置？\n 5. redis 如何利用多线程机制分配和处理客户端请求？具体流程是怎样的？\n 6. 多线程 io 如何解决单线程模式下的性能瓶颈？有哪些场景下效果最显著？\n 7. 什么是 redis 多线程 io 的主要性能优化点？如何避免潜在的问题？\n 8. 多 io 线程对 redis 命令执行的影响有哪些？是否会带来新的并发挑战？\n\n\n# 前言\n\n复杂的架构系统通常是逐渐演进的，从单线程到多线程，从单体应用到复杂功能的分布式系统，redis 也经历了类似的发展历程。\n\n在单线程模式下，redis 能够实现极高的吞吐量，但在某些情况下，处理时间可能会显著增加，导致性能下降。为了解决这些问题，redis 引入了后台线程来处理一些耗时的操作。随着对更高吞吐量的需求增加，网络模块也成为瓶颈，因此 redis 在 6.0 版本中引入了多线程来解决这个问题——这也是本文主要探讨的内容。\n\n本文内容包括：\n\n 1. 早期单线程 io 处理过程及其缺点\n 2. redis 多线程 io 的工作原理\n 3. redis 多线程 io 核心源码解析\n\n注意\n\n请下载 redis 6.0.15 的源码，以便查看与多 io 线程机制相关的代码。\n\n\n# 单线程 io 及其缺陷\n\n\n# 异步处理\n\nredis 的核心负载由单线程处理，但为何其性能仍能如此优异？\n\n * 纯内存操作：redis 的操作大多在内存中完成。\n * 非阻塞 io：redis 使用非阻塞的 io 机制。\n * 异步 io 处理：每个命令在接收、处理和返回的过程中，经过多个“不连续”的步骤。\n\n> 需要特别指出的是，此处的“异步处理”并非指同步/异步 io，而是指 io 处理过程的异步化，即各个处理步骤之间不是同步执行的，而是通过事件循环机制和非阻塞 io，使 redis 能在单线程环境下高效处理多个请求。\n\n假设客户端发送以下命令：\n\nget key-how-to-be-a-better-man\n\n\nredis 的回应是：\n\n努力加把劲把文章写完\n\n\n处理这个命令的过程包括以下几个步骤：\n\n * 接收：通过 tcp 接收命令，可能经历多次 tcp 包、确认应答 (ack) 和 io 操作。\n * 解析：从接收到的数据中提取命令。\n * 执行：读取指定位置的值。\n * 返回：通过 tcp 返回值给客户端。如果值较大，io 负载也更重。\n\n其中，解析和执行步骤主要是 cpu 和内存操作，而接收和返回主要涉及 io 操作，这是我们关注的重点。以接收为例，redis 采用了两种策略：\n\n * 同步处理：在接收完整命令之前，始终保持等待状态，接收到完整命令后才进行处理，然后返回结果。在网络状况不佳时，这种方法可能会导致较长的等待时间。\n * 异步处理：通过非阻塞 io 和事件循环机制，在处理一个请求时，redis 可以继续处理其他请求，从而避免了阻塞等待。redis 使用高效的事件驱动机制（如 epoll）来监控 io 事件，从而提高单线程下的并发处理能力。\n\n以下是对异步处理的类比：\n\n * 同步：当聊天框显示“正在输入”时，你需要等对方输入完成后，才能回答对方的问题。完成回答后，才会转向其他人。\n * 异步：当聊天框显示“正在输入”时，你可以回答其他已完成输入的问题，而不必等对方输入完成，待对方输入完成后再继续回答其他问题。\n\n显然，异步处理的效率更高，因为同步处理在等待上浪费了时间。异步处理策略总结如下：\n\n * 在网络包到达时立即读取并放入缓冲区，读取完成后立即进行其他操作，而不等待下一个包。\n * 解析缓冲区中的数据是否完整。若数据完整，则执行命令；若不完整，则继续处理其他任务。\n * 数据完整后立即执行命令，将结果放入缓冲区。\n * 将数据返回给客户端。如果一次不能全部发送，则等到可以发送时再继续发送，直到全部发送完毕。\n\n\n# 事件驱动\n\n尽管异步处理避免了零散的等待时间，但如何得知“网络包有数据”或“下次可以发送数据”呢？如果通过轮询检查这些时机，效率会很低。redis 使用事件驱动机制来解决这一问题。\n\n事件驱动框架能够高效地通知 redis 在何时需要处理事件。redis 通过事件驱动机制（如 epoll）来监听和处理事件。linux 中的 epoll 机制专为高效通知而设计。redis 基于 epoll 等机制抽象出了一套事件驱动框架，整个服务器由事件驱动，当事件发生时进行处理，无事件时则处于空闲状态。\n\n * 可读事件：表示对应的 socket 中有新的 tcp 数据包到达。\n * 可写事件：表示对应的 socket 的写缓冲区已经空了（数据已通过网络发送给客户端）。\n\n处理流程如下：\n\n\n\n * aemain() 内部为一个死循环，在 epoll_wait 处短暂休眠。\n * epoll_wait 返回当前可读、可写的 socket 列表。\n * beforesleep 是进入休眠前执行的逻辑，主要是将数据回写到 socket。\n * 核心逻辑由 io 事件触发，可能是可读事件，也可能是可写事件，否则执行定时任务。\n * 第一次的 io 可读事件是监听 socket（如监听 6379 端口），当有握手请求时，执行 accept 调用，获取一个连接 socket，并注册可读回调 createclient，此后客户端与 redis 的数据通过该 socket 进行传输。\n * 一个完整的命令可能通过多次 readqueryfromclient 读取完毕，意味着会有多次可读 io 事件。\n * 命令执行结果也可能需要通过多次写操作完成。\n * 命令执行完毕后，对应的连接会被加入 clients_pending_write，beforesleep 会尝试回写到 socket，若写不完则注册可写事件，下次继续写。\n * 整个过程中的 io 全部是同步非阻塞的，没有时间浪费。\n\n\n# 单线程 io 的瓶颈\n\n尽管单线程 io 处理过程避免了等待时间的浪费，并能实现较高的 qps，但仍然存在一些瓶颈：\n\n * 仅使用一个 cpu 核心（忽略后台线程）。\n * 当数据量较大时，redis 的 qps 可能会显著下降，有时一个大的 key 会拖垮整个系统。\n * 难以进一步提升 qps。\n\nredis 主线程的时间消耗主要集中在以下两个方面：\n\n * 逻辑计算消耗\n * 同步 io 读写消耗，包括数据拷贝的消耗。\n\n当数据量较大时，瓶颈主要出现在同步 io 上（假设带宽和内存充足）。主要的消耗包括：\n\n * 从 socket 中读取请求数据时，会将数据从内核态拷贝到用户态（read 调用）。\n * 将数据回写到 socket 时，会将数据从用户态拷贝到内核态（write 调用）。\n\n这些数据读写操作会占用大量 cpu 时间，并直接导致性能瓶颈。如果能通过多线程来分担这些消耗，redis 的吞吐量有望得到显著提升，这也是 redis 引入多线程 io 的主要目的。\n\n\n# redis 多线程 io 的工作原理\n\n接下来将目光放到： 如何用多线程分担io的负荷。其做法用简单的话来说就是：\n\n * 用一组单独的线程专门进行 read/write socket读写调用 （同步io）\n * 读回调函数中不再读数据，而是将对应的连接追加到可读 clients_pending_read 的链表\n * 主线程在 beforesleep 中将io读任务分给io线程组\n * 主线程自己也处理一个 io 读任务，并自旋式等 io 线程组处理完，再继续往下\n * 主线程在 beforesleep 中将 io 写任务分给io线程组\n * 主线程自己也处理一个 io 写任务，并自旋式等 io 线程组处理完，再继续往下\n * io线程组要么同时在读，要么同时在写\n * 命令的执行由主线程串行执行（保持单线程）\n * io线程数量可配置\n\n完整流程图如下：\n\n\n\nbeforesleep 中，先让 io 线程读数据，然后再让 io 线程写数据。 读写时，多线程能并发执行，利用多核。\n\n 1. 将读任务均匀分发到各个io线程的任务链表 io_threads_list[i]，将 io_threads_pending[i] 设置为对应的任务数，此时 io 线程将从死循环中被激活，开始执行任务，执行完毕后，会将 io_threads_pending[i] 清零。 函数名为： handleclientswithpendingreadsusingthreads\n 2. 将写任务均匀分发到各个io线程的任务链表 io_threads_list[i]，将io_threads_pending[i] 设置为对应的任务数，此时io线程将从死循环中被激活，开始执行任务，执行完毕后，会将 io_threads_pending[i]清零。 函数名为： handleclientswithpendingwritesusingthreads\n 3. beforesleep中主线程也会执行其中一个任务，执行完后自旋等待 io 线程处理完。\n 4. 读任务要么在 beforesleep 中被执行，要么在 io 线程被执行，不会再在读回调中执行\n 5. 写任务会分散到 beforesleep、io线程、写回调中执行\n 6. 主线程和 io 线程交互是无锁的，通过标志位设置进行，不会同时写任务链表\n\n\n# redis 多线程 io 核心源码解析\n\n\n# 1、初始化 io 线程\n\n在 redis 的执行模式 中提到：redis 5.0 版本中的三个后台线程是在 server.c 文件的 main 函数启动的最后阶段调用 initserverlast 函数来初始化的，而 initserverlast 函数则进一步调用 bioinit 函数来完成初始化。\n\n在 redis 6.0 中，initserverlast 函数在调用 bioinit 后，新增了对 initthreadedio 函数的调用，以初始化多线程 io 机制。\n\ninitthreadedio 函数用于初始化多 io 线程，其代码实现如下所示：\n\n// server.c#initserverlast\nvoid initserverlast() {\n    bioinit();\n    initthreadedio();\n    set_jemalloc_bg_thread(server.jemalloc_bg_thread);\n    server.initial_memory_usage = zmalloc_used_memory();\n}\n\n\n> bioinit 函数用于初始化 redis 的后台 io 线程，处理如 rdb/aof 持久化等耗时操作。而 initthreadedio 函数在此基础上进一步初始化多线程 io 机制，以支持更高效的客户端请求处理。\n\ninitthreadedio 函数的主要任务是初始化 io 线程，其代码实现如下：\n\n// networking.c#initthreadedio\nvoid initthreadedio(void) {\n    server.io_threads_active = 0; /* 初始化时线程未激活。 */\n\n    /* 如果用户选择了单线程，则不创建额外线程：\n     * 所有 i/o 操作将由主线程处理。 */\n    if (server.io_threads_num == 1) return;\n\n    if (server.io_threads_num > io_threads_max_num) {\n        serverlog(ll_warning,"致命错误：配置了过多的 i/o 线程。"\n                             "最大允许数量为 %d。", io_threads_max_num);\n        exit(1);\n    }\n\n    /* 创建并初始化 i/o 线程。 */\n    for (int i = 0; i < server.io_threads_num; i++) {\n        /* 对所有线程（包括主线程）执行的操作。 */\n        io_threads_list[i] = listcreate();\n        if (i == 0) continue; /* 线程 0 是主线程。 */\n\n        /* 对额外线程执行的操作。 */\n        pthread_t tid;\n        pthread_mutex_init(&io_threads_mutex[i],null);\n        setiopendingcount(i, 0);\n        pthread_mutex_lock(&io_threads_mutex[i]); /* 线程将被暂停。 */\n        // 创建线程，并指定处理方法 iothreadmain\n        if (pthread_create(&tid,null,iothreadmain,(void*)(long)i) != 0) {\n            serverlog(ll_warning,"致命错误：无法初始化 io 线程。");\n            exit(1);\n        }\n        io_threads[i] = tid;\n    }\n}\n\n\n 1. 首先，initthreadedio 函数会设置 io 线程的激活标志。\n 2. 随后，initthreadedio 函数对设置的 io 线程数量进行检查：\n    1. 如果 io 线程数量为 1，则表示只有一个主线程，initthreadedio 函数将直接返回。在这种情况下，redis server 的 io 线程配置与 redis 6.0 之前的版本相同。\n    2. 如果 io 线程数量超过宏定义 io_threads_max_num（默认值为 128），initthreadedio 函数会报错并退出程序。\n    3. 如果 io 线程数量在 1 和 io_threads_max_num 之间，initthreadedio 函数会执行一个循环，该循环次数等于设置的 io 线程数量（注意，i == 0 表示主线程）。\n\n> io_threads_max_num 是一个宏定义，表示 redis 支持的最大 io 线程数量，默认值为 128。这一限制旨在防止过多线程对系统性能造成负担。\n\n在该循环中，initthreadedio 函数会对以下四个数组进行初始化：\n\n * io_threads_list 数组：保存每个 io 线程要处理的客户端列表，数组的每个元素初始化为一个 list 类型的列表。\n * io_threads_pending 数组：保存等待每个 io 线程处理的客户端数量。\n * io_threads_mutex 数组：保存线程的互斥锁。\n * io_threads 数组：保存每个 io 线程的描述符。\n\n这些数组的定义都在 networking.c 文件中，如下所示：\n\npthread_t io_threads[io_threads_max_num];   // 记录线程描述符的数组\npthread_mutex_t io_threads_mutex[io_threads_max_num];  // 记录线程互斥锁的数组\n_atomic unsigned long io_threads_pending[io_threads_max_num];  // 记录线程待处理的客户端数量\nlist *io_threads_list[io_threads_max_num];  // 记录线程对应处理的客户端列表\n\n\n在对这些数组进行初始化的同时，initthreadedio 函数还会根据 io 线程数量，调用 pthread_create 函数创建相应数量的线程。\n\n在 for 循环中，pthread_create 函数用于创建线程。每个线程执行 iothreadmain 函数来处理客户端请求。如果 pthread_create 返回非零值，则说明线程创建失败，此时 redis 会记录错误并退出。\n\n因此，initthreadedio 函数创建的线程运行的函数是 iothreadmain，参数为当前创建线程的编号。需要注意的是，该编号从 1 开始，而编号为 0 的线程实际上是运行 redis server 主流程的主线程。\n\nvoid *iothreadmain(void *myid) {\n    /* id 是线程编号（从 0 到 server.iothreads_num-1） */\n    long id = (unsigned long)myid;\n    char thdname[16];\n\n    snprintf(thdname, sizeof(thdname), "io_thd_%ld", id);\n    redis_set_thread_title(thdname);\n    redissetcpuaffinity(server.server_cpulist);\n    makethreadkillable();\n\n    while(1) {\n        /* 等待开始 */\n        for (int j = 0; j < 1000000; j++) {\n            if (getiopendingcount(id) != 0) break;\n        }\n\n        /* 给主线程一个机会来停止此线程。 */\n        if (getiopendingcount(id) == 0) {\n            pthread_mutex_lock(&io_threads_mutex[id]);\n            pthread_mutex_unlock(&io_threads_mutex[id]);\n            continue;\n        }\n\n        serverassert(getiopendingcount(id) != 0);\n\n        if (tio_debug) printf("[%ld] %d to handle\\n", id, (int)listlength(io_threads_list[id]));\n\n        /* 处理：注意主线程不会在我们将待处理数量减少到 0 之前触碰我们的列表 */\n        listiter li;\n        listnode *ln;\n        listrewind(io_threads_list[id],&li);\n        while((ln = listnext(&li))) {\n            client *c = listnodevalue(ln);\n            if (io_threads_op == io_threads_op_write) {\n                writetoclient(c,0);\n            } else if (io_threads_op == io_threads_op_read) {\n                readqueryfromclient(c->conn);\n            } else {\n                serverpanic("io_threads_op 值未知");\n            }\n        }\n        listempty(io_threads_list[id]);\n        setiopendingcount(id, 0);\n\n        if (tio_debug) printf("[%ld] done\\n", id);\n    }\n}\n\n\niothreadmain 函数也在 networking.c 文件中定义，其主要逻辑为一个 while(1) 循环。\n\niothreadmain 函数在循环中处理 io_threads_list 数组中每个线程的客户端请求。\n\n正如之前所述，io_threads_list 数组中为每个 io 线程使用一个列表记录待处理的客户端。因此，iothreadmain 函数会从每个 io 线程对应的列表中取出待处理的客户端，并根据操作类型执行相应操作。操作类型由变量 io_threads_op 表示，其值有两种：\n\n * io_threads_op 的值为宏定义 io_threads_op_write：表示该 io 线程进行写操作，将数据从 redis 写回客户端，线程会调用 writetoclient 函数。\n * io_threads_op 的值为宏定义 io_threads_op_read：表示该 io 线程进行读操作，从客户端读取数据，线程会调用 readqueryfromclient 函数。\n\n笔记\n\n如果您对 java 编程熟悉，可以将 iothreadmain 函数视为 runnable 的具体实现。其核心逻辑在于 while(1) 无限循环中。根据源码，io 线程从 io_threads_list 队列（或列表）中获取待处理的客户端，并根据操作类型选择具体的执行逻辑。这是一种典型的 生产者-消费者模型，主线程负责投递事件，io 线程负责消费事件（主线程也参与）。\n\n我绘制了下图，以展示 iothreadmain 函数的基本流程，请参考：\n\n\n\n如上所示，每个 io 线程在运行过程中，会不断检查是否有待处理的客户端请求。如果存在待处理的客户端，线程会根据操作类型，从客户端读取数据或将数据写回客户端。这些操作涉及 redis 与客户端之间的 i/o 交互，因此这些线程被称为 io 线程。\n\n在此，您可能会产生一些疑问：io 线程如何将客户端添加到 io_threads_list 数组中？\n\n这涉及 redis server 的全局变量 server。server 变量中包含两个 list 类型的成员变量：clients_pending_write 和 clients_pending_read，分别记录待写回数据的客户端和待读取数据的客户端，如下所示：\n\nstruct redisserver {\n    ...\n    // 待写回数据的客户端\n    list *clients_pending_write;  \n    // 待读取数据的客户端\n    list *clients_pending_read;\n    ...\n}\n\n\nredis server 在接收客户端请求和返回数据的过程中，会根据特定条件推迟客户端的读写操作，并将这些客户端分别保存到这两个列表中。随后，在每次进入事件循环前，redis server 会将列表中的客户端添加到 io_threads_list 数组中，由 io 线程进行处理。\n\n接下来，我们将探讨 redis 如何推迟客户端的读写操作，并将这些客户端添加到 clients_pending_write 和 clients_pending_read 列表中。\n\n\n# 2. 多线程读\n\n在早期的单线程版本中，当多路复用检测到客户端数据准备就绪时，主事件循环会轮询处理这些就绪的客户端，步骤如下：\n\n 1. 读取数据\n 2. 解析数据\n 3. 执行命令\n 4. 将数据写回客户端缓冲区\n 5. 等待下一轮主事件循环\n 6. 将客户端缓冲数据写回客户端\n\n在多线程模式下（假设配置了多线程读），上述流程有所变化：数据读取和解析操作将被分配给多个 io 线程（包括主线程）。\n\n所有就绪客户端将暂存至队列中：\n\nstruct redisserver {  \n   ...\n   list *clients_pending_read;\n   ...\n}\n\n\n处理流程如下：\n\n 1. 主线程开始监听 io 事件\n 2. 主线程调用 readqueryfromclient\n 3. postponeclientread 将客户端添加至 clients_pending_read\n 4. handleclientswithpendingreadsusingthreads 将 clients_pending_read 列表中的客户端分配给所有 io 线程\n 5. 主线程阻塞并等待所有 io 线程完成读取\n 6. 主线程循环遍历并处理所有读取到的数据\n\n# 入队：如何推迟客户端读操作？\n\nredis server 在与客户端建立连接后，会开始监听客户端的可读事件。处理可读事件的回调函数是 readqueryfromclient。我在某处已介绍了这一过程，您可以再次回顾。\n\n在 redis 6.0 版本中，readqueryfromclient 函数首先从传入的参数 conn 中获取客户端 c，然后调用 postponeclientread 函数来判断是否推迟从客户端读取数据。执行逻辑如下：\n\nvoid readqueryfromclient(connection *conn) {\n    client *c = conngetprivatedata(conn);  // 从连接数据结构中获取客户端\n    ...\n    if (postponeclientread(c)) return;  // 判断是否推迟从客户端读取数据\n    ...\n}\n\n\n接下来，我们将分析 postponeclientread 函数的执行逻辑。该函数会根据以下四个条件判断是否可以推迟从客户端读取数据：\n\n条件一：全局变量 server 的 io_threads_active 值为 1\n\n这表示多 io 线程已激活。正如前述，该变量在 initthreadedio 函数中初始化为 0，表明多 io 线程初始化后默认未激活（后文将详细介绍何时将该变量值设置为 1）。\n\n条件二：全局变量 server 的 io_threads_do_read 值为 1\n\n这表示多 io 线程可以处理延后执行的客户端读操作。该变量在 redis 配置文件 redis.conf 中通过 io-threads-do-reads 配置项设置，默认为 no，即多 io 线程机制默认不用于客户端读操作。若要启用多 io 线程处理客户端读操作，需将 io-threads-do-reads 配置项设为 yes。\n\n条件三：processingeventswhileblocked 变量值为 0\n\n这表示 processeventswhileblocked 函数未在执行中。processingeventswhileblocked 是一个全局变量，当 processeventswhileblocked 函数执行时，该变量值为 1，函数执行完成后值为 0。processeventswhileblocked 函数在 networking.c 文件中实现，主要用于在 redis 读取 rdb 或 aof 文件时处理事件，避免因读取文件阻塞 redis 导致事件处理延迟。因此，当 processeventswhileblocked 函数处理客户端可读事件时，这些客户端读操作不会被推迟。\n\n条件四：客户端当前标识不能包含 client_master、client_slave 和 client_pending_read\n\n其中，client_master 和 client_slave 标识表示客户端用于主从复制，这些客户端的读操作不会被推迟。client_pending_read 标识表示客户端已设置为推迟读操作，因此，对于已带有 client_pending_read 标识的客户端，postponeclientread 函数不会再次推迟其读操作。\n\n只有当上述四个条件均满足时，postponeclientread 函数才会推迟当前客户端的读操作。具体来说，postponeclientread 函数会为该客户端设置 client_pending_read 标识，并调用 listaddnodehead 函数，将客户端添加到全局变量 server 的 clients_pending_read 列表中。\n\n以下是 postponeclientread 函数的代码：\n\nint postponeclientread(client *c) {\n    // 判断 io 线程是否激活\n    if (server.io_threads_active && server.io_threads_do_reads &&      \n         !processingeventswhileblocked &&\n        !(c->flags & (client_master|client_slave|client_pending_read)))\n    {\n        c->flags |= client_pending_read; // 设置客户端标识为 client_pending_read，表示推迟该客户端的读操作\n        listaddnodehead(server.clients_pending_read,c); // 将客户端添加到 clients_pending_read 列表中\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\n\n综上所述，redis 在客户端读事件回调函数 readqueryfromclient 中，通过调用 postponeclientread 函数来判断并推迟客户端读操作。接下来，我们将探讨 redis 如何推迟客户端写操作。\n\n# 分配：如何将待读客户端分配给 io 线程执行？\n\n首先，我们需要了解 handleclientswithpendingreadsusingthreads 函数的作用。该函数在 beforesleep 函数中被调用。\n\n在 redis 6.0 版本的实现中，事件驱动框架通过调用 aemain 函数执行事件循环，aemain 函数进一步调用 aeprocessevents 处理各种事件。在 aeprocessevents 实际调用 aeapipoll 捕获 io 事件之前，beforesleep 函数会被执行。\n\n该过程如图所示：\n\n\n\nhandleclientswithpendingreadsusingthreads 函数的执行逻辑可分为四个步骤：\n\n第一步，该函数首先检查全局变量 server 的 io_threads_active 成员变量，确认 io 线程是否激活，同时依据 io_threads_do_reads 成员变量判断是否允许 io 线程处理待读客户端。只有在 io 线程被激活并且允许处理待读客户端的情况下，handleclientswithpendingreadsusingthreads 函数才会继续执行，否则函数将直接返回。判断逻辑如下：\n\nif (!server.io_threads_active || !server.io_threads_do_reads) \n\treturn 0;\n\n\n第二步，函数获取 clients_pending_read 列表的长度，表示待处理客户端的数量。随后，函数从 clients_pending_read 列表中逐一取出待处理的客户端，并通过客户端在列表中的序号对 io 线程数量进行取模运算。\n\n通过这种方式，客户端将被分配给对应的 io 线程。接着，函数会调用 listaddnodetail 将分配好的客户端添加到 io_threads_list 数组的相应元素中。io_threads_list 数组的每个元素是一个列表，保存了每个 io 线程需要处理的客户端。\n\n以下是具体的示例：\n\n假设 io 线程数量为 3，而 clients_pending_read 列表中有 5 个客户端，其序号分别为 0、1、2、3 和 4。在此步骤中，这些客户端的序号对线程数量 3 取模的结果分别是 0、1、2、0、1，这对应了处理这些客户端的 io 线程编号。也就是说，客户端 0 由线程 0 处理，客户端 1 由线程 1 处理，以此类推。客户端的分配方式实际上是一种 轮询 方式。\n\n下图展示了这种分配结果：\n\n\n\n以下代码展示了如何以轮询方式将客户端分配给 io 线程的执行逻辑：\n\nint processed = listlength(server.clients_pending_read);\nlistrewind(server.clients_pending_read, &li);\nint item_id = 0;\nwhile ((ln = listnext(&li))) {\n    client *c = listnodevalue(ln);\n    int target_id = item_id % server.io_threads_num;\n    listaddnodetail(io_threads_list[target_id], c);\n    item_id++;\n}\n\n\n当 handleclientswithpendingreadsusingthreads 函数完成客户端的 io 线程分配后，它会将 io 线程的操作标识设置为 读操作，即 io_threads_op_read。然后，它会遍历 io_threads_list 数组中的每个元素列表，记录每个线程待处理客户端的数量，并赋值给 io_threads_pending 数组。具体过程如下：\n\nio_threads_op = io_threads_op_read;\nfor (int j = 1; j < server.io_threads_num; j++) {\n    int count = listlength(io_threads_list[j]);\n    io_threads_pending[j] = count;\n}\n\n\n第三步，函数会将 io_threads_list 数组中的 0 号列表（即 io_threads_list[0]）中的客户端逐一取出，并调用 readqueryfromclient 函数进行处理。\n\n需要注意的是，handleclientswithpendingreadsusingthreads 函数本身由 io 主线程执行，而 io_threads_list 数组中的 0 号线程即为 io 主线程，因此此步骤是由主线程处理其待读客户端：\n\nlistrewind(io_threads_list[0], &li);  // 获取 0 号列表中的所有客户端\nwhile ((ln = listnext(&li))) {\n    client *c = listnodevalue(ln);\n    readqueryfromclient(c->conn);\n}\nlistempty(io_threads_list[0]); // 处理完后，清空 0 号列表\n\n\n接下来，handleclientswithpendingreadsusingthreads 函数会进入一个 while(1) 循环，等待所有 io 线程完成对待读客户端的处理，如下所示：\n\nwhile (1) {\n    unsigned long pending = 0;\n    for (int j = 1; j < server.io_threads_num; j++)\n        pending += io_threads_pending[j];\n    if (pending == 0) break;\n}\n\n\n第四步，函数会再次遍历 clients_pending_read 列表，逐一取出其中的客户端。接着，函数会检查客户端是否有 client_pending_command 标识。如果存在，说明该客户端的命令已被某个 io 线程解析，可以执行。\n\n此时，handleclientswithpendingreadsusingthreads 函数会调用 processcommandandresetclient 执行命令，并直接调用 processinputbuffer 解析客户端中所有命令并执行。\n\n相关代码如下：\n\nwhile (listlength(server.clients_pending_read)) {\n    ln = listfirst(server.clients_pending_read);\n    client *c = listnodevalue(ln);\n    ...\n    // 如果命令已解析，则执行该命令\n    if (c->flags & client_pending_command) {\n        c->flags &= ~client_pending_command;\n        if (processcommandandresetclient(c) == c_err) {\n            continue;\n        }\n    }\n    // 解析并执行所有命令\n    processinputbuffer(c);\n}\n\n\n至此，你已经了解了如何将 clients_pending_read 列表中的待读客户端通过上述四个步骤分配给 io 线程进行处理。下图展示了这一主要过程，你可以进一步回顾：\n\n\n\n接下来，我们将探讨待写客户端的分配和处理方式。\n\n需要注意的是，当 待处理客户端 数量较少时，redis 认为不需要多线程共同处理，所有任务将由主线程完成：\n\nint stopthreadedioifneeded(void) {\n    int pending = listlength(server.clients_pending_write);\n\n    if (server.io_threads_num == 1) return 1;\n\n    if (pending < (server.io_threads_num * 2)) {\n        if (server.io_threads_active) stopthreadedio();\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\n\n当 待处理客户端数量 小于 2倍的 io 线程数 时，所有客户端数据将由主线程处理。\n\n\n# 3、多线程写\n\n# 入队：如何决定是否推迟客户端写操作？\n\n在 redis 中，当执行客户端命令后，需要向客户端返回结果时，会调用 addreply 函数将待返回的结果写入客户端的输出缓冲区。\n\n在 addreply 函数的开始部分，该函数会调用 prepareclienttowrite 函数来判断是否需要推迟执行客户端的写操作。以下代码展示了 addreply 函数如何调用 prepareclienttowrite 函数：\n\nvoid addreply(client *c, robj *obj) {\n    if (prepareclienttowrite(c) != c_ok) return;\n    ...\n}\n\n\n接下来，我们来看一下 prepareclienttowrite 函数。该函数根据客户端的设置进行一系列判断。其中，clienthaspendingreplies 函数会被调用，用于检查当前客户端的输出缓冲区中是否还有待写回的数据。\n\n如果缓冲区中没有待写回的数据，prepareclienttowrite 函数会进一步调用 clientinstallwritehandler 函数，以判断是否能够推迟客户端的写操作。以下代码展示了这一调用过程：\n\nint prepareclienttowrite(client *c) {\n    ...\n    // 如果客户端没有待写回数据，则调用 clientinstallwritehandler 函数\n    if (!clienthaspendingreplies(c)) clientinstallwritehandler(c);\n    return c_ok;\n}\n\n\n因此，推迟客户端写操作的最终决定由 clientinstallwritehandler 函数做出。该函数会检查两个条件：\n\n * 条件一：客户端未设置 client_pending_write 标识，即尚未推迟过写操作。\n * 条件二：客户端所在实例未进行主从复制，或即使正在进行主从复制，客户端所在实例作为从节点且全量复制的 rdb 文件已传输完成，可以接收请求。\n\n当上述两个条件都满足时，clientinstallwritehandler 函数会将客户端标识设置为 client_pending_write，以表示推迟该客户端的写操作。同时，该函数会将客户端添加到全局变量 server 的待写回客户端列表 clients_pending_write 中。\n\nvoid clientinstallwritehandler(client *c) {\n    // 如果客户端没有设置过 client_pending_write 标识，且客户端不在主从复制中，或作为从节点且已接收请求\n    if (!(c->flags & client_pending_write) &&\n        (c->replstate == repl_state_none ||\n         (c->replstate == slave_state_online && !c->repl_put_online_on_ack)))\n    {\n        // 将客户端标识设置为待写回，即 client_pending_write\n        c->flags |= client_pending_write;\n        listaddnodehead(server.clients_pending_write, c);  // 将客户端添加到 clients_pending_write 列表\n    }\n}\n\n\n为帮助理解，我绘制了一张图，展示了 redis 推迟客户端写操作的函数调用关系，供参考。\n\n\n\n然而，当 redis 使用 clients_pending_read 和 clients_pending_write 两个列表保存推迟执行的客户端时，这些客户端如何分配给多个 i/o 线程进行处理呢？ 这涉及到以下两个函数：\n\n * handleclientswithpendingreadsusingthreads 函数：负责将 clients_pending_read 列表中的客户端分配给 i/o 线程处理。\n * handleclientswithpendingwritesusingthreads 函数：负责将 clients_pending_write 列表中的客户端分配给 i/o 线程处理。\n\n接下来，我们将详细介绍这两个函数的具体操作。\n\n# 分配：如何将待读客户端分配给 10 个线程执行？\n\n与待读客户端的分配类似，待写客户端的分配处理由 handleclientswithpendingwritesusingthreads 函数完成，该函数同样在 beforesleep 函数中被调用。\n\nhandleclientswithpendingwritesusingthreads 函数的主要流程可以分为四个步骤，其中第二、第三和第四步的执行逻辑与 handleclientswithpendingreadsusingthreads 函数类似。\n\n简言之，在第二步中，handleclientswithpendingwritesusingthreads 函数会将待写客户端按照 轮询方式 分配给 i/o 线程，并将其添加到 io_threads_list 数组的各个元素中。\n\n在第三步中，handleclientswithpendingwritesusingthreads 函数会让主 i/o 线程处理其待写客户端，并执行 while(1) 循环以等待所有 i/o 线程完成处理。\n\n在第四步中，handleclientswithpendingwritesusingthreads 函数会再次检查 clients_pending_write 列表中是否还有待写客户端。如果存在且这些客户端仍有数据待写，函数会调用 connsetwritehandler 函数注册可写事件，该事件的回调函数为 sendreplytoclient。\n\n当事件循环流程再次执行时，sendreplytoclient 函数会被调用，它会直接调用 writetoclient 函数，将客户端缓冲区中的数据写回。\n\n需要注意的是，connsetwritehandler 函数最终会映射为 connsocketsetwritehandler 函数，后者在 connection.c 文件中实现。connsocketsetwritehandler 函数会调用 aecreatefileevent 函数创建 ae_writable 事件，这即为可写事件的注册（有关 aecreatefileevent 函数的使用，可以参见第 11 讲）。\n\n与 handleclientswithpendingreadsusingthreads 函数不同的是，在第一步中，handleclientswithpendingwritesusingthreads 函数会判断 i/o 线程数量是否为 1，或待写客户端数量是否少于 i/o 线程数量的两倍。\n\n如果满足上述任一条件，则 handleclientswithpendingwritesusingthreads 函数不会采用多线程处理客户端，而是调用 handleclientswithpendingwrites 函数由主 i/o 线程直接处理待写客户端。这主要是为了在待写客户端数量较少时，节省 cpu 开销。\n\n以下是条件判断逻辑：\n\nif (server.io_threads_num == 1 || stopthreadedioifneeded()) {\n    return handleclientswithpendingwrites();\n}\n\n\n此外，handleclientswithpendingwritesusingthreads 函数在第一步中还会判断 i/o 线程是否已激活。如果未激活，则调用 startthreadedio 函数，将全局变量 server 的 io_threads_active 成员变量设置为 1，以表示 i/o 线程已激活。此判断操作如下：\n\nif (!server.io_threads_active) startthreadedio();\n\n\n总之，redis 通过 handleclientswithpendingwritesusingthreads 函数将待写客户端按轮询方式分配给各个 i/o 线程，并由这些线程负责数据的写回。\n\n\n# redis 多线程 io 的性能调优与实际问题\n\nredis 默认情况下不会开启多线程处理，官方也建议，除非性能达到瓶颈，否则没必要开启多线程。\n\n配置多少合适？\n\n官方文档 redis.conf 中介绍有：\n\n> by default threading is disabled, we suggest enabling it only in machines that have at least 4 or more cores, leaving at least one spare core. using more than 8 threads is unlikely to help much. we also recommend using threaded i/o only if you actually have performance problems, with redis instances being able to use a quite big percentage of cpu time, otherwise there is no point in using this feature.\n> \n> so for instance if you have a four cores boxes, try to use 2 or 3 i/o threads, if you have a 8 cores, try to use 6 threads. in order to enable i/o threads use the following configuration directive:\n\ncpu 4 核以上，才考虑开启多线程，其中：\n\n * 4 核开启 2 - 3 个 io 线程\n * 8 核 开启 6 个 io 线程\n * 超过 8 个 io 线程，性能提升已经不大\n\n值得注意的是，以上的 io 线程其实包含了主线程。\n\n配置：\n\n开启多线程：配置 io-thread 即可。io-thread = 1 表示只使用主线程\n\n> io-threads 4\n\n开启之后，默认写操作会通过多线程来处理，而读操作则不会。\n\n如果读操作也想要开启多线程，则需要配置：\n\n> io-threads-do-reads yes\n\n\n# 总结\n\n今天这节课，我给你介绍了 redis 6.0 中新设计实现的多 io 线程机制。这个机制的设计主要是为了使用多个 io 线程，来并发处理客户端读取数据、解析命令和写回数据。使用了多线程后，redis 就可以充分利用服务器的多核特性，从而提高 io 效率。\n\n总结来说，redis 6.0 先是在初始化过程中，根据用户设置的 io 线程数量，创建对应数量的 io 线程。\n\n当 redis server 初始化完成后正常运行时，它会在 readqueryfromclient 函数中通过调用 postponeclientread 函数来决定是否推迟客户端读操作。同时，redis server 会在 addreply 函数中通过调用 prepareclienttowrite 函数，来决定是否推迟客户端写操作。而待读写的客户端会被分别加入到 clients_pending_read 和 clients_pending_write 两个列表中。\n\n这样，每当 redis server 要进入事件循环流程前，都会在 beforesleep 函数中分别调用 handleclientswithpendingreadsusingthreads 函数和 handleclientswithpendingwritesusingthreads 函数，将待读写客户端以轮询方式分配给 io 线程，加入到 io 线程的待处理客户端列表 io_threads_list 中。\n\n而 io 线程一旦运行后，本身会一直检测 io_threads_list 中的客户端，如果有待读写客户端，io 线程就会调用 readqueryfromclient 或 writetoclient 函数来进行处理。\n\n最后，我也想再提醒你一下，多 io 线程本身并不会执行命令，它们只是利用多核并行地读取数据和解析命令，或是将 server 数据写回（下节课我还会结合分布式锁的原子性保证，来给你介绍这一部分的源码实现。）。所以，redis 执行命令的线程还是主线程。这一点对于你理解多 io 线程机制很重要，可以避免你误解 redis 有多线程同时执行命令。\n\n这样一来，我们原来针对 redis 单个主线程做的优化仍然有效，比如避免 bigkey、避免阻塞操作等。\n\n\n# 参考文献\n\n * redis 6.0之多线程，深入解读 - 知乎 (zhihu.com)\n * redis 6.0 多线程io处理过程详解 (zhihu.com)\n * redis 源码剖析与实战 (geekbang.org)',charsets:{cjk:!0},lastUpdated:"2024/09/17, 14:54:21",lastUpdatedTimestamp:1726584861e3},{title:"LRU 策略",frontmatter:{title:"LRU 策略",date:"2024-09-15T23:59:53.000Z",permalink:"/pages/b43a19/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E6%94%AF%E7%BA%BF/05.LRU%20%E7%AD%96%E7%95%A5.html",relativePath:"Redis 系统设计/04.支线/05.LRU 策略.md",key:"v-443fe0ec",path:"/pages/b43a19/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:399},{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:924},{level:2,title:"全局 LRU 时钟值的计算",slug:"全局-lru-时钟值的计算",normalizedTitle:"全局 lru 时钟值的计算",charIndex:1210},{level:2,title:"键值对中 LRU 时钟值的初始化与更新",slug:"键值对中-lru-时钟值的初始化与更新",normalizedTitle:"键值对中 lru 时钟值的初始化与更新",charIndex:4327},{level:2,title:"近似 LRU 算法的实际执行",slug:"近似-lru-算法的实际执行",normalizedTitle:"近似 lru 算法的实际执行",charIndex:1360},{level:3,title:"When：什么时候执行",slug:"when-什么时候执行",normalizedTitle:"when：什么时候执行",charIndex:7381},{level:3,title:"How：如何执行",slug:"how-如何执行",normalizedTitle:"how：如何执行",charIndex:7396},{level:4,title:"判断当前内存使用情况",slug:"判断当前内存使用情况",normalizedTitle:"判断当前内存使用情况",charIndex:8701},{level:4,title:"更新待淘汰的候选键值对集合",slug:"更新待淘汰的候选键值对集合",normalizedTitle:"更新待淘汰的候选键值对集合",charIndex:8716},{level:4,title:"选择被淘汰的键值对并删除",slug:"选择被淘汰的键值对并删除",normalizedTitle:"选择被淘汰的键值对并删除",charIndex:8734},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:17247},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:17886}],headersStr:"前言 概述 全局 LRU 时钟值的计算 键值对中 LRU 时钟值的初始化与更新 近似 LRU 算法的实际执行 When：什么时候执行 How：如何执行 判断当前内存使用情况 更新待淘汰的候选键值对集合 选择被淘汰的键值对并删除 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. 为什么严格 LRU 算法在 Redis 中性能开销高？近似 LRU 如何避免问题？\n 2. Redis 如何利用全局 LRU 时钟判断淘汰数据？其优势与局限？\n 3. 为什么近似 LRU 算法中要使用“采样”策略？设计权衡是什么？\n 4. Redis 的 LRU 时钟精度为 1 秒，访问间隔小于 1 秒会影响淘汰准确性吗？\n 5. Redis 何时触发内存淘汰机制？与 Lua 脚本执行状态有何关联？\n 6. Redis 如何找到“最近最少使用”的数据？EvictionPoolLRU 数组设计考量是什么？\n 7. Redis 通过全局 LRU 时钟如何更新键值对的访问时间戳？哪些操作会更新？\n 8. EvictionPoolLRU 数组的固定大小会导致淘汰不准确吗？\n 9. Redis 为什么区分“同步删除”和“异步删除”？各自适用场景是什么？\n\n\n# 前言\n\nLRU，全称 Least Recently Used，最近最少使用，在Redis中语义就是 优先淘汰最近最不常用的数据。\n\n你觉得有哪几种方式可以实现 LRU 淘汰策略？\n\n * 「最直观的想法」：记录下每个 key 最近一次的访问时间 timestamp，timestamp 最小的 Key，就是最近未使用的，然后择时删除。**但是 **首先需要存储每个 Key 和它的 timestamp。其次，还要比较 timestamp 得出最小值。代价很大，不现实。\n\n * 「双链表+HashMap」：echo 的 LRU 算法详解，不记录具体的访问时间点(unix timestamp)，而是记录空闲时间 idle time：idle time 越小，意味着是最近被访问的\n\n你其实可以发现，如果要严格按照 LRU 基本算法「双链表+HashMap」来实现的话，你需要在代码中实现如下内容：\n\n * 要为 Redis 可容纳的所有数据维护一个链表\n * 每当有新数据插入或是现有数据被再次访问时，需要执行多次链表操作\n\nRedis 并没有采用常见的 LRU 实现，而是采用**「近似 LRU 算法」**，听 echo 娓娓道来....\n\n\n# 概述\n\nRedis 的 内存淘汰主要和两个 redis.conf 中的配置有关：\n\n * maxmemory，该配置项设定了 Redis server 可以使用的最大内存容量，一旦 server 使用的实际内存量超出该阈值时，server 就会根据 maxmemory-policy 配置项定义的策略，执行内存淘汰操作\n * maxmemory-policy，该配置项设定了 Redis server 的内存淘汰策略，主要包括近似 LRU 算法、LFU 算法、按 TTL 值淘汰和随机淘汰等几种算法\n\n我们把 Redis 对近似 LRU 算法的实现分成三个部分。\n\n * 全局 LRU 时钟值的计算：这部分包括，Redis 源码为了实现近似 LRU 算法的效果，是 如何计算全局 LRU 时钟值 的，以用来判断数据访问的时效性\n * 键值对 LRU 时钟值的初始化与更新：这部分包括，Redis 源码在哪些函数中对每个键值对对应的 LRU 时钟值，进行初始化与更新\n * 近似 LRU 算法的实际执行：这部分包括，Redis 源码具体如何执行近似 LRU 算法，也就是何时触发数据淘汰，以及实际淘汰的机制是怎么实现的。\n\n上述三部分的整体流程：Redis 在某个时刻去取 全局LRU时钟值 来刷新 键值对的LRU时钟值，然后在某个时刻根据这个时钟值去 淘汰数据\n\n\n# 全局 LRU 时钟值的计算\n\n虽然 Redis 使用了近似 LRU 算法，但是，这个算法仍然需要区分不同数据的访问时效性，也就是说，Redis 需要知道数据的最近一次访问时间。因此，Redis 就设计了 LRU 时钟来记录数据每次访问的时间戳。\n\nRedis 在源码中对于每个键值对中的值，会使用一个 redisObject 结构体来保存指向值的指针。\n\n那么，redisObject 结构体除了记录值的指针以外，还会 使用 24 bits 来保存 LRU 时钟信息，对应的是 LRU 成员变量。所以这样一来，每个键值对都会把它最近一次被访问的时间戳，记录在 LRU 变量当中。\n\ntypedef struct redisObject {\n    unsigned type:4;\n    unsigned encoding:4;\n    unsigned lru:LRU_BITS;  //记录LRU信息，宏定义LRU_BITS是24 bits\n    int refcount;\n    void *ptr;\n} robj;\n\n\n但是，每个键值对的 LRU 时钟值具体是 如何计算 的呢？其实，Redis server 使用了一个实例级别的 全局 LRU 时钟，每个键值对的 LRU 时钟值会根据全局 LRU 时钟进行设置\n\n这个全局 LRU 时钟保存在了 Redis 全局变量 server 的成员变量 lruclock 中。当 Redis server 启动后，调用 initServerConfig 函数初始化各项参数时，就会对这个全局 LRU 时钟 lruclock 进行设置。具体来说，initServerConfig 函数是调用 getLRUClock 函数，来设置 lruclock 的值，如下所示：\n\n// 调用getLRUClock函数计算全局LRU时钟值\nunsigned int lruclock = getLRUClock();\n//设置lruclock为刚计算的LRU时钟值\natomicSet(server.lruclock,lruclock);\n\n\n所以，全局 LRU 时钟值就是通过 getLRUClock 函数计算得到的。\n\ngetLRUClock 函数是在 evict.c 文件中实现的，它会调用 mstime 函数（在 server.c 文件中）获得以毫秒为单位计算的 UNIX 时间戳，然后将这个 UNIX 时间戳除以宏定义 LRU_CLOCK_RESOLUTION。宏定义 LRU_CLOCK_RESOLUTION 是在 server.h 文件中定义的，它表示的是以毫秒为单位的 LRU 时钟精度，也就是以毫秒为单位来表示的 LRU 时钟最小单位。\n\n因为 LRU_CLOCK_RESOLUTION 的默认值是 1000，所以，LRU 时钟精度就是 1000 毫秒，也就是 1 秒。\n\n这样一来，你需要注意的就是，如果一个数据前后两次访问的时间间隔小于 1 秒，那么这两次访问的时间戳就是一样的。因为 LRU 时钟的精度就是 1 秒，它无法区分间隔小于 1 秒的不同时间戳。\n\n了解了宏定义 LRU_CLOCK_RESOLUTION 的含义之后，我们再来看下 getLRUClock 函数中的计算。\n\n 1. 首先，getLRUClock 函数将获得的 UNIX 时间戳，除以 LRU_CLOCK_RESOLUTION 后，就得到了以 LRU 时钟精度来计算的 UNIX 时间戳，也就是当前的 LRU 时钟值。\n 2. 紧接着，getLRUClock 函数会把 LRU 时钟值和宏定义 LRU_CLOCK_MAX 做与运算，其中宏定义 LRU_CLOCK_MAX 表示的是 LRU 时钟能表示的最大值。\n\n/* Return the LRU clock, based on the clock resolution. This is a time\n * in a reduced-bits format that can be used to set and check the\n * object->lru field of redisObject structures. */\nunsigned int getLRUClock(void) {\n    return (mstime()/LRU_CLOCK_RESOLUTION) & LRU_CLOCK_MAX;\n}\n\n\n#define LRU_BITS 24\n#define LRU_CLOCK_MAX ((1<<LRU_BITS)-1) /* Max value of obj->lru */\n#define LRU_CLOCK_RESOLUTION 1000 /* LRU clock resolution in ms */\n\n\n所以现在，你就知道了在默认情况下，全局 LRU 时钟值是 以 1 秒为精度 来计算的 UNIX 时间戳，并且它是在 initServerConfig 函数中进行了初始化。\n\n那么接下来，你可能还会困惑的问题是：在 Redis server 的运行过程中，全局 LRU 时钟值是如何更新的呢？\n\n这就和 Redis server 在事件驱动框架中，定期运行的时间事件所对应的 serverCron 函数有关了\n\nserverCron 函数作为时间事件的回调函数，本身会按照一定的频率周期性执行，其频率值是由 Redis 配置文件 redis.conf 中的 hz 配置项决定的。hz 配置项的默认值是 10，这表示 serverCron 函数会每 100 毫秒（1 秒 /10 = 100 毫秒）运行一次。\n\n这样，在 serverCron 函数中，全局 LRU 时钟值就会按照这个函数的执行频率，定期调用 getLRUClock 函数进行更新，如下所示：\n\nint serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {\n    ...\n   \t//默认情况下，每100毫秒调用getLRUClock函数更新一次全局LRU时钟值\n    unsigned int lruclock = getLRUClock(); \n    \n    //设置lruclock变量\n    atomicSet(server.lruclock,lruclock); \n    ...\n}\n\n\n所以这样一来，每个键值对就可以从全局 LRU 时钟获取最新的访问时间戳了\n\n好，那么接下来，我们就来了解下，对于每个键值对来说，它对应的 redisObject 结构体中的 lru 变量，是在哪些函数中进行初始化和更新的\n\n\n# 键值对中 LRU 时钟值的初始化与更新\n\n首先，对于一个键值对来说，它的 LRU 时钟值最初是在这个键值对被创建的时候，进行初始化设置的，这个初始化操作是在 createObject 函数中调用的。\n\ncreateObject 函数实现在 object.c 文件当中，当 Redis 要创建一个键值对时，就会调用这个函数。\n\nrobj *createObject(int type, void *ptr) {\n    robj *o = zmalloc(sizeof(*o));\n    o->type = type;\n    o->encoding = OBJ_ENCODING_RAW;\n    o->ptr = ptr;\n    o->refcount = 1;\n\n    /* Set the LRU to the current lruclock (minutes resolution), or\n     * alternatively the LFU counter. */\n    if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n        o->lru = (LFUGetTimeInMinutes()<<8) | LFU_INIT_VAL;\n    } else {\n        o->lru = LRU_CLOCK();\n    }\n    return o;\n}\n\n\n而 createObject 函数除了会给 redisObject 结构体分配内存空间之外，它还会根据我刚才提到的 maxmemory_policy 配置项的值，来初始化设置 redisObject 结构体中的 lru 变量。\n\n具体来说，就是如果 maxmemory_policy 配置为使用 LFU 策略，那么 lru 变量值会被初始化设置为 LFU 算法的计算值。而如果 maxmemory_policy 配置项没有使用 LFU 策略，那么，createObject 函数就会调用 LRU_CLOCK 函数来设置 lru 变量的值，也就是键值对对应的 LRU 时钟值。\n\nLRU_CLOCK 函数是在 evict.c 文件中实现的，它的作用就是返回当前的全局 LRU 时钟值。因为一个键值对一旦被创建，也就相当于有了一次访问，所以它对应的 LRU 时钟值就表示了它的访问时间戳。\n\n/* This function is used to obtain the current LRU clock.\n * If the current resolution is lower than the frequency we refresh the\n * LRU clock (as it should be in production servers) we return the\n * precomputed value, otherwise we need to resort to a system call. */\nunsigned int LRU_CLOCK(void) {\n    unsigned int lruclock;\n    if (1000/server.hz <= LRU_CLOCK_RESOLUTION) {\n        atomicGet(server.lruclock,lruclock);\n    } else {\n        lruclock = getLRUClock();\n    }\n    return lruclock;\n}\n\n\n那么到这里，又出现了一个新的问题：一个键值对的 LRU 时钟值又是在什么时候被再次更新的呢？\n\n其实，只要一个键值对被访问了，它的 LRU 时钟值就会被更新。而当一个键值对被访问时，访问操作最终都会调用 lookupKey 函数。\n\nlookupKey 函数是在 db.c 文件中实现的，它会从全局哈希表中查找要访问的键值对。如果该键值对存在，那么 lookupKey 函数就会根据 maxmemory_policy 的配置值，来更新键值对的 LRU 时钟值，也就是它的访问时间戳。\n\n而当 maxmemory_policy 没有配置为 LFU 策略时，lookupKey 函数就会调用 LRU_CLOCK 函数，来获取当前的全局 LRU 时钟值，并将其赋值给键值对的 redisObject 结构体中的 lru 变量，如下所示：\n\n/* Low level key lookup API, not actually called directly from commands\n * implementations that should instead rely on lookupKeyRead(),\n * lookupKeyWrite() and lookupKeyReadWithFlags(). */\nrobj *lookupKey(redisDb *db, robj *key, int flags) {\n    dictEntry *de = dictFind(db->dict,key->ptr);\n    if (de) {\n        // 获取键值对对应的redisObject结构体\n        robj *val = dictGetVal(de);\n\n        /* Update the access time for the ageing algorithm.\n         * Don't do it if we have a saving child, as this will trigger\n         * a copy on write madness. */\n        if (!hasActiveChildProcess() && !(flags & LOOKUP_NOTOUCH)){\n            if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n                // 如果使用了LFU策略，更新LFU计数值\n                updateLFU(val);\n            } else {\n                 // 否则，调用LRU_CLOCK函数获取全局LRU时钟值\n                val->lru = LRU_CLOCK();\n            }\n        }\n        return val;\n    } else {\n        return NULL;\n    }\n}\n\n\n这样一来，每个键值对一旦被访问，就能获得最新的访问时间戳了\n\n不过现在，你可能要问了：这些访问时间戳最终是如何被用于近似 LRU 算法，来进行数据淘汰的呢？接下来，我们就来学习下近似 LRU 算法的实际执行过程\n\n\n# 近似 LRU 算法的实际执行\n\n现在我们已经知道，Redis 之所以实现近似 LRU 算法的目的，是为了减少内存资源和操作时间上的开销。那么在这里，我们其实可以从两个方面来了解近似 LRU 算法的执行过程，分别是\n\n * When：什么时候执行\n * How：如何执行\n\n\n# When：什么时候执行\n\n近似 LRU 算法的主要逻辑是在 freeMemoryIfNeeded 函数中实现的，而这个函数本身是在 evict.c 文件中实现。\n\n首先，近似 LRU 算法的主要逻辑是在 freeMemoryIfNeeded 函数中实现的，而这个函数本身是在 evict.c 文件中实现。\n\nfreeMemoryIfNeeded 函数是被 freeMemoryIfNeededAndSafe 函数（在 evict.c 文件中）调用，而 freeMemoryIfNeededAndSafe 函数又是被 processCommand 函数所调用的。你可以参考下面的图，展示了这三者的调用关系。\n\n\n\n所以，我们看到 processCommand 函数，就应该知道这个函数是 Redis 处理每个命令时都会被调用的。\n\n那么，processCommand 函数在执行的时候，实际上会根据两个条件来判断是否调用 freeMemoryIfNeededAndSafe 函数。\n\n * 条件一：设置了 maxmemory 配置项为非 0 值。\n * 条件二：Lua 脚本没有在超时运行。\n\n如果这两个条件成立，那么 processCommand 函数就会调用 freeMemoryIfNeededAndSafe 函数，如下所示：\n\nif (server.maxmemory && !server.lua_timedout) {\n    \n        int out_of_memory = freeMemoryIfNeededAndSafe() == C_ERR;\n    \n}\n\n\n也就是说，只有在这两个条件都不成立的情况下，freeMemoryIfNeeded 函数才会被调用。下面的代码展示了 freeMemoryIfNeededAndSafe 函数的执行逻辑，你可以看下。\n\n * 条件一：Lua 脚本在超时运行\n * 条件二：Redis server 正在加载数据\n\n也就是说，只有在这两个条件都不成立的情况下，freeMemoryIfNeeded 函数才会被调用。下面的代码展示了 freeMemoryIfNeededAndSafe 函数的执行逻辑，你可以看下。\n\nint freeMemoryIfNeededAndSafe(void) {\n    if (server.lua_timedout || server.loading) return C_OK;\n    return freeMemoryIfNeeded();\n}\n\n\n这样，一旦 freeMemoryIfNeeded 函数被调用了，并且 maxmemory-policy 被设置为了 allkeys-lru 或 volatile-lru，那么近似 LRU 算法就开始被触发执行了。接下来，我们就来看下近似 LRU 算法具体是如何执行的，也就是来了解 freeMemoryIfNeeded 函数的主要执行流程。\n\n\n# How：如何执行\n\n近似 LRU 算法的执行可以分成三大步骤，分别是\n\n 1. 判断当前内存使用情况\n 2. 更新待淘汰的候选键值对集合\n 3. 选择被淘汰的键值对并删除\n\n# 判断当前内存使用情况\n\n * 首先，freeMemoryIfNeeded 函数会调用 getMaxmemoryState 函数，评估当前的内存使用情况。getMaxmemoryState 函数是在 evict.c 文件中实现的，它会判断当前 Redis server 使用的内存容量是否超过了 maxmemory 配置的值。\n * 如果当前内存使用量没有超过 maxmemory，那么，getMaxmemoryState 函数会返回 C_OK，紧接着，freeMemoryIfNeeded 函数也会直接返回了。\n\nint freeMemoryIfNeeded(void) {\n    ...\n    if (getMaxmemoryState(&mem_reported,NULL,&mem_tofree,NULL) == C_OK)\n            return C_OK;\n    ...\n}\n\n\n这里，你需要注意的是，getMaxmemoryState 函数在评估当前内存使用情况的时候，如果发现已用内存超出了 maxmemory，它就会计算需要释放的内存量。这个释放的内存大小等于已使用的内存量减去 maxmemory。不过，已使用的内存量并不包括用于主从复制的复制缓冲区大小，这是 getMaxmemoryState 函数，通过调用 freeMemoryGetNotCountedMemory 函数来计算的。\n\n/* Get the memory status from the point of view of the maxmemory directive:\n * if the memory used is under the maxmemory setting then C_OK is returned.\n * Otherwise, if we are over the memory limit, the function returns\n * C_ERR.\n *\n * The function may return additional info via reference, only if the\n * pointers to the respective arguments is not NULL. Certain fields are\n * populated only when C_ERR is returned:\n *\n *  'total'     total amount of bytes used.\n *              (Populated both for C_ERR and C_OK)\n *\n *  'logical'   the amount of memory used minus the slaves/AOF buffers.\n *              (Populated when C_ERR is returned)\n *\n *  'tofree'    the amount of memory that should be released\n *              in order to return back into the memory limits.\n *              (Populated when C_ERR is returned)\n *\n *  'level'     this usually ranges from 0 to 1, and reports the amount of\n *              memory currently used. May be > 1 if we are over the memory\n *              limit.\n *              (Populated both for C_ERR and C_OK)\n */\nint getMaxmemoryState(size_t *total, size_t *logical, size_t *tofree, float *level) {\n    size_t mem_reported, mem_used, mem_tofree;\n\n    /* Check if we are over the memory usage limit. If we are not, no need\n     * to subtract the slaves output buffers. We can just return ASAP. */\n    // 计算已使用的内存量\n    mem_reported = zmalloc_used_memory();\n    if (total) *total = mem_reported;\n\n    /* We may return ASAP if there is no need to compute the level. */\n    int return_ok_asap = !server.maxmemory || mem_reported <= server.maxmemory;\n    if (return_ok_asap && !level) return C_OK;\n\n    /* Remove the size of slaves output buffers and AOF buffer from the\n     * count of used memory. */\n    // 将用于主从复制的复制缓冲区大小和AOF缓冲区大小从已使用内存量中扣除\n    mem_used = mem_reported;\n    size_t overhead = freeMemoryGetNotCountedMemory();\n    mem_used = (mem_used > overhead) ? mem_used-overhead : 0;\n\n\n    /* Compute the ratio of memory usage. */\n    // 计算内存使用率。\n    if (level) {\n        if (!server.maxmemory) {\n            *level = 0;\n        } else {\n            *level = (float)mem_used / (float)server.maxmemory;\n        }\n    }\n\n    if (return_ok_asap) return C_OK;\n\n    /* Check if we are still over the memory limit. */\n    // 检查我们是否仍然超过内存限制。\n    if (mem_used <= server.maxmemory) return C_OK;\n\n    // 计算需要释放的内存量\n    /* Compute how much memory we need to free. */\n    mem_tofree = mem_used - server.maxmemory;\n\n    if (logical) *logical = mem_used;\n    if (tofree) *tofree = mem_tofree;\n\n    return C_ERR;\n}\n\n\n而如果当前 server 使用的内存量，的确已经超出 maxmemory 的上限了，那么 freeMemoryIfNeeded 函数就会执行一个 while 循环，来淘汰数据释放内存。\n\n其实，为了淘汰数据，Redis 定义了一个数组 EvictionPoolLRU，用来保存待淘汰的候选键值对。这个数组的元素类型是 evictionPoolEntry 结构体，该结构体保存了待淘汰键值对的空闲时间 idle、对应的 key 等信息。以下代码展示了 EvictionPoolLRU 数组和 evictionPoolEntry 结构体，它们都是在 evict.c 文件中定义的。\n\nstruct evictionPoolEntry {\n    // 待淘汰的键值对的空闲时间\n    unsigned long long idle;    \n    // 待淘汰的键值对的key\n    sds key;                    \n    // 缓存的SDS对象\n    sds cached;                 \n    // 待淘汰键值对的key所在的数据库ID\n    int dbid;                   \n};\n\nstatic struct evictionPoolEntry *EvictionPoolLRU;\n\n\n这样，Redis server 在执行 initSever 函数进行初始化时，会调用 evictionPoolAlloc 函数（在 evict.c 文件中）为 EvictionPoolLRU 数组分配内存空间，该数组的大小由宏定义 EVPOOL_SIZE（在 evict.c 文件中）决定，默认是 16 个元素，也就是可以保存 16 个待淘汰的候选键值对。\n\n#define EVPOOL_SIZE 16\n\n/* Create a new eviction pool. */\nvoid evictionPoolAlloc(void) {\n    struct evictionPoolEntry *ep;\n    int j;\n\n    ep = zmalloc(sizeof(*ep)*EVPOOL_SIZE);\n    for (j = 0; j < EVPOOL_SIZE; j++) {\n        ep[j].idle = 0;\n        ep[j].key = NULL;\n        ep[j].cached = sdsnewlen(NULL,EVPOOL_CACHED_SDS_SIZE);\n        ep[j].dbid = 0;\n    }\n    EvictionPoolLRU = ep;\n}\n\n\n那么，freeMemoryIfNeeded 函数在淘汰数据的循环流程中，就会更新这个待淘汰的候选键值对集合，也就是 EvictionPoolLRU 数组。下面我就来给你具体介绍一下。\n\n# 更新待淘汰的候选键值对集合\n\n首先，freeMemoryIfNeeded 函数会调用 evictionPoolPopulate 函数（在 evict.c 文件中），而 evictionPoolPopulate 函数会先调用 dictGetSomeKeys 函数（在 dict.c 文件中），从待采样的哈希表中随机获取一定数量的 key。\n\n不过，这里还有两个地方你需要注意下。\n\n第一点，dictGetSomeKeys 函数采样的哈希表，是由 maxmemory_policy 配置项来决定的。\n\n如果 maxmemory_policy 配置的是 allkeys_lru，那么待采样哈希表就是 Redis server 的全局哈希表，也就是在所有键值对中进行采样；否则，待采样哈希表就是保存着设置了过期时间的 key 的哈希表。\n\n以下代码是 freeMemoryIfNeeded 函数中对 evictionPoolPopulate 函数的调用过程，你可以看下。\n\n/* We don't want to make local-db choices when expiring keys,\n * so to start populate the eviction pool sampling keys from\n * every DB. */\nfor (i = 0; i < server.dbnum; i++) {\n    // 对Redis server上的每一个数据库都执行\n    db = server.db+i;\n    // 根据淘汰策略，决定使用全局哈希表还是设置了过期时间的key的哈希表\n    dict = (server.maxmemory_policy & MAXMEMORY_FLAG_ALLKEYS) ?\n            db->dict : db->expires;\n    // 将选择的哈希表dict传入evictionPoolPopulate函数，同时将全局哈希表也传给evictionPoolPopulate函数\n    if ((keys = dictSize(dict)) != 0) {\n        evictionPoolPopulate(i, dict, db->dict, pool);\n        total_keys += keys;\n    }\n}\n\n\n第二点，dictGetSomeKeys 函数采样的 key 的数量，是由 redis.conf 中的配置项 maxmemory-samples 决定的，该配置项的默认值是 5。下面代码就展示了 evictionPoolPopulate 函数对 dictGetSomeKeys 函数的调用：\n\nvoid evictionPoolPopulate(int dbid, dict *sampledict, dict *keydict, struct evictionPoolEntry *pool) {\n    ...\n    //采样后的集合，大小为maxmemory_samples\n    dictEntry *samples[server.maxmemory_samples]; \n    \n    //将待采样的哈希表sampledict、采样后的集合samples、以及采样数量maxmemory_samples，作为参数传给dictGetSomeKeys\n    count = dictGetSomeKeys(sampledict,samples,server.maxmemory_samples);\n    ...\n}\n\n\n如此一来，dictGetSomeKeys 函数就能返回采样的键值对集合了。然后，evictionPoolPopulate 函数会根据实际采样到的键值对数量 count，执行一个循环。\n\nfor (j = 0; j < count; j++) {\n    ...\n    if (server.maxmemory_policy & MAXMEMORY_FLAG_LRU) {\n    \tidle = estimateObjectIdleTime(o);\n    }\n...\n\n\n紧接着，evictionPoolPopulate 函数会遍历待淘汰的候选键值对集合，也就是 EvictionPoolLRU 数组。在遍历过程中，它会尝试把采样的每一个键值对插入 EvictionPoolLRU 数组，这主要取决于以下两个条件之一：\n\n * 一是，它能在数组中找到一个尚未插入键值对的空位\n * 二是，它能在数组中找到一个空闲时间小于采样键值对空闲时间的键值对\n\n这两个条件有一个成立的话，evictionPoolPopulate 函数就可以把采样键值对插入 EvictionPoolLRU 数组。等所有采样键值对都处理完后，evictionPoolPopulate 函数就完成对待淘汰候选键值对集合的更新了。\n\n接下来，freeMemoryIfNeeded 函数，就可以开始选择最终被淘汰的键值对了。\n\n# 选择被淘汰的键值对并删除\n\n因为 evictionPoolPopulate 函数已经更新了 EvictionPoolLRU 数组，而且这个数组里面的 key，是按照空闲时间从小到大排好序了。所以，freeMemoryIfNeeded 函数会遍历一次 EvictionPoolLRU 数组，从数组的最后一个 key 开始选择，如果选到的 key 不是空值，那么就把它作为最终淘汰的 key。\n\n// 从数组最后一个key开始查找\n/* Go backward from best to worst element to evict. */\nfor (k = EVPOOL_SIZE-1; k >= 0; k--) {\n    // 当前key为空值，则查找下一个key\n    if (pool[k].key == NULL) continue;\n    bestdbid = pool[k].dbid;\n    // 从全局哈希表或是expire哈希表中，获取当前key对应的键值对；并将当前key从EvictionPoolLRU数组删除\n    if (server.maxmemory_policy & MAXMEMORY_FLAG_ALLKEYS) {\n        de = dictFind(server.db[pool[k].dbid].dict,\n            pool[k].key);\n    } else {\n        de = dictFind(server.db[pool[k].dbid].expires,\n            pool[k].key);\n    }\n\n    /* Remove the entry from the pool. */\n    if (pool[k].key != pool[k].cached)\n        sdsfree(pool[k].key);\n    pool[k].key = NULL;\n    pool[k].idle = 0;\n\n    /* If the key exists, is our pick. Otherwise it is\n     * a ghost and we need to try the next element. */\n    // 如果当前key对应的键值对不为空，选择当前key为被淘汰的key\n    if (de) {\n        bestkey = dictGetKey(de);\n        break;\n    } else {\n        //否则，继续查找下个key\n        /* Ghost... Iterate again. */\n    }\n}\n\n\n最后，一旦选到了被淘汰的 key，freeMemoryIfNeeded 函数就会 根据 Redis server 的惰性删除配置，来执行同步删除或异步删除，如下所示：\n\nif (bestkey) {\n    db = server.db+bestdbid;\n    robj *keyobj = createStringObject(bestkey,sdslen(bestkey));        //将删除key的信息传递给从库和AOF文件\n    propagateExpire(db,keyobj,server.lazyfree_lazy_eviction);\n    //如果配置了惰性删除，则进行异步删除\n    if (server.lazyfree_lazy_eviction)\n    \tdbAsyncDelete(db,keyobj);\n    else  //否则进行同步删除\n    \tdbSyncDelete(db,keyobj);\n}\n\n\n好了，到这里，freeMemoryIfNeeded 函数就淘汰了一个 key。而如果此时，释放的内存空间还不够，也就是说没有达到我前面介绍的待释放空间，那么 freeMemoryIfNeeded 函数还会重复执行前面所说的更新待淘汰候选键值对集合、选择最终淘汰 key 的过程，直到满足待释放空间的大小要求。\n\n下图就展示了 freeMemoryIfNeeded 函数涉及的基本流程，你可以再来整体回顾下。\n\n\n\n所以，你会发现\n\n近似 LRU 算法并没有使用耗时耗空间的链表，而是使用了固定大小的待淘汰数据集合，每次随机选择一些 key 加入待淘汰数据集合中。最后，再按照待淘汰集合中 key 的空闲时间长度，删除空闲时间最长的 key。\n\n这样一来，Redis 就近似实现了 LRU 算法的效果了。\n\n\n# 总结\n\n在本文中，我们深入探讨了 Redis 中LRU（Least Recently Used）算法的实现方式，以及其在内存淘汰策略中的应用。\n\n 1. 我们了解了传统LRU算法，尽管它通过维护一个精确的双向链表来记录每个键的访问时间顺序，但在实际大规模应用中会带来严重的性能开销。这种实现需要频繁更新链表，尤其是在高并发情况下，操作成本极高且资源占用过多\n 2. 为了应对这些问题，Redis采用了近似LRU算法，通过全局LRU时钟和随机采样的方式，有效降低了资源消耗。全局时钟以秒为精度，尽管存在微小的时间戳冲突风险，但它大大减少了为每个键记录精确时间的开销。而通过随机采样选择淘汰对象，Redis避免了遍历所有数据带来的性能瓶颈，进一步提高了算法效率。\n 3. 探讨了Redis的内存管理机制，尤其是它何时以及如何触发内存淘汰。通过EvictionPoolLRU数组维护待淘汰键值对的集合，Redis确保可以在较小的时间复杂度内找到空闲时间最长的键。这个固定大小的数组设计避免了复杂的链表操作，实现了在性能和准确性之间的平衡。\n 4. Redis提供了灵活的内存淘汰策略，如同步删除和异步删除，使得在不同使用场景下可以自由选择最适合的策略，进一步优化了内存管理的灵活性。\n 5. Redis通过这种近似LRU策略实现了高效、低成本的内存管理机制，为大规模高并发场景提供了强有力的支持。这一设计展示了在资源受限的环境中，如何在性能与实现成本之间做出合理的权衡与优化\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 为什么严格 lru 算法在 redis 中性能开销高？近似 lru 如何避免问题？\n 2. redis 如何利用全局 lru 时钟判断淘汰数据？其优势与局限？\n 3. 为什么近似 lru 算法中要使用“采样”策略？设计权衡是什么？\n 4. redis 的 lru 时钟精度为 1 秒，访问间隔小于 1 秒会影响淘汰准确性吗？\n 5. redis 何时触发内存淘汰机制？与 lua 脚本执行状态有何关联？\n 6. redis 如何找到“最近最少使用”的数据？evictionpoollru 数组设计考量是什么？\n 7. redis 通过全局 lru 时钟如何更新键值对的访问时间戳？哪些操作会更新？\n 8. evictionpoollru 数组的固定大小会导致淘汰不准确吗？\n 9. redis 为什么区分“同步删除”和“异步删除”？各自适用场景是什么？\n\n\n# 前言\n\nlru，全称 least recently used，最近最少使用，在redis中语义就是 优先淘汰最近最不常用的数据。\n\n你觉得有哪几种方式可以实现 lru 淘汰策略？\n\n * 「最直观的想法」：记录下每个 key 最近一次的访问时间 timestamp，timestamp 最小的 key，就是最近未使用的，然后择时删除。**但是 **首先需要存储每个 key 和它的 timestamp。其次，还要比较 timestamp 得出最小值。代价很大，不现实。\n\n * 「双链表+hashmap」：echo 的 lru 算法详解，不记录具体的访问时间点(unix timestamp)，而是记录空闲时间 idle time：idle time 越小，意味着是最近被访问的\n\n你其实可以发现，如果要严格按照 lru 基本算法「双链表+hashmap」来实现的话，你需要在代码中实现如下内容：\n\n * 要为 redis 可容纳的所有数据维护一个链表\n * 每当有新数据插入或是现有数据被再次访问时，需要执行多次链表操作\n\nredis 并没有采用常见的 lru 实现，而是采用**「近似 lru 算法」**，听 echo 娓娓道来....\n\n\n# 概述\n\nredis 的 内存淘汰主要和两个 redis.conf 中的配置有关：\n\n * maxmemory，该配置项设定了 redis server 可以使用的最大内存容量，一旦 server 使用的实际内存量超出该阈值时，server 就会根据 maxmemory-policy 配置项定义的策略，执行内存淘汰操作\n * maxmemory-policy，该配置项设定了 redis server 的内存淘汰策略，主要包括近似 lru 算法、lfu 算法、按 ttl 值淘汰和随机淘汰等几种算法\n\n我们把 redis 对近似 lru 算法的实现分成三个部分。\n\n * 全局 lru 时钟值的计算：这部分包括，redis 源码为了实现近似 lru 算法的效果，是 如何计算全局 lru 时钟值 的，以用来判断数据访问的时效性\n * 键值对 lru 时钟值的初始化与更新：这部分包括，redis 源码在哪些函数中对每个键值对对应的 lru 时钟值，进行初始化与更新\n * 近似 lru 算法的实际执行：这部分包括，redis 源码具体如何执行近似 lru 算法，也就是何时触发数据淘汰，以及实际淘汰的机制是怎么实现的。\n\n上述三部分的整体流程：redis 在某个时刻去取 全局lru时钟值 来刷新 键值对的lru时钟值，然后在某个时刻根据这个时钟值去 淘汰数据\n\n\n# 全局 lru 时钟值的计算\n\n虽然 redis 使用了近似 lru 算法，但是，这个算法仍然需要区分不同数据的访问时效性，也就是说，redis 需要知道数据的最近一次访问时间。因此，redis 就设计了 lru 时钟来记录数据每次访问的时间戳。\n\nredis 在源码中对于每个键值对中的值，会使用一个 redisobject 结构体来保存指向值的指针。\n\n那么，redisobject 结构体除了记录值的指针以外，还会 使用 24 bits 来保存 lru 时钟信息，对应的是 lru 成员变量。所以这样一来，每个键值对都会把它最近一次被访问的时间戳，记录在 lru 变量当中。\n\ntypedef struct redisobject {\n    unsigned type:4;\n    unsigned encoding:4;\n    unsigned lru:lru_bits;  //记录lru信息，宏定义lru_bits是24 bits\n    int refcount;\n    void *ptr;\n} robj;\n\n\n但是，每个键值对的 lru 时钟值具体是 如何计算 的呢？其实，redis server 使用了一个实例级别的 全局 lru 时钟，每个键值对的 lru 时钟值会根据全局 lru 时钟进行设置\n\n这个全局 lru 时钟保存在了 redis 全局变量 server 的成员变量 lruclock 中。当 redis server 启动后，调用 initserverconfig 函数初始化各项参数时，就会对这个全局 lru 时钟 lruclock 进行设置。具体来说，initserverconfig 函数是调用 getlruclock 函数，来设置 lruclock 的值，如下所示：\n\n// 调用getlruclock函数计算全局lru时钟值\nunsigned int lruclock = getlruclock();\n//设置lruclock为刚计算的lru时钟值\natomicset(server.lruclock,lruclock);\n\n\n所以，全局 lru 时钟值就是通过 getlruclock 函数计算得到的。\n\ngetlruclock 函数是在 evict.c 文件中实现的，它会调用 mstime 函数（在 server.c 文件中）获得以毫秒为单位计算的 unix 时间戳，然后将这个 unix 时间戳除以宏定义 lru_clock_resolution。宏定义 lru_clock_resolution 是在 server.h 文件中定义的，它表示的是以毫秒为单位的 lru 时钟精度，也就是以毫秒为单位来表示的 lru 时钟最小单位。\n\n因为 lru_clock_resolution 的默认值是 1000，所以，lru 时钟精度就是 1000 毫秒，也就是 1 秒。\n\n这样一来，你需要注意的就是，如果一个数据前后两次访问的时间间隔小于 1 秒，那么这两次访问的时间戳就是一样的。因为 lru 时钟的精度就是 1 秒，它无法区分间隔小于 1 秒的不同时间戳。\n\n了解了宏定义 lru_clock_resolution 的含义之后，我们再来看下 getlruclock 函数中的计算。\n\n 1. 首先，getlruclock 函数将获得的 unix 时间戳，除以 lru_clock_resolution 后，就得到了以 lru 时钟精度来计算的 unix 时间戳，也就是当前的 lru 时钟值。\n 2. 紧接着，getlruclock 函数会把 lru 时钟值和宏定义 lru_clock_max 做与运算，其中宏定义 lru_clock_max 表示的是 lru 时钟能表示的最大值。\n\n/* return the lru clock, based on the clock resolution. this is a time\n * in a reduced-bits format that can be used to set and check the\n * object->lru field of redisobject structures. */\nunsigned int getlruclock(void) {\n    return (mstime()/lru_clock_resolution) & lru_clock_max;\n}\n\n\n#define lru_bits 24\n#define lru_clock_max ((1<<lru_bits)-1) /* max value of obj->lru */\n#define lru_clock_resolution 1000 /* lru clock resolution in ms */\n\n\n所以现在，你就知道了在默认情况下，全局 lru 时钟值是 以 1 秒为精度 来计算的 unix 时间戳，并且它是在 initserverconfig 函数中进行了初始化。\n\n那么接下来，你可能还会困惑的问题是：在 redis server 的运行过程中，全局 lru 时钟值是如何更新的呢？\n\n这就和 redis server 在事件驱动框架中，定期运行的时间事件所对应的 servercron 函数有关了\n\nservercron 函数作为时间事件的回调函数，本身会按照一定的频率周期性执行，其频率值是由 redis 配置文件 redis.conf 中的 hz 配置项决定的。hz 配置项的默认值是 10，这表示 servercron 函数会每 100 毫秒（1 秒 /10 = 100 毫秒）运行一次。\n\n这样，在 servercron 函数中，全局 lru 时钟值就会按照这个函数的执行频率，定期调用 getlruclock 函数进行更新，如下所示：\n\nint servercron(struct aeeventloop *eventloop, long long id, void *clientdata) {\n    ...\n   \t//默认情况下，每100毫秒调用getlruclock函数更新一次全局lru时钟值\n    unsigned int lruclock = getlruclock(); \n    \n    //设置lruclock变量\n    atomicset(server.lruclock,lruclock); \n    ...\n}\n\n\n所以这样一来，每个键值对就可以从全局 lru 时钟获取最新的访问时间戳了\n\n好，那么接下来，我们就来了解下，对于每个键值对来说，它对应的 redisobject 结构体中的 lru 变量，是在哪些函数中进行初始化和更新的\n\n\n# 键值对中 lru 时钟值的初始化与更新\n\n首先，对于一个键值对来说，它的 lru 时钟值最初是在这个键值对被创建的时候，进行初始化设置的，这个初始化操作是在 createobject 函数中调用的。\n\ncreateobject 函数实现在 object.c 文件当中，当 redis 要创建一个键值对时，就会调用这个函数。\n\nrobj *createobject(int type, void *ptr) {\n    robj *o = zmalloc(sizeof(*o));\n    o->type = type;\n    o->encoding = obj_encoding_raw;\n    o->ptr = ptr;\n    o->refcount = 1;\n\n    /* set the lru to the current lruclock (minutes resolution), or\n     * alternatively the lfu counter. */\n    if (server.maxmemory_policy & maxmemory_flag_lfu) {\n        o->lru = (lfugettimeinminutes()<<8) | lfu_init_val;\n    } else {\n        o->lru = lru_clock();\n    }\n    return o;\n}\n\n\n而 createobject 函数除了会给 redisobject 结构体分配内存空间之外，它还会根据我刚才提到的 maxmemory_policy 配置项的值，来初始化设置 redisobject 结构体中的 lru 变量。\n\n具体来说，就是如果 maxmemory_policy 配置为使用 lfu 策略，那么 lru 变量值会被初始化设置为 lfu 算法的计算值。而如果 maxmemory_policy 配置项没有使用 lfu 策略，那么，createobject 函数就会调用 lru_clock 函数来设置 lru 变量的值，也就是键值对对应的 lru 时钟值。\n\nlru_clock 函数是在 evict.c 文件中实现的，它的作用就是返回当前的全局 lru 时钟值。因为一个键值对一旦被创建，也就相当于有了一次访问，所以它对应的 lru 时钟值就表示了它的访问时间戳。\n\n/* this function is used to obtain the current lru clock.\n * if the current resolution is lower than the frequency we refresh the\n * lru clock (as it should be in production servers) we return the\n * precomputed value, otherwise we need to resort to a system call. */\nunsigned int lru_clock(void) {\n    unsigned int lruclock;\n    if (1000/server.hz <= lru_clock_resolution) {\n        atomicget(server.lruclock,lruclock);\n    } else {\n        lruclock = getlruclock();\n    }\n    return lruclock;\n}\n\n\n那么到这里，又出现了一个新的问题：一个键值对的 lru 时钟值又是在什么时候被再次更新的呢？\n\n其实，只要一个键值对被访问了，它的 lru 时钟值就会被更新。而当一个键值对被访问时，访问操作最终都会调用 lookupkey 函数。\n\nlookupkey 函数是在 db.c 文件中实现的，它会从全局哈希表中查找要访问的键值对。如果该键值对存在，那么 lookupkey 函数就会根据 maxmemory_policy 的配置值，来更新键值对的 lru 时钟值，也就是它的访问时间戳。\n\n而当 maxmemory_policy 没有配置为 lfu 策略时，lookupkey 函数就会调用 lru_clock 函数，来获取当前的全局 lru 时钟值，并将其赋值给键值对的 redisobject 结构体中的 lru 变量，如下所示：\n\n/* low level key lookup api, not actually called directly from commands\n * implementations that should instead rely on lookupkeyread(),\n * lookupkeywrite() and lookupkeyreadwithflags(). */\nrobj *lookupkey(redisdb *db, robj *key, int flags) {\n    dictentry *de = dictfind(db->dict,key->ptr);\n    if (de) {\n        // 获取键值对对应的redisobject结构体\n        robj *val = dictgetval(de);\n\n        /* update the access time for the ageing algorithm.\n         * don't do it if we have a saving child, as this will trigger\n         * a copy on write madness. */\n        if (!hasactivechildprocess() && !(flags & lookup_notouch)){\n            if (server.maxmemory_policy & maxmemory_flag_lfu) {\n                // 如果使用了lfu策略，更新lfu计数值\n                updatelfu(val);\n            } else {\n                 // 否则，调用lru_clock函数获取全局lru时钟值\n                val->lru = lru_clock();\n            }\n        }\n        return val;\n    } else {\n        return null;\n    }\n}\n\n\n这样一来，每个键值对一旦被访问，就能获得最新的访问时间戳了\n\n不过现在，你可能要问了：这些访问时间戳最终是如何被用于近似 lru 算法，来进行数据淘汰的呢？接下来，我们就来学习下近似 lru 算法的实际执行过程\n\n\n# 近似 lru 算法的实际执行\n\n现在我们已经知道，redis 之所以实现近似 lru 算法的目的，是为了减少内存资源和操作时间上的开销。那么在这里，我们其实可以从两个方面来了解近似 lru 算法的执行过程，分别是\n\n * when：什么时候执行\n * how：如何执行\n\n\n# when：什么时候执行\n\n近似 lru 算法的主要逻辑是在 freememoryifneeded 函数中实现的，而这个函数本身是在 evict.c 文件中实现。\n\n首先，近似 lru 算法的主要逻辑是在 freememoryifneeded 函数中实现的，而这个函数本身是在 evict.c 文件中实现。\n\nfreememoryifneeded 函数是被 freememoryifneededandsafe 函数（在 evict.c 文件中）调用，而 freememoryifneededandsafe 函数又是被 processcommand 函数所调用的。你可以参考下面的图，展示了这三者的调用关系。\n\n\n\n所以，我们看到 processcommand 函数，就应该知道这个函数是 redis 处理每个命令时都会被调用的。\n\n那么，processcommand 函数在执行的时候，实际上会根据两个条件来判断是否调用 freememoryifneededandsafe 函数。\n\n * 条件一：设置了 maxmemory 配置项为非 0 值。\n * 条件二：lua 脚本没有在超时运行。\n\n如果这两个条件成立，那么 processcommand 函数就会调用 freememoryifneededandsafe 函数，如下所示：\n\nif (server.maxmemory && !server.lua_timedout) {\n    \n        int out_of_memory = freememoryifneededandsafe() == c_err;\n    \n}\n\n\n也就是说，只有在这两个条件都不成立的情况下，freememoryifneeded 函数才会被调用。下面的代码展示了 freememoryifneededandsafe 函数的执行逻辑，你可以看下。\n\n * 条件一：lua 脚本在超时运行\n * 条件二：redis server 正在加载数据\n\n也就是说，只有在这两个条件都不成立的情况下，freememoryifneeded 函数才会被调用。下面的代码展示了 freememoryifneededandsafe 函数的执行逻辑，你可以看下。\n\nint freememoryifneededandsafe(void) {\n    if (server.lua_timedout || server.loading) return c_ok;\n    return freememoryifneeded();\n}\n\n\n这样，一旦 freememoryifneeded 函数被调用了，并且 maxmemory-policy 被设置为了 allkeys-lru 或 volatile-lru，那么近似 lru 算法就开始被触发执行了。接下来，我们就来看下近似 lru 算法具体是如何执行的，也就是来了解 freememoryifneeded 函数的主要执行流程。\n\n\n# how：如何执行\n\n近似 lru 算法的执行可以分成三大步骤，分别是\n\n 1. 判断当前内存使用情况\n 2. 更新待淘汰的候选键值对集合\n 3. 选择被淘汰的键值对并删除\n\n# 判断当前内存使用情况\n\n * 首先，freememoryifneeded 函数会调用 getmaxmemorystate 函数，评估当前的内存使用情况。getmaxmemorystate 函数是在 evict.c 文件中实现的，它会判断当前 redis server 使用的内存容量是否超过了 maxmemory 配置的值。\n * 如果当前内存使用量没有超过 maxmemory，那么，getmaxmemorystate 函数会返回 c_ok，紧接着，freememoryifneeded 函数也会直接返回了。\n\nint freememoryifneeded(void) {\n    ...\n    if (getmaxmemorystate(&mem_reported,null,&mem_tofree,null) == c_ok)\n            return c_ok;\n    ...\n}\n\n\n这里，你需要注意的是，getmaxmemorystate 函数在评估当前内存使用情况的时候，如果发现已用内存超出了 maxmemory，它就会计算需要释放的内存量。这个释放的内存大小等于已使用的内存量减去 maxmemory。不过，已使用的内存量并不包括用于主从复制的复制缓冲区大小，这是 getmaxmemorystate 函数，通过调用 freememorygetnotcountedmemory 函数来计算的。\n\n/* get the memory status from the point of view of the maxmemory directive:\n * if the memory used is under the maxmemory setting then c_ok is returned.\n * otherwise, if we are over the memory limit, the function returns\n * c_err.\n *\n * the function may return additional info via reference, only if the\n * pointers to the respective arguments is not null. certain fields are\n * populated only when c_err is returned:\n *\n *  'total'     total amount of bytes used.\n *              (populated both for c_err and c_ok)\n *\n *  'logical'   the amount of memory used minus the slaves/aof buffers.\n *              (populated when c_err is returned)\n *\n *  'tofree'    the amount of memory that should be released\n *              in order to return back into the memory limits.\n *              (populated when c_err is returned)\n *\n *  'level'     this usually ranges from 0 to 1, and reports the amount of\n *              memory currently used. may be > 1 if we are over the memory\n *              limit.\n *              (populated both for c_err and c_ok)\n */\nint getmaxmemorystate(size_t *total, size_t *logical, size_t *tofree, float *level) {\n    size_t mem_reported, mem_used, mem_tofree;\n\n    /* check if we are over the memory usage limit. if we are not, no need\n     * to subtract the slaves output buffers. we can just return asap. */\n    // 计算已使用的内存量\n    mem_reported = zmalloc_used_memory();\n    if (total) *total = mem_reported;\n\n    /* we may return asap if there is no need to compute the level. */\n    int return_ok_asap = !server.maxmemory || mem_reported <= server.maxmemory;\n    if (return_ok_asap && !level) return c_ok;\n\n    /* remove the size of slaves output buffers and aof buffer from the\n     * count of used memory. */\n    // 将用于主从复制的复制缓冲区大小和aof缓冲区大小从已使用内存量中扣除\n    mem_used = mem_reported;\n    size_t overhead = freememorygetnotcountedmemory();\n    mem_used = (mem_used > overhead) ? mem_used-overhead : 0;\n\n\n    /* compute the ratio of memory usage. */\n    // 计算内存使用率。\n    if (level) {\n        if (!server.maxmemory) {\n            *level = 0;\n        } else {\n            *level = (float)mem_used / (float)server.maxmemory;\n        }\n    }\n\n    if (return_ok_asap) return c_ok;\n\n    /* check if we are still over the memory limit. */\n    // 检查我们是否仍然超过内存限制。\n    if (mem_used <= server.maxmemory) return c_ok;\n\n    // 计算需要释放的内存量\n    /* compute how much memory we need to free. */\n    mem_tofree = mem_used - server.maxmemory;\n\n    if (logical) *logical = mem_used;\n    if (tofree) *tofree = mem_tofree;\n\n    return c_err;\n}\n\n\n而如果当前 server 使用的内存量，的确已经超出 maxmemory 的上限了，那么 freememoryifneeded 函数就会执行一个 while 循环，来淘汰数据释放内存。\n\n其实，为了淘汰数据，redis 定义了一个数组 evictionpoollru，用来保存待淘汰的候选键值对。这个数组的元素类型是 evictionpoolentry 结构体，该结构体保存了待淘汰键值对的空闲时间 idle、对应的 key 等信息。以下代码展示了 evictionpoollru 数组和 evictionpoolentry 结构体，它们都是在 evict.c 文件中定义的。\n\nstruct evictionpoolentry {\n    // 待淘汰的键值对的空闲时间\n    unsigned long long idle;    \n    // 待淘汰的键值对的key\n    sds key;                    \n    // 缓存的sds对象\n    sds cached;                 \n    // 待淘汰键值对的key所在的数据库id\n    int dbid;                   \n};\n\nstatic struct evictionpoolentry *evictionpoollru;\n\n\n这样，redis server 在执行 initsever 函数进行初始化时，会调用 evictionpoolalloc 函数（在 evict.c 文件中）为 evictionpoollru 数组分配内存空间，该数组的大小由宏定义 evpool_size（在 evict.c 文件中）决定，默认是 16 个元素，也就是可以保存 16 个待淘汰的候选键值对。\n\n#define evpool_size 16\n\n/* create a new eviction pool. */\nvoid evictionpoolalloc(void) {\n    struct evictionpoolentry *ep;\n    int j;\n\n    ep = zmalloc(sizeof(*ep)*evpool_size);\n    for (j = 0; j < evpool_size; j++) {\n        ep[j].idle = 0;\n        ep[j].key = null;\n        ep[j].cached = sdsnewlen(null,evpool_cached_sds_size);\n        ep[j].dbid = 0;\n    }\n    evictionpoollru = ep;\n}\n\n\n那么，freememoryifneeded 函数在淘汰数据的循环流程中，就会更新这个待淘汰的候选键值对集合，也就是 evictionpoollru 数组。下面我就来给你具体介绍一下。\n\n# 更新待淘汰的候选键值对集合\n\n首先，freememoryifneeded 函数会调用 evictionpoolpopulate 函数（在 evict.c 文件中），而 evictionpoolpopulate 函数会先调用 dictgetsomekeys 函数（在 dict.c 文件中），从待采样的哈希表中随机获取一定数量的 key。\n\n不过，这里还有两个地方你需要注意下。\n\n第一点，dictgetsomekeys 函数采样的哈希表，是由 maxmemory_policy 配置项来决定的。\n\n如果 maxmemory_policy 配置的是 allkeys_lru，那么待采样哈希表就是 redis server 的全局哈希表，也就是在所有键值对中进行采样；否则，待采样哈希表就是保存着设置了过期时间的 key 的哈希表。\n\n以下代码是 freememoryifneeded 函数中对 evictionpoolpopulate 函数的调用过程，你可以看下。\n\n/* we don't want to make local-db choices when expiring keys,\n * so to start populate the eviction pool sampling keys from\n * every db. */\nfor (i = 0; i < server.dbnum; i++) {\n    // 对redis server上的每一个数据库都执行\n    db = server.db+i;\n    // 根据淘汰策略，决定使用全局哈希表还是设置了过期时间的key的哈希表\n    dict = (server.maxmemory_policy & maxmemory_flag_allkeys) ?\n            db->dict : db->expires;\n    // 将选择的哈希表dict传入evictionpoolpopulate函数，同时将全局哈希表也传给evictionpoolpopulate函数\n    if ((keys = dictsize(dict)) != 0) {\n        evictionpoolpopulate(i, dict, db->dict, pool);\n        total_keys += keys;\n    }\n}\n\n\n第二点，dictgetsomekeys 函数采样的 key 的数量，是由 redis.conf 中的配置项 maxmemory-samples 决定的，该配置项的默认值是 5。下面代码就展示了 evictionpoolpopulate 函数对 dictgetsomekeys 函数的调用：\n\nvoid evictionpoolpopulate(int dbid, dict *sampledict, dict *keydict, struct evictionpoolentry *pool) {\n    ...\n    //采样后的集合，大小为maxmemory_samples\n    dictentry *samples[server.maxmemory_samples]; \n    \n    //将待采样的哈希表sampledict、采样后的集合samples、以及采样数量maxmemory_samples，作为参数传给dictgetsomekeys\n    count = dictgetsomekeys(sampledict,samples,server.maxmemory_samples);\n    ...\n}\n\n\n如此一来，dictgetsomekeys 函数就能返回采样的键值对集合了。然后，evictionpoolpopulate 函数会根据实际采样到的键值对数量 count，执行一个循环。\n\nfor (j = 0; j < count; j++) {\n    ...\n    if (server.maxmemory_policy & maxmemory_flag_lru) {\n    \tidle = estimateobjectidletime(o);\n    }\n...\n\n\n紧接着，evictionpoolpopulate 函数会遍历待淘汰的候选键值对集合，也就是 evictionpoollru 数组。在遍历过程中，它会尝试把采样的每一个键值对插入 evictionpoollru 数组，这主要取决于以下两个条件之一：\n\n * 一是，它能在数组中找到一个尚未插入键值对的空位\n * 二是，它能在数组中找到一个空闲时间小于采样键值对空闲时间的键值对\n\n这两个条件有一个成立的话，evictionpoolpopulate 函数就可以把采样键值对插入 evictionpoollru 数组。等所有采样键值对都处理完后，evictionpoolpopulate 函数就完成对待淘汰候选键值对集合的更新了。\n\n接下来，freememoryifneeded 函数，就可以开始选择最终被淘汰的键值对了。\n\n# 选择被淘汰的键值对并删除\n\n因为 evictionpoolpopulate 函数已经更新了 evictionpoollru 数组，而且这个数组里面的 key，是按照空闲时间从小到大排好序了。所以，freememoryifneeded 函数会遍历一次 evictionpoollru 数组，从数组的最后一个 key 开始选择，如果选到的 key 不是空值，那么就把它作为最终淘汰的 key。\n\n// 从数组最后一个key开始查找\n/* go backward from best to worst element to evict. */\nfor (k = evpool_size-1; k >= 0; k--) {\n    // 当前key为空值，则查找下一个key\n    if (pool[k].key == null) continue;\n    bestdbid = pool[k].dbid;\n    // 从全局哈希表或是expire哈希表中，获取当前key对应的键值对；并将当前key从evictionpoollru数组删除\n    if (server.maxmemory_policy & maxmemory_flag_allkeys) {\n        de = dictfind(server.db[pool[k].dbid].dict,\n            pool[k].key);\n    } else {\n        de = dictfind(server.db[pool[k].dbid].expires,\n            pool[k].key);\n    }\n\n    /* remove the entry from the pool. */\n    if (pool[k].key != pool[k].cached)\n        sdsfree(pool[k].key);\n    pool[k].key = null;\n    pool[k].idle = 0;\n\n    /* if the key exists, is our pick. otherwise it is\n     * a ghost and we need to try the next element. */\n    // 如果当前key对应的键值对不为空，选择当前key为被淘汰的key\n    if (de) {\n        bestkey = dictgetkey(de);\n        break;\n    } else {\n        //否则，继续查找下个key\n        /* ghost... iterate again. */\n    }\n}\n\n\n最后，一旦选到了被淘汰的 key，freememoryifneeded 函数就会 根据 redis server 的惰性删除配置，来执行同步删除或异步删除，如下所示：\n\nif (bestkey) {\n    db = server.db+bestdbid;\n    robj *keyobj = createstringobject(bestkey,sdslen(bestkey));        //将删除key的信息传递给从库和aof文件\n    propagateexpire(db,keyobj,server.lazyfree_lazy_eviction);\n    //如果配置了惰性删除，则进行异步删除\n    if (server.lazyfree_lazy_eviction)\n    \tdbasyncdelete(db,keyobj);\n    else  //否则进行同步删除\n    \tdbsyncdelete(db,keyobj);\n}\n\n\n好了，到这里，freememoryifneeded 函数就淘汰了一个 key。而如果此时，释放的内存空间还不够，也就是说没有达到我前面介绍的待释放空间，那么 freememoryifneeded 函数还会重复执行前面所说的更新待淘汰候选键值对集合、选择最终淘汰 key 的过程，直到满足待释放空间的大小要求。\n\n下图就展示了 freememoryifneeded 函数涉及的基本流程，你可以再来整体回顾下。\n\n\n\n所以，你会发现\n\n近似 lru 算法并没有使用耗时耗空间的链表，而是使用了固定大小的待淘汰数据集合，每次随机选择一些 key 加入待淘汰数据集合中。最后，再按照待淘汰集合中 key 的空闲时间长度，删除空闲时间最长的 key。\n\n这样一来，redis 就近似实现了 lru 算法的效果了。\n\n\n# 总结\n\n在本文中，我们深入探讨了 redis 中lru（least recently used）算法的实现方式，以及其在内存淘汰策略中的应用。\n\n 1. 我们了解了传统lru算法，尽管它通过维护一个精确的双向链表来记录每个键的访问时间顺序，但在实际大规模应用中会带来严重的性能开销。这种实现需要频繁更新链表，尤其是在高并发情况下，操作成本极高且资源占用过多\n 2. 为了应对这些问题，redis采用了近似lru算法，通过全局lru时钟和随机采样的方式，有效降低了资源消耗。全局时钟以秒为精度，尽管存在微小的时间戳冲突风险，但它大大减少了为每个键记录精确时间的开销。而通过随机采样选择淘汰对象，redis避免了遍历所有数据带来的性能瓶颈，进一步提高了算法效率。\n 3. 探讨了redis的内存管理机制，尤其是它何时以及如何触发内存淘汰。通过evictionpoollru数组维护待淘汰键值对的集合，redis确保可以在较小的时间复杂度内找到空闲时间最长的键。这个固定大小的数组设计避免了复杂的链表操作，实现了在性能和准确性之间的平衡。\n 4. redis提供了灵活的内存淘汰策略，如同步删除和异步删除，使得在不同使用场景下可以自由选择最适合的策略，进一步优化了内存管理的灵活性。\n 5. redis通过这种近似lru策略实现了高效、低成本的内存管理机制，为大规模高并发场景提供了强有力的支持。这一设计展示了在资源受限的环境中，如何在性能与实现成本之间做出合理的权衡与优化\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"LFU 策略",frontmatter:{title:"LFU 策略",date:"2024-09-15T23:59:53.000Z",permalink:"/pages/b43a89/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E6%94%AF%E7%BA%BF/10.LFU%20%E7%AD%96%E7%95%A5.html",relativePath:"Redis 系统设计/04.支线/10.LFU 策略.md",key:"v-78e6cfd6",path:"/pages/b43a89/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:311},{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:510},{level:2,title:"LFU 算法的实现",slug:"lfu-算法的实现",normalizedTitle:"lfu 算法的实现",charIndex:1011},{level:3,title:"键值对访问频率记录",slug:"键值对访问频率记录",normalizedTitle:"键值对访问频率记录",charIndex:1247},{level:3,title:"键值对访问频率的初始化与更新",slug:"键值对访问频率的初始化与更新",normalizedTitle:"键值对访问频率的初始化与更新",charIndex:1719},{level:4,title:"第一步，衰减访问次数",slug:"第一步-衰减访问次数",normalizedTitle:"第一步，衰减访问次数",charIndex:4252},{level:4,title:"第二步，根据当前访问更新访问次数",slug:"第二步-根据当前访问更新访问次数",normalizedTitle:"第二步，根据当前访问更新访问次数",charIndex:7252},{level:4,title:"第三步，更新 lru 变量值",slug:"第三步-更新-lru-变量值",normalizedTitle:"第三步，更新 lru 变量值",charIndex:9016},{level:3,title:"LFU 算法淘汰数据",slug:"lfu-算法淘汰数据",normalizedTitle:"lfu 算法淘汰数据",charIndex:1279},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:9719},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:11203}],headersStr:"前言 概述 LFU 算法的实现 键值对访问频率记录 键值对访问频率的初始化与更新 第一步，衰减访问次数 第二步，根据当前访问更新访问次数 第三步，更新 lru 变量值 LFU 算法淘汰数据 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. 在高频访问与低频访问的数据中，如何有效决定哪个数据应该优先淘汰？\n 2. 为什么 LFU 算法比 LRU 更适合处理那些偶尔被频繁访问的数据？\n 3. LFU 算法如何在缓存淘汰中既考虑访问频率，又避免单纯的访问次数统计？\n 4. 当缓存容量不足时，Redis 的 LFU 算法是如何动态调整和减少数据误判的？\n 5. 如何在 Redis 中通过 LFU 算法记录访问频率，并避免访问次数无限增长？\n 6. 如何通过调整 LFU 的衰减机制来平衡频繁访问和长时间未访问的数据淘汰？\n 7. Redis 是如何利用 LFU 算法实现缓存淘汰的同时，确保系统性能和内存的高效使用？\n\n\n# 前言\n\nRedis 在 4.0 版本后，还引入了 LFU 算法，也就是，最不频繁使用（Least Frequently Used，LFU）\n\n * LFU 算法在进行数据淘汰时，会把最不频繁访问的数据淘汰掉\n * 而 LRU 算法是把最近最少使用的数据淘汰掉，看起来也是淘汰不频繁访问的数据。\n\nLFU 算法和 LRU 算法的区别到底有哪些呢？我们在实际场景中，需要使用 LFU 算法吗？\n\n\n# 概述\n\n因为 LFU 算法是根据 数据访问的频率 来选择被淘汰数据的，所以 LFU 算法会 记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。\n\n不过，访问次数和访问频率还不能完全等同。\n\n访问频率是指在一定时间内的访问次数，也就是说，在计算访问频率时，我们不仅需要记录访问次数，还要记录这些访问是在多长时间内执行的。否则，如果只记录访问次数的话，就缺少了时间维度的信息，进而就无法按照频率来淘汰数据了\n\n> 我来给你举个例子，假设数据 A 在 15 分钟内访问了 15 次，数据 B 在 5 分钟内访问了 10 次。如果只是按访问次数来统计的话，数据 A 的访问次数大于数据 B，所以淘汰数据时会优先淘汰数据 B。不过，如果按照访问频率来统计的话，数据 A 的访问频率是 1 分钟访问 1 次，而数据 B 的访问频率是 1 分钟访问 2 次，所以按访问频率淘汰数据的话，数据 A 应该被淘汰掉。 所以说，当要实现 LFU 算法时，我们需要能统计到数据的访问频率，而不是简单地记录数据访问次数就行。\n\n那么接下来，我们就来学习下 Redis 是如何实现 LFU 算法的\n\n\n# LFU 算法的实现\n\n首先，LFU 算法的启用，是通过设置 Redis 配置文件 redis.conf 中的 maxmemory 和 maxmemory-policy。\n\n * maxmemory 设置为 Redis 会用的最大内存容量\n * maxmemory-policy 可以设置为 allkeys-lfu 或是 volatile-lfu，表示淘汰的键值对会分别从所有键值对或是设置了过期时间的键值对中筛选\n\nLFU 算法的实现可以分成三部分内容，分别是\n\n 1. 键值对访问频率记录\n 2. 键值对访问频率初始化和更新\n 3. LFU 算法淘汰数据\n\n\n# 键值对访问频率记录\n\n每个键值对的值都对应了一个redisObject结构体，其中有一个 24 bits 的 lru 变量。\n\nlru 变量在 LRU 算法实现时，是用来记录数据的访问时间戳。因为 Redis server 每次运行时，只能将 maxmemory-policy 配置项设置为使用一种淘汰策略，所以，LRU 算法和 LFU 算法并不会同时使用。而为了节省内存开销，Redis 源码就复用了 lru 变量来记录 LFU 算法所需的访问频率信息。\n\n但是如何在LRU对访问时间的记录之上，再记录其访问频率呢？\n\n具体来说，当 lru 变量用来记录 LFU 算法的所需信息时，它会这样使用这珍贵的 24 bits\n\n * 低 8 bits：作为计数器，来记录键值对的访问次数\n * 高 16 bits：记录访问的时间戳\n\n好，了解了 LFU 算法所需的访问频率是如何记录的，接下来，我们再来看下键值对的访问频率是如何初始化和更新的。\n\n\n# 键值对访问频率的初始化与更新\n\n首先，我们要知道，LFU 算法和 LRU 算法的基本步骤，实际上是在相同的入口函数中执行的。围绕 LRU 算法的实现，我们已经了解到这些基本步骤包括数据访问信息的初始化、访问信息更新，以及实际淘汰数据。这些步骤对应的入口函数如下表所示，你也可以再去回顾下内容。\n\n了解了这些入口函数后，我们再去分析 LFU 算法的实现，就容易找到对应的函数了。\n\n对于键值对访问频率的初始化来说，当一个键值对被创建后，createObject 函数就会被调用，用来分配 redisObject 结构体的空间和设置初始化值。如果 Redis 将 maxmemory-policy 设置为 LFU 算法，那么，键值对 redisObject 结构体中的 lru 变量初始化值，会由两部分组成：\n\n * 第一部分是 lru 变量的高 16 位，是以 1 分钟为精度的 UNIX 时间戳。这是通过调用 LFUGetTimeInMinutes 函数（在 evict.c 文件中）计算得到的。\n * 第二部分是 lru 变量的低 8 位，被设置为宏定义 LFU_INIT_VAL（在 server.h 文件中），默认值为 5。\n\n你会发现，这和我刚才给你介绍的键值对访问频率记录是一致的，也就是说，当使用 LFU 算法时，lru 变量包括了键值对的访问时间戳和访问次数。以下代码也展示了这部分的执行逻辑，你可以看下。\n\nrobj *createObject(int type, void *ptr) {\n    robj *o = zmalloc(sizeof(*o));\n    o->type = type;\n    o->encoding = OBJ_ENCODING_RAW;\n    o->ptr = ptr;\n    o->refcount = 1;\n\n    /* Set the LRU to the current lruclock (minutes resolution), or\n     * alternatively the LFU counter. */\n    // 使用LFU算法时，lru变量包括以分钟为精度的UNIX时间戳和访问次数5\n    if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n        o->lru = (LFUGetTimeInMinutes()<<8) | LFU_INIT_VAL;\n    } else {\n        o->lru = LRU_CLOCK();\n    }\n    return o;\n}\n\n\n/* Return the current time in minutes, just taking the least significant\n * 16 bits. The returned time is suitable to be stored as LDT (last decrement\n * time) for the LFU implementation. */\nunsigned long LFUGetTimeInMinutes(void) {\n    return (server.unixtime/60) & 65535;\n}\n\n\n#define LFU_INIT_VAL 5\n\n\n下面，我们再来看下键值对访问频率的更新。\n\n当一个键值对被访问时，Redis 会调用 lookupKey 函数进行查找。当 maxmemory-policy 设置使用 LFU 算法时，lookupKey 函数会调用 updateLFU 函数来更新键值对的访问频率，也就是 lru 变量值，如下所示：\n\n/* Low level key lookup API, not actually called directly from commands\n * implementations that should instead rely on lookupKeyRead(),\n * lookupKeyWrite() and lookupKeyReadWithFlags(). */\nrobj *lookupKey(redisDb *db, robj *key, int flags) {\n    dictEntry *de = dictFind(db->dict,key->ptr);\n    if (de) {\n        robj *val = dictGetVal(de);\n\n        /* Update the access time for the ageing algorithm.\n         * Don't do it if we have a saving child, as this will trigger\n         * a copy on write madness. */\n        if (!hasActiveChildProcess() && !(flags & LOOKUP_NOTOUCH)){\n            // 使用LFU算法时，调用updateLFU函数更新访问频率\n            if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n                updateLFU(val);\n            } else {\n                // 使用LRU算法时，调用LRU_CLOCK\n                val->lru = LRU_CLOCK();\n            }\n        }\n        return val;\n    } else {\n        return NULL;\n    }\n}\n\n\nupdateLFU 函数是在 db.c 文件中实现的，它的执行逻辑比较明确，一共分成三步。\n\n# 第一步，衰减访问次数\n\nupdateLFU 函数首先会调用 LFUDecrAndReturn 函数（在 evict.c 文件中），对键值对的访问次数进行衰减操作，如下所示：\n\n/* Update LFU when an object is accessed.\n * Firstly, decrement the counter if the decrement time is reached.\n * Then logarithmically increment the counter, and update the access time. */\nvoid updateLFU(robj *val) {\n    // 首先，递减计数器\n    unsigned long counter = LFUDecrAndReturn(val);\n    // 然后以logN级别递增计数器，并更新访问次数。\n    counter = LFULogIncr(counter);\n    val->lru = (LFUGetTimeInMinutes()<<8) | counter;\n}\n\n\n看到这里，你可能会有疑问：访问键值对时不是要增加键值对的访问次数吗，为什么要先衰减访问次数呢？\n\n其实，这就是我在前面一开始和你介绍的，LFU 算法是根据访问频率来淘汰数据的，而不只是访问次数。访问频率需要考虑键值对的访问是多长时间段内发生的。键值对的先前访问距离当前时间越长，那么这个键值对的访问频率相应地也就会降低。\n\n我给你举个例子，假设数据 A 在时刻 T 到 T+10 分钟这段时间内，被访问了 30 次，那么，这段时间内数据 A 的访问频率可以计算为 3 次 / 分钟（30 次 /10 分钟 = 3 次 / 分钟）。\n\n紧接着，在 T+10 分钟到 T+20 分钟这段时间内，数据 A 没有再被访问，那么此时，如果我们计算数据 A 在 T 到 T+20 分钟这段时间内的访问频率，它的访问频率就会降为 1.5 次 / 分钟（30 次 /20 分钟 = 1.5 次 / 分钟）。以此类推，随着时间的推移，如果数据 A 在 T+10 分钟后一直没有新的访问，那么它的访问频率就会逐步降低。这就是所谓的访问频率衰减。\n\n因为 Redis 是使用 lru 变量中的访问次数来表示访问频率，所以在每次更新键值对的访问频率时，就会通过 LFUDecrAndReturn 函数对访问次数进行衰减。\n\n具体来说，LFUDecrAndReturn 函数会首先获取当前键值对的上一次访问时间，这是保存在 lru 变量高 16 位上的值。然后，LFUDecrAndReturn 函数会根据全局变量 server 的 lru_decay_time 成员变量的取值，来计算衰减的大小 num_period。\n\n这个计算过程会判断 lfu_decay_time 的值是否为 0。如果 lfu_decay_time 值为 0，那么衰减大小也为 0。此时，访问次数不进行衰减。\n\n否则的话，LFUDecrAndReturn 函数会调用 LFUTimeElapsed 函数（在 evict.c 文件中），计算距离键值对的上一次访问已经过去的时长。这个时长也是以 1 分钟为精度来计算的。有了距离上次访问的时长后，LFUDecrAndReturn 函数会把这个时长除以 lfu_decay_time 的值，并把结果作为访问次数的衰减大小。\n\n这里，你需要注意的是，lfu_decay_time 变量值，是由 redis.conf 文件中的配置项 lfu-decay-time 来决定的。Redis 在初始化时，会通过 initServerConfig 函数来设置 lfu_decay_time 变量的值，默认值为 1。所以，在默认情况下，访问次数的衰减大小就是等于上一次访问距离当前的分钟数。比如，假设上一次访问是 10 分钟前，那么在默认情况下，访问次数的衰减大小就等于 10。\n\n当然，如果上一次访问距离当前的分钟数，已经超过访问次数的值了，那么访问次数就会被设置为 0，这就表示键值对已经很长时间没有被访问了。\n\n下面的代码展示了 LFUDecrAndReturn 函数的执行逻辑，你可以看下。\n\n/* If the object decrement time is reached decrement the LFU counter but\n * do not update LFU fields of the object, we update the access time\n * and counter in an explicit way when the object is really accessed.\n * And we will times halve the counter according to the times of\n * elapsed time than server.lfu_decay_time.\n * Return the object frequency counter.\n *\n * This function is used in order to scan the dataset for the best object\n * to fit: as we check for the candidate, we incrementally decrement the\n * counter of the scanned objects if needed. */\nunsigned long LFUDecrAndReturn(robj *o) {\n    // 获取当前键值对的上一次访问时间，lru右移8位，相当于保留的是前面16位的时间戳\n    unsigned long ldt = o->lru >> 8;\n    // 获取当前的访问次数，相当于后8位与255做与运算，即得到计数器\n    unsigned long counter = o->lru & 255;\n    // 计算衰减大小\n    unsigned long num_periods = server.lfu_decay_time ? LFUTimeElapsed(ldt) / server.lfu_decay_time : 0;\n    // 如果衰减大小不为0\n    if (num_periods)\n        // 如果衰减大小小于当前访问次数，那么，衰减后的访问次数是当前访问次数减去衰减大小；否则，衰减后的访问次数等于0\n        counter = (num_periods > counter) ? 0 : counter - num_periods;\n    // 如果衰减大小为0，则返回原来的访问次数\n    return counter;\n}\n\n\n好了，到这里，updateLFU 函数就通过 LFUDecrAndReturn 函数，完成了键值对访问次数的衰减。紧接着，updateLFU 函数还是会基于键值对当前的这次访问，来更新它的访问次数。\n\n# 第二步，根据当前访问更新访问次数\n\n在这一步中，updateLFU 函数会调用 LFULogIncr 函数，来增加键值对的访问次数，如下所示：\n\n/* Logarithmically increment a counter. The greater is the current counter value\n * the less likely is that it gets really implemented. Saturate it at 255. */\n// 对数递增计数值\n//核心就是访问次数越大，访问次数被递增的可能性越小，最大 255，此外你可以在配置 redis.conf 中写明访问多少次递增多少。\nuint8_t LFULogIncr(uint8_t counter) {\n    // 到最大值了，不能在增加了\n    if (counter == 255) return 255;\n    //    rand()产生一个0-0x7fff的随机数,一个随机数去除以 RAND_MAX也就是Ox7FFF，也就是随机概率\n    double r = (double)rand()/RAND_MAX;\n    // 减去新对象初始化的基数值 (LFU_INIT_VAL 默认是 5)\n    double baseval = counter - LFU_INIT_VAL;\n    // baseval 如果小于零，说明这个对象快不行了，不过本次 incr 将会延长它的寿命\n    if (baseval < 0) baseval = 0;\n    // baseval * LFU 对数计数器因子 + 1保证分母大于1\n    // 当 baseval 特别大时，最大是 (255-5)，p 值会非常小，很难会走到 counter++ 这一步\n    // p 就是 counter 通往 [+1] 权力的门缝，baseval 越大，这个门缝越窄，通过就越艰难\n    double p = 1.0/(baseval*server.lfu_log_factor+1);\n    // 如果随机概率小于当前计算的访问概率，那么访问次数加1\n    if (r < p) counter++;\n    return counter;\n}\n\n\n * 第一个分支对应了当前访问次数等于最大值 255 的情况。此时，LFULogIncr 函数不再增加访问次数。\n\n * 第二个分支对应了当前访问次数小于 255 的情况。此时，LFULogIncr 函数会计算一个阈值 p，以及一个取值为 0 到 1 之间的随机概率值 r。如果概率 r 小于阈值 p，那么 LFULogIncr 函数才会将访问次数加 1。否则的话，LFULogIncr 函数会返回当前的访问次数，不做更新。\n\n从这里你可以看到，因为概率值 r 是随机定的，所以，阈值 p 的大小就决定了访问次数增加的难度。阈值 p 越小，概率值 r 小于 p 的可能性也越小，此时，访问次数也越难增加；相反，如果阈值 p 越大，概率值 r 小于 p 的可能性就越大，访问次数就越容易增加。\n\n而阈值 p 的值大小，其实是由两个因素决定的。一个是当前访问次数和宏定义 LFU_INIT_VAL 的差值 baseval，另一个是 reids.conf 文件中定义的配置项 lfu-log-factor。\n\n当计算阈值 p 时，我们是把 baseval 和 lfu-log-factor 乘积后，加上 1，然后再取其倒数。所以，baseval 或者 lfu-log-factor 越大，那么其倒数就越小，也就是阈值 p 就越小；反之，阈值 p 就越大。也就是说，这里其实就对应了两种影响因素。\n\n * baseval 的大小：这反映了当前访问次数的多少。比如，访问次数越多的键值对，它的访问次数再增加的难度就会越大；(有点类似指数退避算法)\n * lfu-log-factor 的大小：这是可以被设置的。也就是说，Redis 源码提供了让我们人为调节访问次数增加难度的方法。\n\n这样，等到 LFULogIncr 函数执行完成后，键值对的访问次数就算更新完了。\n\n# 第三步，更新 lru 变量值\n\n最后，到这一步，updateLFU 函数已经完成了键值对访问次数的更新。接着，它就会调用 LFUGetTimeInMinutes 函数，来获取当前的时间戳，并和更新后的访问次数组合，形成最新的访问频率信息，赋值给键值对的 lru 变量，如下所示：\n\nvoid updateLFU(robj *val) {\n    ...\n    val->lru = (LFUGetTimeInMinutes()<<8) | counter;\n}\n\n\n好了，到这里，你就了解了，Redis 源码在更新键值对访问频率时，对于访问次数，它是先按照上次访问距离当前的时长，来对访问次数进行衰减。然后，再按照一定概率增加访问次数。这样的设计方法，就既包含了访问的时间段对访问频率的影响，也避免了 8 bits 计数器对访问次数的影响。而对于访问时间来说，Redis 还会获取最新访问时间戳并更新到 lru 变量中\n\n那么最后，我们再来看下 Redis 是如何基于 LFU 算法淘汰数据的\n\n\n# LFU 算法淘汰数据\n\n在实现使用 LFU 算法淘汰数据时，Redis 是采用了和实现近似 LRU 算法相同的方法。也就是说，Redis 会使用一个全局数组 EvictionPoolLRU，来保存待淘汰候选键值对集合。然后，在 processCommand 函数处理每个命令时，它会调用 freeMemoryIfNeededAndSafe 函数和 freeMemoryIfNeeded 函数，来执行具体的数据淘汰流程。\n\n这个淘汰流程我在上篇文章已经给你介绍过了，你可以再去整体回顾下。这里，我也再简要总结下，也就是分成三个步骤：\n\n * 第一步，调用 getMaxmemoryState 函数计算待释放的内存空间；\n * 第二步，调用 evictionPoolPopulate 函数随机采样键值对，并插入到待淘汰集合 EvictionPoolLRU 中；\n * 第三步，遍历待淘汰集合 EvictionPoolLRU，选择实际被淘汰数据，并删除。\n\n虽然这个基本流程和 LRU 算法相同，但是你要注意，LFU 算法在淘汰数据时，在第二步的 evictionPoolPopulate 函数中，使用了不同的方法来计算每个待淘汰键值对的空闲时间\n\n具体来说，在实现 LRU 算法时，待淘汰候选键值对集合 EvictionPoolLRU 中的每个元素，都使用成员变量 idle 来记录它距离上次访问的空闲时间。\n\n而当实现 LFU 算法时，因为 LFU 算法会对访问次数进行衰减和按概率增加，所以，它是使用访问次数来近似表示访问频率的。相应的，LFU 算法其实是用 255 减去键值对的访问次数，这样来计算 EvictionPoolLRU 数组中每个元素的 idle 变量值的。而且，在计算 idle 变量值前，LFU 算法还会调用 LFUDecrAndReturn 函数，衰减一次键值对的访问次数，以便能更加准确地反映实际选择待淘汰数据时，数据的访问频率。\n\n下面的代码展示了 LFU 算法计算 idle 变量值的过程，你可以看下。\n\nif (server.maxmemory_policy & MAXMEMORY_FLAG_LRU) {\n    idle = estimateObjectIdleTime(o);\n} else if (server.maxmemory_policy & MAXMEMORY_FLAG_LFU) {\n    idle = 255-LFUDecrAndReturn(o);\n}\n\n\n所以说，当 LFU 算法按照访问频率，计算了待淘汰键值对集合中每个元素的 idle 值后，键值对访问次数越大，它的 idle 值就越小，反之 idle 值越大。而 EvictionPoolLRU 数组中的元素，是按 idle 值从小到大来排序的。最后当 freeMemoryIfNeeded 函数按照 idle 值从大到小，遍历 EvictionPoolLRU 数组，选择实际被淘汰的键值对时，它就能选出访问次数小的键值对了，也就是把访问频率低的键值对淘汰出去。\n\n这样，Redis 就完成了按访问频率来淘汰数据的操作了。\n\n\n# 总结\n\n 1. LFU 是在 Redis 4.0 新增的淘汰策略，它涉及的巧妙之处在于，其复用了 redisObject 结构的 lru 字段，把这个字段「一分为二」，高 16 位保存最后访问时间和低 8 位保存访问次数\n 2. key 的访问次数不能只增不减，它需要根据时间间隔来做衰减，才能达到 LFU 的目的\n 3. 每次在访问一个 key 时，会**「懒惰」**更新这个 key 的访问次数：先衰减访问次数，再更新访问次数\n 4. 衰减访问次数，会根据时间间隔计算，间隔时间越久，衰减越厉害\n 5. 因为 redisObject lru 字段宽度限制，这个访问次数是有上限的（8 bit 最大值 255），所以递增访问次数时，会根据「当前」访问次数和「概率」的方式做递增，访问次数越大，递增因子越大，递增概率越低\n 6. Redis 实现的 LFU 算法也是**「近似」**LFU，是在性能和内存方面平衡的结果\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. 在高频访问与低频访问的数据中，如何有效决定哪个数据应该优先淘汰？\n 2. 为什么 lfu 算法比 lru 更适合处理那些偶尔被频繁访问的数据？\n 3. lfu 算法如何在缓存淘汰中既考虑访问频率，又避免单纯的访问次数统计？\n 4. 当缓存容量不足时，redis 的 lfu 算法是如何动态调整和减少数据误判的？\n 5. 如何在 redis 中通过 lfu 算法记录访问频率，并避免访问次数无限增长？\n 6. 如何通过调整 lfu 的衰减机制来平衡频繁访问和长时间未访问的数据淘汰？\n 7. redis 是如何利用 lfu 算法实现缓存淘汰的同时，确保系统性能和内存的高效使用？\n\n\n# 前言\n\nredis 在 4.0 版本后，还引入了 lfu 算法，也就是，最不频繁使用（least frequently used，lfu）\n\n * lfu 算法在进行数据淘汰时，会把最不频繁访问的数据淘汰掉\n * 而 lru 算法是把最近最少使用的数据淘汰掉，看起来也是淘汰不频繁访问的数据。\n\nlfu 算法和 lru 算法的区别到底有哪些呢？我们在实际场景中，需要使用 lfu 算法吗？\n\n\n# 概述\n\n因为 lfu 算法是根据 数据访问的频率 来选择被淘汰数据的，所以 lfu 算法会 记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。\n\n不过，访问次数和访问频率还不能完全等同。\n\n访问频率是指在一定时间内的访问次数，也就是说，在计算访问频率时，我们不仅需要记录访问次数，还要记录这些访问是在多长时间内执行的。否则，如果只记录访问次数的话，就缺少了时间维度的信息，进而就无法按照频率来淘汰数据了\n\n> 我来给你举个例子，假设数据 a 在 15 分钟内访问了 15 次，数据 b 在 5 分钟内访问了 10 次。如果只是按访问次数来统计的话，数据 a 的访问次数大于数据 b，所以淘汰数据时会优先淘汰数据 b。不过，如果按照访问频率来统计的话，数据 a 的访问频率是 1 分钟访问 1 次，而数据 b 的访问频率是 1 分钟访问 2 次，所以按访问频率淘汰数据的话，数据 a 应该被淘汰掉。 所以说，当要实现 lfu 算法时，我们需要能统计到数据的访问频率，而不是简单地记录数据访问次数就行。\n\n那么接下来，我们就来学习下 redis 是如何实现 lfu 算法的\n\n\n# lfu 算法的实现\n\n首先，lfu 算法的启用，是通过设置 redis 配置文件 redis.conf 中的 maxmemory 和 maxmemory-policy。\n\n * maxmemory 设置为 redis 会用的最大内存容量\n * maxmemory-policy 可以设置为 allkeys-lfu 或是 volatile-lfu，表示淘汰的键值对会分别从所有键值对或是设置了过期时间的键值对中筛选\n\nlfu 算法的实现可以分成三部分内容，分别是\n\n 1. 键值对访问频率记录\n 2. 键值对访问频率初始化和更新\n 3. lfu 算法淘汰数据\n\n\n# 键值对访问频率记录\n\n每个键值对的值都对应了一个redisobject结构体，其中有一个 24 bits 的 lru 变量。\n\nlru 变量在 lru 算法实现时，是用来记录数据的访问时间戳。因为 redis server 每次运行时，只能将 maxmemory-policy 配置项设置为使用一种淘汰策略，所以，lru 算法和 lfu 算法并不会同时使用。而为了节省内存开销，redis 源码就复用了 lru 变量来记录 lfu 算法所需的访问频率信息。\n\n但是如何在lru对访问时间的记录之上，再记录其访问频率呢？\n\n具体来说，当 lru 变量用来记录 lfu 算法的所需信息时，它会这样使用这珍贵的 24 bits\n\n * 低 8 bits：作为计数器，来记录键值对的访问次数\n * 高 16 bits：记录访问的时间戳\n\n好，了解了 lfu 算法所需的访问频率是如何记录的，接下来，我们再来看下键值对的访问频率是如何初始化和更新的。\n\n\n# 键值对访问频率的初始化与更新\n\n首先，我们要知道，lfu 算法和 lru 算法的基本步骤，实际上是在相同的入口函数中执行的。围绕 lru 算法的实现，我们已经了解到这些基本步骤包括数据访问信息的初始化、访问信息更新，以及实际淘汰数据。这些步骤对应的入口函数如下表所示，你也可以再去回顾下内容。\n\n了解了这些入口函数后，我们再去分析 lfu 算法的实现，就容易找到对应的函数了。\n\n对于键值对访问频率的初始化来说，当一个键值对被创建后，createobject 函数就会被调用，用来分配 redisobject 结构体的空间和设置初始化值。如果 redis 将 maxmemory-policy 设置为 lfu 算法，那么，键值对 redisobject 结构体中的 lru 变量初始化值，会由两部分组成：\n\n * 第一部分是 lru 变量的高 16 位，是以 1 分钟为精度的 unix 时间戳。这是通过调用 lfugettimeinminutes 函数（在 evict.c 文件中）计算得到的。\n * 第二部分是 lru 变量的低 8 位，被设置为宏定义 lfu_init_val（在 server.h 文件中），默认值为 5。\n\n你会发现，这和我刚才给你介绍的键值对访问频率记录是一致的，也就是说，当使用 lfu 算法时，lru 变量包括了键值对的访问时间戳和访问次数。以下代码也展示了这部分的执行逻辑，你可以看下。\n\nrobj *createobject(int type, void *ptr) {\n    robj *o = zmalloc(sizeof(*o));\n    o->type = type;\n    o->encoding = obj_encoding_raw;\n    o->ptr = ptr;\n    o->refcount = 1;\n\n    /* set the lru to the current lruclock (minutes resolution), or\n     * alternatively the lfu counter. */\n    // 使用lfu算法时，lru变量包括以分钟为精度的unix时间戳和访问次数5\n    if (server.maxmemory_policy & maxmemory_flag_lfu) {\n        o->lru = (lfugettimeinminutes()<<8) | lfu_init_val;\n    } else {\n        o->lru = lru_clock();\n    }\n    return o;\n}\n\n\n/* return the current time in minutes, just taking the least significant\n * 16 bits. the returned time is suitable to be stored as ldt (last decrement\n * time) for the lfu implementation. */\nunsigned long lfugettimeinminutes(void) {\n    return (server.unixtime/60) & 65535;\n}\n\n\n#define lfu_init_val 5\n\n\n下面，我们再来看下键值对访问频率的更新。\n\n当一个键值对被访问时，redis 会调用 lookupkey 函数进行查找。当 maxmemory-policy 设置使用 lfu 算法时，lookupkey 函数会调用 updatelfu 函数来更新键值对的访问频率，也就是 lru 变量值，如下所示：\n\n/* low level key lookup api, not actually called directly from commands\n * implementations that should instead rely on lookupkeyread(),\n * lookupkeywrite() and lookupkeyreadwithflags(). */\nrobj *lookupkey(redisdb *db, robj *key, int flags) {\n    dictentry *de = dictfind(db->dict,key->ptr);\n    if (de) {\n        robj *val = dictgetval(de);\n\n        /* update the access time for the ageing algorithm.\n         * don't do it if we have a saving child, as this will trigger\n         * a copy on write madness. */\n        if (!hasactivechildprocess() && !(flags & lookup_notouch)){\n            // 使用lfu算法时，调用updatelfu函数更新访问频率\n            if (server.maxmemory_policy & maxmemory_flag_lfu) {\n                updatelfu(val);\n            } else {\n                // 使用lru算法时，调用lru_clock\n                val->lru = lru_clock();\n            }\n        }\n        return val;\n    } else {\n        return null;\n    }\n}\n\n\nupdatelfu 函数是在 db.c 文件中实现的，它的执行逻辑比较明确，一共分成三步。\n\n# 第一步，衰减访问次数\n\nupdatelfu 函数首先会调用 lfudecrandreturn 函数（在 evict.c 文件中），对键值对的访问次数进行衰减操作，如下所示：\n\n/* update lfu when an object is accessed.\n * firstly, decrement the counter if the decrement time is reached.\n * then logarithmically increment the counter, and update the access time. */\nvoid updatelfu(robj *val) {\n    // 首先，递减计数器\n    unsigned long counter = lfudecrandreturn(val);\n    // 然后以logn级别递增计数器，并更新访问次数。\n    counter = lfulogincr(counter);\n    val->lru = (lfugettimeinminutes()<<8) | counter;\n}\n\n\n看到这里，你可能会有疑问：访问键值对时不是要增加键值对的访问次数吗，为什么要先衰减访问次数呢？\n\n其实，这就是我在前面一开始和你介绍的，lfu 算法是根据访问频率来淘汰数据的，而不只是访问次数。访问频率需要考虑键值对的访问是多长时间段内发生的。键值对的先前访问距离当前时间越长，那么这个键值对的访问频率相应地也就会降低。\n\n我给你举个例子，假设数据 a 在时刻 t 到 t+10 分钟这段时间内，被访问了 30 次，那么，这段时间内数据 a 的访问频率可以计算为 3 次 / 分钟（30 次 /10 分钟 = 3 次 / 分钟）。\n\n紧接着，在 t+10 分钟到 t+20 分钟这段时间内，数据 a 没有再被访问，那么此时，如果我们计算数据 a 在 t 到 t+20 分钟这段时间内的访问频率，它的访问频率就会降为 1.5 次 / 分钟（30 次 /20 分钟 = 1.5 次 / 分钟）。以此类推，随着时间的推移，如果数据 a 在 t+10 分钟后一直没有新的访问，那么它的访问频率就会逐步降低。这就是所谓的访问频率衰减。\n\n因为 redis 是使用 lru 变量中的访问次数来表示访问频率，所以在每次更新键值对的访问频率时，就会通过 lfudecrandreturn 函数对访问次数进行衰减。\n\n具体来说，lfudecrandreturn 函数会首先获取当前键值对的上一次访问时间，这是保存在 lru 变量高 16 位上的值。然后，lfudecrandreturn 函数会根据全局变量 server 的 lru_decay_time 成员变量的取值，来计算衰减的大小 num_period。\n\n这个计算过程会判断 lfu_decay_time 的值是否为 0。如果 lfu_decay_time 值为 0，那么衰减大小也为 0。此时，访问次数不进行衰减。\n\n否则的话，lfudecrandreturn 函数会调用 lfutimeelapsed 函数（在 evict.c 文件中），计算距离键值对的上一次访问已经过去的时长。这个时长也是以 1 分钟为精度来计算的。有了距离上次访问的时长后，lfudecrandreturn 函数会把这个时长除以 lfu_decay_time 的值，并把结果作为访问次数的衰减大小。\n\n这里，你需要注意的是，lfu_decay_time 变量值，是由 redis.conf 文件中的配置项 lfu-decay-time 来决定的。redis 在初始化时，会通过 initserverconfig 函数来设置 lfu_decay_time 变量的值，默认值为 1。所以，在默认情况下，访问次数的衰减大小就是等于上一次访问距离当前的分钟数。比如，假设上一次访问是 10 分钟前，那么在默认情况下，访问次数的衰减大小就等于 10。\n\n当然，如果上一次访问距离当前的分钟数，已经超过访问次数的值了，那么访问次数就会被设置为 0，这就表示键值对已经很长时间没有被访问了。\n\n下面的代码展示了 lfudecrandreturn 函数的执行逻辑，你可以看下。\n\n/* if the object decrement time is reached decrement the lfu counter but\n * do not update lfu fields of the object, we update the access time\n * and counter in an explicit way when the object is really accessed.\n * and we will times halve the counter according to the times of\n * elapsed time than server.lfu_decay_time.\n * return the object frequency counter.\n *\n * this function is used in order to scan the dataset for the best object\n * to fit: as we check for the candidate, we incrementally decrement the\n * counter of the scanned objects if needed. */\nunsigned long lfudecrandreturn(robj *o) {\n    // 获取当前键值对的上一次访问时间，lru右移8位，相当于保留的是前面16位的时间戳\n    unsigned long ldt = o->lru >> 8;\n    // 获取当前的访问次数，相当于后8位与255做与运算，即得到计数器\n    unsigned long counter = o->lru & 255;\n    // 计算衰减大小\n    unsigned long num_periods = server.lfu_decay_time ? lfutimeelapsed(ldt) / server.lfu_decay_time : 0;\n    // 如果衰减大小不为0\n    if (num_periods)\n        // 如果衰减大小小于当前访问次数，那么，衰减后的访问次数是当前访问次数减去衰减大小；否则，衰减后的访问次数等于0\n        counter = (num_periods > counter) ? 0 : counter - num_periods;\n    // 如果衰减大小为0，则返回原来的访问次数\n    return counter;\n}\n\n\n好了，到这里，updatelfu 函数就通过 lfudecrandreturn 函数，完成了键值对访问次数的衰减。紧接着，updatelfu 函数还是会基于键值对当前的这次访问，来更新它的访问次数。\n\n# 第二步，根据当前访问更新访问次数\n\n在这一步中，updatelfu 函数会调用 lfulogincr 函数，来增加键值对的访问次数，如下所示：\n\n/* logarithmically increment a counter. the greater is the current counter value\n * the less likely is that it gets really implemented. saturate it at 255. */\n// 对数递增计数值\n//核心就是访问次数越大，访问次数被递增的可能性越小，最大 255，此外你可以在配置 redis.conf 中写明访问多少次递增多少。\nuint8_t lfulogincr(uint8_t counter) {\n    // 到最大值了，不能在增加了\n    if (counter == 255) return 255;\n    //    rand()产生一个0-0x7fff的随机数,一个随机数去除以 rand_max也就是ox7fff，也就是随机概率\n    double r = (double)rand()/rand_max;\n    // 减去新对象初始化的基数值 (lfu_init_val 默认是 5)\n    double baseval = counter - lfu_init_val;\n    // baseval 如果小于零，说明这个对象快不行了，不过本次 incr 将会延长它的寿命\n    if (baseval < 0) baseval = 0;\n    // baseval * lfu 对数计数器因子 + 1保证分母大于1\n    // 当 baseval 特别大时，最大是 (255-5)，p 值会非常小，很难会走到 counter++ 这一步\n    // p 就是 counter 通往 [+1] 权力的门缝，baseval 越大，这个门缝越窄，通过就越艰难\n    double p = 1.0/(baseval*server.lfu_log_factor+1);\n    // 如果随机概率小于当前计算的访问概率，那么访问次数加1\n    if (r < p) counter++;\n    return counter;\n}\n\n\n * 第一个分支对应了当前访问次数等于最大值 255 的情况。此时，lfulogincr 函数不再增加访问次数。\n\n * 第二个分支对应了当前访问次数小于 255 的情况。此时，lfulogincr 函数会计算一个阈值 p，以及一个取值为 0 到 1 之间的随机概率值 r。如果概率 r 小于阈值 p，那么 lfulogincr 函数才会将访问次数加 1。否则的话，lfulogincr 函数会返回当前的访问次数，不做更新。\n\n从这里你可以看到，因为概率值 r 是随机定的，所以，阈值 p 的大小就决定了访问次数增加的难度。阈值 p 越小，概率值 r 小于 p 的可能性也越小，此时，访问次数也越难增加；相反，如果阈值 p 越大，概率值 r 小于 p 的可能性就越大，访问次数就越容易增加。\n\n而阈值 p 的值大小，其实是由两个因素决定的。一个是当前访问次数和宏定义 lfu_init_val 的差值 baseval，另一个是 reids.conf 文件中定义的配置项 lfu-log-factor。\n\n当计算阈值 p 时，我们是把 baseval 和 lfu-log-factor 乘积后，加上 1，然后再取其倒数。所以，baseval 或者 lfu-log-factor 越大，那么其倒数就越小，也就是阈值 p 就越小；反之，阈值 p 就越大。也就是说，这里其实就对应了两种影响因素。\n\n * baseval 的大小：这反映了当前访问次数的多少。比如，访问次数越多的键值对，它的访问次数再增加的难度就会越大；(有点类似指数退避算法)\n * lfu-log-factor 的大小：这是可以被设置的。也就是说，redis 源码提供了让我们人为调节访问次数增加难度的方法。\n\n这样，等到 lfulogincr 函数执行完成后，键值对的访问次数就算更新完了。\n\n# 第三步，更新 lru 变量值\n\n最后，到这一步，updatelfu 函数已经完成了键值对访问次数的更新。接着，它就会调用 lfugettimeinminutes 函数，来获取当前的时间戳，并和更新后的访问次数组合，形成最新的访问频率信息，赋值给键值对的 lru 变量，如下所示：\n\nvoid updatelfu(robj *val) {\n    ...\n    val->lru = (lfugettimeinminutes()<<8) | counter;\n}\n\n\n好了，到这里，你就了解了，redis 源码在更新键值对访问频率时，对于访问次数，它是先按照上次访问距离当前的时长，来对访问次数进行衰减。然后，再按照一定概率增加访问次数。这样的设计方法，就既包含了访问的时间段对访问频率的影响，也避免了 8 bits 计数器对访问次数的影响。而对于访问时间来说，redis 还会获取最新访问时间戳并更新到 lru 变量中\n\n那么最后，我们再来看下 redis 是如何基于 lfu 算法淘汰数据的\n\n\n# lfu 算法淘汰数据\n\n在实现使用 lfu 算法淘汰数据时，redis 是采用了和实现近似 lru 算法相同的方法。也就是说，redis 会使用一个全局数组 evictionpoollru，来保存待淘汰候选键值对集合。然后，在 processcommand 函数处理每个命令时，它会调用 freememoryifneededandsafe 函数和 freememoryifneeded 函数，来执行具体的数据淘汰流程。\n\n这个淘汰流程我在上篇文章已经给你介绍过了，你可以再去整体回顾下。这里，我也再简要总结下，也就是分成三个步骤：\n\n * 第一步，调用 getmaxmemorystate 函数计算待释放的内存空间；\n * 第二步，调用 evictionpoolpopulate 函数随机采样键值对，并插入到待淘汰集合 evictionpoollru 中；\n * 第三步，遍历待淘汰集合 evictionpoollru，选择实际被淘汰数据，并删除。\n\n虽然这个基本流程和 lru 算法相同，但是你要注意，lfu 算法在淘汰数据时，在第二步的 evictionpoolpopulate 函数中，使用了不同的方法来计算每个待淘汰键值对的空闲时间\n\n具体来说，在实现 lru 算法时，待淘汰候选键值对集合 evictionpoollru 中的每个元素，都使用成员变量 idle 来记录它距离上次访问的空闲时间。\n\n而当实现 lfu 算法时，因为 lfu 算法会对访问次数进行衰减和按概率增加，所以，它是使用访问次数来近似表示访问频率的。相应的，lfu 算法其实是用 255 减去键值对的访问次数，这样来计算 evictionpoollru 数组中每个元素的 idle 变量值的。而且，在计算 idle 变量值前，lfu 算法还会调用 lfudecrandreturn 函数，衰减一次键值对的访问次数，以便能更加准确地反映实际选择待淘汰数据时，数据的访问频率。\n\n下面的代码展示了 lfu 算法计算 idle 变量值的过程，你可以看下。\n\nif (server.maxmemory_policy & maxmemory_flag_lru) {\n    idle = estimateobjectidletime(o);\n} else if (server.maxmemory_policy & maxmemory_flag_lfu) {\n    idle = 255-lfudecrandreturn(o);\n}\n\n\n所以说，当 lfu 算法按照访问频率，计算了待淘汰键值对集合中每个元素的 idle 值后，键值对访问次数越大，它的 idle 值就越小，反之 idle 值越大。而 evictionpoollru 数组中的元素，是按 idle 值从小到大来排序的。最后当 freememoryifneeded 函数按照 idle 值从大到小，遍历 evictionpoollru 数组，选择实际被淘汰的键值对时，它就能选出访问次数小的键值对了，也就是把访问频率低的键值对淘汰出去。\n\n这样，redis 就完成了按访问频率来淘汰数据的操作了。\n\n\n# 总结\n\n 1. lfu 是在 redis 4.0 新增的淘汰策略，它涉及的巧妙之处在于，其复用了 redisobject 结构的 lru 字段，把这个字段「一分为二」，高 16 位保存最后访问时间和低 8 位保存访问次数\n 2. key 的访问次数不能只增不减，它需要根据时间间隔来做衰减，才能达到 lfu 的目的\n 3. 每次在访问一个 key 时，会**「懒惰」**更新这个 key 的访问次数：先衰减访问次数，再更新访问次数\n 4. 衰减访问次数，会根据时间间隔计算，间隔时间越久，衰减越厉害\n 5. 因为 redisobject lru 字段宽度限制，这个访问次数是有上限的（8 bit 最大值 255），所以递增访问次数时，会根据「当前」访问次数和「概率」的方式做递增，访问次数越大，递增因子越大，递增概率越低\n 6. redis 实现的 lfu 算法也是**「近似」**lfu，是在性能和内存方面平衡的结果\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Redis 过期策略",frontmatter:{title:"Redis 过期策略",date:"2024-09-16T03:23:25.000Z",permalink:"/pages/f44fbe/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E6%94%AF%E7%BA%BF/13.Redis%20%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5.html",relativePath:"Redis 系统设计/04.支线/13.Redis 过期策略.md",key:"v-525e0584",path:"/pages/f44fbe/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:242},{level:2,title:"过期键初始化结构",slug:"过期键初始化结构",normalizedTitle:"过期键初始化结构",charIndex:302},{level:2,title:"过期策略",slug:"过期策略",normalizedTitle:"过期策略",charIndex:398},{level:3,title:"惰性删除",slug:"惰性删除",normalizedTitle:"惰性删除",charIndex:53},{level:3,title:"定期删除",slug:"定期删除",normalizedTitle:"定期删除",charIndex:58},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6752},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:6940}],headersStr:"前言 过期键初始化结构 过期策略 惰性删除 定期删除 总结 参考资料",content:"提出问题是一切智慧的开端\n\n 1. Redis 内存接近上限时，过期键未及时清理会带来什么影响？\n 2. 惰性删除和定期删除能确保所有过期键都被清理吗？\n 3. 为什么 Redis 不选定时删除，而用惰性和定期删除的组合？\n 4. 哪些场景下，过期键清理不及时会导致性能问题？如何避免？\n 5. 定期删除随机抽取过期键，这样能防止过期键堆积吗？\n 6. 如果某个键从未被访问，惰性删除是否会让它一直占用内存？\n 7. Redis 高负载时，过期键清理频率会下降吗？如何优化？\n\n\n# 前言\n\n在 Redis 中我们可以给一些元素设置过期时间，那当它过期之后 Redis 是如何处理这些过期键呢？\n\n\n# 过期键初始化结构\n\nRedis 之所以能知道那些键值过期，是因为在 Redis 中维护了一个字典，存储了所有设置了过期时间的键值，我们称之为过期字典。\n\n过期键判断流程如下图所示：\n\n![内存过期策略-过期键判断流程.png](https://learn.lianglianglee.com/专栏/Redis 核心原理与实战/assets/3bf71ae0-5de7-11ea-9e57-957b6467a3fc)\n\n过期键存储在 redisDb 结构中，源代码在 src/server.h 文件中：\n\n/* Redis database representation. There are multiple databases identified\n * by integers from 0 (the default database) up to the max configured\n * database. The database number is the 'id' field in the structure. */\ntypedef struct redisDb {\n    dict *dict;                 /* 数据库键空间，存放着所有的键值对 */\n    dict *expires;              /* 键的过期时间 */\n    dict *blocking_keys;        /* Keys with clients waiting for data (BLPOP)*/\n    dict *ready_keys;           /* Blocked keys that received a PUSH */\n    dict *watched_keys;         /* WATCHED keys for MULTI/EXEC CAS */\n    int id;                     /* Database ID */\n    long long avg_ttl;          /* Average TTL, just for stats */\n    list *defrag_later;         /* List of key names to attempt to defrag one by one, gradually. */\n} redisDb;\n\n\n过期键数据结构如下图所示：\n\n\n\n\n# 过期策略\n\nRedis 会删除已过期的键值，以此来减少 Redis 的空间占用，但因为 Redis 本身是单线的，如果因为删除操作而影响主业务的执行就得不偿失了，为此 Redis 需要制定多个（过期）删除策略来保证糟糕的事情不会发生。\n\n常见的过期策略有以下三种：\n\n * 定时删除：在设置键值过期时间时，创建一个定时事件，当过期时间到达时，由事件处理器自动执行键的删除操作\n * 惰性删除：不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null\n * 定期删除：每隔一段时间检查一次数据库，随机删除一些过期键。\n\nRedis 中采用了 惰性删除+定期删除 策略\n\n> 定时删除虽然 可以保证内存可以被尽快地释放，但是，在 Redis 高负载的情况下或有大量过期键需要同时处理时，会造成 Redis 服务器卡顿，影响主业务执行\n\n\n# 惰性删除\n\n不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null。\n\n * **优点：**因为每次访问时，才会判断过期键，所以此策略只会使用很少的系统资源。\n * **缺点：**系统占用空间删除不及时，导致空间利用率降低，造成了一定的空间浪费。\n\n惰性删除的源码位于 src/db.c 文件的 expireIfNeeded 方法中，源码如下：\n\nint expireIfNeeded(redisDb *db, robj *key) {\n    // 判断键是否过期\n    if (!keyIsExpired(db,key)) return 0;\n    if (server.masterhost != NULL) return 1;\n    /* 删除过期键 */\n    // 增加过期键个数\n    server.stat_expiredkeys++;\n    // 传播键过期的消息\n    propagateExpire(db,key,server.lazyfree_lazy_expire);\n    notifyKeyspaceEvent(NOTIFY_EXPIRED,\n        \"expired\",key,db->id);\n    // server.lazyfree_lazy_expire 为 1 表示异步删除（懒空间释放），反之同步删除\n    return server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) :\n                                         dbSyncDelete(db,key);\n}\n// 判断键是否过期\nint keyIsExpired(redisDb *db, robj *key) {\n    mstime_t when = getExpire(db,key);\n    if (when < 0) return 0; /* No expire for this key */\n    /* Don't expire anything while loading. It will be done later. */\n    if (server.loading) return 0;\n    mstime_t now = server.lua_caller ? server.lua_time_start : mstime();\n    return now > when;\n}\n// 获取键的过期时间\nlong long getExpire(redisDb *db, robj *key) {\n    dictEntry *de;\n    /* No expire? return ASAP */\n    if (dictSize(db->expires) == 0 ||\n       (de = dictFind(db->expires,key->ptr)) == NULL) return -1;\n    /* The entry was found in the expire dict, this means it should also\n     * be present in the main dict (safety check). */\n    serverAssertWithInfo(NULL,key,dictFind(db->dict,key->ptr) != NULL);\n    return dictGetSignedIntegerVal(de);\n}\n\n\n所有对数据库的读写命令在执行之前，都会调用 expireIfNeeded 方法判断键值是否过期，过期则会从数据库中删除，反之则不做任何处理。\n\n惰性删除执行流程，如下图所示：\n\n\n\n\n# 定期删除\n\n每隔一段时间检查一次数据库，随机删除一些过期键。\n\nRedis 默认每秒进行 10 次过期扫描，此配置可通过 Redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10。\n\n注意\n\nRedis 每次扫描并不是遍历过期字典中的所有键，而是采用随机抽取判断并删除过期键的形式执行的。\n\n定期删除流程如下\n\n 1. 从过期字典中随机取出 20 个键；\n 2. 删除这 20 个键中过期的键；\n 3. 如果过期 key 的比例超过 25%，重复步骤 1。\n\n同时为了保证过期扫描不会出现循环过度，导致线程卡死现象，算法还增加了扫描时间的上限，默认不会超过 25ms\n\n\n\n * **优点：**通过限制删除操作的时长和频率，来减少删除操作对 Redis 主业务的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。\n * **缺点：**内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少。\n\n定期删除的核心源码在 src/expire.c 文件下的 activeExpireCycle 方法中，源码如下：\n\nvoid activeExpireCycle(int type) {\n    static unsigned int current_db = 0; /* 上次定期删除遍历到的数据库ID */\n    static int timelimit_exit = 0;      /* Time limit hit in previous call? */\n    static long long last_fast_cycle = 0; /* 上一次执行快速定期删除的时间点 */\n    int j, iteration = 0;\n    int dbs_per_call = CRON_DBS_PER_CALL; // 每次定期删除，遍历的数据库的数量\n    long long start = ustime(), timelimit, elapsed;\n    if (clientsArePaused()) return;\n    if (type == ACTIVE_EXPIRE_CYCLE_FAST) {\n        if (!timelimit_exit) return;\n        // ACTIVE_EXPIRE_CYCLE_FAST_DURATION 是快速定期删除的执行时长\n        if (start < last_fast_cycle + ACTIVE_EXPIRE_CYCLE_FAST_DURATION*2) return;\n        last_fast_cycle = start;\n    }\n    if (dbs_per_call > server.dbnum || timelimit_exit)\n        dbs_per_call = server.dbnum;\n    // 慢速定期删除的执行时长\n    timelimit = 1000000*ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC/server.hz/100;\n    timelimit_exit = 0;\n    if (timelimit <= 0) timelimit = 1;\n    if (type == ACTIVE_EXPIRE_CYCLE_FAST)\n        timelimit = ACTIVE_EXPIRE_CYCLE_FAST_DURATION; /* 删除操作的执行时长 */\n    long total_sampled = 0;\n    long total_expired = 0;\n    for (j = 0; j < dbs_per_call && timelimit_exit == 0; j++) {\n        int expired;\n        redisDb *db = server.db+(current_db % server.dbnum);\n        current_db++;\n        do {\n            // .......\n            expired = 0;\n            ttl_sum = 0;\n            ttl_samples = 0;\n            // 每个数据库中检查的键的数量\n            if (num > ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP)\n                num = ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP;\n            // 从数据库中随机选取 num 个键进行检查\n            while (num--) {\n                dictEntry *de;\n                long long ttl;\n                if ((de = dictGetRandomKey(db->expires)) == NULL) break;\n                ttl = dictGetSignedInteger\n                // 过期检查，并对过期键进行删除\n                if (activeExpireCycleTryExpire(db,de,now)) expired++;\n                if (ttl > 0) {\n                    /* We want the average TTL of keys yet not expired. */\n                    ttl_sum += ttl;\n                    ttl_samples++;\n                }\n                total_sampled++;\n            }\n            total_expired += expired;\n            if (ttl_samples) {\n                long long avg_ttl = ttl_sum/ttl_samples;\n                if (db->avg_ttl == 0) db->avg_ttl = avg_ttl;\n                db->avg_ttl = (db->avg_ttl/50)*49 + (avg_ttl/50);\n            }\n            if ((iteration & 0xf) == 0) { /* check once every 16 iterations. */\n                elapsed = ustime()-start;\n                if (elapsed > timelimit) {\n                    timelimit_exit = 1;\n                    server.stat_expired_time_cap_reached_count++;\n                    break;\n                }\n            }\n            /* 每次检查只删除 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP/4 个过期键 */\n        } while (expired > ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP/4);\n    }\n    // .......\n}\n\n\n\n# 总结\n\n 1. Redis 是通过设置过期字典的形式来判断过期键的\n 2. Redis 采用的是惰性删除和定期删除的形式删除过期键的\n 3. Redis 的定期删除策略并不会遍历删除每个过期键，而是采用随机抽取的方式删除过期键\n 4. 为了保证过期扫描不影响 Redis 主业务，Redis 的定期删除策略中还提供了最大执行时间，以保证 Redis 正常并高效地运行\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码",normalizedContent:"提出问题是一切智慧的开端\n\n 1. redis 内存接近上限时，过期键未及时清理会带来什么影响？\n 2. 惰性删除和定期删除能确保所有过期键都被清理吗？\n 3. 为什么 redis 不选定时删除，而用惰性和定期删除的组合？\n 4. 哪些场景下，过期键清理不及时会导致性能问题？如何避免？\n 5. 定期删除随机抽取过期键，这样能防止过期键堆积吗？\n 6. 如果某个键从未被访问，惰性删除是否会让它一直占用内存？\n 7. redis 高负载时，过期键清理频率会下降吗？如何优化？\n\n\n# 前言\n\n在 redis 中我们可以给一些元素设置过期时间，那当它过期之后 redis 是如何处理这些过期键呢？\n\n\n# 过期键初始化结构\n\nredis 之所以能知道那些键值过期，是因为在 redis 中维护了一个字典，存储了所有设置了过期时间的键值，我们称之为过期字典。\n\n过期键判断流程如下图所示：\n\n![内存过期策略-过期键判断流程.png](https://learn.lianglianglee.com/专栏/redis 核心原理与实战/assets/3bf71ae0-5de7-11ea-9e57-957b6467a3fc)\n\n过期键存储在 redisdb 结构中，源代码在 src/server.h 文件中：\n\n/* redis database representation. there are multiple databases identified\n * by integers from 0 (the default database) up to the max configured\n * database. the database number is the 'id' field in the structure. */\ntypedef struct redisdb {\n    dict *dict;                 /* 数据库键空间，存放着所有的键值对 */\n    dict *expires;              /* 键的过期时间 */\n    dict *blocking_keys;        /* keys with clients waiting for data (blpop)*/\n    dict *ready_keys;           /* blocked keys that received a push */\n    dict *watched_keys;         /* watched keys for multi/exec cas */\n    int id;                     /* database id */\n    long long avg_ttl;          /* average ttl, just for stats */\n    list *defrag_later;         /* list of key names to attempt to defrag one by one, gradually. */\n} redisdb;\n\n\n过期键数据结构如下图所示：\n\n\n\n\n# 过期策略\n\nredis 会删除已过期的键值，以此来减少 redis 的空间占用，但因为 redis 本身是单线的，如果因为删除操作而影响主业务的执行就得不偿失了，为此 redis 需要制定多个（过期）删除策略来保证糟糕的事情不会发生。\n\n常见的过期策略有以下三种：\n\n * 定时删除：在设置键值过期时间时，创建一个定时事件，当过期时间到达时，由事件处理器自动执行键的删除操作\n * 惰性删除：不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null\n * 定期删除：每隔一段时间检查一次数据库，随机删除一些过期键。\n\nredis 中采用了 惰性删除+定期删除 策略\n\n> 定时删除虽然 可以保证内存可以被尽快地释放，但是，在 redis 高负载的情况下或有大量过期键需要同时处理时，会造成 redis 服务器卡顿，影响主业务执行\n\n\n# 惰性删除\n\n不主动删除过期键，每次从数据库获取键值时判断是否过期，如果过期则删除键值，并返回 null。\n\n * **优点：**因为每次访问时，才会判断过期键，所以此策略只会使用很少的系统资源。\n * **缺点：**系统占用空间删除不及时，导致空间利用率降低，造成了一定的空间浪费。\n\n惰性删除的源码位于 src/db.c 文件的 expireifneeded 方法中，源码如下：\n\nint expireifneeded(redisdb *db, robj *key) {\n    // 判断键是否过期\n    if (!keyisexpired(db,key)) return 0;\n    if (server.masterhost != null) return 1;\n    /* 删除过期键 */\n    // 增加过期键个数\n    server.stat_expiredkeys++;\n    // 传播键过期的消息\n    propagateexpire(db,key,server.lazyfree_lazy_expire);\n    notifykeyspaceevent(notify_expired,\n        \"expired\",key,db->id);\n    // server.lazyfree_lazy_expire 为 1 表示异步删除（懒空间释放），反之同步删除\n    return server.lazyfree_lazy_expire ? dbasyncdelete(db,key) :\n                                         dbsyncdelete(db,key);\n}\n// 判断键是否过期\nint keyisexpired(redisdb *db, robj *key) {\n    mstime_t when = getexpire(db,key);\n    if (when < 0) return 0; /* no expire for this key */\n    /* don't expire anything while loading. it will be done later. */\n    if (server.loading) return 0;\n    mstime_t now = server.lua_caller ? server.lua_time_start : mstime();\n    return now > when;\n}\n// 获取键的过期时间\nlong long getexpire(redisdb *db, robj *key) {\n    dictentry *de;\n    /* no expire? return asap */\n    if (dictsize(db->expires) == 0 ||\n       (de = dictfind(db->expires,key->ptr)) == null) return -1;\n    /* the entry was found in the expire dict, this means it should also\n     * be present in the main dict (safety check). */\n    serverassertwithinfo(null,key,dictfind(db->dict,key->ptr) != null);\n    return dictgetsignedintegerval(de);\n}\n\n\n所有对数据库的读写命令在执行之前，都会调用 expireifneeded 方法判断键值是否过期，过期则会从数据库中删除，反之则不做任何处理。\n\n惰性删除执行流程，如下图所示：\n\n\n\n\n# 定期删除\n\n每隔一段时间检查一次数据库，随机删除一些过期键。\n\nredis 默认每秒进行 10 次过期扫描，此配置可通过 redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10。\n\n注意\n\nredis 每次扫描并不是遍历过期字典中的所有键，而是采用随机抽取判断并删除过期键的形式执行的。\n\n定期删除流程如下\n\n 1. 从过期字典中随机取出 20 个键；\n 2. 删除这 20 个键中过期的键；\n 3. 如果过期 key 的比例超过 25%，重复步骤 1。\n\n同时为了保证过期扫描不会出现循环过度，导致线程卡死现象，算法还增加了扫描时间的上限，默认不会超过 25ms\n\n\n\n * **优点：**通过限制删除操作的时长和频率，来减少删除操作对 redis 主业务的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。\n * **缺点：**内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少。\n\n定期删除的核心源码在 src/expire.c 文件下的 activeexpirecycle 方法中，源码如下：\n\nvoid activeexpirecycle(int type) {\n    static unsigned int current_db = 0; /* 上次定期删除遍历到的数据库id */\n    static int timelimit_exit = 0;      /* time limit hit in previous call? */\n    static long long last_fast_cycle = 0; /* 上一次执行快速定期删除的时间点 */\n    int j, iteration = 0;\n    int dbs_per_call = cron_dbs_per_call; // 每次定期删除，遍历的数据库的数量\n    long long start = ustime(), timelimit, elapsed;\n    if (clientsarepaused()) return;\n    if (type == active_expire_cycle_fast) {\n        if (!timelimit_exit) return;\n        // active_expire_cycle_fast_duration 是快速定期删除的执行时长\n        if (start < last_fast_cycle + active_expire_cycle_fast_duration*2) return;\n        last_fast_cycle = start;\n    }\n    if (dbs_per_call > server.dbnum || timelimit_exit)\n        dbs_per_call = server.dbnum;\n    // 慢速定期删除的执行时长\n    timelimit = 1000000*active_expire_cycle_slow_time_perc/server.hz/100;\n    timelimit_exit = 0;\n    if (timelimit <= 0) timelimit = 1;\n    if (type == active_expire_cycle_fast)\n        timelimit = active_expire_cycle_fast_duration; /* 删除操作的执行时长 */\n    long total_sampled = 0;\n    long total_expired = 0;\n    for (j = 0; j < dbs_per_call && timelimit_exit == 0; j++) {\n        int expired;\n        redisdb *db = server.db+(current_db % server.dbnum);\n        current_db++;\n        do {\n            // .......\n            expired = 0;\n            ttl_sum = 0;\n            ttl_samples = 0;\n            // 每个数据库中检查的键的数量\n            if (num > active_expire_cycle_lookups_per_loop)\n                num = active_expire_cycle_lookups_per_loop;\n            // 从数据库中随机选取 num 个键进行检查\n            while (num--) {\n                dictentry *de;\n                long long ttl;\n                if ((de = dictgetrandomkey(db->expires)) == null) break;\n                ttl = dictgetsignedinteger\n                // 过期检查，并对过期键进行删除\n                if (activeexpirecycletryexpire(db,de,now)) expired++;\n                if (ttl > 0) {\n                    /* we want the average ttl of keys yet not expired. */\n                    ttl_sum += ttl;\n                    ttl_samples++;\n                }\n                total_sampled++;\n            }\n            total_expired += expired;\n            if (ttl_samples) {\n                long long avg_ttl = ttl_sum/ttl_samples;\n                if (db->avg_ttl == 0) db->avg_ttl = avg_ttl;\n                db->avg_ttl = (db->avg_ttl/50)*49 + (avg_ttl/50);\n            }\n            if ((iteration & 0xf) == 0) { /* check once every 16 iterations. */\n                elapsed = ustime()-start;\n                if (elapsed > timelimit) {\n                    timelimit_exit = 1;\n                    server.stat_expired_time_cap_reached_count++;\n                    break;\n                }\n            }\n            /* 每次检查只删除 active_expire_cycle_lookups_per_loop/4 个过期键 */\n        } while (expired > active_expire_cycle_lookups_per_loop/4);\n    }\n    // .......\n}\n\n\n\n# 总结\n\n 1. redis 是通过设置过期字典的形式来判断过期键的\n 2. redis 采用的是惰性删除和定期删除的形式删除过期键的\n 3. redis 的定期删除策略并不会遍历删除每个过期键，而是采用随机抽取的方式删除过期键\n 4. 为了保证过期扫描不影响 redis 主业务，redis 的定期删除策略中还提供了最大执行时间，以保证 redis 正常并高效地运行\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码",charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"RDB 持久化",frontmatter:{title:"RDB 持久化",date:"2024-09-16T03:23:41.000Z",permalink:"/pages/9b17a6/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E6%94%AF%E7%BA%BF/15.RDB%20%E6%8C%81%E4%B9%85%E5%8C%96.html",relativePath:"Redis 系统设计/04.支线/15.RDB 持久化.md",key:"v-67439d1c",path:"/pages/9b17a6/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:285},{level:2,title:"RDB 创建的入口函数和触发时机",slug:"rdb-创建的入口函数和触发时机",normalizedTitle:"rdb 创建的入口函数和触发时机",charIndex:369},{level:2,title:"RDB 文件是如何生成的",slug:"rdb-文件是如何生成的",normalizedTitle:"rdb 文件是如何生成的",charIndex:3241},{level:3,title:"生成文件头",slug:"生成文件头",normalizedTitle:"生成文件头",charIndex:4870},{level:3,title:"生成文件数据部分",slug:"生成文件数据部分",normalizedTitle:"生成文件数据部分",charIndex:8125},{level:3,title:"生成文件尾",slug:"生成文件尾",normalizedTitle:"生成文件尾",charIndex:11525},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:11973},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:12733}],headersStr:"前言 RDB 创建的入口函数和触发时机 RDB 文件是如何生成的 生成文件头 生成文件数据部分 生成文件尾 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. Redis 是如何通过 RDB 文件来实现数据持久化的？其背后有哪些关键过程？\n 2. 除了 save 和 bgsave 命令，Redis 在什么场景下还会创建 RDB 文件？\n 3. RDB 文件中，为什么需要为不同内容设定操作码？这些操作码如何帮助解析数据？\n 4. rdbSaveRio 函数生成 RDB 文件时，如何保证数据的完整性和效率？\n 5. RDB 文件为什么要自包含所有键值对的类型、长度和数据？这对恢复数据有什么好处？\n 6. 如何通过解析 RDB 文件来发现 Redis 中的大 key，从而优化内存？\n\n\n# 前言\n\n我们来到了一个新的模块「可靠性保证模块」\n\n我们就先从RDB文件的生成开始学起。下面呢，我先带你来了解下RDB创建的入口函数，以及调用这些函数的地方。\n\n\n# RDB 创建的入口函数和触发时机\n\nRedis 源码中用来创建 RDB 文件的函数有三个，它们都是在rdb.c文件中实现的，接下来我就带你具体了解下。\n\n * rdbSave 函数\n\n这是 Redis server 在本地磁盘创建 RDB 文件的入口函数。它对应了 Redis 的 save 命令，会在 save 命令的实现函数 saveCommand（在 rdb.c 文件中）中被调用。而 rdbSave 函数最终会调用 rdbSaveRio 函数（在 rdb.c 文件中）来实际创建 RDB 文件。rdbSaveRio 函数的执行逻辑就体现了 RDB 文件的格式和生成过程，我稍后向你介绍\n\n * rdbSaveBackground 函数\n\n这是 Redis server 使用后台子进程方式，在本地磁盘创建 RDB 文件的入口函数。它对应了 Redis 的 bgsave 命令，会在 bgsave 命令的实现函数 bgsaveCommand（在 rdb.c 文件中）中被调用。这个函数会调用 fork 创建一个子进程，让子进程调用 rdbSave 函数来继续创建 RDB 文件，而父进程，也就是主线程本身可以继续处理客户端请求。\n\n下面的代码展示了 rdbSaveBackground 函数创建子进程的过程，你可以看下\n\nint rdbSaveBackground(char *filename, rdbSaveInfo *rsi) {\n    ...\n    if ((childpid = fork()) == 0) {  //子进程的代码执行分支\n       ...\n       retval = rdbSave(filename,rsi);  //调用rdbSave函数创建RDB文件\n       ...\n       exitFromChild((retval == C_OK) ? 0 : 1);  //子进程退出\n    } else {\n       ...  //父进程代码执行分支\n    }\n}\n\n\n * rdbSaveToSlavesSockets 函数\n\n这是 Redis server 在采用不落盘方式传输 RDB 文件进行主从复制时，创建 RDB 文件的入口函数。它会被 startBgsaveForReplication 函数调用（在replication.c文件中）。而 startBgsaveForReplication 函数会被 replication.c 文件中的 syncCommand 函数和 replicationCron 函数调用，这对应了 Redis server 执行主从复制命令，以及周期性检测主从复制状态时触发 RDB 生成。\n\n和 rdbSaveBackground 函数类似，rdbSaveToSlavesSockets 函数也是通过 fork 创建子进程，让子进程生成 RDB。不过和 rdbSaveBackground 函数不同的是，rdbSaveToSlavesSockets 函数是通过网络以字节流的形式，直接发送 RDB 文件的二进制数据给从节点。\n\n而为了让从节点能够识别用来同步数据的 RDB 内容，rdbSaveToSlavesSockets 函数调用 rdbSaveRioWithEOFMark 函数（在 rdb.c 文件中），在 RDB 二进制数据的前后加上了标识字符串，如下图所示：\n\n\n\n以下代码也展示了rdbSaveRioWithEOFMark函数的基本执行逻辑。你可以看到，它除了写入前后标识字符串之外，还是会调用rdbSaveRio函数实际生成RDB内容。\n\nint rdbSaveRioWithEOFMark(rio *rdb, int *error, rdbSaveInfo *rsi) {\n    ...\n    getRandomHexChars(eofmark,RDB_EOF_MARK_SIZE); //随机生成40字节的16进制字符串，保存在eofmark中，宏定义RDB_EOF_MARK_SIZE的值为40\n    if (rioWrite(rdb,"$EOF:",5) == 0) goto werr;  //写入$EOF\n    if (rioWrite(rdb,eofmark,RDB_EOF_MARK_SIZE) == 0) goto werr; //写入40字节的16进制字符串eofmark\n    if (rioWrite(rdb,"\\r\\n",2) == 0) goto werr; //写入\\r\\n\n    if (rdbSaveRio(rdb,error,RDB_SAVE_NONE,rsi) == C_ERR) goto werr; //生成RDB内容\n    if (rioWrite(rdb,eofmark,RDB_EOF_MARK_SIZE) == 0) goto werr; //再次写入40字节的16进制字符串eofmark\n    ...\n}\n\n\n好了，了解了 RDB 文件创建的三个入口函数后，我们也看到了，RDB 文件创建的三个时机，分别是 save 命令执行、bgsave 命令执行以及主从复制。那么，除了这三个时机外，在 Redis 源码中，还有哪些地方会触发 RDB 文件创建呢？\n\n实际上，因为 rdbSaveToSlavesSockets 函数只会在主从复制时调用，所以，我们只要通过在 Redis 源码中查找 rdbSave、rdbSaveBackground 这两个函数，就可以了解触发 RDB 文件创建的其他时机。\n\n那么经过查找，我们可以发现在 Redis 源码中，rdbSave 还会在 flushallCommand 函数（在db.c文件中）、prepareForShutdown 函数（在server.c文件中）中被调用。这也就是说，Redis 在执行 flushall 命令以及正常关闭时，会创建 RDB 文件。\n\n对于 rdbSaveBackground 函数来说，它除了在执行 bgsave 命令时被调用，当主从复制采用落盘文件方式传输 RDB 时，它也会被 startBgsaveForReplication 函数调用。此外，Redis server 运行时的周期性执行函数 serverCron（在server.c文件中），也会调用 rdbSaveBackground 函数来创建 RDB 文件。\n\n为了便于你掌握 RDB 文件创建的整体情况，我画了下面这张图，展示了 Redis 源码中创建 RDB 文件的函数调用关系，你可以看下。\n\n\n\n好了，到这里，你可以看到，实际最终生成 RDB 文件的函数是 rdbSaveRio。所以接下来，我们就来看看 rdbSaveRio 函数的执行过程。同时，我还会给你介绍 RDB 文件的格式是如何组织的\n\n\n# RDB 文件是如何生成的\n\n不过在了解 rdbSaveRio 函数具体是如何生成 RDB 文件之前，你还需要先了解下 RDB 文件的基本组成部分。这样，你就可以按照 RDB 文件的组成部分，依次了解 rdbSaveRio 函数的执行逻辑了。\n\n那么，一个 RDB 文件主要是由三个部分组成的。\n\n * 文件头：这部分内容保存了 Redis 的魔数、RDB 版本、Redis 版本、RDB 文件创建时间、键值对占用的内存大小等信息。\n * 文件数据部分：这部分保存了 Redis 数据库实际的所有键值对。\n * 文件尾：这部分保存了 RDB 文件的结束标识符，以及整个文件的校验值。这个校验值用来在 Redis server 加载 RDB 文件后，检查文件是否被篡改过。\n\n下图就展示了 RDB 文件的组成，你可以看下。\n\n\n\n好，接下来，我们就来看看 rdbSaveRio 函数是如何生成 RDB 文件中的每一部分的。这里，为了方便你理解 RDB 文件格式以及文件内容，你可以先按照如下步骤准备一个 RDB 文件。\n\n第一步，在你电脑上 Redis 的目录下，启动一个用来测试的 Redis server，可以执行如下命令：\n\n好，接下来，我们就来看看 rdbSaveRio 函数是如何生成 RDB 文件中的每一部分的。这里，为了方便你理解 RDB 文件格式以及文件内容，你可以先按照如下步骤准备一个 RDB 文件。\n\n第一步，在你电脑上 Redis 的目录下，启动一个用来测试的 Redis server，可以执行如下命令：\n\n./redis-server\n\n\n第二步，执行 flushall 命令，清空当前的数据库：\n\n./redis-cli flushall\n\n\n第三步，使用 redis-cli 登录刚启动的 Redis server，执行 set 命令插入一个 String 类型的键值对，再执行 hmset 命令插入一个 Hash 类型的键值对。执行 save 命令，将当前数据库内容保存到 RDB 文件中。这个过程如下所示：\n\n127.0.0.1:6379>set hello redis\nOK\n127.0.0.1:6379>hmset userinfo uid 1 name zs age 32\nOK\n127.0.0.1:6379> save\nOK\n\n\n好了，到这里，你就可以在刚才执行 redis-cli 命令的目录下，找见刚生成的 RDB 文件，文件名应该是 dump.rdb。\n\n不过，因为 RDB 文件实际是一个二进制数据组成的文件，所以如果你使用一般的文本编辑软件，比如 Linux 系统上的 Vim，在打开 RDB 文件时，你会看到文件中都是乱码。所以这里，我给你提供一个小工具，如果你想查看 RDB 文件中二进制数据和对应的 ASCII 字符，你可以使用 Linux 上的 od 命令，这个命令可以用不同进制的方式展示数据，并显示对应的 ASCII 字符。\n\n比如，你可以执行如下的命令，读取 dump.rdb 文件，并用十六进制展示文件内容，同时文件中每个字节对应的 ASCII 字符也会被对应显示出来。\n\nod -A x -t x1c -v dump.rdb\n\n\n以下代码展示的就是我用 od 命令，查看刚才生成的 dump.rdb 文件后，输出的从文件头开始的部分内容。你可以看到这四行结果中，第一和第三行是用十六进制显示的 dump.rdb 文件的字节内容，这里每两个十六进制数对应了一个字节。而第二和第四行是 od 命令生成的每个字节所对应的 ASCII 字符。\n\n\n\n这也就是说，在刚才生成的 RDB 文件中，如果想要转换成 ASCII 字符，它的文件头内容其实就已经包含了 REDIS 的字符串和一些数字，而这正是 RDB 文件头包含的内容。\n\n那么下面，我们就来看看 RDB 文件的文件头是如何生成的。\n\n\n# 生成文件头\n\n就像刚才给你介绍的，RDB 文件头的内容首先是魔数，这对应记录了 RDB 文件的版本。在 rdbSaveRio 函数中，魔数是通过 snprintf 函数生成的，它的具体内容是字符串“REDIS”，再加上 RDB 版本的宏定义 RDB_VERSION（在rdb.h文件中，值为 9）。然后，rdbSaveRio 函数会调用 rdbWriteRaw 函数（在 rdb.c 文件中），将魔数写入 RDB 文件，如下所示：\n\nsnprintf(magic,sizeof(magic),"REDIS%04d",RDB_VERSION);  //生成魔数magic\nif (rdbWriteRaw(rdb,magic,9) == -1) goto werr;  //将magic写入RDB文件\n\n\n刚才用来写入魔数的 rdbWriteRaw 函数，它实际会调用 rioWrite 函数（在 rdb.h 文件中）来完成写入。而 rioWrite 函数是 RDB 文件内容的最终写入函数，它负责根据要写入数据的长度，把待写入缓冲区中的内容写入 RDB。这里，你需要注意的是，RDB 文件生成过程中，会有不同的函数负责写入不同部分的内容，不过这些函数最终都还是调用 rioWrite 函数，来完成数据的实际写入的。\n\n好了，当在 RDB 文件头中写入魔数后，rdbSaveRio 函数紧接着会调用 rdbSaveInfoAuxFields 函数，将和 Redis server 相关的一些属性信息写入 RDB 文件头，如下所示：\n\nif (rdbSaveInfoAuxFields(rdb,flags,rsi) == -1) goto werr; //写入属性信息\n\n\nrdbSaveInfoAuxFields 函数是在 rdb.c 文件中实现的，它会使用键值对的形式，在 RDB 文件头中记录 Redis server 的属性信息。下表中列出了 RDB 文件头记录的一些主要信息，以及它们对应的键和值，你可以看下。\n\n\n\n那么，当属性值为字符串时，rdbSaveInfoAuxFields 函数会调用 rdbSaveAuxFieldStrStr 函数写入属性信息；而当属性值为整数时，rdbSaveInfoAuxFields 函数会调用 rdbSaveAuxFieldStrInt 函数写入属性信息，如下所示：\n\nif (rdbSaveAuxFieldStrStr(rdb,"redis-ver",REDIS_VERSION) == -1) return -1;\nif (rdbSaveAuxFieldStrInt(rdb,"redis-bits",redis_bits) == -1) return -1;\nif (rdbSaveAuxFieldStrInt(rdb,"ctime",time(NULL)) == -1) return -1;\nif (rdbSaveAuxFieldStrInt(rdb,"used-mem",zmalloc_used_memory()) == -1) return -1;\n\n\n这里，无论是 rdbSaveAuxFieldStrStr 函数还是 rdbSaveAuxFieldStrInt 函数，它们都会调用 rdbSaveAuxField 函数来写入属性值。rdbSaveAuxField 函数是在 rdb.c 文件中实现的，它会分三步来完成一个属性信息的写入。\n\n第一步，它调用 rdbSaveType 函数写入一个操作码。这个操作码的目的，是用来在 RDB 文件中标识接下来的内容是什么。当写入属性信息时，这个操作码对应了宏定义 RDB_OPCODE_AUX（在 rdb.h 文件中），值为 250，对应的十六进制值为 FA。这样一来，就方便我们解析 RDB 文件了。比如，在读取 RDB 文件时，如果程序读取到 FA 这个字节，那么，这就表明接下来的内容是一个属性信息。\n\n这里，你需要注意的是，RDB 文件使用了多个操作码，来标识文件中的不同内容。它们都是在 rdb.h 文件中定义的，下面的代码中展示了部分操作码，你可以看下。\n\n#define RDB_OPCODE_IDLE       248   //标识LRU空闲时间\n#define RDB_OPCODE_FREQ       249   //标识LFU访问频率信息\n#define RDB_OPCODE_AUX        250   //标识RDB文件头的属性信息\n#define RDB_OPCODE_EXPIRETIME_MS 252    //标识以毫秒记录的过期时间\n#define RDB_OPCODE_SELECTDB   254   //标识文件中后续键值对所属的数据库编号\n#define RDB_OPCODE_EOF        255   //标识RDB文件结束，用在文件尾\n\n\n第二步，rdbSaveAuxField 函数调用 rdbSaveRawString 函数（在 rdb.c 文件中）写入属性信息的键，而键通常是一个字符串。rdbSaveRawString 函数是用来写入字符串的通用函数，它会先记录字符串长度，然后再记录实际字符串，如下图所示。这个长度信息是为了解析 RDB 文件时，程序可以基于它知道当前读取的字符串应该读取多少个字节。\n\n\n\n不过，为了节省 RDB 文件消耗的空间，如果字符串中记录的实际是一个整数，rdbSaveRawString 函数还会调用 rdbTryIntegerEncoding 函数（在 rdb.c 文件中），尝试用紧凑结构对字符串进行编码。具体做法你可以进一步阅读 rdbTryIntegerEncoding 函数。\n\n下图展示了 rdbSaveRawString 函数的基本执行逻辑，你可以看下。其中，它调用 rdbSaveLen 函数写入字符串长度，调用 rdbWriteRaw 函数写入实际数据。\n\n\n\n第三步，rdbSaveAuxField 函数就需要写入属性信息的值了。因为属性信息的值通常也是字符串，所以和第二步写入属性信息的键类似，rdbSaveAuxField 函数会调用 rdbSaveRawString 函数来写入属性信息的值。\n\n下面的代码展示了 rdbSaveAuxField 函数的执行整体过程，你可以再回顾下。\n\nssize_t rdbSaveAuxField(rio *rdb, void *key, size_t keylen, void *val, size_t vallen) {\n    ssize_t ret, len = 0;\n    //写入操作码\n    if ((ret = rdbSaveType(rdb,RDB_OPCODE_AUX)) == -1) return -1;\n    len += ret;\n    //写入属性信息中的键\n    if ((ret = rdbSaveRawString(rdb,key,keylen)) == -1) return -1;\n    len += ret;\n    //写入属性信息中的值\n    if ((ret = rdbSaveRawString(rdb,val,vallen)) == -1) return -1;\n    len += ret;\n    return len;\n}\n\n\n到这里，RDB 文件头的内容已经写完了。我把刚才创建的 RDB 文件头的部分内容，画在了下图当中，并且标识了十六进制对应的 ASCII 字符以及一些关键信息，你可以结合图例来理解刚才介绍的代码。\n\n\n\n这样接下来，rdbSaveRio 函数就要开始写入实际的键值对了，这也是文件中实际记录数据的部分。下面，我们就来具体看下。\n\n\n# 生成文件数据部分\n\n因为 Redis server 上的键值对可能被保存在不同的数据库中，所以，rdbSaveRio 函数会执行一个循环，遍历每个数据库，将其中的键值对写入 RDB 文件。\n\n在这个循环流程中，rdbSaveRio 函数会先将 **SELECTDB 操作码 **和对应的数据库编号写入 RDB 文件，这样一来，程序在解析 RDB 文件时，就可以知道接下来的键值对是属于哪个数据库的了。这个过程如下所示：\n\n...\nfor (j = 0; j < server.dbnum; j++) { //循环遍历每一个数据库\n    ...\n    //写入SELECTDB操作码\n    if (rdbSaveType(rdb,RDB_OPCODE_SELECTDB) == -1) goto werr;\n    if (rdbSaveLen(rdb,j) == -1) goto werr;  //写入当前数据库编号j\n    ...\n}\n\n\n下图展示了刚才我创建的 RDB 文件中 SELECTDB 操作码的信息，你可以看到，数据库编号为 0。\n\n\n\n紧接着，rdbSaveRio 函数会写入 RESIZEDB 操作码，用来标识全局哈希表和过期 key 哈希表中键值对数量的记录，这个过程的执行代码如下所示：\n\n...\ndb_size = dictSize(db->dict);   //获取全局哈希表大小\nexpires_size = dictSize(db->expires);  //获取过期key哈希表的大小\nif (rdbSaveType(rdb,RDB_OPCODE_RESIZEDB) == -1) goto werr;  //写入RESIZEDB操作码\nif (rdbSaveLen(rdb,db_size) == -1) goto werr;  //写入全局哈希表大小\nif (rdbSaveLen(rdb,expires_size) == -1) goto werr; //写入过期key哈希表大小\n...\n\n\n我也把刚才创建的 RDB 文件中，RESIZEDB 操作码的内容画在了下图中，你可以看下。\n\n\n\n你可以看到，在 RESIZEDB 操作码后，紧接着记录的是全局哈希表中的键值对，它的数量是 2，然后是过期 key 哈希表中的键值对，其数量为 0。我们刚才在生成 RDB 文件前，只插入了两个键值对，所以，RDB 文件中记录的信息和我们刚才的操作结果是一致的。\n\n好了，在记录完这些信息后，rdbSaveRio 函数会接着执行一个循环流程，在该流程中，rdbSaveRio 函数会取出当前数据库中的每一个键值对，并调用 rdbSaveKeyValuePair 函数（在 rdb.c 文件中），将它写入 RDB 文件。这个基本的循环流程如下所示：\n\n while((de = dictNext(di)) != NULL) {  //读取数据库中的每一个键值对\n    sds keystr = dictGetKey(de);  //获取键值对的key\n    robj key, *o = dictGetVal(de);  //获取键值对的value\n    initStaticStringObject(key,keystr);  //为key生成String对象\n    expire = getExpire(db,&key);  //获取键值对的过期时间\n    //把key和value写入RDB文件\n    if (rdbSaveKeyValuePair(rdb,&key,o,expire) == -1) goto werr;\n    ...\n}\n\n\n这里，rdbSaveKeyValuePair 函数主要是负责将键值对实际写入 RDB 文件。它会先将键值对的过期时间、LRU 空闲时间或是 LFU 访问频率写入 RDB 文件。在写入这些信息时，rdbSaveKeyValuePair 函数都会先调用 rdbSaveType 函数，写入标识这些信息的操作码，你可以看下下面的代码。\n\nif (expiretime != -1) {\n    //写入过期时间操作码标识\n   if (rdbSaveType(rdb,RDB_OPCODE_EXPIRETIME_MS) == -1) return -1;\n   if (rdbSaveMillisecondTime(rdb,expiretime) == -1) return -1;\n}\nif (savelru) {\n   ...\n   //写入LRU空闲时间操作码标识\n   if (rdbSaveType(rdb,RDB_OPCODE_IDLE) == -1) return -1;\n   if (rdbSaveLen(rdb,idletime) == -1) return -1;\n}\nif (savelfu) {\n   ...\n   //写入LFU访问频率操作码标识\n   if (rdbSaveType(rdb,RDB_OPCODE_FREQ) == -1) return -1;\n   if (rdbWriteRaw(rdb,buf,1) == -1) return -1;\n}\n\n\n好了，到这里，rdbSaveKeyValuePair 函数就要开始实际写入键值对了。为了便于解析 RDB 文件时恢复键值对，rdbSaveKeyValuePair 函数会先调用 rdbSaveObjectType 函数，写入键值对的类型标识；然后调用 rdbSaveStringObject 写入键值对的 key；最后，它会调用 rdbSaveObject 函数写入键值对的 value。这个过程如下所示，这几个函数都是在 rdb.c 文件中实现的：\n\nif (rdbSaveObjectType(rdb,val) == -1) return -1;  //写入键值对的类型标识\nif (rdbSaveStringObject(rdb,key) == -1) return -1; //写入键值对的key\nif (rdbSaveObject(rdb,val,key) == -1) return -1; //写入键值对的value\n\n\n这里，你需要注意的是，rdbSaveObjectType 函数会根据键值对的 value 类型，来决定写入到 RDB 中的键值对类型标识，这些类型标识在 rdb.h 文件中有对应的宏定义。比如，我在刚才创建 RDB 文件前，写入的键值对分别是 String 类型和 Hash 类型，而 Hash 类型因为它包含的元素个数不多，所以默认采用 ziplist 数据结构来保存。这两个类型标识对应的数值如下所示：\n\n#define RDB_TYPE_STRING   0\n#define RDB_TYPE_HASH_ZIPLIST  13\n\n\n我把刚才写入的 String 类型键值对“hello”“redis”在 RDB 文件中对应的记录内容，画在了下图中，你可以看下。\n\n\n\n你可以看到，这个键值对的开头类型标识就是 0，和刚才介绍的 RDB_TYPE_STRING 宏定义的值是一致的。而紧接着的 key 和 value，它们都会先记录长度信息，然后才记录实际内容。\n\n因为键值对的 key 都是 String 类型，所以 rdbSaveKeyValuePair 函数就用 rdbSaveStringObject 函数来写入了。而键值对的 value 有不同的类型，所以，rdbSaveObject 函数会根据 value 的类型，执行不同的代码分支，将 value 底层数据结构中的内容写入 RDB。\n\n好了，到这里，我们就了解了 rdbSaveKeyValuePair 函数是如何将键值对写入 RDB 文件中的了。在这个过程中，除了键值对类型、键值对的 key 和 value 会被记录以外，键值对的过期时间、LRU 空闲时间或是 LFU 访问频率也都会记录到 RDB 文件中。这就生成 RDB 文件的数据部分。\n\n最后，我们再来看下 RDB 文件尾的生成。\n\n\n# 生成文件尾\n\n当所有键值对都写入 RDB 文件后，**rdbSaveRio 函数 **就可以写入文件尾内容了。文件尾的内容比较简单，主要包括两个部分，一个是 RDB 文件结束的操作码标识，另一个是 RDB 文件的校验值。\n\nrdbSaveRio 函数会先调用 rdbSaveType 函数，写入文件结束操作码 RDB_OPCODE_EOF，然后调用 rioWrite 写入检验值，如下所示：\n\n...\n//写入结束操作码\nif (rdbSaveType(rdb,RDB_OPCODE_EOF) == -1) goto werr;\n\n//写入校验值\ncksum = rdb->cksum;\nmemrev64ifbe(&cksum);\nif (rioWrite(rdb,&cksum,8) == 0) goto werr;\n...\n\n\n下图展示了我刚才生成的 RDB 文件的文件尾，你可以看下。\n\n\n\n这样，我们也就整体了解了 RDB 文件从文件头、文件数据部分再到文件尾的整个生成过程了。\n\n\n# 总结\n\n 1. 创建 RDB 文件的三个入口函数分别是 rdbSave、rdbSaveBackground、rdbSaveToSlavesSockets，它们在 Redis 源码中被调用的地方，也就是触发 RDB 文件生成的时机\n\n 2. RDB 文件的基本组成，并且也要结合 rdbSaveRio 函数的执行流程，来掌握 RDB 文件头、文件数据部分和文件尾这三个部分的生成。我总结了以下两点，方便你对 RDB 文件结构和内容有个整体把握：\n    \n    * RDB 文件使用多种操作码来标识 Redis 不同的属性信息，以及使用类型码来标识不同 value 类型；\n    \n    * RDB 文件内容是自包含的，也就是说，无论是属性信息还是键值对，RDB 文件都会按照类型、长度、实际数据的格式来记录，这样方便程序对 RDB 文件的解析。\n\n 3. RDB 文件包含了 Redis 数据库某一时刻的所有键值对，以及这些键值对的类型、大小、过期时间等信息。\n\n当你了解了 RDB 文件的格式和生成方法后，其实你就可以根据需求，开发解析 RDB 文件的程序或是加载 RDB 文件的程序了\n\n比如，你可以在 RDB 文件中查找内存空间消耗大的键值对，也就是在优化 Redis 性能时通常需要查找的 bigkey；你也可以分析不同类型键值对的数量、空间占用等分布情况，来了解业务数据的特点；你还可以自行加载 RDB 文件，用于测试或故障排查。\n\n当然，这里我也再给你一个小提示，就是在你实际开发 RDB 文件分析工具之前，可以看下redis-rdb-tools这个工具，它能够帮助你分析 RDB 文件中的内容。而如果它还不能满足你的定制化需求，你就可以用上这节课学习的内容，来开发自己的 RDB 分析工具了\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. redis 是如何通过 rdb 文件来实现数据持久化的？其背后有哪些关键过程？\n 2. 除了 save 和 bgsave 命令，redis 在什么场景下还会创建 rdb 文件？\n 3. rdb 文件中，为什么需要为不同内容设定操作码？这些操作码如何帮助解析数据？\n 4. rdbsaverio 函数生成 rdb 文件时，如何保证数据的完整性和效率？\n 5. rdb 文件为什么要自包含所有键值对的类型、长度和数据？这对恢复数据有什么好处？\n 6. 如何通过解析 rdb 文件来发现 redis 中的大 key，从而优化内存？\n\n\n# 前言\n\n我们来到了一个新的模块「可靠性保证模块」\n\n我们就先从rdb文件的生成开始学起。下面呢，我先带你来了解下rdb创建的入口函数，以及调用这些函数的地方。\n\n\n# rdb 创建的入口函数和触发时机\n\nredis 源码中用来创建 rdb 文件的函数有三个，它们都是在rdb.c文件中实现的，接下来我就带你具体了解下。\n\n * rdbsave 函数\n\n这是 redis server 在本地磁盘创建 rdb 文件的入口函数。它对应了 redis 的 save 命令，会在 save 命令的实现函数 savecommand（在 rdb.c 文件中）中被调用。而 rdbsave 函数最终会调用 rdbsaverio 函数（在 rdb.c 文件中）来实际创建 rdb 文件。rdbsaverio 函数的执行逻辑就体现了 rdb 文件的格式和生成过程，我稍后向你介绍\n\n * rdbsavebackground 函数\n\n这是 redis server 使用后台子进程方式，在本地磁盘创建 rdb 文件的入口函数。它对应了 redis 的 bgsave 命令，会在 bgsave 命令的实现函数 bgsavecommand（在 rdb.c 文件中）中被调用。这个函数会调用 fork 创建一个子进程，让子进程调用 rdbsave 函数来继续创建 rdb 文件，而父进程，也就是主线程本身可以继续处理客户端请求。\n\n下面的代码展示了 rdbsavebackground 函数创建子进程的过程，你可以看下\n\nint rdbsavebackground(char *filename, rdbsaveinfo *rsi) {\n    ...\n    if ((childpid = fork()) == 0) {  //子进程的代码执行分支\n       ...\n       retval = rdbsave(filename,rsi);  //调用rdbsave函数创建rdb文件\n       ...\n       exitfromchild((retval == c_ok) ? 0 : 1);  //子进程退出\n    } else {\n       ...  //父进程代码执行分支\n    }\n}\n\n\n * rdbsavetoslavessockets 函数\n\n这是 redis server 在采用不落盘方式传输 rdb 文件进行主从复制时，创建 rdb 文件的入口函数。它会被 startbgsaveforreplication 函数调用（在replication.c文件中）。而 startbgsaveforreplication 函数会被 replication.c 文件中的 synccommand 函数和 replicationcron 函数调用，这对应了 redis server 执行主从复制命令，以及周期性检测主从复制状态时触发 rdb 生成。\n\n和 rdbsavebackground 函数类似，rdbsavetoslavessockets 函数也是通过 fork 创建子进程，让子进程生成 rdb。不过和 rdbsavebackground 函数不同的是，rdbsavetoslavessockets 函数是通过网络以字节流的形式，直接发送 rdb 文件的二进制数据给从节点。\n\n而为了让从节点能够识别用来同步数据的 rdb 内容，rdbsavetoslavessockets 函数调用 rdbsaveriowitheofmark 函数（在 rdb.c 文件中），在 rdb 二进制数据的前后加上了标识字符串，如下图所示：\n\n\n\n以下代码也展示了rdbsaveriowitheofmark函数的基本执行逻辑。你可以看到，它除了写入前后标识字符串之外，还是会调用rdbsaverio函数实际生成rdb内容。\n\nint rdbsaveriowitheofmark(rio *rdb, int *error, rdbsaveinfo *rsi) {\n    ...\n    getrandomhexchars(eofmark,rdb_eof_mark_size); //随机生成40字节的16进制字符串，保存在eofmark中，宏定义rdb_eof_mark_size的值为40\n    if (riowrite(rdb,"$eof:",5) == 0) goto werr;  //写入$eof\n    if (riowrite(rdb,eofmark,rdb_eof_mark_size) == 0) goto werr; //写入40字节的16进制字符串eofmark\n    if (riowrite(rdb,"\\r\\n",2) == 0) goto werr; //写入\\r\\n\n    if (rdbsaverio(rdb,error,rdb_save_none,rsi) == c_err) goto werr; //生成rdb内容\n    if (riowrite(rdb,eofmark,rdb_eof_mark_size) == 0) goto werr; //再次写入40字节的16进制字符串eofmark\n    ...\n}\n\n\n好了，了解了 rdb 文件创建的三个入口函数后，我们也看到了，rdb 文件创建的三个时机，分别是 save 命令执行、bgsave 命令执行以及主从复制。那么，除了这三个时机外，在 redis 源码中，还有哪些地方会触发 rdb 文件创建呢？\n\n实际上，因为 rdbsavetoslavessockets 函数只会在主从复制时调用，所以，我们只要通过在 redis 源码中查找 rdbsave、rdbsavebackground 这两个函数，就可以了解触发 rdb 文件创建的其他时机。\n\n那么经过查找，我们可以发现在 redis 源码中，rdbsave 还会在 flushallcommand 函数（在db.c文件中）、prepareforshutdown 函数（在server.c文件中）中被调用。这也就是说，redis 在执行 flushall 命令以及正常关闭时，会创建 rdb 文件。\n\n对于 rdbsavebackground 函数来说，它除了在执行 bgsave 命令时被调用，当主从复制采用落盘文件方式传输 rdb 时，它也会被 startbgsaveforreplication 函数调用。此外，redis server 运行时的周期性执行函数 servercron（在server.c文件中），也会调用 rdbsavebackground 函数来创建 rdb 文件。\n\n为了便于你掌握 rdb 文件创建的整体情况，我画了下面这张图，展示了 redis 源码中创建 rdb 文件的函数调用关系，你可以看下。\n\n\n\n好了，到这里，你可以看到，实际最终生成 rdb 文件的函数是 rdbsaverio。所以接下来，我们就来看看 rdbsaverio 函数的执行过程。同时，我还会给你介绍 rdb 文件的格式是如何组织的\n\n\n# rdb 文件是如何生成的\n\n不过在了解 rdbsaverio 函数具体是如何生成 rdb 文件之前，你还需要先了解下 rdb 文件的基本组成部分。这样，你就可以按照 rdb 文件的组成部分，依次了解 rdbsaverio 函数的执行逻辑了。\n\n那么，一个 rdb 文件主要是由三个部分组成的。\n\n * 文件头：这部分内容保存了 redis 的魔数、rdb 版本、redis 版本、rdb 文件创建时间、键值对占用的内存大小等信息。\n * 文件数据部分：这部分保存了 redis 数据库实际的所有键值对。\n * 文件尾：这部分保存了 rdb 文件的结束标识符，以及整个文件的校验值。这个校验值用来在 redis server 加载 rdb 文件后，检查文件是否被篡改过。\n\n下图就展示了 rdb 文件的组成，你可以看下。\n\n\n\n好，接下来，我们就来看看 rdbsaverio 函数是如何生成 rdb 文件中的每一部分的。这里，为了方便你理解 rdb 文件格式以及文件内容，你可以先按照如下步骤准备一个 rdb 文件。\n\n第一步，在你电脑上 redis 的目录下，启动一个用来测试的 redis server，可以执行如下命令：\n\n好，接下来，我们就来看看 rdbsaverio 函数是如何生成 rdb 文件中的每一部分的。这里，为了方便你理解 rdb 文件格式以及文件内容，你可以先按照如下步骤准备一个 rdb 文件。\n\n第一步，在你电脑上 redis 的目录下，启动一个用来测试的 redis server，可以执行如下命令：\n\n./redis-server\n\n\n第二步，执行 flushall 命令，清空当前的数据库：\n\n./redis-cli flushall\n\n\n第三步，使用 redis-cli 登录刚启动的 redis server，执行 set 命令插入一个 string 类型的键值对，再执行 hmset 命令插入一个 hash 类型的键值对。执行 save 命令，将当前数据库内容保存到 rdb 文件中。这个过程如下所示：\n\n127.0.0.1:6379>set hello redis\nok\n127.0.0.1:6379>hmset userinfo uid 1 name zs age 32\nok\n127.0.0.1:6379> save\nok\n\n\n好了，到这里，你就可以在刚才执行 redis-cli 命令的目录下，找见刚生成的 rdb 文件，文件名应该是 dump.rdb。\n\n不过，因为 rdb 文件实际是一个二进制数据组成的文件，所以如果你使用一般的文本编辑软件，比如 linux 系统上的 vim，在打开 rdb 文件时，你会看到文件中都是乱码。所以这里，我给你提供一个小工具，如果你想查看 rdb 文件中二进制数据和对应的 ascii 字符，你可以使用 linux 上的 od 命令，这个命令可以用不同进制的方式展示数据，并显示对应的 ascii 字符。\n\n比如，你可以执行如下的命令，读取 dump.rdb 文件，并用十六进制展示文件内容，同时文件中每个字节对应的 ascii 字符也会被对应显示出来。\n\nod -a x -t x1c -v dump.rdb\n\n\n以下代码展示的就是我用 od 命令，查看刚才生成的 dump.rdb 文件后，输出的从文件头开始的部分内容。你可以看到这四行结果中，第一和第三行是用十六进制显示的 dump.rdb 文件的字节内容，这里每两个十六进制数对应了一个字节。而第二和第四行是 od 命令生成的每个字节所对应的 ascii 字符。\n\n\n\n这也就是说，在刚才生成的 rdb 文件中，如果想要转换成 ascii 字符，它的文件头内容其实就已经包含了 redis 的字符串和一些数字，而这正是 rdb 文件头包含的内容。\n\n那么下面，我们就来看看 rdb 文件的文件头是如何生成的。\n\n\n# 生成文件头\n\n就像刚才给你介绍的，rdb 文件头的内容首先是魔数，这对应记录了 rdb 文件的版本。在 rdbsaverio 函数中，魔数是通过 snprintf 函数生成的，它的具体内容是字符串“redis”，再加上 rdb 版本的宏定义 rdb_version（在rdb.h文件中，值为 9）。然后，rdbsaverio 函数会调用 rdbwriteraw 函数（在 rdb.c 文件中），将魔数写入 rdb 文件，如下所示：\n\nsnprintf(magic,sizeof(magic),"redis%04d",rdb_version);  //生成魔数magic\nif (rdbwriteraw(rdb,magic,9) == -1) goto werr;  //将magic写入rdb文件\n\n\n刚才用来写入魔数的 rdbwriteraw 函数，它实际会调用 riowrite 函数（在 rdb.h 文件中）来完成写入。而 riowrite 函数是 rdb 文件内容的最终写入函数，它负责根据要写入数据的长度，把待写入缓冲区中的内容写入 rdb。这里，你需要注意的是，rdb 文件生成过程中，会有不同的函数负责写入不同部分的内容，不过这些函数最终都还是调用 riowrite 函数，来完成数据的实际写入的。\n\n好了，当在 rdb 文件头中写入魔数后，rdbsaverio 函数紧接着会调用 rdbsaveinfoauxfields 函数，将和 redis server 相关的一些属性信息写入 rdb 文件头，如下所示：\n\nif (rdbsaveinfoauxfields(rdb,flags,rsi) == -1) goto werr; //写入属性信息\n\n\nrdbsaveinfoauxfields 函数是在 rdb.c 文件中实现的，它会使用键值对的形式，在 rdb 文件头中记录 redis server 的属性信息。下表中列出了 rdb 文件头记录的一些主要信息，以及它们对应的键和值，你可以看下。\n\n\n\n那么，当属性值为字符串时，rdbsaveinfoauxfields 函数会调用 rdbsaveauxfieldstrstr 函数写入属性信息；而当属性值为整数时，rdbsaveinfoauxfields 函数会调用 rdbsaveauxfieldstrint 函数写入属性信息，如下所示：\n\nif (rdbsaveauxfieldstrstr(rdb,"redis-ver",redis_version) == -1) return -1;\nif (rdbsaveauxfieldstrint(rdb,"redis-bits",redis_bits) == -1) return -1;\nif (rdbsaveauxfieldstrint(rdb,"ctime",time(null)) == -1) return -1;\nif (rdbsaveauxfieldstrint(rdb,"used-mem",zmalloc_used_memory()) == -1) return -1;\n\n\n这里，无论是 rdbsaveauxfieldstrstr 函数还是 rdbsaveauxfieldstrint 函数，它们都会调用 rdbsaveauxfield 函数来写入属性值。rdbsaveauxfield 函数是在 rdb.c 文件中实现的，它会分三步来完成一个属性信息的写入。\n\n第一步，它调用 rdbsavetype 函数写入一个操作码。这个操作码的目的，是用来在 rdb 文件中标识接下来的内容是什么。当写入属性信息时，这个操作码对应了宏定义 rdb_opcode_aux（在 rdb.h 文件中），值为 250，对应的十六进制值为 fa。这样一来，就方便我们解析 rdb 文件了。比如，在读取 rdb 文件时，如果程序读取到 fa 这个字节，那么，这就表明接下来的内容是一个属性信息。\n\n这里，你需要注意的是，rdb 文件使用了多个操作码，来标识文件中的不同内容。它们都是在 rdb.h 文件中定义的，下面的代码中展示了部分操作码，你可以看下。\n\n#define rdb_opcode_idle       248   //标识lru空闲时间\n#define rdb_opcode_freq       249   //标识lfu访问频率信息\n#define rdb_opcode_aux        250   //标识rdb文件头的属性信息\n#define rdb_opcode_expiretime_ms 252    //标识以毫秒记录的过期时间\n#define rdb_opcode_selectdb   254   //标识文件中后续键值对所属的数据库编号\n#define rdb_opcode_eof        255   //标识rdb文件结束，用在文件尾\n\n\n第二步，rdbsaveauxfield 函数调用 rdbsaverawstring 函数（在 rdb.c 文件中）写入属性信息的键，而键通常是一个字符串。rdbsaverawstring 函数是用来写入字符串的通用函数，它会先记录字符串长度，然后再记录实际字符串，如下图所示。这个长度信息是为了解析 rdb 文件时，程序可以基于它知道当前读取的字符串应该读取多少个字节。\n\n\n\n不过，为了节省 rdb 文件消耗的空间，如果字符串中记录的实际是一个整数，rdbsaverawstring 函数还会调用 rdbtryintegerencoding 函数（在 rdb.c 文件中），尝试用紧凑结构对字符串进行编码。具体做法你可以进一步阅读 rdbtryintegerencoding 函数。\n\n下图展示了 rdbsaverawstring 函数的基本执行逻辑，你可以看下。其中，它调用 rdbsavelen 函数写入字符串长度，调用 rdbwriteraw 函数写入实际数据。\n\n\n\n第三步，rdbsaveauxfield 函数就需要写入属性信息的值了。因为属性信息的值通常也是字符串，所以和第二步写入属性信息的键类似，rdbsaveauxfield 函数会调用 rdbsaverawstring 函数来写入属性信息的值。\n\n下面的代码展示了 rdbsaveauxfield 函数的执行整体过程，你可以再回顾下。\n\nssize_t rdbsaveauxfield(rio *rdb, void *key, size_t keylen, void *val, size_t vallen) {\n    ssize_t ret, len = 0;\n    //写入操作码\n    if ((ret = rdbsavetype(rdb,rdb_opcode_aux)) == -1) return -1;\n    len += ret;\n    //写入属性信息中的键\n    if ((ret = rdbsaverawstring(rdb,key,keylen)) == -1) return -1;\n    len += ret;\n    //写入属性信息中的值\n    if ((ret = rdbsaverawstring(rdb,val,vallen)) == -1) return -1;\n    len += ret;\n    return len;\n}\n\n\n到这里，rdb 文件头的内容已经写完了。我把刚才创建的 rdb 文件头的部分内容，画在了下图当中，并且标识了十六进制对应的 ascii 字符以及一些关键信息，你可以结合图例来理解刚才介绍的代码。\n\n\n\n这样接下来，rdbsaverio 函数就要开始写入实际的键值对了，这也是文件中实际记录数据的部分。下面，我们就来具体看下。\n\n\n# 生成文件数据部分\n\n因为 redis server 上的键值对可能被保存在不同的数据库中，所以，rdbsaverio 函数会执行一个循环，遍历每个数据库，将其中的键值对写入 rdb 文件。\n\n在这个循环流程中，rdbsaverio 函数会先将 **selectdb 操作码 **和对应的数据库编号写入 rdb 文件，这样一来，程序在解析 rdb 文件时，就可以知道接下来的键值对是属于哪个数据库的了。这个过程如下所示：\n\n...\nfor (j = 0; j < server.dbnum; j++) { //循环遍历每一个数据库\n    ...\n    //写入selectdb操作码\n    if (rdbsavetype(rdb,rdb_opcode_selectdb) == -1) goto werr;\n    if (rdbsavelen(rdb,j) == -1) goto werr;  //写入当前数据库编号j\n    ...\n}\n\n\n下图展示了刚才我创建的 rdb 文件中 selectdb 操作码的信息，你可以看到，数据库编号为 0。\n\n\n\n紧接着，rdbsaverio 函数会写入 resizedb 操作码，用来标识全局哈希表和过期 key 哈希表中键值对数量的记录，这个过程的执行代码如下所示：\n\n...\ndb_size = dictsize(db->dict);   //获取全局哈希表大小\nexpires_size = dictsize(db->expires);  //获取过期key哈希表的大小\nif (rdbsavetype(rdb,rdb_opcode_resizedb) == -1) goto werr;  //写入resizedb操作码\nif (rdbsavelen(rdb,db_size) == -1) goto werr;  //写入全局哈希表大小\nif (rdbsavelen(rdb,expires_size) == -1) goto werr; //写入过期key哈希表大小\n...\n\n\n我也把刚才创建的 rdb 文件中，resizedb 操作码的内容画在了下图中，你可以看下。\n\n\n\n你可以看到，在 resizedb 操作码后，紧接着记录的是全局哈希表中的键值对，它的数量是 2，然后是过期 key 哈希表中的键值对，其数量为 0。我们刚才在生成 rdb 文件前，只插入了两个键值对，所以，rdb 文件中记录的信息和我们刚才的操作结果是一致的。\n\n好了，在记录完这些信息后，rdbsaverio 函数会接着执行一个循环流程，在该流程中，rdbsaverio 函数会取出当前数据库中的每一个键值对，并调用 rdbsavekeyvaluepair 函数（在 rdb.c 文件中），将它写入 rdb 文件。这个基本的循环流程如下所示：\n\n while((de = dictnext(di)) != null) {  //读取数据库中的每一个键值对\n    sds keystr = dictgetkey(de);  //获取键值对的key\n    robj key, *o = dictgetval(de);  //获取键值对的value\n    initstaticstringobject(key,keystr);  //为key生成string对象\n    expire = getexpire(db,&key);  //获取键值对的过期时间\n    //把key和value写入rdb文件\n    if (rdbsavekeyvaluepair(rdb,&key,o,expire) == -1) goto werr;\n    ...\n}\n\n\n这里，rdbsavekeyvaluepair 函数主要是负责将键值对实际写入 rdb 文件。它会先将键值对的过期时间、lru 空闲时间或是 lfu 访问频率写入 rdb 文件。在写入这些信息时，rdbsavekeyvaluepair 函数都会先调用 rdbsavetype 函数，写入标识这些信息的操作码，你可以看下下面的代码。\n\nif (expiretime != -1) {\n    //写入过期时间操作码标识\n   if (rdbsavetype(rdb,rdb_opcode_expiretime_ms) == -1) return -1;\n   if (rdbsavemillisecondtime(rdb,expiretime) == -1) return -1;\n}\nif (savelru) {\n   ...\n   //写入lru空闲时间操作码标识\n   if (rdbsavetype(rdb,rdb_opcode_idle) == -1) return -1;\n   if (rdbsavelen(rdb,idletime) == -1) return -1;\n}\nif (savelfu) {\n   ...\n   //写入lfu访问频率操作码标识\n   if (rdbsavetype(rdb,rdb_opcode_freq) == -1) return -1;\n   if (rdbwriteraw(rdb,buf,1) == -1) return -1;\n}\n\n\n好了，到这里，rdbsavekeyvaluepair 函数就要开始实际写入键值对了。为了便于解析 rdb 文件时恢复键值对，rdbsavekeyvaluepair 函数会先调用 rdbsaveobjecttype 函数，写入键值对的类型标识；然后调用 rdbsavestringobject 写入键值对的 key；最后，它会调用 rdbsaveobject 函数写入键值对的 value。这个过程如下所示，这几个函数都是在 rdb.c 文件中实现的：\n\nif (rdbsaveobjecttype(rdb,val) == -1) return -1;  //写入键值对的类型标识\nif (rdbsavestringobject(rdb,key) == -1) return -1; //写入键值对的key\nif (rdbsaveobject(rdb,val,key) == -1) return -1; //写入键值对的value\n\n\n这里，你需要注意的是，rdbsaveobjecttype 函数会根据键值对的 value 类型，来决定写入到 rdb 中的键值对类型标识，这些类型标识在 rdb.h 文件中有对应的宏定义。比如，我在刚才创建 rdb 文件前，写入的键值对分别是 string 类型和 hash 类型，而 hash 类型因为它包含的元素个数不多，所以默认采用 ziplist 数据结构来保存。这两个类型标识对应的数值如下所示：\n\n#define rdb_type_string   0\n#define rdb_type_hash_ziplist  13\n\n\n我把刚才写入的 string 类型键值对“hello”“redis”在 rdb 文件中对应的记录内容，画在了下图中，你可以看下。\n\n\n\n你可以看到，这个键值对的开头类型标识就是 0，和刚才介绍的 rdb_type_string 宏定义的值是一致的。而紧接着的 key 和 value，它们都会先记录长度信息，然后才记录实际内容。\n\n因为键值对的 key 都是 string 类型，所以 rdbsavekeyvaluepair 函数就用 rdbsavestringobject 函数来写入了。而键值对的 value 有不同的类型，所以，rdbsaveobject 函数会根据 value 的类型，执行不同的代码分支，将 value 底层数据结构中的内容写入 rdb。\n\n好了，到这里，我们就了解了 rdbsavekeyvaluepair 函数是如何将键值对写入 rdb 文件中的了。在这个过程中，除了键值对类型、键值对的 key 和 value 会被记录以外，键值对的过期时间、lru 空闲时间或是 lfu 访问频率也都会记录到 rdb 文件中。这就生成 rdb 文件的数据部分。\n\n最后，我们再来看下 rdb 文件尾的生成。\n\n\n# 生成文件尾\n\n当所有键值对都写入 rdb 文件后，**rdbsaverio 函数 **就可以写入文件尾内容了。文件尾的内容比较简单，主要包括两个部分，一个是 rdb 文件结束的操作码标识，另一个是 rdb 文件的校验值。\n\nrdbsaverio 函数会先调用 rdbsavetype 函数，写入文件结束操作码 rdb_opcode_eof，然后调用 riowrite 写入检验值，如下所示：\n\n...\n//写入结束操作码\nif (rdbsavetype(rdb,rdb_opcode_eof) == -1) goto werr;\n\n//写入校验值\ncksum = rdb->cksum;\nmemrev64ifbe(&cksum);\nif (riowrite(rdb,&cksum,8) == 0) goto werr;\n...\n\n\n下图展示了我刚才生成的 rdb 文件的文件尾，你可以看下。\n\n\n\n这样，我们也就整体了解了 rdb 文件从文件头、文件数据部分再到文件尾的整个生成过程了。\n\n\n# 总结\n\n 1. 创建 rdb 文件的三个入口函数分别是 rdbsave、rdbsavebackground、rdbsavetoslavessockets，它们在 redis 源码中被调用的地方，也就是触发 rdb 文件生成的时机\n\n 2. rdb 文件的基本组成，并且也要结合 rdbsaverio 函数的执行流程，来掌握 rdb 文件头、文件数据部分和文件尾这三个部分的生成。我总结了以下两点，方便你对 rdb 文件结构和内容有个整体把握：\n    \n    * rdb 文件使用多种操作码来标识 redis 不同的属性信息，以及使用类型码来标识不同 value 类型；\n    \n    * rdb 文件内容是自包含的，也就是说，无论是属性信息还是键值对，rdb 文件都会按照类型、长度、实际数据的格式来记录，这样方便程序对 rdb 文件的解析。\n\n 3. rdb 文件包含了 redis 数据库某一时刻的所有键值对，以及这些键值对的类型、大小、过期时间等信息。\n\n当你了解了 rdb 文件的格式和生成方法后，其实你就可以根据需求，开发解析 rdb 文件的程序或是加载 rdb 文件的程序了\n\n比如，你可以在 rdb 文件中查找内存空间消耗大的键值对，也就是在优化 redis 性能时通常需要查找的 bigkey；你也可以分析不同类型键值对的数量、空间占用等分布情况，来了解业务数据的特点；你还可以自行加载 rdb 文件，用于测试或故障排查。\n\n当然，这里我也再给你一个小提示，就是在你实际开发 rdb 文件分析工具之前，可以看下redis-rdb-tools这个工具，它能够帮助你分析 rdb 文件中的内容。而如果它还不能满足你的定制化需求，你就可以用上这节课学习的内容，来开发自己的 rdb 分析工具了\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"AOF 持久化",frontmatter:{title:"AOF 持久化",date:"2024-09-16T03:23:41.000Z",permalink:"/pages/9b17a7/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E6%94%AF%E7%BA%BF/17.AOF%20%E6%8C%81%E4%B9%85%E5%8C%96.html",relativePath:"Redis 系统设计/04.支线/17.AOF 持久化.md",key:"v-459c5fdc",path:"/pages/9b17a7/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:327},{level:2,title:"AOF 核心流程",slug:"aof-核心流程",normalizedTitle:"aof 核心流程",charIndex:695},{level:3,title:"命令追加",slug:"命令追加",normalizedTitle:"命令追加",charIndex:876},{level:3,title:"写入与同步",slug:"写入与同步",normalizedTitle:"写入与同步",charIndex:1043},{level:3,title:"持久化的效率和安全性",slug:"持久化的效率和安全性",normalizedTitle:"持久化的效率和安全性",charIndex:2266},{level:4,title:"如果 Redis 宕机了，操作系统没有宕机，会有数据丢失吗？",slug:"如果-redis-宕机了-操作系统没有宕机-会有数据丢失吗",normalizedTitle:"如果 redis 宕机了，操作系统没有宕机，会有数据丢失吗？",charIndex:2960},{level:4,title:"为什么在 always 下也可能会丢失一个事件循环中所产生的数据？",slug:"为什么在-always-下也可能会丢失一个事件循环中所产生的数据",normalizedTitle:"为什么在 always 下也可能会丢失一个事件循环中所产生的数据？",charIndex:3046},{level:3,title:"AOF 文件载入",slug:"aof-文件载入",normalizedTitle:"aof 文件载入",charIndex:3548},{level:2,title:"AOF 重写",slug:"aof-重写",normalizedTitle:"aof 重写",charIndex:18},{level:3,title:"AOF 重写函数与触发时机",slug:"aof-重写函数与触发时机",normalizedTitle:"aof 重写函数与触发时机",charIndex:3666},{level:3,title:"AOF 重写的基本过程",slug:"aof-重写的基本过程",normalizedTitle:"aof 重写的基本过程",charIndex:8343},{level:2,title:"深入重写缓冲区",slug:"深入重写缓冲区",normalizedTitle:"深入重写缓冲区",charIndex:10743},{level:3,title:"如何使用管道进行父子进程间通信？",slug:"如何使用管道进行父子进程间通信",normalizedTitle:"如何使用管道进行父子进程间通信？",charIndex:10755},{level:3,title:"AOF 重写子进程如何使用管道和父进程交互？",slug:"aof-重写子进程如何使用管道和父进程交互",normalizedTitle:"aof 重写子进程如何使用管道和父进程交互？",charIndex:12531},{level:4,title:"操作命令传输管道的使用",slug:"操作命令传输管道的使用",normalizedTitle:"操作命令传输管道的使用",charIndex:14521},{level:4,title:"ACK 管道的使用",slug:"ack-管道的使用",normalizedTitle:"ack 管道的使用",charIndex:18925},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:8083},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:21203}],headersStr:"前言 AOF 核心流程 命令追加 写入与同步 持久化的效率和安全性 如果 Redis 宕机了，操作系统没有宕机，会有数据丢失吗？ 为什么在 always 下也可能会丢失一个事件循环中所产生的数据？ AOF 文件载入 AOF 重写 AOF 重写函数与触发时机 AOF 重写的基本过程 深入重写缓冲区 如何使用管道进行父子进程间通信？ AOF 重写子进程如何使用管道和父进程交互？ 操作命令传输管道的使用 ACK 管道的使用 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. AOF 重写会在什么条件下触发？这些条件如何影响 Redis 性能？\n 2. 在 AOF 重写时，Redis 如何确保新的写操作不会丢失？\n 3. 为什么 Redis 要在 AOF 重写中使用管道通信？这个机制解决了哪些问题？\n 4. 即使操作系统没宕机，为什么在 AOF 模式下仍可能存在数据丢失风险？\n 5. AOF 重写如何确保写入日志的完整性？子进程是怎么处理新的写操作的？\n 6. 在什么情况下，AOF 重写可能会导致 Redis 性能变慢？\n 7. always 模式下，数据为什么还可能丢失？如何平衡 AOF 的性能与安全？\n 8. 当 Redis 变慢时，如何判断是否是 AOF 重写导致的？\n\n\n# 前言\n\n我们知道，Redis 除了使用内存快照 RDB 来保证数据可靠性之外，还可以使用 AOF 日志。不过，RDB 文件是将某一时刻的内存数据保存成一个文件，而 AOF 日志则会记录接收到的所有写操作。如果 Redis server 的写请求很多，那么 AOF 日志中记录的操作也会越来越多，进而就导致 AOF 日志文件越来越大。\n\n然后，为了避免产生过大的 AOF 日志文件，Redis 会对 AOF 文件进行重写，也就是针对当前数据库中每个键值对的最新内容，记录它的插入操作，而不再记录它的历史写操作了。这样一来，重写后的 AOF 日志文件就能变小了。\n\n那么，AOF 重写在哪些时候会被触发呢？以及 AOF 重写需要写文件，这个过程会阻塞 Redis 的主线程，进而影响 Redis 的性能吗？\n\necho 接下来就给你介绍下 AOF 核心流程以及重写的实现过程，通过了解它的实现，我们就可以清楚地了解到 AOF 重写过程的表现，以及它对 Redis server 的影响。这样，当你再遇到 Redis server 性能变慢的问题时，你就可以排查是否是 AOF 重写导致的了。\n\n好，接下来，我们先来看下 AOF 核心流程\n\n\n# AOF 核心流程\n\nAOF 持久化分为三个步骤\n\n * 命令追加\n * 文件写入\n * 文件同步\n\n\n# 命令追加\n\n当 AOF 持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的 aof buf 缓冲区的末尾\n\nstruct redisServer{\n\t...\n\t//AOF 缓冲区\n\tSDS aof_buf;\n\t...\n}\n\n\n\n# 写入与同步\n\nRedis 的服务器进程就是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行像 servercron 函数这样需要定时运行的函数\n\n因为服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到 aof buf 缓冲区里面，所以在服务器每次结束一个事件循环之前，它都会调用 flushAppendonlyFile 函数，考虑是否需要将 aof buf 缓冲区中的内容写入和保存到 AOF 文件里面，这个过程的伪代码如下\n\ndef eventloop():\n\twhile True:\n        # 在处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中\n\t\tprocessFileEvents()\n\t\tpricessTimeEvents()\n        # 考虑是否将 aof_buf 中的内容写入和保存到 AOF 文件里\n\t\tflushAppendOnlyFile();\n\n\nflushAppendOnlyFile 的行为由在 redis.conf 中的 appendfsync选项的值来决定\n\nAPPENDFSYNC 选项的值   FLUSHAPPENDONLYFILE 函数行为\nalways             将 AOF 缓冲区中的所有内容写入并同步到 AOF 文件。\neverysec           将 AOF 缓冲区中的所有内容写入到 AOF 文件，如果上次同步 AOF 文件的时间距离现在超过一秒钟，那么再次对\n                   AOF 文件进行同步，并且这个同步操作是由一个线程专门负责执行的。\nno                 将 AOF 缓冲区中的所有内容写入到 AOF 文件，但并不对 AOF 文件进行同步，何时同步由操作系统来决定。\n\n默认值是 everysec\n\n文件的写入和同步\n\n为了提高文件的写入效率，在现代操作系统中，当用户调用 write 函数，将一些数据写入到文件的时候，操作系统通常会将写入数据暂时保存在一个内存缓冲区里面等到缓冲区的空间被填满、或者超过了指定的时限之后，才真正地将缓冲区中的数据写入到磁盘里面。 这种做法虽然提高了效率，但也为写入数据带来了安全问题，因为如果计算机发生停机，那么保存在内存缓冲区里面的写入数据将会丢失。 为此，系统提供了 fsync 和 fdatasync 两个同步函数，它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里面，从而确保写入数据的安全性\n\n如果这时 flushAppendonlyFile 函数被调用，假设服务器当前 appendfsyne 选项的值为 everysec，并且距离上次同步 AOF 文件已经超过一秒钟，那么服务器会先将 aof buf 中的内容写人到 AOF 文件中，然后再对 AOF 文件进行同步。\n\n\n# 持久化的效率和安全性\n\n服务器配置 appendfsync 选项的值直接决定 AOF 持久化功能的效率和安全性。\n\n * 当 appendfsync 的值为 always 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 AOF 文件，并且同步 AOF 文件，所以 always 的效率是 appendfsync 选项三个值当中最慢的一个，但从安全性来说，always 也是最安全的，因为即使出现故障停机，AOF 持久化也只会丢失一个事件循环中所产生的命令数据\n * 当 appendfsync 的值为 everysec 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 AOF 文件，并且每隔一秒就要在子线程中对 AOF 文件进行一次同步。从效率上来讲，everysec 模式足够快，并且就算出现故障停机，数据库也只丢失一秒钟的命令数据。\n * 当 appendfsync 的值为 no 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 AOF 文件，至于何时对 AOF 文件进行同步，则由操作系统控制。因为处于 no 模式下的 flushappendOnlyFile 调用无须执行同步操作，所以该模式下的 AOF 文件写入速度总是最快的，不过因为这种模式会在系统缓存中积累一段时间的写入数据，所以该模式的单次同步时长通常是三种模式中时间最长的。从平摊操作的角度来看，no 模式和 everysec 模式的效率类似，当出现故障停机时，使用 no 模式的服务器将丢失上次同步 AOF 文件之后的所有写命令数据\n\n# 如果 Redis 宕机了，操作系统没有宕机，会有数据丢失吗？\n\n不一定 因为写入了 系统缓存 操作系统如果刷盘成功 就不会有丢失，redis 宕机不影响操作系统刷盘\n\n# 为什么在 always 下也可能会丢失一个事件循环中所产生的数据？\n\n如果客户端没有收到 OK 响应，可能会丢失一条数据\n\n但是\n\n在 appendfsync 为 always 模式下，如果客户端收到 Redis 返回的 OK 响应，意味着该命令的结果已经成功写入到 AOF 文件并同步到磁盘\n\n * 在 always 模式下，Redis 在每个命令执行后都会立刻将该命令追加到 aof_buf 缓冲区，然后立即将缓冲区中的内容写入 AOF 文件，并执行 fsync 操作（即将数据同步到磁盘）\n * 当 fsync 成功完成后，Redis 才会返回 OK 响应给客户端\n\n因此，在 always 模式下，如果客户端收到 OK 响应，意味着：\n\n 1. 命令已经被执行。\n 2. 命令的结果已经成功追加到 AOF 文件。\n 3. AOF 文件已经通过 fsync 操作将数据同步到了磁盘。\n\n正因为 always 模式确保每条命令在返回 OK 之前都已经被同步到磁盘，所以从客户端的视角来看，只要收到了 OK，就可以认为该数据已经被持久化到磁盘，不会因为 Redis 崩溃或服务器断电等原因丢失\n\n\n# AOF 文件载入\n\n\n\n 1. 创建一个不带网络连接的伪客户端\n 2. 从 AOF 文件中分析并读取出一条写命令\n 3. 使用伪客户端执行被读出的写命令\n 4. 重复 2 3，知道所有写命令被处理完毕为止\n\n\n# AOF 重写\n\n\n# AOF 重写函数与触发时机\n\n首先，实现 AOF 重写的函数是 rewriteAppendOnlyFileBackground，它是在aof.c文件中实现的。在这个函数中，会调用 fork 函数创建一个 AOF 重写子进程，来实际执行重写操作。关于这个函数的具体实现，我稍后会给你详细介绍。这里呢，我们先来看看，这个函数会被哪些函数调用，这样我们就可以了解 AOF 重写的触发时机了。\n\n实际上，rewriteAppendOnlyFileBackground 函数一共会在三个函数中被调用。\n\n**第一个是 bgrewriteaofCommand 函数。**这个函数是在 aof.c 文件中实现的，对应了我们在 Redis server 上执行 bgrewriteaof 命令，也就是说，我们手动触发了 AOF rewrite 的执行。\n\n不过，即使我们手动执行了 bgrewriteaof 命令，bgrewriteaofCommand 函数也会根据以下两个条件，来判断是否实际执行 AOF 重写。\n\n * **条件一：当前是否已经有 AOF 重写的子进程正在执行。**如果有的话，那么 bgrewriteaofCommand 函数就不再执行 AOF 重写了。\n * **条件二：当前是否有创建 RDB 的子进程正在执行。**如果有的话，bgrewriteaofCommand 函数会把全局变量 server 的 aof_rewrite_scheduled 成员变量设置为 1，这个标志表明 Redis server 已经将 AOF 重写设为待调度运行，等后续条件满足时，它就会实际执行 AOF 重写（我们一会儿就会看到，当 aof_rewrite_scheduled 设置为 1 以后，Redis server 会在哪些条件下实际执行重写操作）。\n\n所以这也就是说，只有当前既没有 AOF 重写子进程也没有 RDB 子进程，bgrewriteaofCommand 函数才会立即调用 rewriteAppendOnlyFileBackground 函数，实际执行 AOF 重写。\n\n以下代码展示了 bgrewriteaofCommand 函数的基本执行逻辑，你可以看下。\n\nvoid bgrewriteaofCommand(client *c) {\n    if (server.aof_child_pid != -1) {\n        .. //有AOF重写子进程，因此不执行重写\n    } else if (server.rdb_child_pid != -1) {\n        server.aof_rewrite_scheduled = 1; //有RDB子进程，将AOF重写设置为待调度运行\n        ...\n    } else if (rewriteAppendOnlyFileBackground() == C_OK) { //实际执行AOF重写\n        ...\n    }\n    ...\n}\n\n\n**第二个是 startAppendOnly 函数。**这个函数也是在 aof.c 文件中实现的，它本身会被 configSetCommand 函数（在config.c文件中）和 restartAOFAfterSYNC 函数（在replication.c文件中）调用。\n\n首先，对于 configSetCommand 函数来说，它对应了我们在 Redis 中执行 config 命令启用 AOF 功能，如下所示：\n\nconfig set appendonly yes\n\n\n这样，一旦 AOF 功能启用后，configSetCommand 函数就会调用 startAppendOnly 函数，执行一次 AOF 重写。\n\n而对于 restartAOFAfterSYNC 函数来说，它会在主从节点的复制过程中被调用。简单来说，就是当主从节点在进行复制时，如果从节点的 AOF 选项被打开，那么在加载解析 RDB 文件时，AOF 选项就会被关闭。然后，无论从节点是否成功加载了 RDB 文件，restartAOFAfterSYNC 函数都会被调用，用来恢复被关闭的 AOF 功能。\n\n那么在这个过程中，restartAOFAfterSYNC 函数就会调用 startAppendOnly 函数，并进一步调用 rewriteAppendOnlyFileBackground 函数，来执行一次 AOF 重写。\n\n这里你要注意，和 bgrewriteaofCommand 函数类似，startAppendOnly 函数也会判断当前是否有 RDB 子进程在执行，如果有的话，它会将 AOF 重写设置为待调度执行。除此之外，如果 startAppendOnly 函数检测到有 AOF 重写子进程在执行，那么它就会把该子进程先 kill 掉，然后再调用 rewriteAppendOnlyFileBackground 函数进行 AOF 重写。\n\n所以到这里，我们其实可以发现，无论是 bgrewriteaofCommand 函数还是 startAppendOnly 函数，当它们检测到有 RDB 子进程在执行的时候，就会把 aof_rewrite_scheduled 变量设置为 1，这表示 AOF 重写操作将在条件满足时再被执行。\n\n**那么，Redis server 什么时候会再检查 AOF 重写操作的条件是否满足呢？**这就和 rewriteAppendOnlyFileBackground 函数被调用的第三个函数，serverCron 函数相关了。\n\n**第三个是 serverCron 函数。**在 Redis server 运行时，serverCron 函数是会被周期性执行的。然后它在执行的过程中，会做两次判断来决定是否执行 AOF 重写。\n\n首先，serverCron 函数会检测当前是否没有 RDB 子进程和 AOF 重写子进程在执行，并检测是否有 AOF 重写操作被设置为了待调度执行，也就是 aof_rewrite_scheduled 变量值为 1。\n\n如果这三个条件都满足，那么 serverCron 函数就会调用 rewriteAppendOnlyFileBackground 函数来执行 AOF 重写。serverCron 函数里面的这部分执行逻辑如下所示：\n\n//如果没有 RDB 子进程，也没有 AOF 重写子进程，并且 AOF 重写被设置为待调度执行，那么调用 rewriteAppendOnlyFileBackground 函数进行 AOF 重写\n\n//如果没有RDB子进程，也没有AOF重写子进程，并且AOF重写被设置为待调度执行，那么调用rewriteAppendOnlyFileBackground函数进行AOF重写\nif (server.rdb_child_pid == -1 && server.aof_child_pid == -1 &&\n        server.aof_rewrite_scheduled)\n{\n        rewriteAppendOnlyFileBackground();\n}\n\n\n事实上，这里的代码也回答了我们刚才提到的问题：待调度执行的 AOF 重写会在什么时候执行？\n\n其实，如果 AOF 重写没法立即执行的话，我们也不用担心。因为只要 aof_rewrite_scheduled 变量被设置为 1 了，那么 serverCron 函数就默认会每 100 毫秒执行并检测这个变量值。所以，如果正在执行的 RDB 子进程和 AOF 重写子进程结束了之后，被调度执行的 AOF 重写就可以很快得到执行。\n\n其次，即使 AOF 重写操作没有被设置为待调度执行，serverCron 函数也会周期性判断是否需要执行 AOF 重写。这里的判断条件主要有三个，分别是 AOF 功能已启用、AOF 文件大小比例超出阈值，以及 AOF 文件大小绝对值超出阈值。\n\n这样一来，当这三个条件都满足时，并且也没有 RDB 子进程和 AOF 子进程在运行的话，此时，serverCron 函数就会调用 rewriteAppendOnlyFileBackground 函数执行 AOF 重写。这部分的代码逻辑如下所示：\n\n//如果 AOF 功能启用、没有 RDB 子进程和 AOF 重写子进程在执行、AOF 文件大小比例设定了阈值，以及 AOF 文件大小绝对值超出了阈值，那么，进一步判断 AOF 文件大小比例是否超出阈值\n\n//如果AOF功能启用、没有RDB子进程和AOF重写子进程在执行、AOF文件大小比例设定了阈值，以及AOF文件大小绝对值超出了阈值，那么，进一步判断AOF文件大小比例是否超出阈值\nif (server.aof_state == AOF_ON && server.rdb_child_pid == -1 && server.aof_child_pid == -1 && server.aof_rewrite_perc && server.aof_current_size > server.aof_rewrite_min_size) {\n   //计算AOF文件当前大小超出基础大小的比例\n   long long base = server.aof_rewrite_base_size ? server.aof_rewrite_base_size : 1;\n   long long growth = (server.aof_current_size*100/base) - 100;\n   //如果AOF文件当前大小超出基础大小的比例已经超出预设阈值，那么执行AOF重写\n   if (growth >= server.aof_rewrite_perc) {\n      ...\n      rewriteAppendOnlyFileBackground();\n   }\n}\n\n\n那么，从这里的代码中，你会看到，为了避免 AOF 文件过大导致占用过多的磁盘空间，以及增加恢复时长，你其实可以通过设置 redis.conf 文件中的以下两个阈值，来让 Redis server 自动重写 AOF 文件。\n\n * auto-aof-rewrite-percentage：AOF 文件大小超出基础大小的比例，默认值为 100%，即超出 1 倍大小。\n * auto-aof-rewrite-min-size：AOF 文件大小绝对值的最小值，默认为 64MB。\n\n好了，到这里，我们就了解了 AOF 重写的四个触发时机，这里我也给你总结下，方便你回顾复习。\n\n * 时机一：bgrewriteaof 命令被执行。\n * 时机二：主从复制完成 RDB 文件解析和加载（无论是否成功）。\n * 时机三：AOF 重写被设置为待调度执行。\n * 时机四：AOF 被启用，同时 AOF 文件的大小比例超出阈值，以及 AOF 文件的大小绝对值超出阈值。\n\n另外，这里你还需要注意，在这四个时机下，其实都不能有正在执行的 RDB 子进程和 AOF 重写子进程，否则的话，AOF 重写就无法执行了。\n\n所以接下来，我们就来学习下 AOF 重写的基本执行过程。\n\n\n# AOF 重写的基本过程\n\n首先，我们再来看下刚才介绍的 rewriteAppendOnlyFileBackground 函数。这个函数的主体逻辑比较简单，一方面，它会通过调用 fork 函数创建一个子进程，然后在子进程中调用 rewriteAppendOnlyFile 函数进行 AOF 文件重写。\n\nrewriteAppendOnlyFile 函数是在 aof.c 文件中实现的。它主要会调用 rewriteAppendOnlyFileRio 函数（在 aof.c 文件中）来完成 AOF 日志文件的重写。具体来说，就是 rewriteAppendOnlyFileRio 函数会遍历 Redis server 的每一个数据库，把其中的每个键值对读取出来，然后记录该键值对类型对应的插入命令，以及键值对本身的内容。\n\n比如，如果读取的是一个 String 类型的键值对，那么 rewriteAppendOnlyFileRio 函数，就会记录 SET 命令和键值对本身内容；而如果读取的是 Set 类型键值对，那么它会记录 SADD 命令和键值对内容。这样一来，当需要恢复 Redis 数据库时，我们重新执行一遍 AOF 重写日志中记录的命令操作，就可以依次插入所有键值对了。\n\n另一方面，在父进程中，这个 rewriteAppendOnlyFileBackground 函数会把 aof_rewrite_scheduled 变量设置为 0，同时记录 AOF 重写开始的时间，以及记录 AOF 子进程的进程号。\n\n此外，rewriteAppendOnlyFileBackground 函数还会调用 updateDictResizePolicy 函数，禁止在 AOF 重写期间进行 rehash 操作。这是因为 rehash 操作会带来较多的数据移动操作，对于 AOF 重写子进程来说，这就意味着父进程中的内存修改会比较多。因此，AOF 重写子进程就需要执行更多的写时复制，进而完成 AOF 文件的写入，这就会给 Redis 系统的性能造成负面影响。\n\n以下代码就展示了 rewriteAppendOnlyFileBackground 函数的基本执行逻辑，你可以看下。\n\nint rewriteAppendOnlyFileBackground(void) {\n   ...\n   if ((childpid = fork()) == 0) {  //创建子进程\n      ...\n      //子进程调用rewriteAppendOnlyFile进行AOF重写\n      if (rewriteAppendOnlyFile(tmpfile) == C_OK) {\n            size_t private_dirty = zmalloc_get_private_dirty(-1);\n            ...\n            exitFromChild(0);\n        } else {\n            exitFromChild(1);\n        }\n   }\n   else{ //父进程执行的逻辑\n      ...\n      server.aof_rewrite_scheduled = 0;\n      server.aof_rewrite_time_start = time(NULL);\n      server.aof_child_pid = childpid; //记录重写子进程的进程号\n      updateDictResizePolicy(); //关闭rehash功能\n}\n\n\n而从这里，你可以看到，AOF 重写和 RDB 创建是比较类似的，它们都会创建一个子进程来遍历所有的数据库，并把数据库中的每个键值对记录到文件中。不过，AOF 重写和 RDB 文件又有两个不同的地方：\n\n * 一是，AOF 文件中是以“命令 + 键值对”的形式，来记录每个键值对的插入操作，而 RDB 文件记录的是键值对数据本身；\n * 二是，在 AOF 重写或是创建 RDB 的过程中，主进程仍然可以接收客户端写请求。不过，因为 RDB 文件只需要记录某个时刻下数据库的所有数据就行，而 AOF 重写则需要尽可能地把主进程收到的写操作，也记录到重写的日志文件中。所以，AOF 重写子进程就需要有相应的机制来和主进程进行通信，以此来接收主进程收到的写操作。\n\n下图就展示了 rewriteAppendOnlyFileBackground 函数执行的基本逻辑、主进程和 AOF 重写子进程各自执行的内容，以及主进程和子进程间的通信过程，你可以再来整体回顾下。\n\n\n\n到这里，我们就大概掌握了 AOF 重写的基本执行过程。但是在这里，你可能还会有疑问，比如说，AOF 重写的子进程和父进程，它们之间的通信过程是怎么样的呢？\n\n其实，这个通信过程是通过操作系统的管道机制（pipe）来实现的\n\n在 AOF 重写时，主进程仍然在接收客户端写操作，那么这些新写操作会记录到 AOF 重写日志中吗？如果需要记录的话，重写子进程又是通过什么方式向主进程获取这些写操作的呢？\n\necho 接下来就带你了解下 AOF 重写过程中所使用的管道机制，以及主进程和重写子进程的交互过程\n\n * 一方面，你就可以了解 AOF 重写日志包含的写操作的完整程度，当你要使用 AOF 日志恢复 Redis 数据库时，就知道 AOF 能恢复到的程度是怎样的\n * 一方面，因为 AOF 重写子进程就是通过操作系统提供的管道机制，来和 Redis 主进程交互的，所以学完这节课之后，你还可以掌握管道技术，从而用来实现进程间的通信\n\n好了，接下来，我们就先来了解下管道机制\n\n\n# 深入重写缓冲区\n\n\n# 如何使用管道进行父子进程间通信？\n\n首先我们要知道，当进程 A 通过调用 fork 函数创建一个子进程 B，然后进程 A 和 B 要进行通信时，我们通常都需要依赖操作系统提供的通信机制，而管道（pipe）就是一种用于父子进程间通信的常用机制。\n\n具体来说，管道机制在操作系统内核中创建了一块缓冲区，父进程 A 可以打开管道，并往这块缓冲区中写入数据。同时，子进程 B 也可以打开管道，从这块缓冲区中读取数据。这里，你需要注意的是，进程每次往管道中写入数据时，只能追加写到缓冲区中当前数据所在的尾部，而进程每次从管道中读取数据时，只能从缓冲区的头部读取数据。\n\n其实，管道创建的这块缓冲区就像一个先进先出的队列一样，写数据的进程写到队列尾部，而读数据的进程则从队列头读取。下图就展示了两个进程使用管道进行数据通信的过程，你可以看下。\n\n\n\n好了，了解了管道的基本功能后，我们再来看下使用管道时需要注意的一个关键点。管道中的数据在一个时刻只能向一个方向流动，这也就是说，如果父进程 A 往管道中写入了数据，那么此时子进程 B 只能从管道中读取数据。类似的，如果子进程 B 往管道中写入了数据，那么此时父进程 A 只能从管道中读取数据。而如果父子进程间需要同时进行数据传输通信，我们就需要创建两个管道了。\n\n下面，我们就来看下怎么用代码实现管道通信。这其实是和操作系统提供的管道的系统调用 pipe 有关，pipe 的函数原型如下所示：\n\nint pipe(int pipefd[2]);\n\n\n你可以看到，pipe 的参数是一个数组 pipefd，表示的是管道的文件描述符。这是因为进程在往管道中写入或读取数据时，其实是使用 write 或 read 函数的，而 write 和 read 函数需要通过文件描述符才能进行写数据和读数据操作。\n\n数组 pipefd 有两个元素 pipefd[0]和 pipefd[1]，分别对应了管道的读描述符和写描述符。这也就是说，当进程需要从管道中读数据时，就需要用到 pipefd[0]，而往管道中写入数据时，就使用 pipefd[1]。\n\n这里我写了一份示例代码，展示了父子进程如何使用管道通信，你可以看下。\n\nint main()\n{\n    int fd[2], nr = 0, nw = 0;\n    char buf[128];\n    pipe(fd);\n    pid = fork();\n\n  if(pid == 0) {\n      //子进程调用read从fd[0]描述符中读取数据\n        printf("child process wait for message\\n");\n        nr = read(fds[0], buf, sizeof(buf))\n        printf("child process receive %s\\n", buf);\n  }else{\n       //父进程调用write往fd[1]描述符中写入数据\n        printf("parent process send message\\n");\n        strcpy(buf, "Hello from parent");\n        nw = write(fd[1], buf, sizeof(buf));\n        printf("parent process send %d bytes to child.\\n", nw);\n    }\n    return 0;\n}\n\n\n从代码中，你可以看到，在父子进程进行管道通信前，我们需要在代码中定义用于保存读写描述符的数组 fd，然后调用 pipe 系统创建管道，并把数组 fd 作为参数传给 pipe 函数。紧接着，在父进程的代码中，父进程会调用 write 函数往管道文件描述符 fd[1]中写入数据，另一方面，子进程调用 read 函数从管道文件描述符 fd[0]中读取数据。\n\n这里，为了便于你理解，我也画了一张图，你可以参考。\n\n\n\n好了，现在你就了解了如何使用管道来进行父子进程的通信了。那么下面，我们就来看下在 AOF 重写过程中，重写子进程是如何用管道和主进程（也就是它的父进程）进行通信的。\n\n\n# AOF 重写子进程如何使用管道和父进程交互？\n\n我们先来看下在 AOF 重写过程中，都创建了几个管道。\n\n这实际上是 AOF 重写函数 rewriteAppendOnlyFileBackground 在执行过程中，通过调用 aofCreatePipes 函数来完成的，如下所示：\n\nint rewriteAppendOnlyFileBackground(void) {\n…\nif (aofCreatePipes() != C_OK) return C_ERR;\n…\n}\n\n\n这个 aofCreatePipes 函数是在aof.c文件中实现的，它的逻辑比较简单，可以分成三步。\n\n第一步，aofCreatePipes 函数创建了包含 6 个文件描述符元素的数组 fds。就像我刚才给你介绍的，每一个管道会对应两个文件描述符，所以，数组 fds 其实对应了 AOF 重写过程中要用到的三个管道。紧接着，aofCreatePipes 函数就调用 pipe 系统调用函数，分别创建三个管道。\n\n这部分代码如下所示，你可以看下。\n\nint aofCreatePipes(void) {\n    int fds[6] = {-1, -1, -1, -1, -1, -1};\n    int j;\n    if (pipe(fds) == -1) goto error; /* parent -> children data. */\n    if (pipe(fds+2) == -1) goto error; /* children -> parent ack. */\n  if (pipe(fds+4) == -1) goto error;\n  …}\n}\n\n\n第二步，aofCreatePipes 函数会调用 anetNonBlock 函数（在anet.c文件中），将 fds\n\n数组的第一和第二个描述符（fds[0]和 fds[1]）对应的管道设置为非阻塞。然后，aofCreatePipes 函数会调用 aeCreateFileEvent 函数，在数组 fds 的第三个描述符 (fds[2]) 上注册了读事件的监听，对应的回调函数是 aofChildPipeReadable。aofChildPipeReadable 函数也是在 aof.c 文件中实现的，我稍后会给你详细介绍它。\n\nint aofCreatePipes(void) {\n…\nif (anetNonBlock(NULL,fds[0]) != ANET_OK) goto error;\nif (anetNonBlock(NULL,fds[1]) != ANET_OK) goto error;\nif (aeCreateFileEvent(server.el, fds[2], AE_READABLE, aofChildPipeReadable, NULL) == AE_ERR) goto error;\n…\n}\n\n\n这样，在完成了管道创建、管道设置和读事件注册后，最后一步，aofCreatePipes 函数会将数组 fds 中的六个文件描述符，分别复制给 server 变量的成员变量，如下所示：\n\nint aofCreatePipes(void) {\n…\nserver.aof_pipe_write_data_to_child = fds[1];\nserver.aof_pipe_read_data_from_parent = fds[0];\nserver.aof_pipe_write_ack_to_parent = fds[3];\nserver.aof_pipe_read_ack_from_child = fds[2];\nserver.aof_pipe_write_ack_to_child = fds[5];\nserver.aof_pipe_read_ack_from_parent = fds[4];\n…\n}\n\n\n在这一步中，我们就可以从 server 变量的成员变量名中，看到 aofCreatePipes 函数创建的三个管道，以及它们各自的用途。\n\n * fds[0]和 fds[1]：对应了主进程和重写子进程间用于传递操作命令的管道，它们分别对应读描述符和写描述符。\n * fds[2]和 fds[3]：对应了重写子进程向父进程发送 ACK 信息的管道，它们分别对应读描述符和写描述符。\n * fds[4]和 fds[5]：对应了父进程向重写子进程发送 ACK 信息的管道，它们分别对应读描述符和写描述符。\n\n下图也展示了 aofCreatePipes 函数的基本执行流程，你可以再回顾下。\n\n\n\n好了，了解了 AOF 重写过程中的管道个数和用途后，下面我们再来看下这些管道具体是如何使用的。\n\n# 操作命令传输管道的使用\n\n实际上，当 AOF 重写子进程在执行时，主进程还会继续接收和处理客户端写请求。这些写操作会被主进程正常写入 AOF 日志文件，这个过程是由 feedAppendOnlyFile 函数（在 aof.c 文件中）来完成。\n\nfeedAppendOnlyFile 函数在执行的最后一步，会判断当前是否有 AOF 重写子进程在运行。如果有的话，它就会调用 aofRewriteBufferAppend 函数（在 aof.c 文件中），如下所示：\n\nif (server.aof_child_pid != -1)\n        aofRewriteBufferAppend((unsigned char*)buf,sdslen(buf));\n\n\naofRewriteBufferAppend 函数的作用是将参数 buf，追加写到全局变量 server 的 aof_rewrite_buf_blocks 这个列表中。\n\n这里，你需要注意的是，参数 buf 是一个字节数组，feedAppendOnlyFile 函数会将主进程收到的命令操作写入到 buf 中。而 aof_rewrite_buf_blocks 列表中的每个元素是 aofrwblock 结构体类型，这个结构体中包括了一个字节数组，大小是 AOF_RW_BUF_BLOCK_SIZE，默认值是 10MB。此外，aofrwblock 结构体还记录了字节数组已经使用的空间和剩余可用的空间。\n\n以下代码展示了 aofrwblock 结构体的定义，你可以看下。\n\ntypedef struct aofrwblock {\n    unsigned long used, free; //buf数组已用空间和剩余可用空间\n    char buf[AOF_RW_BUF_BLOCK_SIZE]; //宏定义AOF_RW_BUF_BLOCK_SIZE默认为10MB\n} aofrwblock;\n\n\n这样一来，aofrwblock 结构体就相当于是一个 10MB 的数据块，记录了 AOF 重写期间主进程收到的命令，而 aof_rewrite_buf_blocks 列表负责将这些数据块连接起来。当 aofRewriteBufferAppend 函数执行时，它会从 aof_rewrite_buf_blocks 列表中取出一个 aofrwblock 类型的数据块，用来记录命令操作。\n\n当然，如果当前数据块中的空间不够保存参数 buf 中记录的命令操作，那么 aofRewriteBufferAppend 函数就会再分配一个 aofrwblock 数据块。\n\n好了，当 aofRewriteBufferAppend 函数将命令操作记录到 aof_rewrite_buf_blocks 列表中之后，它还会检查 aof_pipe_write_data_to_child 管道描述符上是否注册了写事件，这个管道描述符就对应了我刚才给你介绍的 fds[1]。\n\n如果没有注册写事件，那么 aofRewriteBufferAppend 函数就会调用 aeCreateFileEvent 函数，注册一个写事件，这个写事件会监听 aof_pipe_write_data_to_child 这个管道描述符，也就是主进程和重写子进程间的操作命令传输管道。\n\n当这个管道可以写入数据时，写事件对应的回调函数 aofChildWriteDiffData（在 aof.c 文件中）就会被调用执行。这个过程你可以参考下面的代码：\n\nvoid aofRewriteBufferAppend(unsigned char *s, unsigned long len) {\n...\n//检查aof_pipe_write_data_to_child描述符上是否有事件\nif (aeGetFileEvents(server.el,server.aof_pipe_write_data_to_child) == 0) {\n     //如果没有注册事件，那么注册一个写事件，回调函数是aofChildWriteDiffData\n     aeCreateFileEvent(server.el, server.aof_pipe_write_data_to_child,\n            AE_WRITABLE, aofChildWriteDiffData, NULL);\n}\n...}\n\n\n其实，刚才我介绍的写事件回调函数 aofChildWriteDiffData，它的主要作用是从 aof_rewrite_buf_blocks 列表中逐个取出数据块，然后通过 aof_pipe_write_data_to_child 管道描述符，将数据块中的命令操作通过管道发给重写子进程，这个过程如下所示：\n\nvoid aofChildWriteDiffData(aeEventLoop *el, int fd, void *privdata, int mask) {\n...\nwhile(1) {\n   //从aof_rewrite_buf_blocks列表中取出数据块\n   ln = listFirst(server.aof_rewrite_buf_blocks);\n   block = ln ? ln->value : NULL;\n   if (block->used > 0) {\n      //调用write将数据块写入主进程和重写子进程间的管道\n      nwritten = write(server.aof_pipe_write_data_to_child,\n                             block->buf,block->used);\n      if (nwritten <= 0) return;\n            ...\n        }\n ...}}\n\n\n好了，这样一来，你就了解了主进程其实是在正常记录 AOF 日志时，将收到的命令操作写入 aof_rewrite_buf_blocks 列表中的数据块，然后再通过 aofChildWriteDiffData 函数将记录的命令操作通过主进程和重写子进程间的管道发给子进程。\n\n下图也展示了这个过程，你可以再来回顾下。\n\n\n\n然后，我们接着来看下重写子进程，是如何从管道中读取父进程发送的命令操作的。\n\n这实际上是由 aofReadDiffFromParent 函数（在 aof.c 文件中）来完成的。这个函数会使用一个 64KB 大小的缓冲区，然后调用 read 函数，读取父进程和重写子进程间的操作命令传输管道中的数据。以下代码也展示了 aofReadDiffFromParent 函数的基本执行流程，你可以看下。\n\nssize_t aofReadDiffFromParent(void) {\n    char buf[65536]; //管道默认的缓冲区大小\n    ssize_t nread, total = 0;\n    //调用read函数从aof_pipe_read_data_from_parent中读取数据\n    while ((nread =\n      read(server.aof_pipe_read_data_from_parent,buf,sizeof(buf))) > 0) {\n        server.aof_child_diff = sdscatlen(server.aof_child_diff,buf,nread);\n        total += nread;\n    }\n    return total;\n}\n\n\n那么，从代码中，你可以看到 aofReadDiffFromParent 函数会通过 aof_pipe_read_data_from_parent 描述符读取数据。然后，它会将读取的操作命令追加到全局变量 server 的 aof_child_diff 字符串中。而在 AOF 重写函数 rewriteAppendOnlyFile 的执行过程最后，aof_child_diff 字符串会被写入 AOF 重写日志文件，以便我们在使用 AOF 重写日志时，能尽可能地恢复重写期间收到的操作。\n\n这个 aof_child_diff 字符串写入重写日志文件的过程，你可以参考下面给出的代码：\n\nint rewriteAppendOnlyFile(char *filename) {\n...\n//将aof_child_diff中累积的操作命令写入AOF重写日志文件\nif (rioWrite(&aof,server.aof_child_diff,sdslen(server.aof_child_diff)) == 0)\n        goto werr;\n...\n}\n\n\n所以也就是说，aofReadDiffFromParent 函数实现了重写子进程向主进程读取操作命令。那么在这里，我们还需要搞清楚的问题是：aofReadDiffFromParent 函数会在哪里被调用，也就是重写子进程会在什么时候从管道中读取主进程收到的操作。\n\n其实，aofReadDiffFromParent 函数一共会被以下三个函数调用。\n\n * rewriteAppendOnlyFileRio 函数：这个函数是由重写子进程执行的，它负责遍历 Redis 每个数据库，生成 AOF 重写日志，在这个过程中，它会不时地调用 aofReadDiffFromParent 函数。\n * rewriteAppendOnlyFile 函数：这个函数是重写日志的主体函数，也是由重写子进程执行的，它本身会调用 rewriteAppendOnlyFileRio 函数。此外，它在调用完 rewriteAppendOnlyFileRio 函数后，还会多次调用 aofReadDiffFromParent 函数，以尽可能多地读取主进程在重写日志期间收到的操作命令。\n * rdbSaveRio 函数：这个函数是创建 RDB 文件的主体函数。当我们使用 AOF 和 RDB 混合持久化机制时，这个函数也会调用 aofReadDiffFromParent 函数。\n\n从这里，我们可以看到，Redis 源码在实现 AOF 重写过程中，其实会多次让重写子进程向主进程读取新收到的操作命令，这也是为了让重写日志尽可能多地记录最新的操作，提供更加完整的操作记录。\n\n最后，我们再来看下重写子进程和主进程间用来传递 ACK 信息的两个管道的使用。\n\n# ACK 管道的使用\n\n刚才在介绍主进程调用 aofCreatePipes 函数创建管道时，你就了解到了，主进程会在 aof_pipe_read_ack_from_child 管道描述符上注册读事件。这个描述符对应了重写子进程向主进程发送 ACK 信息的管道。同时，这个描述符是一个读描述符，表示主进程从管道中读取 ACK 信息。\n\n其实，重写子进程在执行 rewriteAppendOnlyFile 函数时，这个函数在完成日志重写，以及多次向父进程读取操作命令后，就会调用 write 函数，向 aof_pipe_write_ack_to_parent 描述符对应的管道中写入“！”，这就是重写子进程向主进程发送 ACK 信号，让主进程停止发送收到的新写操作。这个过程如下所示：\n\nint rewriteAppendOnlyFile(char *filename) {\n...\nif (write(server.aof_pipe_write_ack_to_parent,"!",1) != 1) goto werr;\n...}\n\n\n一旦重写子进程向主进程发送 ACK 信息的管道中有了数据，aof_pipe_read_ack_from_child 管道描述符上注册的读事件就会被触发，也就是说，这个管道中有数据可以读取了。那么，aof_pipe_read_ack_from_child 管道描述符上，注册的回调函数 aofChildPipeReadable（在 aof.c 文件中）就会执行。\n\n这个函数会判断从 aof_pipe_read_ack_from_child 管道描述符读取的数据是否是“！”，如果是的话，那它就会调用 write 函数，往 aof_pipe_write_ack_to_child 管道描述符上写入“！”，表示主进程已经收到重写子进程发送的 ACK 信息，同时它会给重写子进程回复一个 ACK 信息。这个过程如下所示：\n\nvoid aofChildPipeReadable(aeEventLoop *el, int fd, void *privdata, int mask) {\n...\nif (read(fd,&byte,1) == 1 && byte == \'!\') {\n   ...\n   if (write(server.aof_pipe_write_ack_to_child,"!",1) != 1) { ...}\n}\n...\n}\n\n\n好了，到这里，我们就了解了，重写子进程在完成日志重写后，是先给主进程发送 ACK 信息。然后主进程在 aof_pipe_read_ack_from_child 描述符上监听读事件发生，并调用 aofChildPipeReadable 函数向子进程发送 ACK 信息。\n\n最后，重写子进程执行的 rewriteAppendOnlyFile 函数，会调用 syncRead 函数，从 aof_pipe_read_ack_from_parent 管道描述符上，读取主进程发送给它的 ACK 信息，如下所示：\n\nint rewriteAppendOnlyFile(char *filename) {\n...\nif (syncRead(server.aof_pipe_read_ack_from_parent,&byte,1,5000) != 1  || byte != \'!\') goto werr\n...\n}\n\n\n下图也展示了 ACK 管道的使用过程，你可以再回顾下。\n\n\n\n这样一来，重写子进程和主进程之间就通过两个 ACK 管道，相互确认重写过程结束了。\n\n\n# 总结\n\n 1. AOF 重写的触发时机。这既包括了我们主动执行 bgrewriteaof 命令，也包括了 Redis server 根据 AOF 文件大小而自动触发的重写。此外，在主从复制的过程中，从节点也会启动 AOF 重写，形成一份完整的 AOF 日志，以便后续进行恢复。当然你也要知道，当要触发 AOF 重写时，Redis server 是不能运行 RDB 子进程和 AOF 重写子进程的。\n\n 2. AOF 重写的基本执行过程。AOF 重写和 RDB 创建的过程类似，它也是创建了一个子进程来完成重写工作。这是因为 AOF 重写操作，实际上需要遍历 Redis server 上的所有数据库，把每个键值对以插入操作的形式写入日志文件，而日志文件又要进行写盘操作。所以，Redis 源码使用子进程来实现 AOF 重写，这就避免了阻塞主线程，也减少了对 Redis 整体性能的影响。\n\n 3. 注管道机制的使用\n\n 4. 主进程和重写子进程使用管道通信的过程\n\n在这个过程中，AOF 重写子进程和主进程是使用了一个操作命令传输管道和两个 ACK 信息发送管道。操作命令传输管道是用于主进程写入收到的新操作命令，以及用于重写子进程读取操作命令，而 ACK 信息发送管道是在重写结束时，重写子进程和主进程用来相互确认重写过程的结束。最后，重写子进程会进一步将收到的操作命令记录到重写日志文件中。\n\n这样一来，AOF 重写过程中主进程收到的新写操作，就不会被遗漏了\n\n * 一方面，这些新写操作会被记录在正常的 AOF 日志中\n * 一方面，主进程会将新写操作缓存在 aof_rewrite_buf_blocks 数据块列表中，并通过管道发送给重写子进程。这样，就能尽可能地保证重写日志具有最新、最完整的写操作了\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. aof 重写会在什么条件下触发？这些条件如何影响 redis 性能？\n 2. 在 aof 重写时，redis 如何确保新的写操作不会丢失？\n 3. 为什么 redis 要在 aof 重写中使用管道通信？这个机制解决了哪些问题？\n 4. 即使操作系统没宕机，为什么在 aof 模式下仍可能存在数据丢失风险？\n 5. aof 重写如何确保写入日志的完整性？子进程是怎么处理新的写操作的？\n 6. 在什么情况下，aof 重写可能会导致 redis 性能变慢？\n 7. always 模式下，数据为什么还可能丢失？如何平衡 aof 的性能与安全？\n 8. 当 redis 变慢时，如何判断是否是 aof 重写导致的？\n\n\n# 前言\n\n我们知道，redis 除了使用内存快照 rdb 来保证数据可靠性之外，还可以使用 aof 日志。不过，rdb 文件是将某一时刻的内存数据保存成一个文件，而 aof 日志则会记录接收到的所有写操作。如果 redis server 的写请求很多，那么 aof 日志中记录的操作也会越来越多，进而就导致 aof 日志文件越来越大。\n\n然后，为了避免产生过大的 aof 日志文件，redis 会对 aof 文件进行重写，也就是针对当前数据库中每个键值对的最新内容，记录它的插入操作，而不再记录它的历史写操作了。这样一来，重写后的 aof 日志文件就能变小了。\n\n那么，aof 重写在哪些时候会被触发呢？以及 aof 重写需要写文件，这个过程会阻塞 redis 的主线程，进而影响 redis 的性能吗？\n\necho 接下来就给你介绍下 aof 核心流程以及重写的实现过程，通过了解它的实现，我们就可以清楚地了解到 aof 重写过程的表现，以及它对 redis server 的影响。这样，当你再遇到 redis server 性能变慢的问题时，你就可以排查是否是 aof 重写导致的了。\n\n好，接下来，我们先来看下 aof 核心流程\n\n\n# aof 核心流程\n\naof 持久化分为三个步骤\n\n * 命令追加\n * 文件写入\n * 文件同步\n\n\n# 命令追加\n\n当 aof 持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的 aof buf 缓冲区的末尾\n\nstruct redisserver{\n\t...\n\t//aof 缓冲区\n\tsds aof_buf;\n\t...\n}\n\n\n\n# 写入与同步\n\nredis 的服务器进程就是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行像 servercron 函数这样需要定时运行的函数\n\n因为服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到 aof buf 缓冲区里面，所以在服务器每次结束一个事件循环之前，它都会调用 flushappendonlyfile 函数，考虑是否需要将 aof buf 缓冲区中的内容写入和保存到 aof 文件里面，这个过程的伪代码如下\n\ndef eventloop():\n\twhile true:\n        # 在处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中\n\t\tprocessfileevents()\n\t\tpricesstimeevents()\n        # 考虑是否将 aof_buf 中的内容写入和保存到 aof 文件里\n\t\tflushappendonlyfile();\n\n\nflushappendonlyfile 的行为由在 redis.conf 中的 appendfsync选项的值来决定\n\nappendfsync 选项的值   flushappendonlyfile 函数行为\nalways             将 aof 缓冲区中的所有内容写入并同步到 aof 文件。\neverysec           将 aof 缓冲区中的所有内容写入到 aof 文件，如果上次同步 aof 文件的时间距离现在超过一秒钟，那么再次对\n                   aof 文件进行同步，并且这个同步操作是由一个线程专门负责执行的。\nno                 将 aof 缓冲区中的所有内容写入到 aof 文件，但并不对 aof 文件进行同步，何时同步由操作系统来决定。\n\n默认值是 everysec\n\n文件的写入和同步\n\n为了提高文件的写入效率，在现代操作系统中，当用户调用 write 函数，将一些数据写入到文件的时候，操作系统通常会将写入数据暂时保存在一个内存缓冲区里面等到缓冲区的空间被填满、或者超过了指定的时限之后，才真正地将缓冲区中的数据写入到磁盘里面。 这种做法虽然提高了效率，但也为写入数据带来了安全问题，因为如果计算机发生停机，那么保存在内存缓冲区里面的写入数据将会丢失。 为此，系统提供了 fsync 和 fdatasync 两个同步函数，它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里面，从而确保写入数据的安全性\n\n如果这时 flushappendonlyfile 函数被调用，假设服务器当前 appendfsyne 选项的值为 everysec，并且距离上次同步 aof 文件已经超过一秒钟，那么服务器会先将 aof buf 中的内容写人到 aof 文件中，然后再对 aof 文件进行同步。\n\n\n# 持久化的效率和安全性\n\n服务器配置 appendfsync 选项的值直接决定 aof 持久化功能的效率和安全性。\n\n * 当 appendfsync 的值为 always 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 aof 文件，并且同步 aof 文件，所以 always 的效率是 appendfsync 选项三个值当中最慢的一个，但从安全性来说，always 也是最安全的，因为即使出现故障停机，aof 持久化也只会丢失一个事件循环中所产生的命令数据\n * 当 appendfsync 的值为 everysec 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 aof 文件，并且每隔一秒就要在子线程中对 aof 文件进行一次同步。从效率上来讲，everysec 模式足够快，并且就算出现故障停机，数据库也只丢失一秒钟的命令数据。\n * 当 appendfsync 的值为 no 时，服务器在每个事件循环都要将 aof buf 缓冲区中的所有内容写入到 aof 文件，至于何时对 aof 文件进行同步，则由操作系统控制。因为处于 no 模式下的 flushappendonlyfile 调用无须执行同步操作，所以该模式下的 aof 文件写入速度总是最快的，不过因为这种模式会在系统缓存中积累一段时间的写入数据，所以该模式的单次同步时长通常是三种模式中时间最长的。从平摊操作的角度来看，no 模式和 everysec 模式的效率类似，当出现故障停机时，使用 no 模式的服务器将丢失上次同步 aof 文件之后的所有写命令数据\n\n# 如果 redis 宕机了，操作系统没有宕机，会有数据丢失吗？\n\n不一定 因为写入了 系统缓存 操作系统如果刷盘成功 就不会有丢失，redis 宕机不影响操作系统刷盘\n\n# 为什么在 always 下也可能会丢失一个事件循环中所产生的数据？\n\n如果客户端没有收到 ok 响应，可能会丢失一条数据\n\n但是\n\n在 appendfsync 为 always 模式下，如果客户端收到 redis 返回的 ok 响应，意味着该命令的结果已经成功写入到 aof 文件并同步到磁盘\n\n * 在 always 模式下，redis 在每个命令执行后都会立刻将该命令追加到 aof_buf 缓冲区，然后立即将缓冲区中的内容写入 aof 文件，并执行 fsync 操作（即将数据同步到磁盘）\n * 当 fsync 成功完成后，redis 才会返回 ok 响应给客户端\n\n因此，在 always 模式下，如果客户端收到 ok 响应，意味着：\n\n 1. 命令已经被执行。\n 2. 命令的结果已经成功追加到 aof 文件。\n 3. aof 文件已经通过 fsync 操作将数据同步到了磁盘。\n\n正因为 always 模式确保每条命令在返回 ok 之前都已经被同步到磁盘，所以从客户端的视角来看，只要收到了 ok，就可以认为该数据已经被持久化到磁盘，不会因为 redis 崩溃或服务器断电等原因丢失\n\n\n# aof 文件载入\n\n\n\n 1. 创建一个不带网络连接的伪客户端\n 2. 从 aof 文件中分析并读取出一条写命令\n 3. 使用伪客户端执行被读出的写命令\n 4. 重复 2 3，知道所有写命令被处理完毕为止\n\n\n# aof 重写\n\n\n# aof 重写函数与触发时机\n\n首先，实现 aof 重写的函数是 rewriteappendonlyfilebackground，它是在aof.c文件中实现的。在这个函数中，会调用 fork 函数创建一个 aof 重写子进程，来实际执行重写操作。关于这个函数的具体实现，我稍后会给你详细介绍。这里呢，我们先来看看，这个函数会被哪些函数调用，这样我们就可以了解 aof 重写的触发时机了。\n\n实际上，rewriteappendonlyfilebackground 函数一共会在三个函数中被调用。\n\n**第一个是 bgrewriteaofcommand 函数。**这个函数是在 aof.c 文件中实现的，对应了我们在 redis server 上执行 bgrewriteaof 命令，也就是说，我们手动触发了 aof rewrite 的执行。\n\n不过，即使我们手动执行了 bgrewriteaof 命令，bgrewriteaofcommand 函数也会根据以下两个条件，来判断是否实际执行 aof 重写。\n\n * **条件一：当前是否已经有 aof 重写的子进程正在执行。**如果有的话，那么 bgrewriteaofcommand 函数就不再执行 aof 重写了。\n * **条件二：当前是否有创建 rdb 的子进程正在执行。**如果有的话，bgrewriteaofcommand 函数会把全局变量 server 的 aof_rewrite_scheduled 成员变量设置为 1，这个标志表明 redis server 已经将 aof 重写设为待调度运行，等后续条件满足时，它就会实际执行 aof 重写（我们一会儿就会看到，当 aof_rewrite_scheduled 设置为 1 以后，redis server 会在哪些条件下实际执行重写操作）。\n\n所以这也就是说，只有当前既没有 aof 重写子进程也没有 rdb 子进程，bgrewriteaofcommand 函数才会立即调用 rewriteappendonlyfilebackground 函数，实际执行 aof 重写。\n\n以下代码展示了 bgrewriteaofcommand 函数的基本执行逻辑，你可以看下。\n\nvoid bgrewriteaofcommand(client *c) {\n    if (server.aof_child_pid != -1) {\n        .. //有aof重写子进程，因此不执行重写\n    } else if (server.rdb_child_pid != -1) {\n        server.aof_rewrite_scheduled = 1; //有rdb子进程，将aof重写设置为待调度运行\n        ...\n    } else if (rewriteappendonlyfilebackground() == c_ok) { //实际执行aof重写\n        ...\n    }\n    ...\n}\n\n\n**第二个是 startappendonly 函数。**这个函数也是在 aof.c 文件中实现的，它本身会被 configsetcommand 函数（在config.c文件中）和 restartaofaftersync 函数（在replication.c文件中）调用。\n\n首先，对于 configsetcommand 函数来说，它对应了我们在 redis 中执行 config 命令启用 aof 功能，如下所示：\n\nconfig set appendonly yes\n\n\n这样，一旦 aof 功能启用后，configsetcommand 函数就会调用 startappendonly 函数，执行一次 aof 重写。\n\n而对于 restartaofaftersync 函数来说，它会在主从节点的复制过程中被调用。简单来说，就是当主从节点在进行复制时，如果从节点的 aof 选项被打开，那么在加载解析 rdb 文件时，aof 选项就会被关闭。然后，无论从节点是否成功加载了 rdb 文件，restartaofaftersync 函数都会被调用，用来恢复被关闭的 aof 功能。\n\n那么在这个过程中，restartaofaftersync 函数就会调用 startappendonly 函数，并进一步调用 rewriteappendonlyfilebackground 函数，来执行一次 aof 重写。\n\n这里你要注意，和 bgrewriteaofcommand 函数类似，startappendonly 函数也会判断当前是否有 rdb 子进程在执行，如果有的话，它会将 aof 重写设置为待调度执行。除此之外，如果 startappendonly 函数检测到有 aof 重写子进程在执行，那么它就会把该子进程先 kill 掉，然后再调用 rewriteappendonlyfilebackground 函数进行 aof 重写。\n\n所以到这里，我们其实可以发现，无论是 bgrewriteaofcommand 函数还是 startappendonly 函数，当它们检测到有 rdb 子进程在执行的时候，就会把 aof_rewrite_scheduled 变量设置为 1，这表示 aof 重写操作将在条件满足时再被执行。\n\n**那么，redis server 什么时候会再检查 aof 重写操作的条件是否满足呢？**这就和 rewriteappendonlyfilebackground 函数被调用的第三个函数，servercron 函数相关了。\n\n**第三个是 servercron 函数。**在 redis server 运行时，servercron 函数是会被周期性执行的。然后它在执行的过程中，会做两次判断来决定是否执行 aof 重写。\n\n首先，servercron 函数会检测当前是否没有 rdb 子进程和 aof 重写子进程在执行，并检测是否有 aof 重写操作被设置为了待调度执行，也就是 aof_rewrite_scheduled 变量值为 1。\n\n如果这三个条件都满足，那么 servercron 函数就会调用 rewriteappendonlyfilebackground 函数来执行 aof 重写。servercron 函数里面的这部分执行逻辑如下所示：\n\n//如果没有 rdb 子进程，也没有 aof 重写子进程，并且 aof 重写被设置为待调度执行，那么调用 rewriteappendonlyfilebackground 函数进行 aof 重写\n\n//如果没有rdb子进程，也没有aof重写子进程，并且aof重写被设置为待调度执行，那么调用rewriteappendonlyfilebackground函数进行aof重写\nif (server.rdb_child_pid == -1 && server.aof_child_pid == -1 &&\n        server.aof_rewrite_scheduled)\n{\n        rewriteappendonlyfilebackground();\n}\n\n\n事实上，这里的代码也回答了我们刚才提到的问题：待调度执行的 aof 重写会在什么时候执行？\n\n其实，如果 aof 重写没法立即执行的话，我们也不用担心。因为只要 aof_rewrite_scheduled 变量被设置为 1 了，那么 servercron 函数就默认会每 100 毫秒执行并检测这个变量值。所以，如果正在执行的 rdb 子进程和 aof 重写子进程结束了之后，被调度执行的 aof 重写就可以很快得到执行。\n\n其次，即使 aof 重写操作没有被设置为待调度执行，servercron 函数也会周期性判断是否需要执行 aof 重写。这里的判断条件主要有三个，分别是 aof 功能已启用、aof 文件大小比例超出阈值，以及 aof 文件大小绝对值超出阈值。\n\n这样一来，当这三个条件都满足时，并且也没有 rdb 子进程和 aof 子进程在运行的话，此时，servercron 函数就会调用 rewriteappendonlyfilebackground 函数执行 aof 重写。这部分的代码逻辑如下所示：\n\n//如果 aof 功能启用、没有 rdb 子进程和 aof 重写子进程在执行、aof 文件大小比例设定了阈值，以及 aof 文件大小绝对值超出了阈值，那么，进一步判断 aof 文件大小比例是否超出阈值\n\n//如果aof功能启用、没有rdb子进程和aof重写子进程在执行、aof文件大小比例设定了阈值，以及aof文件大小绝对值超出了阈值，那么，进一步判断aof文件大小比例是否超出阈值\nif (server.aof_state == aof_on && server.rdb_child_pid == -1 && server.aof_child_pid == -1 && server.aof_rewrite_perc && server.aof_current_size > server.aof_rewrite_min_size) {\n   //计算aof文件当前大小超出基础大小的比例\n   long long base = server.aof_rewrite_base_size ? server.aof_rewrite_base_size : 1;\n   long long growth = (server.aof_current_size*100/base) - 100;\n   //如果aof文件当前大小超出基础大小的比例已经超出预设阈值，那么执行aof重写\n   if (growth >= server.aof_rewrite_perc) {\n      ...\n      rewriteappendonlyfilebackground();\n   }\n}\n\n\n那么，从这里的代码中，你会看到，为了避免 aof 文件过大导致占用过多的磁盘空间，以及增加恢复时长，你其实可以通过设置 redis.conf 文件中的以下两个阈值，来让 redis server 自动重写 aof 文件。\n\n * auto-aof-rewrite-percentage：aof 文件大小超出基础大小的比例，默认值为 100%，即超出 1 倍大小。\n * auto-aof-rewrite-min-size：aof 文件大小绝对值的最小值，默认为 64mb。\n\n好了，到这里，我们就了解了 aof 重写的四个触发时机，这里我也给你总结下，方便你回顾复习。\n\n * 时机一：bgrewriteaof 命令被执行。\n * 时机二：主从复制完成 rdb 文件解析和加载（无论是否成功）。\n * 时机三：aof 重写被设置为待调度执行。\n * 时机四：aof 被启用，同时 aof 文件的大小比例超出阈值，以及 aof 文件的大小绝对值超出阈值。\n\n另外，这里你还需要注意，在这四个时机下，其实都不能有正在执行的 rdb 子进程和 aof 重写子进程，否则的话，aof 重写就无法执行了。\n\n所以接下来，我们就来学习下 aof 重写的基本执行过程。\n\n\n# aof 重写的基本过程\n\n首先，我们再来看下刚才介绍的 rewriteappendonlyfilebackground 函数。这个函数的主体逻辑比较简单，一方面，它会通过调用 fork 函数创建一个子进程，然后在子进程中调用 rewriteappendonlyfile 函数进行 aof 文件重写。\n\nrewriteappendonlyfile 函数是在 aof.c 文件中实现的。它主要会调用 rewriteappendonlyfilerio 函数（在 aof.c 文件中）来完成 aof 日志文件的重写。具体来说，就是 rewriteappendonlyfilerio 函数会遍历 redis server 的每一个数据库，把其中的每个键值对读取出来，然后记录该键值对类型对应的插入命令，以及键值对本身的内容。\n\n比如，如果读取的是一个 string 类型的键值对，那么 rewriteappendonlyfilerio 函数，就会记录 set 命令和键值对本身内容；而如果读取的是 set 类型键值对，那么它会记录 sadd 命令和键值对内容。这样一来，当需要恢复 redis 数据库时，我们重新执行一遍 aof 重写日志中记录的命令操作，就可以依次插入所有键值对了。\n\n另一方面，在父进程中，这个 rewriteappendonlyfilebackground 函数会把 aof_rewrite_scheduled 变量设置为 0，同时记录 aof 重写开始的时间，以及记录 aof 子进程的进程号。\n\n此外，rewriteappendonlyfilebackground 函数还会调用 updatedictresizepolicy 函数，禁止在 aof 重写期间进行 rehash 操作。这是因为 rehash 操作会带来较多的数据移动操作，对于 aof 重写子进程来说，这就意味着父进程中的内存修改会比较多。因此，aof 重写子进程就需要执行更多的写时复制，进而完成 aof 文件的写入，这就会给 redis 系统的性能造成负面影响。\n\n以下代码就展示了 rewriteappendonlyfilebackground 函数的基本执行逻辑，你可以看下。\n\nint rewriteappendonlyfilebackground(void) {\n   ...\n   if ((childpid = fork()) == 0) {  //创建子进程\n      ...\n      //子进程调用rewriteappendonlyfile进行aof重写\n      if (rewriteappendonlyfile(tmpfile) == c_ok) {\n            size_t private_dirty = zmalloc_get_private_dirty(-1);\n            ...\n            exitfromchild(0);\n        } else {\n            exitfromchild(1);\n        }\n   }\n   else{ //父进程执行的逻辑\n      ...\n      server.aof_rewrite_scheduled = 0;\n      server.aof_rewrite_time_start = time(null);\n      server.aof_child_pid = childpid; //记录重写子进程的进程号\n      updatedictresizepolicy(); //关闭rehash功能\n}\n\n\n而从这里，你可以看到，aof 重写和 rdb 创建是比较类似的，它们都会创建一个子进程来遍历所有的数据库，并把数据库中的每个键值对记录到文件中。不过，aof 重写和 rdb 文件又有两个不同的地方：\n\n * 一是，aof 文件中是以“命令 + 键值对”的形式，来记录每个键值对的插入操作，而 rdb 文件记录的是键值对数据本身；\n * 二是，在 aof 重写或是创建 rdb 的过程中，主进程仍然可以接收客户端写请求。不过，因为 rdb 文件只需要记录某个时刻下数据库的所有数据就行，而 aof 重写则需要尽可能地把主进程收到的写操作，也记录到重写的日志文件中。所以，aof 重写子进程就需要有相应的机制来和主进程进行通信，以此来接收主进程收到的写操作。\n\n下图就展示了 rewriteappendonlyfilebackground 函数执行的基本逻辑、主进程和 aof 重写子进程各自执行的内容，以及主进程和子进程间的通信过程，你可以再来整体回顾下。\n\n\n\n到这里，我们就大概掌握了 aof 重写的基本执行过程。但是在这里，你可能还会有疑问，比如说，aof 重写的子进程和父进程，它们之间的通信过程是怎么样的呢？\n\n其实，这个通信过程是通过操作系统的管道机制（pipe）来实现的\n\n在 aof 重写时，主进程仍然在接收客户端写操作，那么这些新写操作会记录到 aof 重写日志中吗？如果需要记录的话，重写子进程又是通过什么方式向主进程获取这些写操作的呢？\n\necho 接下来就带你了解下 aof 重写过程中所使用的管道机制，以及主进程和重写子进程的交互过程\n\n * 一方面，你就可以了解 aof 重写日志包含的写操作的完整程度，当你要使用 aof 日志恢复 redis 数据库时，就知道 aof 能恢复到的程度是怎样的\n * 一方面，因为 aof 重写子进程就是通过操作系统提供的管道机制，来和 redis 主进程交互的，所以学完这节课之后，你还可以掌握管道技术，从而用来实现进程间的通信\n\n好了，接下来，我们就先来了解下管道机制\n\n\n# 深入重写缓冲区\n\n\n# 如何使用管道进行父子进程间通信？\n\n首先我们要知道，当进程 a 通过调用 fork 函数创建一个子进程 b，然后进程 a 和 b 要进行通信时，我们通常都需要依赖操作系统提供的通信机制，而管道（pipe）就是一种用于父子进程间通信的常用机制。\n\n具体来说，管道机制在操作系统内核中创建了一块缓冲区，父进程 a 可以打开管道，并往这块缓冲区中写入数据。同时，子进程 b 也可以打开管道，从这块缓冲区中读取数据。这里，你需要注意的是，进程每次往管道中写入数据时，只能追加写到缓冲区中当前数据所在的尾部，而进程每次从管道中读取数据时，只能从缓冲区的头部读取数据。\n\n其实，管道创建的这块缓冲区就像一个先进先出的队列一样，写数据的进程写到队列尾部，而读数据的进程则从队列头读取。下图就展示了两个进程使用管道进行数据通信的过程，你可以看下。\n\n\n\n好了，了解了管道的基本功能后，我们再来看下使用管道时需要注意的一个关键点。管道中的数据在一个时刻只能向一个方向流动，这也就是说，如果父进程 a 往管道中写入了数据，那么此时子进程 b 只能从管道中读取数据。类似的，如果子进程 b 往管道中写入了数据，那么此时父进程 a 只能从管道中读取数据。而如果父子进程间需要同时进行数据传输通信，我们就需要创建两个管道了。\n\n下面，我们就来看下怎么用代码实现管道通信。这其实是和操作系统提供的管道的系统调用 pipe 有关，pipe 的函数原型如下所示：\n\nint pipe(int pipefd[2]);\n\n\n你可以看到，pipe 的参数是一个数组 pipefd，表示的是管道的文件描述符。这是因为进程在往管道中写入或读取数据时，其实是使用 write 或 read 函数的，而 write 和 read 函数需要通过文件描述符才能进行写数据和读数据操作。\n\n数组 pipefd 有两个元素 pipefd[0]和 pipefd[1]，分别对应了管道的读描述符和写描述符。这也就是说，当进程需要从管道中读数据时，就需要用到 pipefd[0]，而往管道中写入数据时，就使用 pipefd[1]。\n\n这里我写了一份示例代码，展示了父子进程如何使用管道通信，你可以看下。\n\nint main()\n{\n    int fd[2], nr = 0, nw = 0;\n    char buf[128];\n    pipe(fd);\n    pid = fork();\n\n  if(pid == 0) {\n      //子进程调用read从fd[0]描述符中读取数据\n        printf("child process wait for message\\n");\n        nr = read(fds[0], buf, sizeof(buf))\n        printf("child process receive %s\\n", buf);\n  }else{\n       //父进程调用write往fd[1]描述符中写入数据\n        printf("parent process send message\\n");\n        strcpy(buf, "hello from parent");\n        nw = write(fd[1], buf, sizeof(buf));\n        printf("parent process send %d bytes to child.\\n", nw);\n    }\n    return 0;\n}\n\n\n从代码中，你可以看到，在父子进程进行管道通信前，我们需要在代码中定义用于保存读写描述符的数组 fd，然后调用 pipe 系统创建管道，并把数组 fd 作为参数传给 pipe 函数。紧接着，在父进程的代码中，父进程会调用 write 函数往管道文件描述符 fd[1]中写入数据，另一方面，子进程调用 read 函数从管道文件描述符 fd[0]中读取数据。\n\n这里，为了便于你理解，我也画了一张图，你可以参考。\n\n\n\n好了，现在你就了解了如何使用管道来进行父子进程的通信了。那么下面，我们就来看下在 aof 重写过程中，重写子进程是如何用管道和主进程（也就是它的父进程）进行通信的。\n\n\n# aof 重写子进程如何使用管道和父进程交互？\n\n我们先来看下在 aof 重写过程中，都创建了几个管道。\n\n这实际上是 aof 重写函数 rewriteappendonlyfilebackground 在执行过程中，通过调用 aofcreatepipes 函数来完成的，如下所示：\n\nint rewriteappendonlyfilebackground(void) {\n…\nif (aofcreatepipes() != c_ok) return c_err;\n…\n}\n\n\n这个 aofcreatepipes 函数是在aof.c文件中实现的，它的逻辑比较简单，可以分成三步。\n\n第一步，aofcreatepipes 函数创建了包含 6 个文件描述符元素的数组 fds。就像我刚才给你介绍的，每一个管道会对应两个文件描述符，所以，数组 fds 其实对应了 aof 重写过程中要用到的三个管道。紧接着，aofcreatepipes 函数就调用 pipe 系统调用函数，分别创建三个管道。\n\n这部分代码如下所示，你可以看下。\n\nint aofcreatepipes(void) {\n    int fds[6] = {-1, -1, -1, -1, -1, -1};\n    int j;\n    if (pipe(fds) == -1) goto error; /* parent -> children data. */\n    if (pipe(fds+2) == -1) goto error; /* children -> parent ack. */\n  if (pipe(fds+4) == -1) goto error;\n  …}\n}\n\n\n第二步，aofcreatepipes 函数会调用 anetnonblock 函数（在anet.c文件中），将 fds\n\n数组的第一和第二个描述符（fds[0]和 fds[1]）对应的管道设置为非阻塞。然后，aofcreatepipes 函数会调用 aecreatefileevent 函数，在数组 fds 的第三个描述符 (fds[2]) 上注册了读事件的监听，对应的回调函数是 aofchildpipereadable。aofchildpipereadable 函数也是在 aof.c 文件中实现的，我稍后会给你详细介绍它。\n\nint aofcreatepipes(void) {\n…\nif (anetnonblock(null,fds[0]) != anet_ok) goto error;\nif (anetnonblock(null,fds[1]) != anet_ok) goto error;\nif (aecreatefileevent(server.el, fds[2], ae_readable, aofchildpipereadable, null) == ae_err) goto error;\n…\n}\n\n\n这样，在完成了管道创建、管道设置和读事件注册后，最后一步，aofcreatepipes 函数会将数组 fds 中的六个文件描述符，分别复制给 server 变量的成员变量，如下所示：\n\nint aofcreatepipes(void) {\n…\nserver.aof_pipe_write_data_to_child = fds[1];\nserver.aof_pipe_read_data_from_parent = fds[0];\nserver.aof_pipe_write_ack_to_parent = fds[3];\nserver.aof_pipe_read_ack_from_child = fds[2];\nserver.aof_pipe_write_ack_to_child = fds[5];\nserver.aof_pipe_read_ack_from_parent = fds[4];\n…\n}\n\n\n在这一步中，我们就可以从 server 变量的成员变量名中，看到 aofcreatepipes 函数创建的三个管道，以及它们各自的用途。\n\n * fds[0]和 fds[1]：对应了主进程和重写子进程间用于传递操作命令的管道，它们分别对应读描述符和写描述符。\n * fds[2]和 fds[3]：对应了重写子进程向父进程发送 ack 信息的管道，它们分别对应读描述符和写描述符。\n * fds[4]和 fds[5]：对应了父进程向重写子进程发送 ack 信息的管道，它们分别对应读描述符和写描述符。\n\n下图也展示了 aofcreatepipes 函数的基本执行流程，你可以再回顾下。\n\n\n\n好了，了解了 aof 重写过程中的管道个数和用途后，下面我们再来看下这些管道具体是如何使用的。\n\n# 操作命令传输管道的使用\n\n实际上，当 aof 重写子进程在执行时，主进程还会继续接收和处理客户端写请求。这些写操作会被主进程正常写入 aof 日志文件，这个过程是由 feedappendonlyfile 函数（在 aof.c 文件中）来完成。\n\nfeedappendonlyfile 函数在执行的最后一步，会判断当前是否有 aof 重写子进程在运行。如果有的话，它就会调用 aofrewritebufferappend 函数（在 aof.c 文件中），如下所示：\n\nif (server.aof_child_pid != -1)\n        aofrewritebufferappend((unsigned char*)buf,sdslen(buf));\n\n\naofrewritebufferappend 函数的作用是将参数 buf，追加写到全局变量 server 的 aof_rewrite_buf_blocks 这个列表中。\n\n这里，你需要注意的是，参数 buf 是一个字节数组，feedappendonlyfile 函数会将主进程收到的命令操作写入到 buf 中。而 aof_rewrite_buf_blocks 列表中的每个元素是 aofrwblock 结构体类型，这个结构体中包括了一个字节数组，大小是 aof_rw_buf_block_size，默认值是 10mb。此外，aofrwblock 结构体还记录了字节数组已经使用的空间和剩余可用的空间。\n\n以下代码展示了 aofrwblock 结构体的定义，你可以看下。\n\ntypedef struct aofrwblock {\n    unsigned long used, free; //buf数组已用空间和剩余可用空间\n    char buf[aof_rw_buf_block_size]; //宏定义aof_rw_buf_block_size默认为10mb\n} aofrwblock;\n\n\n这样一来，aofrwblock 结构体就相当于是一个 10mb 的数据块，记录了 aof 重写期间主进程收到的命令，而 aof_rewrite_buf_blocks 列表负责将这些数据块连接起来。当 aofrewritebufferappend 函数执行时，它会从 aof_rewrite_buf_blocks 列表中取出一个 aofrwblock 类型的数据块，用来记录命令操作。\n\n当然，如果当前数据块中的空间不够保存参数 buf 中记录的命令操作，那么 aofrewritebufferappend 函数就会再分配一个 aofrwblock 数据块。\n\n好了，当 aofrewritebufferappend 函数将命令操作记录到 aof_rewrite_buf_blocks 列表中之后，它还会检查 aof_pipe_write_data_to_child 管道描述符上是否注册了写事件，这个管道描述符就对应了我刚才给你介绍的 fds[1]。\n\n如果没有注册写事件，那么 aofrewritebufferappend 函数就会调用 aecreatefileevent 函数，注册一个写事件，这个写事件会监听 aof_pipe_write_data_to_child 这个管道描述符，也就是主进程和重写子进程间的操作命令传输管道。\n\n当这个管道可以写入数据时，写事件对应的回调函数 aofchildwritediffdata（在 aof.c 文件中）就会被调用执行。这个过程你可以参考下面的代码：\n\nvoid aofrewritebufferappend(unsigned char *s, unsigned long len) {\n...\n//检查aof_pipe_write_data_to_child描述符上是否有事件\nif (aegetfileevents(server.el,server.aof_pipe_write_data_to_child) == 0) {\n     //如果没有注册事件，那么注册一个写事件，回调函数是aofchildwritediffdata\n     aecreatefileevent(server.el, server.aof_pipe_write_data_to_child,\n            ae_writable, aofchildwritediffdata, null);\n}\n...}\n\n\n其实，刚才我介绍的写事件回调函数 aofchildwritediffdata，它的主要作用是从 aof_rewrite_buf_blocks 列表中逐个取出数据块，然后通过 aof_pipe_write_data_to_child 管道描述符，将数据块中的命令操作通过管道发给重写子进程，这个过程如下所示：\n\nvoid aofchildwritediffdata(aeeventloop *el, int fd, void *privdata, int mask) {\n...\nwhile(1) {\n   //从aof_rewrite_buf_blocks列表中取出数据块\n   ln = listfirst(server.aof_rewrite_buf_blocks);\n   block = ln ? ln->value : null;\n   if (block->used > 0) {\n      //调用write将数据块写入主进程和重写子进程间的管道\n      nwritten = write(server.aof_pipe_write_data_to_child,\n                             block->buf,block->used);\n      if (nwritten <= 0) return;\n            ...\n        }\n ...}}\n\n\n好了，这样一来，你就了解了主进程其实是在正常记录 aof 日志时，将收到的命令操作写入 aof_rewrite_buf_blocks 列表中的数据块，然后再通过 aofchildwritediffdata 函数将记录的命令操作通过主进程和重写子进程间的管道发给子进程。\n\n下图也展示了这个过程，你可以再来回顾下。\n\n\n\n然后，我们接着来看下重写子进程，是如何从管道中读取父进程发送的命令操作的。\n\n这实际上是由 aofreaddifffromparent 函数（在 aof.c 文件中）来完成的。这个函数会使用一个 64kb 大小的缓冲区，然后调用 read 函数，读取父进程和重写子进程间的操作命令传输管道中的数据。以下代码也展示了 aofreaddifffromparent 函数的基本执行流程，你可以看下。\n\nssize_t aofreaddifffromparent(void) {\n    char buf[65536]; //管道默认的缓冲区大小\n    ssize_t nread, total = 0;\n    //调用read函数从aof_pipe_read_data_from_parent中读取数据\n    while ((nread =\n      read(server.aof_pipe_read_data_from_parent,buf,sizeof(buf))) > 0) {\n        server.aof_child_diff = sdscatlen(server.aof_child_diff,buf,nread);\n        total += nread;\n    }\n    return total;\n}\n\n\n那么，从代码中，你可以看到 aofreaddifffromparent 函数会通过 aof_pipe_read_data_from_parent 描述符读取数据。然后，它会将读取的操作命令追加到全局变量 server 的 aof_child_diff 字符串中。而在 aof 重写函数 rewriteappendonlyfile 的执行过程最后，aof_child_diff 字符串会被写入 aof 重写日志文件，以便我们在使用 aof 重写日志时，能尽可能地恢复重写期间收到的操作。\n\n这个 aof_child_diff 字符串写入重写日志文件的过程，你可以参考下面给出的代码：\n\nint rewriteappendonlyfile(char *filename) {\n...\n//将aof_child_diff中累积的操作命令写入aof重写日志文件\nif (riowrite(&aof,server.aof_child_diff,sdslen(server.aof_child_diff)) == 0)\n        goto werr;\n...\n}\n\n\n所以也就是说，aofreaddifffromparent 函数实现了重写子进程向主进程读取操作命令。那么在这里，我们还需要搞清楚的问题是：aofreaddifffromparent 函数会在哪里被调用，也就是重写子进程会在什么时候从管道中读取主进程收到的操作。\n\n其实，aofreaddifffromparent 函数一共会被以下三个函数调用。\n\n * rewriteappendonlyfilerio 函数：这个函数是由重写子进程执行的，它负责遍历 redis 每个数据库，生成 aof 重写日志，在这个过程中，它会不时地调用 aofreaddifffromparent 函数。\n * rewriteappendonlyfile 函数：这个函数是重写日志的主体函数，也是由重写子进程执行的，它本身会调用 rewriteappendonlyfilerio 函数。此外，它在调用完 rewriteappendonlyfilerio 函数后，还会多次调用 aofreaddifffromparent 函数，以尽可能多地读取主进程在重写日志期间收到的操作命令。\n * rdbsaverio 函数：这个函数是创建 rdb 文件的主体函数。当我们使用 aof 和 rdb 混合持久化机制时，这个函数也会调用 aofreaddifffromparent 函数。\n\n从这里，我们可以看到，redis 源码在实现 aof 重写过程中，其实会多次让重写子进程向主进程读取新收到的操作命令，这也是为了让重写日志尽可能多地记录最新的操作，提供更加完整的操作记录。\n\n最后，我们再来看下重写子进程和主进程间用来传递 ack 信息的两个管道的使用。\n\n# ack 管道的使用\n\n刚才在介绍主进程调用 aofcreatepipes 函数创建管道时，你就了解到了，主进程会在 aof_pipe_read_ack_from_child 管道描述符上注册读事件。这个描述符对应了重写子进程向主进程发送 ack 信息的管道。同时，这个描述符是一个读描述符，表示主进程从管道中读取 ack 信息。\n\n其实，重写子进程在执行 rewriteappendonlyfile 函数时，这个函数在完成日志重写，以及多次向父进程读取操作命令后，就会调用 write 函数，向 aof_pipe_write_ack_to_parent 描述符对应的管道中写入“！”，这就是重写子进程向主进程发送 ack 信号，让主进程停止发送收到的新写操作。这个过程如下所示：\n\nint rewriteappendonlyfile(char *filename) {\n...\nif (write(server.aof_pipe_write_ack_to_parent,"!",1) != 1) goto werr;\n...}\n\n\n一旦重写子进程向主进程发送 ack 信息的管道中有了数据，aof_pipe_read_ack_from_child 管道描述符上注册的读事件就会被触发，也就是说，这个管道中有数据可以读取了。那么，aof_pipe_read_ack_from_child 管道描述符上，注册的回调函数 aofchildpipereadable（在 aof.c 文件中）就会执行。\n\n这个函数会判断从 aof_pipe_read_ack_from_child 管道描述符读取的数据是否是“！”，如果是的话，那它就会调用 write 函数，往 aof_pipe_write_ack_to_child 管道描述符上写入“！”，表示主进程已经收到重写子进程发送的 ack 信息，同时它会给重写子进程回复一个 ack 信息。这个过程如下所示：\n\nvoid aofchildpipereadable(aeeventloop *el, int fd, void *privdata, int mask) {\n...\nif (read(fd,&byte,1) == 1 && byte == \'!\') {\n   ...\n   if (write(server.aof_pipe_write_ack_to_child,"!",1) != 1) { ...}\n}\n...\n}\n\n\n好了，到这里，我们就了解了，重写子进程在完成日志重写后，是先给主进程发送 ack 信息。然后主进程在 aof_pipe_read_ack_from_child 描述符上监听读事件发生，并调用 aofchildpipereadable 函数向子进程发送 ack 信息。\n\n最后，重写子进程执行的 rewriteappendonlyfile 函数，会调用 syncread 函数，从 aof_pipe_read_ack_from_parent 管道描述符上，读取主进程发送给它的 ack 信息，如下所示：\n\nint rewriteappendonlyfile(char *filename) {\n...\nif (syncread(server.aof_pipe_read_ack_from_parent,&byte,1,5000) != 1  || byte != \'!\') goto werr\n...\n}\n\n\n下图也展示了 ack 管道的使用过程，你可以再回顾下。\n\n\n\n这样一来，重写子进程和主进程之间就通过两个 ack 管道，相互确认重写过程结束了。\n\n\n# 总结\n\n 1. aof 重写的触发时机。这既包括了我们主动执行 bgrewriteaof 命令，也包括了 redis server 根据 aof 文件大小而自动触发的重写。此外，在主从复制的过程中，从节点也会启动 aof 重写，形成一份完整的 aof 日志，以便后续进行恢复。当然你也要知道，当要触发 aof 重写时，redis server 是不能运行 rdb 子进程和 aof 重写子进程的。\n\n 2. aof 重写的基本执行过程。aof 重写和 rdb 创建的过程类似，它也是创建了一个子进程来完成重写工作。这是因为 aof 重写操作，实际上需要遍历 redis server 上的所有数据库，把每个键值对以插入操作的形式写入日志文件，而日志文件又要进行写盘操作。所以，redis 源码使用子进程来实现 aof 重写，这就避免了阻塞主线程，也减少了对 redis 整体性能的影响。\n\n 3. 注管道机制的使用\n\n 4. 主进程和重写子进程使用管道通信的过程\n\n在这个过程中，aof 重写子进程和主进程是使用了一个操作命令传输管道和两个 ack 信息发送管道。操作命令传输管道是用于主进程写入收到的新操作命令，以及用于重写子进程读取操作命令，而 ack 信息发送管道是在重写结束时，重写子进程和主进程用来相互确认重写过程的结束。最后，重写子进程会进一步将收到的操作命令记录到重写日志文件中。\n\n这样一来，aof 重写过程中主进程收到的新写操作，就不会被遗漏了\n\n * 一方面，这些新写操作会被记录在正常的 aof 日志中\n * 一方面，主进程会将新写操作缓存在 aof_rewrite_buf_blocks 数据块列表中，并通过管道发送给重写子进程。这样，就能尽可能地保证重写日志具有最新、最完整的写操作了\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"Redis 中的延迟监控",frontmatter:{title:"Redis 中的延迟监控",date:"2024-09-15T23:27:13.000Z",permalink:"/pages/aa75e9/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E6%94%AF%E7%BA%BF/20.Redis%20%E4%B8%AD%E7%9A%84%E5%BB%B6%E8%BF%9F%E7%9B%91%E6%8E%A7.html",relativePath:"Redis 系统设计/04.支线/20.Redis 中的延迟监控.md",key:"v-1aeed160",path:"/pages/aa75e9/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:405},{level:2,title:"延迟监控框架的实现",slug:"延迟监控框架的实现",normalizedTitle:"延迟监控框架的实现",charIndex:760},{level:3,title:"记录事件执行情况的数据结构",slug:"记录事件执行情况的数据结构",normalizedTitle:"记录事件执行情况的数据结构",charIndex:1236},{level:3,title:"如何实现延迟事件的采样？",slug:"如何实现延迟事件的采样",normalizedTitle:"如何实现延迟事件的采样？",charIndex:2273},{level:3,title:"延迟分析和提供应对措施建议",slug:"延迟分析和提供应对措施建议",normalizedTitle:"延迟分析和提供应对措施建议",charIndex:5478},{level:2,title:"慢命令日志的实现",slug:"慢命令日志的实现",normalizedTitle:"慢命令日志的实现",charIndex:7015},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:10252},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10666}],headersStr:"前言 延迟监控框架的实现 记录事件执行情况的数据结构 如何实现延迟事件的采样？ 延迟分析和提供应对措施建议 慢命令日志的实现 总结 参考资料",content:'提出问题是一切智慧的开端\n\n 1. Redis 是以低延迟著称的数据库，那么当它的响应速度变慢时，如何快速诊断出问题所在？\n 2. 你知道 Redis 的哪些事件可能导致它运行缓慢吗？它们是如何被监控的？\n 3. 当 Redis 的某个命令执行变慢时，我们如何捕捉这些“慢命令”并进行详细分析？\n 4. 在 Redis 延迟监控中，如何通过采样机制高效记录每类事件的执行时长？\n 5. 为什么 Redis 需要为延迟事件和慢命令分别设计不同的监控和日志机制？这两者的核心区别是什么？\n 6. Redis 如何通过“时间序列”的方式保存和分析多次延迟事件的数据，以便为后续问题排查提供更多线索？\n 7. 在延迟监控中，Redis 是如何通过统计延迟事件的最大值、最小值、均值等数据来帮助用户诊断问题的？\n 8. Redis 的慢命令日志能提供哪些关键信息？如何通过这些信息定位到具体的慢命令和它的来源？\n\n\n# 前言\n\nRedis的一个显著特征就是能提供低延迟的数据访问。而如果Redis在运行过程中变慢了，我们就需要有方法能监控到哪些命令执行变慢了。更进一步的需求，就是我们需要有方法监控到，是Redis运行过程中的哪些事件导致Redis变慢了。这样一来，我们就既可以检查这些慢命令，分析它们的操作类型和访问的数据量，进而提出应对方法，也可以检查监控记录的事件，分析事件发生的原因并提出应对方法\n\n那么，为了满足这些需求，我们就需要有一套监控框架\n\n * 一方面能监测导致Redis变慢的事件\n * 另一方面，能监控并记录变慢的命令。\n\n其实，这也是我们在开发后端系统时，经常会面临的一个运维开发需求，也就是如何监控后端系统的运行状态。\n\necho 来带你了解Redis的延迟监控框架和慢命令日志的设计与实现。\n\n\n# 延迟监控框架的实现\n\n实际上，Redis 在运行过程中，以下表格中给出的几类事件都会导致 Redis 变慢，我们通常也把这些事件称为延迟事件。你可以参考表格中的这些事件类型，以及它们在源码中对应的事件名称。\n\n事件类型      源码中对应名称\n命令事件      command、fast-command\nAOF事件     aof-write-pending-fsync, aof-write-active-child,\n          aof-write-alone, aof-fstat, aof-rewrite-diff-write,\n          aof-rename\nfork事件    fork\n过期Key事件   expire-cycie\n缓存替换事件    eviction-del, eviction-cycie\n\n那么针对这些事件，Redis实现了延迟监控框架，通过采样的方式来记录它们的执行情况。当需要排查问题时，延迟监控框架提供了latency history命令，以便运维人员检查这些事件。\n\n下面，我们就来看下记录事件执行情况的数据结构。因为延迟监控框架是在latency.h和latency.c文件中实现的，你也可以在这两个文件中找到相应的数据结构和函数。\n\n\n# 记录事件执行情况的数据结构\n\n首先，Redis是使用了latencySample结构体，来记录延迟事件的采样时间和事件的实际执行时长，这个结构体的代码如下所示：\n\nstruct latencySample {\n    int32_t time;  //事件的采样时间\n    uint32_t latency;  //事件的执行时长（以毫秒为单位）\n};\n\n\n而在latencySample这个结构体基础上，Redis又设计了latencyTimeSeries结构体，这个结构体使用了一个latencySample类型的数组，记录了针对某一类事件的一系列采样结果，这样就可以为分析Redis变慢提供更多的事件信息。\n\nstruct latencyTimeSeries {\n    int idx;  //采样事件数组的写入位置\n    uint32_t max;  //当前事件的最大延迟\n    struct latencySample samples[LATENCY_TS_LEN]; //采样事件数组，记录LATENCY_TS_LEN个采样结果，LATENCY_TS_LEN默认为160\n};\n\n\n另外，也因为延迟监控框架要记录的延迟事件有很多种，所以 Redis 还进一步设计了一个哈希表latency_events，作为全局变量server的一个成员变量，用来记录不同事件的采样结果数组，如下所示：\n\nstruct redisServer {\n   …\n   dict *latency_events;\n   …\n}\n\n\n这个哈希表是在Redis server启动初始化的函数initServer中，通过调用latencyMonitorInit函数来完成创建的，如下所示：\n\nvoid initServer(void) {\n    …\n    latencyMonitorInit();\n}\n \nvoid latencyMonitorInit(void) {\n    server.latency_events = dictCreate(&latencyTimeSeriesDictType,NULL);\n}\n\n\n好，了解了记录延迟事件的数据结构和初始化操作后，我们再来看下事件采样是如何实现的。\n\n\n# 如何实现延迟事件的采样？\n\n延迟事件的采样函数是latencyAddSample，它的函数原型如下所示。它的参数中包含了要记录的事件名称，这实际是对应了latency_events哈希表中的一个哈希项。此外，它的参数中还包括该事件的执行时长。\n\nvoid latencyAddSample(char *event, mstime_t latency)\n\n\nlatencyAddSample函数的执行逻辑并不复杂，主要可以分成三步。\n\n首先，它会根据传入的事件名称，在latency_events哈希表中查找该事件。如果该事件对应的哈希项还不存在，它就会在哈希表中加入该事件，如下所示：\n\n//查找事件对应的哈希项\nstruct latencyTimeSeries *ts = dictFetchValue(server.latency_events,event);\n…\nif (ts == NULL) { //如果哈希项为空，就新建哈希项\n    ts = zmalloc(sizeof(*ts));\n    ts->idx = 0;\n    ts->max = 0;\n    memset(ts->samples,0,sizeof(ts->samples));\n    dictAdd(server.latency_events,zstrdup(event),ts); //在哈希表中插入哈希项\n}\n\n\n然后，latencyAddSample函数会根据传入的事件执行时间，更新当前记录的该类事件的最大执行时间，如下所示：\n\nif (latency > ts->max) ts->max = latency;\n\n\n最后，latencyAddSample函数会实际记录当前的采样结果。\n\n不过在这一步，如果它发现当前的采样结果，和前一个采样结果是在同一秒中获得的，并且如果当前采样结果的事件执行时长，大于前一个采样结果的话，那么latencyAddSample函数就会直接更新前一个采样结果中记录的执行时长了，而不是新插入一个采样结果。\n\n否则的话，latencyAddSample函数才会新插入一个采样结果。这样设计的目的，也是为了避免在同一秒中记录过多的采样结果。\n\n下面的代码展示了latencyAddSample函数实际记录采样结果的逻辑，你可以看下。\n\n//获得同类事件的前一个采样结果\nprev = (ts->idx + LATENCY_TS_LEN - 1) % LATENCY_TS_LEN;\n//如果当前和前一个采样结果在同一秒中\nif (ts->samples[prev].time == now) { \n    //如果当前采用结果的执行时长大于前一个采样结果\n    if (latency > ts->samples[prev].latency) \n        //直接更新前一个采样结果的执行时长\n        ts->samples[prev].latency = latency;\n    return;\n}\n//否则，新插入当前的采样结果\nts->samples[ts->idx].time = time(NULL);\nts->samples[ts->idx].latency = latency;\n\n\n而在这里，你也要注意一点，就是latencyAddSample函数在记录采样结果时，会重复使用采样结果数组latencyTimeSeries。所以，如果采样结果数量超过数组默认大小时，旧的采样结果是会被覆盖掉的。如果你要记录更多的采样结果，就需要扩大latencyTimeSeries数组的长度。\n\n那么，latencyAddSample函数是在什么时候调用进行采样的呢?\n\n其实，latencyAddSample函数是被封装在了latencyAddSampleIfNeeded函数中。在latencyAddSampleIfNeeded函数中，它只会在事件执行时长超过latency-monitor-threshold配置项的值时，才调用latencyAddSample函数记录采样结果。你可以看看下面给出的latencyAddSampleIfNeeded函数定义。\n\nlatencyAddSampleIfNeeded(event,var){\n    if (server.latency_monitor_threshold &&  (var) >= server.latency_monitor_threshold)\n          latencyAddSample((event),(var));\n}\n    \n\n\n而latencyAddSampleIfNeeded函数，实际上会在刚才介绍的延迟事件发生时被调用。这里我来给你举两个例子。\n\n比如，当Redis命令通过call函数（在server.c文件中）执行时，call函数就会调用latencyAddSampleIfNeeded函数进行采样，如下所示：\n\nif (flags & CMD_CALL_SLOWLOG && c->cmd->proc != execCommand) {\n    //根据命令数据结构中flags的CMD_FAST标记，决定当前是fast-command事件还是command事件\n    char *latency_event = (c->cmd->flags & CMD_FAST) ?\n        "fast-command" : "command";\n    latencyAddSampleIfNeeded(latency_event,duration/1000);\n    …\n}\n\n\n再比如，当Redis调用flushAppendOnlyFile函数写AOF文件时，如果AOF文件刷盘的配置项是AOF_FSYNC_ALWAYS，那么flushAppendOnlyFile函数就会调用latencyAddSampleIfNeeded函数，记录aof-fsync-always延迟事件的采样结果，如下所示：\n\nvoid flushAppendOnlyFile(int force) {\n…\nif (server.aof_fsync == AOF_FSYNC_ALWAYS) {\nlatencyStartMonitor(latency); //调用latencyStartMonitor函数开始计时\nredis_fsync(server.aof_fd); //实际将数据写入磁盘\n        latencyEndMonitor(latency); //调用latencyEndMonitor结束计时，并计算时长\n        latencyAddSampleIfNeeded("aof-fsync-always",latency);\n…}\n}\n\n\n那么在这里，你需要注意的是，Redis源码在调用latencyAddSampleIfNeeded函数记录采样结果时，经常会在延迟事件执行前，调用latencyStartMonitor函数开始计时，并在事件执行结束后，调用latencyEndMonitor函数结束计时和计算事件执行时长。\n\n此外，你也可以在阅读Redis源码的工具中，比如sublime、sourceinsight等，通过查找函数关系调用，找到latencyAddSampleIfNeeded函数被调用的其他地方。\n\n好了，到这里，Redis延迟监控框架就能通过latencyAddSampleIfNeeded函数，来记录延迟事件的采样结果了。而实际上，Redis延迟监控框架还实现了延迟分析，并能提供应对延迟变慢的建议，我们再来看下。\n\n\n# 延迟分析和提供应对措施建议\n\n首先，Redis是提供了latency doctor命令，来给出延迟分析结果和应对方法建议的。当我们执行这条命令的时候，Redis就会使用latencyCommand函数来处理。而在处理这个命令时，latencyCommand函数会调用createLatencyReport函数，来生成延迟分析报告和应对方法建议。\n\n具体来说，createLatencyReport函数会针对latency_events哈希表中记录的每一类事件，先调用analyzeLatencyForEvent函数，计算获得采样的延迟事件执行时长的均值、最大/最小值等统计结果。具体的统计计算过程，你可以仔细阅读下analyzeLatencyForEvent函数的源码。\n\n然后，createLatencyReport函数会针对这类事件，结合Redis配置项等信息给出应对措施。\n\n其实，在createLatencyReport函数中，它定义了多个int变量，当这些变量的值为1时，就表示建议Redis使用者采用一种应对高延迟的措施。我在下面的代码中展示了部分应对措施对应的变量，你可以看下。另外你也可以阅读createLatencyReport函数源码，去了解所有的措施。\n\nsds createLatencyReport(void) {\n    …\n    int advise_slowlog_enabled = 0;  //建议启用slowlog\n    int advise_slowlog_tuning = 0;   //建议重新配置slowlog阈值\n    int advise_slowlog_inspect = 0;   //建议检查slowlog结果\n    int advise_disk_contention = 0;   //建议减少磁盘竞争\n    …\n}\n\n\n我们也来简单举个例子。比如说，针对command事件，createLatencyReport函数就会根据slowlog的设置情况，给出启用slowlog、调整slowlog阈值、检查slowlog日志结果和避免使用bigkey的应对建议。这部分代码如下所示：\n\nif (!strcasecmp(event,"command")) {\n    \n   //如果没有启用slowlog，则建议启用slowlog\n   if (server.slowlog_log_slower_than < 0) {\n       advise_slowlog_enabled = 1;\n       advices++;\n\t}  \n    //如果slowlog使用的命令时长阈值太大，建议调整slowlog阈值\n\telse if (server.slowlog_log_slower_than/1000 >server.latency_monitor_threshold){\n        advise_slowlog_tuning = 1;\n        advices++;\n    }\n    //建议检查slowlog结果\n    advise_slowlog_inspect = 1; \n    //建议避免使用bigkey\n    advise_large_objects = 1; \n    advices += 2;\n}\n\n\n所以，像createLatencyReport函数这样在计算延迟统计结果的同时，也给出应对措施的设计就很不错，这也是从Redis开发者的角度给出的建议，它更具有针对性。\n\n好了，到这里，我们就了解了延迟监控框架的实现。接下来，我们再来学习下Redis中慢命令日志的实现。\n\n\n# 慢命令日志的实现\n\nRedis是使用了一个较为简单的方法来记录慢命令日志，也就是用一个列表，把执行时间超出慢命令日志执行时间阈值的命令记录下来。\n\n在Redis全局变量server对应的数据结构redisServer中，有一个list类型的成员变量slowlog，它就是用来记录慢命令日志的列表的，如下所示：\n\nstruct redisServer {\n    …\n    list *slowlog;\n    …\n}\n\n\n而实现慢命令日志记录功能的代码是在slowlog.c文件中。这里的主要函数是slowlogPushEntryIfNeeded，它的原型如下所示：\n\nvoid slowlogPushEntryIfNeeded(client *c, robj **argv, int argc, long long duration)\n\n\n从代码中你可以看到，这个函数的参数包含了当前执行命令及其参数argv，以及当前命令的执行时长duration。\n\n这个函数的逻辑也不复杂，它会判断当前命令的执行时长duration，是否大于 redis.conf 配置文件中的慢命令日志阈值 slowlog-log-slower-than。如果大于的话，它就会调用slowlogCreateEntry函数，为这条命令创建一条慢命令日志项，并调用listAddNodeHeader函数，把这条日志项加入到日志列表头，如下所示：\n\n//当前命令的执行时长是否大于配置项\nif (duration >= server.slowlog_log_slower_than)\n   listAddNodeHead(server.slowlog, slowlogCreateEntry(c,argv,argc,duration));\n\n\n当然，如果日志列表中记录了太多日志项，它消耗的内存资源也会增加。所以slowlogPushEntryIfNeeded函数在添加日志项时，会判断整个日志列表的长度是否超过配置项slowlog-max-len。一旦超过了，它就会把列表末尾的日志项删除，如下所示：\n\n//如果日志列表超过阈值长度，就删除列表末尾的日志项\nwhile (listLength(server.slowlog) > server.slowlog_max_len)\n        listDelNode(server.slowlog,listLast(server.slowlog))\n\n\n现在，我们也就了解了记录慢命令日志项的主要函数，slowlogPushEntryIfNeeded的基本逻辑了。然后我们再来看下，它在记录日志项时调用的slowlogCreateEntry函数。\n\n这个函数是用来创建一个慢命令日志项。慢命令日志项的数据结构是slowlogEntry，如下所示：\n\ntypedef struct slowlogEntry {\n    //日志项对应的命令及参数\n    robj **argv;     \n    //日志项对应的命令及参数个数\n    int argc;        \n    //日志项的唯一ID\n    long long id; \n    //日志项对应命令的执行时长（以微秒为单位）\n    long long duration;  \n    //日志项对应命令的执行时间戳\n    time_t time;        \n    //日志项对应命令的发送客户端名称\n    sds cname;      \n    //日志项对应命令的发送客户端网络地址\n    sds peerid;         \n} slowlogEntry;\n\n\n从slowLogEntry的定义中，你可以看到，它会把慢命令及其参数，以及发送命令的客户端网络地址记录下来。这样设计的好处是，当我们分析慢命令日志时，就可以直接看到慢命令本身及其参数了，而且可以知道发送命令的客户端信息。而这些信息，就有利于我们排查慢命令的起因和来源。\n\n比如说，如果我们发现日志中记录的命令参数非常多，那么它就可能是一条操作bigkey的命令。\n\n当然，考虑到内存资源有限，slowlogCreateEntry函数在创建慢命令日志项时，也会判断命令参数个数。如果命令参数个数，超出了阈值SLOWLOG_ENTRY_MAX_ARGC这个宏定义的大小（默认32）时，它就不会记录超出阈值的参数了，而是记录下剩余的参数个数。这样一来，慢命令日志项中就既记录了部分命令参数，有助于排查问题，也避免了记录过多参数，占用过多内存。\n\n下面的代码展示了slowlogCreateEntry的基本执行逻辑，你可以看下。\n\nslowlogEntry *slowlogCreateEntry(client *c, robj **argv, int argc, long long duration) {\n    //分配日志项空间\n    slowlogEntry *se = zmalloc(sizeof(*se)); \n    //待记录的参数个数，默认为当前命令的参数个数\n    int j, slargc = argc;  \n\n    //如果当前命令参数个数超出阈值，则只记录阈值个数的参数\n    if (slargc > SLOWLOG_ENTRY_MAX_ARGC) slargc = SLOWLOG_ENTRY_MAX_ARGC;\n    se->argc = slargc;\n    …\n    //逐一记录命令及参数\n    for (j = 0; j < slargc; j++) {\n        //如果命令参数个数超出阈值，使用最后一个参数记录当前命令实际剩余的参数个数\n       if (slargc != argc && j == slargc-1) {  \n          se->argv[j] = createObject(OBJ_STRING,\n                    sdscatprintf(sdsempty(),"... (%d more arguments)",\n                    argc-slargc+1));\n            } else {\n            …  //将命令参数填充到日志项中\n            }}\n    … //将命令执行时长、客户端地址等信息填充到日志项中\n}\n\n\n好了，到这里，你就了解了慢命令日志的实现。最后，你也要注意，慢命令日志只会记录超出执行时长阈值的命令信息，而不会像延迟监控框架那样记录多种事件。所以，记录日志的函数slowlogPushEntryIfNeeded，只会在命令执行函数call（在server.c文件中）中被调用，如下所示：\n\nvoid call(client *c, int flags) {\n    …\n   \t//命令执行前计时\n    start = server.ustime; \n    //命令实际执行\n    c->cmd->proc(c);  \n    //命令执行完成计算耗时\n    duration = ustime()-start; \n    …\n    if (flags & CMD_CALL_SLOWLOG && c->cmd->proc != execCommand) {\n        …\n        //调用 slowlogPushEntryIfNeeded 函数记录慢命令\n        slowlogPushEntryIfNeeded(c,c->argv,c->argc,duration);\n    }\n    …\n}\n\n\n\n# 总结\n\nRedis实现的延迟监控框架和慢命令日志。\n\n你要知道，Redis源码会针对可能导致Redis运行变慢的五类事件，在它们执行时进行采样。而一旦这些事件的执行时长超过阈值时，监控框架就会将采样结果记录下来，以便后续分析使用。这种针对延迟事件进行采样记录的监控方法，其实是很值得我们学习的。\n\n而慢命令日志的实现则较为简单，就是针对运行时长超出阈值的命令，使用一个列表把它们记录下来，这里面包括了命令及参数，以及发送命令的客户端信息，这样可以方便运维人员查看分析。\n\n当然，Redis源码中实现的延迟监控框架主要是关注导致延迟增加的事件，它记录的延迟事件，也是和Redis运行过程中可能会导致运行变慢的操作紧耦合的。此外，Redis的INFO命令也提供了Redis运行时的监控信息，不过你要知道，INFO命令的实现，主要是在全局变量server的成员变量中，用来记录Redis实例的实时运行状态或是资源使用情况的。\n\n\n# 参考资料\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. redis 是以低延迟著称的数据库，那么当它的响应速度变慢时，如何快速诊断出问题所在？\n 2. 你知道 redis 的哪些事件可能导致它运行缓慢吗？它们是如何被监控的？\n 3. 当 redis 的某个命令执行变慢时，我们如何捕捉这些“慢命令”并进行详细分析？\n 4. 在 redis 延迟监控中，如何通过采样机制高效记录每类事件的执行时长？\n 5. 为什么 redis 需要为延迟事件和慢命令分别设计不同的监控和日志机制？这两者的核心区别是什么？\n 6. redis 如何通过“时间序列”的方式保存和分析多次延迟事件的数据，以便为后续问题排查提供更多线索？\n 7. 在延迟监控中，redis 是如何通过统计延迟事件的最大值、最小值、均值等数据来帮助用户诊断问题的？\n 8. redis 的慢命令日志能提供哪些关键信息？如何通过这些信息定位到具体的慢命令和它的来源？\n\n\n# 前言\n\nredis的一个显著特征就是能提供低延迟的数据访问。而如果redis在运行过程中变慢了，我们就需要有方法能监控到哪些命令执行变慢了。更进一步的需求，就是我们需要有方法监控到，是redis运行过程中的哪些事件导致redis变慢了。这样一来，我们就既可以检查这些慢命令，分析它们的操作类型和访问的数据量，进而提出应对方法，也可以检查监控记录的事件，分析事件发生的原因并提出应对方法\n\n那么，为了满足这些需求，我们就需要有一套监控框架\n\n * 一方面能监测导致redis变慢的事件\n * 另一方面，能监控并记录变慢的命令。\n\n其实，这也是我们在开发后端系统时，经常会面临的一个运维开发需求，也就是如何监控后端系统的运行状态。\n\necho 来带你了解redis的延迟监控框架和慢命令日志的设计与实现。\n\n\n# 延迟监控框架的实现\n\n实际上，redis 在运行过程中，以下表格中给出的几类事件都会导致 redis 变慢，我们通常也把这些事件称为延迟事件。你可以参考表格中的这些事件类型，以及它们在源码中对应的事件名称。\n\n事件类型      源码中对应名称\n命令事件      command、fast-command\naof事件     aof-write-pending-fsync, aof-write-active-child,\n          aof-write-alone, aof-fstat, aof-rewrite-diff-write,\n          aof-rename\nfork事件    fork\n过期key事件   expire-cycie\n缓存替换事件    eviction-del, eviction-cycie\n\n那么针对这些事件，redis实现了延迟监控框架，通过采样的方式来记录它们的执行情况。当需要排查问题时，延迟监控框架提供了latency history命令，以便运维人员检查这些事件。\n\n下面，我们就来看下记录事件执行情况的数据结构。因为延迟监控框架是在latency.h和latency.c文件中实现的，你也可以在这两个文件中找到相应的数据结构和函数。\n\n\n# 记录事件执行情况的数据结构\n\n首先，redis是使用了latencysample结构体，来记录延迟事件的采样时间和事件的实际执行时长，这个结构体的代码如下所示：\n\nstruct latencysample {\n    int32_t time;  //事件的采样时间\n    uint32_t latency;  //事件的执行时长（以毫秒为单位）\n};\n\n\n而在latencysample这个结构体基础上，redis又设计了latencytimeseries结构体，这个结构体使用了一个latencysample类型的数组，记录了针对某一类事件的一系列采样结果，这样就可以为分析redis变慢提供更多的事件信息。\n\nstruct latencytimeseries {\n    int idx;  //采样事件数组的写入位置\n    uint32_t max;  //当前事件的最大延迟\n    struct latencysample samples[latency_ts_len]; //采样事件数组，记录latency_ts_len个采样结果，latency_ts_len默认为160\n};\n\n\n另外，也因为延迟监控框架要记录的延迟事件有很多种，所以 redis 还进一步设计了一个哈希表latency_events，作为全局变量server的一个成员变量，用来记录不同事件的采样结果数组，如下所示：\n\nstruct redisserver {\n   …\n   dict *latency_events;\n   …\n}\n\n\n这个哈希表是在redis server启动初始化的函数initserver中，通过调用latencymonitorinit函数来完成创建的，如下所示：\n\nvoid initserver(void) {\n    …\n    latencymonitorinit();\n}\n \nvoid latencymonitorinit(void) {\n    server.latency_events = dictcreate(&latencytimeseriesdicttype,null);\n}\n\n\n好，了解了记录延迟事件的数据结构和初始化操作后，我们再来看下事件采样是如何实现的。\n\n\n# 如何实现延迟事件的采样？\n\n延迟事件的采样函数是latencyaddsample，它的函数原型如下所示。它的参数中包含了要记录的事件名称，这实际是对应了latency_events哈希表中的一个哈希项。此外，它的参数中还包括该事件的执行时长。\n\nvoid latencyaddsample(char *event, mstime_t latency)\n\n\nlatencyaddsample函数的执行逻辑并不复杂，主要可以分成三步。\n\n首先，它会根据传入的事件名称，在latency_events哈希表中查找该事件。如果该事件对应的哈希项还不存在，它就会在哈希表中加入该事件，如下所示：\n\n//查找事件对应的哈希项\nstruct latencytimeseries *ts = dictfetchvalue(server.latency_events,event);\n…\nif (ts == null) { //如果哈希项为空，就新建哈希项\n    ts = zmalloc(sizeof(*ts));\n    ts->idx = 0;\n    ts->max = 0;\n    memset(ts->samples,0,sizeof(ts->samples));\n    dictadd(server.latency_events,zstrdup(event),ts); //在哈希表中插入哈希项\n}\n\n\n然后，latencyaddsample函数会根据传入的事件执行时间，更新当前记录的该类事件的最大执行时间，如下所示：\n\nif (latency > ts->max) ts->max = latency;\n\n\n最后，latencyaddsample函数会实际记录当前的采样结果。\n\n不过在这一步，如果它发现当前的采样结果，和前一个采样结果是在同一秒中获得的，并且如果当前采样结果的事件执行时长，大于前一个采样结果的话，那么latencyaddsample函数就会直接更新前一个采样结果中记录的执行时长了，而不是新插入一个采样结果。\n\n否则的话，latencyaddsample函数才会新插入一个采样结果。这样设计的目的，也是为了避免在同一秒中记录过多的采样结果。\n\n下面的代码展示了latencyaddsample函数实际记录采样结果的逻辑，你可以看下。\n\n//获得同类事件的前一个采样结果\nprev = (ts->idx + latency_ts_len - 1) % latency_ts_len;\n//如果当前和前一个采样结果在同一秒中\nif (ts->samples[prev].time == now) { \n    //如果当前采用结果的执行时长大于前一个采样结果\n    if (latency > ts->samples[prev].latency) \n        //直接更新前一个采样结果的执行时长\n        ts->samples[prev].latency = latency;\n    return;\n}\n//否则，新插入当前的采样结果\nts->samples[ts->idx].time = time(null);\nts->samples[ts->idx].latency = latency;\n\n\n而在这里，你也要注意一点，就是latencyaddsample函数在记录采样结果时，会重复使用采样结果数组latencytimeseries。所以，如果采样结果数量超过数组默认大小时，旧的采样结果是会被覆盖掉的。如果你要记录更多的采样结果，就需要扩大latencytimeseries数组的长度。\n\n那么，latencyaddsample函数是在什么时候调用进行采样的呢?\n\n其实，latencyaddsample函数是被封装在了latencyaddsampleifneeded函数中。在latencyaddsampleifneeded函数中，它只会在事件执行时长超过latency-monitor-threshold配置项的值时，才调用latencyaddsample函数记录采样结果。你可以看看下面给出的latencyaddsampleifneeded函数定义。\n\nlatencyaddsampleifneeded(event,var){\n    if (server.latency_monitor_threshold &&  (var) >= server.latency_monitor_threshold)\n          latencyaddsample((event),(var));\n}\n    \n\n\n而latencyaddsampleifneeded函数，实际上会在刚才介绍的延迟事件发生时被调用。这里我来给你举两个例子。\n\n比如，当redis命令通过call函数（在server.c文件中）执行时，call函数就会调用latencyaddsampleifneeded函数进行采样，如下所示：\n\nif (flags & cmd_call_slowlog && c->cmd->proc != execcommand) {\n    //根据命令数据结构中flags的cmd_fast标记，决定当前是fast-command事件还是command事件\n    char *latency_event = (c->cmd->flags & cmd_fast) ?\n        "fast-command" : "command";\n    latencyaddsampleifneeded(latency_event,duration/1000);\n    …\n}\n\n\n再比如，当redis调用flushappendonlyfile函数写aof文件时，如果aof文件刷盘的配置项是aof_fsync_always，那么flushappendonlyfile函数就会调用latencyaddsampleifneeded函数，记录aof-fsync-always延迟事件的采样结果，如下所示：\n\nvoid flushappendonlyfile(int force) {\n…\nif (server.aof_fsync == aof_fsync_always) {\nlatencystartmonitor(latency); //调用latencystartmonitor函数开始计时\nredis_fsync(server.aof_fd); //实际将数据写入磁盘\n        latencyendmonitor(latency); //调用latencyendmonitor结束计时，并计算时长\n        latencyaddsampleifneeded("aof-fsync-always",latency);\n…}\n}\n\n\n那么在这里，你需要注意的是，redis源码在调用latencyaddsampleifneeded函数记录采样结果时，经常会在延迟事件执行前，调用latencystartmonitor函数开始计时，并在事件执行结束后，调用latencyendmonitor函数结束计时和计算事件执行时长。\n\n此外，你也可以在阅读redis源码的工具中，比如sublime、sourceinsight等，通过查找函数关系调用，找到latencyaddsampleifneeded函数被调用的其他地方。\n\n好了，到这里，redis延迟监控框架就能通过latencyaddsampleifneeded函数，来记录延迟事件的采样结果了。而实际上，redis延迟监控框架还实现了延迟分析，并能提供应对延迟变慢的建议，我们再来看下。\n\n\n# 延迟分析和提供应对措施建议\n\n首先，redis是提供了latency doctor命令，来给出延迟分析结果和应对方法建议的。当我们执行这条命令的时候，redis就会使用latencycommand函数来处理。而在处理这个命令时，latencycommand函数会调用createlatencyreport函数，来生成延迟分析报告和应对方法建议。\n\n具体来说，createlatencyreport函数会针对latency_events哈希表中记录的每一类事件，先调用analyzelatencyforevent函数，计算获得采样的延迟事件执行时长的均值、最大/最小值等统计结果。具体的统计计算过程，你可以仔细阅读下analyzelatencyforevent函数的源码。\n\n然后，createlatencyreport函数会针对这类事件，结合redis配置项等信息给出应对措施。\n\n其实，在createlatencyreport函数中，它定义了多个int变量，当这些变量的值为1时，就表示建议redis使用者采用一种应对高延迟的措施。我在下面的代码中展示了部分应对措施对应的变量，你可以看下。另外你也可以阅读createlatencyreport函数源码，去了解所有的措施。\n\nsds createlatencyreport(void) {\n    …\n    int advise_slowlog_enabled = 0;  //建议启用slowlog\n    int advise_slowlog_tuning = 0;   //建议重新配置slowlog阈值\n    int advise_slowlog_inspect = 0;   //建议检查slowlog结果\n    int advise_disk_contention = 0;   //建议减少磁盘竞争\n    …\n}\n\n\n我们也来简单举个例子。比如说，针对command事件，createlatencyreport函数就会根据slowlog的设置情况，给出启用slowlog、调整slowlog阈值、检查slowlog日志结果和避免使用bigkey的应对建议。这部分代码如下所示：\n\nif (!strcasecmp(event,"command")) {\n    \n   //如果没有启用slowlog，则建议启用slowlog\n   if (server.slowlog_log_slower_than < 0) {\n       advise_slowlog_enabled = 1;\n       advices++;\n\t}  \n    //如果slowlog使用的命令时长阈值太大，建议调整slowlog阈值\n\telse if (server.slowlog_log_slower_than/1000 >server.latency_monitor_threshold){\n        advise_slowlog_tuning = 1;\n        advices++;\n    }\n    //建议检查slowlog结果\n    advise_slowlog_inspect = 1; \n    //建议避免使用bigkey\n    advise_large_objects = 1; \n    advices += 2;\n}\n\n\n所以，像createlatencyreport函数这样在计算延迟统计结果的同时，也给出应对措施的设计就很不错，这也是从redis开发者的角度给出的建议，它更具有针对性。\n\n好了，到这里，我们就了解了延迟监控框架的实现。接下来，我们再来学习下redis中慢命令日志的实现。\n\n\n# 慢命令日志的实现\n\nredis是使用了一个较为简单的方法来记录慢命令日志，也就是用一个列表，把执行时间超出慢命令日志执行时间阈值的命令记录下来。\n\n在redis全局变量server对应的数据结构redisserver中，有一个list类型的成员变量slowlog，它就是用来记录慢命令日志的列表的，如下所示：\n\nstruct redisserver {\n    …\n    list *slowlog;\n    …\n}\n\n\n而实现慢命令日志记录功能的代码是在slowlog.c文件中。这里的主要函数是slowlogpushentryifneeded，它的原型如下所示：\n\nvoid slowlogpushentryifneeded(client *c, robj **argv, int argc, long long duration)\n\n\n从代码中你可以看到，这个函数的参数包含了当前执行命令及其参数argv，以及当前命令的执行时长duration。\n\n这个函数的逻辑也不复杂，它会判断当前命令的执行时长duration，是否大于 redis.conf 配置文件中的慢命令日志阈值 slowlog-log-slower-than。如果大于的话，它就会调用slowlogcreateentry函数，为这条命令创建一条慢命令日志项，并调用listaddnodeheader函数，把这条日志项加入到日志列表头，如下所示：\n\n//当前命令的执行时长是否大于配置项\nif (duration >= server.slowlog_log_slower_than)\n   listaddnodehead(server.slowlog, slowlogcreateentry(c,argv,argc,duration));\n\n\n当然，如果日志列表中记录了太多日志项，它消耗的内存资源也会增加。所以slowlogpushentryifneeded函数在添加日志项时，会判断整个日志列表的长度是否超过配置项slowlog-max-len。一旦超过了，它就会把列表末尾的日志项删除，如下所示：\n\n//如果日志列表超过阈值长度，就删除列表末尾的日志项\nwhile (listlength(server.slowlog) > server.slowlog_max_len)\n        listdelnode(server.slowlog,listlast(server.slowlog))\n\n\n现在，我们也就了解了记录慢命令日志项的主要函数，slowlogpushentryifneeded的基本逻辑了。然后我们再来看下，它在记录日志项时调用的slowlogcreateentry函数。\n\n这个函数是用来创建一个慢命令日志项。慢命令日志项的数据结构是slowlogentry，如下所示：\n\ntypedef struct slowlogentry {\n    //日志项对应的命令及参数\n    robj **argv;     \n    //日志项对应的命令及参数个数\n    int argc;        \n    //日志项的唯一id\n    long long id; \n    //日志项对应命令的执行时长（以微秒为单位）\n    long long duration;  \n    //日志项对应命令的执行时间戳\n    time_t time;        \n    //日志项对应命令的发送客户端名称\n    sds cname;      \n    //日志项对应命令的发送客户端网络地址\n    sds peerid;         \n} slowlogentry;\n\n\n从slowlogentry的定义中，你可以看到，它会把慢命令及其参数，以及发送命令的客户端网络地址记录下来。这样设计的好处是，当我们分析慢命令日志时，就可以直接看到慢命令本身及其参数了，而且可以知道发送命令的客户端信息。而这些信息，就有利于我们排查慢命令的起因和来源。\n\n比如说，如果我们发现日志中记录的命令参数非常多，那么它就可能是一条操作bigkey的命令。\n\n当然，考虑到内存资源有限，slowlogcreateentry函数在创建慢命令日志项时，也会判断命令参数个数。如果命令参数个数，超出了阈值slowlog_entry_max_argc这个宏定义的大小（默认32）时，它就不会记录超出阈值的参数了，而是记录下剩余的参数个数。这样一来，慢命令日志项中就既记录了部分命令参数，有助于排查问题，也避免了记录过多参数，占用过多内存。\n\n下面的代码展示了slowlogcreateentry的基本执行逻辑，你可以看下。\n\nslowlogentry *slowlogcreateentry(client *c, robj **argv, int argc, long long duration) {\n    //分配日志项空间\n    slowlogentry *se = zmalloc(sizeof(*se)); \n    //待记录的参数个数，默认为当前命令的参数个数\n    int j, slargc = argc;  \n\n    //如果当前命令参数个数超出阈值，则只记录阈值个数的参数\n    if (slargc > slowlog_entry_max_argc) slargc = slowlog_entry_max_argc;\n    se->argc = slargc;\n    …\n    //逐一记录命令及参数\n    for (j = 0; j < slargc; j++) {\n        //如果命令参数个数超出阈值，使用最后一个参数记录当前命令实际剩余的参数个数\n       if (slargc != argc && j == slargc-1) {  \n          se->argv[j] = createobject(obj_string,\n                    sdscatprintf(sdsempty(),"... (%d more arguments)",\n                    argc-slargc+1));\n            } else {\n            …  //将命令参数填充到日志项中\n            }}\n    … //将命令执行时长、客户端地址等信息填充到日志项中\n}\n\n\n好了，到这里，你就了解了慢命令日志的实现。最后，你也要注意，慢命令日志只会记录超出执行时长阈值的命令信息，而不会像延迟监控框架那样记录多种事件。所以，记录日志的函数slowlogpushentryifneeded，只会在命令执行函数call（在server.c文件中）中被调用，如下所示：\n\nvoid call(client *c, int flags) {\n    …\n   \t//命令执行前计时\n    start = server.ustime; \n    //命令实际执行\n    c->cmd->proc(c);  \n    //命令执行完成计算耗时\n    duration = ustime()-start; \n    …\n    if (flags & cmd_call_slowlog && c->cmd->proc != execcommand) {\n        …\n        //调用 slowlogpushentryifneeded 函数记录慢命令\n        slowlogpushentryifneeded(c,c->argv,c->argc,duration);\n    }\n    …\n}\n\n\n\n# 总结\n\nredis实现的延迟监控框架和慢命令日志。\n\n你要知道，redis源码会针对可能导致redis运行变慢的五类事件，在它们执行时进行采样。而一旦这些事件的执行时长超过阈值时，监控框架就会将采样结果记录下来，以便后续分析使用。这种针对延迟事件进行采样记录的监控方法，其实是很值得我们学习的。\n\n而慢命令日志的实现则较为简单，就是针对运行时长超出阈值的命令，使用一个列表把它们记录下来，这里面包括了命令及参数，以及发送命令的客户端信息，这样可以方便运维人员查看分析。\n\n当然，redis源码中实现的延迟监控框架主要是关注导致延迟增加的事件，它记录的延迟事件，也是和redis运行过程中可能会导致运行变慢的操作紧耦合的。此外，redis的info命令也提供了redis运行时的监控信息，不过你要知道，info命令的实现，主要是在全局变量server的成员变量中，用来记录redis实例的实时运行状态或是资源使用情况的。\n\n\n# 参考资料\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"发布与订阅",frontmatter:{title:"发布与订阅",date:"2024-09-18T01:00:52.000Z",permalink:"/pages/61d908/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/04.%E6%94%AF%E7%BA%BF/25.%E5%8F%91%E5%B8%83%E4%B8%8E%E8%AE%A2%E9%98%85.html",relativePath:"Redis 系统设计/04.支线/25.发布与订阅.md",key:"v-2060fe15",path:"/pages/61d908/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"主从复制",frontmatter:{title:"主从复制",date:"2024-09-16T03:24:06.000Z",permalink:"/pages/ebc8dc/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E9%9B%86%E7%BE%A4/25.%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6.html",relativePath:"Redis 系统设计/05.集群/25.主从复制.md",key:"v-0c090e56",path:"/pages/ebc8dc/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:214},{level:2,title:"旧版复制",slug:"旧版复制",normalizedTitle:"旧版复制",charIndex:359},{level:3,title:"第一步：同步",slug:"第一步-同步",normalizedTitle:"第一步：同步",charIndex:459},{level:3,title:"第二步：命令传播",slug:"第二步-命令传播",normalizedTitle:"第二步：命令传播",charIndex:728},{level:3,title:"旧版的缺陷",slug:"旧版的缺陷",normalizedTitle:"旧版的缺陷",charIndex:797},{level:2,title:"新版复制",slug:"新版复制",normalizedTitle:"新版复制",charIndex:1199},{level:3,title:"部分重同步的实现",slug:"部分重同步的实现",normalizedTitle:"部分重同步的实现",charIndex:1482},{level:4,title:"复制偏移量",slug:"复制偏移量",normalizedTitle:"复制偏移量",charIndex:1545},{level:4,title:"复制积压缓冲区",slug:"复制积压缓冲区",normalizedTitle:"复制积压缓冲区",charIndex:1580},{level:4,title:"服务器运行 ID",slug:"服务器运行-id",normalizedTitle:"服务器运行 id",charIndex:2380},{level:3,title:"深入了解 PSYNC 命令",slug:"深入了解-psync-命令",normalizedTitle:"深入了解 psync 命令",charIndex:2773},{level:2,title:"复制流程详解",slug:"复制流程详解",normalizedTitle:"复制流程详解",charIndex:3613},{level:4,title:"步骤 1：设置主服务器的地址和端口",slug:"步骤-1-设置主服务器的地址和端口",normalizedTitle:"步骤 1：设置主服务器的地址和端口",charIndex:3665},{level:4,title:"步骤 2：建立套接字连接",slug:"步骤-2-建立套接字连接",normalizedTitle:"步骤 2：建立套接字连接",charIndex:4008},{level:4,title:"步骤 3：发送 PING 命令",slug:"步骤-3-发送-ping-命令",normalizedTitle:"步骤 3：发送 ping 命令",charIndex:4321},{level:4,title:"步骤 4：身份验证",slug:"步骤-4-身份验证",normalizedTitle:"步骤 4：身份验证",charIndex:4349},{level:4,title:"步骤 5：发送端口信息",slug:"步骤-5-发送端口信息",normalizedTitle:"步骤 5：发送端口信息",charIndex:4561},{level:4,title:"步骤 6：同步",slug:"步骤-6-同步",normalizedTitle:"步骤 6：同步",charIndex:4904},{level:4,title:"步骤 7：命令传播",slug:"步骤-7-命令传播",normalizedTitle:"步骤 7：命令传播",charIndex:5318},{level:2,title:"心跳检测",slug:"心跳检测",normalizedTitle:"心跳检测",charIndex:5418},{level:2,title:"主从复制的触发时机",slug:"主从复制的触发时机",normalizedTitle:"主从复制的触发时机",charIndex:6467},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:6998},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:7266}],headersStr:"前言 旧版复制 第一步：同步 第二步：命令传播 旧版的缺陷 新版复制 部分重同步的实现 复制偏移量 复制积压缓冲区 服务器运行 ID 深入了解 PSYNC 命令 复制流程详解 步骤 1：设置主服务器的地址和端口 步骤 2：建立套接字连接 步骤 3：发送 PING 命令 步骤 4：身份验证 步骤 5：发送端口信息 步骤 6：同步 步骤 7：命令传播 心跳检测 主从复制的触发时机 总结 参考文献",content:'提出问题是一切智慧的开端\n\n 1. 为什么 Redis 早期的 SYNC 操作在断线重连时效率低下？\n 2. 如何通过 replication offset 和 replication backlog 实现高效的部分重同步？\n 3. 运行ID在主从断线重连时如何帮助确定同步方式？\n 4. PSYNC 如何优化断线后重连时的主从复制效率？\n 5. 为什么即使有 TCP 传输，Redis 仍需通过心跳机制确保数据一致？\n\n\n# 前言\n\n\n\n在 Redis 中，用户可以通过 SLAVEOF 命令或设置 slaveof 选项，让一个 Redis 服务器复制另一个 Redis 服务器。我们称它们分别为从服务器（slave）和主服务器（master）。\n\n在开始之前，我假设你已经了解主从复制功能的基本使用方法。\n\n\n# 旧版复制\n\nRedis 的复制功能分为两个阶段：\n\n * 同步：用于将从服务器的数据库状态更新至主服务器的当前状态。\n * 命令传播：用于在主服务器状态被修改时，实时将修改命令告知从服务器。\n\n\n# 第一步：同步\n\n当从服务器首次发送 SLAVEOF 命令来复制主服务器时，必须执行同步（SYNC）操作。\n\n\n\n执行步骤如下：\n\n 1. 从服务器向主服务器发送 SYNC 命令。\n 2. 主服务器收到 SYNC 命令后，执行 BGSAVE 命令，并开始记录从当前时刻起的所有写命令到缓冲区中。\n 3. BGSAVE 执行完毕后，主服务器将生成的 RDB 文件发送给从服务器。\n 4. 从服务器接收并载入该 RDB 文件。\n 5. 主服务器将缓冲区中所有的写命令发送给从服务器，从服务器执行这些命令后，状态与主服务器保持一致。\n\n\n\n\n# 第二步：命令传播\n\n同步完成后，主服务器的状态如果发生变化，它会实时向从服务器发送写命令，确保从服务器的状态与主服务器保持一致。\n\n\n# 旧版的缺陷\n\n在同步过程中，可能会遇到以下两种情况：\n\n 1. 初次复制：从服务器首次复制某个主服务器的全部数据。\n 2. 断线后重复制：处于命令传播阶段的从服务器与主服务器断开连接后，再次自动连接上主服务器并继续复制。\n\n对于断线后的重复制来说，没必要再次同步整个 RDB 文件，只需同步断线期间缺失的命令即可。\n\nSYNC 命令是一个非常耗费资源的操作\n\n 1. 主服务器需要执行 BGSAVE 命令来生成 RDB 文件，这会消耗大量的 CPU、内存和磁盘 I/O 资源。\n 2. 主服务器需要将生成的 RDB 文件发送给从服务器，传输过程会占用大量网络资源，并影响主服务器响应其他命令请求的时间。\n 3. 从服务器在接收到 RDB 文件后，需要加载该文件，在加载期间从服务器将无法处理任何命令请求。\n\n由于 SYNC 操作的高资源消耗，Redis 需要确保只在必要时执行 SYNC 操作。\n\n\n# 新版复制\n\n新版复制引入的目的是解决旧版复制在断线后重新同步时的效率低下问题。\n\n自 Redis 2.8 起，使用 PSYNC 命令取代 SYNC 命令来执行复制操作。\n\nPSYNC 具有两种模式：\n\n * 完整重同步：处理初次复制的情况，其执行步骤与 SYNC 命令类似。\n * 部分重同步：用于处理断线后重复制的情况。当从服务器在断线后重新连接到主服务器时，如果条件允许，主服务器可以仅将断线期间的写命令发送给从服务器，从服务器执行这些命令后即可与主服务器保持一致。\n\nPSYNC 的部分重同步模式有效解决了旧版复制在处理断线后重复制时的低效问题。\n\n\n\n\n# 部分重同步的实现\n\n部分重同步的实现依赖以下三个关键组件：\n\n * replication offset：主服务器和从服务器的复制偏移量。\n * replication backlog：主服务器的复制积压缓冲区。\n * run ID：主服务器的运行 ID。\n\n# 复制偏移量\n\n复制的双方都会维护各自的「复制偏移量」：\n\n * 主服务器每次向从服务器传播 N 字节的数据时，会将自身的「复制偏移量」加上 N。\n * 从服务器每次收到 N 字节的数据时，也会将自身的「复制偏移量」加上 N。\n\n\n\n通过比较主从服务器的复制偏移量，可以轻松判断两者是否一致：\n\n * 如果两者的复制偏移量相同，则主从服务器处于一致状态。\n * 如果复制偏移量不同，则主从服务器处于不一致状态。\n\n# 复制积压缓冲区\n\n「复制积压缓冲区」是由主服务器维护的一个固定大小的环形缓冲区，默认大小为 1MB。缓冲区使用先进先出（FIFO）策略，当数据超过缓冲区大小时，最旧的数据会被覆盖。\n\n主服务器在进行命令传播时，除了将写命令发送给所有从服务器，还会将命令存入复制积压缓冲区。\n\n\n\n缓冲区会保存最近传播的写命令，并为每个字节记录对应的复制偏移量。\n\n\n\n当从服务器重新连接主服务器时，会通过 PSYNC 命令将自身的复制偏移量发送给主服务器，主服务器根据偏移量决定同步方式：\n\n * 如果偏移量之后的数据仍然存在于「复制积压缓冲区」中，则主服务器执行部分重同步操作。\n * 如果偏移量之后的数据已不在缓冲区中，则主服务器执行完整重同步操作。\n\n回到断线重连的例子：\n\n 1. 当从服务器 A 断线后重新连接主服务器时，发送 PSYNC 命令并报告自身的复制偏移量为 10086。\n 2. 主服务器检查「复制积压缓冲区」中的数据，确认偏移量 10086 之后的数据仍然存在，因此向从服务器返回 +CONTINUE，表示可以进行部分重同步\n 3. 主服务器将偏移量 10086 之后的数据（偏移量 10087 至 10119）发送给从服务器\n 4. 从服务器接收这 33 字节的数据后，与主服务器的状态保持一致\n\n\n\n# 服务器运行 ID\n\n除复制偏移量和复制积压缓冲区外，部分重同步还依赖于「服务器运行 ID」\n\n * 每个 Redis 服务器（无论是主服务器还是从服务器）都有一个唯一的运行 ID，由 40 个随机的十六进制字符组成，例如：53b9b28df8042fdc9ab5e3fcbbbabff1d5dce2b3\n\n运行 ID 的使用流程如下：\n\n 1. 当从服务器首次复制主服务器时，主服务器会将自己的运行 ID 发送给从服务器，从服务器保存该运行 ID\n 2. 当从服务器断线并重新连接到主服务器时，会发送之前保存的运行 ID 给当前连接的主服务器：\n    * 如果两者的运行 ID 相同，说明从服务器之前复制的就是该主服务器，主服务器可以执行部分重同步操作\n    * 如果运行 ID 不同，说明从服务器之前复制的主服务器与当前连接的主服务器不同，主服务器需要执行完整重同步操作\n\n\n# 深入了解 PSYNC 命令\n\n我们已经学习了 replication offset、replication backlog 和 run ID。接下来，我们将深入探讨 PSYNC 命令的完整细节。\n\nPSYNC 命令的调用方法有两种：\n\n 1. 从服务器的初次复制：如果从服务器以前没有复制过任何主服务器，或者之前执行过 SLAVEOF no one 命令，那么从服务器在开始新的复制时将向主服务器发送 PSYNC ? -1 命令，主动请求主服务器进行完整重同步（因为此时不可能执行部分重同步）。\n 2. 从服务器的再次复制：如果从服务器已经复制过某个主服务器，那么从服务器在开始新的复制时将向主服务器发送 PSYNC <runid> <offset> 命令，其中 runid 是上一次复制的主服务器的运行 ID，而 offset 则是从服务器当前的复制偏移量。接收到这个命令的主服务器会通过这两个参数判断应该对从服务器执行哪种同步操作。\n\n根据情况，接收到 PSYNC 命令的主服务器会向从服务器返回以下三种回复之一：\n\n 1. 如果主服务器返回 +FULLRESYNC <runid> <offset> 回复，则表示主服务器将与从服务器执行完整重同步操作。runid 是该主服务器的运行 ID，从服务器会将该 ID 保存起来，以便在下一次发送 PSYNC 命令时使用；offset 是主服务器当前的复制偏移量，从服务器会将该值作为自己的初始化偏移量。\n 2. 如果主服务器返回 +CONTINUE 回复，则表示主服务器将与从服务器执行部分重同步操作。从服务器只需等待主服务器将缺少的数据发送过来即可。\n 3. 如果主服务器返回 -ERR 回复，则表示主服务器的版本低于 Redis 2.8，无法识别 PSYNC 命令。在这种情况下，从服务器将退回到发送 SYNC 命令，并与主服务器执行完整同步操作。这种降级兼容性处理确保了主从复制在旧版本环境下仍然可以正常进行。\n\n\n\n\n# 复制流程详解\n\n我们之前讲述了新旧版本的复制细节，现在将整个复制流程串联到 Redis 主线中。\n\n# 步骤 1：设置主服务器的地址和端口\n\n当客户端向从服务器发送 SLAVEOF 127.0.0.1 6379 命令时：\n\nstruct redisServer {\n    ...\n    // 主服务器的地址\n    char *masterhost;\n    // 主服务器的端口\n    int masterport;\n    ...\n};\n\n\n从服务器在 redisServer 结构体中设置主服务器的地址和端口。\n\n注意\n\nSLAVEOF 命令是一个异步命令。在完成 masterhost 属性和 masterport 属性的设置工作后，从服务器将向发送 SLAVEOF 命令的客户端返回 OK，表示复制指令已经被接收，而实际的复制工作将在 OK 返回之后才真正开始执行。\n\n# 步骤 2：建立套接字连接\n\n在 SLAVEOF 命令执行之后，从服务器将根据命令所设置的 IP 地址和端口，创建与主服务器的套接字连接。\n\n\n\n如果从服务器创建的套接字成功连接到主服务器，则从服务器将为该套接字关联一个专门用于处理复制工作的文件事件处理器，该处理器负责执行后续的复制工作，比如接收 RDB 文件，以及接收主服务器传播来的写命令等。\n\n主服务器在接受从服务器的套接字连接后，将为该套接字创建相应的客户端状态，并将从服务器视作一个连接到主服务器的客户端。这时，从服务器同时具有服务器（server）和客户端（client）两个身份：从服务器可以向主服务器发送命令请求，而主服务器则会向从服务器返回命令回复。\n\n# 步骤 3：发送 PING 命令\n\n一图胜千言\n\n\n\n# 步骤 4：身份验证\n\n从服务器在收到主服务器返回的 "PONG" 回复之后，下一步是决定是否进行身份验证：\n\n * 如果从服务器设置了 masterauth 选项，则进行身份验证。\n * 如果从服务器没有设置 masterauth 选项，则不进行身份验证。\n\n在需要进行身份验证的情况下，从服务器主动向主服务器发送 AUTH 命令，命令的参数为从服务器 masterauth 选项的值。\n\n\n\n整个流程如下所示：\n\n\n\n# 步骤 5：发送端口信息\n\n在身份验证步骤之后，从服务器将执行命令 REPLCONF listening-port <port-number>，向主服务器发送从服务器的监听端口号。\n\n\n\n主服务器在接收到该命令后，会将端口号记录在从服务器所对应的客户端状态的 slave listening port 属性中：\n\ntypedef struct redisClient {\n    ...\n    // 从服务器的监听端口号\n    int slave_listening_port;\n    ...\n} redisClient;\n\n\n> slave_listening_port 属性目前唯一的作用是在主服务器执行 INFO replication 命令时打印出从服务器的端口号。\n\n# 步骤 6：同步\n\n在这一步，从服务器将向主服务器发送 PSYNC 命令，执行同步操作，并将自己的数据库更新至主服务器数据库当前所处的状态。值得一提的是，在同步操作执行之前，只有从服务器是主服务器的客户端；但在执行同步操作之后，主服务器也会成为从服务器的客户端。\n\n * 如果 PSYNC 命令执行的是完整重同步操作，那么主服务器需要成为从服务器的客户端，才能将保存在缓冲区中的写命令发送给从服务器执行。\n * 如果 PSYNC 命令执行的是部分重同步操作，那么主服务器需要成为从服务器的客户端，才能向从服务器发送保存在复制积压缓冲区中的写命令。\n\n因此，在同步操作执行之后，主从服务器双方都是对方的客户端，它们可以互相发送命令请求或返回命令回复。\n\n正因为主服务器成为了从服务器的客户端，主服务器才能通过发送写命令来改变从服务器的数据库状态。这不仅在同步操作中需要用到，也是主服务器对从服务器执行命令传播的基础操作。\n\n\n\n# 步骤 7：命令传播\n\n完成同步后，主从服务器将进入命令传播阶段。此时，主服务器将持续将其执行的写命令发送给从服务器，而从服务器则持续接收并执行主服务器发来的写命令，以保持主从服务器的一致性。\n\n\n# 心跳检测\n\n在命令传播阶段，从服务器默认以每秒一次的频率，向主服务器发送 REPLCONF ACK <replication offset> 命令，其中 replication offset 是从服务器当前的复制偏移量。\n\n发送 REPLCONF ACK 命令对于主从服务器有三个作用：\n\n 1. 检测主从服务器的网络连接状态：主从服务器通过发送和接收 REPLCONF ACK 命令检查网络连接是否正常，同时监测网络延迟。主服务器通过监测从服务器的复制偏移量，可以快速感知网络抖动和数据同步状态，从而及时识别和处理可能的数据丢失或延迟问题。心跳机制的及时响应有助于主服务器快速感知与从服务器的连接状态，当心跳丢失时，主服务器可以立即采取措施，保证数据的一致性和可靠性。\n\n 2. 辅助实现 min-slaves 配置选项：Redis 的 min-slaves-to-write 和 min-slaves-max-lag 两个选项可以防止主服务器在不安全的情况下执行写命令。例如，如果设置了以下选项：\n    \n    * min-slaves-to-write 3\n    * min-slaves-max-lag 10\n    \n    那么在从服务器数量少于 3 个，或者三个从服务器的延迟值都大于或等于 10 秒时，主服务器将拒绝执行写命令。\n\n 3. 检测命令丢失：如果因为网络故障，主服务器传播给从服务器的写命令在途中丢失，那么从服务器需要通过不断增加的偏移量来告知主服务器，主服务器将检查其复制积压缓冲区，补发丢失的写命令。\n\n已经使用 TCP 了，为什么还要保证消息可靠传达?\n\n\n\n注意\n\n * 在传输层，TCP的三次握手保证了双方通讯的可靠性，稳定性。简而言之，用户发送的消息， 在忽视应用层的情况下，无论如何都会从自身主机的 “发送缓冲区” 抵达对方主机的 “接收缓冲区”\n * 在应用层，数据包有可能因为用户突然的切后台或者是弱网状态导致没法从操作系统内核抵达应用层，反之也是如此, 为此,我们需要在应用层做好可靠传输协议的保证，防止数据丢失的情况\n\n所以，尽管 TCP 能保证传输层的数据可靠性，但在应用层，数据包有可能因为网络不稳定等因素导致丢失。因此，Redis 使用 REPLCONF ACK 命令来在应用层增加传输可靠性。REPLCONF ACK 命令不仅可以检测网络延迟，还能通过主服务器监控从服务器的复制偏移量，及时重传丢失的数据，确保数据的一致性。\n\n\n# 主从复制的触发时机\n\n 1. 从节点首次启动或重新连接时：从节点连接到主节点后，触发全量或增量同步。\n 2. 主节点有数据更新时：主节点每次执行写操作后，增量数据会实时同步给从节点。\n 3. 主节点的手动或自动备份操作：执行 BGSAVE、BGREWRITEAOF 等命令时，可能触发主从数据同步。\n 4. 主从配置变更或故障转移时：如通过 Sentinel 或集群模式进行故障转移，或者手动执行 SLAVEOF 命令时，从节点会重新与新的主节点同步。\n 5. 从节点主动请求同步：从节点落后于主节点的数据或出现不一致时，会主动请求主节点重新同步。\n 6. 复制积压缓冲区溢出：如果从节点长时间未同步，主节点的缓冲区溢出，触发全量同步。\n 7. 集群模式下的主从复制：在 Redis 集群模式中，当集群分片中的某个主节点故障时，集群会通过选举机制选择一个新的主节点。被选中的从节点会与其他从节点进行主从复制，保持数据一致性。\n 8. Sentinel 模式的主从复制：在 Sentinel 模式下，Sentinel 负责监控主从节点的健康状态。当主节点故障时，Sentinel 会自动将某个从节点提升为新的主节点，并通知其他从节点与新的主节点进行同步。\n\n\n# 总结\n\n * Redis 2.8 以前的复制功能不能高效地处理断线后重复制情况，但Redis 2.8新添加的部分重同步功能可以解决这个问题。\n * 部分重同步通过复制偏移量、复制积压缓冲区、服务器运行ID三个部分来实现\n * 在复制操作刚开始的时候，从服务器会成为主服务器的客户端，并通过向主服务器发送命令请求来执行复制步骤，而在复制操作的后期，主从服务器会互相成为对方的客户端。\n * 主服务器通过向从服务器传播命令来更新从服务器的状态，保持主从服务器一致而从服务器则通过向主服务器发送命令来进行心跳检测，以及命令丢失检测。\n\n\n# 参考文献\n\n * 彻底搞懂Redis主从复制原理及实战 - cooffeeli - 博客园 (cnblogs.com)\n\n * 深入学习Redis（3）：主从复制 - 编程迷思 - 博客园 (cnblogs.com)\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'提出问题是一切智慧的开端\n\n 1. 为什么 redis 早期的 sync 操作在断线重连时效率低下？\n 2. 如何通过 replication offset 和 replication backlog 实现高效的部分重同步？\n 3. 运行id在主从断线重连时如何帮助确定同步方式？\n 4. psync 如何优化断线后重连时的主从复制效率？\n 5. 为什么即使有 tcp 传输，redis 仍需通过心跳机制确保数据一致？\n\n\n# 前言\n\n\n\n在 redis 中，用户可以通过 slaveof 命令或设置 slaveof 选项，让一个 redis 服务器复制另一个 redis 服务器。我们称它们分别为从服务器（slave）和主服务器（master）。\n\n在开始之前，我假设你已经了解主从复制功能的基本使用方法。\n\n\n# 旧版复制\n\nredis 的复制功能分为两个阶段：\n\n * 同步：用于将从服务器的数据库状态更新至主服务器的当前状态。\n * 命令传播：用于在主服务器状态被修改时，实时将修改命令告知从服务器。\n\n\n# 第一步：同步\n\n当从服务器首次发送 slaveof 命令来复制主服务器时，必须执行同步（sync）操作。\n\n\n\n执行步骤如下：\n\n 1. 从服务器向主服务器发送 sync 命令。\n 2. 主服务器收到 sync 命令后，执行 bgsave 命令，并开始记录从当前时刻起的所有写命令到缓冲区中。\n 3. bgsave 执行完毕后，主服务器将生成的 rdb 文件发送给从服务器。\n 4. 从服务器接收并载入该 rdb 文件。\n 5. 主服务器将缓冲区中所有的写命令发送给从服务器，从服务器执行这些命令后，状态与主服务器保持一致。\n\n\n\n\n# 第二步：命令传播\n\n同步完成后，主服务器的状态如果发生变化，它会实时向从服务器发送写命令，确保从服务器的状态与主服务器保持一致。\n\n\n# 旧版的缺陷\n\n在同步过程中，可能会遇到以下两种情况：\n\n 1. 初次复制：从服务器首次复制某个主服务器的全部数据。\n 2. 断线后重复制：处于命令传播阶段的从服务器与主服务器断开连接后，再次自动连接上主服务器并继续复制。\n\n对于断线后的重复制来说，没必要再次同步整个 rdb 文件，只需同步断线期间缺失的命令即可。\n\nsync 命令是一个非常耗费资源的操作\n\n 1. 主服务器需要执行 bgsave 命令来生成 rdb 文件，这会消耗大量的 cpu、内存和磁盘 i/o 资源。\n 2. 主服务器需要将生成的 rdb 文件发送给从服务器，传输过程会占用大量网络资源，并影响主服务器响应其他命令请求的时间。\n 3. 从服务器在接收到 rdb 文件后，需要加载该文件，在加载期间从服务器将无法处理任何命令请求。\n\n由于 sync 操作的高资源消耗，redis 需要确保只在必要时执行 sync 操作。\n\n\n# 新版复制\n\n新版复制引入的目的是解决旧版复制在断线后重新同步时的效率低下问题。\n\n自 redis 2.8 起，使用 psync 命令取代 sync 命令来执行复制操作。\n\npsync 具有两种模式：\n\n * 完整重同步：处理初次复制的情况，其执行步骤与 sync 命令类似。\n * 部分重同步：用于处理断线后重复制的情况。当从服务器在断线后重新连接到主服务器时，如果条件允许，主服务器可以仅将断线期间的写命令发送给从服务器，从服务器执行这些命令后即可与主服务器保持一致。\n\npsync 的部分重同步模式有效解决了旧版复制在处理断线后重复制时的低效问题。\n\n\n\n\n# 部分重同步的实现\n\n部分重同步的实现依赖以下三个关键组件：\n\n * replication offset：主服务器和从服务器的复制偏移量。\n * replication backlog：主服务器的复制积压缓冲区。\n * run id：主服务器的运行 id。\n\n# 复制偏移量\n\n复制的双方都会维护各自的「复制偏移量」：\n\n * 主服务器每次向从服务器传播 n 字节的数据时，会将自身的「复制偏移量」加上 n。\n * 从服务器每次收到 n 字节的数据时，也会将自身的「复制偏移量」加上 n。\n\n\n\n通过比较主从服务器的复制偏移量，可以轻松判断两者是否一致：\n\n * 如果两者的复制偏移量相同，则主从服务器处于一致状态。\n * 如果复制偏移量不同，则主从服务器处于不一致状态。\n\n# 复制积压缓冲区\n\n「复制积压缓冲区」是由主服务器维护的一个固定大小的环形缓冲区，默认大小为 1mb。缓冲区使用先进先出（fifo）策略，当数据超过缓冲区大小时，最旧的数据会被覆盖。\n\n主服务器在进行命令传播时，除了将写命令发送给所有从服务器，还会将命令存入复制积压缓冲区。\n\n\n\n缓冲区会保存最近传播的写命令，并为每个字节记录对应的复制偏移量。\n\n\n\n当从服务器重新连接主服务器时，会通过 psync 命令将自身的复制偏移量发送给主服务器，主服务器根据偏移量决定同步方式：\n\n * 如果偏移量之后的数据仍然存在于「复制积压缓冲区」中，则主服务器执行部分重同步操作。\n * 如果偏移量之后的数据已不在缓冲区中，则主服务器执行完整重同步操作。\n\n回到断线重连的例子：\n\n 1. 当从服务器 a 断线后重新连接主服务器时，发送 psync 命令并报告自身的复制偏移量为 10086。\n 2. 主服务器检查「复制积压缓冲区」中的数据，确认偏移量 10086 之后的数据仍然存在，因此向从服务器返回 +continue，表示可以进行部分重同步\n 3. 主服务器将偏移量 10086 之后的数据（偏移量 10087 至 10119）发送给从服务器\n 4. 从服务器接收这 33 字节的数据后，与主服务器的状态保持一致\n\n\n\n# 服务器运行 id\n\n除复制偏移量和复制积压缓冲区外，部分重同步还依赖于「服务器运行 id」\n\n * 每个 redis 服务器（无论是主服务器还是从服务器）都有一个唯一的运行 id，由 40 个随机的十六进制字符组成，例如：53b9b28df8042fdc9ab5e3fcbbbabff1d5dce2b3\n\n运行 id 的使用流程如下：\n\n 1. 当从服务器首次复制主服务器时，主服务器会将自己的运行 id 发送给从服务器，从服务器保存该运行 id\n 2. 当从服务器断线并重新连接到主服务器时，会发送之前保存的运行 id 给当前连接的主服务器：\n    * 如果两者的运行 id 相同，说明从服务器之前复制的就是该主服务器，主服务器可以执行部分重同步操作\n    * 如果运行 id 不同，说明从服务器之前复制的主服务器与当前连接的主服务器不同，主服务器需要执行完整重同步操作\n\n\n# 深入了解 psync 命令\n\n我们已经学习了 replication offset、replication backlog 和 run id。接下来，我们将深入探讨 psync 命令的完整细节。\n\npsync 命令的调用方法有两种：\n\n 1. 从服务器的初次复制：如果从服务器以前没有复制过任何主服务器，或者之前执行过 slaveof no one 命令，那么从服务器在开始新的复制时将向主服务器发送 psync ? -1 命令，主动请求主服务器进行完整重同步（因为此时不可能执行部分重同步）。\n 2. 从服务器的再次复制：如果从服务器已经复制过某个主服务器，那么从服务器在开始新的复制时将向主服务器发送 psync <runid> <offset> 命令，其中 runid 是上一次复制的主服务器的运行 id，而 offset 则是从服务器当前的复制偏移量。接收到这个命令的主服务器会通过这两个参数判断应该对从服务器执行哪种同步操作。\n\n根据情况，接收到 psync 命令的主服务器会向从服务器返回以下三种回复之一：\n\n 1. 如果主服务器返回 +fullresync <runid> <offset> 回复，则表示主服务器将与从服务器执行完整重同步操作。runid 是该主服务器的运行 id，从服务器会将该 id 保存起来，以便在下一次发送 psync 命令时使用；offset 是主服务器当前的复制偏移量，从服务器会将该值作为自己的初始化偏移量。\n 2. 如果主服务器返回 +continue 回复，则表示主服务器将与从服务器执行部分重同步操作。从服务器只需等待主服务器将缺少的数据发送过来即可。\n 3. 如果主服务器返回 -err 回复，则表示主服务器的版本低于 redis 2.8，无法识别 psync 命令。在这种情况下，从服务器将退回到发送 sync 命令，并与主服务器执行完整同步操作。这种降级兼容性处理确保了主从复制在旧版本环境下仍然可以正常进行。\n\n\n\n\n# 复制流程详解\n\n我们之前讲述了新旧版本的复制细节，现在将整个复制流程串联到 redis 主线中。\n\n# 步骤 1：设置主服务器的地址和端口\n\n当客户端向从服务器发送 slaveof 127.0.0.1 6379 命令时：\n\nstruct redisserver {\n    ...\n    // 主服务器的地址\n    char *masterhost;\n    // 主服务器的端口\n    int masterport;\n    ...\n};\n\n\n从服务器在 redisserver 结构体中设置主服务器的地址和端口。\n\n注意\n\nslaveof 命令是一个异步命令。在完成 masterhost 属性和 masterport 属性的设置工作后，从服务器将向发送 slaveof 命令的客户端返回 ok，表示复制指令已经被接收，而实际的复制工作将在 ok 返回之后才真正开始执行。\n\n# 步骤 2：建立套接字连接\n\n在 slaveof 命令执行之后，从服务器将根据命令所设置的 ip 地址和端口，创建与主服务器的套接字连接。\n\n\n\n如果从服务器创建的套接字成功连接到主服务器，则从服务器将为该套接字关联一个专门用于处理复制工作的文件事件处理器，该处理器负责执行后续的复制工作，比如接收 rdb 文件，以及接收主服务器传播来的写命令等。\n\n主服务器在接受从服务器的套接字连接后，将为该套接字创建相应的客户端状态，并将从服务器视作一个连接到主服务器的客户端。这时，从服务器同时具有服务器（server）和客户端（client）两个身份：从服务器可以向主服务器发送命令请求，而主服务器则会向从服务器返回命令回复。\n\n# 步骤 3：发送 ping 命令\n\n一图胜千言\n\n\n\n# 步骤 4：身份验证\n\n从服务器在收到主服务器返回的 "pong" 回复之后，下一步是决定是否进行身份验证：\n\n * 如果从服务器设置了 masterauth 选项，则进行身份验证。\n * 如果从服务器没有设置 masterauth 选项，则不进行身份验证。\n\n在需要进行身份验证的情况下，从服务器主动向主服务器发送 auth 命令，命令的参数为从服务器 masterauth 选项的值。\n\n\n\n整个流程如下所示：\n\n\n\n# 步骤 5：发送端口信息\n\n在身份验证步骤之后，从服务器将执行命令 replconf listening-port <port-number>，向主服务器发送从服务器的监听端口号。\n\n\n\n主服务器在接收到该命令后，会将端口号记录在从服务器所对应的客户端状态的 slave listening port 属性中：\n\ntypedef struct redisclient {\n    ...\n    // 从服务器的监听端口号\n    int slave_listening_port;\n    ...\n} redisclient;\n\n\n> slave_listening_port 属性目前唯一的作用是在主服务器执行 info replication 命令时打印出从服务器的端口号。\n\n# 步骤 6：同步\n\n在这一步，从服务器将向主服务器发送 psync 命令，执行同步操作，并将自己的数据库更新至主服务器数据库当前所处的状态。值得一提的是，在同步操作执行之前，只有从服务器是主服务器的客户端；但在执行同步操作之后，主服务器也会成为从服务器的客户端。\n\n * 如果 psync 命令执行的是完整重同步操作，那么主服务器需要成为从服务器的客户端，才能将保存在缓冲区中的写命令发送给从服务器执行。\n * 如果 psync 命令执行的是部分重同步操作，那么主服务器需要成为从服务器的客户端，才能向从服务器发送保存在复制积压缓冲区中的写命令。\n\n因此，在同步操作执行之后，主从服务器双方都是对方的客户端，它们可以互相发送命令请求或返回命令回复。\n\n正因为主服务器成为了从服务器的客户端，主服务器才能通过发送写命令来改变从服务器的数据库状态。这不仅在同步操作中需要用到，也是主服务器对从服务器执行命令传播的基础操作。\n\n\n\n# 步骤 7：命令传播\n\n完成同步后，主从服务器将进入命令传播阶段。此时，主服务器将持续将其执行的写命令发送给从服务器，而从服务器则持续接收并执行主服务器发来的写命令，以保持主从服务器的一致性。\n\n\n# 心跳检测\n\n在命令传播阶段，从服务器默认以每秒一次的频率，向主服务器发送 replconf ack <replication offset> 命令，其中 replication offset 是从服务器当前的复制偏移量。\n\n发送 replconf ack 命令对于主从服务器有三个作用：\n\n 1. 检测主从服务器的网络连接状态：主从服务器通过发送和接收 replconf ack 命令检查网络连接是否正常，同时监测网络延迟。主服务器通过监测从服务器的复制偏移量，可以快速感知网络抖动和数据同步状态，从而及时识别和处理可能的数据丢失或延迟问题。心跳机制的及时响应有助于主服务器快速感知与从服务器的连接状态，当心跳丢失时，主服务器可以立即采取措施，保证数据的一致性和可靠性。\n\n 2. 辅助实现 min-slaves 配置选项：redis 的 min-slaves-to-write 和 min-slaves-max-lag 两个选项可以防止主服务器在不安全的情况下执行写命令。例如，如果设置了以下选项：\n    \n    * min-slaves-to-write 3\n    * min-slaves-max-lag 10\n    \n    那么在从服务器数量少于 3 个，或者三个从服务器的延迟值都大于或等于 10 秒时，主服务器将拒绝执行写命令。\n\n 3. 检测命令丢失：如果因为网络故障，主服务器传播给从服务器的写命令在途中丢失，那么从服务器需要通过不断增加的偏移量来告知主服务器，主服务器将检查其复制积压缓冲区，补发丢失的写命令。\n\n已经使用 tcp 了，为什么还要保证消息可靠传达?\n\n\n\n注意\n\n * 在传输层，tcp的三次握手保证了双方通讯的可靠性，稳定性。简而言之，用户发送的消息， 在忽视应用层的情况下，无论如何都会从自身主机的 “发送缓冲区” 抵达对方主机的 “接收缓冲区”\n * 在应用层，数据包有可能因为用户突然的切后台或者是弱网状态导致没法从操作系统内核抵达应用层，反之也是如此, 为此,我们需要在应用层做好可靠传输协议的保证，防止数据丢失的情况\n\n所以，尽管 tcp 能保证传输层的数据可靠性，但在应用层，数据包有可能因为网络不稳定等因素导致丢失。因此，redis 使用 replconf ack 命令来在应用层增加传输可靠性。replconf ack 命令不仅可以检测网络延迟，还能通过主服务器监控从服务器的复制偏移量，及时重传丢失的数据，确保数据的一致性。\n\n\n# 主从复制的触发时机\n\n 1. 从节点首次启动或重新连接时：从节点连接到主节点后，触发全量或增量同步。\n 2. 主节点有数据更新时：主节点每次执行写操作后，增量数据会实时同步给从节点。\n 3. 主节点的手动或自动备份操作：执行 bgsave、bgrewriteaof 等命令时，可能触发主从数据同步。\n 4. 主从配置变更或故障转移时：如通过 sentinel 或集群模式进行故障转移，或者手动执行 slaveof 命令时，从节点会重新与新的主节点同步。\n 5. 从节点主动请求同步：从节点落后于主节点的数据或出现不一致时，会主动请求主节点重新同步。\n 6. 复制积压缓冲区溢出：如果从节点长时间未同步，主节点的缓冲区溢出，触发全量同步。\n 7. 集群模式下的主从复制：在 redis 集群模式中，当集群分片中的某个主节点故障时，集群会通过选举机制选择一个新的主节点。被选中的从节点会与其他从节点进行主从复制，保持数据一致性。\n 8. sentinel 模式的主从复制：在 sentinel 模式下，sentinel 负责监控主从节点的健康状态。当主节点故障时，sentinel 会自动将某个从节点提升为新的主节点，并通知其他从节点与新的主节点进行同步。\n\n\n# 总结\n\n * redis 2.8 以前的复制功能不能高效地处理断线后重复制情况，但redis 2.8新添加的部分重同步功能可以解决这个问题。\n * 部分重同步通过复制偏移量、复制积压缓冲区、服务器运行id三个部分来实现\n * 在复制操作刚开始的时候，从服务器会成为主服务器的客户端，并通过向主服务器发送命令请求来执行复制步骤，而在复制操作的后期，主从服务器会互相成为对方的客户端。\n * 主服务器通过向从服务器传播命令来更新从服务器的状态，保持主从服务器一致而从服务器则通过向主服务器发送命令来进行心跳检测，以及命令丢失检测。\n\n\n# 参考文献\n\n * 彻底搞懂redis主从复制原理及实战 - cooffeeli - 博客园 (cnblogs.com)\n\n * 深入学习redis（3）：主从复制 - 编程迷思 - 博客园 (cnblogs.com)\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"哨兵",frontmatter:{title:"哨兵",date:"2024-09-16T03:24:20.000Z",permalink:"/pages/af8752/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E9%9B%86%E7%BE%A4/30.%E5%93%A8%E5%85%B5.html",relativePath:"Redis 系统设计/05.集群/30.哨兵.md",key:"v-00b1d986",path:"/pages/af8752/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/09/16, 13:27:00",lastUpdatedTimestamp:172649322e4},{title:"cluster",frontmatter:{title:"cluster",date:"2024-09-16T03:24:30.000Z",permalink:"/pages/040403/"},regularPath:"/Redis%20%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/05.%E9%9B%86%E7%BE%A4/35.cluster.html",relativePath:"Redis 系统设计/05.集群/35.cluster.md",key:"v-1db3f2e7",path:"/pages/040403/",headers:[{level:2,title:"前言",slug:"前言",normalizedTitle:"前言",charIndex:2},{level:2,title:"节点",slug:"节点",normalizedTitle:"节点",charIndex:80},{level:3,title:"启动节点",slug:"启动节点",normalizedTitle:"启动节点",charIndex:1581},{level:3,title:"集群数据结构",slug:"集群数据结构",normalizedTitle:"集群数据结构",charIndex:2426},{level:3,title:"CLUSTER MEET 命令的实现",slug:"cluster-meet-命令的实现",normalizedTitle:"cluster meet 命令的实现",charIndex:1605},{level:2,title:"槽指派",slug:"槽指派",normalizedTitle:"槽指派",charIndex:83},{level:3,title:"记录节点的槽指派信息",slug:"记录节点的槽指派信息",normalizedTitle:"记录节点的槽指派信息",charIndex:7698},{level:3,title:"传播节点的槽指派信息",slug:"传播节点的槽指派信息",normalizedTitle:"传播节点的槽指派信息",charIndex:8508},{level:3,title:"记录集群所有槽的指派信息",slug:"记录集群所有槽的指派信息",normalizedTitle:"记录集群所有槽的指派信息",charIndex:9243},{level:3,title:"CLUSTER ADDSLOTS 命令的实现",slug:"cluster-addslots-命令的实现",normalizedTitle:"cluster addslots 命令的实现",charIndex:10876},{level:2,title:"在集群中执行命令",slug:"在集群中执行命令",normalizedTitle:"在集群中执行命令",charIndex:12087},{level:3,title:"计算键属于哪个槽",slug:"计算键属于哪个槽",normalizedTitle:"计算键属于哪个槽",charIndex:13006},{level:3,title:"判断槽是否由当前节点负责处理",slug:"判断槽是否由当前节点负责处理",normalizedTitle:"判断槽是否由当前节点负责处理",charIndex:13589},{level:3,title:"MOVED 错误",slug:"moved-错误",normalizedTitle:"moved 错误",charIndex:14442},{level:3,title:"节点数据库的实现",slug:"节点数据库的实现",normalizedTitle:"节点数据库的实现",charIndex:15837},{level:2,title:"重新分片",slug:"重新分片",normalizedTitle:"重新分片",charIndex:92},{level:2,title:"ASK 错误",slug:"ask-错误",normalizedTitle:"ask 错误",charIndex:18918},{level:3,title:"CLUSTER SETSLOT IMPORTING命令的实现",slug:"cluster-setslot-importing命令的实现",normalizedTitle:"cluster setslot importing命令的实现",charIndex:20035},{level:3,title:"CLUSTER SETSLOT MIGRATING 命令的实现",slug:"cluster-setslot-migrating-命令的实现",normalizedTitle:"cluster setslot migrating 命令的实现",charIndex:20702},{level:3,title:"ASK 错误",slug:"ask-错误-2",normalizedTitle:"ask 错误",charIndex:18918},{level:3,title:"ASKING命令",slug:"asking命令",normalizedTitle:"asking命令",charIndex:22001},{level:3,title:"ASK错误和MOVED错误的区别",slug:"ask错误和moved错误的区别",normalizedTitle:"ask错误和moved错误的区别",charIndex:20013},{level:2,title:"复制与故障转移",slug:"复制与故障转移",normalizedTitle:"复制与故障转移",charIndex:23853},{level:3,title:"设置从节点",slug:"设置从节点",normalizedTitle:"设置从节点",charIndex:24703},{level:3,title:"故障检测",slug:"故障检测",normalizedTitle:"故障检测",charIndex:26137},{level:3,title:"故障转移",slug:"故障转移",normalizedTitle:"故障转移",charIndex:64},{level:3,title:"选举新的主节点",slug:"选举新的主节点",normalizedTitle:"选举新的主节点",charIndex:27922},{level:2,title:"消息",slug:"消息",normalizedTitle:"消息",charIndex:105},{level:3,title:"消息头",slug:"消息头",normalizedTitle:"消息头",charIndex:29584},{level:3,title:"MEET、PING、PONG消息的实现",slug:"meet、ping、pong消息的实现",normalizedTitle:"meet、ping、pong消息的实现",charIndex:31234},{level:3,title:"FAIL消息的实现",slug:"fail消息的实现",normalizedTitle:"fail消息的实现",charIndex:32699},{level:3,title:"PUBLISH消息的实现",slug:"publish消息的实现",normalizedTitle:"publish消息的实现",charIndex:33619},{level:2,title:"重点回顾",slug:"重点回顾",normalizedTitle:"重点回顾",charIndex:34943},{level:2,title:"参考文献",slug:"参考文献",normalizedTitle:"参考文献",charIndex:35516}],headersStr:"前言 节点 启动节点 集群数据结构 CLUSTER MEET 命令的实现 槽指派 记录节点的槽指派信息 传播节点的槽指派信息 记录集群所有槽的指派信息 CLUSTER ADDSLOTS 命令的实现 在集群中执行命令 计算键属于哪个槽 判断槽是否由当前节点负责处理 MOVED 错误 节点数据库的实现 重新分片 ASK 错误 CLUSTER SETSLOT IMPORTING命令的实现 CLUSTER SETSLOT MIGRATING 命令的实现 ASK 错误 ASKING命令 ASK错误和MOVED错误的区别 复制与故障转移 设置从节点 故障检测 故障转移 选举新的主节点 消息 消息头 MEET、PING、PONG消息的实现 FAIL消息的实现 PUBLISH消息的实现 重点回顾 参考文献",content:'# 前言\n\nRedis 集群是 Redis 提供的分布式数据库方案，集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能。\n\n本节将对集群的节点、槽指派、命令执行、重新分片、转向、故障转移、消息等各个方面进行介绍\n\n\n# 节点\n\n一个 Redis 集群通常由多个节点（node）组成， 在刚开始的时候， 每个节点都是相互独立的， 它们都处于一个只包含自己的集群当中， 要组建一个真正可工作的集群， 我们必须将各个独立的节点连接起来， 构成一个包含多个节点的集群。\n\n连接各个节点的工作可以使用 CLUSTER MEET 命令来完成， 该命令的格式如下：\n\nCLUSTER MEET <ip> <port>\n\n\n向一个节点 node 发送 CLUSTER MEET 命令， 可以让 node 节点与 ip 和 port 所指定的节点进行握手（handshake）， 当握手成功时， node 节点就会将 ip 和 port 所指定的节点添加到 node 节点当前所在的集群中。\n\n举个例子， 假设现在有三个独立的节点 127.0.0.1:7000 、 127.0.0.1:7001 、 127.0.0.1:7002 （下文省略 IP 地址，直接使用端口号来区分各个节点）， 我们首先使用客户端连上节点 7000 ， 通过发送 CLUSTER NODE 命令可以看到， 集群目前只包含 7000 自己一个节点：\n\n$ redis-cli -c -p 7000\n127.0.0.1:7000> CLUSTER NODES\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n通过向节点 7000 发送以下命令， 我们可以将节点 7001 添加到节点 7000 所在的集群里面：\n\n127.0.0.1:7000> CLUSTER MEET 127.0.0.1 7001\nOK\n\n127.0.0.1:7000> CLUSTER NODES\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388204746210 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n继续向节点 7000 发送以下命令， 我们可以将节点 7002 也添加到节点 7000 和节点 7001 所在的集群里面：\n\n127.0.0.1:7000> CLUSTER MEET 127.0.0.1 7002\nOK\n\n127.0.0.1:7000> CLUSTER NODES\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388204848376 0 connected\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388204847977 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n现在， 这个集群里面包含了 7000 、 7001 和 7002 三个节点， 图 IMAGE_CONNECT_NODES_1 至 IMAGE_CONNECT_NODES_5 展示了这三个节点进行握手的整个过程。\n\n\n\n\n\n\n\n\n\n\n\n本节接下来的内容将介绍启动节点的方法， 和集群有关的数据结构， 以及 CLUSTER MEET 命令的实现原理。\n\n\n# 启动节点\n\n一个节点就是一个运行在集群模式下的 Redis 服务器， Redis 服务器在启动时会根据 cluster-enabled 配置选项的是否为 yes 来决定是否开启服务器的集群模式， 如图 IMAGE_NODE_OR_SERVER 所示。\n\n\n\n节点（运行在集群模式下的 Redis 服务器）会继续使用所有在单机模式中使用的服务器组件， 比如说：\n\n * 节点会继续使用文件事件处理器来处理命令请求和返回命令回复。\n * 节点会继续使用时间事件处理器来执行 serverCron 函数， 而 serverCron 函数又会调用集群模式特有的 clusterCron 函数： clusterCron 函数负责执行在集群模式下需要执行的常规操作， 比如向集群中的其他节点发送 Gossip 消息， 检查节点是否断线； 又或者检查是否需要对下线节点进行自动故障转移， 等等。\n * 节点会继续使用数据库来保存键值对数据，键值对依然会是各种不同类型的对象。\n * 节点会继续使用 RDB 持久化模块和 AOF 持久化模块来执行持久化工作。\n * 节点会继续使用发布与订阅模块来执行 PUBLISH 、 SUBSCRIBE 等命令。\n * 节点会继续使用复制模块来进行节点的复制工作。\n * 节点会继续使用 Lua 脚本环境来执行客户端输入的 Lua 脚本。\n\n诸如此类。\n\n除此之外， 节点会继续使用 redisServer 结构来保存服务器的状态， 使用 redisClient 结构来保存客户端的状态， 至于那些只有在集群模式下才会用到的数据， 节点将它们保存到了 cluster.h/clusterNode 结构， cluster.h/clusterLink 结构， 以及 cluster.h/clusterState 结构里面， 接下来的一节将对这三种数据结构进行介绍\n\n\n# 集群数据结构\n\nclusterNode 结构保存了一个节点的当前状态， 比如节点的创建时间， 节点的名字， 节点当前的配置纪元， 节点的 IP 和地址， 等等。\n\n每个节点都会使用一个 clusterNode 结构来记录自己的状态， 并为集群中的所有其他节点（包括主节点和从节点）都创建一个相应的 clusterNode 结构， 以此来记录其他节点的状态：\n\nstruct clusterNode {\n\n    // 创建节点的时间\n    mstime_t ctime;\n\n    // 节点的名字，由 40 个十六进制字符组成\n    // 例如 68eef66df23420a5862208ef5b1a7005b806f2ff\n    char name[REDIS_CLUSTER_NAMELEN];\n\n    // 节点标识\n    // 使用各种不同的标识值记录节点的角色（比如主节点或者从节点），\n    // 以及节点目前所处的状态（比如在线或者下线）。\n    int flags;\n\n    // 节点当前的配置纪元，用于实现故障转移\n    uint64_t configEpoch;\n\n    // 节点的 IP 地址\n    char ip[REDIS_IP_STR_LEN];\n\n    // 节点的端口号\n    int port;\n\n    // 保存连接节点所需的有关信息\n    clusterLink *link;\n\n};\n\n\nclusterNode 结构的 link 属性是一个 clusterLink 结构， 该结构保存了连接节点所需的有关信息， 比如套接字描述符， 输入缓冲区和输出缓冲区：\n\ntypedef struct clusterLink {\n\n    // 连接的创建时间\n    mstime_t ctime;\n\n    // TCP 套接字描述符\n    int fd;\n\n    // 输出缓冲区，保存着等待发送给其他节点的消息（message）。\n    sds sndbuf;\n\n    // 输入缓冲区，保存着从其他节点接收到的消息。\n    sds rcvbuf;\n\n    // 与这个连接相关联的节点，如果没有的话就为 NULL\n    struct clusterNode *node;\n\n} clusterLink;\n\n\nredisClient 结构和 clusterLink 结构的相同和不同之处\n\nredisClient 结构和 clusterLink 结构都有自己的套接字描述符和输入、输出缓冲区， 这两个结构的区别在于， redisClient 结构中的套接字和缓冲区是用于连接客户端的， 而 clusterLink 结构中的套接字和缓冲区则是用于连接节点的。\n\n最后， 每个节点都保存着一个 clusterState 结构， 这个结构记录了在当前节点的视角下， 集群目前所处的状态 —— 比如集群是在线还是下线， 集群包含多少个节点， 集群当前的配置纪元， 诸如此类：\n\ntypedef struct clusterState {\n\n    // 指向当前节点的指针\n    clusterNode *myself;\n\n    // 集群当前的配置纪元，用于实现故障转移\n    uint64_t currentEpoch;\n\n    // 集群当前的状态：是在线还是下线\n    int state;\n\n    // 集群中至少处理着一个槽的节点的数量\n    int size;\n\n    // 集群节点名单（包括 myself 节点）\n    // 字典的键为节点的名字，字典的值为节点对应的 clusterNode 结构\n    dict *nodes;\n\n} clusterState;\n\n\n以前面介绍的 7000 、 7001 、 7002 三个节点为例， 图 IMAGE_CLUSTER_STATE_OF_7000 展示了节点 7000 创建的 clusterState 结构， 这个结构从节点 7000 的角度记录了集群、以及集群包含的三个节点的当前状态 （为了空间考虑，图中省略了 clusterNode 结构的一部分属性）：\n\n * 结构的 currentEpoch 属性的值为 0 ， 表示集群当前的配置纪元为 0 。\n * 结构的 size 属性的值为 0 ， 表示集群目前没有任何节点在处理槽： 因此结构的 state 属性的值为 REDIS_CLUSTER_FAIL —— 这表示集群目前处于下线状态。\n * 结构的 nodes 字典记录了集群目前包含的三个节点， 这三个节点分别由三个 clusterNode 结构表示： 其中 myself 指针指向代表节点 7000 的 clusterNode 结构， 而字典中的另外两个指针则分别指向代表节点 7001 和代表节点 7002 的 clusterNode 结构， 这两个节点是节点 7000 已知的在集群中的其他节点。\n * 三个节点的 clusterNode 结构的 flags 属性都是 REDIS_NODE_MASTER ，说明三个节点都是主节点。\n\n节点 7001 和节点 7002 也会创建类似的 clusterState 结构：\n\n * 不过在节点 7001 创建的 clusterState 结构中， myself 指针将指向代表节点 7001 的 clusterNode 结构， 而节点 7000 和节点 7002 则是集群中的其他节点。\n * 而在节点 7002 创建的 clusterState 结构中， myself 指针将指向代表节点 7002 的 clusterNode 结构， 而节点 7000 和节点 7001 则是集群中的其他节点。\n\n\n\n\n# CLUSTER MEET 命令的实现\n\n通过向节点 A 发送 CLUSTER MEET 命令， 客户端可以让接收命令的节点 A 将另一个节点 B 添加到节点 A 当前所在的集群里面：\n\nCLUSTER MEET <ip> <port>\n\n\n收到命令的节点 A 将与节点 B 进行握手（handshake）， 以此来确认彼此的存在， 并为将来的进一步通信打好基础：\n\n 1. 节点 A 会为节点 B 创建一个 clusterNode 结构， 并将该结构添加到自己的 clusterState.nodes 字典里面。\n 2. 之后， 节点 A 将根据 CLUSTER MEET 命令给定的 IP 地址和端口号， 向节点 B 发送一条 MEET 消息（message）。\n 3. 如果一切顺利， 节点 B 将接收到节点 A 发送的 MEET 消息， 节点 B 会为节点 A 创建一个 clusterNode 结构， 并将该结构添加到自己的 clusterState.nodes 字典里面。\n 4. 之后， 节点 B 将向节点 A 返回一条 PONG 消息。\n 5. 如果一切顺利， 节点 A 将接收到节点 B 返回的 PONG 消息， 通过这条 PONG 消息节点 A 可以知道节点 B 已经成功地接收到了自己发送的 MEET 消息。\n 6. 之后， 节点 A 将向节点 B 返回一条 PING 消息。\n 7. 如果一切顺利， 节点 B 将接收到节点 A 返回的 PING 消息， 通过这条 PING 消息节点 B 可以知道节点 A 已经成功地接收到了自己返回的 PONG 消息， 握手完成。\n\n图 IMAGE_HANDSHAKE 展示了以上步骤描述的握手过程。\n\n\n\n之后， 节点 A 会将节点 B 的信息通过 Gossip 协议传播给集群中的其他节点， 让其他节点也与节点 B 进行握手， 最终， 经过一段时间之后， 节点 B 会被集群中的所有节点认识\n\n\n# 槽指派\n\nRedis 集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为 16384 个槽（slot），数据库中的每个键都属于这 16384个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。\n\n * 当数据库中的 16384 个槽都有节点在处理时，集群处于上线状态（ok）\n * 如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态（fail）\n\n在上一节，我们使用 CLUSTER MEET 命令将 7000、7001、7002 三个节点连接到了同一个集群里面，不过这个集群目前仍然处于下线状态，因为集群中的三个节点都没有在处理任何槽：\n\n127.0.0.1:7000> CLUSTER INFO\ncluster_state:fail\ncluster_slots_assigned:0\ncluster_slots_ok:0\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:3\ncluster_size:0\ncluster_current_epoch:0\ncluster_stats_messages_sent:110\ncluster_stats_messages_received:28\n\n\n通过向节点发送 CLUSTER ADDSLOTS 命令，我们可以将一个或多个槽指派（assign）给节点负责：\n\nCLUSTER ADDSLOTS <slot> [slot]\n\n\n举个例子，执行以下命令可以将槽0至槽5000指派给节点7000负责：\n\n127.0.0.1:7000> CLUSTER ADDSLOTS 0 1 2 3 4  5000\nOK\n127.0.0.1:7000> CLUSTER NODES\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388316664849 0 connected\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388316665850 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n\n\n为了让 7000、7001、7002 三个节点所在的集群进入上线状态，我们继续执行以下命令，将槽 5001 至槽 10000 指派给节点 7001 负责：\n\n127.0.0.1:7001> CLUSTER ADDSLOTS 5001 5002 5003 5004 10000\nOK\n\n\n然后将槽 10001 至槽 16383 指派给7002负责：\n\n127.0.0.1:7002> CLUSTER ADDSLOTS 10001 10002 10003 10004 16383\nOK\n\n\n当以上三个CLUSTER ADDSLOTS命令都执行完毕之后，数据库中的16384个槽都已经被指派给了相应的节点，集群进入上线状态：\n\n127.0.0.1:7000> CLUSTER INFO\ncluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:3\ncluster_size:3\ncluster_current_epoch:0\ncluster_stats_messages_sent:2699\ncluster_stats_messages_received:2617\n127.0.0.1:7000> CLUSTER NODES\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388317426165 0 connected 10001-16383\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388317427167 0 connected 5001-10000\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n\n\n本节接下来的内容将首先介绍节点保存槽指派信息的方法，以及节点之间传播槽指派信息的方法，之后再介绍CLUSTER ADDSLOTS命令的实现。\n\n\n# 记录节点的槽指派信息\n\nclusterNode结构的slots属性和numslot属性记录了节点负责处理哪些槽：\n\nstruct clusterNode {\n\n  unsigned char slots[16384/8];\n  int numslots;\n \n};\n\n\nslots属性是一个二进制位数组（bit array），这个数组的长度为16384/8=2048个字节，共包含16384个二进制位。\n\nRedis以0为起始索引，16383为终止索引，对slots数组中的16384个二进制位进行编号，并根据索引i上的二进制位的值来判断节点是否负责处理槽i：\n\n * 如果slots数组在索引i上的二进制位的值为1，那么表示节点负责处理槽i。\n\n * 如果slots数组在索引i上的二进制位的值为0，那么表示节点不负责处理槽i。\n\n图17-9展示了一个slots数组示例：这个数组索引0至索引7上的二进制位的值都为1，其余所有二进制位的值都为0，这表示节点负责处理槽0至槽7。\n\n\n\n图17-9　一个slots数组示例\n\n图17-10 展示了另一个slots数组示例：这个数组索引1、3、5、8、9、10上的二进制位的值都为1，而其余所有二进制位的值都为0，这表示节点负责处理槽1、3、5、8、9、10。\n\n\n\n图17-10　另一个slots数组示例\n\n因为取出和设置slots数组中的任意一个二进制位的值的复杂度仅为O（1），所以对于一个给定节点的slots数组来说，程序检查节点是否负责处理某个槽，又或者将某个槽指派给节点负责，这两个动作的复杂度都是O（1）。\n\n至于numslots属性则记录节点负责处理的槽的数量，也即是slots数组中值为1的二进制位的数量。\n\n比如说，对于图17-9所示的slots数组来说，节点处理的槽数量为8，而对于图17-10所示的slots数组来说，节点处理的槽数量为6。\n\n\n# 传播节点的槽指派信息\n\n一个节点除了会将自己负责处理的槽记录在 clusterNode 结构的 slots 属性和 numslots 属性之外，它还会将自己的 slots 数组通过消息发送给集群中的其他节点，以此来告知其他节点自己目前负责处理哪些槽。\n\n举个例子，对于前面展示的包含7000、7001、7002三个节点的集群来说：\n\n * 节点7000会通过消息向节点7001和节点7002发送自己的slots数组，以此来告知这两个节点，自己负责处理槽0至槽5000，如图17-11所示。\n\n * 节点7001会通过消息向节点7000和节点7002发送自己的slots数组，以此来告知这两个节点，自己负责处理槽5001至槽10000，如图17-12所示。\n\n * 节点7002会通过消息向节点7000和节点7001发送自己的slots数组，以此来告知这两个节点，自己负责处理槽10001至槽16383，如图17-13所示。\n\n\n\n图17-11　7000告知7001和7002自己负责处理的槽\n\n\n\n\n\n图17-13　7002告知7000和7001自己负责处理的槽\n\n当节点A通过消息从节点B那里接收到节点B的slots数组时，节点A会在自己的 clusterState.nodes 字典中查找节点B对应的 clusterNode 结构，并对结构中的 slots 数组进行保存或者更新。\n\n因为集群中的每个节点都会将自己的 slots 数组通过消息发送给集群中的其他节点，并且每个接收到 slots 数组的节点都会将数组保存到相应节点的 clusterNode 结构里面，因此，集群中的每个节点都会知道数据库中的 16384 个槽分别被指派给了集群中的哪些节点。\n\n\n# 记录集群所有槽的指派信息\n\nclusterState 结构中的 slots 数组记录了集群中所有 16384 个槽的指派信息：\n\ntypedef struct clusterState {\n \n  clusterNode *slots[16384];\n  \n} clusterState;\n\n\nslots数组包含16384个项，每个数组项都是一个指向clusterNode结构的指针：\n\n * 如果slots[i]指针指向NULL，那么表示槽i尚未指派给任何节点。\n\n * 如果slots[i]指针指向一个clusterNode结构，那么表示槽i已经指派给了clusterNode结构所代表的节点。\n\n举个例子，对于7000、7001、7002三个节点来说，它们的clusterState结构的slots数组将会是图17-14所示的样子：\n\n * 数组项slots[0]至slots[5000]的指针都指向代表节点7000的clusterNode结构，表示槽0至5000都指派给了节点7000。\n\n * 数组项slots[5001]至slots[10000]的指针都指向代表节点7001的clusterNode结构，表示槽5001至10000都指派给了节点7001。\n\n * 数组项slots[10001]至slots[16383]的指针都指向代表节点7002的clusterNode结构，表示槽10001至16383都指派给了节点7002。\n\n如果只将槽指派信息保存在各个节点的clusterNode.slots数组里，会出现一些无法高效地解决的问题，而clusterState.slots数组的存在解决了这些问题：\n\n * 如果节点只使用clusterNode.slots数组来记录槽的指派信息，那么为了知道槽i是否已经被指派，或者槽i被指派给了哪个节点，程序需要遍历clusterState.nodes字典中的所有clusterNode结构，检查这些结构的slots数组，直到找到负责处理槽i的节点为止，这个过程的复杂度为O（N），其中N为clusterState.nodes字典保存的clusterNode结构的数量。\n\n * 而通过将所有槽的指派信息保存在clusterState.slots数组里面，程序要检查槽i是否已经被指派，又或者取得负责处理槽i的节点，只需要访问clusterState.slots[i]的值即可，这个操作的复杂度仅为O(1)。\n\n举个例子，对于图17-14所示的slots数组来说，如果程序需要知道槽10002被指派给了哪个节点，那么只要访问数组项slots[10002]，就可以马上知道槽10002被指派给了节点7002，如图17-15所示。\n\n\n\n图17-15　访问slots[10002]的值\n\n要说明的一点是，虽然clusterState.slots数组记录了集群中所有槽的指派信息，但使用clusterNode结构的slots数组来记录单个节点的槽指派信息仍然是有必要的：\n\n * 因为当程序需要将某个节点的槽指派信息通过消息发送给其他节点时，程序只需要将相应节点的clusterNode.slots数组整个发送出去就可以了。\n\n * 另一方面，如果Redis不使用clusterNode.slots数组，而单独使用clusterState.slots数组的话，那么每次要将节点A的槽指派信息传播给其他节点时，程序必须先遍历整个clusterState.slots数组，记录节点A负责处理哪些槽，然后才能发送节点A的槽指派信息，这比直接发送clusterNode.slots数组要麻烦和低效得多。\n\nclusterState.slots数组记录了集群中所有槽的指派信息，而clusterNode.slots数组只记录了clusterNode结构所代表的节点的槽指派信息，这是两个slots数组的关键区别所在。\n\n\n# CLUSTER ADDSLOTS 命令的实现\n\nCLUSTER ADDSLOTS 命令接受一个或多个槽作为参数，并将所有输入的槽指派给接收该命令的节点负责：\n\nCLUSTER ADDSLOTS <slot> [slot]\n\n\nCLUSTER ADDSLOTS 命令的实现可以用以下伪代码来表示：\n\ndef CLUSTER_ADDSLOTS(*all_input_slots):\n    # 遍历所有输入槽，检查它们是否都是未指派槽\n    for i in all_input_slots:\n        # 如果有哪怕一个槽已经被指派给了某个节点\n        # 那么向客户端返回错误，并终止命令执行\n        if clusterState.slots[i] != NULL:\n            reply_error()\n            return\n    # 如果所有输入槽都是未指派槽\n    # 那么再次遍历所有输入槽，将这些槽指派给当前节点\n    for i in all_input_slots:\n        # 设置clusterState结构的slots数组\n        # 将slots[i]的指针指向代表当前节点的clusterNode结构\n        clusterState.slots[i] = clusterState.myself\n        # 访问代表当前节点的clusterNode结构的slots数组\n        # 将数组在索引i上的二进制位设置为1\n        setSlotBit(clusterState.myself.slots, i)\n\n\n举个例子，图17-16展示了一个节点的clusterState结构，clusterState.slots数组中的所有指针都指向NULL，并且clusterNode.slots数组中的所有二进制位的值都是0，这说明当前节点没有被指派任何槽，并且集群中的所有槽都是未指派的。\n\n\n\n图17-16　节点的clusterState结构\n\n当客户端对17-16所示的节点执行命令：\n\nCLUSTER ADDSLOTS 1 2\n\n\n将槽1和槽2指派给节点之后，节点的clusterState结构将被更新成图17-17所示的样子：\n\n * clusterState.slots数组在索引1和索引2上的指针指向了代表当前节点的clusterNode结构。\n\n * 并且clusterNode.slots数组在索引1和索引2上的位被设置成了1。\n\n\n\n图17-17　执行 CLUSTER ADDSLOTS 命令之后的 clusterState 结构\n\n最后，在 CLUSTER ADDSLOTS 命令执行完毕之后，节点会通过发送消息告知集群中的其他节点，自己目前正在负责处理哪些槽。\n\n\n# 在集群中执行命令\n\n在对数据库中的16384个槽都进行了指派之后，集群就会进入上线状态，这时客户端就可以向集群中的节点发送数据命令了。\n\n当客户端向节点发送与数据库键有关的命令时，接收命令的节点会计算出命令要处理的数据库键属于哪个槽，并检查这个槽是否指派给了自己：\n\n * 如果键所在的槽正好就指派给了当前节点，那么节点直接执行这个命令。\n\n * 如果键所在的槽并没有指派给当前节点，那么节点会向客户端返回一个MOVED错误，指引客户端转向（redirect）至正确的节点，并再次发送之前想要执行的命令。\n\n图17-18展示了这两种情况的判断流程。\n\n\n\n图17-18　判断客户端是否需要转向的流程\n\n举个例子，如果我们在之前提到的，由7000、7001、7002三个节点组成的集群中，用客户端连上节点7000，并发送以下命令，那么命令会直接被节点7000执行：\n\n127.0.0.1:7000> SET date "2013-12-31"\nOK\n\n\n因为键date所在的槽2022正是由节点7000负责处理的。\n\n但是，如果我们执行以下命令，那么客户端会先被转向至节点7001，然后再执行命令：\n\n127.0.0.1:7000> SET msg "happy new year!"\n-> Redirected to slot [6257] located at 127.0.0.1:7001\nOK\n127.0.0.1:7001> GET msg\n"happy new year!"\n\n\n这是因为键msg所在的槽6257是由节点7001负责处理的，而不是由最初接收命令的节点7000负责处理：\n\n * 当客户端第一次向节点7000发送SET命令的时候，节点7000会向客户端返回MOVED错误，指引客户端转向至节点7001。\n\n * 当客户端转向到节点7001之后，客户端重新向节点7001发送SET命令，这个命令会被节点7001成功执行。\n\n本节接下来的内容将介绍计算键所属槽的方法，节点判断某个槽是否由自己负责的方法，以及MOVED错误的实现方法，最后，本节还会介绍节点和单机Redis服务器保存键值对数据的相同和不同之处。\n\n\n# 计算键属于哪个槽\n\n节点使用以下算法来计算给定键key属于哪个槽：\n\ndef slot_number(key):\n    return CRC16(key) & 16383\n\n\n其中CRC16（key）语句用于计算键key的CRC-16校验和，而&16383语句则用于计算出一个介于0至16383之间的整数作为键key的槽号。\n\n使用CLUSTER KEYSLOT命令可以查看一个给定键属于哪个槽：\n\n127.0.0.1:7000> CLUSTER KEYSLOT "date"\n(integer) 2022\n127.0.0.1:7000> CLUSTER KEYSLOT "msg"\n(integer) 6257\n127.0.0.1:7000> CLUSTER KEYSLOT "name"\n(integer) 5798\n127.0.0.1:7000> CLUSTER KEYSLOT "fruits"\n(integer) 14943\n\n\nCLUSTER KEYSLOT命令就是通过调用上面给出的槽分配算法来实现的，以下是该命令的伪代码实现：\n\ndef CLUSTER_KEYSLOT(key):\n    # 计算槽号\n    slot = slot_number(key)\n    # 将槽号返回给客户端\n    reply_client(slot)\n\n\n\n# 判断槽是否由当前节点负责处理\n\n当节点计算出键所属的槽i之后，节点就会检查自己在clusterState.slots数组中的项i，判断键所在的槽是否由自己负责：\n\n1）如果clusterState.slots[i]等于clusterState.myself，那么说明槽i由当前节点负责，节点可以执行客户端发送的命令。\n\n2）如果clusterState.slots[i]不等于clusterState.myself，那么说明槽i并非由当前节点负责，节点会根据clusterState.slots[i]指向的clusterNode结构所记录的节点IP和端口号，向客户端返回MOVED错误，指引客户端转向至正在处理槽i的节点。\n\n举个例子，假设图17-19为节点7000的clusterState结构：\n\n * 当客户端向节点7000发送命令SET date"2013-12-31"的时候，节点首先计算出键date属于槽2022，然后检查得出clusterState.slots[2022]等于clusterState.myself，这说明槽2022正是由节点7000负责，于是节点7000直接执行这个SET命令，并将结果返回给发送命令的客户端。\n\n * 当客户端向节点7000发送命令SET msg"happy new year！"的时候，节点首先计算出键msg属于槽6257，然后检查clusterState.slots[6257]是否等于clusterState.myself，结果发现两者并不相等：这说明槽6257并非由节点7000负责处理，于是节点7000访问clusterState.slots[6257]所指向的clusterNode结构，并根据结构中记录的IP地址127.0.0.1和端口号7001，向客户端返回错误MOVED 6257 127.0.0.1:7001，指引节点转向至正在负责处理槽6257的节点7001。\n\n\n\n图17-19　节点7000的clusterState结构\n\n\n# MOVED 错误\n\n当节点发现键所在的槽并非由自己负责处理的时候，节点就会向客户端返回一个 MOVED 错误，指引客户端转向至正在负责槽的节点。\n\nMOVED错误的格式为：\n\nMOVED <slot> <ip>:<port>\n\n\n其中slot为键所在的槽，而ip和port则是负责处理槽slot的节点的IP地址和端口号。例如错误：\n\nOVED 10086 127.0.0.1:7002\n\n\n表示槽10086正由IP地址为127.0.0.1，端口号为7002的节点负责。\n\n又例如错误：\n\nMOVED 789 127.0.0.1:7000\n\n\n表示槽789正由IP地址为127.0.0.1，端口号为7000的节点负责。\n\n当客户端接收到节点返回的MOVED错误时，客户端会根据MOVED错误中提供的IP地址和端口号，转向至负责处理槽slot的节点，并向该节点重新发送之前想要执行的命令。以前面的客户端从节点7000转向至7001的情况作为例子：\n\n127.0.0.1:7000> SET msg "happy new year!"\n-> Redirected to slot [6257] located at 127.0.0.1:7001\nOK\n127.0.0.1:7001>\n\n\n图17-20展示了客户端向节点7000发送SET命令，并获得MOVED错误的过程。\n\n\n\n图17-20　节点7000向客户端返回MOVED错误\n\n而图17-21则展示了客户端根据MOVED错误，转向至节点7001，并重新发送SET命令的过程。\n\n\n\n图17-21　客户端根据MOVED错误的指示转向至节点7001\n\n一个集群客户端通常会与集群中的多个节点创建套接字连接，而所谓的节点转向实际上就是换一个套接字来发送命令。\n\n如果客户端尚未与想要转向的节点创建套接字连接，那么客户端会先根据MOVED错误提供的IP地址和端口号来连接节点，然后再进行转向。\n\n> 被隐藏的MOVED错误\n> \n> 集群模式的redis-cli客户端在接收到MOVED错误时，并不会打印出MOVED错误，而是根据MOVED错误自动进行节点转向，并打印出转向信息，所以我们是看不见节点返回的MOVED错误的：\n> \n> $ redis-cli -c -p 7000 # \n> 集群模式\n> 127.0.0.1:7000> SET msg "happy new year!"\n> -> Redirected to slot [6257] located at 127.0.0.1:7001\n> OK\n> 127.0.0.1:7001>\n> \n> \n> 但是，如果我们使用单机（stand alone）模式的redis-cli客户端，再次向节点7000发送相同的命令，那么MOVED错误就会被客户端打印出来：\n> \n> $ redis-cli -p 7000 # \n> 单机模式\n> 127.0.0.1:7000> SET msg "happy new year!"\n> (error) MOVED 6257 127.0.0.1:7001\n> 127.0.0.1:7000>\n> \n> \n> 这是因为单机模式的redis-cli客户端不清楚MOVED错误的作用，所以它只会直接将MOVED错误直接打印出来，而不会进行自动转向。\n\n\n# 节点数据库的实现\n\n集群节点保存键值对以及键值对过期时间的方式，与第9章里面介绍的单机Redis服务器保存键值对以及键值对过期时间的方式完全相同。\n\n节点和单机服务器在数据库方面的一个区别是，节点只能使用0号数据库，而单机Redis服务器则没有这一限制。\n\n举个例子，图17-22展示了节点7000的数据库状态，数据库中包含列表键"lst"，哈希键"book"，以及字符串键"date"，其中键"lst"和键"book"带有过期时间。\n\n另外，除了将键值对保存在数据库里面之外，节点还会用clusterState结构中的slots_to_keys跳跃表来保存槽和键之间的关系：\n\ntypedef struct clusterState {\n\n  zskiplist *slots_to_keys;\n \n} clusterState;\n\n\n\n\n图17-22　节点7000的数据库\n\nslots_to_keys跳跃表每个节点的分值（score）都是一个槽号，而每个节点的成员（member）都是一个数据库键：\n\n * 每当节点往数据库中添加一个新的键值对时，节点就会将这个键以及键的槽号关联到slots_to_keys跳跃表。\n\n * 当节点删除数据库中的某个键值对时，节点就会在slots_to_keys跳跃表解除被删除键与槽号的关联。\n\n举个例子，对于图17-22所示的数据库，节点7000将创建类似图17-23所示的slots_to_keys跳跃表：\n\n * 键"book"所在跳跃表节点的分值为1337.0，这表示键"book"所在的槽为1337。\n\n * 键"date"所在跳跃表节点的分值为2022.0，这表示键"date"所在的槽为2022。\n\n * 键"lst"所在跳跃表节点的分值为3347.0，这表示键"lst"所在的槽为3347。\n\n通过在slots_to_keys跳跃表中记录各个数据库键所属的槽，节点可以很方便地对属于某个或某些槽的所有数据库键进行批量操作，例如命令CLUSTER GETKEYSINSLOT命令可以返回最多count个属于槽slot的数据库键，而这个命令就是通过遍历slots_to_keys跳跃表来实现的。\n\n\n\n图17-23　节点7000的slots_to_keys跳跃表\n\n\n# 重新分片\n\nRedis集群的重新分片操作可以将任意数量已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽所属的键值对也会从源节点被移动到目标节点。\n\n重新分片操作可以在线（online）进行，在重新分片的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。\n\n举个例子，对于之前提到的，包含7000、7001、7002三个节点的集群来说，我们可以向这个集群添加一个IP为127.0.0.1，端口号为7003的节点（后面简称节点7003）：\n\n$ redis-cli -c -p 7000\n127.0.0.1:7000> CLUSTER MEET 127.0.0.1 7003\nOK\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388635782831 0 connected 5001-10000\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388635782831 0 connected 10001-16383\n04579925484ce537d3410d7ce97bd2e260c459a2 127.0.0.1:7003 master - 0 1388635782330 0 connected\n\n\n然后通过重新分片操作，将原本指派给节点7002的槽15001至16383改为指派给节点7003。\n\n以下是重新分片操作执行之后，节点的槽分配状态：\n\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master -0 0 0 connected 0-5000\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master -0 1388635782831 0 connected 5001-10000\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master -0 1388635782831 0 connected 10001-15000\n04579925484ce537d3410d7ce97bd2e260c459a2 127.0.0.1:7003 master -0 1388635782330 0 connected 15001-16383\n\n\n重新分片的实现原理\n\nRedis集群的重新分片操作是由Redis的集群管理软件redis-trib负责执行的，Redis提供了进行重新分片所需的所有命令，而redis-trib则通过向源节点和目标节点发送命令来进行重新分片操作。\n\nredis-trib对集群的单个槽slot进行重新分片的步骤如下：\n\n1）redis-trib对目标节点发送CLUSTER SETSLOT <slot> IMPORTING <source_id> 命令，让目标节点准备好从源节点导入（import）属于槽slot的键值对。\n\n2）redis-trib对源节点发送CLUSTER SETSLOT <slot> MIGRATING <target_id>命令，让源节点准备好将属于槽slot的键值对迁移（migrate）至目标节点。\n\n3）redis-trib向源节点发送CLUSTER GETKEYSINSLOT <slot> <count> 命令，获得最多count个属于槽slot的键值对的键名（key name）。\n\n4）对于步骤3获得的每个键名，redis-trib都向源节点发送一个MIGRATE <target_ip ><target_port> <key_name> 0 <timeout>命令，将被选中的键原子地从源节点迁移至目标节点。\n\n5）重复执行步骤3和步骤4，直到源节点保存的所有属于槽slot的键值对都被迁移至目标节点为止。每次迁移键的过程如图17-24所示。\n\n6）redis-trib向集群中的任意一个节点发送CLUSTER SETSLOT <slot> NODE <target_id>命令，将槽slot指派给目标节点，这一指派信息会通过消息发送至整个集群，最终集群中的所有节点都会知道槽slot已经指派给了目标节点。\n\n\n\n图17-24　迁移键的过程\n\n图17-25 展示了对槽slot进行重新分片的整个过程。\n\n如果重新分片涉及多个槽，那么redis-trib将对每个给定的槽分别执行上面给出的步骤。\n\n\n\n图17-25　对槽slot进行重新分片的过程\n\n\n# ASK 错误\n\n在进行重新分片期间，源节点向目标节点迁移一个槽的过程中，可能会出现这样一种情况：属于被迁移槽的一部分键值对保存在源节点里面，而另一部分键值对则保存在目标节点里面。\n\n当客户端向源节点发送一个与数据库键有关的命令，并且命令要处理的数据库键恰好就属于正在被迁移的槽时：\n\n * 源节点会先在自己的数据库里面查找指定的键，如果找到的话，就直接执行客户端发送的命令。\n * 相反地，如果源节点没能在自己的数据库里面找到指定的键，那么这个键有可能已经被迁移到了目标节点，源节点将向客户端返回一个ASK错误，指引客户端转向正在导入槽的目标节点，并再次发送之前想要执行的命令。\n\n图17-26展示了源节点判断是否需要向客户端发送ASK错误的整个过程。\n\n\n\n图17-26　判断是否发送ASK错误的过程\n\n举个例子，假设节点7002正在向节点7003迁移槽16198，这个槽包含"is"和"love"两个键，其中键"is"还留在节点7002，而键"love"已经被迁移到了节点7003。\n\n如果我们向节点7002发送关于键"is"的命令，那么这个命令会直接被节点7002执行：\n\n127.0.0.1:7002> GET "is"\n"you get the key \'is\'"\n\n\n而如果我们向节点7002发送关于键"love"的命令，那么客户端会先被转向至节点7003，然后再次执行命令：\n\n127.0.0.1:7002> GET "love"\n-> Redirected to slot [16198] located at 127.0.0.1:7003\n"you get the key \'love\'"\n127.0.0.1:7003>\n\n\n> 被隐藏的ASK错误\n> \n> 和接到MOVED错误时的情况类似，集群模式的redis-cli在接到ASK错误时也不会打印错误，而是自动根据错误提供的IP地址和端口进行转向动作。如果想看到节点发送的ASK错误的话，可以使用单机模式的redis-cli客户端：\n> \n> $ redis-cli -p 7002\n> 127.0.0.1:7002> GET "love"\n> (error) ASK 16198 127.0.0.1:7003\n\n注意\n\n在写这篇文章的时候，集群模式的redis-cli并未支持ASK自动转向，上面展示的ASK自动转向行为实际上是根据MOVED自动转向行为虚构出来的。因此，当集群模式的redis-cli真正支持ASK自动转向时，它的行为和上面展示的行为可能会有所不同。\n\n本节将对ASK错误的实现原理进行说明，并对比ASK错误和MOVED错误的区别。\n\n\n# CLUSTER SETSLOT IMPORTING命令的实现\n\nclusterState结构的importing_slots_from数组记录了当前节点正在从其他节点导入的槽：\n\ntypedef struct clusterState {\n\n  clusterNode *importing_slots_from[16384];\n \n} clusterState;\n\n\n如果importing_slots_from[i]的值不为NULL，而是指向一个clusterNode结构，那么表示当前节点正在从clusterNode所代表的节点导入槽i。\n\n在对集群进行重新分片的时候，向目标节点发送命令：\n\nCLUSTER SETSLOT <i> IMPORTING <source_id>\n\n\n可以将目标节点clusterState.importing_slots_from[i]的值设置为source_id所代表节点的clusterNode结构。\n\n举个例子，如果客户端向节点7003发送以下命令：\n\n# 9dfb 是节点7002 的ID \n127.0.0.1:7003> CLUSTER SETSLOT 16198 IMPORTING 9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26\nOK\n\n\n那么节点7003的clusterState.importing_slots_from数组将变成图17-27所示的样子。\n\n\n\n图17-27　节点7003的importing_slots_from数组\n\n\n# CLUSTER SETSLOT MIGRATING 命令的实现\n\nclusterState结构的migrating_slots_to数组记录了当前节点正在迁移至其他节点的槽：\n\ntypedef struct clusterState {\n   \n   clusterNode *migrating_slots_to[16384];\n   \n} clusterState;\n\n\n如果migrating_slots_to[i]的值不为NULL，而是指向一个 clusterNode 结构，那么表示当前节点正在将槽i迁移至clusterNode所代表的节点。\n\n在对集群进行重新分片的时候，向源节点发送命令：\n\nCLUSTER SETSLOT <i> MIGRATING <target_id>\n\n\n可以将源节点clusterState.migrating_slots_to[i]的值设置为target_id所代表节点的clusterNode结构。\n\n举个例子，如果客户端向节点7002发送以下命令：\n\n# 0457\n是节点7003 \n的ID \n127.0.0.1:7002> CLUSTER SETSLOT 16198 MIGRATING 04579925484ce537d3410d7ce97bd2e260c459a2\nOK\n\n\n那么节点7002的clusterState.migrating_slots_to数组将变成图17-28所示的样子。\n\n图17-28　节点7002的migrating_slots_to数组\n\n\n# ASK 错误\n\n如果节点收到一个关于键key的命令请求，并且键key所属的槽i正好就指派给了这个节点，那么节点会尝试在自己的数据库里查找键key，如果找到了的话，节点就直接执行客户端发送的命令。\n\n与此相反，如果节点没有在自己的数据库里找到键key，那么节点会检查自己的clusterState.migrating_slots_to[i]，看键key所属的槽i是否正在进行迁移，如果槽i的确在进行迁移的话，那么节点会向客户端发送一个ASK错误，引导客户端到正在导入槽i的节点去查找键key。\n\n举个例子，假设在节点7002向节点7003迁移槽16198期间，有一个客户端向节点7002发送命令：\n\nGET \n“love”\n\n\n因为键"love"正好属于槽16198，所以节点7002会首先在自己的数据库中查找键"love"，但并没有找到，通过检查自己的clusterState.migrating_slots_to[16198]，节点7002发现自己正在将槽16198迁移至节点7003，于是它向客户端返回错误：\n\nASK 16198 127.0.0.1:7003\n\n\n这个错误表示客户端可以尝试到IP为127.0.0.1，端口号为7003的节点去执行和槽16198有关的操作，如图17-29所示。\n\n\n\n图17-29　客户端接收到节点7002返回的ASK错误\n\n接到ASK错误的客户端会根据错误提供的IP地址和端口号，转向至正在导入槽的目标节点，然后首先向目标节点发送一个ASKING命令，之后再重新发送原本想要执行的命令。\n\n以前面的例子来说，当客户端接收到节点7002返回的以下错误时：\n\nASK 16198 127.0.0.1:7003\n\n\n客户端会转向至节点7003，首先发送命令：\n\nASKING\n\n\n然后再次发送命令：\n\nGET "love"\n\n\n并获得回复：\n\n"you get the key \'love\'"\n\n\n整个过程如图17-30所示。\n\n\n\n图17-30　客户端转向至节点7003\n\n\n# ASKING命令\n\nASKING命令唯一要做的就是打开发送该命令的客户端的REDIS_ASKING标识，以下是该命令的伪代码实现：\n\ndef ASKING():\n    # \n打开标识\n    client.flags |= REDIS_ASKING\n    # \n向客户端返回OK \n回复\n    reply("OK")\n\n\n在一般情况下，如果客户端向节点发送一个关于槽i的命令，而槽i又没有指派给这个节点的话，那么节点将向客户端返回一个MOVED错误；但是，如果节点的clusterState.importing_slots_from[i]显示节点正在导入槽i，并且发送命令的客户端带有REDIS_ASKING标识，那么节点将破例执行这个关于槽i的命令一次，图17-31展示了这个判断过程。\n\n\n\n图17-31　节点判断是否执行客户端命令的过程\n\n当客户端接收到ASK错误并转向至正在导入槽的节点时，客户端会先向节点发送一个ASKING命令，然后才重新发送想要执行的命令，这是因为如果客户端不发送ASKING命令，而直接发送想要执行的命令的话，那么客户端发送的命令将被节点拒绝执行，并返回MOVED错误。\n\n举个例子，我们可以使用普通模式的redis-cli客户端，向正在导入槽16198的节点7003发送以下命令：\n\n$ ./redis-cli -p 7003\n127.0.0.1:7003> GET "love"\n(error) MOVED 16198 127.0.0.1:7002\n\n\n虽然节点7003正在导入槽16198，但槽16198目前仍然是指派给了节点7002，所以节点7003会向客户端返回MOVED错误，指引客户端转向至节点7002。\n\n但是，如果我们在发送GET命令之前，先向节点发送一个ASKING命令，那么这个GET命令就会被节点7003执行：\n\n127.0.0.1:7003> ASKING\nOK\n127.0.0.1:7003> GET "love"\n"you get the key \'love\'"\n\n\n另外要注意的是，客户端的REDIS_ASKING标识是一个一次性标识，当节点执行了一个带有REDIS_ASKING标识的客户端发送的命令之后，客户端的REDIS_ASKING标识就会被移除。\n\n举个例子，如果我们在成功执行GET命令之后，再次向节点7003发送GET命令，那么第二次发送的GET命令将执行失败，因为这时客户端的REDIS_ASKING标识已经被移除：\n\n127.0.0.1:7003> ASKING                 #\n打开REDIS_ASKING\n标识\nOK\n127.0.0.1:7003> GET "love"   #\n移除REDIS_ASKING\n标识\n"you get the key \'love\'"\n127.0.0.1:7003> GET "love"   # REDIS_ASKING\n标识未打开，执行失败\n(error) MOVED 16198 127.0.0.1:7002\n\n\n\n# ASK错误和MOVED错误的区别\n\nASK错误和MOVED错误都会导致客户端转向，它们的区别在于：\n\n * MOVED错误代表槽的负责权已经从一个节点转移到了另一个节点：在客户端收到关于槽i的MOVED错误之后，客户端每次遇到关于槽i的命令请求时，都可以直接将命令请求发送至MOVED错误所指向的节点，因为该节点就是目前负责槽i的节点。\n\n * 与此相反，ASK错误只是两个节点在迁移槽的过程中使用的一种临时措施：在客户端收到关于槽i的ASK错误之后，客户端只会在接下来的一次命令请求中将关于槽i的命令请求发送至ASK错误所指示的节点，但这种转向不会对客户端今后发送关于槽i的命令请求产生任何影响，客户端仍然会将关于槽i的命令请求发送至目前负责处理槽i的节点，除非ASK错误再次出现。\n\n\n# 复制与故障转移\n\nRedis集群中的节点分为主节点（master）和从节点（slave），其中主节点用于处理槽，而从节点则用于复制某个主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求。\n\n举个例子，对于包含7000、7001、7002、7003四个主节点的集群来说，我们可以将7004、7005两个节点添加到集群里面，并将这两个节点设定为节点7000的从节点，如图17-32所示（图中以双圆形表示主节点，单圆形表示从节点）。\n\n\n\n图17-32　设置节点7004和节点7005成为节点7000的从节点\n\n表17-1记录了集群各个节点的当前状态，以及它们正在做的工作。\n\n表17-1　集群各个节点的当前状态\n\n\n\n如果这时，节点7000进入下线状态，那么集群中仍在正常运作的几个主节点将在节点7000的两个从节点——节点7004和节点7005中选出一个节点作为新的主节点，这个新的主节点将接管原来节点7000负责处理的槽，并继续处理客户端发送的命令请求。\n\n例如，如果节点7004被选中为新的主节点，那么节点7004将接管原来由节点7000负责处理的槽0至槽5000，节点7005也会从原来的复制节点7000，改为复制节点7004，如图17-33所示（图中用虚线包围的节点为已下线节点）。\n\n\n\n图17-33　节点7004成为新的主节点\n\n表17-2记录了在对节点7000进行故障转移之后，集群各个节点的当前状态，以及它们正在做的工作。\n\n表17-2　集群各个节点的当前状态\n\n\n\n如果在故障转移完成之后，下线的节点7000重新上线，那么它将成为节点7004的从节点，如图17-34所示。\n\n\n\n图17-34　重新上线的节点7000成为节点7004的从节点\n\n表17-3展示了节点7000复制节点7004之后，集群中各个节点的状态。\n\n表17-3　集群各个节点的当前状态\n\n\n\n本节接下来的内容将介绍节点的复制方法，检测节点是否下线的方法，以及对下线主节点进行故障转移的方法。\n\n\n# 设置从节点\n\n向一个节点发送命令：\n\nCLUSTER REPLICATE <node_id>\n\n\n可以让接收命令的节点成为node_id所指定节点的从节点，并开始对主节点进行复制：\n\n * 接收到该命令的节点首先会在自己的clusterState.nodes字典中找到node_id所对应节点的clusterNode结构，并将自己的clusterState.myself.slaveof指针指向这个结构，以此来记录这个节点正在复制的主节点：\n   \n   struct clusterNode {\n   \n        //如果这是一个从节点，那么指向主节点\n        struct clusterNode *slaveof;\n       \n      };\n   \n\n * 然后节点会修改自己在clusterState.myself.flags中的属性，关闭原本的REDIS_NODE_MASTER标识，打开REDIS_NODE_SLAVE标识，表示这个节点已经由原来的主节点变成了从节点。\n\n * 最后，节点会调用复制代码，并根据clusterState.myself.slaveof指向的clusterNode结构所保存的IP地址和端口号，对主节点进行复制。因为节点的复制功能和单机Redis服务器的复制功能使用了相同的代码，所以让从节点复制主节点相当于向从节点发送命令SLAVEOF。\n\n图17-35展示了节点7004在复制节点7000时的clusterState结构：\n\n * clusterState.myself.flags属性的值为REDIS_NODE_SLAVE，表示节点7004是一个从节点。\n\n * clusterState.myself.slaveof指针指向代表节点7000的结构，表示节点7004正在复制的主节点为节点7000。\n\n\n\n图17-35　节点7004的clusterState结构\n\n一个节点成为从节点，并开始复制某个主节点这一信息会通过消息发送给集群中的其他节点，最终集群中的所有节点都会知道某个从节点正在复制某个主节点。\n\n集群中的所有节点都会在代表主节点的clusterNode结构的slaves属性和numslaves属性中记录正在复制这个主节点的从节点名单：\n\nstruct clusterNode {\n    // \n    // \n正在复制这个主节点的从节点数量\n    int numslaves;\n    // \n一个数组\n    // \n每个数组项指向一个正在复制这个主节点的从节点的clusterNode\n结构\n    struct clusterNode **slaves;\n    // \n};\n\n\n举个例子，图17-36记录了节点7004和节点7005成为节点7000的从节点之后，集群中的各个节点为节点7000创建的clusterNode结构的样子：\n\n * 代表节点7000的clusterNode结构的numslaves属性的值为2，这说明有两个从节点正在复制节点7000。\n\n * 代表节点7000的clusterNode结构的slaves数组的两个项分别指向代表节点7004和代表节点7005的clusterNode结构，这说明节点7000的两个从节点分别是节点7004和节点7005。\n\n\n\n图17-36　集群中的各个节点为节点7000创建的clusterNode结构\n\n\n# 故障检测\n\n集群中的每个节点都会定期地向集群中的其他节点发送PING消息，以此来检测对方是否在线，如果接收PING消息的节点没有在规定的时间内，向发送PING消息的节点返回PONG消息，那么发送PING消息的节点就会将接收PING消息的节点标记为疑似下线（probable fail，PFAIL）。\n\n举个例子，如果节点7001向节点7000发送了一条PING消息，但是节点7000没有在规定的时间内，向节点7001返回一条PONG消息，那么节点7001就会在自己的clusterState.nodes字典中找到节点7000所对应的clusterNode结构，并在结构的flags属性中打开REDIS_NODE_PFAIL标识，以此表示节点7000进入了疑似下线状态，如图17-37所示。\n\n\n\n图17-37　代表节点7000的clusterNode结构\n\n集群中的各个节点会通过互相发送消息的方式来交换集群中各个节点的状态信息，例如某个节点是处于在线状态、疑似下线状态（PFAIL），还是已下线状态（FAIL）。\n\n当一个主节点A通过消息得知主节点B认为主节点C进入了疑似下线状态时，主节点A会在自己的clusterState.nodes字典中找到主节点C所对应的clusterNode结构，并将主节点B的下线报告（failure report）添加到clusterNode结构的fail_reports链表里面：\n\nstruct clusterNode {\n  // \n  // \n一个链表，记录了所有其他节点对该节点的下线报告\n  list *fail_reports;\n  // \n};\n\n\n每个下线报告由一个clusterNodeFailReport结构表示：\n\nstruct clusterNodeFailReport {\n  // \n报告目标节点已经下线的节点\n  struct clusterNode *node;\n  // \n最后一次从node\n节点收到下线报告的时间\n  // \n程序使用这个时间戳来检查下线报告是否过期\n  // \n（与当前时间相差太久的下线报告会被删除）\n  mstime_t time;\n} typedef clusterNodeFailReport;\n\n\n举个例子，如果主节点7001在收到主节点7002、主节点7003发送的消息后得知，主节点7002和主节点7003都认为主节点7000进入了疑似下线状态，那么主节点7001将为主节点7000创建图17-38所示的下线报告。\n\n\n\n图17-38　节点7000的下线报告\n\n如果在一个集群里面，半数以上负责处理槽的主节点都将某个主节点x报告为疑似下线，那么这个主节点x将被标记为已下线（FAIL），将主节点x标记为已下线的节点会向集群广播一条关于主节点x的FAIL消息，所有收到这条FAIL消息的节点都会立即将主节点x标记为已下线。\n\n举个例子，对于图17-38所示的下线报告来说，主节点7002和主节点7003都认为主节点7000进入了下线状态，并且主节点7001也认为主节点7000进入了疑似下线状态（代表主节点7000的结构打开了REDIS_NODE_PFAIL标识），综合起来，在集群四个负责处理槽的主节点里面，有三个都将主节点7000标记为下线，数量已经超过了半数，所以主节点7001会将主节点7000标记为已下线，并向集群广播一条关于主节点7000的FAIL消息，如图17-39所示。\n\n\n\n图17-39　节点7001向集群广播FAIL消息\n\n\n# 故障转移\n\n当一个从节点发现自己正在复制的主节点进入了已下线状态时，从节点将开始对下线主节点进行故障转移，以下是故障转移的执行步骤：\n\n1）复制下线主节点的所有从节点里面，会有一个从节点被选中。\n\n2）被选中的从节点会执行SLAVEOF no one命令，成为新的主节点。\n\n3）新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。\n\n4）新的主节点向集群广播一条PONG消息，这条PONG消息可以让集群中的其他节点立即知道这个节点已经由从节点变成了主节点，并且这个主节点已经接管了原本由已下线节点负责处理的槽。\n\n5）新的主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成。\n\n\n# 选举新的主节点\n\n新的主节点是通过选举产生的。\n\n以下是集群选举新的主节点的方法：\n\n1）集群的配置纪元是一个自增计数器，它的初始值为0。\n\n2）当集群里的某个节点开始一次故障转移操作时，集群配置纪元的值会被增一。\n\n3）对于每个配置纪元，集群里每个负责处理槽的主节点都有一次投票的机会，而第一个向主节点要求投票的从节点将获得主节点的投票。\n\n4）当从节点发现自己正在复制的主节点进入已下线状态时，从节点会向集群广播一条CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST消息，要求所有收到这条消息、并且具有投票权的主节点向这个从节点投票。\n\n5）如果一个主节点具有投票权（它正在负责处理槽），并且这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，表示这个主节点支持从节点成为新的主节点。\n\n6）每个参与选举的从节点都会接收CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，并根据自己收到了多少条这种消息来统计自己获得了多少主节点的支持。\n\n7）如果集群里有N个具有投票权的主节点，那么当一个从节点收集到大于等于N/2+1张支持票时，这个从节点就会当选为新的主节点。\n\n8）因为在每一个配置纪元里面，每个具有投票权的主节点只能投一次票，所以如果有N个主节点进行投票，那么具有大于等于N/2+1张支持票的从节点只会有一个，这确保了新的主节点只会有一个。\n\n9）如果在一个配置纪元里面没有从节点能收集到足够多的支持票，那么集群进入一个新的配置纪元，并再次进行选举，直到选出新的主节点为止。\n\n这个选举新主节点的方法和第16章介绍的选举领头Sentinel的方法非常相似，因为两者都是基于Raft算法的领头选举（leader election）方法来实现的。\n\n\n# 消息\n\n集群中的各个节点通过发送和接收消息（message）来进行通信，我们称发送消息的节点为发送者（sender），接收消息的节点为接收者（receiver），如图17-40所示。\n\n\n\n图17-40　发送者和接收者\n\n节点发送的消息主要有以下五种：\n\n * MEET消息：当发送者接到客户端发送的CLUSTER MEET命令时，发送者会向接收者发送MEET消息，请求接收者加入到发送者当前所处的集群里面。\n\n * PING消息：集群里的每个节点默认每隔一秒钟就会从已知节点列表中随机选出五个节点，然后对这五个节点中最长时间没有发送过PING消息的节点发送PING消息，以此来检测被选中的节点是否在线。除此之外，如果节点A最后一次收到节点B发送的PONG消息的时间，距离当前时间已经超过了节点A的cluster-node-timeout选项设置时长的一半，那么节点A也会向节点B发送PING消息，这可以防止节点A因为长时间没有随机选中节点B作为PING消息的发送对象而导致对节点B的信息更新滞后。\n\n * PONG消息：当接收者收到发送者发来的MEET消息或者PING消息时，为了向发送者确认这条MEET消息或者PING消息已到达，接收者会向发送者返回一条PONG消息。另外，一个节点也可以通过向集群广播自己的PONG消息来让集群中的其他节点立即刷新关于这个节点的认识，例如当一次故障转移操作成功执行之后，新的主节点会向集群广播一条PONG消息，以此来让集群中的其他节点立即知道这个节点已经变成了主节点，并且接管了已下线节点负责的槽。\n\n * FAIL消息：当一个主节点A判断另一个主节点B已经进入FAIL状态时，节点A会向集群广播一条关于节点B的FAIL消息，所有收到这条消息的节点都会立即将节点B标记为已下线。\n\n * PUBLISH消息：当节点接收到一个PUBLISH命令时，节点会执行这个命令，并向集群广播一条PUBLISH消息，所有接收到这条PUBLISH消息的节点都会执行相同的PUBLISH命令。\n\n一条消息由消息头（header）和消息正文（data）组成，接下来的内容将首先介绍消息头，然后再分别介绍上面提到的五种不同类型的消息正文。\n\n\n# 消息头\n\n节点发送的所有消息都由一个消息头包裹，消息头除了包含消息正文之外，还记录了消息发送者自身的一些信息，因为这些信息也会被消息接收者用到，所以严格来讲，我们可以认为消息头本身也是消息的一部分。\n\n每个消息头都由一个cluster.h/clusterMsg结构表示：\n\ntypedef struct {\n  // 消息的长度（包括这个消息头的长度和消息正文的长度）\n  uint32_t totlen;\n  // 消息的类型\n  uint16_t type;\n  // 消息正文包含的节点信息数量\n  // 只在发送MEET、PING、PONG这三种Gossip协议消息时使用\n  uint16_t count;\n  // 发送者所处的配置纪元\n  uint64_t currentEpoch;\n // 如果发送者是一个主节点，那么这里记录的是发送者的配置纪元\n  // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的配置纪元\n  uint64_t configEpoch;\n  // 发送者的名字（ID\n） \n  char sender[REDIS_CLUSTER_NAMELEN];\n  // 发送者目前的槽指派信息\n  unsigned char myslots[REDIS_CLUSTER_SLOTS/8];\n  // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的名字\n  // 如果发送者是一个主节点，那么这里记录的是REDIS_NODE_NULL_NAME\n  // （一个40字节长，值全为0的字节数组）\n  char slaveof[REDIS_CLUSTER_NAMELEN];\n  // 发送者的端口号\n  uint16_t port;\n  // 发送者的标识值\n  uint16_t flags;\n  // 发送者所处集群的状态\n  unsigned char state;\n  // 消息的正文（或者说，内容）\n  union clusterMsgData data;\n} clusterMsg;\n\n\nclusterMsg.data属性指向联合cluster.h/clusterMsgData，这个联合就是消息的正文：\n\nunion clusterMsgData {\n  // MEET、PING、PONG消息的正文\n  struct {\n    // 每条MEET、PING、PONG消息都包含两个\n    // clusterMsgDataGossip结构\n    clusterMsgDataGossip gossip[1];\n  } ping;\n  // FAIL消息的正文\n  struct {\n    clusterMsgDataFail about;\n  } fail;\n  // PUBLISH消息的正文\n  struct {\n    clusterMsgDataPublish msg;\n  } publish;\n  // 其他消息的正文\n};\n\n\nclusterMsg结构的currentEpoch、sender、myslots等属性记录了发送者自身的节点信息，接收者会根据这些信息，在自己的clusterState.nodes字典里找到发送者对应的clusterNode结构，并对结构进行更新。\n\n举个例子，通过对比接收者为发送者记录的槽指派信息，以及发送者在消息头的myslots属性记录的槽指派信息，接收者可以知道发送者的槽指派信息是否发生了变化。\n\n又或者说，通过对比接收者为发送者记录的标识值，以及发送者在消息头的flags属性记录的标识值，接收者可以知道发送者的状态和角色是否发生了变化，例如节点状态由原来的在线变成了下线，或者由主节点变成了从节点等等。\n\n\n# MEET、PING、PONG消息的实现\n\nRedis集群中的各个节点通过Gossip协议来交换各自关于不同节点的状态信息，其中Gossip协议由MEET、PING、PONG三种消息实现，这三种消息的正文都由两个cluster.h/clusterMsgDataGossip结构组成：\n\nunion clusterMsgData {\n  // \n  // MEET、PING和PONG消息的正文\n  struct {\n    // 每条MEET、PING、PONG消息都包含两个\n    // clusterMsgDataGossip结构\n    clusterMsgDataGossip gossip[1];\n  } ping;\n  // 其他消息的正文 \n};\n\n\n\n因为MEET、PING、PONG三种消息都使用相同的消息正文，所以节点通过消息头的type属性来判断一条消息是MEET消息、PING消息还是PONG消息。\n\n每次发送MEET、PING、PONG消息时，发送者都从自己的已知节点列表中随机选出两个节点（可以是主节点或者从节点），并将这两个被选中节点的信息分别保存到两个clusterMsgDataGossip结构里面。\n\nclusterMsgDataGossip结构记录了被选中节点的名字，发送者与被选中节点最后一次发送和接收PING消息和PONG消息的时间戳，被选中节点的IP地址和端口号，以及被选中节点的标识值：\n\ntypedef struct {\n  // 节点的名字\n  char nodename[REDIS_CLUSTER_NAMELEN];\n  // 最后一次向该节点发送PING消息的时间戳\n  uint32_t ping_sent;\n  // 最后一次从该节点接收到PONG消息的时间戳\n  uint32_t pong_received;\n  // 节点的IP地址\n  char ip[16];\n  // 节点的端口号\n  uint16_t port;\n  // 节点的标识值\n  uint16_t flags;\n} clusterMsgDataGossip;\n\n\n当接收者收到MEET、PING、PONG消息时，接收者会访问消息正文中的两个clusterMsgDataGossip结构，并根据自己是否认识clusterMsgDataGossip结构中记录的被选中节点来选择进行哪种操作：\n\n * 如果被选中节点不存在于接收者的已知节点列表，那么说明接收者是第一次接触到被选中节点，接收者将根据结构中记录的IP地址和端口号等信息，与被选中节点进行握手。\n\n * 如果被选中节点已经存在于接收者的已知节点列表，那么说明接收者之前已经与被选中节点进行过接触，接收者将根据clusterMsgDataGossip结构记录的信息，对被选中节点所对应的clusterNode结构进行更新。\n\n举个发送PING消息和返回PONG消息的例子，假设在一个包含A、B、C、D、E、F六个节点的集群里：\n\n * 节点A向节点D发送PING消息，并且消息里面包含了节点B和节点C的信息，当节点D收到这条PING消息时，它将更新自己对节点B和节点C的认识。\n\n * 之后，节点D将向节点A返回一条PONG消息，并且消息里面包含了节点E和节点F的消息，当节点A收到这条PONG消息时，它将更新自己对节点E和节点F的认识。\n\n整个通信过程如图17-41所示。\n\n\n\n图17-41　一个PING-PONG消息通信示例\n\n\n# FAIL消息的实现\n\n当集群里的主节点A将主节点B标记为已下线（FAIL）时，主节点A将向集群广播一条关于主节点B的FAIL消息，所有接收到这条FAIL消息的节点都会将主节点B标记为已下线。\n\n在集群的节点数量比较大的情况下，单纯使用Gossip协议来传播节点的已下线信息会给节点的信息更新带来一定延迟，因为Gossip协议消息通常需要一段时间才能传播至整个集群，而发送FAIL消息可以让集群里的所有节点立即知道某个主节点已下线，从而尽快判断是否需要将集群标记为下线，又或者对下线主节点进行故障转移。\n\nFAIL消息的正文由cluster.h/clusterMsgDataFail结构表示，这个结构只包含一个nodename属性，该属性记录了已下线节点的名字：\n\ntypedef struct {\n    char nodename[REDIS_CLUSTER_NAMELEN];\n} clusterMsgDataFail;\n\n\n因为集群里的所有节点都有一个独一无二的名字，所以FAIL消息里面只需要保存下线节点的名字，接收到消息的节点就可以根据这个名字来判断是哪个节点下线了。\n\n举个例子，对于包含7000、7001、7002、7003四个主节点的集群来说：\n\n * 如果主节点7001发现主节点7000已下线，那么主节点7001将向主节点7002和主节点7003发送FAIL消息，其中FAIL消息中包含的节点名字为主节点7000的名字，以此来表示主节点7000已下线。\n\n * 当主节点7002和主节点7003都接收到主节点7001发送的FAIL消息时，它们也会将主节点7000标记为已下线。\n\n * 因为这时集群已经有超过一半的主节点认为主节点7000已下线，所以集群剩下的几个主节点可以判断是否需要将集群标记为下线，又或者开始对主节点7000进行故障转移。\n\n图17-42至图17-44展示了节点发送和接收FAIL消息的整个过程。\n\n\n\n图17-42　节点7001将节点7000标记为已下线\n\n\n\n图17-43　节点7001向集群广播FAIL消息\n\n\n\n图17-44　节点7002和节点7003也将节点7000标记为已下线\n\n\n# PUBLISH消息的实现\n\n当客户端向集群中的某个节点发送命令\n\nPUBLISH <channel> <message>\n\n\n的时候，接收到PUBLISH命令的节点不仅会向channel频道发送消息message，它还会向集群广播一条PUBLISH消息，所有接收到这条PUBLISH消息的节点都会向channel频道发送message消息。\n\n换句话说，向集群中的某个节点发送命令：\n\nPUBLISH <channel> <message>\n\n\n将导致集群中的所有节点都向channel频道发送message消息。\n\n举个例子，对于包含7000、7001、7002、7003四个节点的集群来说，如果节点7000收到了客户端发送的PUBLISH命令，那么节点7000将向7001、7002、7003三个节点发送PUBLISH消息，如图17-45所示。\n\n\n\n图17-45　接收到PUBLISH命令的节点7000向集群广播PUBLISH消息\n\nPUBLISH消息的正文由cluster.h/clusterMsgDataPublish结构表示：\n\ntypedef struct {\n  uint32_t channel_len;\n  uint32_t message_len;\n  // 定义为8 字节只是为了对齐其他消息结构\n  // 实际的长度由保存的内容决定\n  unsigned char bulk_data[8];\n} clusterMsgDataPublish;\n\n\nclusterMsgDataPublish结构的bulk_data属性是一个字节数组，这个字节数组保存了客户端通过PUBLISH命令发送给节点的channel参数和message参数，而结构的channel_len和message_len则分别保存了channel参数的长度和message参数的长度：\n\n * 其中bulk_data的0字节至channel_len-1字节保存的是channel参数。\n\n * 而bulk_data的channel_len字节至channel_len+message_len-1字节保存的则是message参数。\n\n举个例子，如果节点收到的PUBLISH命令为：\n\nPUBLISH "news.it" "hello"\n\n\n那么节点发送的PUBLISH消息的clusterMsgDataPublish结构将如图17-46所示：其中bulk_data数组的前七个字节保存了channel参数的值"news.it"，而bulk_data数组的后五个字节则保存了message参数的值"hello"。\n\n\n\n图17-46　clusterMsgDataPublish结构示例\n\n> 为什么不直接向节点广播PUBLISH命令\n> \n> 实际上，要让集群的所有节点都执行相同的PUBLISH命令，最简单的方法就是向所有节点广播相同的PUBLISH命令，这也是Redis在复制PUBLISH命令时所使用的方法，不过因为这种做法并不符合Redis集群的“各个节点通过发送和接收消息来进行通信”这一规则，所以节点没有采取广播PUBLISH命令的做法。\n\n\n# 重点回顾\n\n * 节点通过握手来将其他节点添加到自己所处的集群当中。\n * 集群中的 16384 个槽可以分别指派给集群中的各个节点， 每个节点都会记录哪些槽指派给了自己， 而哪些槽又被指派给了其他节点。\n * 节点在接到一个命令请求时， 会先检查这个命令请求要处理的键所在的槽是否由自己负责， 如果不是的话， 节点将向客户端返回一个 MOVED 错误， MOVED 错误携带的信息可以指引客户端转向至正在负责相关槽的节点。\n * 对 Redis 集群的重新分片工作是由客户端执行的， 重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点。\n * 如果节点 A 正在迁移槽 i 至节点 B ， 那么当节点 A 没能在自己的数据库中找到命令指定的数据库键时， 节点 A 会向客户端返回一个 ASK 错误， 指引客户端到节点 B 继续查找指定的数据库键。\n * MOVED 错误表示槽的负责权已经从一个节点转移到了另一个节点， 而 ASK 错误只是两个节点在迁移槽的过程中使用的一种临时措施。\n * 集群里的从节点用于复制主节点， 并在主节点下线时， 代替主节点继续处理命令请求。\n * 集群中的节点通过发送和接收消息来进行通讯， 常见的消息包括 MEET 、 PING 、 PONG 、 PUBLISH 、 FAIL 五种。\n\n\n# 参考文献\n\n * 极客时间：Redis源码剖析与实战\n\n * Redis设计与实现\n\n * Github：redis 源码',normalizedContent:'# 前言\n\nredis 集群是 redis 提供的分布式数据库方案，集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能。\n\n本节将对集群的节点、槽指派、命令执行、重新分片、转向、故障转移、消息等各个方面进行介绍\n\n\n# 节点\n\n一个 redis 集群通常由多个节点（node）组成， 在刚开始的时候， 每个节点都是相互独立的， 它们都处于一个只包含自己的集群当中， 要组建一个真正可工作的集群， 我们必须将各个独立的节点连接起来， 构成一个包含多个节点的集群。\n\n连接各个节点的工作可以使用 cluster meet 命令来完成， 该命令的格式如下：\n\ncluster meet <ip> <port>\n\n\n向一个节点 node 发送 cluster meet 命令， 可以让 node 节点与 ip 和 port 所指定的节点进行握手（handshake）， 当握手成功时， node 节点就会将 ip 和 port 所指定的节点添加到 node 节点当前所在的集群中。\n\n举个例子， 假设现在有三个独立的节点 127.0.0.1:7000 、 127.0.0.1:7001 、 127.0.0.1:7002 （下文省略 ip 地址，直接使用端口号来区分各个节点）， 我们首先使用客户端连上节点 7000 ， 通过发送 cluster node 命令可以看到， 集群目前只包含 7000 自己一个节点：\n\n$ redis-cli -c -p 7000\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n通过向节点 7000 发送以下命令， 我们可以将节点 7001 添加到节点 7000 所在的集群里面：\n\n127.0.0.1:7000> cluster meet 127.0.0.1 7001\nok\n\n127.0.0.1:7000> cluster nodes\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388204746210 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n继续向节点 7000 发送以下命令， 我们可以将节点 7002 也添加到节点 7000 和节点 7001 所在的集群里面：\n\n127.0.0.1:7000> cluster meet 127.0.0.1 7002\nok\n\n127.0.0.1:7000> cluster nodes\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388204848376 0 connected\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388204847977 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected\n\n\n现在， 这个集群里面包含了 7000 、 7001 和 7002 三个节点， 图 image_connect_nodes_1 至 image_connect_nodes_5 展示了这三个节点进行握手的整个过程。\n\n\n\n\n\n\n\n\n\n\n\n本节接下来的内容将介绍启动节点的方法， 和集群有关的数据结构， 以及 cluster meet 命令的实现原理。\n\n\n# 启动节点\n\n一个节点就是一个运行在集群模式下的 redis 服务器， redis 服务器在启动时会根据 cluster-enabled 配置选项的是否为 yes 来决定是否开启服务器的集群模式， 如图 image_node_or_server 所示。\n\n\n\n节点（运行在集群模式下的 redis 服务器）会继续使用所有在单机模式中使用的服务器组件， 比如说：\n\n * 节点会继续使用文件事件处理器来处理命令请求和返回命令回复。\n * 节点会继续使用时间事件处理器来执行 servercron 函数， 而 servercron 函数又会调用集群模式特有的 clustercron 函数： clustercron 函数负责执行在集群模式下需要执行的常规操作， 比如向集群中的其他节点发送 gossip 消息， 检查节点是否断线； 又或者检查是否需要对下线节点进行自动故障转移， 等等。\n * 节点会继续使用数据库来保存键值对数据，键值对依然会是各种不同类型的对象。\n * 节点会继续使用 rdb 持久化模块和 aof 持久化模块来执行持久化工作。\n * 节点会继续使用发布与订阅模块来执行 publish 、 subscribe 等命令。\n * 节点会继续使用复制模块来进行节点的复制工作。\n * 节点会继续使用 lua 脚本环境来执行客户端输入的 lua 脚本。\n\n诸如此类。\n\n除此之外， 节点会继续使用 redisserver 结构来保存服务器的状态， 使用 redisclient 结构来保存客户端的状态， 至于那些只有在集群模式下才会用到的数据， 节点将它们保存到了 cluster.h/clusternode 结构， cluster.h/clusterlink 结构， 以及 cluster.h/clusterstate 结构里面， 接下来的一节将对这三种数据结构进行介绍\n\n\n# 集群数据结构\n\nclusternode 结构保存了一个节点的当前状态， 比如节点的创建时间， 节点的名字， 节点当前的配置纪元， 节点的 ip 和地址， 等等。\n\n每个节点都会使用一个 clusternode 结构来记录自己的状态， 并为集群中的所有其他节点（包括主节点和从节点）都创建一个相应的 clusternode 结构， 以此来记录其他节点的状态：\n\nstruct clusternode {\n\n    // 创建节点的时间\n    mstime_t ctime;\n\n    // 节点的名字，由 40 个十六进制字符组成\n    // 例如 68eef66df23420a5862208ef5b1a7005b806f2ff\n    char name[redis_cluster_namelen];\n\n    // 节点标识\n    // 使用各种不同的标识值记录节点的角色（比如主节点或者从节点），\n    // 以及节点目前所处的状态（比如在线或者下线）。\n    int flags;\n\n    // 节点当前的配置纪元，用于实现故障转移\n    uint64_t configepoch;\n\n    // 节点的 ip 地址\n    char ip[redis_ip_str_len];\n\n    // 节点的端口号\n    int port;\n\n    // 保存连接节点所需的有关信息\n    clusterlink *link;\n\n};\n\n\nclusternode 结构的 link 属性是一个 clusterlink 结构， 该结构保存了连接节点所需的有关信息， 比如套接字描述符， 输入缓冲区和输出缓冲区：\n\ntypedef struct clusterlink {\n\n    // 连接的创建时间\n    mstime_t ctime;\n\n    // tcp 套接字描述符\n    int fd;\n\n    // 输出缓冲区，保存着等待发送给其他节点的消息（message）。\n    sds sndbuf;\n\n    // 输入缓冲区，保存着从其他节点接收到的消息。\n    sds rcvbuf;\n\n    // 与这个连接相关联的节点，如果没有的话就为 null\n    struct clusternode *node;\n\n} clusterlink;\n\n\nredisclient 结构和 clusterlink 结构的相同和不同之处\n\nredisclient 结构和 clusterlink 结构都有自己的套接字描述符和输入、输出缓冲区， 这两个结构的区别在于， redisclient 结构中的套接字和缓冲区是用于连接客户端的， 而 clusterlink 结构中的套接字和缓冲区则是用于连接节点的。\n\n最后， 每个节点都保存着一个 clusterstate 结构， 这个结构记录了在当前节点的视角下， 集群目前所处的状态 —— 比如集群是在线还是下线， 集群包含多少个节点， 集群当前的配置纪元， 诸如此类：\n\ntypedef struct clusterstate {\n\n    // 指向当前节点的指针\n    clusternode *myself;\n\n    // 集群当前的配置纪元，用于实现故障转移\n    uint64_t currentepoch;\n\n    // 集群当前的状态：是在线还是下线\n    int state;\n\n    // 集群中至少处理着一个槽的节点的数量\n    int size;\n\n    // 集群节点名单（包括 myself 节点）\n    // 字典的键为节点的名字，字典的值为节点对应的 clusternode 结构\n    dict *nodes;\n\n} clusterstate;\n\n\n以前面介绍的 7000 、 7001 、 7002 三个节点为例， 图 image_cluster_state_of_7000 展示了节点 7000 创建的 clusterstate 结构， 这个结构从节点 7000 的角度记录了集群、以及集群包含的三个节点的当前状态 （为了空间考虑，图中省略了 clusternode 结构的一部分属性）：\n\n * 结构的 currentepoch 属性的值为 0 ， 表示集群当前的配置纪元为 0 。\n * 结构的 size 属性的值为 0 ， 表示集群目前没有任何节点在处理槽： 因此结构的 state 属性的值为 redis_cluster_fail —— 这表示集群目前处于下线状态。\n * 结构的 nodes 字典记录了集群目前包含的三个节点， 这三个节点分别由三个 clusternode 结构表示： 其中 myself 指针指向代表节点 7000 的 clusternode 结构， 而字典中的另外两个指针则分别指向代表节点 7001 和代表节点 7002 的 clusternode 结构， 这两个节点是节点 7000 已知的在集群中的其他节点。\n * 三个节点的 clusternode 结构的 flags 属性都是 redis_node_master ，说明三个节点都是主节点。\n\n节点 7001 和节点 7002 也会创建类似的 clusterstate 结构：\n\n * 不过在节点 7001 创建的 clusterstate 结构中， myself 指针将指向代表节点 7001 的 clusternode 结构， 而节点 7000 和节点 7002 则是集群中的其他节点。\n * 而在节点 7002 创建的 clusterstate 结构中， myself 指针将指向代表节点 7002 的 clusternode 结构， 而节点 7000 和节点 7001 则是集群中的其他节点。\n\n\n\n\n# cluster meet 命令的实现\n\n通过向节点 a 发送 cluster meet 命令， 客户端可以让接收命令的节点 a 将另一个节点 b 添加到节点 a 当前所在的集群里面：\n\ncluster meet <ip> <port>\n\n\n收到命令的节点 a 将与节点 b 进行握手（handshake）， 以此来确认彼此的存在， 并为将来的进一步通信打好基础：\n\n 1. 节点 a 会为节点 b 创建一个 clusternode 结构， 并将该结构添加到自己的 clusterstate.nodes 字典里面。\n 2. 之后， 节点 a 将根据 cluster meet 命令给定的 ip 地址和端口号， 向节点 b 发送一条 meet 消息（message）。\n 3. 如果一切顺利， 节点 b 将接收到节点 a 发送的 meet 消息， 节点 b 会为节点 a 创建一个 clusternode 结构， 并将该结构添加到自己的 clusterstate.nodes 字典里面。\n 4. 之后， 节点 b 将向节点 a 返回一条 pong 消息。\n 5. 如果一切顺利， 节点 a 将接收到节点 b 返回的 pong 消息， 通过这条 pong 消息节点 a 可以知道节点 b 已经成功地接收到了自己发送的 meet 消息。\n 6. 之后， 节点 a 将向节点 b 返回一条 ping 消息。\n 7. 如果一切顺利， 节点 b 将接收到节点 a 返回的 ping 消息， 通过这条 ping 消息节点 b 可以知道节点 a 已经成功地接收到了自己返回的 pong 消息， 握手完成。\n\n图 image_handshake 展示了以上步骤描述的握手过程。\n\n\n\n之后， 节点 a 会将节点 b 的信息通过 gossip 协议传播给集群中的其他节点， 让其他节点也与节点 b 进行握手， 最终， 经过一段时间之后， 节点 b 会被集群中的所有节点认识\n\n\n# 槽指派\n\nredis 集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为 16384 个槽（slot），数据库中的每个键都属于这 16384个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。\n\n * 当数据库中的 16384 个槽都有节点在处理时，集群处于上线状态（ok）\n * 如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态（fail）\n\n在上一节，我们使用 cluster meet 命令将 7000、7001、7002 三个节点连接到了同一个集群里面，不过这个集群目前仍然处于下线状态，因为集群中的三个节点都没有在处理任何槽：\n\n127.0.0.1:7000> cluster info\ncluster_state:fail\ncluster_slots_assigned:0\ncluster_slots_ok:0\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:3\ncluster_size:0\ncluster_current_epoch:0\ncluster_stats_messages_sent:110\ncluster_stats_messages_received:28\n\n\n通过向节点发送 cluster addslots 命令，我们可以将一个或多个槽指派（assign）给节点负责：\n\ncluster addslots <slot> [slot]\n\n\n举个例子，执行以下命令可以将槽0至槽5000指派给节点7000负责：\n\n127.0.0.1:7000> cluster addslots 0 1 2 3 4  5000\nok\n127.0.0.1:7000> cluster nodes\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388316664849 0 connected\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388316665850 0 connected\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n\n\n为了让 7000、7001、7002 三个节点所在的集群进入上线状态，我们继续执行以下命令，将槽 5001 至槽 10000 指派给节点 7001 负责：\n\n127.0.0.1:7001> cluster addslots 5001 5002 5003 5004 10000\nok\n\n\n然后将槽 10001 至槽 16383 指派给7002负责：\n\n127.0.0.1:7002> cluster addslots 10001 10002 10003 10004 16383\nok\n\n\n当以上三个cluster addslots命令都执行完毕之后，数据库中的16384个槽都已经被指派给了相应的节点，集群进入上线状态：\n\n127.0.0.1:7000> cluster info\ncluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:3\ncluster_size:3\ncluster_current_epoch:0\ncluster_stats_messages_sent:2699\ncluster_stats_messages_received:2617\n127.0.0.1:7000> cluster nodes\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388317426165 0 connected 10001-16383\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388317427167 0 connected 5001-10000\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n\n\n本节接下来的内容将首先介绍节点保存槽指派信息的方法，以及节点之间传播槽指派信息的方法，之后再介绍cluster addslots命令的实现。\n\n\n# 记录节点的槽指派信息\n\nclusternode结构的slots属性和numslot属性记录了节点负责处理哪些槽：\n\nstruct clusternode {\n\n  unsigned char slots[16384/8];\n  int numslots;\n \n};\n\n\nslots属性是一个二进制位数组（bit array），这个数组的长度为16384/8=2048个字节，共包含16384个二进制位。\n\nredis以0为起始索引，16383为终止索引，对slots数组中的16384个二进制位进行编号，并根据索引i上的二进制位的值来判断节点是否负责处理槽i：\n\n * 如果slots数组在索引i上的二进制位的值为1，那么表示节点负责处理槽i。\n\n * 如果slots数组在索引i上的二进制位的值为0，那么表示节点不负责处理槽i。\n\n图17-9展示了一个slots数组示例：这个数组索引0至索引7上的二进制位的值都为1，其余所有二进制位的值都为0，这表示节点负责处理槽0至槽7。\n\n\n\n图17-9　一个slots数组示例\n\n图17-10 展示了另一个slots数组示例：这个数组索引1、3、5、8、9、10上的二进制位的值都为1，而其余所有二进制位的值都为0，这表示节点负责处理槽1、3、5、8、9、10。\n\n\n\n图17-10　另一个slots数组示例\n\n因为取出和设置slots数组中的任意一个二进制位的值的复杂度仅为o（1），所以对于一个给定节点的slots数组来说，程序检查节点是否负责处理某个槽，又或者将某个槽指派给节点负责，这两个动作的复杂度都是o（1）。\n\n至于numslots属性则记录节点负责处理的槽的数量，也即是slots数组中值为1的二进制位的数量。\n\n比如说，对于图17-9所示的slots数组来说，节点处理的槽数量为8，而对于图17-10所示的slots数组来说，节点处理的槽数量为6。\n\n\n# 传播节点的槽指派信息\n\n一个节点除了会将自己负责处理的槽记录在 clusternode 结构的 slots 属性和 numslots 属性之外，它还会将自己的 slots 数组通过消息发送给集群中的其他节点，以此来告知其他节点自己目前负责处理哪些槽。\n\n举个例子，对于前面展示的包含7000、7001、7002三个节点的集群来说：\n\n * 节点7000会通过消息向节点7001和节点7002发送自己的slots数组，以此来告知这两个节点，自己负责处理槽0至槽5000，如图17-11所示。\n\n * 节点7001会通过消息向节点7000和节点7002发送自己的slots数组，以此来告知这两个节点，自己负责处理槽5001至槽10000，如图17-12所示。\n\n * 节点7002会通过消息向节点7000和节点7001发送自己的slots数组，以此来告知这两个节点，自己负责处理槽10001至槽16383，如图17-13所示。\n\n\n\n图17-11　7000告知7001和7002自己负责处理的槽\n\n\n\n\n\n图17-13　7002告知7000和7001自己负责处理的槽\n\n当节点a通过消息从节点b那里接收到节点b的slots数组时，节点a会在自己的 clusterstate.nodes 字典中查找节点b对应的 clusternode 结构，并对结构中的 slots 数组进行保存或者更新。\n\n因为集群中的每个节点都会将自己的 slots 数组通过消息发送给集群中的其他节点，并且每个接收到 slots 数组的节点都会将数组保存到相应节点的 clusternode 结构里面，因此，集群中的每个节点都会知道数据库中的 16384 个槽分别被指派给了集群中的哪些节点。\n\n\n# 记录集群所有槽的指派信息\n\nclusterstate 结构中的 slots 数组记录了集群中所有 16384 个槽的指派信息：\n\ntypedef struct clusterstate {\n \n  clusternode *slots[16384];\n  \n} clusterstate;\n\n\nslots数组包含16384个项，每个数组项都是一个指向clusternode结构的指针：\n\n * 如果slots[i]指针指向null，那么表示槽i尚未指派给任何节点。\n\n * 如果slots[i]指针指向一个clusternode结构，那么表示槽i已经指派给了clusternode结构所代表的节点。\n\n举个例子，对于7000、7001、7002三个节点来说，它们的clusterstate结构的slots数组将会是图17-14所示的样子：\n\n * 数组项slots[0]至slots[5000]的指针都指向代表节点7000的clusternode结构，表示槽0至5000都指派给了节点7000。\n\n * 数组项slots[5001]至slots[10000]的指针都指向代表节点7001的clusternode结构，表示槽5001至10000都指派给了节点7001。\n\n * 数组项slots[10001]至slots[16383]的指针都指向代表节点7002的clusternode结构，表示槽10001至16383都指派给了节点7002。\n\n如果只将槽指派信息保存在各个节点的clusternode.slots数组里，会出现一些无法高效地解决的问题，而clusterstate.slots数组的存在解决了这些问题：\n\n * 如果节点只使用clusternode.slots数组来记录槽的指派信息，那么为了知道槽i是否已经被指派，或者槽i被指派给了哪个节点，程序需要遍历clusterstate.nodes字典中的所有clusternode结构，检查这些结构的slots数组，直到找到负责处理槽i的节点为止，这个过程的复杂度为o（n），其中n为clusterstate.nodes字典保存的clusternode结构的数量。\n\n * 而通过将所有槽的指派信息保存在clusterstate.slots数组里面，程序要检查槽i是否已经被指派，又或者取得负责处理槽i的节点，只需要访问clusterstate.slots[i]的值即可，这个操作的复杂度仅为o(1)。\n\n举个例子，对于图17-14所示的slots数组来说，如果程序需要知道槽10002被指派给了哪个节点，那么只要访问数组项slots[10002]，就可以马上知道槽10002被指派给了节点7002，如图17-15所示。\n\n\n\n图17-15　访问slots[10002]的值\n\n要说明的一点是，虽然clusterstate.slots数组记录了集群中所有槽的指派信息，但使用clusternode结构的slots数组来记录单个节点的槽指派信息仍然是有必要的：\n\n * 因为当程序需要将某个节点的槽指派信息通过消息发送给其他节点时，程序只需要将相应节点的clusternode.slots数组整个发送出去就可以了。\n\n * 另一方面，如果redis不使用clusternode.slots数组，而单独使用clusterstate.slots数组的话，那么每次要将节点a的槽指派信息传播给其他节点时，程序必须先遍历整个clusterstate.slots数组，记录节点a负责处理哪些槽，然后才能发送节点a的槽指派信息，这比直接发送clusternode.slots数组要麻烦和低效得多。\n\nclusterstate.slots数组记录了集群中所有槽的指派信息，而clusternode.slots数组只记录了clusternode结构所代表的节点的槽指派信息，这是两个slots数组的关键区别所在。\n\n\n# cluster addslots 命令的实现\n\ncluster addslots 命令接受一个或多个槽作为参数，并将所有输入的槽指派给接收该命令的节点负责：\n\ncluster addslots <slot> [slot]\n\n\ncluster addslots 命令的实现可以用以下伪代码来表示：\n\ndef cluster_addslots(*all_input_slots):\n    # 遍历所有输入槽，检查它们是否都是未指派槽\n    for i in all_input_slots:\n        # 如果有哪怕一个槽已经被指派给了某个节点\n        # 那么向客户端返回错误，并终止命令执行\n        if clusterstate.slots[i] != null:\n            reply_error()\n            return\n    # 如果所有输入槽都是未指派槽\n    # 那么再次遍历所有输入槽，将这些槽指派给当前节点\n    for i in all_input_slots:\n        # 设置clusterstate结构的slots数组\n        # 将slots[i]的指针指向代表当前节点的clusternode结构\n        clusterstate.slots[i] = clusterstate.myself\n        # 访问代表当前节点的clusternode结构的slots数组\n        # 将数组在索引i上的二进制位设置为1\n        setslotbit(clusterstate.myself.slots, i)\n\n\n举个例子，图17-16展示了一个节点的clusterstate结构，clusterstate.slots数组中的所有指针都指向null，并且clusternode.slots数组中的所有二进制位的值都是0，这说明当前节点没有被指派任何槽，并且集群中的所有槽都是未指派的。\n\n\n\n图17-16　节点的clusterstate结构\n\n当客户端对17-16所示的节点执行命令：\n\ncluster addslots 1 2\n\n\n将槽1和槽2指派给节点之后，节点的clusterstate结构将被更新成图17-17所示的样子：\n\n * clusterstate.slots数组在索引1和索引2上的指针指向了代表当前节点的clusternode结构。\n\n * 并且clusternode.slots数组在索引1和索引2上的位被设置成了1。\n\n\n\n图17-17　执行 cluster addslots 命令之后的 clusterstate 结构\n\n最后，在 cluster addslots 命令执行完毕之后，节点会通过发送消息告知集群中的其他节点，自己目前正在负责处理哪些槽。\n\n\n# 在集群中执行命令\n\n在对数据库中的16384个槽都进行了指派之后，集群就会进入上线状态，这时客户端就可以向集群中的节点发送数据命令了。\n\n当客户端向节点发送与数据库键有关的命令时，接收命令的节点会计算出命令要处理的数据库键属于哪个槽，并检查这个槽是否指派给了自己：\n\n * 如果键所在的槽正好就指派给了当前节点，那么节点直接执行这个命令。\n\n * 如果键所在的槽并没有指派给当前节点，那么节点会向客户端返回一个moved错误，指引客户端转向（redirect）至正确的节点，并再次发送之前想要执行的命令。\n\n图17-18展示了这两种情况的判断流程。\n\n\n\n图17-18　判断客户端是否需要转向的流程\n\n举个例子，如果我们在之前提到的，由7000、7001、7002三个节点组成的集群中，用客户端连上节点7000，并发送以下命令，那么命令会直接被节点7000执行：\n\n127.0.0.1:7000> set date "2013-12-31"\nok\n\n\n因为键date所在的槽2022正是由节点7000负责处理的。\n\n但是，如果我们执行以下命令，那么客户端会先被转向至节点7001，然后再执行命令：\n\n127.0.0.1:7000> set msg "happy new year!"\n-> redirected to slot [6257] located at 127.0.0.1:7001\nok\n127.0.0.1:7001> get msg\n"happy new year!"\n\n\n这是因为键msg所在的槽6257是由节点7001负责处理的，而不是由最初接收命令的节点7000负责处理：\n\n * 当客户端第一次向节点7000发送set命令的时候，节点7000会向客户端返回moved错误，指引客户端转向至节点7001。\n\n * 当客户端转向到节点7001之后，客户端重新向节点7001发送set命令，这个命令会被节点7001成功执行。\n\n本节接下来的内容将介绍计算键所属槽的方法，节点判断某个槽是否由自己负责的方法，以及moved错误的实现方法，最后，本节还会介绍节点和单机redis服务器保存键值对数据的相同和不同之处。\n\n\n# 计算键属于哪个槽\n\n节点使用以下算法来计算给定键key属于哪个槽：\n\ndef slot_number(key):\n    return crc16(key) & 16383\n\n\n其中crc16（key）语句用于计算键key的crc-16校验和，而&16383语句则用于计算出一个介于0至16383之间的整数作为键key的槽号。\n\n使用cluster keyslot命令可以查看一个给定键属于哪个槽：\n\n127.0.0.1:7000> cluster keyslot "date"\n(integer) 2022\n127.0.0.1:7000> cluster keyslot "msg"\n(integer) 6257\n127.0.0.1:7000> cluster keyslot "name"\n(integer) 5798\n127.0.0.1:7000> cluster keyslot "fruits"\n(integer) 14943\n\n\ncluster keyslot命令就是通过调用上面给出的槽分配算法来实现的，以下是该命令的伪代码实现：\n\ndef cluster_keyslot(key):\n    # 计算槽号\n    slot = slot_number(key)\n    # 将槽号返回给客户端\n    reply_client(slot)\n\n\n\n# 判断槽是否由当前节点负责处理\n\n当节点计算出键所属的槽i之后，节点就会检查自己在clusterstate.slots数组中的项i，判断键所在的槽是否由自己负责：\n\n1）如果clusterstate.slots[i]等于clusterstate.myself，那么说明槽i由当前节点负责，节点可以执行客户端发送的命令。\n\n2）如果clusterstate.slots[i]不等于clusterstate.myself，那么说明槽i并非由当前节点负责，节点会根据clusterstate.slots[i]指向的clusternode结构所记录的节点ip和端口号，向客户端返回moved错误，指引客户端转向至正在处理槽i的节点。\n\n举个例子，假设图17-19为节点7000的clusterstate结构：\n\n * 当客户端向节点7000发送命令set date"2013-12-31"的时候，节点首先计算出键date属于槽2022，然后检查得出clusterstate.slots[2022]等于clusterstate.myself，这说明槽2022正是由节点7000负责，于是节点7000直接执行这个set命令，并将结果返回给发送命令的客户端。\n\n * 当客户端向节点7000发送命令set msg"happy new year！"的时候，节点首先计算出键msg属于槽6257，然后检查clusterstate.slots[6257]是否等于clusterstate.myself，结果发现两者并不相等：这说明槽6257并非由节点7000负责处理，于是节点7000访问clusterstate.slots[6257]所指向的clusternode结构，并根据结构中记录的ip地址127.0.0.1和端口号7001，向客户端返回错误moved 6257 127.0.0.1:7001，指引节点转向至正在负责处理槽6257的节点7001。\n\n\n\n图17-19　节点7000的clusterstate结构\n\n\n# moved 错误\n\n当节点发现键所在的槽并非由自己负责处理的时候，节点就会向客户端返回一个 moved 错误，指引客户端转向至正在负责槽的节点。\n\nmoved错误的格式为：\n\nmoved <slot> <ip>:<port>\n\n\n其中slot为键所在的槽，而ip和port则是负责处理槽slot的节点的ip地址和端口号。例如错误：\n\noved 10086 127.0.0.1:7002\n\n\n表示槽10086正由ip地址为127.0.0.1，端口号为7002的节点负责。\n\n又例如错误：\n\nmoved 789 127.0.0.1:7000\n\n\n表示槽789正由ip地址为127.0.0.1，端口号为7000的节点负责。\n\n当客户端接收到节点返回的moved错误时，客户端会根据moved错误中提供的ip地址和端口号，转向至负责处理槽slot的节点，并向该节点重新发送之前想要执行的命令。以前面的客户端从节点7000转向至7001的情况作为例子：\n\n127.0.0.1:7000> set msg "happy new year!"\n-> redirected to slot [6257] located at 127.0.0.1:7001\nok\n127.0.0.1:7001>\n\n\n图17-20展示了客户端向节点7000发送set命令，并获得moved错误的过程。\n\n\n\n图17-20　节点7000向客户端返回moved错误\n\n而图17-21则展示了客户端根据moved错误，转向至节点7001，并重新发送set命令的过程。\n\n\n\n图17-21　客户端根据moved错误的指示转向至节点7001\n\n一个集群客户端通常会与集群中的多个节点创建套接字连接，而所谓的节点转向实际上就是换一个套接字来发送命令。\n\n如果客户端尚未与想要转向的节点创建套接字连接，那么客户端会先根据moved错误提供的ip地址和端口号来连接节点，然后再进行转向。\n\n> 被隐藏的moved错误\n> \n> 集群模式的redis-cli客户端在接收到moved错误时，并不会打印出moved错误，而是根据moved错误自动进行节点转向，并打印出转向信息，所以我们是看不见节点返回的moved错误的：\n> \n> $ redis-cli -c -p 7000 # \n> 集群模式\n> 127.0.0.1:7000> set msg "happy new year!"\n> -> redirected to slot [6257] located at 127.0.0.1:7001\n> ok\n> 127.0.0.1:7001>\n> \n> \n> 但是，如果我们使用单机（stand alone）模式的redis-cli客户端，再次向节点7000发送相同的命令，那么moved错误就会被客户端打印出来：\n> \n> $ redis-cli -p 7000 # \n> 单机模式\n> 127.0.0.1:7000> set msg "happy new year!"\n> (error) moved 6257 127.0.0.1:7001\n> 127.0.0.1:7000>\n> \n> \n> 这是因为单机模式的redis-cli客户端不清楚moved错误的作用，所以它只会直接将moved错误直接打印出来，而不会进行自动转向。\n\n\n# 节点数据库的实现\n\n集群节点保存键值对以及键值对过期时间的方式，与第9章里面介绍的单机redis服务器保存键值对以及键值对过期时间的方式完全相同。\n\n节点和单机服务器在数据库方面的一个区别是，节点只能使用0号数据库，而单机redis服务器则没有这一限制。\n\n举个例子，图17-22展示了节点7000的数据库状态，数据库中包含列表键"lst"，哈希键"book"，以及字符串键"date"，其中键"lst"和键"book"带有过期时间。\n\n另外，除了将键值对保存在数据库里面之外，节点还会用clusterstate结构中的slots_to_keys跳跃表来保存槽和键之间的关系：\n\ntypedef struct clusterstate {\n\n  zskiplist *slots_to_keys;\n \n} clusterstate;\n\n\n\n\n图17-22　节点7000的数据库\n\nslots_to_keys跳跃表每个节点的分值（score）都是一个槽号，而每个节点的成员（member）都是一个数据库键：\n\n * 每当节点往数据库中添加一个新的键值对时，节点就会将这个键以及键的槽号关联到slots_to_keys跳跃表。\n\n * 当节点删除数据库中的某个键值对时，节点就会在slots_to_keys跳跃表解除被删除键与槽号的关联。\n\n举个例子，对于图17-22所示的数据库，节点7000将创建类似图17-23所示的slots_to_keys跳跃表：\n\n * 键"book"所在跳跃表节点的分值为1337.0，这表示键"book"所在的槽为1337。\n\n * 键"date"所在跳跃表节点的分值为2022.0，这表示键"date"所在的槽为2022。\n\n * 键"lst"所在跳跃表节点的分值为3347.0，这表示键"lst"所在的槽为3347。\n\n通过在slots_to_keys跳跃表中记录各个数据库键所属的槽，节点可以很方便地对属于某个或某些槽的所有数据库键进行批量操作，例如命令cluster getkeysinslot命令可以返回最多count个属于槽slot的数据库键，而这个命令就是通过遍历slots_to_keys跳跃表来实现的。\n\n\n\n图17-23　节点7000的slots_to_keys跳跃表\n\n\n# 重新分片\n\nredis集群的重新分片操作可以将任意数量已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽所属的键值对也会从源节点被移动到目标节点。\n\n重新分片操作可以在线（online）进行，在重新分片的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。\n\n举个例子，对于之前提到的，包含7000、7001、7002三个节点的集群来说，我们可以向这个集群添加一个ip为127.0.0.1，端口号为7003的节点（后面简称节点7003）：\n\n$ redis-cli -c -p 7000\n127.0.0.1:7000> cluster meet 127.0.0.1 7003\nok\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master - 0 0 0 connected 0-5000\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master - 0 1388635782831 0 connected 5001-10000\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master - 0 1388635782831 0 connected 10001-16383\n04579925484ce537d3410d7ce97bd2e260c459a2 127.0.0.1:7003 master - 0 1388635782330 0 connected\n\n\n然后通过重新分片操作，将原本指派给节点7002的槽15001至16383改为指派给节点7003。\n\n以下是重新分片操作执行之后，节点的槽分配状态：\n\n127.0.0.1:7000> cluster nodes\n51549e625cfda318ad27423a31e7476fe3cd2939 :0 myself,master -0 0 0 connected 0-5000\n68eef66df23420a5862208ef5b1a7005b806f2ff 127.0.0.1:7001 master -0 1388635782831 0 connected 5001-10000\n9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26 127.0.0.1:7002 master -0 1388635782831 0 connected 10001-15000\n04579925484ce537d3410d7ce97bd2e260c459a2 127.0.0.1:7003 master -0 1388635782330 0 connected 15001-16383\n\n\n重新分片的实现原理\n\nredis集群的重新分片操作是由redis的集群管理软件redis-trib负责执行的，redis提供了进行重新分片所需的所有命令，而redis-trib则通过向源节点和目标节点发送命令来进行重新分片操作。\n\nredis-trib对集群的单个槽slot进行重新分片的步骤如下：\n\n1）redis-trib对目标节点发送cluster setslot <slot> importing <source_id> 命令，让目标节点准备好从源节点导入（import）属于槽slot的键值对。\n\n2）redis-trib对源节点发送cluster setslot <slot> migrating <target_id>命令，让源节点准备好将属于槽slot的键值对迁移（migrate）至目标节点。\n\n3）redis-trib向源节点发送cluster getkeysinslot <slot> <count> 命令，获得最多count个属于槽slot的键值对的键名（key name）。\n\n4）对于步骤3获得的每个键名，redis-trib都向源节点发送一个migrate <target_ip ><target_port> <key_name> 0 <timeout>命令，将被选中的键原子地从源节点迁移至目标节点。\n\n5）重复执行步骤3和步骤4，直到源节点保存的所有属于槽slot的键值对都被迁移至目标节点为止。每次迁移键的过程如图17-24所示。\n\n6）redis-trib向集群中的任意一个节点发送cluster setslot <slot> node <target_id>命令，将槽slot指派给目标节点，这一指派信息会通过消息发送至整个集群，最终集群中的所有节点都会知道槽slot已经指派给了目标节点。\n\n\n\n图17-24　迁移键的过程\n\n图17-25 展示了对槽slot进行重新分片的整个过程。\n\n如果重新分片涉及多个槽，那么redis-trib将对每个给定的槽分别执行上面给出的步骤。\n\n\n\n图17-25　对槽slot进行重新分片的过程\n\n\n# ask 错误\n\n在进行重新分片期间，源节点向目标节点迁移一个槽的过程中，可能会出现这样一种情况：属于被迁移槽的一部分键值对保存在源节点里面，而另一部分键值对则保存在目标节点里面。\n\n当客户端向源节点发送一个与数据库键有关的命令，并且命令要处理的数据库键恰好就属于正在被迁移的槽时：\n\n * 源节点会先在自己的数据库里面查找指定的键，如果找到的话，就直接执行客户端发送的命令。\n * 相反地，如果源节点没能在自己的数据库里面找到指定的键，那么这个键有可能已经被迁移到了目标节点，源节点将向客户端返回一个ask错误，指引客户端转向正在导入槽的目标节点，并再次发送之前想要执行的命令。\n\n图17-26展示了源节点判断是否需要向客户端发送ask错误的整个过程。\n\n\n\n图17-26　判断是否发送ask错误的过程\n\n举个例子，假设节点7002正在向节点7003迁移槽16198，这个槽包含"is"和"love"两个键，其中键"is"还留在节点7002，而键"love"已经被迁移到了节点7003。\n\n如果我们向节点7002发送关于键"is"的命令，那么这个命令会直接被节点7002执行：\n\n127.0.0.1:7002> get "is"\n"you get the key \'is\'"\n\n\n而如果我们向节点7002发送关于键"love"的命令，那么客户端会先被转向至节点7003，然后再次执行命令：\n\n127.0.0.1:7002> get "love"\n-> redirected to slot [16198] located at 127.0.0.1:7003\n"you get the key \'love\'"\n127.0.0.1:7003>\n\n\n> 被隐藏的ask错误\n> \n> 和接到moved错误时的情况类似，集群模式的redis-cli在接到ask错误时也不会打印错误，而是自动根据错误提供的ip地址和端口进行转向动作。如果想看到节点发送的ask错误的话，可以使用单机模式的redis-cli客户端：\n> \n> $ redis-cli -p 7002\n> 127.0.0.1:7002> get "love"\n> (error) ask 16198 127.0.0.1:7003\n\n注意\n\n在写这篇文章的时候，集群模式的redis-cli并未支持ask自动转向，上面展示的ask自动转向行为实际上是根据moved自动转向行为虚构出来的。因此，当集群模式的redis-cli真正支持ask自动转向时，它的行为和上面展示的行为可能会有所不同。\n\n本节将对ask错误的实现原理进行说明，并对比ask错误和moved错误的区别。\n\n\n# cluster setslot importing命令的实现\n\nclusterstate结构的importing_slots_from数组记录了当前节点正在从其他节点导入的槽：\n\ntypedef struct clusterstate {\n\n  clusternode *importing_slots_from[16384];\n \n} clusterstate;\n\n\n如果importing_slots_from[i]的值不为null，而是指向一个clusternode结构，那么表示当前节点正在从clusternode所代表的节点导入槽i。\n\n在对集群进行重新分片的时候，向目标节点发送命令：\n\ncluster setslot <i> importing <source_id>\n\n\n可以将目标节点clusterstate.importing_slots_from[i]的值设置为source_id所代表节点的clusternode结构。\n\n举个例子，如果客户端向节点7003发送以下命令：\n\n# 9dfb 是节点7002 的id \n127.0.0.1:7003> cluster setslot 16198 importing 9dfb4c4e016e627d9769e4c9bb0d4fa208e65c26\nok\n\n\n那么节点7003的clusterstate.importing_slots_from数组将变成图17-27所示的样子。\n\n\n\n图17-27　节点7003的importing_slots_from数组\n\n\n# cluster setslot migrating 命令的实现\n\nclusterstate结构的migrating_slots_to数组记录了当前节点正在迁移至其他节点的槽：\n\ntypedef struct clusterstate {\n   \n   clusternode *migrating_slots_to[16384];\n   \n} clusterstate;\n\n\n如果migrating_slots_to[i]的值不为null，而是指向一个 clusternode 结构，那么表示当前节点正在将槽i迁移至clusternode所代表的节点。\n\n在对集群进行重新分片的时候，向源节点发送命令：\n\ncluster setslot <i> migrating <target_id>\n\n\n可以将源节点clusterstate.migrating_slots_to[i]的值设置为target_id所代表节点的clusternode结构。\n\n举个例子，如果客户端向节点7002发送以下命令：\n\n# 0457\n是节点7003 \n的id \n127.0.0.1:7002> cluster setslot 16198 migrating 04579925484ce537d3410d7ce97bd2e260c459a2\nok\n\n\n那么节点7002的clusterstate.migrating_slots_to数组将变成图17-28所示的样子。\n\n图17-28　节点7002的migrating_slots_to数组\n\n\n# ask 错误\n\n如果节点收到一个关于键key的命令请求，并且键key所属的槽i正好就指派给了这个节点，那么节点会尝试在自己的数据库里查找键key，如果找到了的话，节点就直接执行客户端发送的命令。\n\n与此相反，如果节点没有在自己的数据库里找到键key，那么节点会检查自己的clusterstate.migrating_slots_to[i]，看键key所属的槽i是否正在进行迁移，如果槽i的确在进行迁移的话，那么节点会向客户端发送一个ask错误，引导客户端到正在导入槽i的节点去查找键key。\n\n举个例子，假设在节点7002向节点7003迁移槽16198期间，有一个客户端向节点7002发送命令：\n\nget \n“love”\n\n\n因为键"love"正好属于槽16198，所以节点7002会首先在自己的数据库中查找键"love"，但并没有找到，通过检查自己的clusterstate.migrating_slots_to[16198]，节点7002发现自己正在将槽16198迁移至节点7003，于是它向客户端返回错误：\n\nask 16198 127.0.0.1:7003\n\n\n这个错误表示客户端可以尝试到ip为127.0.0.1，端口号为7003的节点去执行和槽16198有关的操作，如图17-29所示。\n\n\n\n图17-29　客户端接收到节点7002返回的ask错误\n\n接到ask错误的客户端会根据错误提供的ip地址和端口号，转向至正在导入槽的目标节点，然后首先向目标节点发送一个asking命令，之后再重新发送原本想要执行的命令。\n\n以前面的例子来说，当客户端接收到节点7002返回的以下错误时：\n\nask 16198 127.0.0.1:7003\n\n\n客户端会转向至节点7003，首先发送命令：\n\nasking\n\n\n然后再次发送命令：\n\nget "love"\n\n\n并获得回复：\n\n"you get the key \'love\'"\n\n\n整个过程如图17-30所示。\n\n\n\n图17-30　客户端转向至节点7003\n\n\n# asking命令\n\nasking命令唯一要做的就是打开发送该命令的客户端的redis_asking标识，以下是该命令的伪代码实现：\n\ndef asking():\n    # \n打开标识\n    client.flags |= redis_asking\n    # \n向客户端返回ok \n回复\n    reply("ok")\n\n\n在一般情况下，如果客户端向节点发送一个关于槽i的命令，而槽i又没有指派给这个节点的话，那么节点将向客户端返回一个moved错误；但是，如果节点的clusterstate.importing_slots_from[i]显示节点正在导入槽i，并且发送命令的客户端带有redis_asking标识，那么节点将破例执行这个关于槽i的命令一次，图17-31展示了这个判断过程。\n\n\n\n图17-31　节点判断是否执行客户端命令的过程\n\n当客户端接收到ask错误并转向至正在导入槽的节点时，客户端会先向节点发送一个asking命令，然后才重新发送想要执行的命令，这是因为如果客户端不发送asking命令，而直接发送想要执行的命令的话，那么客户端发送的命令将被节点拒绝执行，并返回moved错误。\n\n举个例子，我们可以使用普通模式的redis-cli客户端，向正在导入槽16198的节点7003发送以下命令：\n\n$ ./redis-cli -p 7003\n127.0.0.1:7003> get "love"\n(error) moved 16198 127.0.0.1:7002\n\n\n虽然节点7003正在导入槽16198，但槽16198目前仍然是指派给了节点7002，所以节点7003会向客户端返回moved错误，指引客户端转向至节点7002。\n\n但是，如果我们在发送get命令之前，先向节点发送一个asking命令，那么这个get命令就会被节点7003执行：\n\n127.0.0.1:7003> asking\nok\n127.0.0.1:7003> get "love"\n"you get the key \'love\'"\n\n\n另外要注意的是，客户端的redis_asking标识是一个一次性标识，当节点执行了一个带有redis_asking标识的客户端发送的命令之后，客户端的redis_asking标识就会被移除。\n\n举个例子，如果我们在成功执行get命令之后，再次向节点7003发送get命令，那么第二次发送的get命令将执行失败，因为这时客户端的redis_asking标识已经被移除：\n\n127.0.0.1:7003> asking                 #\n打开redis_asking\n标识\nok\n127.0.0.1:7003> get "love"   #\n移除redis_asking\n标识\n"you get the key \'love\'"\n127.0.0.1:7003> get "love"   # redis_asking\n标识未打开，执行失败\n(error) moved 16198 127.0.0.1:7002\n\n\n\n# ask错误和moved错误的区别\n\nask错误和moved错误都会导致客户端转向，它们的区别在于：\n\n * moved错误代表槽的负责权已经从一个节点转移到了另一个节点：在客户端收到关于槽i的moved错误之后，客户端每次遇到关于槽i的命令请求时，都可以直接将命令请求发送至moved错误所指向的节点，因为该节点就是目前负责槽i的节点。\n\n * 与此相反，ask错误只是两个节点在迁移槽的过程中使用的一种临时措施：在客户端收到关于槽i的ask错误之后，客户端只会在接下来的一次命令请求中将关于槽i的命令请求发送至ask错误所指示的节点，但这种转向不会对客户端今后发送关于槽i的命令请求产生任何影响，客户端仍然会将关于槽i的命令请求发送至目前负责处理槽i的节点，除非ask错误再次出现。\n\n\n# 复制与故障转移\n\nredis集群中的节点分为主节点（master）和从节点（slave），其中主节点用于处理槽，而从节点则用于复制某个主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求。\n\n举个例子，对于包含7000、7001、7002、7003四个主节点的集群来说，我们可以将7004、7005两个节点添加到集群里面，并将这两个节点设定为节点7000的从节点，如图17-32所示（图中以双圆形表示主节点，单圆形表示从节点）。\n\n\n\n图17-32　设置节点7004和节点7005成为节点7000的从节点\n\n表17-1记录了集群各个节点的当前状态，以及它们正在做的工作。\n\n表17-1　集群各个节点的当前状态\n\n\n\n如果这时，节点7000进入下线状态，那么集群中仍在正常运作的几个主节点将在节点7000的两个从节点——节点7004和节点7005中选出一个节点作为新的主节点，这个新的主节点将接管原来节点7000负责处理的槽，并继续处理客户端发送的命令请求。\n\n例如，如果节点7004被选中为新的主节点，那么节点7004将接管原来由节点7000负责处理的槽0至槽5000，节点7005也会从原来的复制节点7000，改为复制节点7004，如图17-33所示（图中用虚线包围的节点为已下线节点）。\n\n\n\n图17-33　节点7004成为新的主节点\n\n表17-2记录了在对节点7000进行故障转移之后，集群各个节点的当前状态，以及它们正在做的工作。\n\n表17-2　集群各个节点的当前状态\n\n\n\n如果在故障转移完成之后，下线的节点7000重新上线，那么它将成为节点7004的从节点，如图17-34所示。\n\n\n\n图17-34　重新上线的节点7000成为节点7004的从节点\n\n表17-3展示了节点7000复制节点7004之后，集群中各个节点的状态。\n\n表17-3　集群各个节点的当前状态\n\n\n\n本节接下来的内容将介绍节点的复制方法，检测节点是否下线的方法，以及对下线主节点进行故障转移的方法。\n\n\n# 设置从节点\n\n向一个节点发送命令：\n\ncluster replicate <node_id>\n\n\n可以让接收命令的节点成为node_id所指定节点的从节点，并开始对主节点进行复制：\n\n * 接收到该命令的节点首先会在自己的clusterstate.nodes字典中找到node_id所对应节点的clusternode结构，并将自己的clusterstate.myself.slaveof指针指向这个结构，以此来记录这个节点正在复制的主节点：\n   \n   struct clusternode {\n   \n        //如果这是一个从节点，那么指向主节点\n        struct clusternode *slaveof;\n       \n      };\n   \n\n * 然后节点会修改自己在clusterstate.myself.flags中的属性，关闭原本的redis_node_master标识，打开redis_node_slave标识，表示这个节点已经由原来的主节点变成了从节点。\n\n * 最后，节点会调用复制代码，并根据clusterstate.myself.slaveof指向的clusternode结构所保存的ip地址和端口号，对主节点进行复制。因为节点的复制功能和单机redis服务器的复制功能使用了相同的代码，所以让从节点复制主节点相当于向从节点发送命令slaveof。\n\n图17-35展示了节点7004在复制节点7000时的clusterstate结构：\n\n * clusterstate.myself.flags属性的值为redis_node_slave，表示节点7004是一个从节点。\n\n * clusterstate.myself.slaveof指针指向代表节点7000的结构，表示节点7004正在复制的主节点为节点7000。\n\n\n\n图17-35　节点7004的clusterstate结构\n\n一个节点成为从节点，并开始复制某个主节点这一信息会通过消息发送给集群中的其他节点，最终集群中的所有节点都会知道某个从节点正在复制某个主节点。\n\n集群中的所有节点都会在代表主节点的clusternode结构的slaves属性和numslaves属性中记录正在复制这个主节点的从节点名单：\n\nstruct clusternode {\n    // \n    // \n正在复制这个主节点的从节点数量\n    int numslaves;\n    // \n一个数组\n    // \n每个数组项指向一个正在复制这个主节点的从节点的clusternode\n结构\n    struct clusternode **slaves;\n    // \n};\n\n\n举个例子，图17-36记录了节点7004和节点7005成为节点7000的从节点之后，集群中的各个节点为节点7000创建的clusternode结构的样子：\n\n * 代表节点7000的clusternode结构的numslaves属性的值为2，这说明有两个从节点正在复制节点7000。\n\n * 代表节点7000的clusternode结构的slaves数组的两个项分别指向代表节点7004和代表节点7005的clusternode结构，这说明节点7000的两个从节点分别是节点7004和节点7005。\n\n\n\n图17-36　集群中的各个节点为节点7000创建的clusternode结构\n\n\n# 故障检测\n\n集群中的每个节点都会定期地向集群中的其他节点发送ping消息，以此来检测对方是否在线，如果接收ping消息的节点没有在规定的时间内，向发送ping消息的节点返回pong消息，那么发送ping消息的节点就会将接收ping消息的节点标记为疑似下线（probable fail，pfail）。\n\n举个例子，如果节点7001向节点7000发送了一条ping消息，但是节点7000没有在规定的时间内，向节点7001返回一条pong消息，那么节点7001就会在自己的clusterstate.nodes字典中找到节点7000所对应的clusternode结构，并在结构的flags属性中打开redis_node_pfail标识，以此表示节点7000进入了疑似下线状态，如图17-37所示。\n\n\n\n图17-37　代表节点7000的clusternode结构\n\n集群中的各个节点会通过互相发送消息的方式来交换集群中各个节点的状态信息，例如某个节点是处于在线状态、疑似下线状态（pfail），还是已下线状态（fail）。\n\n当一个主节点a通过消息得知主节点b认为主节点c进入了疑似下线状态时，主节点a会在自己的clusterstate.nodes字典中找到主节点c所对应的clusternode结构，并将主节点b的下线报告（failure report）添加到clusternode结构的fail_reports链表里面：\n\nstruct clusternode {\n  // \n  // \n一个链表，记录了所有其他节点对该节点的下线报告\n  list *fail_reports;\n  // \n};\n\n\n每个下线报告由一个clusternodefailreport结构表示：\n\nstruct clusternodefailreport {\n  // \n报告目标节点已经下线的节点\n  struct clusternode *node;\n  // \n最后一次从node\n节点收到下线报告的时间\n  // \n程序使用这个时间戳来检查下线报告是否过期\n  // \n（与当前时间相差太久的下线报告会被删除）\n  mstime_t time;\n} typedef clusternodefailreport;\n\n\n举个例子，如果主节点7001在收到主节点7002、主节点7003发送的消息后得知，主节点7002和主节点7003都认为主节点7000进入了疑似下线状态，那么主节点7001将为主节点7000创建图17-38所示的下线报告。\n\n\n\n图17-38　节点7000的下线报告\n\n如果在一个集群里面，半数以上负责处理槽的主节点都将某个主节点x报告为疑似下线，那么这个主节点x将被标记为已下线（fail），将主节点x标记为已下线的节点会向集群广播一条关于主节点x的fail消息，所有收到这条fail消息的节点都会立即将主节点x标记为已下线。\n\n举个例子，对于图17-38所示的下线报告来说，主节点7002和主节点7003都认为主节点7000进入了下线状态，并且主节点7001也认为主节点7000进入了疑似下线状态（代表主节点7000的结构打开了redis_node_pfail标识），综合起来，在集群四个负责处理槽的主节点里面，有三个都将主节点7000标记为下线，数量已经超过了半数，所以主节点7001会将主节点7000标记为已下线，并向集群广播一条关于主节点7000的fail消息，如图17-39所示。\n\n\n\n图17-39　节点7001向集群广播fail消息\n\n\n# 故障转移\n\n当一个从节点发现自己正在复制的主节点进入了已下线状态时，从节点将开始对下线主节点进行故障转移，以下是故障转移的执行步骤：\n\n1）复制下线主节点的所有从节点里面，会有一个从节点被选中。\n\n2）被选中的从节点会执行slaveof no one命令，成为新的主节点。\n\n3）新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。\n\n4）新的主节点向集群广播一条pong消息，这条pong消息可以让集群中的其他节点立即知道这个节点已经由从节点变成了主节点，并且这个主节点已经接管了原本由已下线节点负责处理的槽。\n\n5）新的主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成。\n\n\n# 选举新的主节点\n\n新的主节点是通过选举产生的。\n\n以下是集群选举新的主节点的方法：\n\n1）集群的配置纪元是一个自增计数器，它的初始值为0。\n\n2）当集群里的某个节点开始一次故障转移操作时，集群配置纪元的值会被增一。\n\n3）对于每个配置纪元，集群里每个负责处理槽的主节点都有一次投票的机会，而第一个向主节点要求投票的从节点将获得主节点的投票。\n\n4）当从节点发现自己正在复制的主节点进入已下线状态时，从节点会向集群广播一条clustermsg_type_failover_auth_request消息，要求所有收到这条消息、并且具有投票权的主节点向这个从节点投票。\n\n5）如果一个主节点具有投票权（它正在负责处理槽），并且这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条clustermsg_type_failover_auth_ack消息，表示这个主节点支持从节点成为新的主节点。\n\n6）每个参与选举的从节点都会接收clustermsg_type_failover_auth_ack消息，并根据自己收到了多少条这种消息来统计自己获得了多少主节点的支持。\n\n7）如果集群里有n个具有投票权的主节点，那么当一个从节点收集到大于等于n/2+1张支持票时，这个从节点就会当选为新的主节点。\n\n8）因为在每一个配置纪元里面，每个具有投票权的主节点只能投一次票，所以如果有n个主节点进行投票，那么具有大于等于n/2+1张支持票的从节点只会有一个，这确保了新的主节点只会有一个。\n\n9）如果在一个配置纪元里面没有从节点能收集到足够多的支持票，那么集群进入一个新的配置纪元，并再次进行选举，直到选出新的主节点为止。\n\n这个选举新主节点的方法和第16章介绍的选举领头sentinel的方法非常相似，因为两者都是基于raft算法的领头选举（leader election）方法来实现的。\n\n\n# 消息\n\n集群中的各个节点通过发送和接收消息（message）来进行通信，我们称发送消息的节点为发送者（sender），接收消息的节点为接收者（receiver），如图17-40所示。\n\n\n\n图17-40　发送者和接收者\n\n节点发送的消息主要有以下五种：\n\n * meet消息：当发送者接到客户端发送的cluster meet命令时，发送者会向接收者发送meet消息，请求接收者加入到发送者当前所处的集群里面。\n\n * ping消息：集群里的每个节点默认每隔一秒钟就会从已知节点列表中随机选出五个节点，然后对这五个节点中最长时间没有发送过ping消息的节点发送ping消息，以此来检测被选中的节点是否在线。除此之外，如果节点a最后一次收到节点b发送的pong消息的时间，距离当前时间已经超过了节点a的cluster-node-timeout选项设置时长的一半，那么节点a也会向节点b发送ping消息，这可以防止节点a因为长时间没有随机选中节点b作为ping消息的发送对象而导致对节点b的信息更新滞后。\n\n * pong消息：当接收者收到发送者发来的meet消息或者ping消息时，为了向发送者确认这条meet消息或者ping消息已到达，接收者会向发送者返回一条pong消息。另外，一个节点也可以通过向集群广播自己的pong消息来让集群中的其他节点立即刷新关于这个节点的认识，例如当一次故障转移操作成功执行之后，新的主节点会向集群广播一条pong消息，以此来让集群中的其他节点立即知道这个节点已经变成了主节点，并且接管了已下线节点负责的槽。\n\n * fail消息：当一个主节点a判断另一个主节点b已经进入fail状态时，节点a会向集群广播一条关于节点b的fail消息，所有收到这条消息的节点都会立即将节点b标记为已下线。\n\n * publish消息：当节点接收到一个publish命令时，节点会执行这个命令，并向集群广播一条publish消息，所有接收到这条publish消息的节点都会执行相同的publish命令。\n\n一条消息由消息头（header）和消息正文（data）组成，接下来的内容将首先介绍消息头，然后再分别介绍上面提到的五种不同类型的消息正文。\n\n\n# 消息头\n\n节点发送的所有消息都由一个消息头包裹，消息头除了包含消息正文之外，还记录了消息发送者自身的一些信息，因为这些信息也会被消息接收者用到，所以严格来讲，我们可以认为消息头本身也是消息的一部分。\n\n每个消息头都由一个cluster.h/clustermsg结构表示：\n\ntypedef struct {\n  // 消息的长度（包括这个消息头的长度和消息正文的长度）\n  uint32_t totlen;\n  // 消息的类型\n  uint16_t type;\n  // 消息正文包含的节点信息数量\n  // 只在发送meet、ping、pong这三种gossip协议消息时使用\n  uint16_t count;\n  // 发送者所处的配置纪元\n  uint64_t currentepoch;\n // 如果发送者是一个主节点，那么这里记录的是发送者的配置纪元\n  // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的配置纪元\n  uint64_t configepoch;\n  // 发送者的名字（id\n） \n  char sender[redis_cluster_namelen];\n  // 发送者目前的槽指派信息\n  unsigned char myslots[redis_cluster_slots/8];\n  // 如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的名字\n  // 如果发送者是一个主节点，那么这里记录的是redis_node_null_name\n  // （一个40字节长，值全为0的字节数组）\n  char slaveof[redis_cluster_namelen];\n  // 发送者的端口号\n  uint16_t port;\n  // 发送者的标识值\n  uint16_t flags;\n  // 发送者所处集群的状态\n  unsigned char state;\n  // 消息的正文（或者说，内容）\n  union clustermsgdata data;\n} clustermsg;\n\n\nclustermsg.data属性指向联合cluster.h/clustermsgdata，这个联合就是消息的正文：\n\nunion clustermsgdata {\n  // meet、ping、pong消息的正文\n  struct {\n    // 每条meet、ping、pong消息都包含两个\n    // clustermsgdatagossip结构\n    clustermsgdatagossip gossip[1];\n  } ping;\n  // fail消息的正文\n  struct {\n    clustermsgdatafail about;\n  } fail;\n  // publish消息的正文\n  struct {\n    clustermsgdatapublish msg;\n  } publish;\n  // 其他消息的正文\n};\n\n\nclustermsg结构的currentepoch、sender、myslots等属性记录了发送者自身的节点信息，接收者会根据这些信息，在自己的clusterstate.nodes字典里找到发送者对应的clusternode结构，并对结构进行更新。\n\n举个例子，通过对比接收者为发送者记录的槽指派信息，以及发送者在消息头的myslots属性记录的槽指派信息，接收者可以知道发送者的槽指派信息是否发生了变化。\n\n又或者说，通过对比接收者为发送者记录的标识值，以及发送者在消息头的flags属性记录的标识值，接收者可以知道发送者的状态和角色是否发生了变化，例如节点状态由原来的在线变成了下线，或者由主节点变成了从节点等等。\n\n\n# meet、ping、pong消息的实现\n\nredis集群中的各个节点通过gossip协议来交换各自关于不同节点的状态信息，其中gossip协议由meet、ping、pong三种消息实现，这三种消息的正文都由两个cluster.h/clustermsgdatagossip结构组成：\n\nunion clustermsgdata {\n  // \n  // meet、ping和pong消息的正文\n  struct {\n    // 每条meet、ping、pong消息都包含两个\n    // clustermsgdatagossip结构\n    clustermsgdatagossip gossip[1];\n  } ping;\n  // 其他消息的正文 \n};\n\n\n\n因为meet、ping、pong三种消息都使用相同的消息正文，所以节点通过消息头的type属性来判断一条消息是meet消息、ping消息还是pong消息。\n\n每次发送meet、ping、pong消息时，发送者都从自己的已知节点列表中随机选出两个节点（可以是主节点或者从节点），并将这两个被选中节点的信息分别保存到两个clustermsgdatagossip结构里面。\n\nclustermsgdatagossip结构记录了被选中节点的名字，发送者与被选中节点最后一次发送和接收ping消息和pong消息的时间戳，被选中节点的ip地址和端口号，以及被选中节点的标识值：\n\ntypedef struct {\n  // 节点的名字\n  char nodename[redis_cluster_namelen];\n  // 最后一次向该节点发送ping消息的时间戳\n  uint32_t ping_sent;\n  // 最后一次从该节点接收到pong消息的时间戳\n  uint32_t pong_received;\n  // 节点的ip地址\n  char ip[16];\n  // 节点的端口号\n  uint16_t port;\n  // 节点的标识值\n  uint16_t flags;\n} clustermsgdatagossip;\n\n\n当接收者收到meet、ping、pong消息时，接收者会访问消息正文中的两个clustermsgdatagossip结构，并根据自己是否认识clustermsgdatagossip结构中记录的被选中节点来选择进行哪种操作：\n\n * 如果被选中节点不存在于接收者的已知节点列表，那么说明接收者是第一次接触到被选中节点，接收者将根据结构中记录的ip地址和端口号等信息，与被选中节点进行握手。\n\n * 如果被选中节点已经存在于接收者的已知节点列表，那么说明接收者之前已经与被选中节点进行过接触，接收者将根据clustermsgdatagossip结构记录的信息，对被选中节点所对应的clusternode结构进行更新。\n\n举个发送ping消息和返回pong消息的例子，假设在一个包含a、b、c、d、e、f六个节点的集群里：\n\n * 节点a向节点d发送ping消息，并且消息里面包含了节点b和节点c的信息，当节点d收到这条ping消息时，它将更新自己对节点b和节点c的认识。\n\n * 之后，节点d将向节点a返回一条pong消息，并且消息里面包含了节点e和节点f的消息，当节点a收到这条pong消息时，它将更新自己对节点e和节点f的认识。\n\n整个通信过程如图17-41所示。\n\n\n\n图17-41　一个ping-pong消息通信示例\n\n\n# fail消息的实现\n\n当集群里的主节点a将主节点b标记为已下线（fail）时，主节点a将向集群广播一条关于主节点b的fail消息，所有接收到这条fail消息的节点都会将主节点b标记为已下线。\n\n在集群的节点数量比较大的情况下，单纯使用gossip协议来传播节点的已下线信息会给节点的信息更新带来一定延迟，因为gossip协议消息通常需要一段时间才能传播至整个集群，而发送fail消息可以让集群里的所有节点立即知道某个主节点已下线，从而尽快判断是否需要将集群标记为下线，又或者对下线主节点进行故障转移。\n\nfail消息的正文由cluster.h/clustermsgdatafail结构表示，这个结构只包含一个nodename属性，该属性记录了已下线节点的名字：\n\ntypedef struct {\n    char nodename[redis_cluster_namelen];\n} clustermsgdatafail;\n\n\n因为集群里的所有节点都有一个独一无二的名字，所以fail消息里面只需要保存下线节点的名字，接收到消息的节点就可以根据这个名字来判断是哪个节点下线了。\n\n举个例子，对于包含7000、7001、7002、7003四个主节点的集群来说：\n\n * 如果主节点7001发现主节点7000已下线，那么主节点7001将向主节点7002和主节点7003发送fail消息，其中fail消息中包含的节点名字为主节点7000的名字，以此来表示主节点7000已下线。\n\n * 当主节点7002和主节点7003都接收到主节点7001发送的fail消息时，它们也会将主节点7000标记为已下线。\n\n * 因为这时集群已经有超过一半的主节点认为主节点7000已下线，所以集群剩下的几个主节点可以判断是否需要将集群标记为下线，又或者开始对主节点7000进行故障转移。\n\n图17-42至图17-44展示了节点发送和接收fail消息的整个过程。\n\n\n\n图17-42　节点7001将节点7000标记为已下线\n\n\n\n图17-43　节点7001向集群广播fail消息\n\n\n\n图17-44　节点7002和节点7003也将节点7000标记为已下线\n\n\n# publish消息的实现\n\n当客户端向集群中的某个节点发送命令\n\npublish <channel> <message>\n\n\n的时候，接收到publish命令的节点不仅会向channel频道发送消息message，它还会向集群广播一条publish消息，所有接收到这条publish消息的节点都会向channel频道发送message消息。\n\n换句话说，向集群中的某个节点发送命令：\n\npublish <channel> <message>\n\n\n将导致集群中的所有节点都向channel频道发送message消息。\n\n举个例子，对于包含7000、7001、7002、7003四个节点的集群来说，如果节点7000收到了客户端发送的publish命令，那么节点7000将向7001、7002、7003三个节点发送publish消息，如图17-45所示。\n\n\n\n图17-45　接收到publish命令的节点7000向集群广播publish消息\n\npublish消息的正文由cluster.h/clustermsgdatapublish结构表示：\n\ntypedef struct {\n  uint32_t channel_len;\n  uint32_t message_len;\n  // 定义为8 字节只是为了对齐其他消息结构\n  // 实际的长度由保存的内容决定\n  unsigned char bulk_data[8];\n} clustermsgdatapublish;\n\n\nclustermsgdatapublish结构的bulk_data属性是一个字节数组，这个字节数组保存了客户端通过publish命令发送给节点的channel参数和message参数，而结构的channel_len和message_len则分别保存了channel参数的长度和message参数的长度：\n\n * 其中bulk_data的0字节至channel_len-1字节保存的是channel参数。\n\n * 而bulk_data的channel_len字节至channel_len+message_len-1字节保存的则是message参数。\n\n举个例子，如果节点收到的publish命令为：\n\npublish "news.it" "hello"\n\n\n那么节点发送的publish消息的clustermsgdatapublish结构将如图17-46所示：其中bulk_data数组的前七个字节保存了channel参数的值"news.it"，而bulk_data数组的后五个字节则保存了message参数的值"hello"。\n\n\n\n图17-46　clustermsgdatapublish结构示例\n\n> 为什么不直接向节点广播publish命令\n> \n> 实际上，要让集群的所有节点都执行相同的publish命令，最简单的方法就是向所有节点广播相同的publish命令，这也是redis在复制publish命令时所使用的方法，不过因为这种做法并不符合redis集群的“各个节点通过发送和接收消息来进行通信”这一规则，所以节点没有采取广播publish命令的做法。\n\n\n# 重点回顾\n\n * 节点通过握手来将其他节点添加到自己所处的集群当中。\n * 集群中的 16384 个槽可以分别指派给集群中的各个节点， 每个节点都会记录哪些槽指派给了自己， 而哪些槽又被指派给了其他节点。\n * 节点在接到一个命令请求时， 会先检查这个命令请求要处理的键所在的槽是否由自己负责， 如果不是的话， 节点将向客户端返回一个 moved 错误， moved 错误携带的信息可以指引客户端转向至正在负责相关槽的节点。\n * 对 redis 集群的重新分片工作是由客户端执行的， 重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另一个节点。\n * 如果节点 a 正在迁移槽 i 至节点 b ， 那么当节点 a 没能在自己的数据库中找到命令指定的数据库键时， 节点 a 会向客户端返回一个 ask 错误， 指引客户端到节点 b 继续查找指定的数据库键。\n * moved 错误表示槽的负责权已经从一个节点转移到了另一个节点， 而 ask 错误只是两个节点在迁移槽的过程中使用的一种临时措施。\n * 集群里的从节点用于复制主节点， 并在主节点下线时， 代替主节点继续处理命令请求。\n * 集群中的节点通过发送和接收消息来进行通讯， 常见的消息包括 meet 、 ping 、 pong 、 publish 、 fail 五种。\n\n\n# 参考文献\n\n * 极客时间：redis源码剖析与实战\n\n * redis设计与实现\n\n * github：redis 源码',charsets:{cjk:!0},lastUpdated:"2024/09/17, 17:02:40",lastUpdatedTimestamp:172659256e4},{title:"首页",frontmatter:{home:!0,title:"首页",heroImage:"https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409152149836.png",heroText:"Echo 系统设计之美",tagline:"🚀水滴石穿，设计无银弹",actionText:"开始使用 →",actionLink:"/pages/fccd91/",bannerBg:"none",features:[{title:"热门算法",details:"深入了解系统设计相关的热门算法"},{title:"赏析经典",details:"赏析 Redis、Kafka、Nginx等系统的优雅设计"},{title:"实战系统设计",details:"针对基础设施、热门APP，以及热门场景的实战设计"}],postList:"none"},regularPath:"/",relativePath:"index.md",key:"v-a11d1cc4",path:"/",headers:[{level:2,title:"⚡ 反馈与交流",slug:"⚡-反馈与交流",normalizedTitle:"⚡ 反馈与交流",charIndex:2}],headersStr:"⚡ 反馈与交流",content:'# ⚡ 反馈与交流\n\n在使用过程中有任何问题和想法，请给我提 Issue。 你也可以在 Issue 查看别人提的问题和给出解决方案。\n\n或者加入我们的交流群：参与贡献可以榜上留名 💯\n\n「Echo 系统设计之美」微信群(添加我微信备注"进群系统设计")',normalizedContent:'# ⚡ 反馈与交流\n\n在使用过程中有任何问题和想法，请给我提 issue。 你也可以在 issue 查看别人提的问题和给出解决方案。\n\n或者加入我们的交流群：参与贡献可以榜上留名 💯\n\n「echo 系统设计之美」微信群(添加我微信备注"进群系统设计")',charsets:{cjk:!0},lastUpdated:"2024/09/17, 05:13:55",lastUpdatedTimestamp:1726550035e3}],themeConfig:{blogInfo:{pageView:!1,readingTime:!0,eachFileWords:[{name:"Bloom Filter",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/01.Bloom Filter.md",wordsCount:"1.7k",readingTime:"6.2m",title:"Bloom Filter",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/fccd91/"},{name:"Consistent Hashing",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/02.Consistent Hashing.md",wordsCount:"2.8k",readingTime:"9.9m",title:"Consistent Hashing",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/1e28a2/"},{name:"Count-Min Sketch",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/03.Count-Min Sketch.md",wordsCount:"2.4k",readingTime:"9.3m",title:"Count-Min Sketch",date:"2024-09-14T13:30:19.000Z",permalink:"/pages/8624c5"},{name:"LRU",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/04.LRU.md",wordsCount:"1.5k",readingTime:"5.8m",title:"LRU",date:"2024-09-14T16:39:57.000Z",permalink:"/pages/87589a"},{name:"LFU",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/05.LFU.md",wordsCount:24,readingTime:"1",title:"LFU",date:"2024-09-14T18:14:42.000Z",permalink:"/pages/7d22be/"},{name:"hash & rehash",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/06.hash & rehash.md",wordsCount:19,readingTime:"1",title:"hash & rehash",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d43d1/"},{name:"Timing Wheels",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/01.热门算法/01.热门算法/10.Timing Wheels.md",wordsCount:"6.9k",readingTime:"25.8m",title:"Timing Wheels",date:"2024-09-15T02:25:42.000Z",permalink:"/pages/44dcc2/"},{name:"介绍",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/01.Kafka 系统设计/01.介绍.md",wordsCount:13,readingTime:"1",title:"介绍",date:"2024-09-15T21:10:46.000Z",permalink:"/pages/b9733b/"},{name:"指南",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/02.Kafka  系统设计/01.前言/01.指南.md",wordsCount:140,readingTime:"1",title:"指南",date:"2024-09-17T16:51:40.000Z",permalink:"/pages/e21d7f/"},{name:"介绍",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/03.Nginx 系统设计/01.Nginx 系统设计/01.介绍.md",wordsCount:13,readingTime:"1",title:"介绍",date:"2024-09-15T21:10:47.000Z",permalink:"/pages/4601ca/"},{name:"碎碎念",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/06.动态/01.碎碎念.md",wordsCount:353,readingTime:"1.3m",title:"碎碎念",date:"2024-09-15T01:18:31.000Z",permalink:"/pages/52ebd8/"},{name:"分布式缓存",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/20.设计基础设施/01.设计基础设施/01.分布式缓存.md",wordsCount:418,readingTime:"1.7m",title:"分布式缓存",date:"2024-09-14T02:09:39.000Z",permalink:"/pages/84cb49/"},{name:"限流器",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/20.设计基础设施/01.设计基础设施/02.限流器.md",wordsCount:559,readingTime:"2.1m",title:"限流器",date:"2024-09-14T02:09:39.000Z",permalink:"/pages/57d5a5/"},{name:"热点探查（Top k）",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/20.设计基础设施/01.设计基础设施/03.热点探查（Top k）.md",wordsCount:"3.4k",readingTime:"12.3m",title:"热点探查（Top k）",date:"2024-09-14T03:52:03.000Z",permalink:"/pages/5dcb6b/"},{name:"消息队列",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/20.设计基础设施/01.设计基础设施/04.消息队列.md",wordsCount:197,readingTime:"1m",title:"消息队列",date:"2024-09-14T16:43:00.000Z",permalink:"/pages/567090/"},{name:"订阅发布",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/20.设计基础设施/01.设计基础设施/05.订阅发布.md",wordsCount:67,readingTime:"1",title:"服务通知",date:"2024-09-14T16:43:28.000Z",permalink:"/pages/8416e6/"},{name:"动态线程池",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/20.设计基础设施/01.设计基础设施/06.动态线程池.md",wordsCount:16,readingTime:"1",title:"动态线程池",date:"2024-09-14T23:28:32.000Z",permalink:"/pages/d81a42/"},{name:"设计 微信",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/25.设计热门应用/01.设计热门应用/01.设计 微信.md",wordsCount:15,readingTime:"1",title:"设计 微信",date:"2024-09-14T02:08:51.000Z",permalink:"/pages/a95d7d/"},{name:"设计Twitter",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/25.设计热门应用/01.设计热门应用/02.设计Twitter.md",wordsCount:14,readingTime:"1",title:"设计Twitter",date:"2024-09-14T02:08:51.000Z",permalink:"/pages/90ad66/"},{name:"双写一致性",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/01.双写一致性.md",wordsCount:"5.6k",readingTime:"19.3m",title:"双写一致性",date:"2024-09-14T16:50:17.000Z",permalink:"/pages/def08a/"},{name:"缓存穿透",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/02.缓存穿透.md",wordsCount:"1.2k",readingTime:"4.2m",title:"缓存穿透",date:"2024-09-14T16:50:37.000Z",permalink:"/pages/1e9e8e/"},{name:"缓存击穿",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/03.缓存击穿.md",wordsCount:560,readingTime:"2m",title:"缓存击穿",date:"2024-09-14T16:50:56.000Z",permalink:"/pages/1d96b2/"},{name:"任务补偿",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/04.任务补偿.md",wordsCount:"3.3k",readingTime:"11.8m",title:"任务补偿",date:"2024-09-14T16:51:13.000Z",permalink:"/pages/24abe0/"},{name:"秒杀",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/05.秒杀.md",wordsCount:"4.6k",readingTime:"15.5m",title:"秒杀",date:"2024-09-14T16:51:28.000Z",permalink:"/pages/a72629/"},{name:"超卖",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/06.超卖.md",wordsCount:"1.8k",readingTime:"7m",title:"超卖",date:"2024-09-14T22:14:25.000Z",permalink:"/pages/8a57f2/"},{name:"多级缓存",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/07.多级缓存.md",wordsCount:15,readingTime:"1",title:"多级缓存",date:"2024-09-14T16:52:24.000Z",permalink:"/pages/51aa8b/"},{name:"超时&重试",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/08.超时&重试.md",wordsCount:"4.8k",readingTime:"16.9m",title:"超时&重试",date:"2024-09-14T16:52:35.000Z",permalink:"/pages/0dfb49/"},{name:"幂等&防重",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/09.幂等&防重.md",wordsCount:"3.5k",readingTime:"13m",title:"幂等&防重",date:"2024-09-14T16:52:57.000Z",permalink:"/pages/4fc8cb/"},{name:"海量数据计数",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/10.海量数据计数.md",wordsCount:"3.3k",readingTime:"11.5m",title:"海量数据计数",date:"2024-09-14T16:52:01.000Z",permalink:"/pages/f3295f/"},{name:"消息未读数系统",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/30.经典场景设计/01.经典场景设计/11.消息未读数系统.md",wordsCount:"3k",readingTime:"10.5m",title:"消息未读数系统",date:"2024-09-14T23:57:17.000Z",permalink:"/pages/6b9d68/"},{name:"指南",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/01.前言/01.指南.md",wordsCount:"1.1k",readingTime:"4.4m",title:"指南",date:"2024-09-15T17:31:05.000Z",permalink:"/pages/252196/"},{name:"Redis 伪码蓝图【必看】",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/01.前言/05.Redis 伪码蓝图【必看】.md",wordsCount:"1.1k",readingTime:"4.5m",title:"Redis 伪码蓝图【必看】",date:"2024-09-16T01:33:42.000Z",permalink:"/pages/69fbd7/"},{name:"String 设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/02.基础/01.String 设计与实现.md",wordsCount:"5.2k",readingTime:"19.2m",title:"String 设计与实现",date:"2024-09-16T02:46:18.000Z",permalink:"/pages/bdae41/"},{name:"List 设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/02.基础/02.List 设计与实现.md",wordsCount:"9.5k",readingTime:"35.9m",title:"List 设计与实现",date:"2024-09-16T02:46:18.000Z",permalink:"/pages/bd1e41/"},{name:"Hash 设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/02.基础/05.Hash 设计与实现.md",wordsCount:"4.7k",readingTime:"18.9m",title:"Hash 设计与实现",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d4311/"},{name:"ZSet 设计与实现",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/02.基础/10.ZSet 设计与实现.md",wordsCount:"5.6k",readingTime:"20.8m",title:"ZSet 设计与实现",date:"2024-09-14T18:44:06.000Z",permalink:"/pages/2d4312/"},{name:"Linux 中的 IO 多路复用",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.主线/01.Linux 中的 IO 多路复用.md",wordsCount:"5.8k",readingTime:"21.1m",title:"Linux 中的 IO 多路复用",date:"2024-09-15T17:16:08.000Z",permalink:"/pages/34fa27"},{name:"Redis Server 初始化",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.主线/03.Redis Server 初始化.md",wordsCount:"5.3k",readingTime:"19.9m",title:"Redis Server 初始化",date:"2024-09-16T03:04:10.000Z",permalink:"/pages/d4ecb9/"},{name:"Redis 的 Reactor 模型",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.主线/05.Redis 的 Reactor 模型.md",wordsCount:"4.9k",readingTime:"18.1m",title:"Redis 的 Reactor 模型",date:"2024-09-15T17:26:35.000Z",permalink:"/pages/d6b00d"},{name:"深入 Redis 事件驱动框架",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.主线/08.深入 Redis 事件驱动框架.md",wordsCount:"9.3k",readingTime:"34.6m",title:"深入 Redis 事件驱动框架",date:"2024-09-15T01:36:53.000Z",permalink:"/pages/264b06/"},{name:"Redis 的执行模式",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.主线/09.Redis 的执行模式.md",wordsCount:"4k",readingTime:"15.1m",title:"Redis 的执行模式",date:"2024-09-15T20:23:53.000Z",permalink:"/pages/e6d8ef/"},{name:"Redis 多IO线程",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/03.主线/16.Redis 多IO线程.md",wordsCount:"9.1k",readingTime:"34.4m",title:"Redis 多IO线程",date:"2024-09-17T20:15:03.000Z",permalink:"/pages/0850b6/"},{name:"LRU 策略",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.支线/05.LRU 策略.md",wordsCount:"6.5k",readingTime:"25.9m",title:"LRU 策略",date:"2024-09-15T23:59:53.000Z",permalink:"/pages/b43a19/"},{name:"LFU 策略",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.支线/10.LFU 策略.md",wordsCount:"5.3k",readingTime:"20.3m",title:"LFU 策略",date:"2024-09-15T23:59:53.000Z",permalink:"/pages/b43a89/"},{name:"Redis 过期策略",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.支线/13.Redis 过期策略.md",wordsCount:"2.1k",readingTime:"8.6m",title:"Redis 过期策略",date:"2024-09-16T03:23:25.000Z",permalink:"/pages/f44fbe/"},{name:"RDB 持久化",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.支线/15.RDB 持久化.md",wordsCount:"6.2k",readingTime:"23.8m",title:"RDB 持久化",date:"2024-09-16T03:23:41.000Z",permalink:"/pages/9b17a6/"},{name:"AOF 持久化",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.支线/17.AOF 持久化.md",wordsCount:"10.3k",readingTime:"38.3m",title:"AOF 持久化",date:"2024-09-16T03:23:41.000Z",permalink:"/pages/9b17a7/"},{name:"Redis 中的延迟监控",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.支线/20.Redis 中的延迟监控.md",wordsCount:"4.8k",readingTime:"17.7m",title:"Redis 中的延迟监控",date:"2024-09-15T23:27:13.000Z",permalink:"/pages/aa75e9/"},{name:"发布与订阅",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/04.支线/25.发布与订阅.md",wordsCount:16,readingTime:"1",title:"发布与订阅",date:"2024-09-18T01:00:52.000Z",permalink:"/pages/61d908/"},{name:"主从复制",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/05.集群/25.主从复制.md",wordsCount:"5.3k",readingTime:"19.3m",title:"主从复制",date:"2024-09-16T03:24:06.000Z",permalink:"/pages/ebc8dc/"},{name:"哨兵",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/05.集群/30.哨兵.md",wordsCount:13,readingTime:"1",title:"哨兵",date:"2024-09-16T03:24:20.000Z",permalink:"/pages/af8752/"},{name:"cluster",filePath:"/home/runner/work/echo-system-design/echo-system-design/docs/Redis 系统设计/05.集群/35.cluster.md",wordsCount:"18.5k",readingTime:"1h10m",title:"cluster",date:"2024-09-16T03:24:30.000Z",permalink:"/pages/040403/"}]},pageButton:!1,nav:[{text:"🏠首页",link:"/"},{text:"✒️热门算法",link:"/pages/fccd91/"},{text:"🎖️赏析经典",items:[{text:"Redis 系统设计",link:"/pages/252196/"},{text:"Kafka 系统设计",link:"/pages/b9733b/"},{text:"Nginx 系统设计",link:"/pages/4601ca/"}]},{text:"🧑‍💻实战系统设计",items:[{text:"设计基础设施",link:"/pages/84cb49/"},{text:"设计热门应用",link:"/pages/a95d7d/"},{text:"场景设计",link:"/pages/def08a/"}]},{text:"❓问答",link:"/pages/92b2ee/"},{text:"👀动态",link:"/pages/52ebd8/"}],logo:"/img/logo.png",repo:"echo-lxy/echo-system-design",searchMaxSuggestions:10,lastUpdated:"上次更新于",docsDir:"docs",editLinks:!0,editLinkText:"编辑",sidebar:{"/01.热门算法/":[{title:"热门算法",collapsable:!0,children:[["01.热门算法/01.Bloom Filter.md","Bloom Filter","/pages/fccd91/"],["01.热门算法/02.Consistent Hashing.md","Consistent Hashing","/pages/1e28a2/"],["01.热门算法/03.Count-Min Sketch.md","Count-Min Sketch","/pages/8624c5"],["01.热门算法/04.LRU.md","LRU","/pages/87589a"],["01.热门算法/05.LFU.md","LFU","/pages/7d22be/"],["01.热门算法/06.hash & rehash.md","hash & rehash","/pages/2d43d1/"],["01.热门算法/10.Timing Wheels.md","Timing Wheels","/pages/44dcc2/"]]}],catalogue:{},"/02.Kafka  系统设计/":[{title:"前言",collapsable:!0,children:[["01.前言/01.指南.md","指南","/pages/e21d7f/"]]}],"/03.Nginx 系统设计/":[{title:"Nginx 系统设计",collapsable:!0,children:[["01.Nginx 系统设计/01.介绍.md","介绍","/pages/4601ca/"]]}],"/06.动态/":[["01.碎碎念.md","碎碎念","/pages/52ebd8/"]],"/20.设计基础设施/":[{title:"设计基础设施",collapsable:!0,children:[["01.设计基础设施/01.分布式缓存.md","分布式缓存","/pages/84cb49/"],["01.设计基础设施/02.限流器.md","限流器","/pages/57d5a5/"],["01.设计基础设施/03.热点探查（Top k）.md","热点探查（Top k）","/pages/5dcb6b/"],["01.设计基础设施/04.消息队列.md","消息队列","/pages/567090/"],["01.设计基础设施/05.订阅发布.md","服务通知","/pages/8416e6/"],["01.设计基础设施/06.动态线程池.md","动态线程池","/pages/d81a42/"]]}],"/25.设计热门应用/":[{title:"设计热门应用",collapsable:!0,children:[["01.设计热门应用/01.设计 微信.md","设计 微信","/pages/a95d7d/"],["01.设计热门应用/02.设计Twitter.md","设计Twitter","/pages/90ad66/"]]}],"/30.经典场景设计/":[{title:"经典场景设计",collapsable:!0,children:[["01.经典场景设计/01.双写一致性.md","双写一致性","/pages/def08a/"],["01.经典场景设计/02.缓存穿透.md","缓存穿透","/pages/1e9e8e/"],["01.经典场景设计/03.缓存击穿.md","缓存击穿","/pages/1d96b2/"],["01.经典场景设计/04.任务补偿.md","任务补偿","/pages/24abe0/"],["01.经典场景设计/05.秒杀.md","秒杀","/pages/a72629/"],["01.经典场景设计/06.超卖.md","超卖","/pages/8a57f2/"],["01.经典场景设计/07.多级缓存.md","多级缓存","/pages/51aa8b/"],["01.经典场景设计/08.超时&重试.md","超时&重试","/pages/0dfb49/"],["01.经典场景设计/09.幂等&防重.md","幂等&防重","/pages/4fc8cb/"],["01.经典场景设计/10.海量数据计数.md","海量数据计数","/pages/f3295f/"],["01.经典场景设计/11.消息未读数系统.md","消息未读数系统","/pages/6b9d68/"]]}],"/Redis 系统设计/":[{title:"前言",collapsable:!0,children:[["01.前言/01.指南.md","指南","/pages/252196/"],["01.前言/05.Redis 伪码蓝图【必看】.md","Redis 伪码蓝图【必看】","/pages/69fbd7/"]]},{title:"基础",collapsable:!0,children:[["02.基础/01.String 设计与实现.md","String 设计与实现","/pages/bdae41/"],["02.基础/02.List 设计与实现.md","List 设计与实现","/pages/bd1e41/"],["02.基础/05.Hash 设计与实现.md","Hash 设计与实现","/pages/2d4311/"],["02.基础/10.ZSet 设计与实现.md","ZSet 设计与实现","/pages/2d4312/"]]},{title:"主线",collapsable:!0,children:[["03.主线/01.Linux 中的 IO 多路复用.md","Linux 中的 IO 多路复用","/pages/34fa27"],["03.主线/03.Redis Server 初始化.md","Redis Server 初始化","/pages/d4ecb9/"],["03.主线/05.Redis 的 Reactor 模型.md","Redis 的 Reactor 模型","/pages/d6b00d"],["03.主线/08.深入 Redis 事件驱动框架.md","深入 Redis 事件驱动框架","/pages/264b06/"],["03.主线/09.Redis 的执行模式.md","Redis 的执行模式","/pages/e6d8ef/"],["03.主线/16.Redis 多IO线程.md","Redis 多IO线程","/pages/0850b6/"]]},{title:"支线",collapsable:!0,children:[["04.支线/05.LRU 策略.md","LRU 策略","/pages/b43a19/"],["04.支线/10.LFU 策略.md","LFU 策略","/pages/b43a89/"],["04.支线/13.Redis 过期策略.md","Redis 过期策略","/pages/f44fbe/"],["04.支线/15.RDB 持久化.md","RDB 持久化","/pages/9b17a6/"],["04.支线/17.AOF 持久化.md","AOF 持久化","/pages/9b17a7/"],["04.支线/20.Redis 中的延迟监控.md","Redis 中的延迟监控","/pages/aa75e9/"],["04.支线/25.发布与订阅.md","发布与订阅","/pages/61d908/"]]},{title:"集群",collapsable:!0,children:[["05.集群/25.主从复制.md","主从复制","/pages/ebc8dc/"],["05.集群/30.哨兵.md","哨兵","/pages/af8752/"],["05.集群/35.cluster.md","cluster","/pages/040403/"]]}]},updateBar:{showToArticle:!1},pageStyle:"line",category:!1,tag:!1,author:{name:"echo",href:"https://gitee.com/brother-one"},social:{icons:[{iconClass:"icon-youjian",title:"发邮件",link:"mailto:lixinyang2002@163.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/echo-lxy"},{iconClass:"icon-erji",title:"听音乐",link:"https://music.163.com/#/playlist?id=755597173"}]},footer:{createYear:2024,copyrightInfo:"Xinyang Li | MIT License"},htmlModules:{pageT:'\n    <div class="wwads-cn wwads-horizontal page-wwads" data-id="136"></div>\n    <style>\n      .page-wwads{\n        width:100%!important;\n        min-height: 0;\n        margin: 0;\n      }\n      .page-wwads .wwads-img img{\n        width:80px!important;\n      }\n      .page-wwads .wwads-poweredby{\n        width: 40px;\n        position: absolute;\n        right: 25px;\n        bottom: 3px;\n      }\n      .wwads-content .wwads-text, .page-wwads .wwads-text{\n        height: 100%;\n        padding-top: 5px;\n        display: block;\n      }\n  </style>\n  '}}};var Al=t(95),Tl=t(96),Ol=t(11);var zl={computed:{$filterPosts(){return this.$site.pages.filter(n=>{const{frontmatter:{pageComponent:e,article:t,home:r}}=n;return!(e||!1===t||!0===r)})},$sortPosts(){return(n=this.$filterPosts).sort((n,e)=>{const t=n.frontmatter.sticky,r=e.frontmatter.sticky;return t&&r?t==r?Object(Ol.a)(n,e):t-r:t&&!r?-1:!t&&r?1:Object(Ol.a)(n,e)}),n;var n},$sortPostsByDate(){return(n=this.$filterPosts).sort((n,e)=>Object(Ol.a)(n,e)),n;var n},$groupPosts(){return function(n){const e={},t={};for(let r=0,i=n.length;r<i;r++){const{frontmatter:{categories:i,tags:s}}=n[r];"array"===Object(Ol.n)(i)&&i.forEach(t=>{t&&(e[t]||(e[t]=[]),e[t].push(n[r]))}),"array"===Object(Ol.n)(s)&&s.forEach(e=>{e&&(t[e]||(t[e]=[]),t[e].push(n[r]))})}return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags(){return function(n){const e=[],t=[];for(let t in n.categories)e.push({key:t,length:n.categories[t].length});for(let e in n.tags)t.push({key:e,length:n.tags[e].length});return{categories:e,tags:t}}(this.$groupPosts)}}};Vt.component(Al.default),Vt.component(Tl.default);function Ll(n){return n.toString().padStart(2,"0")}t(245);Vt.component("PageInfo",()=>t.e(7).then(t.bind(null,344))),Vt.component("WebInfo",()=>Promise.all([t.e(0),t.e(4)]).then(t.bind(null,342))),Vt.component("Badge",()=>Promise.all([t.e(0),t.e(5)]).then(t.bind(null,399))),Vt.component("CodeBlock",()=>Promise.resolve().then(t.bind(null,95))),Vt.component("CodeGroup",()=>Promise.resolve().then(t.bind(null,96)));t(246);var Cl=[({Vue:n,options:e,router:t,siteData:r,isServer:i})=>{i||t.afterEach(()=>{var n;n=function(){setTimeout((function(){void 0===window._AdBlockInit&&function(){const n=document.getElementsByClassName("wwads-cn"),e=document.querySelector(".wwads-content");n[0]&&!e&&(n[0].innerHTML=h)}()}),3e3)},"complete"===document.readyState||"interactive"===document.readyState?setTimeout(n,1):document.addEventListener("DOMContentLoaded",n),setTimeout(()=>{const n=document.querySelector(".page-wwads");if(!n)return;const e=n.querySelector(".wwads-hide");e&&(e.onclick=()=>{n.style.display="none"}),"none"===n.style.display&&(n.style.display="flex")},900)})},({Vue:n,options:e,router:t,siteData:r})=>{r.pages.map(n=>{const{frontmatter:{date:e,author:t}}=n;"string"==typeof e&&"Z"===e.charAt(e.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return`${n.getUTCFullYear()}-${Ll(n.getUTCMonth()+1)}-${Ll(n.getUTCDate())} ${Ll(n.getUTCHours())}:${Ll(n.getUTCMinutes())}:${Ll(n.getUTCSeconds())}`}(e)),t?n.author=t:r.themeConfig.author&&(n.author=r.themeConfig.author)}),n.mixin(zl)},{},({Vue:n})=>{n.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},({router:n})=>{"undefined"!=typeof window&&(window._hmt=window._hmt||[],function(){var n=document.createElement("script");n.src="https://hm.baidu.com/hm.js?01293bffa6c3962016c08ba685c79d78";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(n,e)}(),n.afterEach((function(n){_hmt.push(["_trackPageview",n.fullPath])})))}],Bl=["PageInfo"];class Pl extends class{constructor(){this.store=new Vt({data:{state:{}}})}$get(n){return this.store.state[n]}$set(n,e){Vt.set(this.store.state,n,e)}$emit(...n){this.store.$emit(...n)}$on(...n){this.store.$on(...n)}}{}Object.assign(Pl.prototype,{getPageAsyncComponent:oo,getLayoutAsyncComponent:lo,getAsyncComponent:co,getVueComponent:uo});var Dl={install(n){const e=new Pl;n.$vuepress=e,n.prototype.$vuepress=e}};function jl(n,e){const t=e.toLowerCase();return n.options.routes.some(n=>n.path.toLowerCase()===t)}var Nl={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(n){const e=this.pageKey||this.$parent.$page.key;return fo("pageKey",e),Vt.component(e)||Vt.component(e,oo(e)),Vt.component(e)?n(e):n("")}},Fl={functional:!0,props:{slotKey:String,required:!0},render:(n,{props:e,slots:t})=>n("div",{class:["content__"+e.slotKey]},t()[e.slotKey])},Ul={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Ml=(t(247),t(248),Object(zo.a)(Ul,(function(){var n=this._self._c;return n("span",[n("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[n("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),n("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),n("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),ql={functional:!0,render(n,{parent:e,children:t}){if(e._isMounted)return t;e.$once("hook:mounted",()=>{e.$forceUpdate()})}};Vt.config.productionTip=!1,Vt.use(Ga),Vt.use(Dl),Vt.mixin(function(n,e,t=Vt){!function(n){n.locales&&Object.keys(n.locales).forEach(e=>{n.locales[e].path=e});Object.freeze(n)}(e),t.$vuepress.$set("siteData",e);const r=new(n(t.$vuepress.$get("siteData"))),i=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(r)),s={};return Object.keys(i).reduce((n,e)=>(e.startsWith("$")&&(n[e]=i[e].get),n),s),{computed:s}}(n=>class{setPage(n){this.__page=n}get $site(){return n}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:n={}}=this.$site;let e,t;for(const r in n)"/"===r?t=n[r]:0===this.$page.path.indexOf(r)&&(e=n[r]);return e||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:n}=this.$page.frontmatter;return"string"==typeof n&&n}get $title(){const n=this.$page,{metaTitle:e}=this.$page.frontmatter;if("string"==typeof e)return e;const t=this.$siteTitle,r=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?r?r+" | "+t:t:r||"VuePress"}get $description(){const n=function(n){if(n){const e=n.filter(n=>"description"===n.name)[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(n,e){for(let t=0;t<n.length;t++){const r=n[t];if(r.path.toLowerCase()===e.toLowerCase())return r}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},Il)),Vt.component("Content",Nl),Vt.component("ContentSlotsDistributor",Fl),Vt.component("OutboundLink",Ml),Vt.component("ClientOnly",ql),Vt.component("Layout",lo("Layout")),Vt.component("NotFound",lo("NotFound")),Vt.prototype.$withBase=function(n){const e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.9",hash:"86464bf"},async function(n){const e="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:Il.routerBase||Il.base,t=new Ga({base:e,mode:"history",fallback:!1,routes:Sl,scrollBehavior:(n,e,t)=>t||(n.hash?!Vt.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})});!function(n){n.beforeEach((e,t,r)=>{if(jl(n,e.path))r();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){const t=e.path.replace(/\/$/,"")+".html";jl(n,t)?r(t):r()}else r();else{const t=e.path+"/",i=e.path+".html";jl(n,i)?r(i):jl(n,t)?r(t):r()}})}(t);const r={};try{await Promise.all(Cl.filter(n=>"function"==typeof n).map(e=>e({Vue:Vt,options:r,router:t,siteData:Il,isServer:n})))}catch(n){console.error(n)}return{app:new Vt(Object.assign(r,{router:t,render:n=>n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},Bl.map(e=>n(e)))])})),router:t}}(!1).then(({app:n,router:e})=>{e.onReady(()=>{n.$mount("#app")})})}]);