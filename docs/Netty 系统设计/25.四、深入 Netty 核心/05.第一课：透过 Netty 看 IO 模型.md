---
title: 第一课：透过 Netty 看 IO 模型
date: 2024-09-18 21:07:35
permalink: /pages/65674f/
---
## 前言

从今天开始我们来聊聊Netty的那些事儿，我们都知道Netty是一个高性能异步事件驱动的网络框架。

它的设计异常优雅简洁，扩展性高，稳定性强。拥有非常详细完整的用户文档。

同时内置了很多非常有用的模块基本上做到了开箱即用，用户只需要编写短短几行代码，就可以快速构建出一个具有`高吞吐`，`低延时`，`更少的资源消耗`，`高性能（非必要的内存拷贝最小化）`等特征的高并发网络应用程序。

本文我们来探讨下支持Netty具有`高吞吐`，`低延时`特征的基石----netty的`网络IO模型`。

由Netty的`网络IO模型`开始，我们来正式揭开本系列Netty源码解析的序幕：

## 网络包接收流程

![image-20240919085201703](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190852001.png)

* 当`网络数据帧`通过网络传输到达网卡时，网卡会将网络数据帧通过`DMA的方式`放到`环形缓冲区RingBuffer`中

::: note

`RingBuffer`是网卡在启动的时候`分配和初始化`的`环形缓冲队列`。当`RingBuffer满`的时候，新来的数据包就会被`丢弃`。我们可以通过`ifconfig`命令查看网卡收发数据包的情况。其中`overruns`数据项表示当`RingBuffer满`时，被`丢弃的数据包`。如果发现出现丢包情况，可以通过`ethtool命令`来增大RingBuffer长度。

:::

* 当`DMA操作完成`时，网卡会向CPU发起一个`硬中断`，告诉`CPU`有网络数据到达。CPU调用网卡驱动注册的`硬中断响应程序`。网卡硬中断响应程序会为网络数据帧创建内核数据结构`sk_buffer`，并将网络数据帧`拷贝`到`sk_buffer`中。然后发起`软中断请求`，通知`内核`有新的网络数据帧到达。

::: note

`sk_buff`缓冲区，是一个维护网络帧结构的`双向链表`，链表中的每一个元素都是一个`网络帧`。虽然 TCP/IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而`无需进行数据复制`。

:::

* 内核线程`ksoftirqd`发现有软中断请求到来，随后调用网卡驱动注册的`poll函数`，`poll函数`将`sk_buffer`中的`网络数据包`送到内核协议栈中注册的`ip_rcv函数`中。

::: note

`每个CPU`会绑定`一个ksoftirqd`内核线程`专门`用来处理`软中断响应`。2个 CPU 时，就会有 `ksoftirqd/0` 和 `ksoftirqd/1`这两个内核线程。

:::

::: note

**这里有个事情需要注意下：** 网卡接收到数据后，当`DMA拷贝完成`时，向CPU发出`硬中断`，这时`哪个CPU`上响应了这个`硬中断`，那么在网卡`硬中断响应程序`中发出的`软中断请求`也会在`这个CPU绑定的ksoftirqd线程`中响应。所以如果发现Linux软中断，CPU消耗都`集中在一个核上`的话，那么就需要调整硬中断的`CPU亲和性`，来将硬中断`打散`到`不通的CPU核`上去。

:::

* 在`ip_rcv函数`中也就是上图中的`网络层`，`取出`数据包的`IP头`，判断该数据包下一跳的走向，如果数据包是发送给本机的，则取出传输层的协议类型（`TCP`或者`UDP`)，并`去掉`数据包的`IP头`，将数据包交给上图中得`传输层`处理。

::: note

传输层的处理函数：`TCP协议`对应内核协议栈中注册的`tcp_rcv函数`，`UDP协议`对应内核协议栈中注册的`udp_rcv函数`。

:::

* 当我们采用的是`TCP协议`时，数据包到达传输层时，会在内核协议栈中的`tcp_rcv函数`处理，在tcp_rcv函数中`去掉`TCP头，根据`四元组（源IP，源端口，目的IP，目的端口）`查找`对应的Socket`，如果找到对应的Socket则将网络数据包中的传输数据拷贝到`Socket`中的`接收缓冲区`中。如果没有找到，则发送一个`目标不可达`的`icmp`包。

内核在接收网络数据包时所做的工作我们就介绍完了，现在我们把视角放到应用层，当我们程序通过系统调用`read`读取`Socket接收缓冲区`中的数据时，如果接收缓冲区中`没有数据`，那么应用程序就会在系统调用上`阻塞`，直到Socket接收缓冲区`有数据`，然后`CPU`将`内核空间`（Socket接收缓冲区）的数据`拷贝`到`用户空间`，最后系统调用`read返回`，应用程序`读取`数据

### 性能开销

从内核处理网络数据包接收的整个过程来看，内核帮我们做了非常之多的工作，最终我们的应用程序才能读取到网络数据。

随着而来的也带来了很多的性能开销，结合前面介绍的网络数据包接收过程我们来看下网络数据包接收的过程中都有哪些性能开销：

- 应用程序通过`系统调用`从`用户态`转为`内核态`的开销以及系统调用`返回`时从`内核态`转为`用户态`的开销。
- 网络数据从`内核空间`通过`CPU拷贝`到`用户空间`的开销。
- 内核线程`ksoftirqd`响应`软中断`的开销。
- `CPU`响应`硬中断`的开销。
- `DMA拷贝`网络数据包到`内存`中的开销。

## 网络包发送流程

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190856740.png)

- 当我们在应用程序中调用`send`系统调用发送数据时，由于是系统调用所以线程会发生一次用户态到内核态的转换，在内核中首先根据`fd`将真正的Socket找出，这个Socket对象中记录着各种协议栈的函数地址，然后构造`struct msghdr`对象，将用户需要发送的数据全部封装在这个`struct msghdr`结构体中。
- 调用内核协议栈函数`inet_sendmsg`，发送流程进入内核协议栈处理。在进入到内核协议栈之后，内核会找到Socket上的具体协议的发送函数。

> 比如：我们使用的是`TCP协议`，对应的`TCP协议`发送函数是`tcp_sendmsg`，如果是`UDP协议`的话，对应的发送函数为`udp_sendmsg`。

- 在`TCP协议`的发送函数`tcp_sendmsg`中，创建内核数据结构`sk_buffer`,将`struct msghdr`结构体中的发送数据`拷贝`到`sk_buffer`中。调用`tcp_write_queue_tail`函数获取`Socket`发送队列中的队尾元素，将新创建的`sk_buffer`添加到`Socket`发送队列的尾部。

> `Socket`的发送队列是由`sk_buffer`组成的一个`双向链表`。

> 发送流程走到这里，用户要发送的数据总算是从`用户空间`拷贝到了`内核`中，这时虽然发送数据已经`拷贝`到了内核`Socket`中的`发送队列`中，但并不代表内核会开始发送，因为`TCP协议`的`流量控制`和`拥塞控制`，用户要发送的数据包`并不一定`会立马被发送出去，需要符合`TCP协议`的发送条件。如果`没有达到发送条件`，那么本次`send`系统调用就会直接返回。

- 如果符合发送条件，则开始调用`tcp_write_xmit`内核函数。在这个函数中，会循环获取`Socket`发送队列中待发送的`sk_buffer`，然后进行`拥塞控制`以及`滑动窗口的管理`。
- 将从`Socket`发送队列中获取到的`sk_buffer`重新`拷贝一份`，设置`sk_buffer副本`中的`TCP HEADER`。

> `sk_buffer` 内部其实包含了网络协议中所有的 `header`。在设置 `TCP HEADER`的时候，只是把指针指向 `sk_buffer`的合适位置。后面再设置 `IP HEADER`的时候，在把指针移动一下就行，避免频繁的内存申请和拷贝，效率很高。

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190856347.png)

> **为什么不直接使用`Socket`发送队列中的`sk_buffer`而是需要拷贝一份呢？**因为`TCP协议`是支持`丢包重传`的，在没有收到对端的`ACK`之前，这个`sk_buffer`是不能删除的。内核每次调用网卡发送数据的时候，实际上传递的是`sk_buffer`的`拷贝副本`，当网卡把数据发送出去后，`sk_buffer`拷贝副本会被释放。当收到对端的`ACK`之后，`Socket`发送队列中的`sk_buffer`才会被真正删除。

- 当设置完`TCP头`后，内核协议栈`传输层`的事情就做完了，下面通过调用`ip_queue_xmit`内核函数，正式来到内核协议栈`网络层`的处理。

  > 通过`route`命令可以查看本机路由配置。

  > 如果你使用 `iptables`配置了一些规则，那么这里将检测`是否命中`规则。如果你设置了非常`复杂的 netfilter 规则`，在这个函数里将会导致你的线程 `CPU 开销`会`极大增加`。

- - 将`sk_buffer`中的指针移动到`IP头`位置上，设置`IP头`。
  - 执行`netfilters`过滤。过滤通过之后，如果数据大于 `MTU`的话，则执行分片。
  - 检查`Socket`中是否有缓存路由表，如果没有的话，则查找路由项，并缓存到`Socket`中。接着在把路由表设置到`sk_buffer`中。

- 内核协议栈`网络层`的事情处理完后，现在发送流程进入了到了`邻居子系统`，`邻居子系统`位于内核协议栈中的`网络层`和`网络接口层`之间，用于发送`ARP请求`获取`MAC地址`，然后将`sk_buffer`中的指针移动到`MAC头`位置，填充`MAC头`。

- 经过`邻居子系统`的处理，现在`sk_buffer`中已经封装了一个完整的`数据帧`，随后内核将`sk_buffer`交给`网络设备子系统`进行处理。`网络设备子系统`主要做以下几项事情：

- - 选择发送队列（`RingBuffer`）。因为网卡拥有多个发送队列，所以在发送前需要选择一个发送队列。
  - 将`sk_buffer`添加到发送队列中。
  - 循环从发送队列（`RingBuffer`）中取出`sk_buffer`，调用内核函数`sch_direct_xmit`发送数据，其中会调用`网卡驱动程序`来发送数据。

> 以上过程全部是用户线程的内核态在执行，占用的CPU时间是系统态时间(`sy`)，当分配给用户线程的`CPU quota`用完的时候，会触发`NET_TX_SOFTIRQ`类型的软中断，内核线程`ksoftirqd`会响应这个软中断，并执行`NET_TX_SOFTIRQ`类型的软中断注册的回调函数`net_tx_action`，在回调函数中会执行到驱动程序函数 `dev_hard_start_xmit`来发送数据。

> **注意：当触发`NET_TX_SOFTIRQ`软中断来发送数据时，后边消耗的 CPU 就都显示在 `si`这里了，不会消耗用户进程的系统态时间（`sy`）了。**

> 从这里可以看到网络包的发送过程和接受过程是不同的，在介绍网络包的接受过程时，我们提到是通过触发`NET_RX_SOFTIRQ`类型的软中断在内核线程`ksoftirqd`中执行`内核网络协议栈`接受数据。而在网络数据包的发送过程中是`用户线程的内核态`在执行`内核网络协议栈`，只有当线程的`CPU quota`用尽时，才触发`NET_TX_SOFTIRQ`软中断来发送数据。

> 在整个网络包的发送和接受过程中，`NET_TX_SOFTIRQ`类型的软中断只会在发送网络包时并且当用户线程的`CPU quota`用尽时，才会触发。剩下的接受过程中触发的软中断类型以及发送完数据触发的软中断类型均为`NET_RX_SOFTIRQ`。所以这就是你在服务器上查看 `/proc/softirqs`，一般 `NET_RX`都要比 `NET_TX`大很多的的原因

- 现在发送流程终于到了网卡真实发送数据的阶段，前边我们讲到无论是用户线程的内核态还是触发`NET_TX_SOFTIRQ`类型的软中断在发送数据的时候最终会调用到网卡的驱动程序函数`dev_hard_start_xmit`来发送数据。在网卡驱动程序函数`dev_hard_start_xmit`中会将`sk_buffer`映射到网卡可访问的`内存 DMA 区域`，最终网卡驱动程序通过`DMA`的方式将`数据帧`通过物理网卡发送出去。
- 当数据发送完毕后，还有最后一项重要的工作，就是清理工作。数据发送完毕后，网卡设备会向`CPU`发送一个硬中断，`CPU`调用网卡驱动程序注册的`硬中断响应程序`，在硬中断响应中触发`NET_RX_SOFTIRQ`类型的软中断，在软中断的回调函数`igb_poll`中清理释放 `sk_buffer`，清理`网卡`发送队列（`RingBuffer`），解除 DMA 映射。

> 无论`硬中断`是因为`有数据要接收`，还是说`发送完成通知`，从硬中断触发的软中断都是 `NET_RX_SOFTIRQ`。

> 这里释放清理的只是`sk_buffer`的副本，真正的`sk_buffer`现在还是存放在`Socket`的发送队列中。前面在`传输层`处理的时候我们提到过，因为传输层需要`保证可靠性`，所以 `sk_buffer`其实还没有删除。它得等收到对方的 ACK 之后才会真正删除。

### 性能开销

前边我们提到了在网络包接收过程中涉及到的性能开销，现在介绍完了网络包的发送过程，我们来看下在数据包发送过程中的性能开销：

- 和接收数据一样，应用程序在调用`系统调用send`的时候会从`用户态`转为`内核态`以及发送完数据后，`系统调用`返回时从`内核态`转为`用户态`的开销。

- 用户线程内核态`CPU quota`用尽时触发`NET_TX_SOFTIRQ`类型软中断，内核响应软中断的开销。

- 网卡发送完数据，向`CPU`发送硬中断，`CPU`响应硬中断的开销。以及在硬中断中发送`NET_RX_SOFTIRQ`软中断执行具体的内存清理动作。内核响应软中断的开销。

- 内存拷贝的开销。我们来回顾下在数据包发送的过程中都发生了哪些内存拷贝：

- - 在内核协议栈的传输层中，`TCP协议`对应的发送函数`tcp_sendmsg`会申请`sk_buffer`，将用户要发送的数据`拷贝`到`sk_buffer`中。
  - 在发送流程从传输层到网络层的时候，会`拷贝`一个`sk_buffer副本`出来，将这个`sk_buffer副本`向下传递。原始`sk_buffer`保留在`Socket`发送队列中，等待网络对端`ACK`，对端`ACK`后删除`Socket`发送队列中的`sk_buffer`。对端没有发送`ACK`，则重新从`Socket`发送队列中发送，实现`TCP协议`的可靠传输。
  - 在网络层，如果发现要发送的数据大于`MTU`，则会进行分片操作，申请额外的`sk_buffer`，并将原来的sk_buffer`拷贝`到多个小的sk_buffer中。

## 再谈(阻塞，非阻塞)与(同步，异步)

在我们聊完网络数据的接收和发送过程后，我们来谈下IO中特别容易混淆的概念：`阻塞与同步`，`非阻塞与异步`。

网上各种博文还有各种书籍中有大量的关于这两个概念的解释，但是笔者觉得还是不够形象化，只是对概念的生硬解释，如果硬套概念的话，其实感觉`阻塞与同步`，`非阻塞与异步`还是没啥区别，时间长了，还是比较模糊容易混淆。

所以笔者在这里尝试换一种更加形象化，更加容易理解记忆的方式来清晰地解释下什么是`阻塞与非阻塞`，什么是`同步与异步`。

经过前边对网络数据包接收流程的介绍，在这里我们可以将整个流程总结为两个阶段：

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190903897.png)

* **数据准备阶段：** 在这个阶段，网络数据包到达网卡，通过`DMA`的方式将数据包拷贝到内存中，然后经过硬中断，软中断，接着通过内核线程`ksoftirqd`经过内核协议栈的处理，最终将数据发送到`内核Socket`的接收缓冲区中。
* **数据拷贝阶段：** 当数据到达`内核Socket`的接收缓冲区中时，此时数据存在于`内核空间`中，需要将数据`拷贝`到`用户空间`中，才能够被应用程序读取。

### 阻塞与非阻塞

阻塞与非阻塞的区别主要发生在第一阶段：`数据准备阶段`。

当应用程序发起`系统调用read`时，线程从用户态转为内核态，读取内核`Socket`的接收缓冲区中的网络数据。

#### 阻塞

如果这时内核`Socket`的接收缓冲区没有数据，那么线程就会一直`等待`，直到`Socket`接收缓冲区有数据为止。随后将数据从内核空间拷贝到用户空间，`系统调用read`返回。

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190906844.png)

从图中我们可以看出：**阻塞**的特点是在第一阶段和第二阶段`都会等待`。

#### 非阻塞

`阻塞`和`非阻塞`主要的区分是在第一阶段：`数据准备阶段`。

- 在第一阶段，当`Socket`的接收缓冲区中没有数据的时候，`阻塞模式下`应用线程会一直等待。`非阻塞模式下`应用线程不会等待，`系统调用`直接返回错误标志`EWOULDBLOCK`。
- 当`Socket`的接收缓冲区中有数据的时候，`阻塞`和`非阻塞`的表现是一样的，都会进入第二阶段`等待`数据从`内核空间`拷贝到`用户空间`，然后`系统调用返回`。

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190906040.png)

从上图中，我们可以看出：**非阻塞**的特点是第一阶段`不会等待`，**但是在第二阶段还是会`等待`。**

### 同步与异步

`同步`与`异步`主要的区别发生在第二阶段：`数据拷贝阶段`。

前边我们提到在`数据拷贝阶段`主要是将数据从`内核空间`拷贝到`用户空间`。然后应用程序才可以读取数据。

当内核`Socket`的接收缓冲区有数据到达时，进入第二阶段。

#### 同步

`同步模式`在数据准备好后，是由`用户线程`的`内核态`来执行`第二阶段`。所以应用程序会在第二阶段发生`阻塞`，直到数据从`内核空间`拷贝到`用户空间`，系统调用才会返回。

Linux下的 `epoll`和Mac 下的 `kqueue`都属于`同步 IO`。

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190907790.png)

#### 异步

`异步模式`下是由`内核`来执行第二阶段的数据拷贝操作，当`内核`执行完第二阶段，会通知用户线程IO操作已经完成，并将数据回调给用户线程。所以在`异步模式`下 `数据准备阶段`和`数据拷贝阶段`均是由`内核`来完成，不会对应用程序造成任何阻塞。

基于以上特征，我们可以看到`异步模式`需要内核的支持，比较依赖操作系统底层的支持。

在目前流行的操作系统中，只有Windows 中的 `IOCP`才真正属于异步 IO，实现的也非常成熟。但Windows很少用来作为服务器使用。

而常用来作为服务器使用的Linux，`异步IO机制`实现的不够成熟，与NIO相比性能提升的也不够明显。

但Linux kernel 在5.1版本由Facebook的大神Jens Axboe引入了新的异步IO库`io_uring` 改善了原来Linux native AIO的一些性能问题。性能相比`Epoll`以及之前原生的`AIO`提高了不少，值得关注。

![](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190908945.png)

## IO模型

在进行网络IO操作时，用什么样的IO模型来读写数据将在很大程度上决定了网络框架的IO性能。所以IO模型的选择是构建一个高性能网络框架的基础。

在《UNIX 网络编程》一书中介绍了五种IO模型：`阻塞IO`,`非阻塞IO`,`IO多路复用`,`信号驱动IO`,`异步IO`，每一种IO模型的出现都是对前一种的升级优化。

下面我们就来分别介绍下这五种IO模型各自都解决了什么问题，适用于哪些场景，各自的优缺点是什么？

### 阻塞IO（BIO）

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190908331.png)

经过前一小节对`阻塞`这个概念的介绍，相信大家可以很容易理解`阻塞IO`的概念和过程。

既然这小节我们谈的是`IO`，那么下边我们来看下在`阻塞IO`模型下，网络数据的读写过程。

#### 阻塞读

当用户线程发起`read`系统调用，用户线程从用户态切换到内核态，在内核中去查看`Socket`接收缓冲区是否有数据到来。

- `Socket`接收缓冲区中`有数据`，则用户线程在内核态将内核空间中的数据拷贝到用户空间，系统IO调用返回。
- `Socket`接收缓冲区中`无数据`，则用户线程让出CPU，进入`阻塞状态`。当数据到达`Socket`接收缓冲区后，内核唤醒`阻塞状态`中的用户线程进入`就绪状态`，随后经过CPU的调度获取到`CPU quota`进入`运行状态`，将内核空间的数据拷贝到用户空间，随后系统调用返回。

#### 阻塞写

当用户线程发起`send`系统调用时，用户线程从用户态切换到内核态，将发送数据从用户空间拷贝到内核空间中的`Socket`发送缓冲区中。

- 当`Socket`发送缓冲区能够容纳下发送数据时，用户线程会将全部的发送数据写入`Socket`缓冲区，然后执行在《网络包发送流程》这小节介绍的后续流程，然后返回。
- 当`Socket`发送缓冲区空间不够，无法容纳下全部发送数据时，用户线程让出CPU,进入`阻塞状态`，直到`Socket`发送缓冲区能够容纳下全部发送数据时，内核唤醒用户线程，执行后续发送流程。

`阻塞IO`模型下的写操作做事风格比较硬刚，非得要把全部的发送数据写入发送缓冲区才肯善罢甘休。

#### 阻塞IO模型

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190910768.png)

由于`阻塞IO`的读写特点，所以导致在`阻塞IO`模型下，每个请求都需要被一个独立的线程处理。一个线程在同一时刻只能与一个连接绑定。来一个请求，服务端就需要创建一个线程用来处理请求。

当客户端请求的并发量突然增大时，服务端在一瞬间就会创建出大量的线程，而创建线程是需要系统资源开销的，这样一来就会一瞬间占用大量的系统资源。

如果客户端创建好连接后，但是一直不发数据，通常大部分情况下，网络连接也`并不`总是有数据可读，那么在空闲的这段时间内，服务端线程就会一直处于`阻塞状态`，无法干其他的事情。CPU也`无法得到充分的发挥`，同时还会`导致大量线程切换的开销`。

#### 适用场景

基于以上`阻塞IO模型`的特点，该模型只适用于`连接数少`，`并发度低`的业务场景。

比如公司内部的一些管理系统，通常请求数在100个左右，使用`阻塞IO模型`还是非常适合的。而且性能还不输NIO。

该模型在C10K之前，是普遍被采用的一种IO模型。

### 非阻塞IO（NIO）

`阻塞IO模型`最大的问题就是一个线程只能处理一个连接，如果这个连接上没有数据的话，那么这个线程就只能阻塞在系统IO调用上，不能干其他的事情。这对系统资源来说，是一种极大的浪费。同时大量的线程上下文切换，也是一个巨大的系统开销。

所以为了解决这个问题，**我们就需要用尽可能少的线程去处理更多的连接。**，`网络IO模型的演变`也是根据这个需求来一步一步演进的。

基于这个需求，第一种解决方案`非阻塞IO`就出现了。我们在上一小节中介绍了`非阻塞`的概念，现在我们来看下网络读写操作在`非阻塞IO`下的特点：

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190911259.png)

#### 非阻塞读

当用户线程发起非阻塞`read`系统调用时，用户线程从`用户态`转为`内核态`，在内核中去查看`Socket`接收缓冲区是否有数据到来。

- `Socket`接收缓冲区中`无数据`，系统调用立马返回，并带有一个 `EWOULDBLOCK` 或 `EAGAIN`错误，这个阶段用户线程`不会阻塞`，也`不会让出CPU`，而是会继续`轮训`直到`Socket`接收缓冲区中有数据为止。
- `Socket`接收缓冲区中`有数据`，用户线程在`内核态`会将`内核空间`中的数据拷贝到`用户空间`，**注意**这个数据拷贝阶段，应用程序是`阻塞的`，当数据拷贝完成，系统调用返回。

#### 非阻塞写

前边我们在介绍`阻塞写`的时候提到`阻塞写`的风格特别的硬朗，头比较铁非要把全部发送数据一次性都写到`Socket`的发送缓冲区中才返回，如果发送缓冲区中没有足够的空间容纳，那么就一直阻塞死等，特别的刚。

相比较而言`非阻塞写`的特点就比较佛系，当发送缓冲区中没有足够的空间容纳全部发送数据时，`非阻塞写`的特点是`能写多少写多少`，写不下了，就立即返回。并将写入到发送缓冲区的字节数返回给应用程序，方便用户线程不断的`轮训`尝试将`剩下的数据`写入发送缓冲区中。

#### 非阻塞IO模型

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190912819.png)

基于以上`非阻塞IO`的特点，我们就不必像`阻塞IO`那样为每个请求分配一个线程去处理连接上的读写了。

我们可以利用**一个线程或者很少的线程**，去`不断地轮询`每个`Socket`的接收缓冲区是否有数据到达，如果没有数据，`不必阻塞`线程，而是接着去`轮询`下一个`Socket`接收缓冲区，直到轮询到数据后，处理连接上的读写，或者交给业务线程池去处理，轮询线程则`继续轮询`其他的`Socket`接收缓冲区。

这样一个`非阻塞IO模型`就实现了我们在本小节开始提出的需求：**我们需要用尽可能少的线程去处理更多的连接**

#### 适用场景 

虽然`非阻塞IO模型`与`阻塞IO模型`相比，减少了很大一部分的资源消耗和系统开销。

但是它仍然有很大的性能问题，因为在`非阻塞IO模型`下，需要用户线程去`不断地`发起`系统调用`去轮训`Socket`接收缓冲区，这就需要用户线程不断地从`用户态`切换到`内核态`，`内核态`切换到`用户态`。随着并发量的增大，这个上下文切换的开销也是巨大的。

所以单纯的`非阻塞IO`模型还是无法适用于高并发的场景。只能适用于`C10K`以下的场景。

### IO 多路复用

### 信号驱动IO 

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190942510.png)信号驱动IO.png

大家对这个装备肯定不会陌生，当我们去一些美食城吃饭的时候，点完餐付了钱，老板会给我们一个信号器。然后我们带着这个信号器可以去找餐桌，或者干些其他的事情。当信号器亮了的时候，这时代表饭餐已经做好，我们可以去窗口取餐了。

这个典型的生活场景和我们要介绍的`信号驱动IO模型`就很像。

在`信号驱动IO模型`下，用户进程操作通过`系统调用 sigaction 函数`发起一个 IO 请求，在对应的`socket`注册一个`信号回调`，此时`不阻塞`用户进程，进程会继续工作。当内核数据就绪时，内核就为该进程生成一个 `SIGIO 信号`，通过信号回调通知进程进行相关 IO 操作。

> 这里需要注意的是：`信号驱动式 IO 模型`依然是`同步IO`，因为它虽然可以在等待数据的时候不被阻塞，也不会频繁的轮询，但是当数据就绪，内核信号通知后，用户进程依然要自己去读取数据，在`数据拷贝阶段`发生阻塞。

> 信号驱动 IO模型 相比于前三种 IO 模型，实现了在等待数据就绪时，进程不被阻塞，主循环可以继续工作，所以`理论上`性能更佳。

但是实际上，使用`TCP协议`通信时，`信号驱动IO模型`几乎`不会被采用`。原因如下：

- 信号IO 在大量 IO 操作时可能会因为信号队列溢出导致没法通知
- `SIGIO 信号`是一种 Unix 信号，信号没有附加信息，如果一个信号源有多种产生信号的原因，信号接收者就无法确定究竟发生了什么。而 TCP socket 生产的信号事件有七种之多，这样应用程序收到 SIGIO，根本无从区分处理。

但`信号驱动IO模型`可以用在 `UDP`通信上，因为UDP 只有`一个数据请求事件`，这也就意味着在正常情况下 UDP 进程只要捕获 SIGIO 信号，就调用 `read 系统调用`读取到达的数据。如果出现异常，就返回一个异常错误。

------

这里插句题外话，大家觉不觉得`阻塞IO模型`在生活中的例子就像是我们在食堂排队打饭。你自己需要排队去打饭同时打饭师傅在配菜的过程中你需要等待。

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190942411.png)阻塞IO.png

`IO多路复用模型`就像是我们在饭店门口排队等待叫号。叫号器就好比`select,poll,epoll`可以统一管理全部顾客的`吃饭就绪`事件，客户好比是`socket`连接，谁可以去吃饭了，叫号器就通知谁。

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190942444.png)IO多路复用.png

### 异步IO（AIO）

以上介绍的四种`IO模型`均为`同步IO`，它们都会阻塞在第二阶段`数据拷贝阶段`。

通过在前边小节《同步与异步》中的介绍，相信大家很容易就会理解`异步IO模型`，在`异步IO模型`下，IO操作在`数据准备阶段`和`数据拷贝阶段`均是由内核来完成，不会对应用程序造成任何阻塞。应用进程只需要在`指定的数组`中引用数据即可。

`异步 IO` 与`信号驱动 IO` 的主要区别在于：`信号驱动 IO` 由内核通知何时可以`开始一个 IO 操作`，而`异步 IO`由内核通知 `IO 操作何时已经完成`。

举个生活中的例子：`异步IO模型`就像我们去一个高档饭店里的包间吃饭，我们只需要坐在包间里面，点完餐（`类比异步IO调用`）之后，我们就什么也不需要管，该喝酒喝酒，该聊天聊天，饭餐做好后服务员（`类比内核`）会自己给我们送到包间（`类比用户空间`）来。整个过程没有任何阻塞。

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190942662.png)异步IO.png

`异步IO`的系统调用需要操作系统内核来支持，目前只有`Window`中的`IOCP`实现了非常成熟的`异步IO机制`。

而`Linux`系统对`异步IO机制`实现的不够成熟，且与`NIO`的性能相比提升也不明显。

> 但Linux kernel 在5.1版本由Facebook的大神Jens Axboe引入了新的异步IO库`io_uring` 改善了原来Linux native AIO的一些性能问题。性能相比`Epoll`以及之前原生的`AIO`提高了不少，值得关注。

再加上`信号驱动IO模型`不适用`TCP协议`，所以目前大部分采用的还是`IO多路复用模型`。

## IO 线程模型 

在前边内容的介绍中，我们详述了网络数据包的接收和发送过程，并通过介绍5种`IO模型`了解了内核是如何读取网络数据并通知给用户线程的。

前边的内容都是以`内核空间`的视角来剖析网络数据的收发模型，本小节我们站在`用户空间`的视角来看下如果对网络数据进行收发。

相对`内核`来讲，`用户空间的IO线程模型`相对就简单一些。这些`用户空间`的`IO线程模型`都是在讨论当多线程一起配合工作时谁负责接收连接，谁负责响应IO 读写、谁负责计算、谁负责发送和接收，仅仅是用户IO线程的不同分工模式罢了。

### Reactor

`Reactor`是利用`NIO`对`IO线程`进行不同的分工：

- 使用前边我们提到的`IO多路复用模型`比如`select,poll,epoll,kqueue`,进行IO事件的注册和监听。
- 将监听到`就绪的IO事件`分发`dispatch`到各个具体的处理`Handler`中进行相应的`IO事件处理`。

通过`IO多路复用技术`就可以不断的监听`IO事件`，不断的分发`dispatch`，就像一个`反应堆`一样，看起来像不断的产生`IO事件`，因此我们称这种模式为`Reactor`模型。

下面我们来看下`Reactor模型`的三种分类：

#### 单Reactor单线程

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190944960.png)单Reactor单线程

`Reactor模型`是依赖`IO多路复用技术`实现监听`IO事件`，从而源源不断的产生`IO就绪事件`，在Linux系统下我们使用`epoll`来进行`IO多路复用`，我们以Linux系统为例：

- 单`Reactor`意味着只有一个`epoll`对象，用来监听所有的事件，比如`连接事件`，`读写事件`。
- `单线程`意味着只有一个线程来执行`epoll_wait`获取`IO就绪`的`Socket`，然后对这些就绪的`Socket`执行读写，以及后边的业务处理也依然是这个线程。

`单Reactor单线程`模型就好比我们开了一个很小很小的小饭馆，作为老板的我们需要一个人干所有的事情，包括：迎接顾客（`accept事件`），为顾客介绍菜单等待顾客点菜(`IO请求`)，做菜（`业务处理`），上菜（`IO响应`），送客（`断开连接`）。

#### 单Reactor多线程

随着客人的增多（`并发请求`），显然饭馆里的事情只有我们一个人干（`单线程`）肯定是忙不过来的，这时候我们就需要多招聘一些员工（`多线程`）来帮着一起干上述的事情。

于是就有了`单Reactor多线程`模型：

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190944074.png)单Reactor多线程

- 这种模式下，也是只有一个`epoll`对象来监听所有的`IO事件`，一个线程来调用`epoll_wait`获取`IO就绪`的`Socket`。
- 但是当`IO就绪事件`产生时，这些`IO事件`对应处理的业务`Handler`，我们是通过线程池来执行。这样相比`单Reactor单线程`模型提高了执行效率，充分发挥了多核CPU的优势。

#### 主从Reactor多线程

做任何事情都要区分`事情的优先级`，我们应该`优先高效`的去做`优先级更高`的事情，而不是一股脑不分优先级的全部去做。

当我们的小饭馆客人越来越多（`并发量越来越大`），我们就需要扩大饭店的规模，在这个过程中我们发现，`迎接客人`是饭店最重要的工作，我们要先把客人迎接进来，不能让客人一看人多就走掉，只要客人进来了，哪怕菜做的慢一点也没关系。

于是，`主从Reactor多线程`模型就产生了：

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190944188.png)主从Reactor多线程

- 我们由原来的`单Reactor`变为了`多Reactor`。`主Reactor`用来优先`专门`做优先级最高的事情，也就是迎接客人（`处理连接事件`），对应的处理`Handler`就是图中的`acceptor`。
- 当创建好连接，建立好对应的`socket`后，在`acceptor`中将要监听的`read事件`注册到`从Reactor`中，由`从Reactor`来监听`socket`上的`读写`事件。
- 最终将读写的业务逻辑处理交给线程池处理。

> **注意**：这里向`从Reactor`注册的只是`read事件`，并没有注册`write事件`，因为`read事件`是由`epoll内核`触发的，而`write事件`则是由用户业务线程触发的（`什么时候发送数据是由具体业务线程决定的`），所以`write事件`理应是由`用户业务线程`去注册。

> 用户线程注册`write事件`的时机是只有当用户发送的数据`无法一次性`全部写入`buffer`时，才会去注册`write事件`，等待`buffer重新可写`时，继续写入剩下的发送数据、如果用户线程可以一股脑的将发送数据全部写入`buffer`，那么也就无需注册`write事件`到`从Reactor`中。

`主从Reactor多线程`模型是现在大部分主流网络框架中采用的一种`IO线程模型`。我们本系列的主题`Netty`就是用的这种模型。

### Proactor

`Proactor`是基于`AIO`对`IO线程`进行分工的一种模型。前边我们介绍了`异步IO模型`，它是操作系统内核支持的一种全异步编程模型，在`数据准备阶段`和`数据拷贝阶段`全程无阻塞。

`ProactorIO线程模型`将`IO事件的监听`，`IO操作的执行`，`IO结果的dispatch`统统交给`内核`来做。

![图片](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/202409190944822.png)proactor.png

**`Proactor模型`组件介绍：**

- `completion handler` 为用户程序定义的异步IO操作回调函数，在异步IO操作完成时会被内核回调并通知IO结果。
- `Completion Event Queue` 异步IO操作完成后，会产生对应的`IO完成事件`，将`IO完成事件`放入该队列中。
- `Asynchronous Operation Processor` 负责`异步IO`的执行。执行完成后产生`IO完成事件`放入`Completion Event Queue` 队列中。
- `Proactor` 是一个事件循环派发器，负责从`Completion Event Queue`中获取`IO完成事件`，并回调与`IO完成事件`关联的`completion handler`。
- `Initiator` 初始化异步操作（`asynchronous operation`）并通过`Asynchronous Operation Processor`将`completion handler`和`proactor`注册到内核。

**`Proactor模型`执行过程：**

- 用户线程发起`aio_read`，并告诉`内核`用户空间中的读缓冲区地址，以便`内核`完成`IO操作`将结果放入`用户空间`的读缓冲区，用户线程直接可以读取结果（`无任何阻塞`）。
- `Initiator` 初始化`aio_read`异步读取操作（`asynchronous operation`）,并将`completion handler`注册到内核。

> 在`Proactor`中我们关心的`IO完成事件`：内核已经帮我们读好数据并放入我们指定的读缓冲区，用户线程可以直接读取。在`Reactor`中我们关心的是`IO就绪事件`：数据已经到来，但是需要用户线程自己去读取。

- 此时用户线程就可以做其他事情了，无需等待IO结果。而内核与此同时开始异步执行IO操作。当`IO操作`完成时会产生一个`completion event`事件，将这个`IO完成事件`放入`completion event queue`中。
- `Proactor`从`completion event queue`中取出`completion event`，并回调与`IO完成事件`关联的`completion handler`。
- 在`completion handler`中完成业务逻辑处理。

### Reactor与Proactor对比

`Reactor`是基于`NIO`实现的一种`IO线程模型`，`Proactor`是基于`AIO`实现的`IO线程模型`。

`Reactor`关心的是`IO就绪事件`，`Proactor`关心的是`IO完成事件`。

在`Proactor`中，用户程序需要向内核传递`用户空间的读缓冲区地址`。`Reactor`则不需要。这也就导致了在`Proactor`中每个并发操作都要求有独立的缓存区，在内存上有一定的开销。

`Proactor` 的实现逻辑复杂，编码成本较 `Reactor`要高很多。

`Proactor` 在处理`高耗时 IO`时的性能要高于 `Reactor`，但对于`低耗时 IO`的执行效率提升`并不明显`。

## Netty的IO模型 

在我们介绍完`网络数据包在内核中的收发过程`以及五种`IO模型`和两种`IO线程模型`后，现在我们来看下`netty`中的IO模型是什么样的。

在我们介绍`Reactor IO线程模型`的时候提到有三种`Reactor模型`：`单Reactor单线程`，`单Reactor多线程`，`主从Reactor多线程`。

这三种`Reactor模型`在`netty`中都是支持的，但是我们常用的是`主从Reactor多线程模型`。

而我们之前介绍的三种`Reactor`只是一种模型，是一种设计思想。实际上各种网络框架在实现中并不是严格按照模型来实现的，会有一些小的不同，但大体设计思想上是一样的。

下面我们来看下`netty`中的`主从Reactor多线程模型`是什么样子的？

![图片](https://mmbiz.qpic.cn/mmbiz_png/sOIZXFW0vUY4ypDTgZibnVV3K2XJZVLcEqBHAhkKJCkVgiaazsXibAeyzHXtCy8fB3JPwWlq0LL8kWQG6OVwFYDgA/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

- `Reactor`在`netty`中是以`group`的形式出现的，`netty`中将`Reactor`分为两组，一组是`MainReactorGroup`也就是我们在编码中常常看到的`EventLoopGroup bossGroup`,另一组是`SubReactorGroup`也就是我们在编码中常常看到的`EventLoopGroup workerGroup`。
- `MainReactorGroup`中通常只有一个`Reactor`，专门负责做最重要的事情，也就是监听连接`accept`事件。当有连接事件产生时，在对应的处理`handler acceptor`中创建初始化相应的`NioSocketChannel`（代表一个`Socket连接`）。然后以`负载均衡`的方式在`SubReactorGroup`中选取一个`Reactor`，注册上去，监听`Read事件`。

> `MainReactorGroup`中只有一个`Reactor`的原因是，通常我们服务端程序只会`绑定监听`一个端口，如果要`绑定监听`多个端口，就会配置多个`Reactor`。

- `SubReactorGroup`中有多个`Reactor`，具体`Reactor`的个数可以由系统参数 `-D io.netty.eventLoopThreads`指定。默认的`Reactor`的个数为`CPU核数 * 2`。`SubReactorGroup`中的`Reactor`主要负责监听`读写事件`，每一个`Reactor`负责监听一组`socket连接`。将全量的连接`分摊`在多个`Reactor`中。
- 一个`Reactor`分配一个`IO线程`，这个`IO线程`负责从`Reactor`中获取`IO就绪事件`，执行`IO调用获取IO数据`，执行`PipeLine`。

> `Socket连接`在创建后就被`固定的分配`给一个`Reactor`，所以一个`Socket连接`也只会被一个固定的`IO线程`执行，每个`Socket连接`分配一个独立的`PipeLine`实例，用来编排这个`Socket连接`上的`IO处理逻辑`。这种`无锁串行化`的设计的目的是为了防止多线程并发执行同一个socket连接上的`IO逻辑处理`，防止出现`线程安全问题`。同时使系统吞吐量达到最大化

> 由于每个`Reactor`中只有一个`IO线程`，这个`IO线程`既要执行`IO活跃Socket连接`对应的`PipeLine`中的`ChannelHandler`，又要从`Reactor`中获取`IO就绪事件`，执行`IO调用`。所以`PipeLine`中`ChannelHandler`中执行的逻辑不能耗时太长，尽量将耗时的业务逻辑处理放入单独的业务线程池中处理，否则会影响其他连接的`IO读写`，从而近一步影响整个服务程序的`IO吞吐`。

- 当`IO请求`在业务线程中完成相应的业务逻辑处理后，在业务线程中利用持有的`ChannelHandlerContext`引用将响应数据在`PipeLine`中反向传播，最终写回给客户端。

`netty`中的`IO模型`我们介绍完了，下面我们来简单介绍下在`netty`中是如何支持前边提到的三种`Reactor模型`的。

### 配置单Reactor单线程

```Java
EventLoopGroup eventGroup = new NioEventLoopGroup(1);
ServerBootstrap serverBootstrap = new ServerBootstrap(); 
serverBootstrap.group(eventGroup);
```

### 配置多Reactor线程

```Java
EventLoopGroup eventGroup = new NioEventLoopGroup();
ServerBootstrap serverBootstrap = new ServerBootstrap(); 
serverBootstrap.group(eventGroup);
```

### 配置主从Reactor多线程

```Java
EventLoopGroup bossGroup = new NioEventLoopGroup(1); 
EventLoopGroup workerGroup = new NioEventLoopGroup();
ServerBootstrap serverBootstrap = new ServerBootstrap(); 
serverBootstrap.group(bossGroup, workerGroup);
```

## 总结

本文是一篇信息量比较大的文章，用了`25`张图，`22336`个字从内核如何处理网络数据包的收发过程开始展开，随后又在`内核角度`介绍了经常容易混淆的`阻塞与非阻塞`，`同步与异步`的概念。以这个作为铺垫，我们通过一个`C10K`的问题，引出了五种`IO模型`，随后在`IO多路复用`中以技术演进的形式介绍了`select,poll,epoll`的原理和它们综合的对比。最后我们介绍了两种`IO线程模型`以及`netty`中的`Reactor模型`。

感谢大家听我唠叨到这里，哈哈，现在大家可以揉揉眼，伸个懒腰，好好休息一下了。

## 参考资料

* https://mp.weixin.qq.com/s/zAh1yD5IfwuoYdrZ1tGf5Q
